{
  "paper_id": "2312.11974v2",
  "title": "Ms-Senet: Enhancing Speech Emotion Recognition Through Multi-Scale Feature Fusion With Squeeze-And-Excitation Blocks",
  "published": "2023-12-19T09:11:55Z",
  "authors": [
    "Mengbo Li",
    "Yuanzhong Zheng",
    "Dichucheng Li",
    "Yulun Wu",
    "Yaoxuan Wang",
    "Haojun Fei"
  ],
  "keywords": [
    "Speech emotion recognition",
    "multi-scale",
    "squeeze-and-excitation",
    "spatial dropout"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) has become a growing focus of research in human-computer interaction. Spatiotemporal features play a crucial role in SER, yet current research lacks comprehensive spatiotemporal feature learning. This paper focuses on addressing this gap by proposing a novel approach. In this paper, we employ Convolutional Neural Network (CNN) with varying kernel sizes for spatial and temporal feature extraction. Additionally, we introduce Squeeze-and-Excitation (SE) modules to capture and fuse multi-scale features, facilitating effective information fusion for improved emotion recognition and a deeper understanding of the temporal evolution of speech emotion. Moreover, we employ skip-connections and Spatial Dropout (SD) layers to prevent overfitting and increase the model's depth. Our method outperforms the previous state-of-the-art method, achieving an average UAR and WAR improvement of 1.62% and 1.32%, respectively, across six benchmark SER datasets. Further experiments demonstrated that our method can fully extract spatiotemporal features in low-resource conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a crucial role in human-computer interaction, especially in cases like phone customer service and voice assistants, where only speech data is available. As a result, the direct automatic identification of human emotions and emotional states from speech is growing in significance  [1] .\n\nExtracting features from speech has always been an important factor in recognizing speech emotion. Traditional machine learning methods  [2, 3]  like Support Vector Machines (SVM) mainly rely on the effective extraction of handcrafted features. In recent times, with the continuous advancement of deep learning, profound changes have been brought about in speech processing due to the ability of Deep Neural Networks (DNN) to extract intricate features directly from data.\n\nConvolutional Neural Network (CNN)  [4, 5]  are highly effective in detecting and capturing complex features in data. Yenigalla et al.  [9]  improved recognition rates by using manually crafted phoneme sequences and spectrograms. Aftab et al.  [14]  extracted spectral, temporal, and spectral-temporal dependency features using three different filter sizes. Temporal neural networks like Long Short-Term Memory (LSTM)  [6]  and Temporal Convolutional Network (TCN) are widely applied in speech recognition to capture dynamic temporal changes in speech signals. Zhao et al.  [7]  extensively utilized CNN and Bi-LSTM to learn spatiotemporal features. Additionally, besides spatiotemporal features, capturing distant dependency relationships for context modeling is crucial for SER. Ye et al.  [16]  proposed a time-aware bidirectional multi-scale network called TIM-Net, achieving significant accuracy improvement in recognition. However, these methods suffer from the following drawbacks: 1) They extracted a large number of irrelevant features without filtering, causing the discriminator to learn these features. 2) They lacked the combination of local frequency features and long-term temporal features. 3) They lacked suitable regularization mechanisms when employing deeper networks for feature extraction, increasing the risk of overfitting.\n\nTo overcome the limitations of existing methods, we propose a novel approach, and we name the network the Multi-Scale Squeeze-and-Excitation Network, termed MS-SENet. We employ convolutional kernels of different sizes for the initial extraction of multi-scale spatial and temporal features in speech emotion recognition. Concurrently, we introduce Squeeze-and-Excitation (SE) modules to capture and fully exploit these multi-scale features. Moreover, we employ skipconnections and Spatial Dropout (SD) layers to prevent overfitting and increase the model's depth.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Input Pipeline",
      "text": "We use the most commonly-used Mel-Frequency Cepstral Coefficients (MFCCs) features  [10]  as the inputs to MS-SENet. We set the sampling rate to match the original sampling rate of each corpus and segmented the audio signal into 50-ms frames using a Hamming window, with a 12.5-ms overlap. Following the application of a 2048-point Fast Fourier Transform (FFT) to each frame, we perform Mel-scale triangular filterbank analysis on the speech signal. Subsequently, the inverse Discrete Cosine Transform is applied to calculate the MFCC for each frame, with the first 39 coefficients selected for model training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Time-Frequency Feature Extraction",
      "text": "We propose a novel feature fusion method, which mainly utilizes various convolutional kernels for the initial extraction of spatial and temporal features in speech emotion recognition. It employs SE modules for the selection and weighting of multi-scale features, further fusing them with the original information at multiple scales. Fig.  1  presents the network architecture of the method in Body Part I: Time-Frequency Feature Extraction. To enhance the robustness of feature fusion, the Time-Frequency Fusion Block incorporates a SD layer. Next, we provide a detailed introduction to each component.\n\nIn the Time-Frequency Fusion Block, due to the feature extraction of multi-dimensional signals, it's customary to individually account for each dimension when calculating receptive fields. Therefore, we employed a widely-used technique, employing three parallel convolutional kernels of dimensions 9 √ó 1, 1 √ó 11, and 3 √ó 3 to extract the spectral, temporal, and spectral-temporal dependencies in MFCCs.\n\nIn the subsequent stages, we opted not to employ pooling layers and instead utilized SD layers. As shown in Fig.  2 , SD layer can randomly sets all values in a specific dimension to zero. This decision was mainly driven by the following Spatial Dropout Fig.  2 . Illustration of Spatial Dropout. reasons: 1) Pooling layers, during downsampling, may result in the loss of certain original feature information, potentially leading to the omission of emotion-relevant details and thus impacting emotion recognition performance. 2) Emotional information might not solely depend on localized acoustic features but could also correlate with the distribution and temporal aspects of the entire signal. Incorporating spatial dropout layers can maintain positional invariance to some extent, enhancing the model's adaptability and capacity to better capture emotion-relevant information. Subsequent experiments also indicated the superiority of SD layers over pooling layers, with specific details presented in the results section. Finally, the features extracted from each pathway are concatenated and input into the SE layer.\n\nSqueeze-and-Excitation Module. We introduce the classic SE module into the model to perform weighted adjustments on feature channels, enhancing the effectiveness of multiscale features. The SE module aims to adaptively learn relationships between feature channels to better capture critical information. Specifically, we utilize the SE module to apply importance weighting to multi-scale features, strengthening those relevant to emotion recognition. The formula for the SE module is as follows:\n\nIn this formula, ùõø represents the ReLU function, ùúé represents the sigmoid function,\n\nare two fully connected matrices, and r is the number of intermediate hidden nodes. After obtaining the gated unit s, the final output X can be computed by:\n\n(2)",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Temporal-Aware Frequency Feature Learning",
      "text": "In the classification network, we employ TIM-Net  [16] , a method that learns remote emotional dependencies from both forward and backward directions while capturing multi-scale features at the frame level. The detailed network architecture of TIM-Net is illustrated in Bodypart II of Fig.  1 . It is a bidirectional network comprising different time-aware blocks, and as the forward structure is the same as the backward, we will briefly introduce the forward structure of this network. TIM-Net employs TAB(T ) for the selection of temporal features. Each of T consists of two sub-blocks and a sigmoid function ùúé(‚Ä¢) to learn temporal attention maps A, so as to generate time-aware features ùë≠ through input and A. For the two identical sub-blocks of the ùëó-th T ùëó , each sub-block starts by adding a DC Conv with the exponentially increasing dilated rate 2 ùëó -1 and causal constraint. The DC Conv is then followed by a batch normalization, a ReLU function, and a spatial dropout. The input √¨ ùë≠ ùëó for each √¨ T ùëó+1 is derived from the previous TAB, as shown in the Eq. (  3 ):\n\nwhere √¨ ùë≠ 0 comes from the output of the first 1 √ó 1 Conv layer. Then, the bidirectional features are integrated and processed through a dynamic fusion structure as follows:\n\nwhere the global temporal pooling operation G takes an average over temporal dimension. ùíò j reprensents the items of ùíò drf = [ùë§ 1 , ùë§ 2 , . . . , ùë§ ùëõ ] T , and they are trainable parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets. In the experiments, six datasets in different languages are employed, including the Institute of Automation of Chinese Academy of Sciences (CASIA)  [19] , Berlin Emotional dataset (EmoDB)  [20] , Italian language dataset (EMOVO)  [21] , interactive emotional dyadic motion capture database (IEMOCAP)  [22] , Surrey Audio-Visual Expressed Emotion dataset (SAVEE)  [24] , and Ryerson Audio-Visual dataset of Emotional Speech and Song (RAVDESS)  [24] . The six datasets include 1200, 535, 588, 5531, 480 and 1440 data. Implementation details. We implement the proposed models by using the Tensorflow deep learning framework. For the acoustic data, 39-dimensional MFCCs are extracted from the Librosa toolbox  [25] .For the T F F , the dropout rate is 0.2. For the ùëó-th TAB T ùëó , there are 39 kernels size 2 in Conv layers, the dropout rate is 0.1, and the dilated rate is 2 ùëó -1 . We set the number of TAB ùëõ in both directions to 10. For fair comparisons with the SOTA approaches, we perform 10-fold cross-validation (CV) as well as previous works  [14, 15, 17]  in experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "Comparison with SOTA methods. To demonstrate the effectiveness of this approach on each corpus, we employed a 10-fold CV strategy. Table  1  presents the results across the six datasets, illustrating that our method consistently outperforms all previous models significantly. Prior to this, TIM-Net had already achieved the State-Of-The-Art (SOTA) on all six datasets. Compared to the original TIM-Net, our feature fusion approach improved the UA and WA of the network by 1.31% and 1.61%, respectively. Furthermore, it can be observed that MS-SENet maintains excellent emotion recognition capabilities even in cases with a higher number of emotion categories and lower data volume, such as in the SAVEE dataset.\n\nVisualization of learned affective representation. In order to explore the influence of MS-SENet on representation learning, we visualized the representations learned by MS-SENet and TIM-Net  [16]  using the t-SNE technique  [26]  in",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies on all the corpus datasets, including the following variations of MS-SENet: TIM: the MS- Impact of spatial dropout: In this task, compared to using AVGPooling, using the SD layer for regularization in ùëá ùêπùêπmodule increased WA and UA on the six corpus by 3.72% and 2.93%, respectively. However, when using average pooling layers, the accuracy was even lower than the baseline model. This is because average pooling layers led to the loss of original features, resulting in a decrease in model accuracy.\n\nImpact of parallel convolutional kernels size: Here, we evaluated the size of parallel convolutional kernels on six corpora. Under various sizes of convolutional kernels, WA and UA increased by 0.72% and 0.96%. This indicates that this approach can better extract multi-scale features, thereby improving recognition accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we present a novel feature fusion approach, termed MS-SENet, which is designed for the extraction, selection, and weighting of spatial and temporal multi-scale features. It subsequently integrates these features with the original information through multi-scale fusion to better represent the features. Our experimental results demonstrate that the fusion and selection of multi-scale features play a significant role in improving SER tasks. Additional ablation studies further clarify the process of this feature fusion method, providing a new avenue for a deeper understanding of the temporal evolution of speech emotion 1  .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The MS-SENet framework for emotion feature learning consists of a Time-Frequency Fusion Block in its feature",
      "page": 2
    },
    {
      "caption": "Figure 1: presents the network ar-",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of Spatial Dropout.",
      "page": 2
    },
    {
      "caption": "Figure 1: It is a bidi-",
      "page": 3
    },
    {
      "caption": "Figure 3: t-SNE visualizations of features learned from SOTA",
      "page": 4
    },
    {
      "caption": "Figure 3: For a fair comparison, we use the same 8:2 hold-out",
      "page": 4
    },
    {
      "caption": "Figure 3: (b) suggests that distinct representations cluster with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Body Part II Temporal-Aware Frequency Feature Learning": "Conv,1√ó 1 \nConv,1√ó 1 \nùë≠ùíã\nùë≠ùüè\nDynamic fusion\nùì£ùíã\nùì£ùüè\n‚Ä¶\nùôúùíÖùíìùíá\nùôúùíã\nùôúùüè\nùì£ùíã\nùì£ùüè\n‚Ä¶\nùë≠ùíã\nùë≠ùüè"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Body Part I Time-Frequency \nFeature Extraction": "SE\nSE\nConv,1√ó 1 \nConv,1√ó 1 \nConcatenation\nConcatenation\nùì£ùìïùìï\nùì£ùìïùìï"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Ms-Senet"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Model Year IEMOCAP Model Year RAVDESS Model Year SAVEE MHA+DRN",
      "venue": "Model Year IEMOCAP Model Year RAVDESS Model Year SAVEE MHA+DRN"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Light-Sernet"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Ms-Senet"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "Speech emotion Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Bj√∂rn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "7",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T√ºrker Tuncer",
        "Seng√ºl Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "8",
      "title": "Optimal feature selection based speech emotion recognition using two-stream deep convolutional neural network",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "Int. J. Intell. Syst"
    },
    {
      "citation_id": "9",
      "title": "Pseudo-colored rate map representation for speech emotion recognition",
      "authors": [
        "Ilyas Ozer"
      ],
      "year": "2021",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "10",
      "title": "The application of Capsule neural network based CNN for speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Kun-Hong Liu",
        "Wei-Ming Zhang",
        "Kai Jiang"
      ],
      "year": "2020",
      "venue": "The application of Capsule neural network based CNN for speech emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition with dual-sequence LSTM architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition with dual-sequence LSTM architecture"
    },
    {
      "citation_id": "12",
      "title": "Exploring spatio-temporal representations by integrating attentionbased Bi-directional-LSTM-RNNs and FCNs for speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Yu Zheng",
        "Zixing Zhang",
        "Othersi"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attentionbased Bi-directional-LSTM-RNNs and FCNs for speech emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "A lightweight model based on separable convolution for speech emotion recognition",
      "authors": [
        "Ying Zhong",
        "Ying Hu",
        "Hao Huang",
        "Wushour Silamu"
      ],
      "year": "2020",
      "venue": "Interspeech 2020, Virtual Event"
    },
    {
      "citation_id": "14",
      "title": "Speech Emotion Recognition Using Spectrogram and Phoneme Embedding",
      "authors": [
        "Promod Yenigalla",
        "Abhay Kumar",
        "Suraj Tripathi",
        "Chirag Singh",
        "Sibsambhu Kar",
        "Jithendra Vepa"
      ],
      "year": "2018",
      "venue": "Interspeech 2018"
    },
    {
      "citation_id": "15",
      "title": "Efficient speech emotion recognition using multi-scale CNN and attention",
      "authors": [
        "Zixuan Peng",
        "Yu Lu",
        "Shengfeng Pan",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021"
    },
    {
      "citation_id": "16",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "Luefeng Chen",
        "Wanjuan Su",
        "Yu Feng",
        "Min Wu",
        "Jinhua She",
        "Kaoru Hirota"
      ],
      "year": "2020",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "17",
      "title": "GM-TCNet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Xuan-Ze Wang",
        "Yong Xu",
        "Yan Luo",
        "Chang-Li Wu",
        "Li-Yan Chen",
        "Kunhong Liu"
      ],
      "year": "2022",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "18",
      "title": "Improved speech emotion recognition with Mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "19",
      "title": "LIGHT-SERNET: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "20",
      "title": "CTL-MTNet: A novel CapsNet and transfer learning-based mixed task net for single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Jiaxin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "IJCAI 2022"
    },
    {
      "citation_id": "21",
      "title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng And Wei",
        "Yujie Xu",
        "Yong Liu",
        "Kunhong Shan"
      ],
      "venue": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition"
    },
    {
      "citation_id": "22",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia"
      ],
      "year": "2019",
      "venue": "ICASSP 2019"
    },
    {
      "citation_id": "23",
      "title": "3D CNN-based speech emotion recognition using K-means clustering and spectrograms",
      "authors": [
        "Noushin Hajarolasvadi",
        "Hasan Demirel"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "24",
      "title": "Design of speech corpus for Mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge 2008 workshop"
    },
    {
      "citation_id": "25",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes"
      ],
      "year": "2005",
      "venue": "INTERSPEECH 2005"
    },
    {
      "citation_id": "26",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "LREC 2014"
    },
    {
      "citation_id": "27",
      "title": "IEMO-CAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audiovisual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "29",
      "title": "Surrey audio-visual expressed emotion (SAVEE) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "30",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Oriol Battenberg",
        "Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "31",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}