{
  "paper_id": "2110.04042v1",
  "title": "Context-Lgm: Leveraging Object-Context Relation For Context-Aware Object Recognition",
  "published": "2021-10-08T11:31:58Z",
  "authors": [
    "Mingzhou Liu",
    "Xinwei Sun",
    "Fandong Zhang",
    "Yizhou Yu",
    "Yizhou Wang"
  ],
  "keywords": [
    "Index Terms-Object-context relation",
    "Object recognition",
    "Latent generative model",
    "Variational Auto-Encoder",
    "Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Context, as referred to situational factors related to the object of interest, can help infer the object's states or properties in visual recognition. As such contextual features are too diverse (across instances) to be annotated, existing attempts simply exploit image labels as supervision to learn them, resulting in various contextual tricks, such as features pyramid, context attention, etc. However, without carefully modeling the context's properties, especially its relation to the object, their estimated context can suffer from large inaccuracy. To amend this problem, we propose a novel Contextual Latent Generative Model (Context-LGM), which considers the object-context relation and models it in a hierarchical manner. Specifically, we firstly introduce a latent generative model with a pair of correlated latent variables to respectively model the object and context, and embed their correlation via the generative process. Then, to infer contextual features, we reformulate the objective function of Variational Auto-Encoder (VAE), where contextual features are learned as a posterior distribution conditioned on the object. Finally, to implement this contextual posterior, we introduce a Transformer that takes the object's information as a reference and locates correlated contextual factors. The effectiveness of our method is verified by state-of-the-art performance on two contextaware object recognition tasks, i.e. lung cancer prediction and emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works A. Context-Aware Object Recognition",
      "text": "Many context-aware methods have been proposed in visual recognition, such as  [6] -  [18] . The main focus lies in identifying context from complicated surrounding scenarios. To achieve this goal, existing methods follow the paradigm of using only image labels to capture context. Their typical designing tricks include features pyramid, atrous convolution, and attention modules.\n\nThe features pyramid  [6] ,  [7]  and atrous convolution  [8]  were designed to enlarge the receptive fields and avoid missing context with extreme sizes or locations. Recently, the attention mechanism has become a new dominance for context modeling  [9] -  [18] . In these methods, various types of context attention implementations, including spatial-wise attention  [9] ,  [12] -  [14] , channel-wise attention  [11] ,  [16] -  [18] , spatial-channel wise attention  [10] , and graph-based attention  [15] , were exploited to select label-correlated context and suppress other irrelevant backgrounds.\n\nHowever, without further careful modeling of context's properties, especially its relation to the object, their learned context can suffer from large inaccuracy. Using even more designing tricks can not overcome these inherent obstacles they face. In contrast, our Context-LGM uses a pair of correlated latent variables to model the object and context, as well as the correlation between them. During inferring contextual features, we additionally incorporate the object's information as a reference. Such careful modeling of context and its relation to the object enable our methods capture better contextual representations, thus benefit the recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Latent Generative Model",
      "text": "The latent generative model starts with a Bayesian Network, which introduces latent variables that are characterized via their generating processes. The simplest example is z → x, in which z denotes the latent variables that generate x. The goal is learning p(x), which is however intractable for maximum likelihood based method if x is high-dimensional (e.g., x denotes image). The Variational Auto-Encoder (VAE) proposed the Evidence Lower BOund (ELBO) as a surrogate and tractable objective, by introducing the variational distribution q(z|x) that is easy for sampling.\n\nNot only as a generator, the VAE can also be used to infer the latent representations from x. Specifically, it adopts the (variational) Encoder and Decoder, which respectively infer z from x and generate x from z. There is a large literature in VAE to learn such meaningful representations  [23] -  [26] . Specifically, the  [23] ,  [24] ,  [27]  proposed to learn disentangled representations. Particularly, the  [26] ,  [28]  considered the supervised learning tasks and split the latent variables into two parts that were modeled differently. Benefited from its tractability and the ability to infer latent variables, we adopt the VAE framework to model the correlation between object and context via their latent generating processes. We then reformulate the ELBO based on the corresponding Bayesian Network. To the best of our knowledge, we are the first to model contextual features and exploit its relation to the object of interest during inference. Our experimental results will show that such modeling can significantly improve the performance and learn interpretable context.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Transformer",
      "text": "Transformer  [21]  was initially proposed in natural language processing (NLP). Equipped with its unique multi-head attention in the self/cross-attention block, it can well model both the short-term and long-term dependencies among word tokens and thus achieves remarkable performance on various NLP tasks. Recently, Transformer and its attention mechanism have been introduced into visual relationships modeling, such as pixel-level dependencies in image classification  [29] ,  [30]  and segmentation  [31] -  [33] , relationships among proposals in detection  [34] , correlation among keypoints in pose estimation  [35] ,  [36] , and temporal correspondence in tracking and action recognition  [37] ,  [38] .\n\nIn our scenario, we implement a Transformer to model object-context correlation and infer contextual representations. To fit our task, we make careful modifications to the original designs. Specifically, we inherit the multi-head attention in the cross-attention block but implement it in a spatial-wise masking manner. This modification allows us to use the learned object-context correlation as a spatial mask and screen out contextual factors that are correlated with the object. Besides, we remove the Encoder-Decoder structure that is specifically designed for sequence-to-sequence translation. We also reduce the number of stacked attention layers and remove all FFNs to improve computational efficiency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Problem Setup & Notation. Our goal is to predict object's state/label y (e.g., benignity/malignancy or emotion) given an observed image x. The training data contains {x i , y i } i i.i.d ∼ p(x, y). In the graph of generative model, we use \"o\" as a metasymbol to represent any kind of ends: '>', '<', and the empty mark,. That is, the a o-o b can represent a → b, a ← b, or the missing link between a and b. The a → b means b is generated after a. We use x, x, X to respectively denote the random variable, its instance and the matrix.\n\nIn this section, we introduce Contextual Latent Generative Model (Context-LGM) from a hierarchical perspectives of generative modeling (in section III-A), objective reformulation (in section III-B) and finally, the Transformer (in section III-C) for implementation. Specifically, in section III-A, we firstly introduce the Bayesian Network and its encoded generative processes related to the object and context. Then, in section III-B, we derive a reformulated ELBO as our objective function in the VAE framework to infer the object's and contextual features for prediction. Particularly, the posterior of contextual features is conditioned on the object's features, in order to take the object's information as a reference. Finally, in section III-C, we implement a contextual posterior Transformer with a carefully modified cross-attention block to optimize the above posterior model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Contextual Generative Modeling",
      "text": "We introduce three latent variables z o , z c , z b to respectively model the object's, contextual and other background features. Together with the observed image x and object's state/label y, their generating processes are illustrated by the Bayesian Network in Fig.  2(a) . As shown, the z o , z c , z b all participate in generating the image x, with each one responsible for corresponding source of variation in the whole image. Only z o , z c are related to the label y. As an intuitive example, consider the man with angry facials and gesture in Fig.  1(c ). The y denotes his angry emotion, the z o models his facial expressions, the z c models his gesture context, and the z b models the other emotion-irrelevant backgrounds such as the door, sofa, etc.\n\nMore importantly, the object's features z o is correlated to the contextual features z c via their generating processes, as shown by the unblocked path between z o and z c in Fig.  2(a) . This path is composed of y → z o (i.e., the object's state y generates its features z o ) ,z o o-o z c and z c o-o y. It includes but not limited to the following possible types:\n\n• y → z o , y → z c , missing link between z o and z c that corresponds to 'Type I\" in Fig.  2 (a). In this case, the object's state y co-generates object's features z o and contextual features z c . For example, one can express his/her angry via simultaneous changes of facial expressions and body gestures. • y → z o , y → z c and z o → z c that corresponds to \"Type II\" in Fig.  2 (a). In this case, the generation of context z c is not only influenced by object's state y, but also directly by the object z o . For example, the structure deformation context is caused not only by the malignant nature of nodule, but also by its invasion pattern. • z c → y, z c → z o , and y → z o that corresponds to \"Type III\" in Fig.  2 (a). In this case, environmental context z c influence the generation of object's state y and features z o . For example, scene objects and other people can influence the experience and expression of emotion  [39]  Regardless of the types of path between z o and z c , the label y and latent variables z o , z c , and z b obey the following properties: i) only z o , z c are related to y; ii) z o is related to z c . Mathematically speaking:\n\nThese properties composite the cornerstone for the learning of object's and contextual representations, which we will introduce in the subsequent section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Learning Method",
      "text": "Guided by the Bayesian Network in Fig.  2 (a) and the properties derived in Eq. (  1 ), we introduce our method to infer z o and z c for prediction, by reformulating the ELBO of VAE. Specifically, the ELBO with q φ (z|x) (z ∶= [z o , z c , z b ] for simplicity) as variational distribution is: where q φ (z|x) is the variational distribution. We further approximate the p ψ (y|x) as:\n\nThen, the optimization for Eq. (  2 ) involves the posterior model q φ (z|x), the recognition model p ψ (y|z), the generation model p ψ (x|z), and the prior p(z). The overall inference process and the network structure are respectively shown in Fig.  2  (b) and Fig.  3 . Posterior Model. For the posterior q φ (z|x), we adopt the mean field approximation for the following factorization:\n\nTo leverage the object-context correlation, we incorporate the object's information as a reference during inferring z c :\n\nwith\n\nTo implement it, the object's posterior network q o φ (z o |x) is parameterized by a backbone network followed by a Region-Of-Interest (RoI) pooling layer. The contextual posterior q c φ (z c |z o , x) is parameterized by the same backbone network, followed by a RoI masking layer and a contextual posterior Transformer. The detailed architecture of our Transformer will be introduced in the subsequent section. Recognition Model. The recognition item has p ψ (y|z) = p ψ (y|z o , z c ), due to the independence between y and z b in Eq.  (1) . The p ψ (y|z o , z c ) is parameterized by a early concatenation of (z o , z c ) followed by a three-layers fully connected classifier.\n\nGeneration Model. The generation model for image p ψ (x|z) is parameterized by a convolution layer followed by four-layer de-convolution layers. Objective Function. With Eq. (  3 ), (  4 ),  (5) , the final loss is:\n\nwhere n denotes samples number in our dataset. λ 1 , λ 2 , and λ 3 respectively denote ratios for recognition loss, KL divergence loss, and reconstruction loss.\n\nTraining & Test. We optimize over q φ (z|x), p ψ (x|z) and p ψ (y|z o , z c ) by minimizing Eq. (  6",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Contextual Posterior Transformer",
      "text": "Overview. Fig.  3  shows an overview of our Transformer. As we can see, we inherit the multi-head attention mechanism as our core implementation of object-context correlation, due to its ability on modeling various aspects of relationships. To adapt the Transformer well into our visual recognition task, we make the following modifications to the original designs:\n\n• For the implementation of attention function in the crossattention block, we replace the dot-product transformation attention (that is originally designed for source-target language translation) with a Hadamard product spatialwise attention, to screen out those features that are highly correlated to the object. • We remove the Encoder-Decoder structure that is specifically designed for the sequence-to-sequence translation but is not required in our task. • we reduce the number of stacked attention layers and remove all FFNs. These can improve computational efficiency without loss of accuracy. Multi-Head Attention Function. Attention function is the cornerstone in Transformer. Given the input query\n\nrespectively denotes token length for query, key, and value, C denotes channel dimension), the attention function is defined as a dot-product between affinity matrix A qk = QK T and V:\n\nwhere τ is a temperature parameter controlling the softmax distribution, col denotes softmax is performed column wise.\n\nIn our scenario, the contextual features can present multiple patterns/locations/quantities. So, we also extend the attention function in Eq.(  7 ) into multi-head attention  [21]  to enhance its ability on modeling various aspects of contextual correlations. Specifically, the multi-head function is defined as:\n\nwhere n h denotes head numbers;\n\nfor query, key, and value;\n\n. We share weight between W Q i and W K i , as it has been suggested  [37]  to embed the (query,key) into the same space and help accurate correlation computing. Self-Attention Block. The self-attention block aims to mutually enhance feature maps representations. It takes the masked RoI feature maps F mask ⊆ R H×W ×C (H,W denote spatial dimensions, and C denotes channel dimension) as input. We reshape F mask into R C×N (N = H × W ) by flattening the spatial dimensions and feed it in into the multi-head attention function. The output of the attention is added to the input feature maps as a residual item:\n\nwhere Norm. denotes a normalization layer and F self denotes output of self-attention block.\n\nCross-Attention Block. As our key implementation of the object-context correlation, the cross-attention block leverages the object's information as guidance in learning the contextual representations. Specifically, it takes both object's latent z o and the output of self-attention block F self as input. In its multi-head attention function, the object's latent z o is used as query and F self is used as key and value, such that feature tokens in F self with high correlation to z o can be enhanced and those with low correlations can be suppressed:  The contextual latent z c is then estimated by a reparameterization on F cross .\n\nNote that originally, the attention in Eq. (  7 ) is implemented by a dot-product between affinity A and value V, such that the source sentence (V) can be transformed into target language (Q) domain according to words correspondence (A). However, as mentioned earlier, our cross-attention block aims to emphasize environmental factors with high correlation to the object and suppress those with low correlation. Hence, we implement our cross-attention in a spatial-wise attention manner:\n\nwith M qk ∶= ∑ row A qk , as the spatial-wise object-context correlation matrix. \"∑ row \" means the summation is performed row-wise and ⊙ denotes a Hardmard product.\n\nDuring the Hardmard product, the left spatial correlation item is a N k × 1 matrix, and the right value item V is a N k × C matrix. The spatial correlation is shared among different channel dimensions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we report quantitative and qualitative results of the proposed Context-LGM on two context-aware object recognition tasks, i.e. lung cancer prediction, and emotion recognition. Correspondingly, in lung cancer prediction, the contextual features include structure deformations/attachments by nodules; In emotion recognition, they include gestures, scene/objects, or other people.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Evaluation Metrics",
      "text": "Lung Cancer Prediction. The Data Science Bowl (DSB) 2017 dataset  [58]  in Kaggle's competition is used. This dataset provides a pathologically confirmed lung cancer label for each patient. There are 1397, 198, and 506 patients in the training, validation, and test set, respectively. We report official evaluation metrics including the Receiver Operating Characteristic (ROC) curve, the Area Under ROC curve (AUC), and the Log Loss (also known as the cross-entropy loss). The various types of contextual features (i.e., structure distortions, attachments) and its relation to the nodule is illustrated in Fig.  8(b-g ). Emotion Recognition. The Context-Aware Emotion Recognition (CAER-S) dataset  [43]  is used. This dataset contains 48,971 images for training and 20,954 images for validation. Each image is assigned to one of the seven emotion categories (neutral, happy, sad, surprise, fear, disgust, and angry) based on people's facial expressions and context. We report its official evaluation metric, i.e. Top-1 Accuracy (Acc). The context specifically denotes gestures, scene/objects, and other people, as marked by the dash blue boxes in Fig.  5 (i-y ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "For both tasks, we implement SGD for optimization, with momentum set to 0.9 and weight decay to 0.0001. Lung Cancer Prediction. Given CT image x and cancer label y ⊆ {0, 1}, we implement an off-the-shell nodule detector  [42]  to detect all nodules {N 1 , N 2 , ..., N n } in the patient. For each nodule N i , an 96 × 96 × 96mm\n\n3 patch containing both nodule and surroundings is cropped and feed into Context-\n\nLGM to predict a malignancy score ŷi . Based on all nodules' malignancy probabilities {ŷ 1 , ŷ2 , ..., ŷn }, the predicted cancer probability is defined by ŷ = 1 -∏ n i=0 ŷi . Our backbone is a 3D U-Net. Data pre-processing includes pixel space re-sampling (  1×1×1mm 3 ), intensity normalization (window center HU= -600, window width HU= 1600), and lung segmentation. Data augmentations include random flipping, resizing, rotation and shifting. Alternative training on classification and detection is used to alleviate over-fitting.\n\nIn the contextual posterior Transformer, short cuts between each attention layer are used to alleviate gradients vanishing as suggested by  [59] . Channel dimension C is 128. The heads number is set to 4, the layers number is set to 2, τ is set to 30. The training takes 60 epochs, with the learning rate set to 0.01 and decreases by a factor of 0.1 at epoch 20, 35. We multiply the learning rates for modules other than the backbone by a factor of 1.25 in the first 20 epochs to promote faster convergence. Due to GPU memory constraints, the batch size is set to 12. The λ 1 , λ 2 , λ 3 in Eq. (  6 ) for recognition loss, KL divergence loss, and reconstruction loss are all set to 1.0. It takes eight hours to train on two Nvidia Tesla V100 GPUs. Emotion Recognition. We first crop the target face in each image and resize it into 96 × 96. We then mask the target face (by zero paddings) and resize its surrounding areas into 140×160. We tried various backbones: 5-layers CNN adopted in  [43] -  [45]  and ResNet-18 adopted in  [46] -  [49] . Data augmentations include random cropping (128 × 128) and flipping. Our contextual posterior Transformer takes fusion of multilevels backbone feature maps as input. Channel dimension C is 256, heads number is set to 2, layer number is set to 1, and τ is set to 30. The training takes 85 epochs, with the learning rate set to 0.01 and decrease by a factor of 0.1 at epoch 55.\n\nThe batch size is set to 128. The λ 1 , λ 2 , λ 3 in Eq. (  6 ) for recognition loss, KL divergence loss, and reconstruction loss are set to 1.0, 0.1, 1.0, respectively. It takes five hours to train on one Nvidia Tesla V100 GPU.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Results Analysis",
      "text": "Lung Cancer Prediction. We compare our method with baselines from Kaggle's competition leader board and other state-of-the-art methods. For baselines from the leader board, team 'grt123' and 'J. de Wit & D. Hammack' contained contextual information in their input image. Specifically, 'grt123' extracted nodule's and contextual features together by a 2 × 2 pooling, 'J. de Wit & D. Hammack' used multiple tasks framework to jointly learn different attributes and nodule's malignancy. Whether using contextual information was not described in the competition reports of team 'Aidence', 'qfqxfd', and 'Pieere Fillard'. 'Aidence' used a multiple tasks learning framework. 'qfqxfd', and 'Pieere Fillard' adopted boosting methods to ensemble models trained with different settings. For other methods, MV-KBC  [40]  did not consider contextual information. They designed a 2D multi-view network to jointly learn the nodule's texture, internal characteristics, and margin. Ozdemir et al.  [41]  used a 3D probabilistic model, where nodule's and contextual features were extracted together by a global average pooling. Liao et al.  [42]  was the extended version of team 'grt123' with refined optimization strategy.\n\nNumerical results are reported in Tab. I. As we can see, our Context-LGM reaches an AUC of 90.24%, outperforming the state-of-the-art baseline by 3.2%. The distribution of predicted cancer probability is shown by the ROC curve in Fig.  4 , it can be further concluded that our method achieves better performance under almost all TP/FP rate settings. These results provide a strong verification of our method's effectiveness.\n\nEmotion Recognition. For our compared baselines, Efficient-Face  [48]  and MA-Net  [49]  only considered facial features. Specifically, EfficientFace learned local and global face features by a local feature extractor and a channel-spatial modulator. MA-Net proposed to learn facial features by a multi-level attention mechanism. Other baselines were methods taking both facial and contextual features into consideration. CAER-Net  [43]  and Jaiswal et al.  [44]   designed a graph convolution network to capture semantic relationships among different small contextual regions. SIB-Net  [46]  constructed a recurrent neural network to propagate the sequential influence in a face-body-scene order. Li et al.\n\n[47] also explored regions that contribute more to the emotion, without explicitly model of the object-context relation.\n\nComparison results are shown in Tab. II. As we can see, our method reaches a Top-1 Accuracy of 85.30%/91.36% under different backbone settings, which outperforms state-of-theart baselines by 4.0%/2.9%, respectively. These results further demonstrate the effectiveness of our methods.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Ablative Study",
      "text": "In this section, we conduct ablative experiments to achieve a better understanding of the effect of each component in our Context-LGM. As shown in Tab. III, the improvement compared to the vanilla cross-entropy method mainly comes from the latent generative model (VAE) that explicitly models the object-context correlation, as well as its following contextual posterior Transformer equipped with modified attention function to select contextual factors that are highly correlated with the object. These results indicate the importance of modeling the relation between contextual features and the object.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Comparison With Other Methods In Learning Z C",
      "text": "To further validate the superiority of incorporating objectcontext relation, we compare with other non object-context relation methods by replacing our Transformer with them when inferring z c . These methods, as categorized based on their applications, include context blocks in neural architectures designs  [7] ,  [9] ,  [11] -  [17] , image classification  [6] ,  [7] ,  [10] -  [17] , detection  [57] , segmentation  [8] ,  [51] ,  [52] ,  [54] -  [56] , scene paring  [50] ,  [53]  and action recognition  [15] . Their corresponding context modeling methods contain stacking of different sizes feature maps  [6] ,  [7] ,  [50] , atrous convolution  [8] , spatial-wise attention  [9] ,  [12] -  [14] , channel-wise attention  [11] ,  [16] ,  [17] ,  [51] , spatial-channel wise attention  [10] ,  [53] ,  [54] , graph based spatial attention  [15] . Some of these works combined two of different types context modeling methods, such as  [52]  combined features pyramid with spatialwise attention,  [55] -  [57]  combined atrous convolution with channel-wise attention. All these methods only exploit object's label to supervise the learning of contextual feature; that is, they overlook the importance of object-context relation.\n\nAs observed in Tab. IV, our contextual posterior Transformer outperforms all the other methods by a noticeable margin (0.8%-1.2% in lung cancer prediction and 0.6%-6.0% in emotion recognition). These results show that, due to the diversity of contextual features, the existing methods may fail to capture them comprehensively; in contrast, the exploitation of an extra inductive bias, i.e. object-context relation, enables our method to capture them well.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Hyper-Parameters Analysis",
      "text": "In this section, we examine the influence of hyperparameters on our contextual posterior Transformer. Layers Number. The results are shown in Tab. V. It can be observed that our Transformer reaches satisfactory results when the layers number is 2 for lung cancer prediction or 1 for emotion recognition. As a matter of fact, since using more layers could significantly increase computation costs, similar layers number setting are also reported in some other visual Transformers, such as  [37]  use 1 layer,  [60] ,  [61]   ships. We conduct experiments on different heads number and report the results in Tab. VI. We can see that our model reaches the best performance when n h = 4 for lung cancer prediction and n h = 2 for emotion recognition. This may be because contextual features in lung cancer are more complicated than those in emotion. As an intuitive example, in emotion recognition, there may be at most two types of contextual features (i.e. gestures, people) presenting together. However, in lung cancer, the malignant nodule can cause multiple pleural indentations (Fig.  7-a ) or vascular convergence (Fig.  5 -f) at the same time.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "G. Visualization",
      "text": "In Context-LGM, we use object-context correlation as extra information to locate contextual factors. Such a correlation is parameterized by the cross-attention block in our Transformer. This block computes a correlation matrix between object z c and surrounding regions in F mask . Regions with high correlation values are contextual factors, while those with low correlation values are backgrounds. Learned Correlation Matrix. We visualize the correlation metrics in Fig.  5 . For each example, the left image is the input x to our Context-LGM, and the right image shows the computed correlation matrix. In the correlation matrix, a large value indicates a high correlation with the object. The object regions are with zero response. This is because we mask them in F mask to avoid interference from object-object correlations.\n\nWe can observe that regions with contextual features show high correlation response, and those without context show low response. For example, in lung nodule related context, indentation to bottom right pleura in Fig.  5 (a), and indentation to the left upper pleura in Fig.  5(b, c ) are captured by high response correlation. In Fig.  5(d, e ), the bronchial interruption context in the nodule's bottom areas is emphasized. In Fig.  5 (f), the vascular convergence context is emphasized, too. Also, the pleural and vascular attachment context are well captured by object-context correlation in Fig.  5(g-i ). In facial emotion related context, various of human gestures (Fig.  5j, k, l, n, o, p, q ) and posture (Fig.  5-m ) are well captured by our object-context correlation. In Fig.  5(r-u ), our method learns correlation between the target face and other people. In Fig.  5(v, w ), it can be seen that our object-context correlation is also able to capture the clothing context (business suit with serious facials, messy collar with sad facials). Also, context from environmental objects such as phones and gifts are well capture in Fig.  5(x, y ).\n\nFrom these observations, we could conclude that our objectcontext correlation mainly looks at contextual regions in an object's surroundings, thus well promote the learning of contextual representations. Comparison with Other Methods. We also compare with context captured by other methods and show the results in Fig.  6 . As we can see, only our Context-LGM correctly capture the pleural indentation and bronchial interruption context in Fig.  6  Such an observation indicates complementary cooperation may be learned among different attention heads. This mechanism could be especially beneficial when multiple contextual regions exist.\n\nV. CONCLUSION In this paper, we propose Context-LGM, a novel latent generative model for context-aware object recognition. We firstly incorporate the object-context relation via their generating processes in the latent generative model. Then, we design a reformulated VAE framework, within which a carefully modified Transformer is equipped to take the object's information as a reference and infer contextual features. Our method can achieve state-of-the-art results on lung nodule prediction and emotion recognition. Moreover, the learned context features are highly explainable.\n\nWe believe that the object-context relation, as widely exists in other vision tasks such as scene understanding, action recognition, and human-computer interaction, can serve a broader family of applications. We leave this exploration in the future work.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Appendix A Pathological Background",
      "text": "Though context in the natural image is easy to understand, it takes some background knowledge to understand it in the medical image. To give an intuition, we provide radiological examples about the common context in lung cancer prediction.\n\nIn Fig.  8 , (a-c) show context correlated with malignant nodules, and (d-e) show context correlated with benign nodules.\n\nSpecifically, pleural indentation  [1]  in Fig.  8 (a) represents pulling the visceral pleura towards the nodule. It suggests a possible pleural invasion by peripheral lung tumor. Invasion and contractile changes of the tumor cause dead space, then the linear strand between tumor and pleural is formed by compensatory expansion of lung tissues to fill the dead space; vascular convergence  [2]  in Fig.  8(b ) is described as vessels converging towards a nodule. This is because angiogenesis is essential for tumor growth and metastasis. The vascular endothelial growth factor is then synthesized continuously and excessively in the tumor, promoting the proliferation of vascular endothelial cells during vessel formation; bronchial interruption  [62]  in Fig.  8(c ) means the bronchus is obstructed abruptly by the tumor or it penetrates into the tumor with tapered narrowing and interruption. This context is resulted by the growth pattern of malignant nodules, i.e. the hilic growth and the lepidic growth. In hilic growth, the tumor cells proliferate and pile up continuously, forming a solid mass and obstruct adjacent bronchus. In lepidic growth, the tumor cells line the alveolar wall and directly spread from one alveolus to another through the pore of Köhn. Thus the bronchus remains intact and penetrates into the mass. Pleural attachment in Fig.  8(d) , and vascular attachment in Fig.  8 (e) respectively represent that the nodule is adjacent to a pleura or blood vessel. Their relation to benignity are mainly confirmed by clinical statistics  [63] .",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: -a) [1], vascular",
      "page": 1
    },
    {
      "caption": "Figure 1: -b) [2]) through invasion and releasing",
      "page": 1
    },
    {
      "caption": "Figure 1: -c) [3]), other people (Fig. 1-d) [4], and visual",
      "page": 1
    },
    {
      "caption": "Figure 5: -v,w,s,y) [5].",
      "page": 1
    },
    {
      "caption": "Figure 5: -a,b) simultaneously.",
      "page": 1
    },
    {
      "caption": "Figure 1: Examples of objects and their correlated contextual features. (a) a",
      "page": 2
    },
    {
      "caption": "Figure 2: (a). As shown, the zo, zc, zb all participate",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). In this case, the",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). In this case, the generation of context zc",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). In this case, environmental context zc",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) and the",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) Contextual generative modeling. The o-o mark denotes →, ←, or the missing link. We argue that an observed image is generated from three",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) and Fig. 3.",
      "page": 4
    },
    {
      "caption": "Figure 3: shows an overview of our Transformer. As",
      "page": 4
    },
    {
      "caption": "Figure 3: Left: network structure of the proposed Context-LGM. Given the input image x, the Encoder (Enc) ﬁrstly extracts latent representations for the object",
      "page": 5
    },
    {
      "caption": "Figure 4: ROC curves of different methods in lung cancer prediction task. Note",
      "page": 6
    },
    {
      "caption": "Figure 5: Visualization of the learned object-context correlation matrices. For each example, the left image shows the input x to our Context-LGM, and the",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of learned context in different methods.",
      "page": 10
    },
    {
      "caption": "Figure 7: -a) or vascular convergence (Fig. 5-f) at",
      "page": 10
    },
    {
      "caption": "Figure 5: For each example, the left image is the",
      "page": 10
    },
    {
      "caption": "Figure 5: (a), and indentation",
      "page": 10
    },
    {
      "caption": "Figure 5: (b,c) are captured by high",
      "page": 10
    },
    {
      "caption": "Figure 5: (d,e), the bronchial interruption",
      "page": 10
    },
    {
      "caption": "Figure 5: (f), the vascular convergence context is emphasized, too.",
      "page": 10
    },
    {
      "caption": "Figure 5: (g-i). In facial",
      "page": 10
    },
    {
      "caption": "Figure 5: -m) are well captured by our",
      "page": 10
    },
    {
      "caption": "Figure 5: (r-u), our method learns",
      "page": 10
    },
    {
      "caption": "Figure 5: (v,w), it can be seen that our object-context correlation is",
      "page": 10
    },
    {
      "caption": "Figure 6: As we can see, only our Context-LGM correctly capture",
      "page": 10
    },
    {
      "caption": "Figure 6: (a) and the bronchial interruption context in Fig. 6(b).",
      "page": 10
    },
    {
      "caption": "Figure 6: (c), only our method comprehensively captures both",
      "page": 10
    },
    {
      "caption": "Figure 7: Comparison of correlation matrices in different attention heads.",
      "page": 11
    },
    {
      "caption": "Figure 7: It can be observed that different heads seem to focus on",
      "page": 11
    },
    {
      "caption": "Figure 7: (a) shows three",
      "page": 11
    },
    {
      "caption": "Figure 7: (b), the nodule shows both pleural",
      "page": 11
    },
    {
      "caption": "Figure 8: , (a-c) show context correlated with malignant nod-",
      "page": 11
    },
    {
      "caption": "Figure 8: (a) represents",
      "page": 11
    },
    {
      "caption": "Figure 8: (b) is described as vessels converging",
      "page": 11
    },
    {
      "caption": "Figure 8: Radiological examples for context in lung cancer prediction.",
      "page": 12
    },
    {
      "caption": "Figure 8: (c) means the bronchus is obstructed abruptly by the tumor",
      "page": 12
    },
    {
      "caption": "Figure 8: (e) respectively represent",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vanilla\nobject\ncontext": "✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "ours\nVAE\nTransformer": "✓\n✓\n✓",
          "DSB2017\nCAER-S\nAUC(%) ↑\nAcc(%) ↑": "88.06 ± 0.67\n71.89 ± 0.26\n88.72 ± 0.15\n76.96 ± 0.17\n88.88 ± 0.02\n80.27 ± 0.14\n90.24 ± 0.16\n85.30 ± 0.07"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Easier understanding of pleural indentation on computed tomography",
      "authors": [
        "N Seki",
        "Y Fujita",
        "R Shibakuki",
        "T Seto",
        "K Uematsu",
        "K Eguchi"
      ],
      "year": "2007",
      "venue": "Internal Medicine"
    },
    {
      "citation_id": "2",
      "title": "Multi-slice computed tomography characteristics of solitary pulmonary ground-glass nodules: Differences between malignant and benign",
      "authors": [
        "H Hu",
        "Q Wang",
        "H Tang",
        "L Xiong",
        "Q Lin"
      ],
      "year": "2016",
      "venue": "Thoracic Cancer"
    },
    {
      "citation_id": "3",
      "title": "Body expressions influence recognition of emotions in the face and voice",
      "authors": [
        "J Stock",
        "R Righart",
        "B De Gelder"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "4",
      "title": "Faces in context: A review and systematization of contextual influences on affective face processing",
      "authors": [
        "M Wieser",
        "T Brosch"
      ],
      "year": "2012",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "5",
      "title": "Context is routinely encoded during emotion perception",
      "authors": [
        "L Barrett",
        "E Kensinger"
      ],
      "year": "2010",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "6",
      "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2014",
      "venue": "Computer Vision -ECCV 2014"
    },
    {
      "citation_id": "7",
      "title": "Multi-crop convolutional neural networks for lung nodule malignancy suspiciousness classification",
      "authors": [
        "W Shen",
        "M Zhou",
        "F Yang",
        "D Yu",
        "D Dong",
        "C Yang",
        "Y Zang",
        "J Tian"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
      "authors": [
        "L.-C Chen",
        "Y Zhu",
        "G Papandreou",
        "F Schroff",
        "H Adam"
      ],
      "year": "2018",
      "venue": "Computer Vision -ECCV 2018"
    },
    {
      "citation_id": "9",
      "title": "Non-local neural networks",
      "authors": [
        "X Wang",
        "R Girshick",
        "A Gupta",
        "K He"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Aˆ2-nets: Double attention networks",
      "authors": [
        "Y Chen",
        "Y Kalantidis",
        "J Li",
        "S Yan",
        "J Feng"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Stand-alone self-attention in vision models",
      "authors": [
        "P Ramachandran",
        "N Parmar",
        "A Vaswani",
        "I Bello",
        "A Levskaya",
        "J Shlens"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Local relation networks for image recognition",
      "authors": [
        "H Hu",
        "Z Zhang",
        "Z Xie",
        "S Lin"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "14",
      "title": "Attention augmented convolutional networks",
      "authors": [
        "I Bello",
        "B Zoph",
        "Q Le",
        "A Vaswani",
        "J Shlens"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Graph-based global reasoning networks",
      "authors": [
        "Y Chen",
        "M Rohrbach",
        "Z Yan",
        "Y Shuicheng",
        "J Feng",
        "Y Kalantidis"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Eca-net: Efficient channel attention for deep convolutional neural networks",
      "authors": [
        "Q Wang",
        "B Wu",
        "P Zhu",
        "P Li",
        "W Zuo",
        "Q Hu"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Linear context transform block",
      "authors": [
        "D Ruan",
        "J Wen",
        "N Zheng",
        "M Zheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Gaussian context transformer",
      "authors": [
        "D Ruan",
        "D Wang",
        "Y Zheng",
        "N Zheng",
        "M Zheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Scene consistency in object and background perception",
      "authors": [
        "J Davenport",
        "M Potter"
      ],
      "year": "2004",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "20",
      "title": "Rapid perceptual integration of facial expression and emotional body language",
      "authors": [
        "H Meeren",
        "C Van Heijnsbergen",
        "B De Gelder"
      ],
      "year": "2005",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Ca-net: Leveraging contextual features for lung cancer prediction",
      "authors": [
        "M Liu",
        "F Zhang",
        "X Sun",
        "Y Yu",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "Medical Image Computing and Computer Assisted Intervention -MICCAI 2021"
    },
    {
      "citation_id": "23",
      "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
      "authors": [
        "I Higgins",
        "L Matthey",
        "A Pal",
        "C Burgess",
        "X Glorot",
        "M Botvinick",
        "S Mohamed",
        "A Lerchner"
      ],
      "year": "2016",
      "venue": "beta-vae: Learning basic visual concepts with a constrained variational framework"
    },
    {
      "citation_id": "24",
      "title": "Isolating sources of disentanglement in variational autoencoders",
      "authors": [
        "T Chen",
        "X Li",
        "R Grosse",
        "D Duvenaud"
      ],
      "year": "2018",
      "venue": "Isolating sources of disentanglement in variational autoencoders"
    },
    {
      "citation_id": "25",
      "title": "Causal hidden markov model for time series disease forecasting",
      "authors": [
        "J Li",
        "B Wu",
        "X Sun",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Latent causal invariant model",
      "authors": [
        "X Sun",
        "B Wu",
        "C Liu",
        "X Zheng",
        "W Chen",
        "T Qin",
        "T.-Y Liu"
      ],
      "year": "2020",
      "venue": "Latent causal invariant model",
      "arxiv": "arXiv:2011.02203"
    },
    {
      "citation_id": "27",
      "title": "Guided variational autoencoder for disentanglement learning",
      "authors": [
        "Z Ding",
        "Y Xu",
        "W Xu",
        "G Parmar",
        "Y Yang",
        "M Welling",
        "Z Tu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Diverse image captioning with context-object split latent spaces",
      "authors": [
        "S Mahajan",
        "S Roth"
      ],
      "year": "2020",
      "venue": "Diverse image captioning with context-object split latent spaces",
      "arxiv": "arXiv:2011.00966"
    },
    {
      "citation_id": "29",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "30",
      "title": "Training data-efficient image transformers amp distillation through attention",
      "authors": [
        "H Touvron",
        "M Cord",
        "M Douze",
        "F Massa",
        "A Sablayrolles",
        "H Jegou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "Cross-modal self-attention network for referring image segmentation",
      "authors": [
        "L Ye",
        "M Rochan",
        "Z Liu",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "Panoptic segmentation",
      "authors": [
        "A Kirillov",
        "K He",
        "R Girshick",
        "C Rother",
        "P Dollar"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "End-to-end video instance segmentation with transformers",
      "authors": [
        "Y Wang",
        "Z Xu",
        "X Wang",
        "C Shen",
        "B Cheng",
        "H Shen",
        "H Xia"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "35",
      "title": "Video transformer network",
      "authors": [
        "D Neimark",
        "O Bar",
        "M Zohar",
        "D Asselmann"
      ],
      "year": "2021",
      "venue": "Video transformer network",
      "arxiv": "arXiv:2103.10455"
    },
    {
      "citation_id": "36",
      "title": "Context modeling in 3d human pose estimation: A unified perspective",
      "authors": [
        "X Ma",
        "J Su",
        "C Wang",
        "H Ci",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "37",
      "title": "Transformer meets tracker: Exploiting temporal context for robust visual tracking",
      "authors": [
        "N Wang",
        "W Zhou",
        "J Wang",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Skeleton-based action recognition via spatial and temporal transformer networks",
      "authors": [
        "C Plizzari",
        "M Cannici",
        "M Matteucci"
      ],
      "year": "2021",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "39",
      "title": "Context is everything (in emotion research)",
      "authors": [
        "K Greenaway",
        "E Kalokerinos",
        "L Williams"
      ],
      "year": "2018",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "40",
      "title": "Knowledge-based collaborative deep learning for benign-malignant lung nodule classification on chest ct",
      "authors": [
        "Y Xie",
        "Y Xia",
        "J Zhang",
        "Y Song",
        "D Feng",
        "M Fulham",
        "W Cai"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "41",
      "title": "A 3d probabilistic deep learning system for detection and diagnosis of lung cancer using lowdose ct scans",
      "authors": [
        "O Ozdemir",
        "R Russell",
        "A Berlin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "42",
      "title": "Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky noisy-or network",
      "authors": [
        "F Liao",
        "M Liang",
        "Z Li",
        "X Hu",
        "S Song"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "43",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "44",
      "title": "Attention-guided context-aware emotional state recognition",
      "authors": [
        "S Jaiswal",
        "S Misra",
        "G Nandi"
      ],
      "year": "2020",
      "venue": "2020 IEEE 7th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)"
    },
    {
      "citation_id": "45",
      "title": "A graph convolutional network for emotion recognition in context",
      "authors": [
        "H Zeng",
        "G Li",
        "T Tong",
        "Q Gao"
      ],
      "year": "2020",
      "venue": "2020 Cross Strait Radio Science Wireless Technology Conference (CSRSWTC)"
    },
    {
      "citation_id": "46",
      "title": "Sequential interactive biased network for context-aware emotion recognition",
      "authors": [
        "X Li",
        "X Peng",
        "C Ding"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "47",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "W Li",
        "X Dong",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Robust lightweight facial expression recognition network with label distribution training",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "49",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Z Zhao",
        "Q Liu",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "50",
      "title": "Pyramid scene parsing network",
      "authors": [
        "H Zhao",
        "J Shi",
        "X Qi",
        "X Wang",
        "J Jia"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "51",
      "title": "Context encoding for semantic segmentation",
      "authors": [
        "H Zhang",
        "K Dana",
        "J Shi",
        "Z Zhang",
        "X Wang",
        "A Tyagi"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Asymmetric nonlocal neural networks for semantic segmentation",
      "authors": [
        "Z Zhu",
        "M Xu",
        "S Bai",
        "T Huang",
        "X Bai"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "53",
      "title": "Dual attention network for scene segmentation",
      "authors": [
        "J Fu",
        "J Liu",
        "H Tian",
        "Y Li",
        "Y Bao",
        "Z Fang",
        "H Lu"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "54",
      "title": "Learning to predict contextadaptive convolution for semantic segmentation",
      "authors": [
        "J Liu",
        "J He",
        "Y Qiao",
        "J Ren",
        "H Li"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "55",
      "title": "Attention deeplabv3+: Multi-level context attention mechanism for skin lesion segmentation",
      "authors": [
        "R Azad",
        "M Asadi-Aghbolaghi",
        "M Fathy",
        "S Escalera"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020 Workshops"
    },
    {
      "citation_id": "56",
      "title": "Cgnet: A light-weight context guided network for semantic segmentation",
      "authors": [
        "T Wu",
        "S Tang",
        "R Zhang",
        "J Cao",
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "57",
      "title": "Rich context aggregation with reflection prior for glass surface detection",
      "authors": [
        "J Lin",
        "Z He",
        "R Lau"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "58",
      "title": "Data science bowl 2017",
      "year": "2017",
      "venue": "Data science bowl 2017"
    },
    {
      "citation_id": "59",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "60",
      "title": "Video action transformer network",
      "authors": [
        "R Girdhar",
        "J Carreira",
        "C Doersch",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "61",
      "title": "Learning texture transformer network for image super-resolution",
      "authors": [
        "F Yang",
        "H Yang",
        "J Fu",
        "H Lu",
        "B Guo"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "62",
      "title": "The relationship between solitary pulmonary nodules and bronchi: multislice ct-pathological correlation",
      "authors": [
        "J Qiang",
        "K Zhou",
        "G Lu",
        "Q Wang",
        "X Ye",
        "S Xu",
        "L Tan"
      ],
      "year": "2004",
      "venue": "Clinical Radiology"
    },
    {
      "citation_id": "63",
      "title": "Smooth or attached solid indeterminate nodules detected at baseline ct screening in the nelson study: Cancer risk during 1 year of follow-up",
      "authors": [
        "D Xu",
        "H Van Der Zaag-Loonen",
        "M Oudkerk",
        "Y Wang",
        "R Vliegenthart",
        "E Scholten",
        "J Verschakelen",
        "M Prokop",
        "H De Koning",
        "R Van Klaveren"
      ],
      "year": "2009",
      "venue": "Radiology"
    }
  ]
}