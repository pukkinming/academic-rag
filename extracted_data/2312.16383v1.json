{
  "paper_id": "2312.16383v1",
  "title": "Frame-Level Emotional State Alignment Method For Speech Emotion Recognition",
  "published": "2023-12-27T03:07:52Z",
  "authors": [
    "Qifei Li",
    "Yingming Gao",
    "Cong Wang",
    "Yayue Deng",
    "Jinlong Xue",
    "Yichen Han",
    "Ya Li"
  ],
  "keywords": [
    "Frame-level emotional state alignment",
    "speech emotion recognition",
    "HuBERT"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) systems aim to recognize human emotional state during human-computer interaction. Most existing SER systems are trained based on utterancelevel labels. However, not all frames in an audio have affective states consistent with utterance-level label, which makes it difficult for the model to distinguish the true emotion of the audio and perform poorly. To address this problem, we propose a frame-level emotional state alignment method for SER. First, we fine-tune HuBERT model to obtain a SER system with task-adaptive pretraining (TAPT) method, and extract embeddings from its transformer layers to form framelevel pseudo-emotion labels with clustering. Then, the pseudo labels are used to pretrain HuBERT. Hence, the each frame output of HuBERT has corresponding emotional information. Finally, we fine-tune the above pretrained HuBERT for SER by adding an attention layer on the top of it, which can focus only on those frames that are emotionally more consistent with utterance-level label. The experimental results performed on IEMOCAP indicate that our proposed method performs better than state-of-the-art (SOTA) methods. The codes are available at github repository 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In order to improve the experience of human-computer interaction, speech emotion recognition has become one of the research hotspots in recent years. Technologies in this field have advanced considerably over the past decade.\n\nThe conventional methods for SER focus on using neural networks to mine emotional information from hand-crafted or spectral features  [1, 2, 3, 4] . Due to limited labeled data, these methods have only shown slight performance improvements. With the success of natural language processing pretraining models  [5] , several self-supervised audio pretraining models have emerged, such as wav2vec  [6] , wav2vec2.0  [7] , HuBERT  [8] , and WavLm  [9] . Those models are obtained by self-supervised pretraining using large amounts of unlabelled data, and what they learn can be transferred to improve the performance of downstream tasks, such as automatic speech recognition  [10] , SER  [11] , speaker recognition  [12] , etc.\n\nThere are mainly three methods for implementing SER using audio pretrained models. The first is to extract the embeddings of the pretrained model as the input of the downstream task model  [11, 13] . The second kind is to fine-tune the model for SER  [14, 15, 16] . The third category involves redesigning the pretext task, pretraining the model based on this pretext task, and then fine-tuning it to implement SER  [15] . Based on the second and third methods, Chen et al.  [15]  utilize wav2vec2.0 to realize a pseudo label task adaptive pretraining approach (P-TAPT) for sentiment analysis, which can align frame-level pseudo-emotion labels with frames to alleviate the inconsistency between emotional states of some frames in the audio and its utterance-level label, and performs better than many SER systems that only utilize utterance-level labels.\n\nHowever, P-TAPT has two aspects that can be improved. The first point is that it aims to achieve a frame-level emotional alignment method similar to that of HuBERT. However, it only introduces frame-level pseudo-emotion labels and fine-tunes wav2vec2.0 directly to realize frame-level emotion state alignment instead of pretraining HuBERT. The HuBERT has been proven to be more suitable for this offline discrete frame-level self-supervised task  [8, 17] . The second one is that the authors fine-tuned the aligned model directly with average pooling to aggregate frame-level representations to utterance-level representations for final utterance-level SER system. This method may not fully exploit the aligned framelevel emotional information.\n\nInspired by  [8, 15, 17] , we propose a frame-level emotion alignment (FLEA) method for SER, which is an extension and improvement of P-TAPT. In the first step, we fine-tune the HuBERT model to implement a SER system and extract the embeddings of its i-th transformer layer for clustering to achieve frame-level pseudo-emotion labels. Then, we continue to pretrain the HuBERT model using pseudo-labels, referred to as CPT-HuBERT that allows to automatically align frames with pseudo-emotion labels. Finally, we add an attention layer on the top of CPT-HuBERT and fine-tune it for SER. The role of attention is to align frame-level representations with utterance-level labels. In addition, we explore the effect of using different transformers layers of embeddings to cluster and different number of clusters on SER performance.\n\nWe perform experiments on IEMOCAP  [18]  to validate the effectiveness of FLEA. The unweighted accuracy (UA) and weighted accuracy (WA) of FLEA are 75.7% and 74.7%, outperforming SOTA methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we first review HuBERT model, which is the key backbone of our proposed method. Then, we introduce our proposed method in detail, and the system framework is shown in Fig.  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Review Of Hubert",
      "text": "HuBERT is a large pretrained model obtained through selfsupervised learning, which is used to learn general audio representations from unlabeled raw audio signals for various downstream speech tasks. First, HuBERT leverages the offline clustering algorithm to generate pesudo labels for mask language model (MLM) pretraining. Second, the raw audio signals need to be encoded into meaningful continuous latent representations by a feature extractor which consist of a stack of CNN layers. Finally, the model utilizes the transformer layers to learn the structure of spoken inputs by predicting the cluster for masked audio segments. The training of HuBERT includes two phases. The pseudo labels used in the first phase of pretraining are generated from the mel frequency cepstrum coefficient (MFCC), and the pseudo labels used in the second phase of pretraining are generated from the embeddings of the model saved in the first phase. The MLM pretraining means that the representations of the masked frames and unmasked frames require computing cross-entropy losses with pseudo-labels respectively. The two losses denote as L m and L u , and the final predictive loss L of the model can be calculated as follows:\n\nThe predictive loss forces the model to learn good high-level representations of unmasked frames to help infer the pseudo labels of masked frames correctly  [5, 8] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Tapt And Pretraining Hubert",
      "text": "TAPT and cluster. Since all of the SER datasets only have utterance-level labels, in order to achieve frame-level affective alignment, we need to introduce frame-level pseudoemotion labels. To this end, we generate pseudo labels by following this methods  [15, 16]  which prove that frame-level emotion state can be inferred by training with a segmentbased classification objective. As shown in phase 1 in Fig.  1 , we first fine-tune HuBERT for SER with TAPT  [19] . TAPT is kind of fine-tune, which first continues pretraining pretrained model on the target datasets to bridge the gap between pretrained and target domains and then fine-tunes above pretrained model for SER. Next, we extract the embeddings from the i-th transformer layer in HuBERT to generate frame-level pseudo-emotion labels by k-means algorithm. Consistency in the k-means mapping from audio input to discrete targets is crucial, as it allows the model to focus on modeling the emotion sequential structure of the inputs during MLM pretraining.\n\nPretraining HuBERT. According to the theory that bad teachers (pseudo labels) make good students (learned representations)  [10, 15, 17] , we continue using MLM to pretrain the HuBERT with the frame-level pseudo-emotion labels as illustrated in Fig.  1 . During pretraining, we set the value of α in the predictive loss to be 1 as in the official HuBERT. In other words, we only apply the loss function over the masked regions. After pretraining, the embeddings of each frame in the last transformer layer of CPT-HuBERT is mapped to a discrete frame-level pseudo-emotion labels to achieve framelevel fine-grained emotion alignment embeddings.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Soft Attention",
      "text": "An advantage of the frame-level fine-grained emotion alignment embeddings is that there are clear differences between the frame embeddings of different emotions, while the embeddings of frames representing the same emotion are typically adjacent to each other, as shown in the third phase of Fig.  1 . By leveraging a simple attention, we can focus on frames that are strongly related to the utterance-level emotion label, while disregarding frames that are irrelevant to the emotion label. This approach effectively addresses the issue of interference from emotion label-unrelated frames in SER.\n\nIn this work, we use the soft attention to align the frame-level fine-grained emotion alignment embeddings with utterance-level emotion labels. The attention is implemented as follows:\n\nwhere the x i is the frame-level fine-grained emotion alignment embedding of the i-th frame and the W ∈ R 1×D is a trainable parameter to encode the attention weights of frames. The variables α i , N , Z and D are the attention weights of the i-th frame, the number of frames, the utterance-level emotional representation and the dimension of embeddings respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset. IEMOCAP  [18]  is a well-known multi-modal emotion corpus, which includes audio, visual and lexical modalities. In this work, we only use the data of audio modality. The corpus contains five recording sessions, each session has one male and one female speaker. In order to prevent leakage of speaker information and labels, whether it is pre-training HuBERT or fine-tuning the model for SER, we perform fivefold cross-validation using a leave-one-session out strategy on the corpus. This means that the data of four sessions are used as training data; the data of one speaker from the remaining session is used as the validation set and the data of other one speaker forms the testing set. We conduct our experiments with the 5531 audio utterances of four emotion categories happy (happy & excited), sad, neutral and angry. The UA and WA are used as evaluation metrics in line with previous methods. The UA is the mean of the accuracy of each category and WA is the accuracy of all samples.\n\nExperimental Details. The pretrained HuBERT 2  we used is the backbone of FLEA, which consists of 6 CNN layers and 12 transformer layers. It has an embedding dimension of 768. When we fine-tune HuBERT for SER with TAPT, we use the official k-means model 3  to generate pseduo labels to continue pretraining HuBERT on the IEMOCAP. In the first and third phases as shown in Fig.  1 , whether TAPT or fine-tuning the model for SER, the batch size is 64, the learn rate is 1e-4, the loss function is cross entropy loss and the optimizer is AdamW. The batch size is 64 and the epochs are all 40. In order to explore the impact of clustering on the final SER performance, we follow the official HuBERT  [8]  extracted the embeddings of the 6-th, 9-th, and extra 11-th transformer layers of HuBERT and performed clustering with 50, 100, and 150 clusters for each layer of embeddings. The reason we chose these numbers of clusters is the small sample size of the dataset.\n\nDuring pretraining HuBERT, the mini-batches are generated by bucket sampler. The learning rate is 5e-4, the training steps are 20,000, and the warmed up steps are 4,000. More details are shown in our github project. The P-TAPT is used as baseline  [15] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "A series of ablation experiments are conducted to evaluate the effect of pretraining HuBERT for frame-level emotion state alignment, attention and the number of clusters produced by different layers of embeddings on the performance of SER system. For comparison with the baseline, we list the results of FLEA with attention pooling, and the results of the same fine-tuning method as baseline, which fine-tunes HuBERT for SER with using average pooling to aggregate embeddings, as shown in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Impact Of Embeddings And Clusters",
      "text": "From Table  1 , we can observe that different layers of embeddings and number of clusters have different effects on SER performance. No matter how many classes are clustered, the effect of pretraining HuBERT for SER with the frame-level pseudo-emotion labels clustered by the embedding of the 9-th layer is better than that of the 6-th and 11-th layers. This experimental results demonstrate that the embedding of the 9-th layer is more suitable for generating pseudo labels to pretrain HuBERT. The finding is consistent with the study  [8] .\n\nIn addition, regardless of which layer of embeddings are used to cluster, the performance of SER decreases as the number of clusters increase. This phenomenon may be related to the size of the dataset. A clustering number of 50 is appropriate on the IEMOCAP dataset for pretraining HuBERT. However, we believe this is due to the limitation of dataset size. With more sentiment data, this cluster number may be larger and the model would be more robust, as in the case of the official HuBERT clustering number of 500.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Role Of Pretraining Hubert And Attention Pooling",
      "text": "As shown in Table  1 , in the 9-th layer, fine-tuning HuBERT with average pooling for SER performs better than the baseline regardless of the numbers of clusters. This indicates that using the MLM method to pretrain HuBERT to realize framelevel emotion state alignment yields better performance than directly fine-tuning wav2vec2.0 for the same purpose.\n\nFurthermore, at a cluster number of 50, the UA and WA of FLEA are improved by 0.8% and 1.6%, respectively, compared to those of fine-tuned CPT-HuBERT with average pooling for SER, and by 1.9% over the UA of the baseline. This indicates that introduction of attention to align frame-level embeddings with utterance-labels is effective. Compared to average pooling, attention pooling can pay attention to the those frames strongly related to utterance-label, which makes better use of aligned frame-level emotion information. Although the performance of average pooling gradually approaches or even exceeds that of attention pooling as the number of clusters increases, the performance of the final SER also decreases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Comparison With Previous Methods",
      "text": "The effectiveness of our proposed method can be spotlighted via comparing with current key results performed on the IEMOCAP corpus (Table  2 ). It shows that the best UA (75.7%) and WA (74.7%) are achieved by our proposed Multi-modal 2023 MTG  [21]  75.0 74.5 2023 MSMSER  [22]  76.4 75.2 method. Moreover, our system outperforms the baseline  4 even though it leaks speaker information while fine-tuning wav2vec for clustering to generate frame-level pseudo labels. After our testing, FLEA performs better if we use the baseline clustering approach, which leaks speaker information.\n\nIn addition, as shown in Table  2 , our method performs well beyond the latest SER methods of the day, such as ShiftCNN, SUPERB, SMW-CAT, etc. Meanwhile, the performance of FLEA is close to some multi-modal methods, which are based on the modalities of audio and lexical.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we propose a novel method called FLEA for SER, which achieves SOTA performance on the IEMOCAP corpus. We show that frame-level emotion state alignment can be achieved by pre-training HuBERT with MLM method using frame-level pseudo-emotion labels. On the above aligned model, performing attention pooling to aggregate frame-level embeddings to utterance-level embeddings can get better performance for SER. Furthermore, we find that the model pre-trained for SER using the pseudo label generated by the embedding clustering of the 9-th transformer layer of HuBERT has the best performance and the most robustness. A cluster number of 50 is best suitable on IEMOCAP, but it may not be robust for other corpora due to the size of dataset.\n\nIn future work, we will explore the relationship between dataset size and number of clusters.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.",
      "page": 2
    },
    {
      "caption": "Figure 1: During pretraining, we set the value of",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "ABSTRACT"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "Speech emotion recognition (SER) systems aim to recognize"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "human emotional state during human-computer interaction."
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "Most existing SER systems are trained based on utterance-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "level labels. However, not all frames in an audio have affec-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "tive states consistent with utterance-level label, which makes"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "it difﬁcult\nfor\nthe model\nto distinguish the true emotion of"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "the audio and perform poorly. To address this problem, we"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "propose a frame-level emotional state alignment method for"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "SER. First, we ﬁne-tune HuBERT model to obtain a SER sys-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "tem with task-adaptive pretraining (TAPT) method, and ex-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "tract embeddings from its transformer layers to form frame-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "level pseudo-emotion labels with clustering. Then, the pseudo"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "labels are used to pretrain HuBERT. Hence,\nthe each frame"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "output of HuBERT has corresponding emotional information."
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "Finally, we ﬁne-tune the above pretrained HuBERT for SER"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "by adding an attention layer on the top of\nit, which can fo-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "cus only on those frames that are emotionally more consis-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "tent with utterance-level label. The experimental results per-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "formed on IEMOCAP indicate that our proposed method per-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "forms better than state-of-the-art (SOTA) methods. The codes"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "are available at github repository1."
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "Index Terms— Frame-level emotional state alignment,"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "speech emotion recognition, HuBERT"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "1.\nINTRODUCTION"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "In order\nto improve the experience of human-computer\nin-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "teraction, speech emotion recognition has become one of the"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "research hotspots in recent years. Technologies in this ﬁeld"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "have advanced considerably over the past decade."
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "The conventional methods for SER focus on using neural"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "networks to mine emotional\ninformation from hand-crafted"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "or spectral features [1, 2, 3, 4]. Due to limited labeled data,"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "these methods have only shown slight performance improve-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "ments. With the success of natural language processing pre-"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "training models [5], several self-supervised audio pretraining"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "models have emerged, such as wav2vec [6], wav2vec2.0 [7],"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "HuBERT [8], and WavLm [9]. Those models are obtained by"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "self-supervised pretraining using large amounts of unlabelled"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "data, and what\nthey learn can be transferred to improve the"
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": ""
        },
        {
          "Beijing University of Posts and Telecommunications, Beijing, China": "1https://github.com/ASolitaryMan/HFLEA.git"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "cluster and different number of clusters on SER performance."
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "We perform experiments on IEMOCAP [18]\nto validate the"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "effectiveness of FLEA. The unweighted accuracy (UA) and"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "weighted accuracy (WA) of FLEA are 75.7% and 74.7%, out-"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "performing SOTA methods."
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": ""
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": ""
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "2. PROPOSED METHOD"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "In this section, we ﬁrst review HuBERT model, which is the"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": ""
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "key backbone of our proposed method. Then, we introduce"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "our proposed method in detail, and the system framework is"
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": ""
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": "shown in Fig.1."
        },
        {
          "Fig. 1. Framework of the proposed FLEA. MSK and FC denote the mask and the fully connected layer respectively.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "regions. After pretraining,"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "transformer"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "discrete frame-level pseudo-emotion labels to achieve frame-"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "level ﬁne-grained emotion alignment embeddings."
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "Soft Attention"
        },
        {
          "other words, we only apply the loss function over the masked": "An advantage of the frame-level ﬁne-grained emotion align-"
        },
        {
          "other words, we only apply the loss function over the masked": "ment embeddings is that"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "the frame embeddings of different emotions, while the em-"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "beddings of frames representing the same emotion are typi-"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "By leveraging a simple attention, we can focus on"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "frames that are strongly related to the utterance-level emo-"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": "tion label, while disregarding frames that are irrelevant to the"
        },
        {
          "other words, we only apply the loss function over the masked": "emotion label. This approach effectively addresses the issue"
        },
        {
          "other words, we only apply the loss function over the masked": "of interference from emotion label-unrelated frames in SER."
        },
        {
          "other words, we only apply the loss function over the masked": "this work, we"
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        },
        {
          "other words, we only apply the loss function over the masked": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: , our method performswell",
      "data": [
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "system. For comparison with the baseline, we list\nthe results"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "of FLEA with attention pooling, and the results of the same"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ﬁne-tuning method as baseline, which ﬁne-tunes HuBERT for"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "SER with using average pooling to aggregate embeddings, as"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "shown in Table 1."
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "3.2.1.\nThe impact of embeddings and clusters"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "From Table 1, we can observe that different layers of embed-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "dings and number of clusters have different effects on SER"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "performance. No matter how many classes are clustered,\nthe"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "effect of pretraining HuBERT for SER with the frame-level"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "pseudo-emotion labels clustered by the embedding of the 9-th"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "layer is better than that of the 6-th and 11-th layers. This ex-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "perimental results demonstrate that the embedding of the 9-th"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "layer is more suitable for generating pseudo labels to pretrain"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "HuBERT. The ﬁnding is consistent with the study [8]."
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "In addition, regardless of which layer of embeddings are"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "used to cluster, the performance of SER decreases as the num-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ber of clusters increase. This phenomenon may be related to"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "the size of the dataset. A clustering number of 50 is appropri-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ate on the IEMOCAP dataset for pretraining HuBERT. How-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ever, we believe this is due to the limitation of dataset size."
        },
        {
          "different\nlayers of embeddings on the performance of SER": "With more sentiment data,\nthis cluster number may be larger"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "and the model would be more robust, as in the case of\nthe"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ofﬁcial HuBERT clustering number of 500."
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "3.2.2.\nThe role of pretraining HuBERT and attention pooling"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "As shown in Table 1,\nin the 9-th layer, ﬁne-tuning HuBERT"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "with average pooling for SER performs better than the base-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "line regardless of the numbers of clusters. This indicates that"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "using the MLM method to pretrain HuBERT to realize frame-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "level emotion state alignment yields better performance than"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "directly ﬁne-tuning wav2vec2.0 for the same purpose."
        },
        {
          "different\nlayers of embeddings on the performance of SER": "Furthermore, at a cluster number of 50,\nthe UA and WA"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "of FLEA are improved by 0.8% and 1.6%, respectively, com-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "pared to those of ﬁne-tuned CPT- HuBERT with average pool-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "ing for SER, and by 1.9% over the UA of the baseline. This in-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "dicates that introduction of attention to align frame-level em-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "beddings with utterance-labels is effective. Compared to av-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "erage pooling, attention pooling can pay attention to the those"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "frames strongly related to utterance-label, which makes better"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "use of aligned frame-level emotion information. Although the"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "performance of average pooling gradually approaches or even"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "exceeds that of attention pooling as the number of clusters in-"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "creases, the performance of the ﬁnal SER also decreases."
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "3.3. Performance Comparison with previous Methods"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "The effectiveness of our proposed method can be spotlighted"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "via\ncomparing with current key results performed on the"
        },
        {
          "different\nlayers of embeddings on the performance of SER": ""
        },
        {
          "different\nlayers of embeddings on the performance of SER": "IEMOCAP corpus\n(Table 2).\nIt\nshows\nthat\nthe best UA"
        },
        {
          "different\nlayers of embeddings on the performance of SER": "(75.7%)\nand WA (74.7%)\nare\nachieved\nby\nour\nproposed"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "ternational Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "[1]\nSeyedmahdad Mirsamadi, Emad Barsoum,\nand Cha Zhang,",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "cessing (ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "“Automatic speech emotion recognition using recurrent neu-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[12]\nItai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais,"
        },
        {
          "6. REFERENCES": "ral networks with local\nattention,”\nin 2017 IEEE Interna-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "and Ron Hoory,\n“Speaker normalization for self-supervised"
        },
        {
          "6. REFERENCES": "tional conference on acoustics, speech and signal processing",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "speech emotion recognition,”\nin ICASSP 2022-2022 IEEE In-"
        },
        {
          "6. REFERENCES": "(ICASSP). IEEE, 2017, pp. 2227–2231.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "ternational Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "6. REFERENCES": "[2] Runnan Li, Zhiyong Wu, Jia Jia, Sheng Zhao, and Helen Meng,",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "cessing (ICASSP). IEEE, 2022, pp. 7342–7346."
        },
        {
          "6. REFERENCES": "“Dilated residual network with multi-head self-attention for",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[13]\nSofoklis Kakouros, Themos Stafylakis, Ladislav Moˇsner, and"
        },
        {
          "6. REFERENCES": "speech emotion recognition,”\nin ICASSP 2019-2019 IEEE In-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Luk´aˇs Burget,\n“Speech-based emotion recognition with self-"
        },
        {
          "6. REFERENCES": "ternational Conference on Acoustics, Speech and Signal Pro-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "supervised models using attentive\nchannel-wise\ncorrelations"
        },
        {
          "6. REFERENCES": "cessing (ICASSP). IEEE, 2019, pp. 6675–6679.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "and label smoothing,”\nin ICASSP 2023-2023 IEEE Interna-"
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. REFERENCES": "[3]\nSrividya\nTirunellai Rajamani, Kumar\nT Rajamani, Adria",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "(ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "Mallol-Ragolta,\nShuo Liu,\nand Bj¨orn Schuller,\n“A novel",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[14] Leonardo Pepino, Pablo Riera, and Luciana Ferrer,\n“Emotion"
        },
        {
          "6. REFERENCES": "attention-based gated recurrent unit and its efﬁcacy in speech",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Recognition from Speech Using wav2vec 2.0 Embeddings,” in"
        },
        {
          "6. REFERENCES": "emotion recognition,”\nin ICASSP 2021-2021 IEEE Interna-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Proc. Interspeech 2021, 2021, pp. 3400–3404."
        },
        {
          "6. REFERENCES": "tional Conference on Acoustics, Speech and Signal Processing",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "(ICASSP). IEEE, 2021, pp. 6294–6298.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[15] Li-Wei Chen and Alexander Rudnicky,\n“Exploring wav2vec"
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "2.0 ﬁne tuning for improved speech emotion recognition,”\nin"
        },
        {
          "6. REFERENCES": "[4] Xixin Wu, Shoukang Hu, Zhiyong Wu, Xunying Liu, and He-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "ICASSP 2023-2023 IEEE International Conference on Acous-"
        },
        {
          "6. REFERENCES": "len Meng,\n“Neural\narchitecture search for\nspeech emotion",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp."
        },
        {
          "6. REFERENCES": "recognition,”\nin ICASSP 2022-2022 IEEE International Con-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "1–5."
        },
        {
          "6. REFERENCES": "ference on Acoustics, Speech and Signal Processing (ICASSP).",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[16] Yangyang Xia, Li-Wei Chen, Alexander Rudnicky, Richard M"
        },
        {
          "6. REFERENCES": "IEEE, 2022, pp. 6902–6906.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Stern, et al.,\n“Temporal context\nin speech emotion recogni-"
        },
        {
          "6. REFERENCES": "[5]\nJacob Devlin Ming-Wei Chang Kenton\nand Lee Kristina",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tion.,” in Interspeech, 2021, vol. 2021, pp. 3370–3374."
        },
        {
          "6. REFERENCES": "Toutanova, “Bert: Pre-training of deep bidirectional transform-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[17] Kaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni, Cheng-I Lai,"
        },
        {
          "6. REFERENCES": "ers for\nlanguage understanding,”\nin Proceedings of NAACL-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "David Cox, Mark Hasegawa-Johnson, and Shiyu Chang, “Con-"
        },
        {
          "6. REFERENCES": "HLT, 2019, pp. 4171–4186.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tentvec: An improved self-supervised speech representation by"
        },
        {
          "6. REFERENCES": "[6]\nSteffen\nSchneider, Alexei Baevski,\nRonan Collobert,\nand",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "disentangling speakers,”\nin International Conference on Ma-"
        },
        {
          "6. REFERENCES": "Michael Auli,\n“wav2vec:\nUnsupervised Pre-Training\nfor",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "chine Learning. PMLR, 2022, pp. 18003–18017."
        },
        {
          "6. REFERENCES": "Speech Recognition,”\nin Proc.\nInterspeech 2019, 2019, pp.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[18] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "6. REFERENCES": "3465–3469.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Chang, Sungbok Lee,\nand Shrikanth S Narayanan,\n“Iemo-"
        },
        {
          "6. REFERENCES": "[7] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "cap:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "Michael Auli, “wav2vec 2.0: A framework for self-supervised",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Language\nresources\nand evaluation,\nvol. 42,\npp. 335–359,"
        },
        {
          "6. REFERENCES": "learning of speech representations,” Advances in neural infor-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "2008."
        },
        {
          "6. REFERENCES": "mation processing systems, vol. 33, pp. 12449–12460, 2020.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[19]\nSuchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta,"
        },
        {
          "6. REFERENCES": "[8] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Kyle Lo,\nIz Beltagy, Doug Downey,\nand Noah A Smith,"
        },
        {
          "6. REFERENCES": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "“Don’t stop pretraining: Adapt\nlanguage models to domains"
        },
        {
          "6. REFERENCES": "Mohamed,\n“Hubert:\nSelf-supervised speech representation",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "the 58th Annual Meeting of\nthe\nand tasks,”\nin Proceedings of"
        },
        {
          "6. REFERENCES": "IEEE/ACM\nlearning by masked prediction of hidden units,”",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Association for Computational Linguistics, 2020, pp. 8342–"
        },
        {
          "6. REFERENCES": "Transactions on Audio, Speech, and Language Processing, vol.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "8360."
        },
        {
          "6. REFERENCES": "29, pp. 3451–3460, 2021.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[20] Yurun He, Nobuaki Minematsu, and Daisuke Saito,\n“Multi-"
        },
        {
          "6. REFERENCES": "[9]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "ple acoustic features speech emotion recognition using cross-"
        },
        {
          "6. REFERENCES": "jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "attention transformer,”\nin ICASSP 2023-2023 IEEE Interna-"
        },
        {
          "6. REFERENCES": "ioka, Xiong Xiao, et al., “Wavlm: Large-scale self-supervised",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. REFERENCES": "pre-training for full stack speech processing,” IEEE Journal of",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "(ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[21]\nJunyi He, Meimei Wu, Meng Li, Xiaobo Zhu, and Feng Ye,"
        },
        {
          "6. REFERENCES": "1518, 2022.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "“Multilevel\ntransformer for multimodal emotion recognition,”"
        },
        {
          "6. REFERENCES": "[10] Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Rus-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "in ICASSP 2023 - 2023 IEEE International Conference on"
        },
        {
          "6. REFERENCES": "lan Salakhutdinov,\nand Abdelrahman Mohamed,\n“Hubert:",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "Acoustics, Speech and Signal Processing (ICASSP), 2023, pp."
        },
        {
          "6. REFERENCES": "How much can a bad teacher beneﬁt asr pre-training?,”\nin",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "1–5."
        },
        {
          "6. REFERENCES": "ICASSP 2021-2021 IEEE International Conference on Acous-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "[22]\nSuzhen Wang, Yifeng Ma,\nand Yu Ding,\n“Exploring com-"
        },
        {
          "6. REFERENCES": "tics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "plementary features in multi-modal speech emotion recogni-"
        },
        {
          "6. REFERENCES": "6533–6537.",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "tion,”\nin ICASSP 2023-2023 IEEE International Conference"
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "6. REFERENCES": "[11]\nSiyuan Shen, Feng Liu, and Aimin Zhou,\n“Mingling or mis-",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        },
        {
          "6. REFERENCES": "",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": "2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "alignment? temporal shift for speech emotion recognition with",
          "pre-trained representations,”\nin ICASSP 2023-2023 IEEE In-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia",
        "Sheng Zhao",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "Srividya Tirunellai Rajamani",
        "Adria Kumar T Rajamani",
        "Shuo Mallol-Ragolta",
        "Björn Liu",
        "Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Neural architecture search for speech emotion recognition",
      "authors": [
        "Xixin Wu",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Lee Kristina"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Hubert: How much can a bad teacher benefit asr pre-training?",
      "authors": [
        "Wei-Ning Hsu",
        "Yao-Hung Hubert Tsai",
        "Benjamin Bolte",
        "Ruslan Salakhutdinov",
        "Abdelrahman Mohamed"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Mingling or misalignment? temporal shift for speech emotion recognition with pre-trained representations",
      "authors": [
        "Siyuan Shen",
        "Feng Liu",
        "Aimin Zhou"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu",
        "Edmilson Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Speech-based emotion recognition with selfsupervised models using attentive channel-wise correlations and label smoothing",
      "authors": [
        "Sofoklis Kakouros",
        "Themos Stafylakis",
        "Ladislav Mošner",
        "Lukáš Burget"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Richard Stern"
      ],
      "year": "2021",
      "venue": "Temporal context in speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Contentvec: An improved self-supervised speech representation by disentangling speakers",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Heting Gao",
        "Junrui Ni",
        "Cheng-I Lai",
        "David Cox",
        "Mark Hasegawa-Johnson",
        "Shiyu Chang"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "Suchin Gururangan",
        "Ana Marasović",
        "Swabha Swayamdipta",
        "Kyle Lo",
        "Iz Beltagy",
        "Doug Downey",
        "Noah Smith"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Multiple acoustic features speech emotion recognition using crossattention transformer",
      "authors": [
        "Yurun He",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Multilevel transformer for multimodal emotion recognition",
      "authors": [
        "Junyi He",
        "Meimei Wu",
        "Meng Li",
        "Xiaobo Zhu",
        "Feng Ye"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "Suzhen Wang",
        "Yifeng Ma",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}