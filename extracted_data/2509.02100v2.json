{
  "paper_id": "2509.02100v2",
  "title": "E-Ther: A Multimodal Dataset For Empathic Ai -Towards Emotional Mismatch Awareness",
  "published": "2025-09-02T08:58:32Z",
  "authors": [
    "Sharjeel Tahir",
    "Judith Johnson",
    "Jumana Abu-Khalaf",
    "Syed Afaq Ali Shah"
  ],
  "keywords": [
    "Multimodal Datasets",
    "Artificial Empathy",
    "Vision-Language Models",
    "Incongruence Detection",
    "Person-Centered Therapy",
    "Therapeutic Communication"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A prevalent shortfall among current empathic AI systems is their inability to recognize when verbal expressions may not fully reflect underlying emotional states. This is because the existing datasets, used for the training of these systems, focus on surface-level emotion recognition without addressing the complex verbal-visual incongruence (mismatch) patterns useful for empathic understanding. In this paper, we present E-THER, the first Person-Centered Therapy (PCT)-grounded multimodal dataset with multidimensional annotations for verbalvisual incongruence detection, enabling training of AI systems to advance towards realistic rather than performative empathic capabilities. The annotations included in the dataset are drawn from humanistic approach, i.e., identifying verbal-visual emotional misalignment in client-counsellor interactions -forming a framework for training and evaluating AI on empathy tasks. Additional engagement scores provide behavioral annotations for research applications. Notable gains in empathic and therapeutic conversational qualities are observed in state-of-the-art visionlanguage models, such as IDEFICS and VideoLLAVA, using evaluation metrics following empathic and therapeutic principles. Empirical findings indicate that our incongruence-trained models outperform general state-of-the-art models in critical traits, such as sustaining therapeutic engagement, minimizing artificial or exaggerated linguistic patterns, and maintaining fidelity to PCT theoretical framework.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Empathic dialogue generation is a central challenge for human-AI interaction. While large language models (LLMs) and vision-language models (VLMs) have advanced opendomain conversation, systems still tend to rely on surface regularities in text, producing responses that appear empathic without deeper situational grounding  [1] ,  [2] . In applied communication settings, this gap is often described as performative empathy -language that signals care but does not reflect nuanced understanding  [3] .\n\nA key source of nuance arises from multimodal inconsistency: what people say can diverge from how they present nonverbally. Counseling and communication theories describe such verbal-visual incongruence as diagnostically meaningful  [4] ,  [5] . Existing empathy and emotion resources, spanning text-only datasets  [6] ,  [7]  and multimodal corpora  [8] -  [11] , provide valuable foundations for response generation and emotion recognition. However, systematic annotation and modeling of verbal-visual incongruence for empathic response generation remain comparatively underexplored. In parallel, engagement level (e.g., low vs. high client engagement) is known to shape effective conversational strategies  [12] , yet many pipelines treat empathic behaviors as context-invariant despite evidence that engagement awareness can enhance interaction quality  [13] .\n\nThis paper addresses these gaps by introducing the Empathic THERapy Conversations (E-THER) dataset and a modeling-evaluation toolkit centered on incongruence-aware empathic communication. E-THER provides multimodal therapeutic dialogues with systematic annotations of verbal-visual mismatch, guided by Person-Centered Therapy (PCT) constructs. We use PCT as a theoretical lens because its core emphasis on empathy, unconditional positive regard, and congruence aligns closely with empathic communication, rather than as a therapeutic protocol to be delivered by AI  [4] ,  [14] . Figure  1  illustrates an example of verbal-visual mismatch and its annotation.\n\nOur main contributions are as follows: (1) E-THER dataset: a multimodal dialogue benchmark with PCT-guided annotations (focusing on mismatch between speaker's expressions and words) of verbal-visual incongruence to support modeling beyond surface cues. (2) Incongruence-aware training: methods that encourage models to attend to potential mismatch between what is said and shown, fostering more contextually grounded empathic responses  [15] . (3) Aligned evaluation: an automatic evaluation framework tailored to our annotations, i.e., conversational authenticity, responsive engagement, and alignment with Rogers' core conditions  [16] , designed to better reflect empathic communication quality.\n\nAcross experiments with multiple VLMs, these components improve incongruence detection and yield higher ratings on our empathy-aligned metrics compared to strong baselines (Sections V-VI), with ablations isolating the contribution of each component (Section VII).\n\nOur focus is empathic communication in AI. We do not claim clinical efficacy or propose AI-delivered therapy. PCT is used to define, annotate, and evaluate empathic behaviors and to highlight safety considerations (e.g., non-directiveness, avoidance of unsolicited advice) relevant to supportive, nonclinical interactions  [17] -  [19] .\n\nThe remainder of the paper is organized as follows. Section II situates our work within artificial empathy research. Section III details E-THER dataset -construction, annotation methodology, and validation. Section IV presents our training procedures for incongruence-aware response generation. Section V reports the evaluation setup, and Section VI provides results and analyses, followed by ablations in Section VII. Section VIII discusses limitations and future directions, and Section IX concludes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Empathic Ai And Dialogue Systems",
      "text": "Existing artificial empathy research has focused primarily on generating emotionally appropriate responses in conversational settings. The EmpatheticDialogues dataset  [1]  is a leading and comprehensive dataset in the domain that is also publicly available, providing 25,000 conversations grounded in emotional situations. Building upon this foundation, ESConv  [20]  introduced emotional support conversation as a structured task with 1,053 conversation exchanges incorporating eight support strategies grounded in Helping Skills Theory, which draws heavily from PCT. More recently, STICKERCONV  [21]  presented the first comprehensive multimodal empathetic dialogue (conversations) dataset with 12.9K sessions and visual sticker responses, while EDOS  [22]  contributed a large-scale dataset focused specifically on empathetic response generation. However, these approaches primarily emphasize response generation with limited focus on underlying cognitive processes that characterize empathic understanding.\n\nIncorporating emotional reasoning into empathic response generation through techniques such as emotion-cause recognition  [23]  and multi-level empathy modeling  [24]  has been seen in recent works. Advanced frameworks have emerged including LLM-based empathetic generation  [25]  and multidimensional evaluation approaches  [26] . Computational empathy has also been explored in mental health support contexts  [27] , demonstrating the potential for AI systems to understand and respond to emotional distress. While these approaches represent important advances, they primarily emphasize response generation over empathic reasoning processes or the ability to detect emotional incongruence.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Therapeutic And Empathy Datasets",
      "text": "The landscape of therapeutic dialogue datasets has expanded significantly, yet opportunities exist to enhance clinical grounding and theoretical grounding. ESConv  [20]  introduced emotional support conversation as a structured task, incorporating eight support strategies. However, these conversations rely on crowdsourced interactions that differ from clinical therapeutic settings. Recent multimodal datasets have begun addressing this limitation: MODMA  [28]  provides the first multi-modal open dataset for mental-disorder analysis with 53 participants including both clinically depressed patients and healthy controls, combining EEG and spoken language data.\n\nRecent multimodal approaches have attempted to address empathy detection in therapeutic contexts. MEDIC  [10]  provides 771 video clips from counseling sessions with empathy mechanism annotations, while MESC  [11]  extends this to comprehensive multimodal emotional support conversations. Clinical dialogue datasets have also emerged, including MTS-Dialog for doctor-patient encounters  [29] . General emotion recognition datasets, including IEMOCAP  [8]  and MELD  [9] , provide multimodal emotion annotations but focus on classification rather than empathic understanding. These datasets utilize acted scenarios or entertainment content that may not generalize to therapeutic interactions, suggesting value in clinically informed datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Multimodal Emotion Recognition And Incongruence Detection",
      "text": "The integration of visual and textual information for emotion recognition has shown significant promise  [30] ,  [31] , but existing multimodal approaches primarily focuses on emotion classification tasks rather than the nuanced detection of emotional misalignment patterns. Alexithymia research demonstrates that individuals can exhibit systematic crossmodal emotional inconsistencies  [32] . Machine learning approaches have been developed to identify complex emotions in alexithymia-affected individuals  [33] , highlighting the clinical relevance of emotion discrepancy detection.\n\nLarge vision-language models have shown promise for contextual emotion recognition  [34] , while specialized approaches for micro-expression analysis using vision transformers demonstrate effectiveness in detecting subtle emotional cues  [35] . Comprehensive surveys indicate that multimodal emotion recognition with deep learning continues to face challenges in handling cross-modal inconsistencies  [36] .\n\nAdvances in vision-language models demonstrate capacity for understanding complex visual-textual relationships  [37] , yet these capabilities have not been systematically applied to therapeutic or empathic communication analysis. This represents a research opportunity, given the successful integration of VLMs in various relevant tasks including emotion recognition from multimodal content  [38] , mental health assessment through visual and textual cues  [39] , and healthcare communication evaluation  [40] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Evaluation Approaches For Empathic Systems",
      "text": "One of the limitations in current artificial empathy research is the reliance on evaluation metrics with limited prowess in capturing empathic communication nuances  [41] . Traditional approaches use lexical matching (BLEU, ROUGE) or semantic matching with gold-standard over empathic understanding  [42] ,  [43] . These word-overlap metrics demonstrate weak or no correlation with human judgments in dialogue evaluation, showing limited effectiveness for the nuanced requirements of empathic communication  [44] . Recent empirical studies reveal that BLEU and ROUGE scores show minimal correlation with human assessments of dialogue quality, with correlations often approaching zero in conversational contexts  [43] .\n\nMore sophisticated evaluation frameworks for empathy in artificial agents have been introduced lately. The Perceived Empathy of Technology Scale (PETS) provides a validated instrument for measuring users' perceptions of AI system empathy  [45] , while research on third-party evaluation demonstrates that AI can be perceived as more compassionate than expert humans in certain contexts  [46] . Multi-dimensional evaluation approaches have emerged that assess empathy across cognitive, affective, and behavioral dimensions  [26] , moving beyond Fig.  1 : A snippet from one of the recorded conversations that depicts the verbal and visual content of the conversations in the E-THER dataset. It also contains an incongruent example of (minimizing) type where the client verbally expresses emotional distress, while their facial expression remains predominantly neutral -illustrating a cross-modal affective mismatch. Such incongruences are manually annotated (I = 1) against each dialogue pair as such in our dataset to support emotion modeling beyond surface cues. The counsellor's response reflects PCT principles, using non-directive reflection and open inquiry to support client-led emotion discovery. surface-level linguistic analysis. Additionally, specialized algorithms for empathy categorization across conversations have been developed  [47] , offering more nuanced evaluation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "E. Person-Centered Therapy And Computational Applications",
      "text": "The PCT framework provides established principles for empathic communication through Rogers' core therapeutic conditions: empathy, unconditional positive regard (the therapist's acceptance and non-judgmental valuing of the client regardless of what is disclosed), and emotional congruence (the consistency between verbal expressions and nonverbal cues, where mismatches may reveal underlying emotions)  [4] . These conditions have been empirically validated across multiple therapeutic modalities and represent fundamental requirements for effective therapeutic relationships  [14] . However, none of the existing artificial empathy approaches have operationalized these principles for empathic AI development.\n\nRecent computational approaches to therapy have focused majorly on Cognitive Behavioral Therapy techniques  [48]  or general mental health support  [49] , with limited attention to PCT-specific empathic communication patterns. The systematic annotation of Rogers' conditions in therapeutic interactions represents a novel contribution to computational psychology research.\n\nWhile artificial empathy systems show promise in supportive roles, research indicates fundamental distinctions between simulated and therapeutic relationships  [50] . Our work positions itself within the domain of supportive AI tools and research applications rather than clinical therapeutic contexts, focusing on advancing computational understanding of empathic communication patterns that characterize effective therapeutic interactions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Development Of E-Ther",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Theoretical Foundation And Data Collection",
      "text": "Our dataset comprises multimodal conversational data from empathic and therapeutic conversations, providing the first source specifically designed for training verbal-visual mismatch-aware empathic AI grounded in established therapeutic theory  [17] ,  [51] ,  [52] . The dataset includes synchronized video, audio, and transcript data from 18 therapeutic sessions conducted by a registered clinical psychologist with 18 participants, totaling approximately 5 hours of interaction data with 1578 dialogue turns after data cleaning.\n\nThe sessions were conducted following PCT principles, emphasizing non-directive, empathic communication that supports client self-discovery rather than prescriptive guidance  [4] . This approach provides a theoretically grounded and ethically appropriate framework for AI empathic training, minimizing risks of inappropriate therapeutic boundary crossing while developing empathic understanding capabilities.\n\nParticipants were recruited through multiple sampling strategies, including digital recruitment materials distributed across university campuses and community networks. The final sample comprised 18 participants (n=8 female, n=10 male) with ages ranging from 19 to 72 years. The cohort represented",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Training Sample Structure",
      "text": "The dataset employs a synchronized multimodal structure where video frames are temporally aligned with dialogue utterances at turn boundaries. Each training instance comprises a RGB frame extracted at the precise moment of client verbal response, coupled with the corresponding transcribed utterance. This temporal synchronization enables systematic analysis of cross-modal affective incongruence by providing simultaneous access to facial expression patterns and verbal emotional content. The frame extraction methodology ensures capture of authentic facial expressions during natural speech production, forming the empirical foundation for verbal-visual misalignment detection in therapeutic contexts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Annotation Framework",
      "text": "Our annotation framework focuses on detecting when verbal expressions contradict visual emotional cues, a discrepancy reflecting non-verbal leakage, which has been shown to reduce perceived responsiveness and trust in emotionally charged contexts  [53] . We also assess engagement levels to capture how people actually interact in therapeutic settings. This approach provides measurable annotations for training empathic AI.\n\n1) Verbal-Visual Incongruence Detection: The detection of verbal-visual incongruence represents an important contribution to empathic AI research, designed to enhance empathic accuracy by enabling AI models to recognize when clients' verbal expressions may not fully reflect their emotional experience -Figure  1  presents an example of such cases from our dataset. This capability supports Rogers' empathic understanding -enabling the ability to perceive the client's internal state more accurately  [15] , drawing on emotion-focused therapy principles that emphasize the importance of recognizing and responding to underlying emotional experiences  [54] .\n\nResearch in empathic accuracy demonstrates that effective empathic responding requires recognition of both explicit verbal content and implicit emotional indicators  [55] . To support this incongruence detection, we had the annotators mark three types of verbal-visual misalignment that commonly occur in therapeutic contexts (methodology detailed in Section III-D): Minimizing incongruence occurs when visual emotional indicators suggest stronger intensity than verbally acknowledged, with individuals appearing more distressed than stated, for instance. It also includes emotional slip through facial expressions despite controlled verbal presentation. Contradiction incongruence involves direct opposition between visual and verbal emotional indicators, such as happy expression while discussing sad events. No incongruence -when facial expressions align with verbal communication.\n\n2) Engagement Level Assessment: Engagement level annotations provide comprehensive behavioral assessments to quantify active participation and psychological presence during therapeutic interactions, supporting research applications and validation of incongruence-focused training approaches. This approach builds on established therapeutic alliance research  [56] , as alliance quality has been consistently linked to treatment outcomes, with engagement serving as a key measurable component of the collaborative therapeutic relationship necessary for effective empathic communication  [57] .\n\nThe engagement assessment employed a continuous scale from 0 to 1 (methodology detailed in Section III-D), with three primary ranges: Low engagement (0.0-0.3) was characterized by minimal eye contact, distracted appearance, monosyllabic responses, and behavioral indicators of withdrawal. Moderate engagement (0.4-0.7) represented typical therapeutic participation, including normal eye contact patterns and appropriate conversational responsiveness. High engagement (0.8-1.0) indicated active participation marked by sustained eye contact, animated facial expressions, and detailed verbal responses.\n\n3) Supporting Annotations: The framework includes traditional Valence, Arousal, and Dominance (VAD) annotations following Russell's circumplex model of affect  [58]  and Bradley and Lang's dimensional approach to emotion  [59] . These dimensions provide compatibility with existing emotion recognition frameworks while supporting empathic understanding through comprehensive emotional state assessment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Quality Assurance And Dataset Characteristics",
      "text": "To ensure methodological rigor, we implemented an expert validation protocol. Three trained raters, briefed on therapeutic communication concepts through relevant literature, completed manual annotations. Subsequently, a certified clinical psychologist and established researcher in therapeutic communication validated annotation accuracy by independently reviewing a stratified random sample of 100 dialogue pairs (12.7% of the total corpus), with proportional representation from each rater. The expert review revealed strong overall agreement, with consensus on 83 of 100 dialogue pairs (83%). Among the 17 disagreements, all pertained exclusively to incongruence annotations: 8 cases involved the presence/absence of incongruence (incongruent vs. congruent), while 9 cases involved misclassification of incongruence type (e.g., annotated as \"minimizing\" when expert assessed as \"contradicting\"). This pattern indicates that raters reliably identified incongruence but showed some variability in categorizing specific incongruence subtypes. Inter-rater reliability metrics are presented in Table  II .\n\nThe final dataset contains 789 dialogue pairs (1,578 utterances) across our corpus. Among these, 161 dialogue pairs (20.4%) contain instances of verbal-visual incongruence, distributed across two primary types. The complete dataset includes 3,945 individual annotations (789 dialogue pairs × 5 annotation dimensions), with substantial representation of varying engagement levels and authentic expression patterns.  2) Scalability Framework: This dataset establishes methodological foundations for larger-scale collection. The annotation framework, inter-rater reliability protocols, and training methodologies provide validated approaches for systematic scaling while maintaining annotation quality standards essential for empathic and therapeutic AI.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "F. Comparison With Existing Datasets",
      "text": "While large-scale datasets like EmpatheticDialogues  [60]  provide extensive conversational data, they rely on crowdsourced interactions lacking authenticity. Therapeutic datasets such as ESConv  [20]  and MESC  [11]  incorporate support strategies but miss the theoretical grounding of established therapeutic frameworks. Multimodal emotion datasets including IEMOCAP  [8]  and MELD  [9]  focus on emotion classification rather than empathic understanding.\n\nE-THER's unique position is evident in its combination of features, as depicted in Table  I : it is the only dataset comprising verbal-visual incongruence detection. Although E-THER is smaller in scale than datasets like EmpatheticDialogues or ESConv, it achieves high annotation intensity (789 annotations/hour of data) through its comprehensive fourdimensional framework. This quality-over-quantity approach ensures that each annotation captures the nuanced therapeutic interactions essential for training empathic AI systems capable of recognizing genuine emotional states beyond surface-level expressions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Training Methodology",
      "text": "To evaluate the generalizability of our benchmarking framework (Figure  2 ), we conducted experiments using three state-of-the-art Vision-Language Models: IDEFICS2 8B  [61] , VideoLLAVA 7B  [62] , and BLIP2  [63] . Each model received identical empathy-enhanced training parameters, enabling direct comparison of empathic and therapeutic improvements across different architectures. These three models were chosen to represent different VLM capabilities -conversational instruction-following (IDEFICS2), temporal understanding (VideoLLAVA), and vision-language foundation modeling (BLIP2) -while remaining computationally feasible for our training methodology  [64] . We prioritized open-source models for reproducible research.\n\nAll models utilized LoRA fine-tuning  [65]  to enable efficient training while preserving base model capabilities. Training was performed on 16 conversations from our dataset with the remaining 2 reserved for evaluation.\n\nA. Empathy-Enhanced Training Architecture 1) Self-Supervised Emotion Understanding: To promote autonomous multimodal emotion recognition capabilities, Valence-Arousal-Dominance annotations are systematically masked during model training. This minimizing protocol prevents dependency on explicit affective labels, compelling models to develop intrinsic cross-modal emotion understanding through direct visual-textual feature correlation.\n\n2) Context Dropout for Implicit Congruence Learning: Traditional empathy training provides explicit emotional context, potentially leading to dependency on explicit emotional cues rather than developing genuine empathic perception capabilities. Our context dropout approach addresses this limitation by randomly removing explicit empathy context during 30% of training iterations.\n\nThis methodology supports PCT's emphasis on empathic understanding through careful observation and emotional attunement  [15] , ensuring that trained models develop robust empathic perception capabilities.\n\n3) Incongruence-Focused Weighted Learning: We replace the binary weight with a bounded, continuous score that scales smoothly with multimodal incongruence while preserving the intended 2:1 emphasis at high incongruence. Let\n\ni ), y i be the per-sample loss. We define a simple incongruence score s i ∈ [0, 1] by combining (i) VAD mismatch and (ii) cross-modal embedding misalignment:\n\nwith small τ e > 0 and λ ≥ 0 (we use τ e as the batch median VAD mismatch and λ=0.5). The loss weight is then a monotone, bounded mapping:\n\nso w i ∈ [1, 2] and increases smoothly with incongruence (larger γ sharpens the emphasis). The training objective becomes\n\nNotes. (i) If only a binary indicator I i ∈ {0, 1} is available, set s i =I i to recover the original scheme (w=1 vs. 2). (ii) For stable scaling across batches, an optional one-line normalization\n\n|B| j∈B w j keeps the batch-mean weight at 1 without changing the relative emphasis.\n\nIncongruent instances receive amplified learning signals proportional to their documented cognitive complexity, enabling models to develop the sophisticated multimodal empathic reasoning capabilities that distinguish effective therapeutic communication from surface-level empathic responses  [66] ,  [67] . Algorithm 1 further illustrates the process of training and it's components. Under our incongruence-aware weighting, w i = 1 + I i , incongruent instances (I i =1) contribute a doubled loss scale, yielding stronger gradients and visibly increased cross-modal coupling, especially at the fusion layers (dashed rows; crossattention blocks in Flamingo/BLIP-2-style architectures). In combination with context dropout (random removal of explicit emotional cues in ∼30% of iterations) and self-supervised emotion understanding (VAD labels masked during training), this regime reduces reliance on explicit affective labels and encourages intrinsic, multimodal empathic reasoning (cf.  [15] ). The effect is architecture-agnostic: across IDEFICS2  [61] , VideoLLaVA  [62] , and BLIP-2  [63]  backbones fine-tuned with LoRA  [65] , incongruent training amplifies information travel around fusion layers, aligning with the cognitive-load view that incongruent cases require greater processing resources  [68] . See Algorithm 1 for training details.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Layer-Wise Information Travel",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Evaluation Framework",
      "text": "We propose an evaluation framework that examines empathic communication quality through assessment of PCT core conditions. Our evaluation approach addresses the critical need for automatic evaluation of empathic communication within appropriate ethical boundaries, emphasizing the non-directive, client-centered approach rather than therapeutic intervention application  [69] .\n\nWhile our proposed PCT-based metrics provide comprehensive assessment of therapeutic communication quality, we complement this evaluation with BERT score analysis to establish external validity against established semantic similarity measures. BERT scores  [70]  compare model-generated responses to original user dialogues, providing an additional perspective on response appropriateness that, while not capturing the nuanced therapeutic principles central to our framework, offers standardized comparison with baseline models and validation of overall semantic coherence.\n\nA. Core Evaluation Metrics 1) Empathic Authenticity Assessment: Based on Rogers' congruence principle  [15] , this composite metric evaluates AI responses for authenticity versus performative empathy through comprehensive analysis of genuine communication patterns  [71] ,  [72] . This metric combines two complementary approaches: detection of natural conversational elements for sample (v i , t i , I i , E i ) ∈ B do 7:\n\n{Context Dropout for Implicit Learning} 23: return θ * ← θ including acknowledgment responses (\"right\", \"okay\", \"actually\", \"interesting\") and authentic engagement patterns, identifying therapeutic communication originality.\n\n2) Responsive Engagement Assessment: Drawing on empathic accuracy research  [55] , this metric evaluates models' ability to respond specifically to individual client presentations rather than providing generic empathic responses. The assessment combines situational responsiveness detection (\"given\", \"considering\", \"in your situation\") with client language mirroring analysis that measures semantic alignment between client content and AI responses. This ensures understanding of specific user contexts rather than relying on universally applicable empathic statements, supporting PCT's emphasis on accurate perception of individual internal frames of reference  [4] .\n\n3) Therapeutic Concision: This metric measures communication clarity and purposefulness that facilitates selfexploration  [73] . Therapeutic concision assessment evaluates:\n\n• Communication Clarity: Detection of clear communication markers (\"specifically\", \"exactly\", \"what I hear\") • Purposefulness: Identification of goal-directed empathic language (\"to understand\", \"to help\") 4) PCT Adherence Composite Score: Our framework includes a comprehensive PCT adherence measure calculated as the average of Rogers Core Conditions, Conversational Authenticity, and Therapeutic Concision scores. Rogers' three necessary therapeutic conditions are as follows  [4] :\n\n• Empathic Understanding: Combines traditional empa-thy markers (\"you feel\", \"you're experiencing\") with genuine curiosity indicators (\"I'm wondering\", \"what's that like\") that demonstrate authentic interest in client experience.\n\n• Unconditional Positive Regard: Measures nonjudgmental acceptance language (\"that makes sense\", \"that's understandable\") while applying judgment penalties for directive or prescriptive responses (\"you should\", \"you need to\"). • Therapeutic Congruence: Evaluates authenticity markers enhanced with semantic similarity analysis against therapeutic congruence concept embeddings when GloVe vectors are available. Together, these sub-scales capture foundational empathic capabilities across all three conditions while maintaining appropriate boundaries for deployment  [14] ,  [74] .\n\nThis composite metric (PCT Adherence) provides an overall assessment of person-centered empathic competence that integrates relationship foundation with communication effectiveness while maintaining theoretical coherence with established PCT principles.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Baseline Model Configuration",
      "text": "To ensure fair comparison, GPT-4V serves as the primary baseline for evaluating our training effectiveness. The configuration details ensure reproducible evaluation and appropriate comparison with fine-tuned models. • Each test instance processed independently to prevent conversation history effects. • Three response generations per instance with temperature 0.7, selecting median length response for consistency. 3) Quality Assurance: Manual verification that all GPT-4V responses addressed both visual and textual components. Responses failing to demonstrate multimodal processing (ignoring visual cues) were excluded from analysis (<3% of total).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Experimental Results",
      "text": "In this section, we provide the comparison of performance between models trained on our dataset and GPT-4V on a wide range of metrics -semantic vlaidation (BERT scores) and our proposed PCT-grounded scores.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Performance Pattern Analysis",
      "text": "Both VideoLLaVA and IDEFICS2 achieved superior empathic capabilities compared to GPT-4V (Tables  III  and IV ), with substantial improvements in empathic authenticity.\n\nEnhanced questioning strategies emerged as a significant strength across both models, with notable betterment in appropriate question density suggesting better therapeutic concision techniques compared to GPT-4V's more passive approach.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Semantic Similarity Validation",
      "text": "To establish external validity of our PCT-based improvements, we conducted BERT score analysis (a widely used evaluation metric in the domain) comparing model responses to original user dialogues.  The BERT score analysis reveals both the utility and limitations of semantic similarity metrics for therapeutic dialogue evaluation. VideoLLaVA achieved the highest F1 score of 0.845, demonstrating superior semantic alignment with user dialogues compared to IDEFICS2 (0.836), GPT-4V (0.840), and BLIP2 (0.806). However, a critical observation from the BLIP2 results illustrates the inadequacy of semantic similarity alone for task-specific evaluation.\n\nDespite BLIP2 producing remarkably short and monotonous responses consisting primarily of repetitive sympathetic phrases such as \"You are not alone\" and \"I'm sorry to hear that,\" the model still achieved a respectable BERT F1 score of 0.806. This phenomenon demonstrates that semantic similarity metrics, while valuable for general text coherence, fail to capture the nuanced requirements of empathic and therapeutic communication. The disconnect between BLIP2's high BERT scores and its inadequate therapeutic responses exemplifies why domain-specific evaluation is crucial, precisely where our evaluation framework becomes indispensable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Architectural Considerations",
      "text": "The comparative performance across different visionlanguage architectures provides insights into empathic capability development. VideoLLAVA demonstrated strong balanced performance across all metrics, suggesting robust empathic communication capabilities with particular strength in multimodal emotional understanding. IDEFICS2 showed exceptional performance in specific areas, particularly question density and situational responsiveness, indicating strong language capabilities.\n\nBLIP2 showed limited improvement in empathic communication capabilities, with difficulties maintaining natural conversational flow in therapeutic contexts. The model tended toward fragmented or overly formal responses lacking conversational authenticity essential for effective therapeutic communication. This limitation suggests that conversational coherence capabilities represent essential prerequisites for effective therapeutic communication that some architectures may not adequately support.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Ablation Study On Weighting And Modalities",
      "text": "To understand the contribution of each component in our incongruence-focused empathic dialogue framework, we conduct a comprehensive ablation study with VideoLLaVA (Table  VI ) and IDEFICS2 (VII). BLIP2 was not included due to its constrained dialogue generation profile yielding minimal acknowledgment responses rather than substantive therapeutic exchanges.\n\nA. Experimental Design 1) Ablation Conditions: We evaluate three experimental conditions to isolate the effects of different components: a) No Incongruence Weighting: Removes incongruencebased weighting, using uniform weights (w = 1.0). This tests the contribution of verbal-visual incongruence detection prioritization.\n\nb) Engagement-Informed Weighting: Implements theoretical principles from therapeutic alliance research by incorporating engagement-based weighting alongside incongruence detection:\n\nThis inverse engagement weighting reflects therapeutic alliance theory that low-engagement scenarios require more sophisticated empathic calibration  [56] , warranting enhanced training attention. The formulation tests whether engagementinformed training develops empathic response modulation capabilities essential for effective therapeutic communication across varying client psychological availability levels. c) Text-Only: Uses incongruence weighting but removing visual inputs during training (w = 1.0 + 1.0 × I). This evaluates the contribution of multimodal data (visual input) to the claimed better empathic dialogue generation.\n\n2) Training Configuration: All ablation models are trained using similar configurations (determined empirically) to ensure fair comparison. Table IX reports these configurations for VideoLLaVA and IDEFICS models.\n\n3) Evaluation Protocol: We evaluate all models on the held-out test set containing 60 dialogue pairs form 2 conversations. Each model generates responses to the same prompts, which are then assessed using our PCT-based evaluation framework comprising seven core metrics.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Results And Analysis",
      "text": "The ablation analysis reveals distinct patterns of component dependency across vision-language architectures, with three statistically significant findings that illuminate the fundamental mechanisms underlying empathic dialogue generation (Table VIII).     1) Universal Multimodal Dependency: The most consistent finding across both architectures is the degradation of PCT Adherence when visual information is removed during training. Both VideoLLaVA (-8.7%, p=0.007, d=-0.52) and IDEFICS2 (-8.0%, p=0.047, d=-0.32) demonstrate significant performance decrements under text-only conditions, with VideoLLaVA showing a medium effect size that approaches the threshold for practical significance. This convergent evidence establishes multimodal processing as an advantageous element for therapeutic dialogue generation, supporting the theoretical premise that empathic understanding necessitates integration of verbal and visual emotional cues  [5] .\n\n2) Architecture-Specific Vulnerabilities: A striking asymmetry emerges in the response to incongruence weighting removal. IDEFICS2 exhibits a substantial degradation in Responsive Engagement (-17.8%, p=0.019, d=-0.30) when incongruence-based sample weighting is eliminated, while VideoLLaVA shows no statistically significant change in this domain. This differential sensitivity suggests that IDEFICS2's empathic capabilities are more tightly coupled to explicit incongruence detection signals during training, whereas Vide-oLLaVA may develop more robust implicit incongruence recognition through its architectural design.\n\n3) Therapeutic Communication Precision: VideoLLaVA demonstrates consistent vulnerability in Therapeutic Concision across multiple ablation conditions, with both incongruence weighting removal (-4.9%, p=0.081) and text-only training (-5.0%, p=0.060) producing trend-level degradations. While these effects do not reach conventional statistical significance, the consistency of the pattern (Cohen's d = -0.33 for both conditions) suggests a meaningful relationship between component availability and communicative precision.\n\n4) Practical Significance: The effect sizes observed -ranging from small to medium according to Cohen's conventions -represent meaningful changes in therapeutic communication quality. The 8-9% degradations in PCT Adherence correspond to clinically relevant differences in empathic communication effectiveness, while the 18% reduction in IDEFICS2's Responsive Engagement represents an impairment in contextual empathic responsiveness. These findings demonstrate that the proposed framework components contribute meaningfully to plausible empathic dialogue generation.\n\nThe convergent evidence across architectures establishes that effective empathic dialogue generation requires careful integration of multimodal information processing and incongruence-aware training procedures, though the specific mechanisms through which these components contribute may vary systematically across different VLM designs.\n\n5) Component Interaction Effects: Figure  4  illustrates the distribution of effect sizes across both models and all ablation conditions. The analysis reveals that empathic dialogue generation depends on architecture-specific interactions between incongruence detection, engagement assessment, and multimodal processing. Table  X  provides qualitative examples illustrating how each ablation affects response generation, demonstrating the specific therapeutic communication deficits that arise when key model components are removed.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Viii. Limitations And Future Research Directions",
      "text": "Several methodological and theoretical considerations warrant careful examination for advancing empathic AI research. While E-THER establishes a foundation for PCT-grounded empathic systems, these areas present specific opportunities for scientific advancement and framework refinement.\n\nTheoretical and Methodological Considerations: The exclusive focus on Person-Centered Therapy, while theoretically principled, may limit direct transferability to other therapeutic modalities (e.g., Cognitive Behavioral Therapy, Dialectical Behavior Therapy) that employ different empathic communication strategies. Cross-theoretical validation studies comparing incongruence detection patterns across therapeutic approaches would establish the framework's broader applicability. Additionally, the binary classification of incongruence types may oversimplify the continuous nature of verbalvisual misalignment, suggesting opportunities for dimensional approaches that capture incongruence severity and temporal dynamics.\n\nEngagement Annotation Research Directions: Our ablation analysis reveals mixed patterns in engagement-based training modifications, with VideoLLaVA showing improvements in Rogers Core Conditions (+5.7%) while IDEFICS2 demonstrates no significant benefits from engagementinformed weighting. These differential responses suggest that engagement annotations may serve more effectively as analytical tools for understanding client presentation variations rather than direct training signals. Future research should investigate whether engagement patterns correlate with therapeutic outcomes and explore alternative computational approaches to modeling client participation dynamics that may prove more suitable for training objectives.\n\nDataset Scale and Architectural Considerations: E-THER's focused approach prioritizes annotation depth over dataset scale, achieving high annotation intensity (789 annotations/hour) through comprehensive four-dimensional analysis. This design choice enables detailed incongruence detection training while creating opportunities for investigating scaling strategies that maintain annotation quality. Future work should explore data augmentation techniques specific to therapeutic interactions and examine how incongruence detection capabilities transfer across different VLM architectures and larger datasets.\n\nEvaluation Framework Robustness: Our PCT-based evaluation metrics demonstrate effectiveness in distinguishing sincere empathic communication from performative responses, yet systematic comparison with established clinical assessment instruments represents an important validation direction. Integration studies comparing our computational metrics against gold-standard measures such as the Jefferson Scale of Empathy  [75]  and Consultation and Relational Empathy Scale  [76]  would establish convergent validity and potential calibration protocols.\n\nHuman expert evaluation represents a widely-adapted validation component for establishing clinical or practical application of such systems. While computational metrics provide scalable and consistent measurement capabilities, further validation of the responses through licensed counsellors and other human participants can signify the achieved performance.\n\nEmpirical Observations Requiring Investigation: Preliminary analysis reveals evidence of therapeutic memory persistence and conversational compactness within sessions -phenomena where the model references earlier session content and demonstrate increasing communicative efficiency over time. These patterns, while theoretically consistent with therapeutic alliance development, require systematic quantitative analysis. Future research should develop metrics for measuring conversational coherence across session time and its correlation with therapeutic progress indicators.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Ix. Conclusion",
      "text": "We present the first multimodal empathy dataset that enables artificial agents to develop empathic capabilities through detection of verbal-visual emotional incongruence. Our novel training methodologies demonstrate noticeable improvements over state-of-the-art models across three vision-language architectures using Person-Centered Therapy evaluation principles.\n\nThe comprehensive evaluation reveals significant performance improvements over GPT-4V across metrics, with notable gains in empathic authenticity, and appropriate questioning strategies. These results demonstrate that PCT-grounded training produces empathic AI models capable of genuine rather than superficial empathic communication.\n\nThe demonstrated effectiveness across multiple visionlanguage models suggests that incongruence aware empathic training represents a generalizable approach for enhancing empathy in AI. Future work should explore application of these principles to larger-scale models and diverse empathic communication contexts and clinical validation.\n\nOur contributions provide foundational resources for artificial empathy research that prioritizes genuine empathic capabilities over superficial empathic language generation, promoting the development of nuanced empathic understanding and modeling in AI.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates an example of verbal–visual mismatch and",
      "page": 1
    },
    {
      "caption": "Figure 1: A snippet from one of the recorded conversations that depicts the verbal and visual content of the conversations in the",
      "page": 3
    },
    {
      "caption": "Figure 1: presents an example of such cases from our",
      "page": 4
    },
    {
      "caption": "Figure 2: Training pipeline showing dataset preparation (18 conversations - 16 Train + 2 Eval), VLM fine-tuning with LoRA",
      "page": 5
    },
    {
      "caption": "Figure 2: ), we conducted experiments using three",
      "page": 6
    },
    {
      "caption": "Figure 3: visualizes layer-wise cross-modal information flow",
      "page": 6
    },
    {
      "caption": "Figure 3: Layer-wise information travel during empathy-enhanced training. Each heatmap shows relative cross-modal flow (rows:",
      "page": 7
    },
    {
      "caption": "Figure 4: Effect sizes and significance levels across all ablation conditions for both VideoLLaVA and IDEFICS2. Points represent",
      "page": 11
    },
    {
      "caption": "Figure 4: illustrates the",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2018",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "2",
      "title": "Are large language models more empathetic than humans?",
      "authors": [
        "A Welivita",
        "P Pu"
      ],
      "year": "2024",
      "venue": "Are large language models more empathetic than humans?",
      "arxiv": "arXiv:2406.05063"
    },
    {
      "citation_id": "3",
      "title": "In principle obstacles for empathic ai: why we can't replace human empathy in healthcare",
      "authors": [
        "C Montemayor",
        "J Halpern",
        "A Fairweather"
      ],
      "year": "2022",
      "venue": "AI & society"
    },
    {
      "citation_id": "4",
      "title": "The necessary and sufficient conditions of therapeutic personality change",
      "authors": [
        "C Rogers"
      ],
      "year": "1957",
      "venue": "Journal of consulting psychology"
    },
    {
      "citation_id": "5",
      "title": "Decoding of inconsistent communications",
      "authors": [
        "A Mehrabian",
        "M Wiener"
      ],
      "year": "1967",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Modeling empathy and distress in reaction to news stories",
      "authors": [
        "S Buechel",
        "A Buffone",
        "B Slaff",
        "L Ungar",
        "J Sedoc"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Z Lin",
        "A Madotto",
        "J Shin",
        "P Xu",
        "P Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Medic: A multimodal empathy dataset in counseling",
      "authors": [
        "Z Zhu",
        "X Li",
        "J Pan",
        "Y Xiao",
        "Y Chang",
        "A Zhou",
        "Y Zheng",
        "Y Zhang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Towards multimodal emotional support conversation systems",
      "authors": [
        "Y Chen",
        "H Liu",
        "Y Wang",
        "J Li",
        "F Zhang",
        "H Wu",
        "J Liu",
        "M Zhang"
      ],
      "year": "2024",
      "venue": "Towards multimodal emotional support conversation systems",
      "arxiv": "arXiv:2408.03650"
    },
    {
      "citation_id": "12",
      "title": "Motivational interviewing: Helping people change",
      "authors": [
        "W Miller",
        "S Rollnick"
      ],
      "year": "2012",
      "venue": "Motivational interviewing: Helping people change"
    },
    {
      "citation_id": "13",
      "title": "The role of socioemotional attributes in enhancing human-ai collaboration",
      "authors": [
        "M Kolomaznik",
        "V Petrik",
        "M Slama",
        "V Jurik"
      ],
      "year": "2024",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "14",
      "title": "The role of empathy in promoting change",
      "authors": [
        "J Watson",
        "P Steckley",
        "E Mcmullen"
      ],
      "year": "2014",
      "venue": "Psychotherapy Research"
    },
    {
      "citation_id": "15",
      "title": "On becoming a person: A therapist's view of psychotherapy",
      "authors": [
        "C Rogers"
      ],
      "year": "1961",
      "venue": "On becoming a person: A therapist's view of psychotherapy"
    },
    {
      "citation_id": "16",
      "title": "A theory of therapy, personality, and interpersonal relationships: As developed in the client-centered framework",
      "year": "1959",
      "venue": "Psychology: A study of a science"
    },
    {
      "citation_id": "17",
      "title": "The friendly relationship between therapeutic empathy and person-centred care",
      "authors": [
        "D Hardman",
        "J Howick"
      ],
      "year": "2019",
      "venue": "European Journal for Person Centered Healthcare"
    },
    {
      "citation_id": "18",
      "title": "Person-centered therapy: A revolutionary paradigm",
      "authors": [
        "J Bozarth"
      ],
      "year": "1998",
      "venue": "Person-centered therapy: A revolutionary paradigm"
    },
    {
      "citation_id": "19",
      "title": "Nature Machine Intelligence, 2024, discusses risks and ethical concerns of empathic AI implementations",
      "authors": [
        "E Stevenson"
      ],
      "venue": "Nature Machine Intelligence, 2024, discusses risks and ethical concerns of empathic AI implementations"
    },
    {
      "citation_id": "20",
      "title": "Towards emotional support dialog systems",
      "authors": [
        "S Liu",
        "C Zheng",
        "O Demasi",
        "S Sabour",
        "Y Li",
        "Z Yu",
        "Y Jiang",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Stickerconv: Generating multimodal empathetic responses from scratch",
      "authors": [
        "Y Zhang",
        "F Kong",
        "P Wang",
        "S Sun",
        "S Swangling",
        "S Feng",
        "D Wang",
        "Y Zhang",
        "K Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "A large-scale dataset for empathetic response generation",
      "authors": [
        "A Welivita",
        "Y Xie",
        "P Pu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Emotionx-ar: Affective empathetic response generation with emotion regulation",
      "authors": [
        "Z Wang",
        "Y Liu",
        "T Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of EMNLP 2023"
    },
    {
      "citation_id": "24",
      "title": "A computational framework for understanding empathy in conversational ai",
      "authors": [
        "S Sabour",
        "C Zheng",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of ACL 2022"
    },
    {
      "citation_id": "25",
      "title": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements",
      "authors": [
        "Y Qian",
        "W Zhang",
        "T Liu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "26",
      "title": "Multi-dimensional evaluation of empathetic dialogue responses",
      "authors": [
        "Z Xu",
        "J Jiang"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "27",
      "title": "Computational approaches to empathy: A survey",
      "authors": [
        "A Sharma",
        "A Miner",
        "D Atkins",
        "T Althoff"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "28",
      "title": "A multi-modal open dataset for mentaldisorder analysis",
      "authors": [
        "H Cai",
        "Z Yuan",
        "Y Gao"
      ],
      "year": "2022",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "29",
      "title": "An empirical study of clinical note generation from doctor-patient encounters",
      "authors": [
        "A Ben Abacha",
        "W -W. Yim",
        "Y Fan",
        "T Lin"
      ],
      "year": "2023",
      "venue": "Proceedings of EACL"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition: A survey of methods, datasets, and challenges",
      "authors": [
        "T Zhang",
        "S Sclaroff",
        "M Betke"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "A multimodal investigation of emotional responding in alexithymia",
      "authors": [
        "R Bagby",
        "G Taylor"
      ],
      "year": "2004",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "33",
      "title": "Identifying complex emotions in alexithymia affected adolescents using machine learning techniques",
      "authors": [
        "S Gannouni"
      ],
      "year": "2022",
      "venue": "Diagnostics"
    },
    {
      "citation_id": "34",
      "title": "Contextual emotion recognition using large vision language models",
      "authors": [
        "Y Etesam"
      ],
      "year": "2024",
      "venue": "Contextual emotion recognition using large vision language models",
      "arxiv": "arXiv:2405.08992"
    },
    {
      "citation_id": "35",
      "title": "Leveraging vision transformers and entropy-based attention for accurate micro-expression recognition",
      "authors": [
        "Z Wang"
      ],
      "year": "2025",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "36",
      "title": "Multimodal emotion recognition: A comprehensive review, trends, and challenges",
      "authors": [
        "M Ramaswamy",
        "S Palaniswamy"
      ],
      "year": "2024",
      "venue": "WIREs Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "37",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "38",
      "title": "Multimodal emotion recognition in social media: Integrating visual and textual cues with vision-language models",
      "authors": [
        "A Gandhi",
        "R Sharma",
        "N Patel",
        "S Kumar"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Vision-language models for mental health assessment: Analyzing visual and textual indicators of depression and anxiety",
      "authors": [
        "S Yoon",
        "J Kim",
        "S Lee",
        "C Park"
      ],
      "year": "2023",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "40",
      "title": "Enhancing healthcare communication assessment with vision-language models: Applications in patient-provider interactions",
      "authors": [
        "L Chen",
        "J Williams",
        "C Martinez",
        "D Thompson"
      ],
      "year": "2023",
      "venue": "Digital Medicine"
    },
    {
      "citation_id": "41",
      "title": "Towards understanding and mitigating social biases in language models for conversation ai",
      "authors": [
        "A Sharma",
        "I Lin",
        "A Miner",
        "D Atkins",
        "T Althoff"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Multi-dimensional evaluation of empathetic dialogue responses",
      "authors": [
        "Z Xu",
        "J Jiang"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
      "citation_id": "43",
      "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "authors": [
        "C.-W Liu",
        "R Lowe",
        "I Serban",
        "M Noseworthy",
        "L Charlin",
        "J Pineau"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Psychological metrics for dialog system evaluation",
      "authors": [
        "S Giorgi",
        "J Sedoc",
        "L Ungar",
        "S Buechel",
        "A Buffone",
        "H Schwartz",
        "S Dill",
        "A Ramakrishna",
        "V Ganesan"
      ],
      "year": "2023",
      "venue": "Psychological metrics for dialog system evaluation",
      "arxiv": "arXiv:2305.14757"
    },
    {
      "citation_id": "45",
      "title": "Perceived empathy of technology scale (pets): Measuring empathy of systems toward the user",
      "year": "2024",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "46",
      "title": "Third-party evaluators perceive ai as more compassionate than expert humans",
      "authors": [
        "D Ovsyannikova",
        "V De Mello",
        "M Inzlicht"
      ],
      "year": "2025",
      "venue": "Communications Psychology"
    },
    {
      "citation_id": "47",
      "title": "Algorithms for empathy: Using machine learning to categorize common empathetic traits across professional and peer-based conversations",
      "authors": [
        "S Provence",
        "A Forcehimes"
      ],
      "year": "2024",
      "venue": "PMC"
    },
    {
      "citation_id": "48",
      "title": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): a randomized controlled trial",
      "authors": [
        "K Fitzpatrick",
        "A Darcy",
        "M Vierhile"
      ],
      "year": "2017",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "49",
      "title": "Mental health surveillance over social media with digital cohorts",
      "authors": [
        "S Chancellor",
        "Z Lin",
        "E Goodman",
        "S Zerwas",
        "M Choudhury"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "50",
      "title": "Empathy toward artificial intelligence versus human experiences and the role of transparency in mental health and social support chatbot design: Comparative study",
      "authors": [
        "J Shen",
        "D Dipaola",
        "S Ali",
        "M Sap",
        "H Park",
        "C Breazeal"
      ],
      "year": "2024",
      "venue": "JMIR Mental Health"
    },
    {
      "citation_id": "51",
      "title": "The role of empathy in psychoanalytic psychotherapy: A historical exploration",
      "authors": [
        "G Kaluzeviciute"
      ],
      "year": "2020",
      "venue": "Cogent Psychology"
    },
    {
      "citation_id": "52",
      "title": "Evidence based relationships 4: Empathy, congruence, unconditional positive regard, and real relationship",
      "authors": [
        "D Mahon"
      ],
      "year": "2023",
      "venue": "Evidence based relationships 4: Empathy, congruence, unconditional positive regard, and real relationship"
    },
    {
      "citation_id": "53",
      "title": "Silent signals: New review highlights the importance of nonverbal signals for perceived responsiveness",
      "authors": [
        "R Ramadurai"
      ],
      "year": "2024",
      "venue": "Silent signals: New review highlights the importance of nonverbal signals for perceived responsiveness"
    },
    {
      "citation_id": "54",
      "title": "Emotion in psychotherapy: Affect, cognition, and the process of change",
      "authors": [
        "L Greenberg",
        "J Safran"
      ],
      "year": "1987",
      "venue": "Psychotherapy"
    },
    {
      "citation_id": "55",
      "title": "Empathic accuracy",
      "authors": [
        "W Ickes"
      ],
      "year": "1993",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "56",
      "title": "The generalizability of the psychoanalytic concept of the working alliance",
      "authors": [
        "E Bordin"
      ],
      "year": "1979",
      "venue": "Psychotherapy: Theory, research & practice"
    },
    {
      "citation_id": "57",
      "title": "Alliance in individual psychotherapy",
      "authors": [
        "A Horvath",
        "A Del Re",
        "C Flückiger",
        "D Symonds"
      ],
      "year": "2011",
      "venue": "Psychotherapy"
    },
    {
      "citation_id": "58",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "59",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "60",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "61",
      "title": "What matters when building vision-language models?",
      "authors": [
        "H Laurenc ¸on",
        "L Saulnier",
        "L Tronchon",
        "V Saulnier",
        "C Akiki",
        "A Villegas",
        "M Haddad",
        "L Barrault",
        "X Bresson",
        "A Aji"
      ],
      "year": "2024",
      "venue": "What matters when building vision-language models?",
      "arxiv": "arXiv:2405.02246"
    },
    {
      "citation_id": "62",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "B Lin",
        "B Zhu",
        "Y Ye",
        "M Ning",
        "P Jin",
        "L Yuan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "63",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "64",
      "title": "Mm-llms: Recent advances in multimodal large language models",
      "authors": [
        "D Zhang",
        "Y Yu",
        "J Dong",
        "C Li",
        "D Su",
        "C Chu",
        "D Yu"
      ],
      "year": "2024",
      "venue": "Mm-llms: Recent advances in multimodal large language models"
    },
    {
      "citation_id": "65",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "66",
      "title": "Psychotherapy",
      "authors": [
        "R Elliott",
        "A Bohart",
        "J Watson",
        "L Greenberg",
        "\" Empathy"
      ],
      "year": "2011",
      "venue": "Psychotherapy"
    },
    {
      "citation_id": "67",
      "title": "Level of emotional awareness and mean length of utterance",
      "authors": [
        "J Hall",
        "S Carter",
        "M Jimenez",
        "N Frost"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "68",
      "title": "Cognitive load during problem solving: Effects on learning",
      "authors": [
        "J Sweller"
      ],
      "year": "1988",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "69",
      "title": "Language features for automated evaluation of cognitive behavior psychotherapy sessions",
      "authors": [
        "N Flemotomos",
        "V Martinez",
        "J Gibson",
        "D Atkins",
        "T Creed",
        "S Narayanan"
      ],
      "year": "2018",
      "venue": "Language features for automated evaluation of cognitive behavior psychotherapy sessions"
    },
    {
      "citation_id": "70",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "T Zhang",
        "V Kishore",
        "F Wu",
        "K Weinberger",
        "Y Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "71",
      "title": "The empathy cycle: Refinement of a nuclear concept",
      "authors": [
        "G Barrett-Lennard"
      ],
      "year": "1981",
      "venue": "Journal of Counseling Psychology"
    },
    {
      "citation_id": "72",
      "title": "Artificial empathy in healthcare chatbots: Does it feel authentic?",
      "authors": [
        "J Luo",
        "J Huang",
        "H Li"
      ],
      "year": "2024",
      "venue": "Computers in Human Behavior Reports"
    },
    {
      "citation_id": "73",
      "title": "The skilled helper: A problem-management and opportunitydevelopment approach to helping",
      "authors": [
        "G Egan"
      ],
      "year": "2014",
      "venue": "The skilled helper: A problem-management and opportunitydevelopment approach to helping"
    },
    {
      "citation_id": "74",
      "title": "Toward effective counseling and psychotherapy: Training and practice",
      "authors": [
        "C Truax",
        "R Carkhuff"
      ],
      "year": "1967",
      "venue": "Toward effective counseling and psychotherapy: Training and practice"
    },
    {
      "citation_id": "75",
      "title": "The jefferson scale of empathy: development and preliminary psychometric data",
      "authors": [
        "M Hojat",
        "S Mangione",
        "T Nasca",
        "M Cohen",
        "J Gonnella",
        "J Erdmann",
        "J Veloski",
        "M Magee"
      ],
      "year": "2001",
      "venue": "Educational and Psychological Measurement"
    },
    {
      "citation_id": "76",
      "title": "The development and preliminary validation of the consultation and relational empathy (care) scale for use in primary care",
      "authors": [
        "S Mercer",
        "W Reynolds"
      ],
      "year": "2004",
      "venue": "Family Practice"
    }
  ]
}