{
  "paper_id": "2509.01401v1",
  "title": "Arabemonet: A Lightweight Hybrid 2D Cnn-Bilstm Model With Attention For Robust Arabic Speech Emotion Recognition",
  "published": "2025-09-01T11:51:38Z",
  "authors": [
    "Ali Abouzeid",
    "Bilal Elbouardi",
    "Mohamed Maged",
    "Shady Shehata"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is vital for humancomputer interaction, particularly for lowresource languages like Arabic, which face challenges due to limited data and research. We introduce ArabEmoNet, a lightweight architecture designed to overcome these limitations and deliver state-of-the-art performance. Unlike previous systems relying on discrete MFCC features and 1D convolutions, which miss nuanced spectro-temporal patterns, ArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving critical emotional cues often lost in traditional methods. While recent models favor large-scale architectures with millions of parameters, ArabEmoNet achieves superior results with just 1 million parameters, which is 90 times smaller than HuBERT base and 74 times smaller than Whisper. This efficiency makes it ideal for resource-constrained environments. ArabEmoNet advances Arabic speech emotion recognition, offering exceptional performance and accessibility for real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is essential for improving human-computer interaction, particularly in linguistically diverse contexts like Arabic speech. The complexity of detecting emotions from speech arises from variations in prosody, phonetics, and speaker expression. Over time, SER has evolved from statistical approaches to deep learning, significantly enhancing recognition accuracy.\n\nEarly SER systems relied on handcrafted acoustic features (e.g., pitch, energy, and MFCCs) processed using classical machine learning models like Support Vector Machines (SVMs) and Gaussian Mixture Models (GMMs)  (Lieskovska et al., 2021) . While effective, these methods struggled with cross-dataset generalization, particularly in Arabic * Equal contribution speech, which exhibits rich phonetic and prosodic diversity. Deep learning mitigated these limitations by enabling automatic feature extraction, with CNNs capturing localized spectro-temporal patterns and LSTMs modeling sequential dependencies  (Fayek et al., 2017) . However, many Arabic SER systems still rely on MFCCs and 1D convolutions, which fail to capture essential spectraltemporal structures for robust emotion recognition.\n\nTransformer-based models  (Vaswani et al., 2017)  introduced attention mechanisms to dynamically focus on emotionally salient speech segments  (Mirsamadi et al., 2017) . While effective in modeling long-range dependencies and parallelizing computations across emotional speech sequences, their high computational complexity (O(n 2 ) for selfattention) and substantial memory requirements render them impractical for resource-constrained environments. To address these constraints, we propose ArabEmoNet, a lightweight architecture leveraging Mel spectrograms with 2D convolutions, effectively capturing both fine-grained spectral features and global contextual relationships  (Kurpukdee et al., 2017) .\n\nOur model achieves competitive accuracy with just 0.97M parameters, making it significantly more efficient than HuBERT  (Hsu et al., 2021)  and Whisper  (Radford et al., 2022)  while maintaining state-of-the-art performance. Additionally, we augmented the data by integrating SpecAugment  (Park et al., 2019)  and Additive White Gaussian Noise (AWGN), which enhances the robustness of our model  (Huh et al., 2024) .\n\nExperiments on  KSUEmotions (Meftah et al., 2021)  and KEDAS  (Belhadj et al., 2022)  datasets confirm that ArabEmoNet surpasses prior architectures while maintaining efficiency, marking a significant step forward in Arabic SER.\n\nThe main contributions of this paper can be summarized as follows:   (Sainath et al., 2015)  and  (Trigeorgis et al., 2016) . End-to-end frameworks combining CNNs and RNNs have further improved performance by learning from raw or minimally processed data  (Fayek et al., 2017) .\n\nAttention mechanisms, introduced in  (Bahdanau et al., 2015)  and further refined in  (Vaswani et al., 2017) , have recently shown promise in SER. They allow models to focus on the most relevant parts of the input, which is particularly useful for tasks such as emotion recognition, where only a subset of characteristics may contribute to emotional cues  (Mirsamadi et al., 2017)  and  (Neumann and Vu, 2017) .\n\nPrevious work in  (Hifny and Ali, 2019)  proposed an efficient Arabic Speech Emotion Recognition (SER) system, combining Convolutional Neu-ral Networks (CNNs), Bidirectional LSTMs (BiL-STMs), and attention mechanisms, which achieved state-of-the-art performance on the KSUEmotions dataset. However, their approach relied on 13-feature Mel Frequency Cepstral Coefficients (MFCCs) and 1D convolutions, which may restrict the richness of captured acoustic features. Building on the insights from their work and the demonstrated importance of leveraging richer representations by  (Meng et al., 2019)  and  (Lieskovska et al., 2021) , we diverge from  (Hifny and Ali, 2019)  by utilizing Log-Mel spectrograms as input, which offer a more comprehensive acoustic feature representation, and employs 2D convolutions to effectively capture spatial dependencies within these spectrograms Furthermore, advances in data augmentation techniques, such as SpecAugment  (Park et al., 2019)  and Gaussian noise, have addressed data scarcity challenges, enhancing the generalization capabilities of SER systems. Studies have also investigated the impact of kernel sizes in CNN layers and regularization techniques to optimize performance  (Kumar and Raj, 2024) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this work, we introduce ArabEmoNet, a dedicated 2D NN-Attention and BiLSTM framework optimized for Arabic Speech Emotion Recognition. Our model processes Log-Mel spectrograms to effectively capture the multifaceted nature of emotional speech through three complementary components: 2D convolutional layers that identify emotion-specific spectral patterns, bidirectional LSTMs that model the temporal evolution of emotional cues, and an attention mechanism that highlights emotionally salient segments within utterances. This integrated approach addresses the unique challenges of recognizing Arabic emotional expressions while maintaining a lightweight, efficient architecture. Figure  1  illustrates our complete model design.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input Prepossessing",
      "text": "For our classification model, raw audio signals are transformed into Log-Mel spectrograms. This process involves computing the Mel spectrogram using a Fast Fourier Transform (FFT) window length of 2048 samples and a hop length of 256 samples. We generate 128 Mel bands across a frequency range from 80 Hz to 7600 Hz . A Hann window is applied to each frame to minimize spectral leakage. Subsequently, the resulting Mel spectrogram is converted to a logarithmic scale (decibels), referenced to the maximum power, to optimize the dynamic range for neural network processing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "To improve the generalization ability of the model and mitigate overfitting, we incorporate Gaussian noise augmentation during training. This technique simulates variations in the input data and leads to a more robust model. Optimization is performed using the Adam optimizer, which adapts learning rates for each parameter based on the first and second moments of the gradients. Additionally, we utilize batch normalization and early stopping based on validation loss to further stabilize the training process and prevent overfitting.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction Via Convolutional Layers",
      "text": "The initial stage of the model employs a series of convolutional layers to extract high-level representations from the input Mel spectrograms. These layers are responsible for detecting local timefrequency patterns that are crucial for emotion discrimination. Mathematically, the feature maps F l at layer l are computed as:\n\nwhere F l-1 represents the input to the current layer (with the initial input being the spectrogram S), W l and b l denote the learnable weights and biases, respectively, p l is the specified padding, and σ is the ReLU activation function. It is important to note that we employ 2D CNNs rather than 1D CNNs because Mel spectrograms provide a two-dimensional (time-frequency) representation. This allows the model to capture both temporal and spectral dependencies more effectively. The use of multiple convolutional layers, combined with max-pooling and dropout, enhances the network's ability to learn robust, hierarchical feature representations while mitigating overfitting. Following the convolutional layers, the extracted features are passed through a fully connected layer before being passed to the next stage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Modeling With Bidirectional Lstm",
      "text": "After the convolutional layers, the network integrates a Bidirectional LSTM to model the temporal structure and contextual dependencies across time frames. By processing the sequential output in both forward and backward directions, the BiLSTM effectively captures transitions between emotional states, ensuring a more nuanced understanding of temporal variations in speech. The hidden state at time step t is given by:\n\nwhere -→ h t and ←h t denote the forward and backward hidden states, respectively. This bidirectional processing is particularly important for SER tasks, as emotions in speech often evolve gradually rather than appearing in isolation. Capturing the transitions between emotional states allows the model to account for contextual cues, such as shifts in pitch, intensity, and rhythm, which are crucial for accurately interpreting emotional expressions over time.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attention Mechanism",
      "text": "To enhance the model's ability to distinguish subtle variations in emotional expressions, an attention mechanism is integrated atop the BiLSTM outputs. This mechanism computes a context vector c that selectively aggregates the BiLSTM hidden states, assigning higher importance to frames that carry more salient emotional cues, thereby improving emotion classification. The context vector is defined as:\n\n,\n\nwhere the attention score e t is computed as:\n\nHere, w e and b e are learnable parameters that transform the hidden states into a scalar score, and the softmax function normalizes these scores into a probability distribution over time steps. By dynamically focusing on the most emotionally informative segments of the speech signal, this mechanism enhances the model's ability to capture key variations in tone, prosody, and intensity that define different emotional states, making it more effective for Speech Emotion Recognition (SER).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Layer:",
      "text": "Finally, the context vector is passed through one fully connected layer, culminating in an output layer that produces the logits corresponding to the target emotion classes:\n\nThe logits are then typically passed through a softmax function during training to compute the crossentropy loss for classification. The entire architecture is illustrated in Figure  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup 4.1 Training Platform",
      "text": "Training was done on a single Nvidia RTX 4090 GPU with 24 GB of memory. The training process utilized the Adam optimizer with an initial learning rate of 1 × 10 -4 and a weight decay of 1 × 10 -5 . An adaptive learning rate scheduler that reduces the learning rate when a metric's improvement plateaus was incorporated to adjust the learning rate during training, and the Adam optimizer was included.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "For our baseline models, we used Whisper-base, Whisper-small, and HuBERT-base speech encoders due to their vast popularity in the speech domain.\n\nWe applied two identical feed-forward sublayers, each comprising a fully connected layer followed by a ReLU activation function and a dropout layer. This feed-forward block is repeated twice. After the feed-forward modules, the output is passed to a final classification layer that maps the learned features to the desired output classes. We trained the models using Adam optimizer with learning rate 1 × 10 -3 and dropout 0.5. In addition to these general speech encoders, we also compared ArabEmoNet against several dataset-specific baseline models from the literature:\n\n• For the KSUEmotion dataset, we compared against the ResNet-based Architecture  (Meftah et al., 2021)  and the CNN-BLSTM-DNN Model  (Hifny and Ali, 2019) .\n\n• For the KEDAS dataset, baseline  (Belhadj et al., 2022)  reported in the original dataset paper.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "In this work, we utilized two Arabic emotional speech datasets: the KSUEmotions corpus and KEDAS, both designed to advance speech emotion recognition (SER) research in Arabic, addressing the scarcity of non-English SER resources. We sampled both datasets at their native frequencies: 16kHz for KSUEmotions and 48kHz for KEDAS.\n\nTo handle varying sequence lengths in the dataset, shorter sequences within a batch were padded with zeros to match the longest sequence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ksuemotions Dataset",
      "text": "The KSUEmotions corpus  (Meftah et al., 2021)  provides recordings from 23 native Arabic speakers (10 males, 13 females) representing diverse dialectal backgrounds from Yemen, Saudi Arabia, and Syria. The corpus was collected in two phases:\n\n1) Phase 1: Included 20 speakers (10 males, 10 females) recording five emotions: neutral, sadness, happiness, surprise, and questioning, totaling 2 hours and 55 minutes of high-quality audio recorded in controlled environments.\n\n2) Phase 2: Featured 14 speakers (7 males and 4 females from Phase 1, plus 3 new Yemeni females), replacing the questioning emotion with anger, contributing an additional 2 hours and 15 minutes of recordings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Kedas Dataset",
      "text": "The KEDAS dataset  (Belhadj et al., 2022)  comprises 5000 audio recording files in standard Arabic, featuring five emotional states: anger, happiness, sadness, fear, and neutrality. The recordings were collected from 500 actors within the university community, including students, professors, and staff. The dataset is based on 10 carefully selected phrases commonly used in communication, chosen through literary and scientific studies. The data collection and validation process involved 55 evaluators, including Arabic linguists, literary researchers, and clinical psychology specialists, ensuring highquality emotional content and linguistic accuracy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation",
      "text": "To evaluate our classification model's performance, we used two key metrics: Macro F1-score and Micro F1-score. Since no specific train-test split was provided for the datasets, we follow  (Hifny and Ali, 2019)  and report the average of a 5-fold crossvalidation with stratified splits on both datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Macro F1-Score",
      "text": "The macro F1-score  (Sokolova et al., 2009)  calculates the unweighted mean of F1-scores for each class. It treats all classes equally, regardless of their size, making it suitable for imbalanced datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Micro F1-Score",
      "text": "The micro F1-score  (Sokolova et al., 2009)  aggregates the contributions of all classes to compute the average metric. Instead of treating all classes equally, it is weighted by the number of instances in each class, making it more suitable for balanced datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "The results presented in Table  2  demonstrate the effectiveness and efficiency of the ArabEmoNet architecture for Arabic speech emotion recognition across two distinct datasets: KSUEmotion and KEDAS.\n\nOn the KSUEmotion dataset, ArabEmoNet achieves an accuracy of 91.48%, which represents state-of-the-art performance. This significantly outperforms previously established benchmarks for this dataset, including the CNN-BLSTM-DNN model  (Hifny and Ali, 2019)  and the ResNet-based architecture  (Meftah et al., 2021) . Furthermore, ArabEmoNet also surpasses the performance of larger, pre-trained models such as HuBERT-base  (Hsu et al., 2021)  and Whisper-small  (Radford et al., 2022) , despite its significantly smaller parameter count.\n\nSimilarly, on the KEDAS dataset, our model achieves an exceptional accuracy of 99.46%. This result substantially surpasses the original Baseline Model  (Belhadj et al., 2022)  and demonstrates competitive performance even when compared to highly resource-intensive pre-trained models like Whisper-small  (Radford et al., 2022)  and HuBERTbase  (Hsu et al., 2021) . Notably, ArabEmoNet achieves these superior or competitive results with significantly fewer parameters (0.97M) compared to pretrained models such as HuBERT-base (95M) and Whisper-small (74M).\n\n6 Discussion and Analysis",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cnn Kernel Size",
      "text": "Table  3  shows the impact of kernel size on ArabE-moNet's performance for the KSUEmotion Dataset. As the kernel size increases from 3 to 7, the model's accuracy steadily improves, peaking at 91.48% with a kernel size of 7 and a corresponding padding of 3. Beyond this point, increasing the kernel size further (to 9 and 11) leads to a decline in accuracy. Larger kernels, while increasing the receptive field, may introduce too much noise or become less adept at capturing fine-grained details, leading to a dip in accuracy. Conversely, smaller kernels might not encompass enough contextual information to achieve optimal recognition. Therefore, the kernel size of 7 represents the best trade-off between performance and model complexity in this experimental setup.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces ArabEmoNet, a lightweight yet highly effective architecture for Arabic Speech Emotion Recognition. By integrating 2D CNN layers, BiLSTM networks, and an attention mechanism with Mel spectrogram inputs, ArabEmoNet significantly advances the state-of-the-art, achieving a remarkable 4% improvement over existing models on the KSUEmotions dataset. Our results demonstrate that 2D convolutions substantially outperform traditional approaches using 1D convolutions and MFCC features, capturing richer and more nuanced acoustic patterns essential for emotion classification. Furthermore, employing Gaussian noise augmentation successfully enhanced the model's robustness and addressed data imbalance issues, underscoring the importance of effective augmentation strategies. Comparative experiments revealed that transformer-based architectures, while powerful in other contexts, were less effective for this task, highlighting the particular suitability of BiL-STM layers in capturing temporal emotional dynamics.\n\nIn future work, we aim to extend ArabEmoNet's training to larger, multilingual datasets, validating its applicability and generalizability across diverse linguistic and cultural contexts. This expansion promises significant contributions toward more inclusive and effective global emotion recognition systems.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "A potential limitation to our architecture arises from the method used to handle variable audio lengths. To standardize the input size for model processing, the architecture employs zero-padding. Specifically, shorter audio sequences within any given batch are padded with zeros to equal the length of the longest sequence in that same batch. While this is a standard technique, it can introduce a limitation if there is significant variance in the duration of audio clips within a batch. In such cases, shorter clips will be appended with a large amount of non-informative zero values, which can lead to unnecessary computational processing and potentially impact the model's learning efficiency",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates our complete",
      "page": 2
    },
    {
      "caption": "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "diversity. Deep learning mitigated these limita-"
        },
        {
          "Abstract": "Speech emotion recognition is vital for human-",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "tions by enabling automatic feature extraction, with"
        },
        {
          "Abstract": "computer\ninteraction,\nparticularly\nfor\nlow-",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "CNNs capturing localized spectro-temporal pat-"
        },
        {
          "Abstract": "resource languages\nlike Arabic, which face",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "terns and LSTMs modeling sequential dependen-"
        },
        {
          "Abstract": "challenges due to limited data and research. We",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "cies (Fayek et al., 2017). However, many Arabic"
        },
        {
          "Abstract": "introduce ArabEmoNet, a lightweight architec-",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "ture designed to overcome these limitations and",
          "speech, which exhibits rich phonetic and prosodic": "SER systems still\nrely on MFCCs and 1D con-"
        },
        {
          "Abstract": "deliver state-of-the-art performance. Unlike",
          "speech, which exhibits rich phonetic and prosodic": "volutions, which fail to capture essential spectral-"
        },
        {
          "Abstract": "previous systems relying on discrete MFCC fea-",
          "speech, which exhibits rich phonetic and prosodic": "temporal structures for robust emotion recognition."
        },
        {
          "Abstract": "tures and 1D convolutions, which miss nuanced",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "Transformer-based models (Vaswani et al., 2017)"
        },
        {
          "Abstract": "spectro-temporal patterns, ArabEmoNet uses",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "introduced attention mechanisms to dynamically"
        },
        {
          "Abstract": "Mel spectrograms processed through 2D convo-",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "focus on emotionally salient speech segments (Mir-"
        },
        {
          "Abstract": "lutions, preserving critical emotional cues often",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "lost in traditional methods. While recent mod-",
          "speech, which exhibits rich phonetic and prosodic": "samadi et al., 2017). While effective in modeling"
        },
        {
          "Abstract": "els favor large-scale architectures with millions",
          "speech, which exhibits rich phonetic and prosodic": "long-range dependencies and parallelizing compu-"
        },
        {
          "Abstract": "of parameters, ArabEmoNet achieves superior",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "tations across emotional speech sequences,\ntheir"
        },
        {
          "Abstract": "results with just 1 million parameters, which",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "high computational complexity (O(n2)\nfor\nself-"
        },
        {
          "Abstract": "is 90 times smaller than HuBERT base and 74",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "attention) and substantial memory requirements"
        },
        {
          "Abstract": "times smaller\nthan Whisper.\nThis efficiency",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "render them impractical for resource-constrained"
        },
        {
          "Abstract": "makes it ideal for resource-constrained environ-",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "environments.\nTo address these constraints, we"
        },
        {
          "Abstract": "ments. ArabEmoNet advances Arabic speech",
          "speech, which exhibits rich phonetic and prosodic": ""
        },
        {
          "Abstract": "emotion recognition, offering exceptional per-",
          "speech, which exhibits rich phonetic and prosodic": "propose ArabEmoNet, a lightweight architecture"
        },
        {
          "Abstract": "formance and accessibility for real-world appli-",
          "speech, which exhibits rich phonetic and prosodic": "leveraging Mel\nspectrograms with 2D convolu-"
        },
        {
          "Abstract": "cations.",
          "speech, which exhibits rich phonetic and prosodic": "tions, effectively capturing both fine-grained spec-"
        },
        {
          "Abstract": "",
          "speech, which exhibits rich phonetic and prosodic": "tral\nfeatures and global contextual\nrelationships"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "formance and accessibility for real-world appli-": "cations.",
          "leveraging Mel\nspectrograms with 2D convolu-": "tions, effectively capturing both fine-grained spec-"
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "tral\nfeatures and global contextual\nrelationships"
        },
        {
          "formance and accessibility for real-world appli-": "1\nIntroduction",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "(Kurpukdee et al., 2017)."
        },
        {
          "formance and accessibility for real-world appli-": "Speech Emotion Recognition (SER)\nis essential",
          "leveraging Mel\nspectrograms with 2D convolu-": "Our model achieves competitive accuracy with"
        },
        {
          "formance and accessibility for real-world appli-": "for improving human-computer interaction, partic-",
          "leveraging Mel\nspectrograms with 2D convolu-": "just 0.97M parameters, making it\nsignificantly"
        },
        {
          "formance and accessibility for real-world appli-": "ularly in linguistically diverse contexts like Arabic",
          "leveraging Mel\nspectrograms with 2D convolu-": "more efficient\nthan HuBERT (Hsu et al., 2021)"
        },
        {
          "formance and accessibility for real-world appli-": "speech. The complexity of detecting emotions from",
          "leveraging Mel\nspectrograms with 2D convolu-": "and Whisper (Radford et al., 2022) while maintain-"
        },
        {
          "formance and accessibility for real-world appli-": "speech arises from variations in prosody, phonet-",
          "leveraging Mel\nspectrograms with 2D convolu-": "ing state-of-the-art performance. Additionally, we"
        },
        {
          "formance and accessibility for real-world appli-": "ics, and speaker expression. Over time, SER has",
          "leveraging Mel\nspectrograms with 2D convolu-": "augmented the data by integrating SpecAugment"
        },
        {
          "formance and accessibility for real-world appli-": "evolved from statistical approaches to deep learn-",
          "leveraging Mel\nspectrograms with 2D convolu-": "(Park et al., 2019) and Additive White Gaussian"
        },
        {
          "formance and accessibility for real-world appli-": "ing, significantly enhancing recognition accuracy.",
          "leveraging Mel\nspectrograms with 2D convolu-": "Noise (AWGN), which enhances the robustness of"
        },
        {
          "formance and accessibility for real-world appli-": "Early SER systems relied on handcrafted acous-",
          "leveraging Mel\nspectrograms with 2D convolu-": "our model (Huh et al., 2024)."
        },
        {
          "formance and accessibility for real-world appli-": "tic features (e.g., pitch, energy, and MFCCs) pro-",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "Experiments on KSUEmotions (Meftah et al.,"
        },
        {
          "formance and accessibility for real-world appli-": "cessed using classical machine learning models like",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "2021) and KEDAS (Belhadj et al., 2022) datasets"
        },
        {
          "formance and accessibility for real-world appli-": "Support Vector Machines (SVMs) and Gaussian",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "confirm that ArabEmoNet surpasses prior archi-"
        },
        {
          "formance and accessibility for real-world appli-": "Mixture Models (GMMs) (Lieskovska et al., 2021).",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "tectures while maintaining efficiency, marking a"
        },
        {
          "formance and accessibility for real-world appli-": "While\neffective,\nthese methods\nstruggled with",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "significant step forward in Arabic SER."
        },
        {
          "formance and accessibility for real-world appli-": "cross-dataset generalization, particularly in Arabic",
          "leveraging Mel\nspectrograms with 2D convolu-": ""
        },
        {
          "formance and accessibility for real-world appli-": "",
          "leveraging Mel\nspectrograms with 2D convolu-": "The main contributions of this paper can be sum-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "spectrograms"
        },
        {
          "fectively capture spatial dependencies within these": "Furthermore,\nadvances\nin data\naugmentation"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "techniques,\nsuch as SpecAugment\n(Park et\nal.,"
        },
        {
          "fectively capture spatial dependencies within these": "2019) and Gaussian noise, have addressed data"
        },
        {
          "fectively capture spatial dependencies within these": "scarcity challenges, enhancing the generalization"
        },
        {
          "fectively capture spatial dependencies within these": "capabilities of SER systems. Studies have also in-"
        },
        {
          "fectively capture spatial dependencies within these": "vestigated the impact of kernel sizes in CNN layers"
        },
        {
          "fectively capture spatial dependencies within these": "and regularization techniques to optimize perfor-"
        },
        {
          "fectively capture spatial dependencies within these": "mance (Kumar and Raj, 2024)."
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "3\nProposed Approach"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "In this work, we introduce ArabEmoNet, a dedi-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "cated 2D NN-Attention and BiLSTM framework"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "optimized for Arabic Speech Emotion Recogni-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "tion. Our model processes Log-Mel spectrograms"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "to effectively capture the multifaceted nature of"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "emotional\nspeech through three complementary"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "components: 2D convolutional layers that identify"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "emotion-specific spectral patterns, bidirectional"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "LSTMs that model the temporal evolution of emo-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "tional cues, and an attention mechanism that high-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "lights emotionally salient segments within utter-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "ances.\nThis\nintegrated approach addresses\nthe"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "unique challenges of recognizing Arabic emotional"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "expressions while maintaining a lightweight, effi-"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "cient architecture. Figure 1 illustrates our complete"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "model design."
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "3.1\nInput Prepossessing"
        },
        {
          "fectively capture spatial dependencies within these": ""
        },
        {
          "fectively capture spatial dependencies within these": "For our classification model, raw audio signals are"
        },
        {
          "fectively capture spatial dependencies within these": "transformed into Log-Mel spectrograms. This pro-"
        },
        {
          "fectively capture spatial dependencies within these": "cess involves computing the Mel spectrogram using"
        },
        {
          "fectively capture spatial dependencies within these": "a Fast Fourier Transform (FFT) window length of"
        },
        {
          "fectively capture spatial dependencies within these": "2048 samples and a hop length of 256 samples. We"
        },
        {
          "fectively capture spatial dependencies within these": "generate 128 Mel bands across a frequency range"
        },
        {
          "fectively capture spatial dependencies within these": "from 80 Hz to 7600 Hz . A Hann window is applied"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• We propose ArabEmoNet: a novel lightweight": "hybrid architecture combining 2D Convolu-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "STMs), and attention mechanisms, which achieved"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "tional Neural Networks (CNN) with Bidirec-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "state-of-the-art\nperformance\non\nthe KSUEmo-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "tional Long Short-Term Memory (BiLSTM)",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "tions dataset. However,\ntheir approach relied on"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "and an attention mechanism",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "13-feature Mel Frequency Cepstral Coefficients"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "(MFCCs) and 1D convolutions, which may restrict"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "• ArabEmoNet (1M parameters) achieves supe-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "the richness of captured acoustic features. Building"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "rior results with just 1 million parameters—90",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "on the insights from their work and the demon-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "times smaller than HuBERT base (95M pa-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "strated importance of leveraging richer representa-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "rameters) and 74 times smaller than Whisper",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "tions by (Meng et al., 2019) and (Lieskovska et al.,"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "(74M parameters).",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "2021), we diverge from (Hifny and Ali, 2019) by"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "utilizing Log-Mel spectrograms as input, which"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "• We demonstrate ArabEmoNet’s superior per-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "offer a more comprehensive acoustic feature rep-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "formance by achieving state-of-the-art results",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "resentation, and employs 2D convolutions to ef-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "on the KSUEmotion and KEDAS datasets,",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "fectively capture spatial dependencies within these"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "surpassing previous benchmark models.",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "spectrograms"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "Furthermore,\nadvances\nin data\naugmentation"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "2\nRelated Work",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "techniques,\nsuch as SpecAugment\n(Park et\nal.,"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "Speech Emotion Recognition (SER) has been an",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "2019) and Gaussian noise, have addressed data"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "active area of\nresearch for decades.\nTraditional",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "scarcity challenges, enhancing the generalization"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "approaches often relied on statistical evaluations",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "capabilities of SER systems. Studies have also in-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "of speech features such as pitch, energy, and spec-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "vestigated the impact of kernel sizes in CNN layers"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "tral coefficients, combined with classifiers such",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "and regularization techniques to optimize perfor-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "as SVMs or Hidden Markov Models (HMMs) (Il-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "mance (Kumar and Raj, 2024)."
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "iou and Anagnostopoulos, 2009) and (Nwe et al.,",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "2003).\nAlthough these methods provided foun-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "3\nProposed Approach"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "dational information, they struggled to generalize",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "In this work, we introduce ArabEmoNet, a dedi-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "across different datasets and languages.",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "cated 2D NN-Attention and BiLSTM framework"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "The advent of deep learning significantly ad-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "optimized for Arabic Speech Emotion Recogni-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "vanced SER by enabling the extraction of hierarchi-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "tion. Our model processes Log-Mel spectrograms"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "cal feature representations. Convolutional Neural",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "to effectively capture the multifaceted nature of"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "Networks (CNNs) demonstrated their ability to cap-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "emotional\nspeech through three complementary"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "ture local spectral features, while Recurrent Neu-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "components: 2D convolutional layers that identify"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "ral Networks (RNNs), including Long Short-Term",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "emotion-specific spectral patterns, bidirectional"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "Memory (LSTM) networks, excelled at modeling",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "LSTMs that model the temporal evolution of emo-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "temporal dependencies (Sainath et al., 2015) and",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "tional cues, and an attention mechanism that high-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "(Trigeorgis et al., 2016). End-to-end frameworks",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "lights emotionally salient segments within utter-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "combining CNNs and RNNs have further improved",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "ances.\nThis\nintegrated approach addresses\nthe"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "performance by learning from raw or minimally",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "unique challenges of recognizing Arabic emotional"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "processed data (Fayek et al., 2017).",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "expressions while maintaining a lightweight, effi-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "Attention mechanisms, introduced in (Bahdanau",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "cient architecture. Figure 1 illustrates our complete"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "et al., 2015) and further refined in (Vaswani et al.,",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "model design."
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "2017), have recently shown promise in SER. They",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "allow models to focus on the most relevant parts",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "3.1\nInput Prepossessing"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "of the input, which is particularly useful for tasks",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": ""
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "such as emotion recognition, where only a subset",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "For our classification model, raw audio signals are"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "of characteristics may contribute to emotional cues",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "transformed into Log-Mel spectrograms. This pro-"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "(Mirsamadi et al., 2017) and (Neumann and Vu,",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "cess involves computing the Mel spectrogram using"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "2017).",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "a Fast Fourier Transform (FFT) window length of"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "Previous work in (Hifny and Ali, 2019) pro-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "2048 samples and a hop length of 256 samples. We"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "posed an efficient Arabic Speech Emotion Recogni-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "generate 128 Mel bands across a frequency range"
        },
        {
          "• We propose ArabEmoNet: a novel lightweight": "tion (SER) system, combining Convolutional Neu-",
          "ral Networks (CNNs), Bidirectional LSTMs (BiL-": "from 80 Hz to 7600 Hz . A Hann window is applied"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "to each frame to minimize spectral leakage. Subse-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "quently, the resulting Mel spectrogram is converted"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "to a logarithmic scale (decibels), referenced to the"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "maximum power,\nto optimize the dynamic range"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "for neural network processing."
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "3.2\nData Augmentation"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "To improve the generalization ability of the model"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "and mitigate overfitting, we incorporate Gaussian"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "noise augmentation during training. This technique"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "simulates variations in the input data and leads to"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "a more robust model. Optimization is performed"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "using the Adam optimizer, which adapts learning"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "rates\nfor each parameter based on the first and"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "second moments of\nthe gradients. Additionally,"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "we utilize batch normalization and early stopping"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "based on validation loss\nto further\nstabilize the"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "training process and prevent overfitting."
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "3.3\nFeature Extraction via Convolutional"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "Layers"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "The initial stage of the model employs a series of"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "convolutional layers to extract high-level represen-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "tations from the input Mel spectrograms. These"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "layers\nare\nresponsible\nfor detecting local\ntime-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "frequency patterns that are crucial for emotion dis-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "crimination. Mathematically, the feature maps Fl"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "at layer l are computed as:"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "Fl = σ (Conv2D(Fl−1, Wl, padding = pl) + bl)"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": ""
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "where Fl−1 represents the input to the current layer"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "(with the initial input being the spectrogram S), Wl"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "and bl denote the learnable weights and biases, re-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "is the specified padding, and σ is the\nspectively, pl"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "ReLU activation function.\nIt is important to note"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "that we employ 2D CNNs rather than 1D CNNs be-"
        },
        {
          "Figure 1: ArabEmoNet:2D CNN-Attention and BiLSTM Model Architecture.": "cause Mel spectrograms provide a two-dimensional"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.5\nAttention Mechanism": "",
          "Component": "Convolutional Layers",
          "Configuration": "3 stages with filters: 32, 64, 128"
        },
        {
          "3.5\nAttention Mechanism": "To enhance the model’s ability to distinguish subtle",
          "Component": "",
          "Configuration": ""
        },
        {
          "3.5\nAttention Mechanism": "",
          "Component": "",
          "Configuration": "Kernel: 7 × 7, ReLU activation"
        },
        {
          "3.5\nAttention Mechanism": "variations in emotional expressions, an attention",
          "Component": "",
          "Configuration": "Max pooling: 2 × 2, dropout: 0.3"
        },
        {
          "3.5\nAttention Mechanism": "mechanism is integrated atop the BiLSTM outputs.",
          "Component": "Fully Connected",
          "Configuration": "Input: 128 × H ′; Output: 128"
        },
        {
          "3.5\nAttention Mechanism": "This mechanism computes a context vector c that",
          "Component": "",
          "Configuration": "ReLU activation; dropout: 0.3"
        },
        {
          "3.5\nAttention Mechanism": "selectively aggregates the BiLSTM hidden states,",
          "Component": "BiLSTM",
          "Configuration": "2 layers, 64 hidden units per direction"
        },
        {
          "3.5\nAttention Mechanism": "",
          "Component": "",
          "Configuration": "Dropout: 0.3"
        },
        {
          "3.5\nAttention Mechanism": "assigning higher importance to frames that carry",
          "Component": "",
          "Configuration": ""
        },
        {
          "3.5\nAttention Mechanism": "",
          "Component": "Attention",
          "Configuration": "Applied to 128-dim BiLSTM output"
        },
        {
          "3.5\nAttention Mechanism": "more salient emotional cues,\nthereby improving",
          "Component": "",
          "Configuration": ""
        },
        {
          "3.5\nAttention Mechanism": "",
          "Component": "Classification",
          "Configuration": "Units equal to number of emotion categories"
        },
        {
          "3.5\nAttention Mechanism": "emotion classification. The context vector is de-",
          "Component": "",
          "Configuration": ""
        },
        {
          "3.5\nAttention Mechanism": "fined as:",
          "Component": "",
          "Configuration": ""
        },
        {
          "3.5\nAttention Mechanism": "",
          "Component": "",
          "Configuration": "Table 1: Model Hyperparameter Configuration"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "Speech Emotion Recognition (SER)."
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "3.6"
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "Finally,"
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "fully connected layer, culminating in an output"
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "layer that produces the logits corresponding to the"
        },
        {
          "ent emotional states, making it more effective for": "target emotion classes:"
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": ""
        },
        {
          "ent emotional states, making it more effective for": "The logits are then typically passed through a soft-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: demonstrate the",
      "data": [
        {
          "(10 males, 13 females) representing diverse dialec-": "tal backgrounds from Yemen, Saudi Arabia, and",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "Syria. The corpus was collected in two phases:",
          "5\nResults": "The results presented in Table 2 demonstrate the"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "effectiveness and efficiency of\nthe ArabEmoNet"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "1) Phase 1: Included 20 speakers (10 males, 10",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "architecture for Arabic speech emotion recogni-"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "females) recording five emotions: neutral, sad-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "tion across two distinct datasets: KSUEmotion and"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "ness, happiness, surprise, and questioning, to-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "KEDAS."
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "taling 2 hours and 55 minutes of high-quality",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "On\nthe KSUEmotion\ndataset, ArabEmoNet"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "audio recorded in controlled environments.",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "achieves an accuracy of 91.48%, which represents"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "state-of-the-art performance.\nThis\nsignificantly"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "2) Phase 2: Featured 14 speakers (7 males and",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "outperforms previously established benchmarks"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "4 females from Phase 1, plus 3 new Yemeni",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "for this dataset, including the CNN-BLSTM-DNN"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "females), replacing the questioning emotion",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "model (Hifny and Ali, 2019) and the ResNet-based"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "with anger, contributing an additional 2 hours",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "architecture (Meftah et al., 2021).\nFurthermore,"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "and 15 minutes of recordings.",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "ArabEmoNet also surpasses the performance of"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "larger, pre-trained models such as HuBERT-base"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "4.3.2\nKEDAS Dataset",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "(Hsu et al., 2021) and Whisper-small\n(Radford"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "The KEDAS dataset\n(Belhadj et al., 2022) com-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "et al., 2022), despite its significantly smaller pa-"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "prises 5000 audio recording files in standard Ara-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "rameter count."
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "bic, featuring five emotional states: anger, happi-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "Similarly, on the KEDAS dataset, our model"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "ness, sadness, fear, and neutrality. The recordings",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "achieves an exceptional accuracy of 99.46%. This"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "were collected from 500 actors within the univer-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "result\nsubstantially surpasses\nthe original Base-"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "sity community, including students, professors, and",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "line Model (Belhadj et al., 2022) and demonstrates"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "staff. The dataset is based on 10 carefully selected",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "competitive performance even when compared to"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "phrases commonly used in communication, chosen",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "highly resource-intensive pre-trained models like"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "through literary and scientific studies. The data col-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "Whisper-small (Radford et al., 2022) and HuBERT-"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "lection and validation process involved 55 evalua-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "base (Hsu et al., 2021). Notably, ArabEmoNet"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "tors, including Arabic linguists, literary researchers,",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "achieves these superior or competitive results with"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "and clinical psychology specialists, ensuring high-",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "significantly fewer parameters (0.97M) compared"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "quality emotional content and linguistic accuracy.",
          "5\nResults": ""
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "",
          "5\nResults": "to pretrained models such as HuBERT-base (95M)"
        },
        {
          "(10 males, 13 females) representing diverse dialec-": "4.4\nEvaluation",
          "5\nResults": "and Whisper-small (74M)."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Impact of Changing Kernel Size for CNN",
      "data": [
        {
          "Dataset": "",
          "Model": "",
          "Accuracy": "(%) ↑",
          "Micro F1": "(%)",
          "Macro": "F1 (%)",
          "Params": "(M)"
        },
        {
          "Dataset": "",
          "Model": "Whisper-base (Radford et al., 2022)",
          "Accuracy": "78.81",
          "Micro F1": "76.77",
          "Macro": "78.81",
          "Params": "74"
        },
        {
          "Dataset": "",
          "Model": "Hubert-base-Emotion",
          "Accuracy": "84.30",
          "Micro F1": "83.00",
          "Macro": "84.00",
          "Params": "95"
        },
        {
          "Dataset": "",
          "Model": "ResNet-based Architecture (Meftah et al., 2021)",
          "Accuracy": "85.53",
          "Micro F1": "85.53",
          "Macro": "85.53",
          "Params": "25"
        },
        {
          "Dataset": "",
          "Model": "Whisper-small (Radford et al., 2022)",
          "Accuracy": "85.98",
          "Micro F1": "85.96",
          "Macro": "85.98",
          "Params": "244"
        },
        {
          "Dataset": "KSUEmotion",
          "Model": "",
          "Accuracy": "",
          "Micro F1": "",
          "Macro": "",
          "Params": ""
        },
        {
          "Dataset": "",
          "Model": "Hubert-base (Hsu et al., 2021)",
          "Accuracy": "87.04",
          "Micro F1": "87.22",
          "Macro": "87.04",
          "Params": "95"
        },
        {
          "Dataset": "",
          "Model": "ArabEmoNet (Transformer) - Ours",
          "Accuracy": "86.66",
          "Micro F1": "86.66",
          "Macro": "86.66",
          "Params": "1"
        },
        {
          "Dataset": "",
          "Model": "CNN-BLSTM-DNN Model (Hifny and Ali, 2019)",
          "Accuracy": "87.20",
          "Micro F1": "87.20",
          "Macro": "87.20",
          "Params": "-"
        },
        {
          "Dataset": "",
          "Model": "ArabEmoNet - Ours",
          "Accuracy": "91.48",
          "Micro F1": "91.48",
          "Macro": "91.46",
          "Params": "1"
        },
        {
          "Dataset": "",
          "Model": "Baseline Model (Belhadj et al., 2022)",
          "Accuracy": "75.00",
          "Micro F1": "75.00",
          "Macro": "75.00",
          "Params": "-"
        },
        {
          "Dataset": "KEDAS",
          "Model": "",
          "Accuracy": "",
          "Micro F1": "",
          "Macro": "",
          "Params": ""
        },
        {
          "Dataset": "",
          "Model": "Whisper-base (Radford et al., 2022)",
          "Accuracy": "97.60",
          "Micro F1": "97.56",
          "Macro": "97.60",
          "Params": "74"
        },
        {
          "Dataset": "",
          "Model": "Hubert-base-Emotion",
          "Accuracy": "98.00",
          "Micro F1": "97.98",
          "Macro": "98.00",
          "Params": "95"
        },
        {
          "Dataset": "",
          "Model": "Hubert-base (Hsu et al., 2021)",
          "Accuracy": "99.35",
          "Micro F1": "99.48",
          "Macro": "99.50",
          "Params": "95"
        },
        {
          "Dataset": "",
          "Model": "Whisper-small (Radford et al., 2022)",
          "Accuracy": "99.40",
          "Micro F1": "99.38",
          "Macro": "99.40",
          "Params": "244"
        },
        {
          "Dataset": "",
          "Model": "ArabEmoNet - Ours",
          "Accuracy": "99.46",
          "Micro F1": "99.46",
          "Macro": "99.42",
          "Params": "1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Impact of Changing Kernel Size for CNN",
      "data": [
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "Accuracy (%)"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": ""
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "89.90"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "91.15"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "91.48"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "90.08"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": ""
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "89.71"
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": ""
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": ""
        },
        {
          "Table 2: Comparison of Models on KSUEmotion and KEDAS Datasets": "Impact of Changing Kernel Size for CNN"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "cations, pages 121–126."
        },
        {
          "training to larger, multilingual datasets, validating": "its applicability and generalizability across diverse",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "linguistic and cultural contexts.\nThis expansion",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Manish Kumar and Anurag Raj. 2024. On the effect"
        },
        {
          "training to larger, multilingual datasets, validating": "promises significant contributions toward more in-",
          "In International Conference on Digital Telecommuni-": "of log-mel spectrogram parameter tuning for deep"
        },
        {
          "training to larger, multilingual datasets, validating": "clusive and effective global emotion recognition",
          "In International Conference on Digital Telecommuni-": "learning based speech emotion recognition.\nIn Pro-"
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "ceedings of the 2024 IEEE International Conference"
        },
        {
          "training to larger, multilingual datasets, validating": "systems.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "on Acoustics, Speech and Signal Processing, pages"
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "1–5."
        },
        {
          "training to larger, multilingual datasets, validating": "8\nLimitations",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Nattapong Kurpukdee,\nTomoki Koriyama,\nTakao"
        },
        {
          "training to larger, multilingual datasets, validating": "A potential\nlimitation to our architecture arises",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Kobayashi, Sawit Kasuriya, Chai Wutiwiwatchai,"
        },
        {
          "training to larger, multilingual datasets, validating": "from the method used to handle variable audio",
          "In International Conference on Digital Telecommuni-": "and\nPoonlap Lamsrichan.\n2017.\nSpeech\nemo-"
        },
        {
          "training to larger, multilingual datasets, validating": "lengths. To standardize the input size for model",
          "In International Conference on Digital Telecommuni-": "tion recognition using convolutional long short-term"
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "memory neural network and support vector machines."
        },
        {
          "training to larger, multilingual datasets, validating": "processing, the architecture employs zero-padding.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "In 2017 Asia-Pacific Signal and Information Pro-"
        },
        {
          "training to larger, multilingual datasets, validating": "Specifically, shorter audio sequences within any",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "cessing Association Annual Summit and Conference"
        },
        {
          "training to larger, multilingual datasets, validating": "given batch are padded with zeros to equal\nthe",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "(APSIPA ASC), pages 1744–1749."
        },
        {
          "training to larger, multilingual datasets, validating": "length of the longest sequence in that same batch.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "E. Lieskovska, M. Jakubec, R. Jarina, and M. Chmulik."
        },
        {
          "training to larger, multilingual datasets, validating": "While this is a standard technique, it can introduce",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "2021. A review on speech emotion recognition using"
        },
        {
          "training to larger, multilingual datasets, validating": "a limitation if there is significant variance in the",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "deep learning and attention mechanism. Electronics,"
        },
        {
          "training to larger, multilingual datasets, validating": "duration of audio clips within a batch.\nIn such",
          "In International Conference on Digital Telecommuni-": "10:1163."
        },
        {
          "training to larger, multilingual datasets, validating": "cases, shorter clips will be appended with a large",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Ali Hamid Meftah, Mustafa A. Qamhan, Yasser Seddiq,"
        },
        {
          "training to larger, multilingual datasets, validating": "amount of non-informative zero values, which can",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Yousef A. Alotaibi, and Sid Ahmed Selouani. 2021."
        },
        {
          "training to larger, multilingual datasets, validating": "lead to unnecessary computational processing and",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "King saud university emotions corpus: Construction,"
        },
        {
          "training to larger, multilingual datasets, validating": "potentially impact the model’s learning efficiency",
          "In International Conference on Digital Telecommuni-": "analysis, evaluation, and comparison.\nIEEE Access,"
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "9:54201–54219."
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "H. Meng, T. Yan, F. Yuan, and H. Wei. 2019. Speech"
        },
        {
          "training to larger, multilingual datasets, validating": "References",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "emotion recognition from 3d log-mel spectrograms"
        },
        {
          "training to larger, multilingual datasets, validating": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-",
          "In International Conference on Digital Telecommuni-": "with deep learning network.\nIEEE Access, 7:125868–"
        },
        {
          "training to larger, multilingual datasets, validating": "gio. 2015.\nNeural machine translation by jointly",
          "In International Conference on Digital Telecommuni-": "125881."
        },
        {
          "training to larger, multilingual datasets, validating": "learning to align and translate.\nIn ICLR.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Seyed-Ahmad Mirsamadi, Eslam Barsoum, and Chao"
        },
        {
          "training to larger, multilingual datasets, validating": "Mourad Belhadj, Ilham Bendellali, and Elalia Lakhdari.",
          "In International Conference on Digital Telecommuni-": "Zhang. 2017. Automatic speech emotion recogni-"
        },
        {
          "training to larger, multilingual datasets, validating": "2022. Kedas: A validated arabic speech emotion",
          "In International Conference on Digital Telecommuni-": "tion using recurrent neural networks with local atten-"
        },
        {
          "training to larger, multilingual datasets, validating": "dataset.\nIn 2022 International Symposium on iN-",
          "In International Conference on Digital Telecommuni-": "tion.\nIn IEEE International Conference on Acoustics,"
        },
        {
          "training to larger, multilingual datasets, validating": "novative Informatics of Biskra (ISNIB), pages 1–6.",
          "In International Conference on Digital Telecommuni-": "Speech and Signal Processing (ICASSP)."
        },
        {
          "training to larger, multilingual datasets, validating": "IEEE.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Matthias Neumann and Ngoc Thang Vu. 2017. Atten-"
        },
        {
          "training to larger, multilingual datasets, validating": "Hazem M Fayek, Marcin Lech, and Luis Cavedon. 2017.",
          "In International Conference on Digital Telecommuni-": "tive convolutional neural network based speech emo-"
        },
        {
          "training to larger, multilingual datasets, validating": "Evaluating deep learning architectures for speech",
          "In International Conference on Digital Telecommuni-": "tion recognition: A study on the iemocap database."
        },
        {
          "training to larger, multilingual datasets, validating": "emotion recognition. Neural Networks, 92:60–68.",
          "In International Conference on Digital Telecommuni-": "In Interspeech 2017."
        },
        {
          "training to larger, multilingual datasets, validating": "Youssef Hifny and Ahmed Ali. 2019. Efficient arabic",
          "In International Conference on Digital Telecommuni-": "Tin Lay Nwe, Say Wei Foo, and Liyanage C. De Silva."
        },
        {
          "training to larger, multilingual datasets, validating": "emotion recognition using deep neural networks.\nIn",
          "In International Conference on Digital Telecommuni-": "2003.\nSpeech emotion recognition using hidden"
        },
        {
          "training to larger, multilingual datasets, validating": "ICASSP 2019 - 2019 IEEE International Conference",
          "In International Conference on Digital Telecommuni-": "markov models. Speech Communication, 41(4):603–"
        },
        {
          "training to larger, multilingual datasets, validating": "on Acoustics, Speech and Signal Processing, pages",
          "In International Conference on Digital Telecommuni-": "623."
        },
        {
          "training to larger, multilingual datasets, validating": "6710–6714. IEEE.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "David S Park and 1 others. 2019. Specaugment: A sim-"
        },
        {
          "training to larger, multilingual datasets, validating": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,",
          "In International Conference on Digital Telecommuni-": "ple data augmentation method for automatic speech"
        },
        {
          "training to larger, multilingual datasets, validating": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-",
          "In International Conference on Digital Telecommuni-": "recognition.\nIn INTERSPEECH, pages 2613–2617."
        },
        {
          "training to larger, multilingual datasets, validating": "rahman Mohamed. 2021. Hubert: Self-supervised",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-"
        },
        {
          "training to larger, multilingual datasets, validating": "speech representation learning by masked prediction",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "man, Christine McLeavey, and Ilya Sutskever. 2022."
        },
        {
          "training to larger, multilingual datasets, validating": "of hidden units.\nIn IEEE/ACM Transactions on Au-",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Robust speech recognition via large-scale weak su-"
        },
        {
          "training to larger, multilingual datasets, validating": "dio, Speech, and Language Processing, volume 29,",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "pervision.\nIn Proceedings of the 40th International"
        },
        {
          "training to larger, multilingual datasets, validating": "pages 3451–3460. IEEE.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Conference on Machine Learning (ICML). PMLR."
        },
        {
          "training to larger, multilingual datasets, validating": "Mina Huh, Ruchira Ray, and Corey Karnei. 2024. A",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Tara N Sainath, Carolina Parada, Brian Kingsbury, and"
        },
        {
          "training to larger, multilingual datasets, validating": "comparison of speech data augmentation methods",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "Bhuvana Ramabhadran. 2015. Convolutional, long"
        },
        {
          "training to larger, multilingual datasets, validating": "using s3prl toolkit. Preprint, arXiv:2303.00510.",
          "In International Conference on Digital Telecommuni-": ""
        },
        {
          "training to larger, multilingual datasets, validating": "",
          "In International Conference on Digital Telecommuni-": "short-term memory, fully connected deep neural net-"
        },
        {
          "training to larger, multilingual datasets, validating": "T. Iliou and C.N. Anagnostopoulos. 2009. Statistical",
          "In International Conference on Digital Telecommuni-": "works.\nIn IEEE International Conference on Acous-"
        },
        {
          "training to larger, multilingual datasets, validating": "evaluation of speech features for emotion recognition.",
          "In International Conference on Digital Telecommuni-": "tics, Speech and Signal Processing (ICASSP)."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "pakowicz. 2009. A systematic analysis of perfor-"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Information\nmance measures for classification tasks."
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Processing & Management, 45(4):427–437."
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "George\nTrigeorgis, Marios A Nicolaou,\nStefanos"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Zafeiriou, and Björn W Schuller. 2016. Adieu fea-"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "tures? end-to-end speech emotion recognition using"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "a deep convolutional\nrecurrent network.\nIn IEEE"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "International Conference on Acoustics, Speech and"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Signal Processing (ICASSP)."
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Ashish Vaswani and 1 others. 2017. Attention is all you"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "need.\nIn Advances in Neural Information Processing"
        },
        {
          "Marina Sokolova, Nathalie Japkowicz, and Stan Sz-": "Systems, pages 5998–6008."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "2",
      "title": "Kedas: A validated arabic speech emotion dataset",
      "authors": [
        "Mourad Belhadj",
        "Ilham Bendellali",
        "Elalia Lakhdari"
      ],
      "year": "2022",
      "venue": "2022 International Symposium on iNnovative Informatics of Biskra (ISNIB)"
    },
    {
      "citation_id": "3",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Marcin Hazem M Fayek",
        "Luis Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "4",
      "title": "Efficient arabic emotion recognition using deep neural networks",
      "authors": [
        "Youssef Hifny",
        "Ahmed Ali"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "A comparison of speech data augmentation methods using s3prl toolkit",
      "authors": [
        "Mina Huh",
        "Ruchira Ray",
        "Corey Karnei"
      ],
      "year": "2024",
      "venue": "A comparison of speech data augmentation methods using s3prl toolkit",
      "arxiv": "arXiv:2303.00510"
    },
    {
      "citation_id": "7",
      "title": "Statistical evaluation of speech features for emotion recognition",
      "authors": [
        "T Iliou",
        "C Anagnostopoulos"
      ],
      "year": "2009",
      "venue": "International Conference on Digital Telecommunications"
    },
    {
      "citation_id": "8",
      "title": "On the effect of log-mel spectrogram parameter tuning for deep learning based speech emotion recognition",
      "authors": [
        "Manish Kumar",
        "Anurag Raj"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using convolutional long short-term memory neural network and support vector machines",
      "authors": [
        "Nattapong Kurpukdee",
        "Tomoki Koriyama",
        "Takao Kobayashi",
        "Sawit Kasuriya",
        "Chai Wutiwiwatchai",
        "Poonlap Lamsrichan"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)",
      "doi": "10.1109/APSIPA.2017.8282315"
    },
    {
      "citation_id": "10",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovska",
        "M Jakubec",
        "R Jarina",
        "M Chmulik"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "11",
      "title": "King saud university emotions corpus: Construction, analysis, evaluation, and comparison",
      "authors": [
        "Ali Hamid Meftah",
        "Mustafa Qamhan",
        "Yasser Seddiq",
        "Yousef Alotaibi",
        "Sid Selouani"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3070751"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyed-Ahmad Mirsamadi",
        "Eslam Barsoum",
        "Chao Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the iemocap database",
      "authors": [
        "Matthias Neumann",
        "Ngoc Vu"
      ],
      "year": "2017",
      "venue": "Attentive convolutional neural network based speech emotion recognition: A study on the iemocap database"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "Tin Lay Nwe",
        "Say Foo",
        "Liyanage Silva"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "16",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "S David",
        "Park"
      ],
      "year": "2019",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "17",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Proceedings of the 40th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "18",
      "title": "Convolutional, long short-term memory, fully connected deep neural networks",
      "authors": [
        "Tara Sainath",
        "Carolina Parada",
        "Brian Kingsbury",
        "Bhuvana Ramabhadran"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "Marina Sokolova",
        "Nathalie Japkowicz",
        "Stan Szpakowicz"
      ],
      "year": "2009",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "20",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Marios Nicolaou",
        "Stefanos Zafeiriou",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Ashish Vaswani and 1 others",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}