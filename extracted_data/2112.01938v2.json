{
  "paper_id": "2112.01938v2",
  "title": "Shapes Of Emotions: Multimodal Emotion Recognition In Conversations Via Emotion Shifts",
  "published": "2021-12-03T14:39:04Z",
  "authors": [
    "Harsh Agarwal",
    "Keshav Bansal",
    "Abhinav Joshi",
    "Ashutosh Modi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is an important and active research area. Recent work has shown the benefits of using multiple modalities (e.g., text, audio, and video) for the ERC task. In a conversation, participants tend to maintain a particular emotional state unless some stimuli evokes a change. There is a continuous ebb and flow of emotions in a conversation. Inspired by this observation, we propose a multimodal ERC model and augment it with an emotion-shift component that improves performance. The proposed emotion-shift component is modular and can be added to any existing multimodal ERC model (with a few modifications). We experiment with different variants of the model, and results show that the inclusion of emotion shift signal helps the model to outperform existing models for ERC on MOSEI and IEMOCAP datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans are complex social beings, and emotions are indicative of not just their inner state and feelings but also their internal thinking process  (Minsky, 2007) . To fully understand a person, one needs to understand their inherent emotions. Recent research has witnessed colossal interest in including artificially intelligent machines as conversable companions for humans, e.g., personal digital assistants. However, communication with AI systems is quite limited. AI systems do not understand the inherent emotions expressed implicitly by humans making them unable to comprehend the underlying thought processes and respond appropriately. Consequently, a wide variety of approaches have been proposed for developing emotion understanding and generation systems  (Sharma and Dhall, 2021; Witon et al., 2018; Singh et al., 2021a; Goswamy et al., 2020; Colombo et al., 2019; Singh et al., 2021b; Joshi et al., 2022) . * Equal Contributions Most of the time in an online course the student to content interactivity is assumed but --the first thing to think about here is that they are three interactivity types. You have student to student interactivity, student to content interactivity, and student to instructor interactivity mostly --assumed. Student to student relationships and student to instructor relationships aren't always assumed.\n\nOne of the opportunities you can do is to have a hallway conversation area i know that some ..",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Negative Sentiment Positive Sentiment Emotional Shift",
      "text": "Figure  1 : Emotional shift on the dialogue \"m7SJs73SF8w\" from CMU-MOSEI dataset During an interaction, humans express different emotions and fluctuate between multiple emotional states. It is often the case that participants in a conversation tend to maintain a particular emotional state unless some stimuli evokes a change. This observation is closely related to Shapes of Stories proposed by renowned writer Kurt  Vonnegut (Vonnegut, 1995) , who posits that every story has a shape plotted by the ups and downs experienced by the characters of the story, and this, in turn, defines an Emotional Arc of a story. This phenomenon has also been empirically verified by  Reagan et al. (2016) , who analyzed around 1300 stories to come up with common emotional arc patterns across various stories. Moreover, apart from these flows, there exists a sudden shift of emotions from positive to negative sentiments. Consider an example shown in Figure  1 , where the sentiment of the third utterance shifts from positive to negative and back again to positive in the fourth utterance. Current state-ofthe-art methods are often oblivious to the presence of such emotion shifts and tend to fail in cases where there is a sudden change in the emotional state  (Poria et al., 2019) . To address this issue, we propose incorporating a novel module that explicitly tracks such emotional shifts in conversations. Humans express their emotions via various modalities, such as language, modulations in voice, facial expressions, and body gestures. In this paper, to fully and correctly recognize human emotions, we propose a multimodal emotion recognition system that utilizes language, audio, and video modalities. We propose a multimodal ERC model based on GRUs that fuses information from different modalities. An independent emotional shift component captures the emotion shift signal between consecutive utterances, allowing the model to forget past information in case of an emotional shift. We make the following contributions: show that emotional shift component helps to outperform some of the existing models. We perform detailed analysis and ablation studies of the model and show the contribution of different components. We analyse the performance of our model in the classification of utterances having a shift in emotion and compare this with previous models and report an improvement due to the use of emotionshift information. We further examine how the internal GRU gates behave during emotion shifts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition using multiple modalities is an active area of research leading to the development of widely popular benchmark datasets, e.g.,  CMU-MOSEI (Bagher Zadeh et al., 2018) , and IEMOCAP  (Busso et al., 2008) . Recent works have highlighted the crucial aspects of self, and interpersonal dependencies in the emotional dynamics of the conversations  (Poria et al., 2019) . Another essential feature is the role of the local and global context for emotion recognition systems. Some notable works like Dialogue RNN  (Majumder et al., 2018b)  try to capture these properties by modeling each speaker with a party state and the emotion of each utterance by an emotional state. Furthermore, a context state is maintained to model the global conversation context. Another work Multilogue-Net  (Shenoy and Sardana, 2020)  highlights the limitation of the fusion mechanism used in Dialogue RNN  (Majumder et al., 2018b)  and tries to solve it using a party, context, and emotion GRUs for each modality. It uses a pairwise attention mechanism proposed by  (Ghosal et al., 2018)  to fuse the emotion states for all the modalities effectively. However, DialogueRNN highlights the poor performance in predicting the emotions with the utterances where the emotion shifts from positive to negative sentiments. Our work considers the emotion shifts present in the dialogues and tries to leverage them for improving emotion recognition. Another line of work includes Transformer-Based Joint-Encoding (TBJE)  (Delbrouck et al., 2020)  that achieves the state-of-the-art results on the sentiment task for the MOSEI dataset using a multimodal transformer-based model for combining multiple modalities. However, in the emotion task, TBJE is outperformed by the Multilogue-Net model. The possible reason highlighted by the paper is the lack of context-awareness in the architecture, as TBJE neither uses the previous nor next utterance to predict the emotion for the current utterance. Some of the other works in multimodal emotion recognition include the Memory Fusion network (MFN)  (Zadeh et al., 2018) , which aligns multimodal sequences using multi-view gated memory, Graph-MFN  (Bagher Zadeh et al., 2018)  which uses Dynamic Fusion Graph (DFG) and learns to model the n-modal interactions dynamically, and bc-LSTM  (Poria et al., 2017)  which uses an LSTM-based model to capture contextual information. CESTa  (Wang et al., 2020)  captures the emotional consistency in the utterances using CRF model  (Lafferty et al., 2001)  for boosting the performance of emotion classification and comes close to our idea of leveraging emotion shifts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task And Corpus",
      "text": "Problem Definition: Consider a conversation having utterances u 1 , .  We define an utterance to be a coherent piece of information (single or multiple sentences) conveyed by a single participant at a given time. We model an utterance in terms of different modalities: u t = {l t , a t , v t }. An utterance (u t ) at time-step t is represented via features from textual transcript (l t ), audio (a t ), and visuals (v t ) of the speaker. We denote the speaker of utterance u t as q t .\n\n3.1 Corpus Details",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cmu-Mosei:",
      "text": "The CMU Multimodal Opinion Sentiment and Emotion Intensity  (Bagher Zadeh et al., 2018 ) is an English language dataset containing more than 65 hours of annotated video from more than 10000 speakers and 250 topics. Each sentence is annotated for a sentiment on a [-3, 3] Likert scale. However, in this work, we project these labels to a twoclass classification setup with values â‰¥ 0 signifies positive sentiments and values < 0 convey negative sentiments. Dataset also contains six emotion labels, namely angry, happy, sad, surprise, fear and disgust for each utterance. Note that in case of emotions labels the utterances are multi-label. Which means a single utterance can have more than one emotion label. We have shown results for both sentiment and emotion prediction tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iemocap:",
      "text": "The IEMOCAP benchmark  (Busso et al., 2008)  consists of a conversation between ten distinct speakers. The dataset contains two-way conversations in videos where every video clip contains a single dyadic English dialogue. Further, each dialogue segments into utterances with an emotion label from six emotion labels, i.e., happy, sad, neutral, angry, excited, and frustrated. The dataset incorporates an acted setting where actors perform improvisations or scripted scenarios, specifically selected to elicit emotional expression.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Proposed Model",
      "text": "During a conversation, speakers tend to maintain an emotional flow of affective states. These states majorly rely on the context of the entire conversation; for example, if the overall gist of the speaker about the topic is positive, the emotions like happiness, joy, and surprise can be seen more often than negative emotions like anger and sadness. Moreover, a speaker's emotions are often affected by the past emotions present in the conversation. Hence, an emotion prediction model should not only take into account the context but should also be able to maintain the speaker-level information along with the emotions present in the past utterances. Considering these assumptions, we propose the primary component of our emotion recognition model: the Emotion Classification Component. The emotions classification component predicts an utterance's emotion label using information from the current speaker, emotions of the previous utterances, and the entire conversation context. Another significant insight about the emotions in a conversation is the sudden shift of emotional states. Many of the existing state-of-the-art approaches highlight this in their error analysis, where the model fails to capture the sudden shifts in emotional states leading to a misclassified emotion prediction. To incorporate the effect of a sudden shift in the emotion, we introduce a separately trained component called the Emotion Shift Component. The emotion shift component explicitly models the probability (p shif t t\n\n) of a shift in emotion between the utterances u t-1 and u t . This shift in emotion can be expressed as moving from a positive (e.g., happy) to negative emotion (e.g., sad) or vice-versa between consecutive utterances. The emotion shift component being independent of the primary architecture is pretrained, and helps control the information flow from past to future during a sudden change. The signal from the pretrained emotional shift component is added to the emotion classification component to control the flow of emotions from past to future. Figure  2  shows detailed architecture of the proposed model. Emotion Classification Component: For modeling the underlying emotions in a conversation, we maintain a party state, emotion state and context state. The party state is maintained for each speaker and helps to keep track of the participant specific aspect in a conversation. The context state is global (common across each participant) and helps to encode the entire conversation context, thereby capturing inter-utterance dependencies. Akin to the context state, the emotion state is also global and helps to leverage the emotion information flow between utterances. Moreover, the emotion shift signal between the current and previous utterance is used to update the global emotion state. The emotion label for each utterance is then predicted by decoding the emotion state. In our model each of the party, context and emotion states are modality specific and are updated using a modality specific GRU  (Chung et al., 2014)  network for each modality m âˆˆ {l, a, v} (indicated by the superscript m). We employ late fusion to combine the emotion states from different modalities. Next, we explain different GRU networks used in the model. For each modality m âˆˆ {l, a, v}, q t 's party state s qt,m t-1 is updated to s qt,m t using an attention vector x m t and modality specific feature m t (Eq. 1), âŠ• denotes concatenation operation. Here x m t is calculated using a simple dot product attention mechanism over the context states (c m t ). Note that for all speakers other than q t , the party state at t -1 and t remains the same. Context State Update (GRU c ): Global conversation context is modeled using the context state update GRU c . For each modality m âˆˆ {l, a, v}, the global context state c m t-1 is updated to c m t (Eq. 2) using the q t 's party state s qt,m t and the corresponding modality feature\n\n) are used for calculating the attention vector x m t for each modality m âˆˆ {l, a, v} as follows:\n\n(5)\n\nEmotion State Update (GRU arc ): For each modality m âˆˆ {l, a, v}, the global emotion state e m t-1 is updated to e m t (Eq. 3) using the current party state s qt,m t and modulated by the emotion shift component (p shift t ). The emotion states for all the three modalities are fused together (Eq. 4) to create e t using a pairwise attention mechanism  (Shenoy and Sardana, 2020 ). e t is later used to decode the emotion class for an utterance.\n\nThe emotion classification component is a context-aware model similar to that of previous works like Multilogue-net (Shenoy and Sardana, 2020) but with a few key differences. Firstly, instead of modelling an emotion state for each participant, we introduce global emotion state for each conversation. This is done to make use of the flow of emotion between utterances. Secondly, the emotion shift signal between the current and previous utterance (p shift t ) is used to update the global emotion state using a GRU arc which aims to model the emotion arc in the conversation. Emotion Shift Component: To capture the emotional arc across the conversation, we explicitly model probability of emotion shift (p shif t t ) between successive utterances (u t-1 and u t ). We use a Siamese network  (Bromley et al., 1993)  to model the emotional shift present across utterances. A Siamese network generally consists of two or more identical subnetworks having the same configuration with shared parameters and weights. The proposed emotion shift architecture takes the textual features of the current (l t ) and previous (l t-1 ) utterances and outputs the probability of maintaining emotional inertia (p inertia t ). The architecture of the emotion shift prediction network is shown in lower half of Figure  2 . We use Sentence-BERT (SBERT)  (Reimers and Gurevych, 2019)  embeddings as textual features. SBERT is a modification of the pretrained BERT  (Devlin et al., 2019)  network that uses the Siamese network to derive semantically meaningful sentence embeddings for transfer learning tasks. The emotion shift prediction network makes use of only the text modality for two reasons. Firstly, it has often been found empirically that among the text, audio, and video modalities, text modality carries more information for ERC tasks  (Poria et al., 2018) . Secondly, early fusion techniques to combine the three modalities can suffer in a Siamese-type architecture due to difficulty in mapping the fused modality vector to a vector space in which similar vectors are close. We also experimentally verify this ( Â§6).\n\nThe emotion shift prediction network (between u t and u t-1 ) takes in text features corresponding to utterances (l t and l t-1 ) and their element wise differences to output the probability of a shift as given by (Eq. 7). Here, p inertia t is calculated using Siamese network (Eq. 8, 9). Here, H t is the Siamese hidden state, W âˆˆ R 3d l is the model parameter. For the Siamese network, we use Binary Cross Entropy loss (L s ) over the distribution p shift t . The emotion shift component modulates the Emotion State GRU arc via p shift t and hence controls the flow of information during the conversation. The Emotion Shift component captures the emotional consistency in the utterances and can act as an independent modular component that can be pretrained and added to any existing multi-modal ERC framework with a few modifications for improving emotion recognition in conversations.\n\nOverall Architecture: The motivation for the proposed architecture follows from the intuition that we need to weigh down the contribution of the previous emotion state in case of an emotion shift. In other words, we need to reduce the influence of e m t-1 in the calculation of e m t when there is a high p shif t t . To do so, we modify the reset and update gates in the GRU modelling the emotional arc of the conversation i.e. GRU arc . A GRU has gating units (reset and update gates) that modulate the flow of information inside the unit.  Ravanelli et al. (2017)  mention the usefulness of reset gate in scenarios where significant discontinuities are present in the sequence, thereby indicating its crucial role to forget information. Their work also finds a redundancy in the activations of the reset and update gates when processing speech sequences. Motivated by this, and the intuition that we need to forget more information when there is a higher probability of an emotional shift, we directly use the value of (1 -p shif t t ) for both the reset and update gates. The updates for GRU arc unit are given by Eqs. 10, 11. Eq. 10 calculates a candidate emotion state áº½m t in which the prior emotion state's (e m t-1 ) is controlled by the emotion shift signal. The output e m t is a linear interpolation between áº½m t-1 and e m t-1 . Again, p shif t t controls the influence of e m t-1 (Eq. 11). Therefore, a higher value of p shif t t will limit the contribution of the previous emotion state. In the absence of the emotion shift component, the GRU gates are learned using only the classification data, much like the rest of the parameters in the model. If the total number of parameters in a model is huge (as is the case with most deep learning models), the gates might be unable to learn well. We verify that the modeling of the shift in emotion encourages better learning of these gates ( Â§6).\n\nFor prediction at time t, the emotion vector e t (formed from fusion of e m t as described in (Eq. 4)) is passed through a final classification layer W c (âˆˆ R deÃ—K ) where K is the number of emotion or sentiment classes. This is used to obtain probability distribution over emotion labels via the Softmax activation: o = softmax(W T c e t ). We use the Cross-Entropy Loss over this distribution to train the weights. We define an emotion shift between consecutive utterances if there is a shift from a positive to a negative emotion or viceversa. CMU-MOSEI dataset provides annotated (positive/negative) sentiment label for each utterance. This is not the case for the IEMOCAP dataset, therefore we divide the emotion classes into a positive and negative category. Happiness and surprise are taken into the positive category while disgust, angry and sad are considered as the negative category. Note that IEMOCAP also has a neutral emotion, but a shift is only counted if it is from a positive to negative emotion or vice-versa. Table  1  shows the percentage of emotion shift observed in the datasets. Since CMU-MOSEI shows a larger amount of emotion shift, we were motivated to perform experiments on CMU-MOSEI first.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "We evaluate our approaches using standard F1 score and Accuracy evaluation metrics (App. A). We train and report the performance of our model for four sub-tasks, 2-way sentiment classification and binary emotion classification on CMU-MOSEI, and four-class and six-class emotion classification task for IEMOCAP. The focus of our work is multimodal ERC and consequently, as is done in previous work, we compare only with previous multimodal approaches, since comparison with unimodal (e.g., text) only approaches does not make sense. Moreover, SOTA unimodal approaches (such as text based) use additional information such external knowledge sources (e.g.,  (Ghosal et al., 2020) ) which makes the comparison with multimodal approach unfair specially given that such knowledge may not be available for other modalities. Nevertheless, it is possible to incorporate the emotion shift component into existing emotion prediction architectures (unimodal or multimodal) and we leave this exploration for future. Results on CMU-MOSEI: Table  2  shows comparison of our best performing model on CMU-MOSEI sentiment labels, with current state of the art models: TBJE  (Delbrouck et al., 2020) , Multilogue-Net  (Shenoy and Sardana, 2020) , Dialogue RNN  (Majumder et al., 2018b) , and Graph-MFN  (Poria et al., 2017) . As evident from the results, we are able to significantly outperform the previous SOTA Multilogue-Net model with an increase of 3% in F1 score. We further compare our model on the emotion classification task with TBJE and Multilogue-Net (Table  3 ). As shown in the table, our model outperforms for some of the emotion classes. We speculate that poor performance is due to the multilabel setting in the CMU-MOSEI dataset. As the emotion labels are multilabel, the emotion shift component is not able to play a meaningful role in providing a performance boost to the emotion classification component. We consider multilabel settings as another line of future work where the emotion shift modeling takes into account the multilabel property.   4 ). Our model significantly outperforms both of these on average weighted F1 and Accuracy. Also, emotion classes neutral and angry show improved performance. We also provide results on six emotion classes -happy, sad, neutral, angry, excited, and frustrated (Table  5 ). For these experiments, we use BERT features for text ( Â§6), OpenSmile features for audio and 3D-CNN features  (Majumder et al., 2018b)  for video. We did not come across any existing work on 6-class multimodal IEMOCAP for the comparison.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results On Iemocap: Previous Works On",
      "text": "Performance of emotion shift component: The results describing the capability of the emotion shift component to predict the shift for CMU-MOSEI and IEMOCAP dataset are shown in Table  6 . It is to be noted that predicting the shift accurately is not our primary objective. Our objective is to be able to improve the emotion prediction by using the signal (p shif t t ) received from the emotion shift component.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Analysis And Ablation Studies",
      "text": "Due to a wide variety of components, it becomes vital to perform a detailed analysis of the architecture to understand the importance of various choices. Feature and Design choices: For understanding the importance of features used for different modalities, we choose two different sets of features for text and visual modalities. In one setting, we use averaged GloVe embeddings  (Pennington et al., 2014)  for text, OpenSmile features  (Eyben et al., 2010)  for Audio and Facet features  (StÃ¶ckli et al., 2017)     (Poria et al., 2019) . In particular, in cases where the emotion of the target utterance differs from the previous utterance, DialogueRNN could only correctly predict 47.5% instances, much lesser than the 69.2% success rate that it achieves at the regions of no emotional shift. In Table  8  we compare our results with another multimodal ERC SOTA: Multilogue-Net.\n\nThe results show a significant increase in accuracy for both positive to negative and negative to positive emotion shifts on the CMU-MOSEI dataset depicting the importance of the independent emotion shift component introduced in our architecture.\n\nEven though the Siamese network can predict the presence of emotion shift with an accuracy of about 72.65% (Table  7 ), the signal received from it (in the form of reset and update gates of GRU) helps the emotion classification network to overcome the emotional inertia and predict the correct emotion. We also show the accuracy of our model on emotion shift utterances of the IEMOCAP dataset in  from the test set. This dialogue has four utterances, and we see a shift from positive to negative emotion between utterances two and three and a shift from negative to positive emotion between utterances three and four. As seen in the left graph in Figure  3 , the emotion shift component learns to set a low reset gate value when there is an emotion shift (namely timestamps t = 3 and t = 4). This low reset gate value helps to weigh down the contribution of the previous emotion state for the predictions at the current timestamp. Comparing it to the case when we remove the emotion shift component (right graph in Figure  3 ), the reset gate activations learned by the GRU do not follow the same trend, indicating that the previous emotion state will still significantly contribute to predictions at the current timestamp. Overall, the emotion shift component plays a vital role in effectively controlling information from the past.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "The presence of emotion shifts in human-to-human conversation is prominent in the conversational datasets. The existing works based on sequential modeling often suffer from these shifts, leading to poor performance for utterances with emotion shifts. In this work, we try to control the effect of previous utterances using an independent emotion shift module. As highlighted in Tables  8  and 9 , the proposed architecture performs significantly better on emotion shift cases when compared to Multilogue-Net (20% improvement in negativepositive and 4% improvement on positive-negative shifts). The novel design of the emotion shift-based gating mechanism in the GRU unit helps boost the prediction performance for utterances with emotion shifts. As noticed in Fig.  3 , the reset and update gates provide a significant signal when there is an emotional shift in conversation.\n\nModularity: The modular design and idea of the proposed emotion shift component can further be used to improve any emotion prediction systems that have poor performance in emotion shift cases. Moreover, the designed emotion shift component works considering only the textual modality, mak- ing it applicable to both multimodal as well as unimodal systems.\n\nApplication to Real-Time Systems: A notable limitation of all the existing Emotion Recognition state-of-the-art systems often comes from the incapability of their implementations for real-time use cases as they require the entire context to be given in the form of multiple utterances to the model. For future approaches where the models will target the real-time setting, the proposed emotion shift component can be handy as it only uses two consecutive utterances to predict the emotion shift.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Directions",
      "text": "In this paper, we proposed a deep learning based model for multimodal emotion recognition in conversations. We proposed a new emotion shift component (modeled using the Siamese net) that captures the emotional arc in a conversation and steers the main emotion recognition model. We performed a battery of experiments on two main emotion recognition datasets. Results and analysis show the importance of the emotion shift component. Currently, the emotion shift component uses only the text modality for predicting the shift and we plan to explore more sophisticated ways of using information from multiple modalities.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , where the sentiment of the third utter-",
      "page": 1
    },
    {
      "caption": "Figure 2: The model architecture for a conversation between two speakers, A and B, at time t and t + 1. The upper",
      "page": 3
    },
    {
      "caption": "Figure 2: shows detailed architecture of the proposed model.",
      "page": 4
    },
    {
      "caption": "Figure 2: We use Sentence-BERT (SBERT)",
      "page": 5
    },
    {
      "caption": "Figure 3: , we show these activations for",
      "page": 8
    },
    {
      "caption": "Figure 3: , the emotion shift component learns to",
      "page": 9
    },
    {
      "caption": "Figure 3: ), the reset gate",
      "page": 9
    },
    {
      "caption": "Figure 3: , the reset and update",
      "page": 9
    },
    {
      "caption": "Figure 3: Reset Gate activations on the dialogue",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Predicted Emotion Predicted Emotion\nð•¤ð• ð•—ð•¥ð•žð•’ð•© ð•¤ð• ð•—ð•¥ð•žð•’ð•©\nSpeaker A\nSpeaker B Speaker B\nsBERT Feature sBERT Feature\nEmbeddings Difference Embeddings Difference\nEmotion Shift Component Emotion Shift Component": "e2:Themodelarchitectureforaconversationbetweentwospeakers,AandB,attimetandt+1. The\nighlightstheEmotionClassificationComponent,andthelowerparthighlightstheEmotionShiftCompo\ndefine an utterance to be a coherent piece 3.1.2 IEMOCAP:\nformation(singleormultiplesentences)con-\nThe IEMOCAP benchmark (Busso et al., 2\ndbyasingleparticipantatagiventime. We\nconsists of a conversation between ten dis\nelanutteranceintermsofdifferentmodalities:\nspeakers. The dataset contains two-way co\n{l ,a ,v }. An utterance (u ) at time-step t\nt t t t sationsinvideoswhereeveryvideoclipcon\npresentedviafeaturesfromtextualtranscript\na single dyadic English dialogue. Further,\naudio(a ),andvisuals(v )ofthespeaker. We\nt t dialoguesegmentsintoutteranceswithanem\ntethespeakerofutteranceu asq .\nt t labelfromsixemotionlabels,i.e.,happy,sad,\ntral, angry, excited, and frustrated. The da\nincorporatesanactedsettingwhereactorsper\nCorpusDetails\nimprovisations or scripted scenarios, specifi\nselectedtoelicitemotionalexpression.\nCMU-MOSEI:\n4 TheProposedModel",
          "Column_2": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 12: Performance using other modalities in",
      "data": [
        {
          "0.87": "",
          "Column_2": "",
          "Column_3": "0.23",
          "Column_4": ""
        },
        {
          "0.87": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "0.33"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 12: Performance using other modalities in",
      "data": [
        {
          "Column_1": "",
          "0.75": "0.87",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "0.75": "0.84",
          "Column_3": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "2",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "3",
      "title": "Signature verification using a siamese time delay neural network",
      "authors": [
        "Jane Bromley",
        "J Bentz",
        "Leon Bottou",
        "I Guyon",
        "C Yann Lecun",
        "Eduard Moore",
        "R Sackinger",
        "Shah"
      ],
      "year": "1993",
      "venue": "International Journal of Pattern Recognition and Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "API design for machine learning software: experiences from the scikit-learn project",
      "authors": [
        "Lars Buitinck",
        "Gilles Louppe",
        "Mathieu Blondel",
        "Fabian Pedregosa",
        "Andreas Mueller",
        "Olivier Grisel",
        "Vlad Niculae",
        "Peter Prettenhofer",
        "Alexandre Gramfort",
        "Jaques Grobler",
        "Robert Layton",
        "Jake Vanderplas",
        "Arnaud Joly",
        "Brian Holt",
        "GaÃ«l Varoquaux"
      ],
      "year": "2013",
      "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "6",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "7",
      "title": "Affect-driven dialog generation",
      "authors": [
        "Pierre Colombo",
        "Wojciech Witon",
        "Ashutosh Modi",
        "James Kennedy",
        "Mubbasir Kapadia"
      ],
      "year": "2019",
      "venue": "Affect-driven dialog generation",
      "arxiv": "arXiv:1904.02793"
    },
    {
      "citation_id": "8",
      "title": "A transformerbased joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "NoÃ© Tits"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challengepages",
      "doi": "10.18653/v1/2020.challengehml-1.1"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "10",
      "title": "opensmile -the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "MM'10 -Proceedings of the ACM Multimedia 2010 International Conference",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "11",
      "title": "Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Chauhan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1382"
    },
    {
      "citation_id": "12",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations"
    },
    {
      "citation_id": "13",
      "title": "Adapting a language model for controlled affective text generation",
      "authors": [
        "Tushar Goswamy",
        "Ishika Singh",
        "Ahsan Barkati",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "15",
      "title": "Adam: A method for stochastic optimization. International Conference on Learning Representations",
      "authors": [
        "Diederik Kingma",
        "Jimmy Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization. International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Pereira"
      ],
      "year": "2001",
      "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01"
    },
    {
      "citation_id": "17",
      "title": "Erik Cambria, and Soujanya Poria. 2018a. Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "venue": "Erik Cambria, and Soujanya Poria. 2018a. Multimodal sentiment analysis using hierarchical fusion with context modeling"
    },
    {
      "citation_id": "18",
      "title": "Devamanyu Hazarika, Rada Mihalcea, Alexander F. Gelbukh, and Erik Cambria. 2018b. Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "venue": "Devamanyu Hazarika, Rada Mihalcea, Alexander F. Gelbukh, and Erik Cambria. 2018b. Dialoguernn: An attentive RNN for emotion detection in conversations"
    },
    {
      "citation_id": "19",
      "title": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind",
      "authors": [
        "Marvin Minsky"
      ],
      "year": "2007",
      "venue": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind"
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Zach Yang",
        "Martin Devito",
        "Alykhan Raison",
        "Sasank Tejani",
        "Benoit Chilamkurthy",
        "Lu Steiner",
        "Junjie Fang",
        "Soumith Bai",
        "Chintala"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library"
    },
    {
      "citation_id": "22",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "23",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "24",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up baselines",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Majumder",
        "Erik Hazarika",
        "Amir Cambria",
        "Alexander Hussain",
        "Gelbukh"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis: Addressing key issues and setting up baselines"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversation: Research challenges, datasets, and recent advances"
    },
    {
      "citation_id": "26",
      "title": "Improving speech recognition by revising gated recurrent units",
      "authors": [
        "Mirco Ravanelli"
      ],
      "year": "2017",
      "venue": "Improving speech recognition by revising gated recurrent units"
    },
    {
      "citation_id": "27",
      "title": "The emotional arcs of stories are dominated by six basic shapes",
      "authors": [
        "Lewis Andrew J Reagan",
        "Dilan Mitchell",
        "Christopher Kiley",
        "Peter Danforth",
        "Dodds"
      ],
      "year": "2016",
      "venue": "EPJ Data Science"
    },
    {
      "citation_id": "28",
      "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Sentencebert: Sentence embeddings using siamese bertnetworks"
    },
    {
      "citation_id": "29",
      "title": "A survey on automatic multimodal emotion recognition in the wild",
      "authors": [
        "Garima Sharma",
        "Abhinav Dhall"
      ],
      "year": "2021",
      "venue": "Advances in Data Science: Methodologies and Applications"
    },
    {
      "citation_id": "30",
      "title": "Multiloguenet: A context-aware rnn for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "31",
      "title": "2021a. An end-to-end network for emotion-cause pair extraction",
      "authors": [
        "Aaditya Singh",
        "Shreeshail Hingane",
        "Saim Wani",
        "Ashutosh Modi"
      ],
      "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "32",
      "title": "2021b. Fine-grained emotion prediction by modeling emotion definitions",
      "authors": [
        "Gargi Singh",
        "Dhanajit Brahma",
        "Piyush Rai",
        "Ashutosh Modi"
      ],
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "33",
      "title": "Facial expression analysis with affdex and facet: A validation study",
      "authors": [
        "Sabrina StÃ¶ckli",
        "Michael Schulte-Mecklenbeck",
        "Stefan Borer",
        "Andrea Samson"
      ],
      "year": "2017",
      "venue": "Facial expression analysis with affdex and facet: A validation study",
      "doi": "10.3758/s13428-017-0996-1"
    },
    {
      "citation_id": "34",
      "title": "Shapes of stories. Vonnegut's Shapes of Stories",
      "authors": [
        "Kurt Vonnegut"
      ],
      "year": "1995",
      "venue": "Shapes of stories. Vonnegut's Shapes of Stories"
    },
    {
      "citation_id": "35",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "36",
      "title": "Disney at iest 2018: Predicting emotions using an ensemble",
      "authors": [
        "Wojciech Witon",
        "Pierre Colombo",
        "Ashutosh Modi",
        "Mubbasir Kapadia"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "37",
      "title": "Memory fusion network for multiview sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Memory fusion network for multiview sequential learning"
    }
  ]
}