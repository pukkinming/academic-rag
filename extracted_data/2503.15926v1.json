{
  "paper_id": "2503.15926v1",
  "title": "Modeling Face Emotion Perception From Naturalistic Face Viewing: Insights From Fixational Events And Gaze Strategies",
  "published": "2025-03-20T08:01:59Z",
  "authors": [
    "Meisam J. Seikavandi",
    "Maria J. Barrett",
    "Paolo Burelli"
  ],
  "keywords": [
    "Face Emotion Perception",
    "Emotional Gaze Strategies",
    "Free Face viewing",
    "Microsaccade"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Face Emotion Recognition (FER) is essential for social interactions and understanding others' mental states. Utilizing eye tracking to investigate FER has yielded insights into cognitive processes. In this study, we utilized an instructionless paradigm to collect eye movement data from 21 participants, examining two FER processes: free viewing and grounded FER. We analyzed fixational, pupillary, and microsaccadic events from eye movements, establishing their correlation with emotion perception and performance in the grounded task. By identifying regions of interest on the face, we explored the impact of eye-gaze strategies on face processing, their connection to emotions, and performance in emotion perception. During free viewing, participants displayed specific attention patterns for various emotions. In grounded tasks, where emotions were interpreted based on words, we assessed performance and contextual understanding. Notably, gaze patterns during free viewing predicted success in grounded FER tasks, underscoring the significance of initial gaze behavior. We also employed features from pre-trained deep-learning models for face recognition to enhance the scalability and comparability of attention analysis during free viewing across different datasets and populations. This method facilitated the prediction and modeling of individual emotion perception performance from minimal observations. Our findings advance the understanding of the link between eye movements and emotion perception, with implications for psychology, human-computer interaction, and affective computing, and pave the way for developing precise emotion recognition systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotion recognition (FER) is fundamental to human social interactions, enabling individuals to decipher emotions from facial expressions  [1] ,  [2] . While method-ologies like brain imaging and physiological signals have been employed to probe this complex process  [3] -  [5] , eye tracking emerges as a non-invasive yet insightful tool, offering deep insights into visual attention and emotional processing  [6] .\n\nPrevious research has leveraged eye movements to understand emotion perception (EP) in adults  [7] ,  [8] , shedding light on atypical EP associated with conditions such as autism, Attention-Deficit/Hyperactivity Disorder (ADHD), schizophrenia, and certain types of dementia  [9] ,  [10] . Diagnostic eye-tracking tasks, such as the antisaccade task, play a crucial role in identifying diseases like dementia and Alzheimer's  [11] ,  [12] .\n\nThe shift towards naturalistic tasks, mirroring real-life scenarios, gains attraction due to their relaxed environment, making them suitable for lightweight EP assessments. Eye-tracking research extends its applications to clinical diagnosis and humanrobot interaction (HRI), with potential implications for designing socially intelligent robots  [13] . In this chapter, we extend the work we have done in  [14]  and delve more into FER using an instructionless paradigm, exploring two FER processes: free viewing and grounded FER. We aim to unravel the intricate relationship between eye movements and emotion perception by scrutinizing fixational, pupillary, and microsaccadic events extracted from eye movement data. By integrating these gaze features with deeplearning models, our study uniquely combines the analysis of microsaccadic events and advanced computational techniques to enhance the understanding of emotional processing.\n\nBy delineating regions of interest in the face and harnessing deep-learning models for face recognition, we investigate the role of eye-gaze strategies in face processing and their link to presented emotions and emotion perception performance. Leveraging features extracted from pre-trained deep-learning models, we analyze attention during free viewing, enhancing scalability and comparability across datasets and populations. Furthermore, we employ a sequential model with bidirectional LSTM layers to capture the temporal aspects of fixation between regions of interest, providing insights into the dynamic nature of gaze behavior during face viewing.\n\nOur study contributes to the efficiency of EP assessments by predicting FER success based on gaze features during face viewing. This approach not only enhances the precision and ecological validity of emotion recognition systems but also has significant implications for clinical diagnostics and human-computer interaction. By extending the understanding of the eye movements-emotion perception relationship, our work impacts psychology, HCI, and affective computing domains, offering potential advancements in FER research and applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Instructionless Fer Task",
      "text": "This section outlines the instructionless FER task initially developed by Russell et al.  [10]  and our modifications to enhance the understanding of the FER process.\n\nRussell et al. designed the task to identify early-stage frontotemporal dementia, comprising:\n\n(1) Displaying four faces with distinct emotions for 10 seconds (2) Presenting an emotion word for 2 seconds (3) Showing the word and faces together for 3 seconds This setup aimed to intuitively direct participants' gaze to the face matching the emotion if they remembered its position. The constant positions of the faces simulated a memory retrieval task, which helped analyze free-viewing and retrieval phases and considered working memory as a potential confounder.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Modifications To The Instructionless Fer Task",
      "text": "To deepen our insight into the FER process and mitigate the effect of working memory, we adjusted Russell et al.'s design by randomizing face positions in Step 3. This required participants to recognize rather than recall face positions, effectively isolating the role of memory. This alteration helped distinguish between the free-viewing (Step 1) and grounded FER (Step 3) phases.\n\nThrough these changes, we analyzed gaze behavior and performance variations between these phases, generating quantitative data on cognitive processes in FER. These adjustments better simulate real-world scenarios, enhancing our study of the links among eye movements, emotion perception, and attention in FER tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Microsaccades",
      "text": "Microsaccades are small, involuntary eye movements that occur during visual fixation, typically lasting 6-30 milliseconds and with amplitudes under 0.1 degree of visual angle. Interspersed with slow drifts, they prevent retinal image fading and are vital for tasks requiring sustained visual attention, like reading  [15] . Studies suggest that microsaccadic activity responds to emotional stimuli, affecting attention and emotion-related cognitive processes. Emotional arousal, fatigue, and saccade preparation can influence microsaccades, altering their rate and magnitude depending on the emotional context  [16] . While some research reports significant changes in microsaccadic behavior in response to different emotions, others find no noticeable differences  [17] . Further studies link microsaccades with cognitive effort, affective priming, and arousal, highlighting their sensitivity to these factors and their role in emotional and cognitive processing  [17] . This underlines the importance of microsaccades in understanding visual attention and emotion perception.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eye Gaze Strategies",
      "text": "Studies indicate that eye movements during emotion recognition in faces follow both stimulus-driven and goal-driven perceptual strategies  [18] . Different facial regions contain varying levels of useful information for distinguishing emotions. For example, joyful faces may draw attention to the lips, while sad faces may attract attention to the eyes. These fixation patterns are influenced by attention to the most diagnostic regions of the face for each emotion, indicating a goal-driven influence on gaze patterns  [19] . Furthermore, gaze direction can modulate emotion perception in facial expres-   sions, influencing how emotions are perceived based on gaze direction. A study found that faces with averted gaze were rated higher overall in terms of perceived likelihood of experiencing emotion compared to direct gaze faces, demonstrating an interaction between gaze direction and perceived emotional disposition  [20] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Participants",
      "text": "We recruited 21 volunteers with normal or corrected vision using glasses or lenses who reported no history of attention deficits or cognitive impairments. One participant was excluded from the analysis due to missing information in some trials. The remaining 20 participants had completed education ranging from high school to PhD, with MSc being the mode. Table  1  provides an overview of other demographics. The information statement form, which was approved by the legal department of our institution, was signed by all participants.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Apparatus",
      "text": "We utilized the Eyelink 1000 Plus eye tracker from SR Research for recording eye movements during FER tasks. Participants were seated in a dark room with their chin stabilized on a chin rest. An 18-inch high-resolution display screen with a resolution of 1024 x 768 pixels was placed 70 cm from the participants. Before each experiment, we performed a 9-point calibration for the eye tracker, followed by a drift correction between trial rounds to ensure accuracy. Recalibration was done whenever the accuracy dropped below the desired threshold.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Stimuli",
      "text": "The NimStim face emotion dataset  [21]  was employed, generating 60 trials involving four emotive facial images and one emotion word per trial, aiming for a balanced representation of target emotions and diversity in facial identities. We randomized the positions of the faces and the target face to promote effective FER. Each face, sized 200 x 200 pixels, was placed towards the screen corners. The emotion word appeared in a 40-point Times New Roman font, centrally positioned. A fixation cross was shown for 200 ms at the start of each trial to center participant attention.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Areas Of Interest",
      "text": "Areas of interest included the four facial images and their corresponding words per trial. Within each face, we defined subareas for the eyes, nose, and mouth based on a CNN-based landmark detection model  [22] . These landmarks helped segment the faces into seven regions: mouth, both eyebrows, both eyes, nose, and jaw, further grouped into three categories: eye, nose, and mouth regions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Protocol",
      "text": "Participants naturally viewed the screen without specific instructions. After six preliminary trials for acclimation, they completed two rounds of 27 trials each, with a short break in between. The total duration was about 15 minutes. Post-experiment, participants could opt to complete the Reading Minds in the Eyes test online to assess their emotion decoding ability. Fifteen participants took this test, with their performance detailed in Table  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Preprocessing And Cleaning",
      "text": "Data from both eyes were collected, but fixation detection relied on the eye with the highest accuracy, confirmed through drift checks. For other analyses, we used binocular data or eye averages. The first six trials were excluded to eliminate initial bias. Fixations were assigned to the nearest area of interest, ensuring clear and consistent data interpretation across trials, simplifying analysis and enhancing data reliability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Microsaccade Extraction Algorithm",
      "text": "To extract microsaccades from eye-tracking data, we implemented an algorithm that processes velocity and acceleration thresholds to identify significant eye movements. The algorithm preprocesses the data for both left and right eyes, extracts relevant features, and iterates through the samples to detect microsaccades based on predefined velocity and acceleration thresholds. Detected microsaccades are validated by their duration and spatial displacement criteria. The complete algorithm is outlined in Algorithm 1.  Following the guidelines suggested by Skaramagkas et al.  [6] , we employed the dwell time percentage (dwell time %) as our primary measure for assessing visual attention. This metric calculates the focus duration on a specific area of interest (AOI) as a percentage of the total time spent on a particular step. The change in dwell time for target faces, as proposed by Russell et al.  [10] , was used to gauge emotion perception (EP) performance:\n\nResults indicate a pattern where participants significantly focus more on the target face after the emotion word is presented, aligning with findings from previous studies  [10] ,  [21] ,  [23] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis Methods",
      "text": "Chi-Square and ANOVA tests were used to analyze the relationship between categorical and numerical variables respectively, with Bonferroni correction applied to control for the risk of Type I errors across multiple comparisons  [24] ,  [25] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Significant Findings",
      "text": "• Target Emotion: Variations in fixation duration and microsaccade activity highlighted differences influenced by the emotion depicted on the target faces. • Face Regions: Significant disparities in microsaccade rates across different facial regions underscore their importance in analyzing facial emotions. • Interest Periods: Observable differences in microsaccades and pupil size among different steps suggest varying cognitive demands, which can inform more nuanced analyses in emotion perception studies. • Participants: The diversity in participant responses to identical stimuli illustrates the challenge in crafting a universally applicable model for emotion perception but also highlights the potential of personalized data analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance Analysis",
      "text": "The analysis revealed that fear generally resulted in lower performance scores, whereas happiness was associated with higher scores. This variability demonstrates differences in the efficiency of emotion recognition among participants. .: Distribution of fixational, microsaccadic, and pupillary events across different emotions, showing the variability and potential connections between these events and emotion perception.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experimental Observations",
      "text": "In Step 2, participants intuitively sought to match the emotion word to the corresponding face. The emotion word and the position of the target face attracted the most attention, indicative of a memory effect. Specifically, the position of the target face received more focus, particularly for emotions like sadness, fear, and surprise, as depicted in Figure  3 .\n\nIn Step 3, we noticed a new FER process where target faces (M = 40.9, SD = 20.2) received significantly more attention than non-target faces (M = 15.7, SD = 11.8) (t(4318) = 64.2, p < 0.0001). Non-target faces showing fear and surprise still attracted more attention than other non-target faces. Our results align well with previous studies that found participants tend to fixate longer on emotional faces, especially fearful and surprised ones, during daily communication.\n\nFigure  5  depicts the relative distribution of eye, nose, and mouth regions of target and non-target faces in step 3 and all faces in step 1. The results reveal distinct attentional strategies for different emotions depending on the task requirements, whether free or emotion-guided observation. For instance, consistent with the findings of Polet et al.  [23] , the eye region of surprised, fearful, and sad faces appears more crucial than that of other emotions in Step 1. Interestingly, this effect is even more pronounced when recognizing emotions in 3. Additionally, the mouth region is more critical for recognizing anger and disgust in 1 and, to a lesser extent, 3, compared to other emo-",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Tions.",
      "text": "This comprehensive analysis highlights the intricate dynamics of eye movements in relation to emotion perception, underscoring the significance of detailed attention to microsaccades and fixation patterns across different stages of the emotion recognition task.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Modeling",
      "text": "We analyzed data from 20 participants over 54 trials, deriving 54 input-output pairs by averaging fixation events. We employed leave-one-out cross-validation due to the limited dataset size and focused on mean squared error (MSE) as our main performance metric.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Hyperparameter Selection And Justification",
      "text": "Our choice of hyperparameters for deep-learning models was informed by literature, grid search, and practical constraints, aiming to balance complexity and overfitting risk given our small dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Task 1",
      "text": "Task 1 predicted dwell times for each face in Step 3 from Step 1's fixation data. This could improve instruction-free FER tasks reflecting everyday interactions. We predicted using spatial and temporal features of fixation, such as dwell time percentage and fixation counts, and emotion-related features, across four faces into a 3-layer Multilayer Perceptron (MLP) with 32-16-4 nodes. The model, trained for 500 epochs at a learning rate of 0.001, emphasized target face predictions by weighting them higher in the loss function.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Task 2",
      "text": "This task forecasted fixation dwell times using only visual features of faces. Employing a pre-trained VGG-Face model, we extracted 2622-dimensional embeddings as inputs for a 3-layer MLP (100-16-4 nodes). The larger first layer managed the highdimensional data, reducing it progressively. The model ran for 1000 epochs with a learning rate of 0.001, focusing separately on data from Steps 1 and 3, prioritizing target face features. Microsaccades and pupil size show significant differences, indicating the varying cognitive load and task complexity in each step.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Task 3",
      "text": "Task 3 aimed to predict individual user performance from eye movement events in Step 1. We calculated microsaccade rates and average pupil sizes from seven facial regions of interest for each emotion, using a 64-unit bidirectional LSTM for 100 epochs to capture temporal patterns. Performance prediction was handled by an XGBoost model, chosen for its effectiveness with structured data.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Baseline Model",
      "text": "The baseline model assumed the target face attracted the most attention, allocating the longest viewing time to it and dividing the remainder equally among other faces. This strategy provided a dwell time distribution of 0.50 for the target and 0.1666 for non-target faces in Tasks 1 and 2's Step 3, and equal times in Task 2's Step 1.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Results",
      "text": "Results indicated successful dwell time predictions with low MSE rates in both tasks. Task 1 showed that temporal features outperformed spatial ones, suggesting the importance of temporal data in modeling complex FER tasks. Task 2 confirmed the difficulty of emotion-related task predictions, with Step 3 posing greater challenges Table  5  displays the results for performance detection models as discussed in the previous section. We utilized mean squared error as a metric for evaluating the results, although it may not provide a comprehensive understanding of the model's performance quality. Therefore, we employed correlation as an evaluation metric to demonstrate the degree of correlation between predicted and true performances. Contrary to our expectations, the BiLSTM sequential model performed worse than the XGBoost model. However, neither of the models exhibited strong performance. As demonstrated in the statistical analysis, developing a single model capable of accurately predicting performance for all users is a highly challenging task.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "We adapted Russell et al.'s (2021) instructionless Facial Expression Recognition (FER) task to delve deeper into the FER process. These modifications facilitated extensive statistical analysis and revealed crucial disparities in processing various emotions.\n\nBy dividing the emotion processing into two steps-free-viewing and emotion grounding we gained deeper insights into emotion perception. We demonstrated the ability to predict individuals' performance solely from features observed during the free-viewing steps, aligning with eye-gaze strategies for facial emotion perception. Our findings indicate that gaze events, particularly temporal features, can predict FER performance by merely observing faces. We also forecasted the fixation duration of FER tasks based on facial visual features, aiding in the assessment of trial difficulty. Notably, we predicted emotion perception accuracy from free face viewing, marking progress towards lightweight emotion recognition assessments not dependent on language skills.\n\nAs depicted in Figure  11 , the difficulty levels of trials vary significantly. The work conducted in task 2 is instrumental in evaluating the difficulty level of any given trial. Additionally, our statistical analysis revealed substantial variations in participants' performances, reflected in their fixational, microsaccadic, and pupillary activities. Understanding these differences requires approximating the average participant, which the work in task 1 provides. Therefore, tasks 1 and 2 furnish information about trial difficulty and average fixation distribution, thereby informing performance. Task 3 predicts individual performance based on free face viewing, amalgamating all this information to offer a comprehensive emotion analysis of a user and enabling the modeling of users solely from their data during free face processing. This provides a naturalistic platform that can implicitly learn from users while they engage in their natural tasks.\n\nMoreover, we introduced a standardized tool for FER datasets, enhancing the comparability of results. Overall, our work offers insights for FER research and could shape the development of more naturalistic emotion recognition assessments.\n\nWhile the temporal modeling of eye gaze strategies presents a promising avenue, further investigation is warranted. Specifically, the extraction of compatible events and an increase in data collection could significantly enhance performance. Another factor to consider is the challenge posed by having four faces on the screen simultaneously, which can make it difficult to distinguish between regions of interest across different faces. Designing a new setting where each face is presented separately could address this issue and provide more valid temporal data on gaze strategies. Additionally, incorporating participant annotations of perceived emotions could enrich the analysis by providing a more valid ground truth for modeling emotion perception.\n\nIn conclusion, our work advances FER by exploring new paradigms and models. Predicting FER performance from free-viewing eye movements offers a pathway for efficient and ecologically valid emotion perception assessments. We anticipate that our work will inspire further research and foster improved tools and methodologies for studying human emotion perception.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: : The network structure designed for predicting fixations in Task 2. The model",
      "page": 2
    },
    {
      "caption": "Figure 2: : The FER task unfolds in three steps: (1) overlaying areas of interest, (2)",
      "page": 3
    },
    {
      "caption": "Figure 3: : The heatmaps display FER trial with no instructions. Different emotions",
      "page": 3
    },
    {
      "caption": "Figure 4: : Microsaccades across different steps. Step 1 shows baseline microsaccadic",
      "page": 5
    },
    {
      "caption": "Figure 5: : Fixation distributions over different face regions (eye, nose, and mouth)",
      "page": 10
    },
    {
      "caption": "Figure 6: : Distribution of fixational, microsaccadic, and pupillary events across differ-",
      "page": 12
    },
    {
      "caption": "Figure 3: In Step 3, we noticed a new FER process where target faces (M = 40.9, SD = 20.2)",
      "page": 12
    },
    {
      "caption": "Figure 5: depicts the relative distribution of eye, nose, and mouth regions of target",
      "page": 12
    },
    {
      "caption": "Figure 7: : Comparison of fixation duration, microsaccade rate, and duration across",
      "page": 13
    },
    {
      "caption": "Figure 8: : Microsaccade rate variations across different face regions, highlighting the",
      "page": 14
    },
    {
      "caption": "Figure 9: : Comparison of numerical events across different interest periods (steps).",
      "page": 15
    },
    {
      "caption": "Figure 10: : Variability in pupillary, microsaccadic, and fixational activities among par-",
      "page": 16
    },
    {
      "caption": "Figure 11: : Performance analysis across participants, trials, and emotions. The anal-",
      "page": 17
    },
    {
      "caption": "Figure 11: , the difficulty levels of trials vary significantly. The work",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Emotions"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Fixation index in trial\nAverage pupil size\nAverage of both eyes microsaccade’s rate\nBinocular microsaccade’s rate\nBinocular microsaccade average duration\nFixation duration"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "RoI Label"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Fixation index in trial\nAverage pupil size\nAverage of both eyes microsaccade’s rate\nBinocular microsaccade’s rate\nBinocular microsaccade average duration\nFixation duration"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Face Region"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Fixation index in trial\nAverage pupil size\nAverage of both eyes microsaccade’s rate\nBinocular microsaccade’s rate\nBinocular microsaccade average duration\nFixation duration"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Participant ID"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Fixation index in trial\nAverage pupil size\nAverage of both eyes microsaccade’s rate\nBinocular microsaccade’s rate\nBinocular microsaccade average duration\nFixation duration"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Interest Period Index"
        },
        {
          "Variable\nCategory\nF-statistic\nP-value (ANOVA)": "Fixation index in trial\nAverage pupil size\nAverage of both eyes microsaccade’s rate\nBinocular microsaccade’s rate\nBinocular microsaccade average duration\nFixation duration"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: : Chi-Square Test Results between categorical variables. The results indicate",
      "data": [
        {
          "Variable\nCategory\nChi-Square\nP-value": "Emotions"
        },
        {
          "Variable\nCategory\nChi-Square\nP-value": "Emotions\nRoI Label\n398.9696\n4.5144 × 10−66\nEmotions\nTarget Emotion\n11232.5746\n0.0*\nEmotions\nFace region\n286.1583\n1.3057 × 10−55"
        },
        {
          "Variable\nCategory\nChi-Square\nP-value": "RoI Label"
        },
        {
          "Variable\nCategory\nChi-Square\nP-value": "RoI Label\nTarget Emotion\n68.7352\n0.0001*\nRoI Label\nFace region\n83284.0\n0.0*"
        },
        {
          "Variable\nCategory\nChi-Square\nP-value": "Target Emotion"
        },
        {
          "Variable\nCategory\nChi-Square\nP-value": "Target Emotion\nFace region\n7.2468\n0.7020"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions and social development: Infants' recognition of emotions in others",
      "authors": [
        "A Walker-Andrews"
      ],
      "year": "1998",
      "venue": "Pediatrics"
    },
    {
      "citation_id": "2",
      "title": "Transmitting and decoding facial expressions",
      "authors": [
        "M Smith",
        "G Cottrell",
        "F Gosselin",
        "P Schyns"
      ],
      "year": "2005",
      "venue": "Psychological science"
    },
    {
      "citation_id": "3",
      "title": "Facial emotion recognition in child psychiatry: A systematic review",
      "authors": [
        "L Collin",
        "J Bindra",
        "M Raju",
        "C Gillberg",
        "H Minnis"
      ],
      "year": "2013",
      "venue": "Research in developmental disabilities"
    },
    {
      "citation_id": "4",
      "title": "Revealing real-time emotional responses: A personalized assessment based on heartbeat dynamics",
      "authors": [
        "G Valenza",
        "L Citi",
        "A Lanatá",
        "E Scilingo",
        "R Barbieri"
      ],
      "year": "2014",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "5",
      "title": "The pupil as a measure of emotional arousal and autonomic activation",
      "authors": [
        "M Bradley",
        "L Miccoli",
        "M Escrig",
        "P Lang"
      ],
      "year": "2008",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "6",
      "title": "Review of eye tracking metrics involved in emotional and cognitive processes",
      "authors": [
        "V Skaramagkas",
        "G Giannakakis",
        "E Ktistakis"
      ],
      "year": "2021",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "7",
      "title": "Neural networks for emotion recognition based on eye tracking data",
      "authors": [
        "C Aracena",
        "S Basterrech",
        "V Snáel",
        "J Velásquez"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Systems, Man, and Cybernetics",
      "doi": "10.1109/SMC.2015.460"
    },
    {
      "citation_id": "8",
      "title": "Gaze behavior consistency among older and younger adults when looking at emotional faces",
      "authors": [
        "L Chaby",
        "I Hupont",
        "M Avril",
        "V Luherne-Du Boullay",
        "M Chetouani"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "9",
      "title": "Eye-tracking study on facial emotion recognition tasks in individuals with high-functioning autism spectrum disorders",
      "authors": [
        "V Tsang"
      ],
      "year": "2018",
      "venue": "Autism"
    },
    {
      "citation_id": "10",
      "title": "Novel instructionless eye tracking tasks identify emotion recognition deficits in frontotemporal dementia",
      "authors": [
        "L Russell",
        "C Greaves",
        "R Convery"
      ],
      "year": "2021",
      "venue": "Alzheimer's research & therapy"
    },
    {
      "citation_id": "11",
      "title": "The potential of naturalistic eye movement tasks in the diagnosis of alzheimer's disease: A review",
      "authors": [
        "M Readman",
        "M Polden",
        "M Gibbs",
        "L Wareing",
        "T Crawford"
      ],
      "year": "2021",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "12",
      "title": "Eye movements in frontotemporal dementia: Abnormalities of fixation, saccades and anti-saccades",
      "authors": [
        "L Russell",
        "C Greaves",
        "R Convery"
      ],
      "year": "2021",
      "venue": "Alzheimer's & Dementia: Translational Research & Clinical Interventions"
    },
    {
      "citation_id": "13",
      "title": "A preliminary study on realizing human-robot mental comforting dialogue via sharing experience emotionally",
      "authors": [
        "C Fu",
        "Q Deng",
        "J Shen",
        "H Mahzoon",
        "H Ishiguro"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Gaze reveals emotion perception: Insights from modelling naturalistic face viewing",
      "authors": [
        "M Seikavandi",
        "M Barret"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "15",
      "title": "Temporal sampling and representation updating",
      "authors": [
        "C Howard"
      ],
      "year": "2017",
      "venue": "Temporal sampling and representation updating"
    },
    {
      "citation_id": "16",
      "title": "Microsaccadic modulation evoked by emotional events",
      "authors": [
        "K Kashihara"
      ],
      "year": "2020",
      "venue": "Journal of Physiological Anthropology"
    },
    {
      "citation_id": "17",
      "title": "Pupil dilation but not microsaccade rate robustly reveals decision formation",
      "authors": [
        "C Strauch",
        "L Greiter",
        "A Huckauf"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "18",
      "title": "Developmental eye movement strategies for decoding facial expressions of emotion",
      "authors": [
        "H Rodger",
        "N Sokhn",
        "J Lao",
        "Y Liu",
        "R Caldara"
      ],
      "year": "2023",
      "venue": "Journal of Experimental Child Psychology"
    },
    {
      "citation_id": "19",
      "title": "Eye movements during emotion recognition in faces",
      "authors": [
        "M Schurgin",
        "S Nelson",
        "Iida",
        "J Ohira",
        "S Chiao",
        "Franconeri"
      ],
      "year": "2014",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "20",
      "title": "Emotional gaze: The effects of gaze direction on the perception of facial emotions",
      "authors": [
        "J Liang",
        "Y.-Q Zou",
        "S.-Y Liang",
        "Y.-W Wu",
        "W.-J Yan"
      ],
      "year": "2021",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "21",
      "title": "The nimstim set of facial expressions: Judgments from untrained research participants",
      "authors": [
        "N Tottenham",
        "J Tanaka",
        "A Leon"
      ],
      "year": "2009",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "22",
      "title": "Convolutional experts constrained local model for 3d facial landmark detection",
      "authors": [
        "A Zadeh",
        "Y Lim",
        "T Baltrusaitis",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "23",
      "title": "Eye-gaze strategies during facial emotion recognition in neurodegenerative diseases and links with neuropsychiatric disorders",
      "authors": [
        "K Polet",
        "S Hesse",
        "A Morisot"
      ],
      "year": "2022",
      "venue": "Cognitive and Behavioral Neurology"
    },
    {
      "citation_id": "24",
      "title": "Introduction to linear regression analysis",
      "authors": [
        "D Montgomery",
        "E Peck",
        "G Vining"
      ],
      "year": "2021",
      "venue": "Introduction to linear regression analysis"
    },
    {
      "citation_id": "25",
      "title": "Teoria statistica delle classi e calcolo delle probabilita",
      "authors": [
        "C Bonferroni"
      ],
      "year": "1936",
      "venue": "Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze"
    }
  ]
}