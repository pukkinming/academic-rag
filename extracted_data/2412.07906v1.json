{
  "paper_id": "2412.07906v1",
  "title": "Rethinking Emotion Annotations In The Era Of Large Language Models",
  "published": "2024-12-10T20:30:51Z",
  "authors": [
    "Minxue Niu",
    "Yara El-Tawil",
    "Amrit Romana",
    "Emily Mower Provost"
  ],
  "keywords": [
    "Emotion Recognition",
    "LLMs",
    "Annotation",
    "Crowdsourcing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Modern affective computing systems rely heavily on datasets with human-annotated emotion labels, for training and evaluation. However, human annotations are expensive to obtain, sensitive to study design, and difficult to quality control, because of the subjective nature of emotions. Meanwhile, Large Language Models (LLMs) have shown remarkable performance on many Natural Language Understanding tasks, emerging as a promising tool for text annotation. In this work, we analyze the complexities of emotion annotation in the context of LLMs, focusing on GPT-4 as a leading model. In our experiments, GPT-4 achieves high ratings in a human evaluation study, painting a more positive picture than previous work, in which human labels served as the only ground truth. On the other hand, we observe differences between human and GPT-4 emotion perception, underscoring the importance of human input in annotation studies. To harness GPT-4's strength while preserving human perspective, we explore two ways of integrating GPT-4 into emotion annotation pipelines, showing its potential to flag low-quality labels, reduce the workload of human annotators, and improve downstream model learning performance and efficiency. Together, our findings highlight opportunities for new emotion labeling practices and suggest the use of LLMs as a promising tool to aid human annotation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "T HE field of affective computing is focused on build- ing systems that \"relate to, arise from, or deliberately influence emotion\"  [1] . It is a promising way to create better interactions for humans with technology  [2] . Human emotion understanding is beneficial in various fields, such as education  [3] , healthcare  [4] , and many others  [5] . In recent years, we have seen significant performance advancements in emotion recognition models, especially with the popularity of deep learning models  [6] . However, these models rely heavily on data with human-annotated emotion labels, which are costly in terms of time and resources and difficult to obtain due to the inherent ambiguity of emotions. Currently, there is no standard approach to annotation, as datasets often adopt different protocols at each phase of annotation collection, such as label selection, annotation formats, evaluation methods, etc. In the meantime, recent advances in LLMs have opened new avenues for textbased annotation. In this work, we explore these emotion annotation choices within the context of LLMs, examining how LLMs perform on emotion classification tasks and how they might address existing challenges and provide new perspectives on emotion annotation processes.\n\nEmotions are inherently ambiguous and subjective  [7] ,  [8] , posing great challenges in the design of annotation studies. Low agreement is often observed among annotators  [9] . Annotation outcomes are sensitive to even small changes in study design, such as the label space offered, including the size of the label space and type of emotions presented (see Section 2.1), as well as how the text and labels are presented to a human annotator (see Section 2.2)  [9] . These changes can all lead to different annotation outcomes  [10] -  [13] . This lack of consistency raises serious concerns about the reliability of emotion labels  [14] ,  [15] . Additionally, emotion perception naturally differs from person to person, influenced by individual experiences and demographic factors  [16] -  [19] , making it difficult to identify actual errors from legitimate perceptual differences. As a result, it is also hard to apply quality control methods post hoc. Many studies have sought to improve annotation reliability by exploring factors such as label space selection  [11] , study design choices  [12] ,  [13] , annotation interface improvement  [20] , trade-offs between annotators' quality and quantity  [21] . However, establishing a general pipeline for consistent and reliable emotion labeling remains an open challenge.\n\nWith the impressive advances in LLMs, there is a growing interest in using LLMs for various tasks such as generation, assessment, filtering, and annotation  [22] ,  [23] . Related work has also found the emerging ability of LLMs to understand and interpret emotions (see Section 2.3). However, much of this research is based on individual datasets, each with its own specific label space  [24] -  [27] , leaving questions about the generalizability of findings across different label spaces. Further, current evaluations tend to benchmark LLMs against human emotion labels  [25] ,  [28] , which themselves may contain errors. In our previous work, we conducted a small-scale in-house evaluation study. We found that human evaluators often preferred GPT-4 annotations over traditional human labels, particularly on larger label spaces  [29] . While these findings provide valuable insights, further verification with larger samples, more annotators, and more comprehensive analysis is needed for a deeper understanding. Lastly, beyond fully human-driven or fully automated GPT-4-based annotation, a promising and underexplored direction is to integrate GPT-4 as a supporting component within the annotation pipeline.\n\nIn this work, we focus on two Research Questions. First, we ask how well GPT-4 performs on emotion recognition.\n\nTo address this, we conduct a human evaluation study to compare the zero-shot predictions of GPT-4 with human labels (Section 4). Interestingly, although automatic metrics indicate that GPT-4 performs no better than small supervised models trained on human labels, evaluators consistently prefer GPT-4 labels over human labels, showing a misalignment between automatic metrics and human perspectives. A closer inspection reveals that larger label spaces enable more precise descriptions of emotions, and GPT-4 especially excels at managing a wide range of options. This study expands on our previous work  [29]  with a larger sample size and more evaluators within a crowdsourcing environment, providing stronger support for our findings and deeper insights into the reasons behind human preferences.\n\nBuilding on this understanding of LLMs' emotion capabilities, we explore the second Research Question: Can GPT-4 help humans annotate emotions? We examine two strategies to incorporate GPT-4 into annotation pipelines (Section 5). While previous work has explored automatic pre-annotation as a process to narrow label choices for human annotators, these works have focused on singlelabel annotation and have relied on traditional text analysis tools, such as lexicons  [20] ,  [30] . In our study, we propose to leverage LLMs as a more advanced tool. We present a novel investigation into the feasibility of (1) employing GPT-4 as a pre-annotation filter to dynamically suggest appropriate labels, and (2) using GPT-4 as a post-annotation filter to flag samples with low-quality human labels. Our experiments find some clear advantages, such as enhancing model training outcomes and efficiency, reducing cognitive load on annotators, and preserving the granularity benefits of large emotion spaces. To the best of our knowledge, this is the first study to propose and evaluate the pre-filtering and post-filtering methods, showing encouraging results.\n\nThroughout our analysis, we carefully consider the complexities of emotion label spaces and the varying perspectives captured by different evaluation methods, yielding valuable insights for future emotion annotation practices. These findings advocate for thoughtful consideration of annotation design choices, highlighting the potential of LLMs as a powerful tool to leverage alongside human labelers to elevate the annotation process in emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Label Spaces",
      "text": "The complexity and ambiguity of emotion pose significant challenges in quantifying and labeling emotions for building emotion recognition systems. The most commonly used frameworks for describing emotions fall into two categories: categorical label space, where emotions are represented as one or more pre-defined categories (e.g., joy, sadness)  [31] , and dimensional label space, which conceptualizes emotions along continuous axes, such as valence (positive to negative) and activation (excited to calm)  [32] .\n\nWithin the emotion classification framework, selecting an appropriate set of emotion labels still takes much consideration. A common approach is to follow established theories of basic emotions. For example, Emobank  [33]  and DailyDialog  [34]  datasets adopt Ekman's theory of six basic emotions (i.e. Anger, Disgust, Fear, Happiness, Sadness, and Surprise)  [31] . Other works make small modifications based on existing theories; for example, ISEAR  [35]  removed Surprise while adding Shame and Guilt to their label set. Another common strategy is conducting pre-annotation studies to determine the most appropriate set of emotion labels for the target data. SemEval-2018 Task 1 ran pilot annotation and included 11 emotion classes  [36] . GoEmotions  [37] , with the goal of contributing to fine-grained emotion classification models, settled on 27 classes after an iterative refinement process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Challenges In Obtaining Human Annotations",
      "text": "Obtaining high-quality, reliable human emotion annotations is a nontrivial task. It is common to see low agreement among annotators (e.g., the unanimous agreement can easily be below 10% in some datasets  [9] ). One reason for the low agreement lies in the inherent subjectivity of the task  [16] . Research has found that demographic factors, such as gender  [17] , age  [18]  , and race  [19] , significantly affect how emotions are perceived. As a result, a lack of diversity among annotators may result in datasets failing to capture the full spectrum of emotional perspectives, potentially leading to biased data and models  [38] . In addition, many design choices can significantly affect the annotation experience and outcomes. For example, the choice of label spaces plays an important role  [11] . Larger label spaces include more diverse and nuanced options, allowing for more accurate descriptions of emotion. However, more options reduce the agreement between annotators, possibly amplifying perspective differences or causing annotation fatigue  [9] . The availability of context is another key factor. Providing context during annotation generally helps reduce repetition, ease the task, and produce annotations more aligned with speakers' self-reported emotions  [12] ,  [13] . However, contextual influence can introduce inconsistencies, as variations in sample order affect annotators' judgments  [10] . Finally, the effort and attention devoted to the task varies significantly by individuals. A study evaluating annotation quality across four crowdsourcing platforms revealed that roughly half of the participants failed at least one attention check, with failure rates reaching 72.9% on the least reliable platform  [39] . In summary, human annotations are subjective and sensitive, and the quality is often far from perfect.\n\nEvaluating the quality of obtained labels is also challenged by the ambiguous nature of emotion. Without ground-truth labels, agreement metrics have been used as a major quality indicator or as a criterion to remove potentially low-quality samples/annotations  [40] . However, a higher level of agreement does not necessarily indicate more meaningful labels  [9] : it can result from reduced diversity in annotations. Another way to evaluate annotations is to put them in use -to train models with those labels and measure the performance on a test set  [9] ,  [21] . However, this approach relies on the assumption that \"golden\" labels of high quality and reliability are available in the test setan assumption many existing datasets fail to meet. Finally, human annotations are expensive, requiring significant time and effort, often involving recruitment, training, and extensive post-analysis  [7] ,  [35] . In some cases, multiple iterations are necessary for reliable results  [37] . As models grow in size, the cost of data collection increases further due to the need for more data to adequately train them  [41] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Emotional Capability Of Llms",
      "text": "Previous work has found that through conversational interactions, LLMs show emerging emotional intelligence  [42] : they can recognize sentiment  [28] , analyze the cause of emotions  [43] ,  [44] , and engage in dialogues with empathy  [43] ,  [45] . The natural question that follows is whether they can be used to annotate emotions in a structured manner, adhering to predefined labels and producing consistent outputs. Existing work has examined the zero-shot emotion recognition performance of various LLMs, from smaller open-sourced models like RoBERTa  [46]  to larger commercial models like GPT-3.5/4  [47] , generally finding reasonable performance. However, different evaluation criteria have led to different findings: many studies use human annotations as ground-truth  [25] ,  [28] , and find that LLMs do not outperform smaller, supervised models, particularly on complex tasks with numerous emotion labels. On the other hand, preliminary studies incorporating human evaluators in their assessment have shown more promising results  [47] . Our own work, which conducted a small-scale human evaluation study comparing GPT-4 and human labels, also reported more positive findings on LLM performance compared to humans  [29] . Further, some initial results suggest that LLMs are worse at larger label spaces than small, welldefined ones  [46] ,  [48] . Still, this effect is under-explored, and it is not clear whether this is inherent in LLMs or can be mitigated through proper prompting methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We use three existing English Emotion Classification datasets. They are all commonly used datasets to evaluate emotion models, covering diverse domains, topics, and different levels of granularities of emotion classes. Table  1  shows a summary of the datasets and label spaces.  [35]  was collected as part of a research project that aimed to study emotional experiences across cultures. The dataset contains more than 7000 self-reported descriptions of emotional experiences in English from participants in 27 countries, each describing emotional experiences in one of seven categories (listed in Table  1 ). We randomly split it into 60% train/20% dev/20% test sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "International Survey On Emotion Antecedents And Reactions (Isear)",
      "text": "SemEval 2018 Task 1 (SemEval)  [36]  is part of a multilingual affect analysis task released at the International Workshop on Semantic Evaluation. We take the English subset from the Emotion Classification subtask (E-c), where each tweet is annotated with zero, one or more labels from eleven emotion classes. The annotations were collected by crowdsourcing. The dataset was released with train/dev/test splits.\n\nGoEmotions  [37]  is a large-scale multilabel emotion classification dataset consisting of over 58,000 English Reddit comments annotated for 27 emotion categories (plus a neutral category) through crowdsourcing. GoEmotions is notable for its large data size and label granularity, offering a rich resource for fine-grained emotion classification. We also use its released train/dev/test splits.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gpt-4'S Emotion Annotation Capability",
      "text": "In this section, we evaluate GPT-4's emotion annotation capabilities through a crowdsourcing-based human evaluation study, assessing its alignment with human perceptions. We provide a comprehensive analysis with a focus on the disagreements between human and GPT-4 annotations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gpt-4 Prompting",
      "text": "To evaluate the zero-shot emotion recognition capability of GPT-4, we first query its predictions for all three datasets using the Microsoft Azure API. We employ an instructiondriven approach  [49] : we prompt GPT-4 with a text sample, a list of emotion labels, and task-specific instructions. The instructions ask GPT-4 to identify the appropriate label(s) from the provided list and ensure its outputs follow a predefined format that can later be parsed with rule-based post-processing. The instructions are designed to mirror those given to human annotators, creating a consistent and comparable task framework. Additionally, we enhance the prompts by establishing a persona at the start, which has been found beneficial in some work  [50] .\n\nWe used the following prompt for multilabel emotion classification  [29]  on the SemEval dataset: GPT-4 prompt for emotion classification \"You are an emotionally-intelligent and empathetic agent. You will be given a piece of text, and your task is to identify all the emotions expressed by the writer of the text. You are only allowed to make selections from the following emotions, and don't use any other words: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust. Only select those ones for which you are reasonably confident that they are expressed in the text. If no emotion is clearly expressed, reply with 'neutral'. Reply with only the list of emotions, separated by comma.\" For ISEAR and GoEmotions, we made minimal adjustments to the prompt to reflect different emotion options and task settings (i.e., whether multilabel is allowed). In rare cases where the output did not follow the specified format and could not be parsed, we retried with the same query. GPT-4 also has content policies and may refuse potentially harmful or sensitive content 1 . We excluded those samples from our analysis (ISEAR 3.7%, Semeval 4.5%, GoEmotions 2.6%).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Human Evaluation Study",
      "text": "Given the inherent ambiguity of emotion and the absence of absolute \"truth\" labels, human judgment remains essential for evaluation in this domain. We conduct a human evaluation study, engaging a separate group of humans (we refer to them as \"evaluators\", to differentiate from \"annotators\" who provided the label annotations in the datasets) to assess how accurately GPT-4 and human annotations reflect the emotions in text.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sample Selection",
      "text": "We selected 500 samples from the test split of each dataset for the human evaluation study, to be consistent with our evaluation-only study design. Due to the imbalanced label distributions in SemEval and GoEmotions, we applied weighted sampling with log inverse frequency as the weights to encourage a more representative inclusion of different emotions. We removed samples that were rejected by GPT-4 due to its content policy (17 in ISEAR, 12 in SemEval, 14 in GoEmotions). Since one of our main goals is to investigate the differences between their annotations, we dropped samples where the two sources gave the exact same label(s). This left 990 samples (out of 1500) for human evaluation: 124 from ISEAR, 438 from SemEval, and 438 from GoEmotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Crowdsourcing Experiments",
      "text": "We design a human evaluation study with the goal of comparing and understanding the disagreement between human and GPT-4 annotations. We present the evaluators with text samples alongside labels from both GPT-4 and 1. https://openai.com/policies/usage-policies/ human annotators, randomized and without revealing their source. We ask them to provide feedback on three aspects: Emotional Ambiguity. We ask \"Do you feel confident that you can describe the emotion expressed in the sentence(s)?\" with three options \"Yes\", \"No\" and \"Maybe\".\n\nPerceived Accuracy. We then present annotations from both sources (as Option A or Option B) and ask the evaluators to rate \"How accurately do you think that the description in Option A/B reflects the text writer's emotion?\" on a 7-point Likert scale (1-totally inaccurate, 7-totally accurate).\n\nPreference. Finally, to make a direct comparison, we ask \"If you have to choose one, which emotion description do you agree more with?\"\n\nWe aimed to obtain three evaluations on each sample. Each evaluator was assigned 50 samples, to keep session time manageable. To minimize potential confounds, participants were restricted to a single entry (across all studies in this paper). We implemented the annotation interface with Potato  [51] , a web-based text annotation tool. We hosted the annotation webpage on an AWS server and recruited participants from Prolific. The participants were selected to be native speakers of American English, at least 18 years old and live in the United States. They were informed that the goal of this study was to understand how people interpret emotional expressions in text, and they all provided their consent to participate. We received 2948 evaluations from 59 participants (968 samples got three annotations on each, and 22 samples only got two due to connection issues). The average completion time was 20 minutes 42 seconds, resulting in an average compensation of $11.60/hour.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label Distributions And Agreement Analysis",
      "text": "We first analyze the label distributions and disagreements between human and GPT annotations. We visualized the disagreements with confusion matrices in Figure  1 . For clarity and to compare across datasets, we only show results with the five emotion classes that are shared in all three datasets: anger, disgust, fear, joy and sadness. For multilabel datasets, we define the confusion matrix based on the overlap and differences between human and GPT-4 annotations: if an emotion is present in both sets, we increase the count in the diagonal of the matrix for that emotion. If an emotion is present in the human labels but not  in the GPT-4 annotations, and another emotion is present in the GPT-4 annotations but not in the human labels, we increase the count in the off-diagonal cell corresponding to the two emotions by one. For example, if the human labels on a sample are {admiration, joy} and GPT-4 set is {joy, love, excitement}, we record the agreement on the diagonal element of joy-joy, and we record confusion of admirationlove and admiration-excitement.\n\nWe see that most samples fall on the diagonal of the confusion matrices, indicating that GPT-4 annotations generally align with human annotations. Besides, as would have been expected, it is more common to see disagreements between similar emotion labels: confusion between a positive emotion (e.g., joy) and a negative one (e.g., anger) is less common than confusion between two negative emotions (e.g., anger and disgust). Finally, we notice that the confusion matrices are largely asymmetric. For example, in the ISEAR dataset, GPT-4 more often takes humanperceived anger as sadness (260 samples) than the reverse (27 samples). Such differences, however, do not generalize across datasets: the same anger-sadness confusion is shown in SemEval, but in GoEmotions the numbers are closer and the direction is reversed. These findings suggest potential perspective differences between GPT-4 and humans, specific to datasets, emotion categories, and annotation processes. This observation aligns with previous research showing a significant performance variation across emotions  [25] , which has been attributed to the sensitivity of LLMs to word choice and usage. We leave more in-depth explorations on this perspective difference, for example identifying the factors contributing to the (directions) of the difference, for future work.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Human Evaluation Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preference",
      "text": "We first look at the responses to the \"preference\" question. Figure  2  shows the votes for human versus GPT-4 annotations. GPT-4 annotations were significantly more preferred than human annotations (overall 62%), and this trend held across all three datasets (ISEAR 60.7%, SemEval 58.2%, GoEmotions 66.4%). We also ran per-evaluator statistics to test the between-person consistency. The vast majority (53 out of 59 evaluators, 89.8%) preferred GPT annotations on more samples, while three (5.1%) preferred human annotations more and three (5.1%) indicate equal preference. Interestingly, in our previous work, we compared GPT-4 predictions to those from smaller models finetuned on human labels and found comparable performance when human labels were used as the ground truth  [29] . However, the results of this human evaluation study present a more favorable picture for GPT-4. This conveys an important message that the common method that evaluates LLMs against human labels  [25] ,  [28]  is prone to underestimate their performance and may give misleading results. We should rethink the concept of \"ground-truth\" in emotion recognition tasks, especially as LLMs approach human performance. Further, comparing the datasets, we find that the preference discrepancy is larger in GoEmotions, where the label space is larger. We hypothesize that as the label space gets larger and more complicated, humans may be more challenged and tend to make more mistakes due to the increased cognitive load  [52] , while GPT-4 is less affected, especially with proper prompting methods. We discuss this hypothesis in more detail in Section 4.4.3, and we run follow-up human annotation studies to further explore the complexity of the label space as a factor in human and GPT-4 performance (Section 5).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Perceived Accuracy Ratings",
      "text": "We then look at the individual perceived accuracy ratings and compare those on GPT-4 versus human annotations, to gain more insights into human preference results. We compare the total number of samples that fall into each rating category, as shown in Figure  3 . The results reveal a clear and consistent advantage for GPT-4 annotations: human annotators generated a greater number of labels deemed inaccurate (Rating ≤ 3, Human 24.3% vs. GPT-4 15.1%), suggesting a higher probability of errors. In contrast, GPT-4 demonstrates stronger performance in identifying emotions deemed totally accurate by evaluators, indicating good comprehension of the complexity of emotion labels and subtlety of emotion expressions. We further compare the accuracy ratings on each dataset in Table  2 . We see that the trend also holds on each dataset, adding to the robustness of our findings. What's more, as the label space expands, both human annotators and GPT-4 are more likely to produce labels rated as fully accurate (see last row in Table  2 , across all datasets). This behavior is both reasonable and desirable, as when the label space is limited, it lacks the necessary granularity to capture subtle emotional distinctions, making it impossible to provide perfectly accurate descriptions. In contrast, a larger label space increases the likelihood of encompassing the correct label(s), thus facilitating \"totally accurate\" outcomes. In Section 5, we will further explore the influence of label space complexity with a dataset-controlled annotation study.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Confidence And Agreement",
      "text": "We assess the confidence and agreement among human evaluators to understand the perceived ambiguity of this task and perceptual differences across evaluators. When asked if they could confidently describe the emotions expressed in the text, evaluators responded \"Yes\" for 74% of the samples, \"No\" 18.2%, and \"Maybe\" 7.7%. Although most samples were found to convey clear emotions, evaluators disagreed a lot on their preference: among annotations marked with confidence, only 59.2% of samples with two annotations had agreement (i.e., both evaluators preferred the same label source), and 40.5% of samples with three annotations had agreement. This highlights significant variation in emotion perception: even when selecting between two options, agreement remains relatively low.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gpt-4 Weakness Analysis",
      "text": "We also analyze the samples to understand whether and how certain text features may affect GPT-4's emotion classification performance. We extracted the Linguistic Inquiry and Word Count (LIWC)  [53]  frequencies from each sample. LIWC is used to analyze text for psychological and linguistic content. It quantifies the occurrences of 73 word categories in a text, including words that convey emotional and psychological states (e.g., positive emotion, fear), as well as semantic information (e.g., adverb, conjunction)  [54] .\n\nWe augmented LIWC with five additional semantic features commonly used for Twitter data  [55] ,  [56] : text length, word count, emoji count, hashtag count, and mention count (tagging another user with \"@\"). These features allow us to examine if certain types of emotional or semantic content are more likely to mislead or challenge LLMs. We ran a Logistic Regression (LR) model (N=990) using the text features as input and the preference from the human evaluation study as the outcome variable (1 if GPT-4 labels were preferred over human labels by majority vote, 0 otherwise). We first ran independent t-tests on individual features for feature selection  [57]  and kept the 10 features with lowest p-values as the input to our model.\n\nWe found that higher frequencies of mentions, prepositions, future focus and interrogatives had a significant (p < 0.05) negative effect on GPT-4 being the preferred annotator, while the use of impersonal pronouns positively predicted the preference for GPT-4. Prior research has highlighted challenges for LLMs in understanding temporal constructs  [58]  and social cues beyond the text  [59] , which may explain some of these patterns. However, given the limited size of our data, further investigation is needed to interpret these findings meaningfully.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Feasibility Of Gpt-4 Aided Emotion Anno-Tation",
      "text": "Our experiments in Section 4 reveal the potential of GPT-4 in emotion recognition. However, we also identified its weaknesses. One notable concern is the instability and unpredictability that often accompany LLMs. They are known to be sensitive to both training data and prompting methods, which can greatly impact their performance  [60] . Therefore, using GPT-4 to perform annotations without human oversight can be risky. Furthermore, as shown in Figure  1  and Section 4.3, there may be systematic differences between GPT-4 and human perspectives. While it is crucial to avoid human errors, we also want annotations to accurately reflect human perspectives. Therefore, in this section, we propose and evaluate two methods for incorporating GPT-4 into emotion annotation pipelines, with the goal of harnessing the strength of both sources. We focus on the GoEmotions dataset for the coverage of diverse emotions in its samples.\n\nWe consider two ways to use GPT-4 in emotion labeling pipelines: using GPT-4 as a pre-annotation label filter to dynamically present a smaller set of classes to human annotators, and, on existing datasets, using GPT-4 as a sample filter to flag potentially low-quality samples. Below we describe each method and our evaluation experiments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Filtering, Label-Level",
      "text": "There is a trade-off between the benefit of larger label spaces and increased cognitive load (Section 4.3 and 4.4.2, also  [11] ). We hypothesize that we can reduce cognitive load while preserving label diversity by using GPT-4 to dynamically drop unlikely labels for each sample before presenting them to human annotators. We again prompt GPT-4 in a zero-shot manner with text samples and a list of emotion options. Note that since humans will make selections from GPT-4 filtered labels, the goal of the filter step is to include all possible classes; it is less important to avoid false positives as they can later be identified by human annotators. Therefore, instead of asking it to make selections, we ask it to go through the emotions one by one and indicate if each is possibly expressed in the sample. We provide the list of emotion options along with the text samples instead of in the general instruction. In a preliminary analysis of a small exploration set, we found that those changes in our prompting methods encouraged the inclusion of more labels and significantly reduced false negatives.\n\nGPT-4 prompt for emotion Pre-filtering \"You are an emotionally intelligent and empathetic agent. You will be given a piece of text and a list of emotions. Your task is to determine which emotions are present in the text. Please go through the emotion list one by one and think about if the emotion is possibly present in the text. Please respond with each emotion plus \"yes\" to indicate it's possibly present, or \"no\" to indicate it's definitely not present. If you are not 100% sure, please select \"yes\". Reply with only the list of emotions words plus your response, separated by newline.\"",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Setup",
      "text": "To evaluate the feasibility of this approach, we conducted human-annotation experiments on the same set of samples but three different label space setups: 1) Small: We take the 11 emotion classes from SemEval to represent a relatively small label space. 2) Large: We take the union of the emotion classes from SemEval and GoEmotions (30 classes in total), to represent an extensive set of emotion labels. 3) GPT-4 Pre-filtered: We take the large set in 2) and reduce it with GPT-4, as described above.\n\nWe use a between-subject design: the label sets are fixed for both the Small and Large sets, where participants may gradually gain familiarity with the labels. If we mix those setups in one annotation session, such familiarity cannot be reflected. Therefore, we assign each participant to one of three groups, each having the same set of samples and one of the three label sets. We ask the annotators to select all applicable labels from the label list, plus an extra \"None of the above / Others\" option. We also include a question of whether they feel restricted by the options and would use other words to describe the emotion(s).\n\nWe took the set of 486 GoEmotions samples we used for the evaluation study (Section 4.2.1). We further removed samples that GPT-4 didn't output any candidates (N = 4) and defaulted the labels to \"neutral\". We recruited 29 annotators for each group (i.e., Small, Large, and GPT-4 Prefiltered) through crowdsourcing. We used the same crowdsourcing platform and setups as described in Section 4.2.2. Each participant annotated 50 samples, and each sample got 3 annotations.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "For the evaluation of the pre-filtering setup, we focus on three aspects: 1) cognitive load, indicated by subjective reports and time to completion; 2) label reliability, indicated by the agreement level among annotators; and 3) label coverage, i.e., the pre-filtered set should reasonably cover the labels human annotators selected from the Large set.\n\nCognitive Load and Annotators' Experience. Following common approaches, we measured the cognitive load of the annotators in two aspects: perceived load  [61] , as a subjective measure, and time to completion  [62] , as an objective measure. We used the NASA Task Load Index  [63]  as our cognitive load scale. We removed two questions that were not directly relevant in our task (physical demand and temporal demand), and we asked the participants to rate their feelings on four aspects on a 7-point scale at the end of their session: Mental Demand, Confidence, Effort, and Frustration. We found that a large label space significantly increased the mental demand of the annotators compared to the small set (see Figure  4a ). However, a small label space did have a drawback: annotators reported feeling restricted by the options in 11.6% of samples on the Large set, 13.9% on Pre-filter, 33.5% on Small. Consequently, significantly lower confidence was reported on the Small set (5.72±1.25, Large 6.17±1.20, Pre-filter 6.31±0.76). No significant differences were found in Effort and Frustration. We also compared the time the annotators spent on each sample. We excluded samples that took more than 60 seconds, as they were outliers in the time distribution and likely indicated a pause in the task. Annotators spent an average of 17.41 seconds on the Pre-filter set and 18.02 seconds on the Small set, while much longer (25.04 seconds) on the Large set (Figure  4b ).\n\nAgreement. We use the Jaccard Index (JI)  [64]  to measure the agreement between two annotators on each sample. JI is an agreement measure for multi-label classification tasks, defined by the size of the intersection of two label sets divided by their union. We calculated the average JI among pairs of annotators on each sample and the average across samples on each set. The Small set has the highest agreement of 0.29±0.34, slightly higher but not significantly different from the Pre-filter set (0.28±0.30, independent t-test p=0.36). The Large set has the lowest JI of 0.20±0.24, significantly lower than the Pre-filtered and Small sets (both p<0.05). We note that a smaller set tends to see higher agreement, so the JIs among sets are not directly comparable. However, from a crowdsourcing perspective, reasonable agreement indicates  reliability and is generally favorable  [65] . Label Coverage. Finally, we evaluate whether the filtering step retains potentially correct labels, i.e., the labels selected by humans in the Large set group. Following the approach of GoEmotions  [37] , we obtain aggregated labels from each set by using emotion classes that are selected by at least two annotators out of three. If no emotion class is selected for one sample, it is defaulted to \"neutral\". We first compared the chosen class labels with the Pre-filtered candidates, and found an average of 90.19% of the labels selected in the Large set were included in the Pre-filter set, indicating a reasonably low false-negative rate. In addition, the labels annotators chosen from the Pre-filter set have an agreement of 0.30±0.32 JI with the Large set, which is comparable to (even slightly higher than) the within-set agreement levels, and is much higher than the agreement between the Small and the Large sets (0.15±0.28, p<0.05).\n\nTogether, the results show that the Pre-filtered set can match the advantages of both the Small and the Large sets: it maintains a low cognitive load and shorter time to completion (which often indicates lower cost) while allowing more descriptive accuracy and confidence of the annotators.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Post-Filtering, Sample-Level",
      "text": "In Section 5.1.2, we show some benefits of using GPT-4 for pre-annotation to collect new labels. In this section, we investigate a second approach: when a dataset with humanannotated labels is available, we propose to use GPT-4 as a quality checker to filter out potentially low-quality labels. Specifically, we compare the labels from human and GPT-4 annotation and drop the samples where the two sources totally disagree: i.e., they selected different labels for singlelabel classification datasets, or where they do not contain any overlapping labels for multi-label classification datasets. By applying this filtering step to GoEmotions, we obtained a much smaller Filtered set of 16,592 samples (out of 42,287).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Setup",
      "text": "Since the post-filtering step removes samples where GPT-4 and human annotations disagree, it is expected to remove samples with mistakes in annotations, resulting in higherquality labels. While this generally benefits model training, this filtering step also decreases the number of samples and potentially the diversity or ambiguity in the samples. Therefore, a key question is whether this trade-off eventually enhances or hurts model training outcomes. To evaluate this, we train smaller models with either the whole labeled GoEmotions training set, or the smaller Filtered set. We measure the performance on its test set as an indicator of the usefulness of the labels. We report performance on both the whole test set and a filtered test set.\n\nBase model selection. We choose two models from the BERT family for our training experiments: BERT  [66]  and DistilBERT  [67] . BERT is one of the earliest transformerbased LLMs that gained broad attention, and it has been used as a baseline for many NLU tasks  [68] , including in the GoEmotions paper  [37] . With 110 million parameters, BERT is significantly smaller than leading LLMs like GPT-4, making it practical for use on most modern GPUs. Dis-tilBERT is a distilled version of the BERT model with a 40% reduction in the number of parameters while delivering comparable performance in multiple NLU tasks. We compare the models trained on the original human labeled set versus the Filtered set where samples that GPT-4 totally disagree with are dropped. We finetune the models for 30 epochs with a learning rate of 1e-5, and we select the best model measured by performance on the validation set. We use our annotated ground-truth set as the primary test set for evaluation, while we also report the performance on the test split of the human and post-filtered set.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "Results are presented in Table  3 . The filtered set, despite comprising less than 40% of the samples in the full set, consistently led to better model performance across models (both BERT and DistilBERT) and both the full and filtered test sets (with one exception of the F1 score when tested in-domain on the human-labeled set). To isolate the effect of training sample size reduction, we included a training set that was randomly sampled from the Human set and matched the size of the Filter set  (16, 592) . As expected, this smaller set led to a performance drop, with all metrics lower compared to the full Human set. This further highlights the effectiveness of our post-filtering approach, which achieved better performance with much fewer samples. Together, these results show the potential of GPT-4 to flag possibly low-quality samples, thus improving model performance as well as training efficiency.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "Our work examines many design choices involved in emotion annotation and investigates how LLMs, specifically GPT-4, perform in this context and where they may offer new possibilities. In the first part of our study (Section 4), we evaluated GPT-4's ability to classify emotions across three datasets with varying domains and label space complexity. We found that GPT-4 predictions generally align with human-annotated labels. In addition, a human evaluation study revealed preferences for GPT-4 labels over original human annotations, highlighting the value of humancentered evaluations and raising questions about whether human labels should be used as the sole ground-truth for evaluating LLMs. We also compared GPT-4 and human labels across different label spaces. Results suggest that larger label spaces allow nuanced emotion descriptions, which are perceived as more accurate by human evaluators, while smaller spaces are less cognitively demanding and can potentially lead to fewer human mistakes.\n\nIn the second part, we further explored ways to integrate GPT-4 into annotation processes, focusing on the GoEmotions dataset. We found that GPT-4 can serve as a pre-annotation label filter to dynamically exclude highly unlikely labels before presenting them as options to human annotators. Our human annotation study showed that, compared to traditional methods, GPT-4 could effectively reduce more than 70% of options while preserving more than 90% of human-selected labels. This approach leverages the expressivity of larger label spaces and the reduced cognitive load and higher annotator agreement associated with smaller label spaces. What's more, on annotated datasets, GPT-4 can act as a post-annotation sample filter to flag potentially low-quality labels. Models trained on the filtered dataset, although much smaller in training data size, achieved better performance than the original full set with human annotations.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "Through our studies, we encountered several challenging factors that can influence the collection and evaluation of emotion labels. First, our evaluation study (Section 4) reveals that using human labels as ground truth is not always reliable. However, we do not have a good alternative evaluation approach that can account for the inherent subjectivity in emotion. This limitation influenced our quantitative model evaluation in Section 5.2, where we relied on a human-annotated set created through multi-annotator discussions. Although we believe that this annotation method greatly reduces the possibility of mistakes, we also note the ambiguity of many samples, and thus reasonable alternative interpretations are possible. Developing new evaluation metrics that address these complexities would be a valuable, though challenging, direction for future research.\n\nWe also limited our discussion to classification tasks. Our previous work conducted preliminary experiments that included dimensional labels with the Emobank dataset  [29]  and found that the scale of GPT-4 output did not quite align with human labels, raising more questions for evaluation. LLMs are generally good at language-based interactions, and language anchors are helpful for them to understand dimension scales  [69] . As dimensional label spaces gain more popularity  [70] , future research could explore ways to better leverage LLMs in dimensional emotion annotation.\n\nAdditionally, our pre-and post-filtering methods (Section 5) serve as preliminary demonstrations of the feasibility and potential of GPT-4-assisted annotation rather than as definitive solutions. Future work could incorporate more refined approaches to further improve performance. While our focus is not on comparing different perspectives or methods in human annotation processes (e.g., self-reported vs. third-person annotations, or crowdsourcing vs. in-house annotations), previous studies have examined these aspects  [71] -  [73] . Finally, prompting techniques are not the focus of this paper, but we acknowledge the sensitivity of the models and the importance of good prompts. We direct interested readers to related studies  [60] ,  [74] -  [76] , and we encourage new explorations with our prompts and code publicly available 2 .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we conduct a comprehensive evaluation of GPT-4's emotion classification performance and its potential to aid annotation processes. We present encouraging results along with discussions on the complexities and challenges associated with various design choices in emotion annotation studies. Our findings underscore the importance of carefully rethinking these choices with LLMs' capability in mind. We highlight the need for evaluation metrics that better align with human perspectives and the strong promise of using LLMs as tools to aid annotation efforts.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Disagreements between Human and GPT-4 Annotations, visualized as confusion matrices.",
      "page": 4
    },
    {
      "caption": "Figure 2: Proportion of human evaluators’ votes favoring hu-",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the votes for human versus GPT-4 annota-",
      "page": 5
    },
    {
      "caption": "Figure 3: Perceived accuracy ratings on a scale of 1 (totally",
      "page": 5
    },
    {
      "caption": "Figure 3: The results reveal a clear",
      "page": 5
    },
    {
      "caption": "Figure 1: and Section 4.3, there may be systematic differences between",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of the cognitive load on different label",
      "page": 7
    },
    {
      "caption": "Figure 4: a). However, a small label space",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "1-Totally Inaccurate\n2\n3\n4\n5\n6\n7-Totally Accurate",
          "Human\nGPT": "217 (7.4%)\n98 (3.3%)\n245 (8.3%)\n155 (5.3%)\n255 (8.6%)\n193 (6.5%)\n491 (16.7%)\n364 (12.2%)\n577 (19.6%)\n498 (16.6%)\n704 (23.9%)\n463 (15.4%)\n566 (19.2%)\n879 (29.8%)",
          "ISEAR (7 classes)\nHuman\nGPT": "9.3%\n3.6%\n6.0%\n7.1%\n8.5%\n8.5%\n19.1%\n15.8%\n22.1%\n18.5%\n17.8%\n24.6%\n16.4%\n25.4%",
          "SemEval (11 classes)\nHuman\nGPT": "6.1%\n3.6%\n9.0%\n5.3%\n9.9%\n6.9%\n16.9%\n13.9%\n19.4%\n18.1%\n21.0%\n24.0%\n18.2%\n25.9%",
          "GoEmotions (27 classes)\nHuman\nGPT": "8.1%\n3.0%\n8.1%\n4.7%\n9.0%\n6.4%\n15.7%\n9.4%\n18.5%\n18.3%\n20.2%\n27.6%\n21.1%\n35.1%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Affective computing: Recent advances, challenges, and future trends",
      "authors": [
        "G Pei",
        "H Li",
        "Y Lu",
        "Y Wang",
        "S Hua",
        "T Li"
      ],
      "year": "2024",
      "venue": "Intelligent Computing"
    },
    {
      "citation_id": "3",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & education"
    },
    {
      "citation_id": "4",
      "title": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "authors": [
        "Y Liu",
        "K Wang",
        "L Wei",
        "J Chen",
        "Y Zhan",
        "D Tao",
        "Z Chen"
      ],
      "year": "2024",
      "venue": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "arxiv": "arXiv:2402.13589"
    },
    {
      "citation_id": "5",
      "title": "Applied Affective Computing",
      "authors": [
        "L Tian",
        "S Oviatt",
        "M Muszynski",
        "B Chamberlain",
        "J Healey",
        "A Sano"
      ],
      "year": "2022",
      "venue": "Applied Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Objective assessment of subjective tasks in crowdsourcing applications",
      "authors": [
        "G Haralabopoulos",
        "M Tsikandilakis",
        "M Torres",
        "D Mcauley"
      ],
      "year": "2020",
      "venue": "LREC 2020 Workshop on\" Citizen Linguistics in Language Resource Development"
    },
    {
      "citation_id": "8",
      "title": "The ambiguous world of emotion representation",
      "authors": [
        "V Sethu",
        "E Provost",
        "J Epps",
        "C Busso",
        "N Cummins",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "The ambiguous world of emotion representation",
      "arxiv": "arXiv:1909.00360"
    },
    {
      "citation_id": "9",
      "title": "Comparing the utility of different classification schemes for emotive language analysis",
      "authors": [
        "L Williams",
        "M Arribas-Ayllon",
        "A Artemiou",
        "I Spasić"
      ],
      "year": "2019",
      "venue": "Journal of Classification"
    },
    {
      "citation_id": "10",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "M Jaiswal",
        "Z Aldeneh",
        "C.-P Bara",
        "Y Luo",
        "M Burzo",
        "R Mihalcea",
        "E Provost"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Toward effective automatic recognition systems of emotion in speech",
      "authors": [
        "C Busso",
        "M Bulut",
        "S Narayanan",
        "J Gratch",
        "S Marsella"
      ],
      "year": "2013",
      "venue": "Social emotions in nature and artifact: emotions in human and human-computer interaction"
    },
    {
      "citation_id": "12",
      "title": "Challenges in annotation: Annotator experiences from a crowdsourced emotion annotation task",
      "authors": [
        "E Öhman"
      ],
      "year": "2020",
      "venue": "DHN"
    },
    {
      "citation_id": "13",
      "title": "Influence of contextual information in emotion annotation for spoken dialogue systems",
      "authors": [
        "Z Callejas",
        "R Lopez-Cozar"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "14",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "15",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "16",
      "title": "Challenges in reallife emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "17",
      "title": "Gender differences in nonverbal communication of emotion",
      "authors": [
        "J Hall",
        "J Carter",
        "T Horgan"
      ],
      "year": "2000",
      "venue": "Gender and emotion: Social psychological perspectives"
    },
    {
      "citation_id": "18",
      "title": "Emotion regulation and emotion perception in aging: A perspective on age-related differences and similarities",
      "authors": [
        "J Gurera",
        "D Isaacowitz"
      ],
      "year": "2019",
      "venue": "Progress in brain research"
    },
    {
      "citation_id": "19",
      "title": "Race and sex in the perception of emotion",
      "authors": [
        "A Gitter",
        "H Black",
        "D Mostofsky"
      ],
      "year": "1972",
      "venue": "Journal of Social Issues"
    },
    {
      "citation_id": "20",
      "title": "EmoLabel: Semi-automatic methodology for emotion annotation of social media text",
      "authors": [
        "L Canales",
        "W Daelemans",
        "E Boldrini",
        "P Martinez-Barco"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "21",
      "title": "Tradeoff between quality and quantity of emotional annotations to characterize expressive behaviors",
      "authors": [
        "A Burmania",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Large language models for data annotation: A survey",
      "authors": [
        "Z Tan",
        "D Li",
        "S Wang",
        "A Beigi",
        "B Jiang",
        "A Bhattacharjee",
        "M Karami",
        "J Li",
        "L Cheng",
        "H Liu"
      ],
      "year": "2024",
      "venue": "Large language models for data annotation: A survey",
      "arxiv": "arXiv:2402.13446"
    },
    {
      "citation_id": "23",
      "title": "ChatGPT outperforms crowd workers for text-annotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "24",
      "title": "Affect recognition in conversations using large language models",
      "authors": [
        "S Feng",
        "G Sun",
        "N Lubis",
        "C Zhang",
        "M Gašić"
      ],
      "year": "2023",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "25",
      "title": "Bias in emotion recognition with ChatGPT",
      "authors": [
        "N Wake",
        "A Kanehira",
        "K Sasabuchi",
        "J Takamatsu",
        "K Ikeuchi"
      ],
      "year": "2023",
      "venue": "Bias in emotion recognition with ChatGPT"
    },
    {
      "citation_id": "26",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "S Latif",
        "M Usama",
        "M Malik",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "arxiv": "arXiv:2307.06090"
    },
    {
      "citation_id": "27",
      "title": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "authors": [
        "Z Zhang",
        "L Peng",
        "T Pang",
        "J Han",
        "H Zhao",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "28",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL"
    },
    {
      "citation_id": "29",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "M Niu",
        "M Jaiswal",
        "E Provost"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Towards the improvement of automatic emotion pre-annotation with polarity and subjective information",
      "authors": [
        "L Canales ; Alicante",
        "W Spain",
        "E Daelemans",
        "P Boldrini",
        "Martínez-Barco"
      ],
      "year": "2017",
      "venue": "RANLP 2017 -Recent Advances in Natural Language Processing Meet Deep Learning"
    },
    {
      "citation_id": "31",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "32",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2017",
      "venue": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis"
    },
    {
      "citation_id": "34",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "35",
      "title": "How universal and specific is emotional experience? evidence from 27 countries on five continents",
      "authors": [
        "H Wallbott",
        "K Scherer"
      ],
      "year": "1986",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "36",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "S Mohammad",
        "F Bravo-Marquez",
        "M Salameh",
        "S Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "37",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "authors": [
        "T Bolukbasi",
        "K.-W Chang",
        "J Zou",
        "V Saligrama",
        "A Kalai"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "39",
      "title": "Beyond the turk: Alternative platforms for crowdsourcing behavioral research",
      "authors": [
        "E Peer",
        "L Brandimarte",
        "S Samat",
        "A Acquisti"
      ],
      "year": "2017",
      "venue": "Journal of experimental social psychology"
    },
    {
      "citation_id": "40",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Scaling laws for neural language models",
      "authors": [
        "J Kaplan",
        "S Mccandlish",
        "T Henighan",
        "T Brown",
        "B Chess",
        "R Child",
        "S Gray",
        "A Radford",
        "J Wu",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "42",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "43",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "44",
      "title": "Is GPT a computational model of emotion?",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "45",
      "title": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "authors": [
        "J.-T Huang",
        "M Lam",
        "E Li",
        "S Ren",
        "W Wang",
        "W Jiao",
        "Z Tu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "arxiv": "arXiv:2308.03656"
    },
    {
      "citation_id": "46",
      "title": "The biases of pretrained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Gpt-4 emulates average-human emotional cognition from a third-person perspective",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2024",
      "venue": "Gpt-4 emulates average-human emotional cognition from a third-person perspective",
      "arxiv": "arXiv:2408.13718"
    },
    {
      "citation_id": "48",
      "title": "Is gpt-3 a good data annotator",
      "authors": [
        "B Ding",
        "C Qin",
        "L Liu",
        "Y Chia",
        "B Li",
        "S Joty",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "50",
      "title": "Quantifying the persona effect in llm simulations",
      "authors": [
        "T Hu",
        "N Collier"
      ],
      "year": "2024",
      "venue": "Quantifying the persona effect in llm simulations",
      "arxiv": "arXiv:2402.10811"
    },
    {
      "citation_id": "51",
      "title": "Potato: The portable text annotation tool",
      "authors": [
        "J Pei",
        "A Ananthasubramaniam",
        "X Wang",
        "N Zhou",
        "A Dedeloudis",
        "J Sargent",
        "D Jurgens"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "52",
      "title": "Choice overload: A conceptual review and meta-analysis",
      "authors": [
        "A Chernev",
        "J Goodman"
      ],
      "year": "2015",
      "venue": "Journal of Consumer Psychology"
    },
    {
      "citation_id": "53",
      "title": "The development and psychometric properties of LIWC2015",
      "authors": [
        "J Pennebaker",
        "R Boyd",
        "K Jordan",
        "K Blackburn"
      ],
      "year": "2015",
      "venue": "The development and psychometric properties of LIWC2015"
    },
    {
      "citation_id": "54",
      "title": "Incorporating LIWC in neural networks to improve human trait and behavior analysis in low resource scenarios",
      "authors": [
        "I Kilic",
        "S Pan"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "55",
      "title": "A multi-layered psychologicalbased reference model for citizen need assessment using aipowered models",
      "authors": [
        "R Alharthi",
        "A Saddik"
      ],
      "year": "2020",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "56",
      "title": "Detecting fake news spreaders with behavioural, lexical and psycholinguistic features",
      "authors": [
        "H Bello",
        "L Heilmann",
        "E Ronan"
      ],
      "year": "2020",
      "venue": "CLEF (Working Notes)"
    },
    {
      "citation_id": "57",
      "title": "Common pitfalls in statistical analysis: logistic regression",
      "authors": [
        "P Ranganathan",
        "C Pramesh",
        "R Aggarwal"
      ],
      "year": "2017",
      "venue": "Perspectives in clinical research"
    },
    {
      "citation_id": "58",
      "title": "Timebench: A comprehensive evaluation of temporal reasoning abilities in large language models",
      "authors": [
        "Z Chu",
        "J Chen",
        "Q Chen",
        "W Yu",
        "H Wang",
        "M Liu",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Timebench: A comprehensive evaluation of temporal reasoning abilities in large language models",
      "arxiv": "arXiv:2311.17667"
    },
    {
      "citation_id": "59",
      "title": "Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark",
      "authors": [
        "M Choi",
        "J Pei",
        "S Kumar",
        "C Shu",
        "D Jurgens"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "60",
      "title": "Exploring the sensitivity of llms' decision-making capabilities: Insights from prompt variations and hyperparameters",
      "authors": [
        "M Loya",
        "D Sinha",
        "R Futrell"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "61",
      "title": "Nasa-task load index (nasa-tlx); 20 years later",
      "authors": [
        "S Hart"
      ],
      "year": "2006",
      "venue": "Proceedings of the human factors and ergonomics society annual meeting"
    },
    {
      "citation_id": "62",
      "title": "A comparison of emotion annotation approaches for text",
      "authors": [
        "I Wood",
        "J Mccrae",
        "V Andryushechkin",
        "P Buitelaar"
      ],
      "year": "2018",
      "venue": "Information"
    },
    {
      "citation_id": "63",
      "title": "Development of nasa-tlx (task load index): Results of empirical and theoretical research",
      "authors": [
        "S Hart"
      ],
      "year": "1988",
      "venue": "Human mental workload"
    },
    {
      "citation_id": "64",
      "title": "The distribution of the flora in the alpine zone. 1",
      "authors": [
        "P Jaccard"
      ],
      "year": "1912",
      "venue": "New phytologist"
    },
    {
      "citation_id": "65",
      "title": "How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation",
      "authors": [
        "S Nowak",
        "S Üger"
      ],
      "year": "2010",
      "venue": "Proceedings of the international conference on Multimedia information retrieval"
    },
    {
      "citation_id": "66",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "67",
      "title": "Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "68",
      "title": "Bert: a review of applications in natural language processing and understanding",
      "authors": [
        "M Koroteev"
      ],
      "year": "2021",
      "venue": "Bert: a review of applications in natural language processing and understanding",
      "arxiv": "arXiv:2103.11943"
    },
    {
      "citation_id": "69",
      "title": "Joint audio and speech understanding",
      "authors": [
        "Y Gong",
        "A Liu",
        "H Luo",
        "L Karlinsky",
        "J Glass"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "70",
      "title": "Emotion analysis as a regression problem-dimensional models and their implications on emotion representation and metrical evaluation",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2016",
      "venue": "ECAI 2016"
    },
    {
      "citation_id": "71",
      "title": "Tracing vocal expression of emotion along the speech chain: Do listeners perceive what speakers feel?",
      "authors": [
        "S Biersack",
        "V Kempe"
      ],
      "year": "2005",
      "venue": "ISCA Workshop on Plasticity in Speech Perception"
    },
    {
      "citation_id": "72",
      "title": "The expression and perception of emotions: comparing assessments of self versus others",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Interspeech"
    },
    {
      "citation_id": "73",
      "title": "Label quality in affectnet: results of crowd-based re-annotation",
      "authors": [
        "D Kim",
        "C Wallraven"
      ],
      "year": "2021",
      "venue": "Asian Conference on Pattern Recognition"
    },
    {
      "citation_id": "74",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "75",
      "title": "Using cognitive psychology to understand gpt-3",
      "authors": [
        "M Binz",
        "E Schulz"
      ],
      "year": "2023",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America"
    },
    {
      "citation_id": "76",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    }
  ]
}