{
  "paper_id": "2308.14160v1",
  "title": "A Unified Transformer-Based Network For Multimodal Emotion Recognition",
  "published": "2023-08-27T17:30:56Z",
  "authors": [
    "Kamran Ali",
    "Charles E. Hughes"
  ],
  "keywords": [
    "Emotion recognition",
    "Transformers",
    "Biosensors",
    "Multi-model representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The development of transformer-based models has resulted in significant advances in addressing various vision and NLP-based research challenges. However, the progress made in transformer-based methods has not been effectively applied to biosensing research. This paper presents a novel Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method to classify emotions in an arousal-valence space by combining a 2D representation of an ECG/PPG signal with the face information. To achieve this goal, we first investigate and compare the unimodal emotion recognition performance of three image-based representations of the ECG/PPG signal. We then present our UBVMT network which is trained to perform emotion recognition by combining the 2D image-based representation of the ECG/PPG signal and the facial expression features. Our unified transformer model consists of homogeneous transformer blocks that take as an input the 2D representation of the ECG/PPG signal and the corresponding face frame for emotion representation learning with minimal modality-specific design. Our UBVMT model is trained by reconstructing masked patches of video frames and 2D images of ECG/PPG signals, and contrastive modeling to align face and ECG/PPG data. Extensive experiments on the MAHNOB-HCI and DEAP datasets show that our Unified UBVMT-based model produces comparable results to the state-of-the-art techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "O VER the past few years, there has been an increas- ing interest in adopting a bio-sensing perspective for emotion analysis. Certainly, the popularity of bio-sensing research extends beyond affective computing. Various fields such as robotics  [1] ,  [2] , health  [3] ,  [4] , and virtual reality  [5]  have also embraced bio-sensing as a valuable research tool. Bio-sensing systems, including those that measure electrocardiogram (ECG), Photoplethysmography (PPG), electroencephalogram (EEG), galvanic skin response (GSR), etc, have been available for decades; however, their size and complexity have limited their use to controlled laboratory settings and hospitals. The emergence of wearable biosensing systems has fueled the recent interest in utilizing biosensing systems for various applications by simplifying and speeding up data collection.\n\nSeveral research studies have demonstrated the feasibility of recognizing human emotions through facial expressions captured in images and videos  [6] ,  [7] ,  [8] . Although facial expression recognition systems have shown remarkable success on databases captured under controlled conditions, their performance significantly degrades  [9] ,  [10]  when they are applied to real-life situations. The system's unreliability in natural environments is primarily due to various factors, such as varying head pose, illumination conditions, and occlusion of different parts of the face. Additionally, recognizing emotions from facial expressions may not be entirely dependable since they can be easily concealed or manipulated. Furthermore, facial expressions can  be influenced by social and cultural differences, as human expressiveness varies among individuals. In contrast, physiological signals offer a more accurate reflection of emotions and their subtle changes. Therefore, multimodal emotion recognition techniques that combine facial information with physiological data have the potential to compensate for the drawbacks of unimodal methods and achieve more precise recognition outcomes  [11] ,  [12] .\n\nMany emotion recognition techniques have been proposed in the past years combining different modalities. For instance, in  [13]  Shang et al. recognized emotions by fusing EEG, EOG, and EMG signals. Similarly, in  [14] , Miranda et al. combined EEG, ECG, and GSR modalities to perform emotion analysis. In  [15] ,  Koelstra et   EEG signal. However, several research studies show that there is a strong correlation between features extracted from the ECG signal and human emotions. For instance, in  [16] , Hany et al. extracted ECG features that represent the statistical distribution of dominant frequencies by applying spectrogram analysis to classify emotions. Similarly, in  [17] , time and frequency domain features are extracted from the ECG signal to perform emotion analysis. Building on those approaches, this paper presents a novel multimodal emotion recognition technique that fuses information from face video frames and ECG/PPG data. To the best of our knowledge, this is the first study that investigates the fusion of face information with the ECG/PPG signal to classify emotions.\n\nMany attempts have been made to create effective techniques for integrating multimodal information. Initially, methods involved simply combining high-level features from all modalities to predict outcomes (known as \"early fusion\") or adding up decisions made by each unimodal system with learnable weights (called \"late fusion\") to produce the final inference. While these methods outperformed unimodal techniques to some extent, the lack of inter-modality interactions during training limited the potential improvement. In recent times, Transformers  [18]  have emerged as a highly successful approach for multimodel representation learning. Due to the advances in attention mechanisms and transformers, later studies have focused predominantly on using these techniques to develop more sophisticated multimodal fusion methods  [19] ,  [20] ,  [21] ,  [22] ,  [23] . However, most of these transformer-based multimodal emotion recognition techniques leverage video, audio, and text modalities to learn joint emotion representation. In this paper, we present a unified transformer-based model that employs ECG/PPG data along with face information to recognize emotions. The main framework of the proposed method is shown in Figure  1 . Based on our literature survey, no previous work has investigated the application of transformers in learning emotion representation by using face and ECG/PPG modalities.\n\nInspired by the compact architecture of the recently published Textless Vision-Language Transformer (TVLT)  [22] , we present a Unified Biosensor-Vision Multimodal Transformer (UBVMT) network to recognize emotions by fusing face and ECG/PPG data. We first investigated the effectiveness of various image-based representations of ECG/PPG signals for biosensor-based unimodal emotion recognition. Several previous methods have employed an image-based representation of an ECG/PPG signal for heart rate estimation  [24] ,  [25] ,  [26] ,  [27] ,  [28] . For instance, in  [24] , Song et al. obtained a spatiotemporal representation of a pulse signal in a time-delayed way by constructing a Toeplitz matrix. The Toeplitz matrix is then converted into an image that is fed to a CNN to estimate heart rate information. The simplicity of the Toeplitz representation allows it to preserve both the morphological and chronological details of the 1D pulse signal. As a result, the CNN can accurately extract the correct HR values from input feature images.\n\nIn  [25] , average pooling is applied to various blocks within an ROI region. The spatiotemporal maps are then generated by arranging these temporal sequences into rows, which are then fed to a deep network to get HR values. In  [26] , the sample mean sequences of the R, G, and B channels from the ROI of videos are extracted, and Short-Time Fourier Transform (STFT) is applied to construct the 2D time-frequency representations of the sequences. In  [27] , a spatiotemporal map is first computed from the input video as a representation of the BVP signal, and the spatiotemporal images are then mapped to BVP signals using the generative adversarial learning technique. In  [28] , Khare et al. converted the time-domain EEG biosignals into a timefrequency representation by employing Smoothed Pseudo Wigner-Ville distribution (SPWVD).\n\nApart from the image-based representation of ECG signals for heart rate estimation, some works  [29]  and  [30]  have extracted deep learning features from the scalogram of biosensor signals to recognize cognitive load and hypertension risk stratification, respectively. They converted the PPG signals to Scalograms by using Continuous Wavelet Transform. Scalograms were used instead of spectrograms as the image representation for PPG signals. This is because scalograms provide better highlighting of the low-frequency or rapidly changing frequency components of the signal as compared to spectrograms.\n\nIn this paper, we first investigate and compare the performance of three image representations of the ECG/PPG signal for emotion recognition. More specifically, we perform unimodal emotion recognition by converting the time domain ECG/PPG signal into 1. spatiotemporal maps  [24] , 2. time-frequency representation by employing Smoothed Pseudo Wigner-Ville distribution (SPWVD)  [28] , and 3. Scalograms  [29]    [30] . We show that the best emotion recognition performance is obtained by employing the scalogram representation of the ECG and PPG signals. Therefore, our Unified Biosensor-Vision Multimodal Transformer (UBVMT) network is trained and validated by using the scalogram representation of the ECG/PPG signal along with the face images. The effectiveness of the proposed UBVMTbased multimodal emotion recognition technique is validated on two datasets: the MAHNOB-HCI  [31]  and DEAP  [32]  datasets. Extensive experiments show that the proposed approach produces comparable results to the state-of-the-art emotion recognition techniques. Overall, the main contributions of this paper are as follows:\n\n• Comparison of the performance of three imagebased representations, namely spatiotemporal maps, time-frequency representation, and scalograms of the ECG/PPG signal for emotion recognition, and evidence that scalograms better represent the emotion features contained by the ECG/PPG signals.\n\n• A novel technique that employs transformer architecture for biosensor-based multimodal emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "The Unified Biosensor-Vision Multimodal Transformer (UBVMT) network consisting of homogenous blocks that learn vision-and-biosensor emotion representation with minimal modality-specific design, thus, making it compact and applicable in real-time emotion analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Psychologically, human emotions can be characterized based on two main frameworks: categorical or dimensional representations. In the categorical framework of emotions, emotions are classified into distinct labels such as joy, sadness, anger, happiness, fear, and so on. This approach considers emotions as discrete categories rather than continuous dimensions. In dimensional conceptualizations of emotions, a commonly used framework involves representing emotions within a two-dimensional space. In this space, valence is positioned along one axis, while arousal is positioned along the other  [33] . Valence, often positioned on the horizontal axis, signifies the extent of pleasantness or unpleasantness. On the other hand, arousal, typically placed on the vertical axis, represents the level of activation or energy associated with the emotion. Within a two-dimensional (2D) space, as shown in Figure  2 , emotions are portrayed by their valence and arousal level. While the categorical approach to emotions is conceptually straightforward, it faces certain challenges. It struggles to represent compound emotions that do not fit neatly into a single category, and it does not provide a means to quantify the degree or intensity of an emotional state. Therefore, in this paper, a novel multimodal emotion recognition technique is developed to classify emotions along the valence-arousal axis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition With Ecg/Ppg Signals",
      "text": "Previous research shows that there is a strong correlation between ECG/PPG signals and human emotion. In  [34] , Yu et al. converted the rPPG signal of an input image into timefrequency domain spectrogram images to classify the shortterm emotions. In  [35] , Ismail et al. employed ECG and PPG signals to develop an emotion recognition system, and compared the performance of both signals for classifying emotions. Lee et al. in  [36]  investigated the ability of PPG signals to recognize emotions along the arousal-valence axis. Sep úlveda el al. in  [37]  performed emotion recognition from ECG signals using a wavelet scattering algorithm. Emotion features of the ECG signal are extracted at different time scales leveraging the wavelet scattering technique, and the extracted features are then fed into different classifiers to perform emotion analysis. Sarkar et al. in  [38]  exploited a self-supervised deep multi-task learning framework to develop an emotion recognition algorithm using the ECG signals. Mellouk et al. presented a rPPG signal-based emotion classification method using a deep learning architecture, which combines a one-dimensional convolution neural network (1DCNN) and a long short-term memory (LSTM)  [39] . In  [16]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition With Facial Images",
      "text": "Facial images and videos have been widely used by the research community to recognize emotions. Mostly, facial information is used to analyze emotions by using facial expression recognition techniques  [40] ,  [41] ,  [42] ,  [43] ,  [44] . However, in this paper, a transformer-based technique is presented that estimates the continuous dimensions related to emotions. Based on the dimensional approach, affective behavior can be characterized using various underlying continuous dimensions. These dimensions offer a more accurate representation of the emotions individuals experience in their daily lives. As noted in Figure  2 , two widely utilized dimensions are valence, which reflects the positivity or negativity of an emotional state, and arousal, which gauges the intensity of emotional activation  [45] ,  [46] ,  [47] .\n\nTo estimate valence and arousal from facial images, Dimitrios et al.  [48]  used a Convolutional and Recurrent (CNN-RNN) deep neural architecture to extract emotion features for dimensional emotion recognition. Deng et al.  [49]  used the ResNet-50 model for multi-task expression recognition and implemented the teacher-student architecture to enhance the training data without labels. This approach aims to address the imbalanced data distribution across different tasks. In  [50] , Kuhnke et al. utilized facial landmarks to align the image and eliminate conflicting data. They achieve this by leveraging the correlation among different representations to generate pseudo-labels. In  [51] , the features extracted from a 3D-CNN are inputted into the 3D VGG and 2D SENet-101 networks. Subsequently, Gated Recurrent Units (GRUs) are employed to enhance the model's ability to learn temporal features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "There have been many studies focusing on the analysis of multimodal systems for recognizing human emotions. other modalities such as EEG, EMG, GSR, etc., for multimodal emotion analysis. But, unlike the EEG signal, which has been combined with the facial information as proposed in  [15] ,  [57] ,  [58] , the ECG signal has not been fused with facial features to perform emotion recognition. In this paper, a Unified Biosensor-Vision Multimodal Transformer-based (UBVMT) emotion recognition technique is presented that integrates ECG/PPG data with facial information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Research Methods",
      "text": "This section discusses various 1D-to-2D transformation methods to extract emotion features from 2D representation of ECG/PPG signals. We then present our Unified Biosensor-Vision Multimodal Transformer (UBVMT) architecture that leverages the 2D representation of ECG/PPG signals along with face information to recognize emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The 2D Representation Of Ecg/Pgg Signals",
      "text": "In this paper, one of our goals is to find an effective method that can be used to transform the 1D time domain signal of ECG/PPG data into an image-based representation for emotion recognition. Therefore, in this section, we investigate the effectiveness of three different 1D-to-2D transformation techniques to perform 2D image-based emotion analysis using ECG/PPG data. More specifically, we present a unimodal emotion recognition network using ResNet-18  [62]  by converting the time domain ECG and PPG signals into (1) spatiotemporal maps  [24] , (2) time-frequency representation by employing Smoothed Pseudo Wigner-Ville distribution (SPWVD)  [28] , and (3) Scalograms  [29] ,  [30]  as shown in Figure  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spatiotemporal Representation Of Ecg/Ppg Signal",
      "text": "In this section, a novel emotion recognition technique using a spatiotemporal representation of an ECG/PPG signal is presented. Rencheng et al.  [24]  converted a 1D ECG and PPG signal into a 2D spatiotemporal map to estimate heart rate information. It has been reported that the performance degradation of the conventional PPG signals for heart rate estimation due to noise can be overcome by employing deep learning techniques on the spatiotemporal maps of the PPG signal  [24] . Therefore, in this section, we apply the technique proposed in  [24]  to convert the 1D ECG/PPG signal into a 2D spatiotemporal map and perform emotion analysis using the spatiotemporal maps as input to the classifier. The spatiotemporal feature maps can retain both the morphological and chronological features of ECG/PPG signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Construction Of Ecg/Ppg Spatiotemporal Map:",
      "text": "The spatiotemporal feature map of an ECG/PPG signal is constructed by creating a square Toeplitz matrix. Suppose the 1D input signal S = (s 1 , s 2 , ..., s P ) has P samples and P is an even number. The first row of the matrix consists of s 1 to s P/2 samples. Similarly, the second row contains samples from the second point, i.e., s 2 to the (P/2 + 1)th sample, and so on. Therefore, a square Toeplitz matrix T with a size equal to P/2 is constructed as:\n\nA gray image is constructed by converting the matrix T into an image representation. The obtained gray image has a clear structure because the input signal is quasiperiodic. The second row of Figure  4  shows the spatiotemporal image representation of 1D signals. As it can be seen in Figure  4  that the vertical patterns preserve the period information of the 1D signal. This suggests that the 2D Toeplitz representation can effectively capture and portray the periodic nature of a 1D signal. The morphological and chronological information of the 1D signal is preserved in this simple 2D spatiotemporal representation. Therefore, emotion features can be extracted from these 2D maps of the ECG/PPG signals. The spatiotemporal image representations of ECG/PPG signals were then adjusted to the size of 224 × 224 × 3, and fed to a pre-trained ResNet-18  [62]  to perform emotion analysis. Construction of ECG/PPG TF Map: SPWVD provides a straightforward depiction of the localization of signal energy in both time and frequency domains. The choice of the length and type of the cross-term reducing window in both the time and frequency domains can be made independently. Due to the independent selection of the length and type of the cross-term reducing window, SPWVD exhibits favorable time-frequency cluster characteristics. The representation of SPWVD in mathematical terms can be expressed as  [59] ,  [60] . To mitigate cross-terms in both time and frequency domains, the Kaiser window is employed. However, selecting a window size that is too small may lead to diminished resolution, while overly large windows can significantly increase the image size. Consequently, a medium-sized window with a length of 31, as suggested in  [28] , is chosen. For efficient computation, the window size is maintained at 2 n -1, where n represents the number of bits. The TF representations of time-domain ECG/PGG signals are then adjusted to the size of 224 × 224 × 3, and fed to a pre-trained ResNet-18  [62]  to perform emotion recognition. Construction of ECG/PPG scalograms: Continuous Wavelet Transform (CWT) has been used for decades as a valuable technique for analyzing both time and frequency information. In this paper, each segment of the ECG/PPG signal was transformed into a time-frequency representation, known as a scalogram, using the continuous wavelet transform method. A scalogram represents the absolute values of the continuous wavelet transform coefficients of a signal, displayed as a graph of time and frequency. In contrast to a spectrogram, a scalogram provides improved capability in identifying the low-frequency or rapidly changing frequency components of the signal. To convert the PPG signal into a scalogram, we obtained the absolute values of the wavelet coefficients for each signal segment using the analytic morse  (3, 60)  wavelet. The scalogram representation of ECG/PGG signals obtained by employing the CWT technique is shown in the fourth row of Figure  4 . The scalograms were then adjusted to the size of 224 × 224 × 3, and fed to a pre-trained ResNet-18  [62]  to perform emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Unified Transformer-Based Multi-Modal Network",
      "text": "In this section, we discuss our Unified Biosensor-Vision Multimodal Transformer (UBVMT) network where homogeneous transformer blocks take 2D representations of ECG/PPG signals and raw visual inputs for multi-modal emotion representation learning with minimal modalityspecific design. UBVMT is trained by employing masked autoencoding  [63]  and contrastive modeling  [22]  to learn effective emotion representation. The overall architecture of our Unified Biosensor-Vision Multi-modal Transformer (UB-VMT) network is shown in Figure  5 . The objective of masked autoencoding is to reconstruct masked patches of 2D representations of ECG/PPG signals and face video frames, while contrastive modeling is applied to align ECG/PPG and face information. We argue that, due to the unified architecture of UBVMT, the computational redundancy and complexity of our method is reduced as compared to conventional transformer-based multi-modal emotion recognition techniques.\n\nThe input to the UBVMT is the integration of face patch embedding, ECG/PPG patch embedding, and modality embedding. To obtain face embeddings, the face image of 224 × 224 × 3 pixels is divided into a list of 16 × 16 sized patches. The pixel values of each patch are normalized, and a linear projection layer is used to convert face patches into a 768-dimensional patch embedding. For an input face image of size 224 × 224 × 3, the size of the resultant face embedding is 14 × 14. The spatial embedding involves incorporating spatial information for each input patch by adding a distinct trainable vector to the height and width axes of the 14 × 14 embeddings. For ECG/PPG embedding, the 2D representation of an ECG/PPG signal is divided into patches, and a liner projection layer is applied on each patch to obtain a 768-dimensional patch embedding. Similar to the face modality, a patch size of 16 × 16 is used, and trainable temporal and frequency embeddings are utilized to denote the temporal and frequency information of the patches.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Architecture Of Ubvmt",
      "text": "Similar to the transformer model proposed in  [18] , UBVMT has an encoder E which has 12 layers (hidden size 768), and a decoder D with 8 layers (hidden size 512). After pretraining, we only employ the encoder E part of the transformer and finetune it for emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pretraining Ubvmt",
      "text": "UBVMT is pre-trained by employing masked autoencoding  [63]  and contrastive modeling  [22]  to learn effective emotion representation. The pretraining objective function L is the weighted sum of masked autoencoding loss L M and the contrastive modeling loss L C :\n\nwhere λ M = 0.4 and λ C = 1.\n\nMasked Autoencoding Objective Function: The main objective of masked autoencoding is to learn effective unimodal representations in biosensor-and-vision settings. This objective involves masking random patches of ECG/PPG images and the face video frames, allowing us to reconstruct missing inputs effectively. Specifically, we implement a random dropout mechanism on a portion of ECG/PPG embedding x B and face embedding x F . Subsequently, we input the remaining patch embeddings to the encoder E. To generate the input to the decoder D, we add the dropped embeddings as trainable vectors labeled as [MASK] and position them in the same locations as the original inputs (indicated by gray boxes in Figure  5 . Additionally, the corresponding positional, and frequency embeddings, which are separately parametrized, are incorporated into the decoder input. The objective function of masked autoencoding is a mean squared error between the reconstructed and original ECG/PPG images and face video frames:\n\n(4) where N B m and N F m are the number of masked patches for ECG/PPG images and face video frames, respectively. Loss L M is computed only on masked patches. Note that the ECG/PPG and face output of the encoder are fed to the decoder separately.\n\nContrastive Modeling Objective Function: The main objective of contrastive modeling is to perform ECG/PPG-vision matching and learn an effective cross-modal representation, as shown in Figure  5 . For every face image, a positive vision-ECG/PPG pair (x F + ; x B ) is generated. Additionally, we form half of the vision-ECG/PPG pairs within a batch as mismatched (negative) pairs (x F -; x B ) by substituting the face images x F + with randomly selected face images x F - from the training dataset.\n\nSimilar to previous multimodal transformers  [22] ,  [64] ,  [65] ,  [66] ,  [67] , we incorporate a linear layer with sigmoid activation as the classification head. This is applied to the encoder output of the first [CLS] token, resulting in the matching probability p by employing binary cross-entropy loss:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Details",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Database Description",
      "text": "The effectiveness of transformer models to learn multimodal representation is improved by pretraining them on large datasets  [65] ,  [66] ,  [67] . As such, UBVMT is pre-trained on the large, comprehensive CMU-MOSEI  [68]  dataset. Similar to  [24] ,  [25] ,  [69] , rPPG signals are extracted from the CMU-MOSEI video clips using the MTTS-CAN  [70]  method. After pertaining, MAHNOB-HCI  [31]  and DEAP  [32]  datasets are used for multimodal emotion analysis.\n\nThe CMU-MOSEI dataset comprises 23,454 movie review clips, totaling over 65.9 hours of YouTube video content. It features contributions from 1000 speakers and encompasses 250 distinct topics. Videos having non-human faces and faces with more than 80 • head rotations from the frontal position are discarded from the dataset. From each video, multiple video clips of 1.1 secs are extracted, with the number of clips being dependent on the length of each video. For our case, 90,037 video clips are extracted from the entire CMU-MOSEI dataset. These video clips are then fed to the MTTS-CAN  [70] , and only the pulse signals from the MTTS-CAN are used to obtain the rPPG signals of these video clips.\n\nThe MAHNOB-HCI  [31]  (Multimodal Human-Computer Interaction) dataset is a widely used publicly available dataset designed for research in affective computing and multimodal emotion recognition. The dataset includes 527 facial video recordings of 27 participants engaged in various tasks and interactions, while their physiological signals such as 32-channel electroencephalogram (EEG), 3-channel electrocardiogram (ECG), 1-channel galvanic skin response (GSR), and facial expressions are captured. The ECG signals were sampled at a rate of 256 Hz. The ECG data is precisely synchronized and aligned with the face video recordings. Similar to  [71] , only the EXG2 signal from the 3 ECG channel system is extracted for emotion analysis.\n\nThe Database for Emotion Analysis using Physiological Signals (DEAP)  [32]  is a widely used and publicly available database designed for research in emotion analysis and affective computing. The DEAP database contains data from 32 participants, aged between 19 and 37 (50% female), who were recorded watching 40 one-minute music videos. Each participant was asked to evaluate each video by assigning values from 1 to 9 for arousal, valence, dominance, like/dislike, and familiarity. Face video was recorded for 22 out of the 32 participants, and the proposed UBVMT method is evaluated using this particular group of subjects. For each dataset, we conduct a subject-independent 10-fold cross-validation evaluation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Pre-Processing Steps",
      "text": "The MAHNOB-HCI emotion elicitation data contains unstimulated baseline and stimulated response ECG signals. Furthermore, the database includes a synchronization signal that facilitates the separation of the two. Our experiments exclusively utilized the ECG signals recorded during the stimulated phase. To remove motion artifacts, the original signal is subtracted by the smoothing signal  [31] . Subsequently, similar to  [16] ,  [17]  a notch filter was applied at 60 Hz to eliminate power line interference. Baseline drift was mitigated by implementing a highpass filter at 0.4 Hz. Additionally, other noises were eliminated using a low-pass filter set at 200 Hz. Due to the variability in the length of the ECG signals, they are partitioned into segments of 5 seconds each.\n\nThe PPG and rPPG signals from the DEAP and CMU-MOSEI datasets are preprocessed and segmented into segments of pulses corresponding to 1.1 secs following  [36] .\n\nPPG signal can be affected by disturbances, such as movement at the sensor attachment site. Consequently, PPG signals in the DEAP dataset contain movement noise. Hence, before segmenting PPG signals into single pulses of 1.1 secs, a high-order polynomial (an order of 50 polynomials) is fitted to the PPG signal. Subsequently, the fitted curve is subtracted from the original PPG signal, effectively eliminating any movement noise. To partition the PPG signal into single pulses of 1.1 secs, we first find the maximum or peaks of the signal and then use the peak value as the center of the segmenting window. The next pulse is segmented by moving the center of the segmenting window to the next peak, and so on. The biosignal data vary from person to person; as such, the PPG/rPPG signals are normalized to mitigate variations in PPG signal size among individuals. We must exercise caution to retain the personal characteristics of the signal, as they vary depending on the emotions. The PPG signals are normalized by employing personal maximum and minimum  [36] :\n\nwhere z i is the PPG signal, zi is the normalized signal, and α is set to 1000 to normalize z i between 0 to 1000. After extracting the signal segments and their corresponding video frames, we balance the dataset by discarding some segments that contain neutral facial expressions. We employ the offthe-shelf TER-GAN  [6]  FER model trained on the in-thewild AffectNet  [10]  dataset to detect segments containing non-neutral facial expressions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Unimodal Emotion Recognition Details",
      "text": "This section presents the experimental details of the unimodal emotion recognition experiments that were performed to investigate and compare the performance of the three 2D representations of ECG/PPG signals. These experiments were conducted by inputting only the 2D imagebased representations of the ECG signal (in the case of MAHNOB-HCI dataset), and the PPG signal (in the case of DEAP dataset) to a pre-trained ResNet-18  [62] . ResNet-18 was employed to classify emotions in an arousal-valence space using both the MAHNOB-HCI and DEAP datasets.\n\nIn the case of the MAHNOB-HCI dataset, the arousal dimension consisted of three categories: 'calm', 'medium', and 'activated', while the valence dimension included the categories 'unpleasant', 'neutral', and 'pleasant', similar to the class distribution presented in  [17] . The arousal and valence classes in the DEAP dataset are annotated on a 1-9 scale. Therefore, following  [36] , the arousal and valence are split into two binary classes based on a threshold of 5, indicating high and low arousal, and high and low valence, respectively. ResNet-18 is finetuned by using Adam  [72]  optimization algorithm with a learning rate of 0.001, batch size of 128, and cross-entropy loss function. The ResNet-18 is trained for 50 epochs, and 10-fold cross-validation is employed for the evaluation of the 2D ECG/PPG representation-based unimodal emotion recognition technique. The best emotion recognition performance is obtained by converting 1D ECG",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Method",
      "text": "Valance Arousal Spatio-temporal maps  [24]  37.62 41.04 SPWVD  [28]  38.39 42.75 Scalogram  [29] ,  [30]  42.91 49.14",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Emotion Recognition Details",
      "text": "Multimodal emotion recognition is performed by applying our novel Unified Biosensor-Vision Multi-modal Transformer (UBVMT) network for emotion recognition in an arousal-valence space using the facial expression information and 2D representation of ECG/PPG data from the MAHNOB-HCI and DEAP datasets, respectively. As far as we are aware, we are the first ones to model the co-relation between the face and ECG/PPG data for emotion recognition employing a transformer network. For the extraction of the face information, since the ECG and PPG signals are synchronized with the facial videos of the participants in both datasets, we partitioned the videos into clips of 5 secs for the MAHNOB-HCI dataset and 1.1 secs for the DEAP database. Subsequently, following  [7] , we extract the first frame from each video clip, pass it to MTCNN  [77]  to extract only the face region as a facial expression representation, and input it to the UBVMT network along with the 2D representation of the corresponding ECG/PPG signal. As mentioned in section 4.3, the best unimodal emotion recognition performance is obtained by transforming the 1D ECG and PPG signals into a 2D scalogram representation. Therefore, multimodal emotion analysis is performed by combining the facial expression images with the scalograms of ECG and PPG signals using our proposed UBVMT network. UBVMT is trained using the Adam  [72]  optimizer, with a batch size of 4, learning rate of 1e-4, and using a cosine schedule  [73]  with a decay rate set at 0.001. To formulate the pretraining objective in equation 3, the values of λ M and λ C are set to 0.4 and 1, respectively. Following MAE  [63]  and TVLT  [22] , a random masking strategy is applied, where 75% of the face and ECG/PPG patches are randomly masked. After pretraining, the encoder of UBVMT is detached, and a two-layer MLP is added on top of the encoder representation for emotion analysis. The encoder plus the MLP layers are fine-tuned using the Adam  [72]  optimizer, the learning rate of 1e-4, and a decay rate of 0.001. All the training and validation processes are carried out using dual NVIDIA Tesla V100 GPUs.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis",
      "text": "In this section, the experimental results of both the unimodal emotion recognition and the multimodal emotion analysis using the proposed UBVMT method are discussed in detail.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis Of Unimodal Method",
      "text": "The unimodal emotion recognition performance of the three 2D representations of ECG/PPG signals for MAHNOB-HCI and DEAP datasets are presented in Table  1  and Table  2  as an average of the 10-fold cross-validation. As it can be seen, the scalogram representation of both ECG and PPG signals outperforms the emotion recognition performance of the spatiotemporal maps and the SPWVD-based 2D representation. The superior performance of scalogram representation is because it is more effective at identifying the lowfrequency or rapidly-changing frequency components of the signal, as both ECG and PPG are low-frequency signals.\n\nSimilarly, the emotion recognition accuracy of SPWVD is higher than the accuracy of spatio-temporal maps because SPWVD offers better time-frequency resolution, and thus captures emotion features more effectively than the spatiotemporal maps.\n\nThe performance of our unimodal emotion recognition method is also compared with the state-of-the-art unimodal emotion recognition techniques on both the MAHNOB-HCI and DEAP datasets, as presented in Table  3 . For the MAHNOB-HCI dataset, we compare our result with the ECG-based emotion analysis technique proposed by Ferdinando et al.  [17]  and rPPG-based method of Yu et al.  [76] . In  [17] , Ferdinando et al. extracted heart rate variability (HRV) features from ECG signals to classify emotions. As it can be seen in Table  3 , our scalogram-based method outperforms the emotion recognition technique proposed in  [17] . In  [76] , Yu et al. extracted ten-dimensional HRV features from rPPG signals of MAHNOB-HCI video clips, and fed them to a support vector machine for emotion classification. Table  3  shows that they achieve a higher recognition accuracy in the case of valence, while our scalogram-based method outperforms their method in arousal categorization. Similarly, in Table  3 , the performance of our PPG-based emotion classification method is compared with the state-of-the-art emotion analysis techniques using the DEAP database. For the comparison of the techniques involving only the PPG signal, our 2D scalogram-based method is compared with the PPG-based technique proposed by Lee et al.  [36]  and various 2D representations of PPG signal employed by Elalamy et al.  [56] . Table  3  shows that our scalogram-based PPG technique is more effective than the one-dimensional convolutional neural network-based (1D CNN) emotion analysis method proposed by Lee et al.  [36] . Similarly, our technique produces higher recognition accuracy than the 2D spectrogram-based method proposed by Elalamy et al.  [56] . Also, our 2D scalogram representation outperforms the emotion recognition performance of the Recurrence PLotbased (RP) 2D representation technique used in  [56] . Most of the physiological-based emotion analysis methods employ EEG signals for emotion recognition more than any other physiological signal due to its capability to capture emotion features more effectively. Therefore, in Table  3 , we also compare our unimodal method with the state-of-the-art unimodal emotion analysis techniques using EEG signals.\n\nTable  3  shows that our scalogram-based PPG representation produces higher recognition accuracy as compared to the EEG-based emotion recognition techniques of Patras et al.  [32] , Chung et al.  [74]  and Campos et al.  [75] . These results imply that the scalogram-based 2D representation of the PPG signal can be used as an alternative to the EEG signal.\n\nObtaining and utilizing EEG signals, the most prevalent biosignal in emotion recognition, can be inconvenient due to the high cost of EEG measurement devices and the cumbersome measurement process, especially for participants from neurodiverse populations such as kids with ASD.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis Of Multi-Modal Method",
      "text": "The average of the 10-fold cross-validation accuracy of the UBVMT multimodal emotion recognition method is   shown in Table  4 . However, since our method provides a baseline for multimodal emotion recognition by fusing the ECG/PPG signal with the face information, there are no state-of-the-art methods with which we can compare our results. Nonetheless, as can be seen in Table  4  that the proposed method outperforms the multimodal techniques fusing EEG information with other physiological signals, especially for the recognition of arousal. For the MAHNOB-HCI dataset, we compare our results with the EEG-based multimodal methods proposed by Soleymani et al.  [31]  and Koelstra et al.  [15] . Here again, the proposed method outperforms both of these techniques by a large margin in the recognition of arousal by producing an average accuracy of 83.84%. In contrast, the method proposed by Soleymani et al.  [31]  achieves the highest accuracy of 76.10% for the classification of the valence class. We argue that the superior performance of our technique in recognizing the arousal class is because UBVMT is pre-trained by employing an rPPG signal, and it has been reported in past research that PPG signals are more effective in categorizing arousal  [29] . Therefore, given a large enough ECG dataset to pre-train the UBVMT network, we posit that we can improve the classification accuracy of our method for the valence class as well. For the DEAP dataset, we compare our results with the multimodal techniques fusing PPG information with other signals like EEG, EDA GSR, etc. As can be seen in Table  4 , our method outperforms the state-of-theart multimodal emotion classification techniques both in recognizing valence and arousal with average accuracies of 81.53% and 82.64%, respectively. It is interesting to note that our bimodal method outperforms the multimodal technique proposed by Yin et al.  [53] , where the information from EEG, ECG, EOG, GSR, EMG, Skin temperature, blood volume, and respiration signals are fused to classify emotions. Similarly, comparing our technique with the bimodal method proposed by Siddharth et al.  [7]  where EEG and face information are fused for emotion analysis, Table  4  shows that the fusion of PPG and face information using UBVMT is more effective in recognizing emotions. Our PPG-face method is also more accurate in classifying emotions compared to the fusion of EEG, EOG, and EMG signals  [13] . In  [56] , similar to our approach, Elalamy et al. transformed the 1D PPG and EDA signals into 2D representations and fuse them to categorize emotions using deep networks. Table  4  shows that our scalogram-based PPG representation, when fused with the face data, produces higher recognition accuracy by employing our proposed technique. The confusion matrices and the F1 scores of the multimodal emotion recognition results are shown in Figure  6  and Table  5 , respectively.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a novel approach called Unified Biosensor-Vision Multi-modal Transformer-based (UBVMT) method is presented to classify emotions in an arousal-valence space.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of UBVMT-based multimodal emotion recognition",
      "page": 1
    },
    {
      "caption": "Figure 1: Based on our literature survey, no",
      "page": 2
    },
    {
      "caption": "Figure 2: , emotions are portrayed",
      "page": 3
    },
    {
      "caption": "Figure 2: Arousal valence emotion model",
      "page": 3
    },
    {
      "caption": "Figure 2: , two widely utilized",
      "page": 3
    },
    {
      "caption": "Figure 3: Unimodal emotion recognition",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the spatiotempo-",
      "page": 4
    },
    {
      "caption": "Figure 4: that the vertical patterns preserve the period",
      "page": 4
    },
    {
      "caption": "Figure 4: 2D representations of ECG/PPG signal",
      "page": 5
    },
    {
      "caption": "Figure 4: To mitigate cross-terms in both time and frequency do-",
      "page": 5
    },
    {
      "caption": "Figure 4: The scalograms",
      "page": 6
    },
    {
      "caption": "Figure 5: The objective of masked",
      "page": 6
    },
    {
      "caption": "Figure 5: Additionally, the corre-",
      "page": 6
    },
    {
      "caption": "Figure 5: For every face image, a positive vision-",
      "page": 6
    },
    {
      "caption": "Figure 5: Multimodal emotion recognition",
      "page": 7
    },
    {
      "caption": "Figure 6: The confusion matrices for (a). MAHNOB-HCI valence emotion",
      "page": 10
    },
    {
      "caption": "Figure 6: and Table 5, respectively.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nINTRODUCTION": "few years,\nthere has been an increas-"
        },
        {
          "1\nINTRODUCTION": "in adopting a bio-sensing perspective for\nO VER the past"
        },
        {
          "1\nINTRODUCTION": "emotion analysis. Certainly,\nthe popularity of bio-sensing"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "research extends beyond affective computing. Various fields"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "such as robotics [1],\n[2], health [3],\n[4], and virtual\nreality"
        },
        {
          "1\nINTRODUCTION": "[5] have also embraced bio-sensing as a valuable research"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "tool. Bio-sensing systems, including those that measure elec-"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "trocardiogram (ECG), Photoplethysmography (PPG), elec-"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "troencephalogram (EEG), galvanic skin response (GSR), etc,"
        },
        {
          "1\nINTRODUCTION": "have been available for decades; however,\ntheir\nsize and"
        },
        {
          "1\nINTRODUCTION": "complexity have limited their use to controlled laboratory"
        },
        {
          "1\nINTRODUCTION": "settings and hospitals. The emergence of wearable biosens-"
        },
        {
          "1\nINTRODUCTION": "ing systems has fueled the recent\ninterest\nin utilizing bio-"
        },
        {
          "1\nINTRODUCTION": "sensing systems for various applications by simplifying and"
        },
        {
          "1\nINTRODUCTION": "speeding up data collection."
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "Several\nresearch studies have demonstrated the\nfeasi-"
        },
        {
          "1\nINTRODUCTION": "bility of\nrecognizing human emotions\nthrough facial\nex-"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "pressions\ncaptured in images and videos\n[6],\n[7],\n[8]. Al-"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "though facial expression recognition systems have shown"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "remarkable success on databases captured under controlled"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "conditions, their performance significantly degrades [9], [10]"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "when they are applied to real-life situations. The system’s"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "unreliability in natural\nenvironments\nis primarily due\nto"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "various\nfactors,\nsuch as varying head pose,\nillumination"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "conditions,\nand occlusion of different parts\nof\nthe\nface."
        },
        {
          "1\nINTRODUCTION": "Additionally, recognizing emotions from facial expressions"
        },
        {
          "1\nINTRODUCTION": "may not be entirely dependable since they can be easily con-"
        },
        {
          "1\nINTRODUCTION": "cealed or manipulated. Furthermore, facial expressions can"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "•\nKamran Ali\nand Charles E. Hughes\nare\nboth with the Department\nof"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "Computer Science, University\nof Central Florida, USA, Orlando, FL"
        },
        {
          "1\nINTRODUCTION": "32816."
        },
        {
          "1\nINTRODUCTION": "E-mail: kamran.ali@ucf.edu"
        },
        {
          "1\nINTRODUCTION": ""
        },
        {
          "1\nINTRODUCTION": "Manuscript received April 19, 2005; revised August 26, 2015."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "EEG signal. However,\nseveral\nresearch studies\nshow that",
          "2": "channels from the ROI of videos are extracted, and Short-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "there\nis\na\nstrong\ncorrelation\nbetween\nfeatures\nextracted",
          "2": "Time Fourier Transform (STFT)\nis applied to construct\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "from the ECG signal and human emotions. For instance,\nin",
          "2": "2D time-frequency representations of the sequences. In [27],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[16], Hany et al. extracted ECG features that represent\nthe",
          "2": "a spatiotemporal map is first computed from the input video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "statistical distribution of dominant frequencies by applying",
          "2": "as a representation of\nthe BVP signal, and the spatiotem-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "spectrogram analysis to classify emotions. Similarly, in [17],",
          "2": "poral\nimages are\nthen mapped to BVP signals using the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "time and frequency domain features are extracted from the",
          "2": "generative adversarial\nlearning technique. In [28], Khare et"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG signal to perform emotion analysis. Building on those",
          "2": "al. converted the time-domain EEG biosignals into a time-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "approaches, this paper presents a novel multimodal emotion",
          "2": "frequency representation by employing Smoothed Pseudo"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "recognition technique that fuses information from face video",
          "2": "Wigner-Ville distribution (SPWVD)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "frames and ECG/PPG data. To the best of our knowledge,",
          "2": "Apart\nfrom the image-based representation of ECG sig-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "this\nis\nthe first\nstudy that\ninvestigates\nthe fusion of\nface",
          "2": "nals\nfor heart\nrate\nestimation,\nsome works\n[29] and [30]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "information with the ECG/PPG signal to classify emotions.",
          "2": "have extracted deep learning features from the scalogram"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Many attempts have been made to create effective tech-",
          "2": "of biosensor signals to recognize cognitive load and hyper-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "niques\nfor\nintegrating multimodal\ninformation.\nInitially,",
          "2": "tension risk stratification, respectively. They converted the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "methods\ninvolved simply\ncombining\nhigh-level\nfeatures",
          "2": "PPG signals\nto Scalograms by using Continuous Wavelet"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "from all modalities to predict outcomes (known as ”early",
          "2": "Transform. Scalograms were used instead of spectrograms"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "fusion”) or adding up decisions made by each unimodal sys-",
          "2": "as the image representation for PPG signals. This is because"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tem with learnable weights (called ”late fusion”) to produce",
          "2": "scalograms provide better highlighting of the low-frequency"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the final\ninference. While these methods outperformed uni-",
          "2": "or rapidly changing frequency components of the signal as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "modal techniques to some extent, the lack of inter-modality",
          "2": "compared to spectrograms."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "interactions during training limited the potential\nimprove-",
          "2": "In this paper, we first\ninvestigate and compare the per-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ment.\nIn recent\ntimes, Transformers [18] have emerged as",
          "2": "formance of\nthree image representations of\nthe ECG/PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a highly successful approach for multimodel representation",
          "2": "signal\nfor emotion recognition. More specifically, we per-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "learning. Due to the advances in attention mechanisms and",
          "2": "form unimodal emotion recognition by converting the time"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "transformers,\nlater studies have focused predominantly on",
          "2": "domain ECG/PPG signal\ninto 1. spatiotemporal maps [24],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using these techniques to develop more sophisticated mul-",
          "2": "2.\ntime-frequency representation by employing Smoothed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "timodal fusion methods [19], [20], [21], [22], [23]. However,",
          "2": "Pseudo Wigner-Ville\ndistribution\n(SPWVD)\n[28],\nand\n3."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "most of these transformer-based multimodal emotion recog-",
          "2": "Scalograms [29] [30]. We show that the best emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition techniques leverage video, audio, and text modalities",
          "2": "nition performance\nis\nobtained by employing the\nscalo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to\nlearn joint\nemotion representation.\nIn this paper, we",
          "2": "gram representation of\nthe ECG and PPG signals. There-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "present\na unified transformer-based model\nthat\nemploys",
          "2": "fore, our Unified Biosensor-Vision Multimodal Transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG/PPG data along with face information to recognize",
          "2": "(UBVMT) network is\ntrained and validated by using the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotions. The main framework of\nthe proposed method",
          "2": "scalogram representation of the ECG/PPG signal along with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "is\nshown in Figure 1. Based on our\nliterature survey, no",
          "2": "the face images. The effectiveness of the proposed UBVMT-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "previous work has\ninvestigated the\napplication of\ntrans-",
          "2": "based multimodal\nemotion recognition technique\nis vali-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "formers\nin learning emotion representation by using face",
          "2": "dated on two datasets:\nthe MAHNOB-HCI\n[31] and DEAP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and ECG/PPG modalities.",
          "2": "[32] datasets. Extensive experiments show that the proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Inspired by the compact architecture of the recently pub-",
          "2": "approach produces comparable results to the state-of-the-art"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "lished Textless Vision-Language Transformer\n(TVLT)\n[22],",
          "2": "emotion recognition techniques. Overall, the main contribu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "we present a Unified Biosensor-Vision Multimodal Trans-",
          "2": "tions of this paper are as follows:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "former (UBVMT) network to recognize emotions by fusing",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "face and ECG/PPG data. We first investigated the effective-",
          "2": "•\nComparison\nof\nthe\nperformance\nof\nthree\nimage-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ness of various image-based representations of ECG/PPG",
          "2": "based representations, namely spatiotemporal maps,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signals for biosensor-based unimodal emotion recognition.",
          "2": "time-frequency representation, and scalograms of the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Several previous methods have employed an image-based",
          "2": "ECG/PPG signal\nfor emotion recognition, and evi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "representation of an ECG/PPG signal for heart rate estima-",
          "2": "dence that scalograms better\nrepresent\nthe emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion [24], [25], [26], [27], [28]. For instance, in [24], Song et al.",
          "2": "features contained by the ECG/PPG signals."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "obtained a spatiotemporal representation of a pulse signal",
          "2": "•\nA novel technique that employs transformer architec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in a time-delayed way by constructing a Toeplitz matrix.",
          "2": "ture for biosensor-based multimodal emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The Toeplitz matrix is\nthen converted into an image that",
          "2": "nition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "is\nfed to a CNN to estimate heart\nrate\ninformation. The",
          "2": "•\nThe Unified\nBiosensor-Vision Multimodal\nTrans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "simplicity of the Toeplitz representation allows it to preserve",
          "2": "former (UBVMT) network consisting of homogenous"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "both the morphological and chronological details of the 1D",
          "2": "blocks that\nlearn vision-and-biosensor emotion rep-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pulse signal. As a result, the CNN can accurately extract the",
          "2": "resentation with minimal modality-specific design,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "correct HR values from input feature images.",
          "2": "thus, making it compact and applicable in real-time"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In [25],\naverage pooling is\napplied to various blocks",
          "2": "emotion analysis."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "within an ROI\nregion. The spatiotemporal maps are then",
          "2": "•\nExperimental\nresults\nshowing\nthat\nthe\nproposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "generated by arranging these temporal sequences into rows,",
          "2": "Unified Biosensor-Vision Multimodal Transformer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "which are then fed to a deep network to get HR values.",
          "2": "based (UBVMT)\ntechnique\nlearns\neffective multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In [26],\nthe\nsample mean sequences of\nthe R, G,\nand B",
          "2": "modal emotion representation by fusing face with"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "sadness, anger, happiness,\nfear, and so on. This approach"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "considers emotions as discrete categories rather\nthan con-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "tinuous dimensions.\nIn dimensional\nconceptualizations of"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "emotions,\na\ncommonly used framework\ninvolves\nrepre-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "senting emotions within a two-dimensional\nspace.\nIn this"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "space, valence is positioned along one axis, while arousal"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "is positioned along the other [33]. Valence, often positioned"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "on the horizontal axis, signifies the extent of pleasantness or"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "unpleasantness. On the other hand, arousal, typically placed"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "on the vertical axis, represents the level of activation or en-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "ergy associated with the emotion. Within a two-dimensional"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "(2D) space, as shown in Figure 2, emotions are portrayed"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "by their valence and arousal\nlevel. While\nthe\ncategorical"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "approach to emotions\nis\nconceptually straightforward,\nit"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "faces certain challenges. It struggles to represent compound"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "emotions that do not fit neatly into a single category, and it"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "does not provide a means to quantify the degree or intensity"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "of\nan emotional\nstate. Therefore,\nin this paper,\na novel"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "multimodal emotion recognition technique is developed to"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "classify emotions along the valence-arousal axis."
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "2.1\nEmotion Recognition with ECG/PPG Signals"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "Previous research shows\nthat\nthere is a strong correlation"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "between ECG/PPG signals and human emotion. In [34], Yu"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "et al. converted the rPPG signal of an input image into time-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "frequency domain spectrogram images to classify the short-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "term emotions.\nIn [35],\nIsmail\net al.\nemployed ECG and"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "PPG signals to develop an emotion recognition system, and"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "compared the performance of both signals\nfor\nclassifying"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "emotions. Lee et al.\nin [36]\ninvestigated the ability of PPG"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "signals to recognize emotions along the arousal-valence axis."
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "Sep ´ulveda el al. in [37] performed emotion recognition from"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "ECG signals using a wavelet scattering algorithm. Emotion"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "features of\nthe ECG signal are extracted at different\ntime"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "scales leveraging the wavelet scattering technique, and the"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "extracted features are then fed into different classifiers\nto"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "perform emotion analysis. Sarkar\net\nal.\nin [38]\nexploited"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "a\nself-supervised deep multi-task learning framework to"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "develop an emotion recognition algorithm using the ECG"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "signals. Mellouk et al. presented a rPPG signal-based emo-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "tion classification method using a deep learning architec-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "ture, which combines a one-dimensional convolution neural"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "network (1DCNN) and a long short-term memory (LSTM)"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "[39].\nIn [16], Ferdinando et al. extracted emotion features"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "from ECG signals using spectrogram analysis of\nintrinsic"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "mode function after applying the bivariate empirical mode"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "decomposition to ECG. Similarly,\nin [17], Ferdinando et al."
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": ""
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "derived heart rate variability (HRV) features from ECG sig-"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "nals to categorize emotions in an arousal-valance space.\nIn"
        },
        {
          "emotions\nare\nclassified\ninto\ndistinct\nlabels\nsuch\nas\njoy,": "[29], Gasparini et al. used a scalogram-based representation"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "techniques\nto perform 2D image-based emotion analysis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "using ECG/PPG data. More specifically, we present a uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Physiological",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "modal emotion recognition network using ResNet-18 [62] by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Spatiotemporal \nSignal",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Map",
          "4": "converting the time domain ECG and PPG signals into (1)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "spatiotemporal maps [24], (2) time-frequency representation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ResNet-\nPre-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "1D to 2D",
          "4": "by employing Smoothed Pseudo Wigner-Ville distribution"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "18\nprocessing",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG Signal",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "TF Map",
          "4": "(SPWVD)\n[28], and (3) Scalograms\n[29],\n[30] as\nshown in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "OR",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "Figure 3."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "3.1.1\nSpatiotemporal Representation of ECG/PPG signal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Scalogram\nPPG Signal",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "In this section, a novel emotion recognition technique using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Fig. 3. Unimodal emotion recognition",
          "4": "a spatiotemporal\nrepresentation of an ECG/PPG signal\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "presented. Rencheng et al.\n[24]\nconverted a 1D ECG and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "These studies have demonstrated that, by integrating in-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "PPG signal\ninto a 2D spatiotemporal map to estimate heart"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "formation from multiple modalities,\nthe performance\nof",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "rate information. It has been reported that the performance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "models\nfor\nrecognizing\nemotions\ncan be\nenhanced [52].",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "degradation of\nthe conventional PPG signals for heart rate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Therefore, in [53], Yin et al. fused information from ECG sig-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "estimation due\nto noise\ncan be overcome by employing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nal with Electroencephalogram (EEG), Electrooculography",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "deep learning techniques on the\nspatiotemporal maps of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(EOG), Galvanic Skin Response (GSR), Electromyography",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "the PPG signal [24]. Therefore,\nin this section, we apply the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(EMG),\nskin temperature, blood volume,\nand respiration",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "technique proposed in [24]\nto convert\nthe 1D ECG/PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to recognize emotions. Miranda et al.\n[14]\nrecorded EEG,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "signal\ninto a 2D spatiotemporal map and perform emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "GRS, and ECG signals using wearable sensors and com-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "analysis using the\nspatiotemporal maps\nas\ninput\nto\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "bined EEG and GRS information with the ECG signal\nto",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "classifier. The spatiotemporal\nfeature maps can retain both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "categorize human emotions. Soleymani et al. [31] employed",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "the morphological and chronological features of ECG/PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Hidden Markov Models\n(HMMs)\nto classify emotions by",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "signals."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "integrating ECG features with EEG, GSR,\nrespiration, and",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "skin temperature data.\nIn [54], Stamos et al.\nfused emotion",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "Construction of ECG/PPG Spatiotemporal Map: The\nspa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "features extracted from the ECG signal with the features",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "tiotemporal\nfeature map of\nan ECG/PPG signal\nis\ncon-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "obtained from the EEG signal to perform emotion analysis.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "structed by creating a square Toeplitz matrix. Suppose the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Santamaria et al. [55] integrated the modalities of ECG and",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "1D input signal S = (s1, s2, ..., sP ) has P samples and P is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "GSR to recognize emotions by employing Deep Convolu-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "an even number. The first row of\nthe matrix consists of s1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tional Network (DCNN). Elalamy [56] performed emotion",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "to sP/2 samples. Similarly, the second row contains samples"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "analysis using the spectrogram and recurrence plots\n(RP)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "from the second point,\nto the (P/2 + 1)th sample,\ni.e., s2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of ECG, EDA, and Photoplethysmography (PPG) signals in-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "and so on. Therefore, a square Toeplitz matrix T with a size"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "dividually, and investigated the performance by combining",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "equal to P/2 is constructed as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "this information for multi-modal emotion recognition. Most",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "...\ns1\ns2\nsP/2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the aforementioned techniques fuse the ECG signal with",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "...\ns2\ns3\nsP/2+1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "other modalities\nsuch as EEG, EMG, GSR, etc.,\nfor multi-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": " \n \n.\n.\n.\n."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "modal emotion analysis. But, unlike the EEG signal, which",
          "4": "T ="
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": ".\n.\n.\n."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "has been combined with the facial\ninformation as proposed",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": ".\n.\n.\n."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in [15],\n[57],\n[58],\nthe ECG signal has not been fused with",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "4": "...\nsP −1\nsP/2\nsP/2+1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "facial features to perform emotion recognition. In this paper,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a Unified Biosensor-Vision Multimodal Transformer-based",
          "4": "A gray image is constructed by converting the matrix"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(UBVMT) emotion recognition technique is presented that",
          "4": "T into an image representation. The obtained gray image"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "integrates ECG/PPG data with facial information.",
          "4": "has a clear structure because the input signal\nis quasiperi-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "Construction of ECG/PPG TF Map: SPWVD provides a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "straightforward depiction of\nthe localization of\nsignal en-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "ergy in both time and frequency domains. The choice of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "the length and type of\nthe cross-term reducing window in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "both the time and frequency domains can be made inde-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Spatiotemporal",
          "5": "pendently. Due to the independent selection of\nthe length"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Maps",
          "5": "and type of\nthe cross-term reducing window, SPWVD ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "hibits favorable time-frequency cluster characteristics. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "representation of\nSPWVD in mathematical\nterms\ncan be"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "5": "expressed as [59], [60]."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:90) ∞": "τ 2\nτ 2\nx(t +\n)x∗(t −\n)e−j2πf τ dτ\nW (t, f ) =\n(2)"
        },
        {
          "(cid:90) ∞": "−∞"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "where u(t)\nrepresents the smoothing window in the time"
        },
        {
          "(cid:90) ∞": "domain, v(f ) denotes\nthe smoothing window in the fre-"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "quency domain, τ corresponds to a lag, and x(t) is the input"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "signal. Controlling the smoothing scales\nin both the time"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "and frequency domains is straightforward. The length of the"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "windows\nfor u(t) and v(f ) can be chosen independently."
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "The TF representation of\ntime-domain ECG/PGG signals"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "obtained by employing the SPWVD technique is shown in"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "the third row of Figure 4."
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "To mitigate cross-terms in both time and frequency do-"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "mains, the Kaiser window is employed. However, selecting"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "a window size that\nis\ntoo small may lead to diminished"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "resolution, while overly large windows\ncan significantly"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "increase the image size. Consequently, a medium-sized win-"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "dow with a length of 31, as\nsuggested in [28],\nis chosen."
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "For efficient computation,\nthe window size is maintained"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "at 2n-1, where n represents\nthe number of bits. The TF"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "representations of time-domain ECG/PGG signals are then"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "adjusted to the size of 224 × 224 × 3, and fed to a pre-trained"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "ResNet-18 [62] to perform emotion recognition."
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "3.1.3\nScalogram of ECG/PPG Signal"
        },
        {
          "(cid:90) ∞": ""
        },
        {
          "(cid:90) ∞": "Gasparini\net\nal.\n[29]\nconverted\nthe monodimensional"
        },
        {
          "(cid:90) ∞": "photoplethysmography\n(PPG) data\ninto\na\nbidimensional"
        },
        {
          "(cid:90) ∞": "representation, and applied a pre-trained CNN to classify"
        },
        {
          "(cid:90) ∞": "the\ndifferent\nlevels\nof\nthe\nsubject’s\nhypertension. More"
        },
        {
          "(cid:90) ∞": "specifically,\nthe\n1D PPG\nsignals were\nconverted\ninto"
        },
        {
          "(cid:90) ∞": "scalograms by leveraging Continuous Wavelet Transform"
        },
        {
          "(cid:90) ∞": "[61]. Gasparini et al. opted to utilize the scalogram instead"
        },
        {
          "(cid:90) ∞": "of\nthe\nspectrogram as\nthe\nimage\nrepresentation for PPG"
        },
        {
          "(cid:90) ∞": "signals.\nThe\nreason\nfor\nthis\nchoice\nis\nthat\nscalograms"
        },
        {
          "(cid:90) ∞": "offer\nenhanced highlighting of\nlow-frequency or\nrapidly"
        },
        {
          "(cid:90) ∞": "changing frequency components\nin the\nsignal,\ncompared"
        },
        {
          "(cid:90) ∞": "to spectrograms. Similarly,\nin [30], Liang et al.\nconverted"
        },
        {
          "(cid:90) ∞": "1D PPG signals\ninto 2D scalograms by using Continuous"
        },
        {
          "(cid:90) ∞": "Wavelet Transform to classify hypertension.\nIn this paper,"
        },
        {
          "(cid:90) ∞": "we investigate the performance of scalogram representation"
        },
        {
          "(cid:90) ∞": "of\nECG/PPG signals\nfor\ndeep\nlearning-based\nemotion"
        },
        {
          "(cid:90) ∞": "recognition."
        },
        {
          "(cid:90) ∞": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Construction of ECG/PPG scalograms: Continuous Wavelet",
          "6": "3.2.1\nThe architecture of UBVMT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Transform (CWT) has been used for decades as a valuable",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "Similar to the transformer model proposed in [18], UBVMT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "technique for analyzing both time and frequency informa-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "has an encoder E which has 12 layers\n(hidden size 768),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion. In this paper, each segment of the ECG/PPG signal was",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "and a decoder D with 8\nlayers\n(hidden size\n512). After"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "transformed into a time-frequency representation, known",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "pretraining, we only employ the\nencoder E part of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "as\na\nscalogram, using the\ncontinuous wavelet\ntransform",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "transformer and finetune it for emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "method. A scalogram represents\nthe\nabsolute\nvalues\nof",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the continuous wavelet\ntransform coefficients of a signal,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "3.2.2\nPretraining UBVMT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "displayed as\na graph of\ntime\nand frequency.\nIn contrast",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "UBVMT is pre-trained by employing masked autoencoding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to a spectrogram, a scalogram provides improved capabil-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "[63] and contrastive modeling [22] to learn effective emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ity in identifying the\nlow-frequency or\nrapidly changing",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "representation. The pretraining objective function L is the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "frequency components of\nthe signal. To convert\nthe PPG",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "weighted sum of masked autoencoding loss LM and the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signal\ninto a scalogram, we obtained the absolute values",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "contrastive modeling loss LC:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the wavelet coefficients for each signal segment using the",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "analytic morse (3,60) wavelet. The scalogram representation",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "(3)\nL = λM LM + λCLC"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of ECG/PGG signals obtained by employing the CWT tech-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nique is shown in the fourth row of Figure 4. The scalograms",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "where λM = 0.4 and λC = 1."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "were then adjusted to the size of 224 × 224 × 3, and fed to a",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pre-trained ResNet-18 [62] to perform emotion recognition.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "Masked Autoencoding Objective Function: The main ob-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "jective of masked autoencoding is\nto learn effective uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "modal representations in biosensor-and-vision settings. This"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "objective involves masking random patches of ECG/PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "3.2\nThe Unified Transformer-based Multi-modal Net-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "images and the face video frames, allowing us\nto recon-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "work",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "struct missing inputs effectively. Specifically, we implement"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "a random dropout mechanism on a portion of ECG/PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In this\nsection, we discuss our Unified Biosensor-Vision",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "embedding xB and face embedding xF . Subsequently, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Multimodal Transformer\n(UBVMT) network where homo-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "input the remaining patch embeddings to the encoder E. To"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "geneous\ntransformer\nblocks\ntake\n2D representations\nof",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "generate the input\nto the decoder D, we add the dropped"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG/PPG signals and raw visual\ninputs\nfor multi-modal",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "embeddings\nas\ntrainable vectors\nlabeled as\n[MASK]\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion representation learning with minimal modality-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "position them in the same locations as the original\ninputs"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "specific design. UBVMT is\ntrained by employing masked",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "(indicated by gray boxes in Figure 5. Additionally, the corre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "autoencoding [63] and contrastive modeling [22]\nto learn",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "sponding positional, and frequency embeddings, which are"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "effective emotion representation. The overall architecture of",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "separately parametrized, are incorporated into the decoder"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "our Unified Biosensor-Vision Multi-modal Transformer (UB-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "input. The objective function of masked autoencoding is a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "VMT) network is shown in Figure 5. The objective of masked",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "mean squared error between the reconstructed and original"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "autoencoding is to reconstruct masked patches of 2D repre-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "ECG/PPG images and face video frames:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sentations of ECG/PPG signals and face video frames, while",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "contrastive modeling is applied to align ECG/PPG and face",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "(cid:88)\n(cid:88)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "information. We argue that, due to the unified architecture",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "1 N\n1 N\n∥xB\n∥xF\nLM =\ni − ˆxB\ni ∥2\n2 +\nj − ˆxF\nj ∥2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of UBVMT,\nthe computational redundancy and complexity",
          "6": "Bm\nFm"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "i∈masked\nj∈masked"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of\nour method is\nreduced as\ncompared to\nconventional",
          "6": "(4)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "transformer-based multi-modal\nemotion recognition tech-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "6": "m and N F\nm are the number of masked patches for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "niques.",
          "6": "ECG/PPG images and face video frames, respectively. Loss"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The input to the UBVMT is the integration of face patch",
          "6": "computed only on masked patches. Note that\nthe\nLM is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "embedding,\nECG/PPG patch\nembedding,\nand modality",
          "6": "ECG/PPG and face output of\nthe encoder are fed to the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "embedding. To obtain face embeddings,\nthe face image of",
          "6": "decoder separately."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "224 × 224 × 3 pixels is divided into a list of 16 × 16 sized",
          "6": "Contrastive Modeling Objective Function: The main objec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "patches. The pixel values of each patch are normalized, and",
          "6": "tive of contrastive modeling is to perform ECG/PPG-vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a\nlinear projection layer\nis used to convert\nface patches",
          "6": "matching and learn an effective cross-modal representation,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "into a 768-dimensional patch embedding. For an input face",
          "6": "as shown in Figure 5. For every face image, a positive vision-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "image of size 224 × 224 × 3,\nthe size of\nthe resultant\nface",
          "6": "(xF +; xB)\nECG/PPG pair\nis generated. Additionally, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "embedding\nis\n14 × 14. The\nspatial\nembedding\ninvolves",
          "6": "form half of\nthe vision-ECG/PPG pairs within a batch as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "incorporating spatial\ninformation for each input patch by",
          "6": "mismatched (negative) pairs (xF −; xB) by substituting the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "adding a distinct\ntrainable vector to the height and width",
          "6": "face images xF + with randomly selected face images xF −"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "axes of the 14 × 14 embeddings. For ECG/PPG embedding,",
          "6": "from the training dataset."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the 2D representation of an ECG/PPG signal is divided into",
          "6": "Similar to previous multimodal\ntransformers [22],\n[64],"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "patches, and a liner projection layer is applied on each patch",
          "6": "[65],\n[66],\n[67], we incorporate a linear layer with sigmoid"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "to obtain a 768-dimensional patch embedding. Similar to the",
          "6": "activation as the classification head. This is applied to the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "face modality, a patch size of 16 × 16 is used, and trainable",
          "6": "encoder output of\nthe first\n[CLS]\ntoken,\nresulting in the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "temporal and frequency embeddings are utilized to denote",
          "6": "matching probability p by employing binary cross-entropy"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the temporal and frequency information of the patches.",
          "6": "loss:"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Contrastive Modeling": "Fig. 5. Multimodal emotion recognition",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "(ECG), 1-channel galvanic skin response (GSR), and facial"
        },
        {
          "Contrastive Modeling": "(5)\nLC = −y log p",
          "Emotion Recognition": "expressions are captured. The ECG signals were sampled at"
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "a rate of 256 Hz. The ECG data is precisely synchronized"
        },
        {
          "Contrastive Modeling": "where\nthe value of y is\nset\nto 1 when the\ninput vision-",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "and aligned with the face video recordings. Similar to [71],"
        },
        {
          "Contrastive Modeling": "(xF ; xB)\nECG/PPG pair\nis positive\n(match),\nand 0 oth-",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "only the EXG2 signal\nfrom the 3 ECG channel\nsystem is"
        },
        {
          "Contrastive Modeling": "erwise. Throughout\nare\nthe training process, LM and LC",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "extracted for emotion analysis."
        },
        {
          "Contrastive Modeling": "computed using separate forward passes.",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "The Database for Emotion Analysis using Physiological"
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "Signals (DEAP) [32] is a widely used and publicly available"
        },
        {
          "Contrastive Modeling": "4\nEXPERIMENTAL DETAILS",
          "Emotion Recognition": "database designed for\nresearch in emotion analysis\nand"
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "affective computing. The DEAP database contains data from"
        },
        {
          "Contrastive Modeling": "4.1\nDatabase Description",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "32 participants, aged between 19 and 37 (50% female), who"
        },
        {
          "Contrastive Modeling": "The effectiveness of transformer models to learn multimodal",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "were recorded watching 40 one-minute music videos. Each"
        },
        {
          "Contrastive Modeling": "representation is\nimproved by pretraining them on large",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "participant was\nasked to evaluate\neach video by assign-"
        },
        {
          "Contrastive Modeling": "datasets [65],\n[66],\n[67]. As such, UBVMT is pre-trained on",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "ing values\nfrom 1\nto 9\nfor\narousal, valence, dominance,"
        },
        {
          "Contrastive Modeling": "the large, comprehensive CMU-MOSEI [68] dataset. Similar",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "like/dislike, and familiarity. Face video was\nrecorded for"
        },
        {
          "Contrastive Modeling": "to [24], [25], [69], rPPG signals are extracted from the CMU-",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "22 out of\nthe 32 participants, and the proposed UBVMT"
        },
        {
          "Contrastive Modeling": "MOSEI video clips using the MTTS-CAN [70] method. After",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "method is evaluated using this particular group of subjects."
        },
        {
          "Contrastive Modeling": "pertaining, MAHNOB-HCI [31] and DEAP [32] datasets are",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "For each dataset, we conduct a subject-independent 10-fold"
        },
        {
          "Contrastive Modeling": "used for multimodal emotion analysis.",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "cross-validation evaluation."
        },
        {
          "Contrastive Modeling": "The CMU-MOSEI dataset\ncomprises 23,454 movie re-",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "view clips,\ntotaling\nover\n65.9\nhours\nof YouTube\nvideo",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "content.\nIt\nfeatures\ncontributions\nfrom 1000 speakers and",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "",
          "Emotion Recognition": "4.2\nPre-processing Steps"
        },
        {
          "Contrastive Modeling": "encompasses 250 distinct topics. Videos having non-human",
          "Emotion Recognition": ""
        },
        {
          "Contrastive Modeling": "faces and faces with more than 80◦ head rotations from the",
          "Emotion Recognition": "The MAHNOB-HCI emotion elicitation data contains un-"
        },
        {
          "Contrastive Modeling": "frontal position are discarded from the dataset. From each",
          "Emotion Recognition": "stimulated baseline and stimulated response ECG signals."
        },
        {
          "Contrastive Modeling": "video, multiple video clips of 1.1 secs are extracted, with",
          "Emotion Recognition": "Furthermore, the database includes a synchronization signal"
        },
        {
          "Contrastive Modeling": "the number of clips being dependent on the length of each",
          "Emotion Recognition": "that\nfacilitates the separation of\nthe two. Our experiments"
        },
        {
          "Contrastive Modeling": "video. For our case, 90,037 video clips are extracted from",
          "Emotion Recognition": "exclusively utilized the ECG signals\nrecorded during the"
        },
        {
          "Contrastive Modeling": "the entire CMU-MOSEI dataset. These video clips are then",
          "Emotion Recognition": "stimulated phase. To remove motion artifacts,\nthe original"
        },
        {
          "Contrastive Modeling": "fed to the MTTS-CAN [70], and only the pulse signals from",
          "Emotion Recognition": "signal\nis\nsubtracted by the smoothing signal\n[31]. Subse-"
        },
        {
          "Contrastive Modeling": "the MTTS-CAN are used to obtain the rPPG signals of these",
          "Emotion Recognition": "quently,\nsimilar\nto [16],\n[17] a notch filter was applied at"
        },
        {
          "Contrastive Modeling": "video clips.",
          "Emotion Recognition": "60 Hz to eliminate power\nline interference. Baseline drift"
        },
        {
          "Contrastive Modeling": "MAHNOB-HCI\nThe\n[31]\n(Multimodal\nHuman-",
          "Emotion Recognition": "was mitigated by implementing a highpass filter at 0.4 Hz."
        },
        {
          "Contrastive Modeling": "Computer\nInteraction) dataset\nis\na widely used publicly",
          "Emotion Recognition": "Additionally, other noises were eliminated using a low-pass"
        },
        {
          "Contrastive Modeling": "available\ndataset\ndesigned\nfor\nresearch\nin\naffective",
          "Emotion Recognition": "filter set at 200 Hz. Due to the variability in the length of the"
        },
        {
          "Contrastive Modeling": "computing\nand multimodal\nemotion\nrecognition.\nThe",
          "Emotion Recognition": "ECG signals, they are partitioned into segments of 5 seconds"
        },
        {
          "Contrastive Modeling": "dataset\nincludes\n527\nfacial\nvideo\nrecordings\nof\n27",
          "Emotion Recognition": "each."
        },
        {
          "Contrastive Modeling": "participants\nengaged\nin\nvarious\ntasks\nand\ninteractions,",
          "Emotion Recognition": "The PPG and rPPG signals from the DEAP and CMU-"
        },
        {
          "Contrastive Modeling": "while\ntheir\nphysiological\nsignals\nsuch\nas\n32-channel",
          "Emotion Recognition": "MOSEI datasets are preprocessed and segmented into seg-"
        },
        {
          "Contrastive Modeling": "electroencephalogram (EEG),\n3-channel\nelectrocardiogram",
          "Emotion Recognition": "ments of pulses\ncorresponding to 1.1 secs\nfollowing [36]."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "PPG signal can be affected by disturbances, such as move-",
          "8": "TABLE 1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "MAHNOB-HCI dataset: Performance comparison of 2D representations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ment at\nthe sensor attachment site. Consequently, PPG sig-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "of ECG signal\nfor unimodal emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nals in the DEAP dataset contain movement noise. Hence,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Method\nValance\nArousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "before segmenting PPG signals into single pulses of 1.1 secs,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Spatio-temporal maps [24]\n37.62\n41.04"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a high-order polynomial\n(an order of 50 polynomials)\nis",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "SPWVD [28]\n38.39\n42.75"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "fitted to the PPG signal. Subsequently,\nthe fitted curve is",
          "8": "42.91\n49.14\nScalogram [29], [30]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "subtracted from the original PPG signal, effectively elimi-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nating any movement noise. To partition the PPG signal into",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "TABLE 2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "single pulses of 1.1 secs, we first find the maximum or peaks",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "DEAP dataset: Performance comparison of 2D representations of PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the signal and then use the peak value as the center of the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "signal\nfor unimodal emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "segmenting window. The next pulse is segmented by mov-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Method\nValance\nArousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ing the center of\nthe segmenting window to the next peak,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "Spatio-temporal maps [24]\n60.26\n62.71"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and so on. The biosignal data vary from person to person;",
          "8": "SPWVD [28]\n64.03\n65.19"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "as such,\nthe PPG/rPPG signals are normalized to mitigate",
          "8": "76.51\n77.02\nScalogram [29], [30]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "variations in PPG signal size among individuals. We must",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "exercise caution to retain the personal characteristics of the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signal, as they vary depending on the emotions. The PPG",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "and PPG signals into the 2D scalogram representation. Fur-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signals are normalized by employing personal maximum",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "ther details and comparison of the performance of the three"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and minimum [36]:",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "2D representation techniques are discussed in section 5.1."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(zi − minperson)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "× α\n(6)\nzi =",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(maxperson − minperson)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "8": "4.4\nMultimodal Emotion Recognition Details"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "is the normalized signal, and\nwhere zi\nis the PPG signal, ¯zi",
          "8": "Multimodal\nemotion recognition is performed by apply-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "α is set to 1000 to normalize zi between 0 to 1000. After ex-",
          "8": "ing our novel Unified Biosensor-Vision Multi-modal Trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tracting the signal segments and their corresponding video",
          "8": "former\n(UBVMT) network for\nemotion recognition in an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "frames, we balance the dataset by discarding some segments",
          "8": "arousal-valence space using the facial expression informa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "that contain neutral\nfacial expressions. We employ the off-",
          "8": "tion and 2D representation of ECG/PPG data\nfrom the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the-shelf TER-GAN [6] FER model\ntrained on the in-the-",
          "8": "MAHNOB-HCI and DEAP datasets, respectively. As far as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "wild AffectNet\n[10] dataset\nto detect\nsegments containing",
          "8": "we are aware, we are the first ones to model the co-relation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "non-neutral facial expressions.",
          "8": "between the face and ECG/PPG data for emotion recogni-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: and Table 2 Patrasetal.[32] EEG 62.00 57.60",
      "data": [
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "Performance comparison of our multimodal emotion recognition"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "method with the state-of-the-art"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "Modality"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "MAHNOB-HCI"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "EEG and Eye Gaze"
        },
        {
          "TABLE 4": "EEG and Face"
        },
        {
          "TABLE 4": "ECG and Face"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "DEAP"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "EEG,\nECG,\nEOG,"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "GSR,\nEMG,\nSkin"
        },
        {
          "TABLE 4": "temperature,"
        },
        {
          "TABLE 4": "Blood\nvolume,"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "Respiration"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "EEG, EOG, EMG"
        },
        {
          "TABLE 4": "EEG, PPG, GSR"
        },
        {
          "TABLE 4": "EEG and Face"
        },
        {
          "TABLE 4": "PPG, EDA"
        },
        {
          "TABLE 4": ""
        },
        {
          "TABLE 4": "PPG and Face"
        },
        {
          "TABLE 4": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: and Table 2 Patrasetal.[32] EEG 62.00 57.60",
      "data": [
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "Our (UBVMT)\n81.53\n82.64\nPPG and Face"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "unimodal emotion analysis techniques using EEG signals."
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "Table 3 shows that our scalogram-based PPG representation"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "produces higher\nrecognition accuracy as compared to the"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "EEG-based emotion recognition techniques of Patras et al."
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "[32], Chung et al. [74] and Campos et al. [75]. These results"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "imply that\nthe\nscalogram-based 2D representation of\nthe"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "PPG signal can be used as an alternative to the EEG signal."
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "Obtaining and utilizing EEG signals, the most prevalent bio-"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "signal\nin emotion recognition, can be inconvenient due to"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "the high cost of EEG measurement devices and the cumber-"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "some measurement process, especially for participants from"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "neurodiverse populations such as kids with ASD."
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "5.2\nResults and Analysis of Multi-modal Method"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "The\naverage\nof\nthe\n10-fold cross-validation\naccuracy\nof"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": "the UBVMT multimodal\nemotion recognition method is"
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        },
        {
          "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: and Table 2 Patrasetal.[32] EEG 62.00 57.60",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "5\nRESULTS AND ANALYSIS",
          "9": "TABLE 3"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Performance comparison of our unimodal emotion recognition method"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In this section, the experimental results of both the unimodal",
          "9": "with the state-of-the-art\ntechniques."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition and the multimodal emotion analysis",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using the proposed UBVMT method are discussed in detail.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Study\nModality\nValance\nArousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "MAHNOB-HCI"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "5.1\nResults and Analysis of Unimodal Method",
          "9": "Ferdinando et al. [17]\nECG\n42.55\n47.69"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "46.86\nYu et al. [76]\nHRV\n44.02"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The unimodal emotion recognition performance of the three",
          "9": "Our (Scalogram)\n49.14\nECG\n42.91"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2D representations of ECG/PPG signals for MAHNOB-HCI",
          "9": "DEAP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Patras et al. [32]\nEEG\n62.00\n57.60"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and DEAP datasets are presented in Table 1 and Table 2",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Chung et al. [74]\nEEG\n70.90\n70.10"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "as an average of\nthe 10-fold cross-validation. As it can be",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Campos et al. [75]\nEEG\n73.14\n73.06"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "seen,\nthe scalogram representation of both ECG and PPG",
          "9": "Lee et al. [36]\nPPG\n75.30\n76.20"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Elalamy et al. [56]\nPPG(spectrogram)\n69.60\n69.30"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signals outperforms the emotion recognition performance of",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Elalamy et al. [56]\nPPG(Recurrence\n69.40\n69.20"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the spatiotemporal maps and the SPWVD-based 2D repre-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Plots)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sentation. The superior performance of scalogram represen-",
          "9": "Our (Scalogram)\n76.51\n77.02\nPPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tation is because it\nis more effective at\nidentifying the low-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "frequency or rapidly-changing frequency components of the",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "TABLE 4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signal, as both ECG and PPG are\nlow-frequency signals.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Performance comparison of our multimodal emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Similarly,\nthe emotion recognition accuracy of SPWVD is",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "method with the state-of-the-art\ntechniques."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "higher than the accuracy of spatio-temporal maps because",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "SPWVD offers better\ntime-frequency resolution, and thus",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Study\nModality\nValance\nArousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "captures emotion features more effectively than the spatio-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "MAHNOB-HCI"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "temporal maps.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "76.10\nSoleymani et al. [31]\nEEG and Eye Gaze\n67.70"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "The performance of our unimodal emotion recognition",
          "9": "Koelstra et al. [15]\nEEG and Face\n73.00\n68.50"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Our (UBVMT)\n83.84\nECG and Face\n50.01"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "method is also compared with the state-of-the-art unimodal",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "DEAP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition techniques\non both the MAHNOB-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Yin et al. [53]\nEEG,\nECG,\nEOG,\n76.17\n77.19"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "HCI and DEAP datasets, as presented in Table 3. For\nthe",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "GSR,\nEMG,\nSkin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "MAHNOB-HCI dataset, we\ncompare our\nresult with the",
          "9": "temperature,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Blood\nvolume,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG-based emotion analysis technique proposed by Ferdi-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Respiration"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nando et al. [17] and rPPG-based method of Yu et al. [76]. In",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Shang et al. [13]\nEEG, EOG, EMG\n51.20\n60.90"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[17], Ferdinando et al. extracted heart rate variability (HRV)",
          "9": "Siddharth et al. [7]\nEEG, PPG, GSR\n71.87\n73.05"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "features from ECG signals to classify emotions. As it can be",
          "9": "Siddharth et al. [7]\nEEG and Face\n73.94\n74.13"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Elalamy et al. [56]\nPPG, EDA\n69.90\n69.70"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "seen in Table 3, our scalogram-based method outperforms",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Our (UBVMT)\n81.53\n82.64\nPPG and Face"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the emotion recognition technique proposed in [17]. In [76],",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Yu et al. extracted ten-dimensional HRV features from rPPG",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signals of MAHNOB-HCI video clips, and fed them to a",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "support vector machine\nfor\nemotion classification. Table",
          "9": "unimodal emotion analysis techniques using EEG signals."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "3 shows\nthat\nthey achieve a higher\nrecognition accuracy",
          "9": "Table 3 shows that our scalogram-based PPG representation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in the case of valence, while our scalogram-based method",
          "9": "produces higher\nrecognition accuracy as compared to the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "outperforms their method in arousal categorization. Simi-",
          "9": "EEG-based emotion recognition techniques of Patras et al."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "larly, in Table 3, the performance of our PPG-based emotion",
          "9": "[32], Chung et al. [74] and Campos et al. [75]. These results"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "classification method is compared with the state-of-the-art",
          "9": "imply that\nthe\nscalogram-based 2D representation of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion analysis techniques using the DEAP database. For",
          "9": "PPG signal can be used as an alternative to the EEG signal."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the comparison of\nthe techniques involving only the PPG",
          "9": "Obtaining and utilizing EEG signals, the most prevalent bio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "signal, our 2D scalogram-based method is compared with",
          "9": "signal\nin emotion recognition, can be inconvenient due to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the PPG-based technique proposed by Lee et al.\n[36] and",
          "9": "the high cost of EEG measurement devices and the cumber-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "various\n2D representations\nof PPG signal\nemployed by",
          "9": "some measurement process, especially for participants from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Elalamy et al. [56]. Table 3 shows that our scalogram-based",
          "9": "neurodiverse populations such as kids with ASD."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "PPG technique is more effective than the one-dimensional",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "convolutional\nneural\nnetwork-based (1D CNN)\nemotion",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "5.2\nResults and Analysis of Multi-modal Method"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "analysis method proposed by Lee et al.\n[36]. Similarly, our",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "The\naverage\nof\nthe\n10-fold cross-validation\naccuracy\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "technique produces higher\nrecognition accuracy than the",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "the UBVMT multimodal\nemotion recognition method is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2D spectrogram-based method proposed by Elalamy et al.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[56]. Also, our 2D scalogram representation outperforms the",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition performance of\nthe Recurrence PLot-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "TABLE 5"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based (RP) 2D representation technique used in [56]. Most",
          "9": "F1 scores of our multimodal emotion recognition method."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of\nthe physiological-based emotion analysis methods em-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ploy EEG signals\nfor emotion recognition more than any",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "Study\nDataset\nValance\nArousal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "other physiological\nsignal due to its capability to capture",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "UBVMT\nMAHNOB-HCI\n0.501\n0.846"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion features more effectively. Therefore,\nin Table 3, we",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "9": "UBVMT\nDEAP\n0.815\n0.829"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "also compare our unimodal method with the state-of-the-art",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: shows that our scalogram-based PPG",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n1.0",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "signals [13]. In [56], similar to our approach, Elalamy et al."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "0.87\n0.10\n0.23\n0.04\n0.72\n0.04",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "transformed the 1D PPG and EDA signals into 2D represen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "tations and fuse them to categorize emotions using deep"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n0.8\n0.15",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "0.04\n0.81\n0.26\n0.24\n0.50",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "networks. Table\n4\nshows\nthat our\nscalogram-based PPG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "representation, when fused with the\nface data, produces"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "0.56\n0.11\n0.54\n0.37\n0.09\n0.33",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n0.6",
          "10": "higher\nrecognition accuracy by employing our proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "a",
          "10": "technique. The confusion matrices and the F1 scores of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "b",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "multimodal emotion recognition results are shown in Figure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n0.4",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "6 and Table 5, respectively."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "0.18\n0.82\n0.19\n0.81",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n0.2",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "6\nCONCLUSION"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "0.79\n0.17\n0.83\n0.21",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "In this paper, a novel approach called Unified Biosensor-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "-\n0.0",
          "10": "Vision Multi-modal Transformer-based (UBVMT) method is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "c\nd",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "presented to classify emotions in an arousal-valence space."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Fig. 6. The confusion matrices for\n(a). MAHNOB-HCI valence emotion",
          "10": "The proposed technique combines a 2D representation of an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "recognition, (b). MAHNOB-HCI Arousal emotion recognition, (c). DEAP",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "ECG/PPG signal with facial information for emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "valence emotion recognition, and (d). DEAP arousal emotion recogni-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "nition. Initially, we investigated and compared the unimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "emotion recognition performance using three image-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "representations of\nthe ECG/PPG signal. Then,\nthe Unified"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "shown in Table 4. However, since our method provides a",
          "10": "Biosensor-Vision Multi-modal Transformer-based network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "baseline for multimodal emotion recognition by fusing the",
          "10": "is used for emotion recognition by combining the 2D image-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ECG/PPG signal with the face information,\nthere are no",
          "10": "based representation of\nthe ECG/PPG signal\nand facial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "state-of-the-art methods with which we can compare our",
          "10": "information. Our unified transformer model comprises ho-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "results. Nonetheless,\nas\ncan be\nseen in Table\n4\nthat\nthe",
          "10": "mogeneous transformer blocks\nthat\ntake the 2D represen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "proposed method outperforms the multimodal\ntechniques",
          "10": "tation of\nthe ECG/PPG signal and associated face frame"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "fusing EEG information with other physiological\nsignals,",
          "10": "as input\nfor emotion representation learning with minimal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "especially for the recognition of arousal. For the MAHNOB-",
          "10": "modality-specific design. The UBVMT model\nis trained us-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "HCI dataset, we compare our\nresults with the EEG-based",
          "10": "ing a reconstruction loss involving masked patches of video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "multimodal methods proposed by\nSoleymani\net\nal.\n[31]",
          "10": "frames and 2D images of ECG/PPG signals, along with con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and Koelstra et al.\n[15]. Here again,\nthe proposed method",
          "10": "trastive modeling to align face and ECG/PPG data. Exten-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "outperforms both of\nthese techniques by a large margin in",
          "10": "sive experiments on the MAHNOB-HCI and DEAP datasets"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the recognition of arousal by producing an average accuracy",
          "10": "demonstrate that our Unified Biosensor-Vision Multi-modal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of 83.84%. In contrast,\nthe method proposed by Soleymani",
          "10": "Transformer-based model\nachieves\ncomparable\nresults\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "et al.\n[31] achieves the highest accuracy of 76.10% for the",
          "10": "state-of-the-art techniques."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "classification of the valence class. We argue that the superior",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "performance of our\ntechnique\nin recognizing the arousal",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "ACKNOWLEDGMENTS"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "class\nis because UBVMT is pre-trained by employing an",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "The authors would like\nto thank Mustansar Fiaz, Sachin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "rPPG signal, and it has been reported in past research that",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "Shah, Robinson Vasquez, Lisa Dieker, Shaunn Smith. Re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "PPG signals are more effective in categorizing arousal [29].",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "becca Hines, Ilene Wilkins, Kate Ingraham, Caitlyn Bukaty,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Therefore, given a large enough ECG dataset\nto pre-train",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "Karyn Scott, Eric Imperiale, Wilbert Padilla, Maria Demesa"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the UBVMT network, we posit\nthat we can improve the",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "for\nthe discussions\nand suggestions\nthroughout\nthis\nre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "classification accuracy of our method for the valence class",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "search. This research was supported in part by grants from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "as well. For\nthe DEAP dataset, we\ncompare our\nresults",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "the National Science Foundation Grants 2114808, and from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "with the multimodal\ntechniques\nfusing PPG information",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "the U.S. Department\nof Education Grants H327S210005,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "with other\nsignals\nlike EEG, EDA GSR,\netc. As\ncan be",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "H327S200009. The lead author (KA) was also supported in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "seen in Table 4, our method outperforms the state-of-the-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "part by the University of Central Florida’s Preeminent Post-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "art multimodal\nemotion classification techniques both in",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "doctoral Program (P3). Any opinions, findings, conclusions,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "recognizing valence\nand arousal with average\naccuracies",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "or recommendations expressed in this material are those of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of\n81.53% and 82.64%,\nrespectively.\nIt\nis\ninteresting\nto",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "the authors and do not necessarily reflect\nthe views of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "note that our bimodal method outperforms the multimodal",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "sponsors."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "technique proposed by Yin et al.\n[53], where the informa-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion from EEG, ECG, EOG, GSR, EMG, Skin temperature,",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "blood volume,\nand respiration signals\nare\nfused to clas-",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "REFERENCES"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sify emotions. Similarly, comparing our technique with the",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "[1] LaFleur, Karl, Kaitlin Cassady, Alexander Doud, Kaleb Shades,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "bimodal method proposed by Siddharth et al.\n[7] where",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "Eitan Rogin, and Bin He. ”Quadcopter control in three-dimensional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "EEG and face information are fused for emotion analysis,",
          "10": "space using a noninvasive motor\nimagery-based brain–computer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Table 4 shows that\nthe fusion of PPG and face information",
          "10": "interface.” Journal of neural engineering 10, no. 4 (2013): 046003."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "[2] Mill´an,\nJd R.,\nFrederic Renkens,\nJosep Mourino,\nand Wulfram"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "using UBVMT is more effective in recognizing emotions.",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "Gerstner. ”Noninvasive brain-actuated control of a mobile robot"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Our PPG-face method is also more accurate in classifying",
          "10": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "10": "by human EEG.” IEEE Transactions on biomedical Engineering 51,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotions compared to the fusion of EEG, EOG, and EMG",
          "10": "no. 6 (2004): 1026-1033."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[3] Bamidis, Panagiotis D., Christos Papadelis, Chrysoula Kourtidou-",
          "11": "chical spatial\ninformation learning model.” IEEE Sensors Journal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Papadeli, Costas Pappas, and Ana B. Vivas. ”Affective computing",
          "11": "22, no. 5 (2022): 4359-4368."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in the era of contemporary neurophysiology and health informat-",
          "11": "[24]\nSong, Rencheng,\nSenle Zhang, Chang Li, Yunfei Zhang,\nJuan"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ics.” Interacting with Computers 16, no. 4 (2004): 715-721.",
          "11": "Cheng, and Xun Chen. ”Heart\nrate estimation from facial videos"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[4] Coyle,\nShirley, Yanzhe Wu, King-Tong Lau,\nSarah Brady, Gor-",
          "11": "using a spatiotemporal\nrepresentation with convolutional neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "don Wallace, and Dermot Diamond. ”Bio-sensing textiles-wearable",
          "11": "networks.”\nIEEE Transactions on Instrumentation and Measure-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "chemical biosensors\nfor health monitoring.” In 4th International",
          "11": "ment 69, no. 10 (2020): 7411-7421."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Workshop on Wearable and Implantable Body Sensor Networks",
          "11": "[25] Niu, Xuesong, Hu Han, Shiguang Shan, and Xilin Chen. ”Syn-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(BSN 2007) March 26–28, 2007 RWTH Aachen University, Germany,",
          "11": "rhythm: Learning a deep heart rate estimator from general to spe-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 35-39. Springer Berlin Heidelberg, 2007.",
          "11": "cific.” In 2018 24th International Conference on Pattern Recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[5] Riva, Giuseppe, Fabrizia Mantovani, Claret Samantha Capideville,",
          "11": "(ICPR), pp. 3580-3585. IEEE, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Alessandra Preziosa, Francesca Morganti, Daniela Villani, Andrea",
          "11": "[26] Hsu, Gee-Sern, ArulMurugan Ambikapathi,\nand Ming-Shiang"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gaggioli, Cristina Botella, and Mariano Alca ˜niz. ”Affective inter-",
          "11": "Chen. ”Deep learning with time-frequency representation for pulse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "actions using virtual reality:\nthe link between presence and emo-",
          "11": "estimation from facial videos.”\nIn 2017\nIEEE international\njoint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tions.” Cyberpsychology and behavior 10, no. 1 (2007): 45-56.",
          "11": "conference on biometrics (IJCB), pp. 383-389. IEEE, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[6] Ali, Kamran, and Charles E. Hughes. ”Facial expression recognition",
          "11": "[27] Lu, Hao, Hu Han,\nand S. Kevin Zhou.\n”Dual-gan:\nJoint\nbvp"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "by using a disentangled identity-invariant expression representa-",
          "11": "and noise modeling for\nremote physiological measurement.”\nIn"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tion.” In 2020 25th International Conference on Pattern Recognition",
          "11": "Proceedings of the IEEE/CVF Conference on Computer Vision and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(ICPR), pp. 9460-9467. IEEE, 2021.",
          "11": "Pattern Recognition, pp. 12404-12413. 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[7]\nJung, Tzyy-Ping, and Terrence J. Sejnowski. ”Utilizing deep learn-",
          "11": "[28] Khare, Smith K., and Varun Bajaj. ”Time–frequency representa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ing towards multi-modal bio-sensing and vision-based affective",
          "11": "tion and convolutional neural network-based emotion recognition.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "computing.” IEEE Transactions on Affective Computing 13, no. 1",
          "11": "IEEE transactions on neural networks and learning systems 32, no."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(2019): 96-107.",
          "11": "7 (2020): 2901-2909."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[8]\nFard, Ali Pourramezan, and Mohammad H. Mahoor. ”Ad-corre:",
          "11": "[29] Gasparini, Francesca, Alessandra Grossi,\nand Stefania Bandini."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Adaptive correlation-based loss for facial expression recognition in",
          "11": "”A deep learning approach to recognize cognitive load using ppg"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "the wild.” IEEE Access 10 (2022): 26756-26768.",
          "11": "signals.” In The 14th PErvasive Technologies Related to Assistive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Environments Conference, pp. 489-495. 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[9] Kollias, Dimitrios, and Stefanos Zafeiriou. ”Exploiting multi-cnn",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[30] Liang, Yongbo, Zhencheng Chen, Rabab Ward,\nand Mohamed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "features in cnn-rnn based dimensional emotion recognition on the",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Elgendi.\n”Photoplethysmography and deep learning:\nenhancing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "omg in-the-wild dataset.” IEEE Transactions on Affective Comput-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "hypertension risk stratification.” Biosensors 8, no. 4 (2018): 101."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ing 12, no. 3 (2020): 595-606.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[31]\nSoleymani, Mohammad,\nJeroen Lichtenauer, Thierry Pun,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[10] Mollahosseini, Ali, Behzad Hasani, and Mohammad H. Mahoor.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Maja Pantic. ”A multimodal database for affect\nrecognition and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "”Affectnet: A database for facial expression, valence, and arousal",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "implicit tagging.” IEEE transactions on affective computing 3, no. 1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "computing in the wild.” IEEE Transactions on Affective Computing",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "(2011): 42-55."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "10, no. 1 (2017): 18-31.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[32] Koelstra, Sander, Christian Muhl, Mohammad Soleymani,\nJong-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[11] C. M ¨uhl, B. Allison, A. Nijholt, G. Chanel, A survey of affective",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "brain computer\ninterfaces: principles,\nstate-of-the-art,\nand chal-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Nijholt, and Ioannis Patras. ”Deap: A database for emotion anal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "lenges, Brain-Computer Interfaces 1 (2) (2014) 66–84.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "ysis; using physiological\nsignals.” IEEE transactions on affective"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[12]\nS. D’mello,\nJ. Kory, A Review and Meta-Analysis of Multimodal",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "computing 3, no. 1 (2011): 18-31."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Affect Detection .",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[33] Russell,\nJames A.\n”A circumplex model\nof\naffect.”\nJournal\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[13] Wang, Dan,\nand Yi Shang.\n”Modeling physiological data with",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "personality and social psychology 39, no. 6 (1980): 1161."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "deep belief networks.”\nInternational\njournal of\ninformation and",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[34] Yu, Sung-Nien, Shao-Wei Wang, and Yu Ping Chang. ”Improving"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "education technology (IJIET) 3, no. 5 (2013): 505.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Distinguishability of Photoplethysmography in Emotion Recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[14] Miranda, J., M. Khomami Abadi, N. Sebe, and I. Patras. ”Amigos:",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "tion Using Deep Convolutional Generative Adversarial Networks.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "A dataset for mood, personality and affect research on individuals",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "IEEE Access 10 (2022): 119630-119640."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and groups.” IEEE T Affect Comput (2017): 02.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[35]\nIsmail, Sharifah Noor Masidayu Sayed, Nor Azlina Ab Aziz, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[15] Koelstra, Sander, and Ioannis Patras. ”Fusion of facial expressions",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Siti Zainab Ibrahim. ”A comparison of emotion recognition system"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and EEG for implicit affective tagging.” Image and Vision Comput-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "using electrocardiogram (ECG) and photoplethysmogram (PPG).”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ing 31, no. 2 (2013): 164-174.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "Journal of King Saud University-Computer and Information Sci-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[16]\nFerdinando, Hany, Tapio Sepp¨anen, and Esko Alasaarela. ”Com-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "ences 34, no. 6 (2022): 3539-3558."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "paring features from ECG pattern and HRV analysis for emotion",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[36] Lee, M.S., Lee, Y.K., Pae, D.S., Lim, M.T., Kim, D.W. and Kang, T.K.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "recognition system.” In 2016 IEEE Conference on Computational",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "2019. Fast emotion recognition based on single pulse PPG signal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Intelligence in Bioinformatics and Computational Biology (CIBCB),",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "with convolutional neural network. Applied Sciences, 9(16), p.3355."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp. 1-6. IEEE, 2016.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[37]\nSep ´ulveda, A., Castillo, F., Palma, C. and Rodriguez-Fernandez,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[17]\nFerdinando,\nHany,\nLiang\nYe,\nTapio\nSepp¨anen,\nand\nEsko",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "M.,\n2021. Emotion recognition from ECG signals using wavelet"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Alasaarela. ”Emotion recognition by heart\nrate variability.” Aus-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "scattering and machine learning. Applied Sciences, 11(11), p.4945."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tralian Journal of Basic and Applied Science 8, no. 14 (2014): 50-55.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[38]\nSarkar, P. and Etemad, A., 2020. Self-supervised ECG representa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[18] Vaswani, Ashish, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit,",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "tion learning for emotion recognition. IEEE Transactions on Affec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "tive Computing, 13(3), pp.1541-1554."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "”Attention is all you need.” Advances in neural\ninformation pro-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[39] Mellouk, W. and Handouzi, W., 2023. CNN-LSTM for automatic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cessing systems 30 (2017).",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "emotion recognition using contactless photoplythesmographic sig-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[19] Le, Hoai-Duy, Guee-Sang Lee, Soo-Hyung Kim, Seungwon Kim,",
          "11": "nals. Biomedical Signal Processing and Control, 85, p.104907."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "and Hyung-Jeong Yang. ”Multi-Label Multimodal Emotion Recog-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "[40] Ali, K. and Hughes, C.E., 2021, January. Facial expression recogni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition With Transformer-Based Fusion and Emotion-Level Repre-",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "tion by using a disentangled identity-invariant expression represen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sentation Learning.” IEEE Access 11 (2023): 14742-14751.",
          "11": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "11": "tation. In 2020 25th International Conference on Pattern Recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[20] Mocanu, Bogdan, Ruxandra Tapu, and Titus Zaharia. ”Multimodal",
          "11": "(ICPR) (pp. 9460-9467). IEEE."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition using cross modal audio-video fusion with",
          "11": "[41] Li, H., Wang, N., Yang, X., Wang, X. and Gao, X., 2022. Towards"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "attention and deep metric learning.” Image and Vision Computing",
          "11": "semi-supervised deep facial expression recognition with an adap-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(2023): 104676.",
          "11": "tive confidence margin. In Proceedings of the IEEE/CVF conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[21] Yu, Jianfei, Kai Chen, and Rui Xia. ”Hierarchical\ninteractive mul-",
          "11": "on computer vision and pattern recognition (pp. 4166-4175)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "timodal transformer for aspect-based multimodal sentiment analy-",
          "11": "[42] Wang, H., Li, B., Wu, S., Shen, S., Liu, F., Ding, S. and Zhou, A.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sis.” IEEE Transactions on Affective Computing (2022).",
          "11": "2023. Rethinking the Learning Paradigm for Dynamic Facial Ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[22] Tang,\nZineng,\nJaemin\nCho,\nYixin Nie,\nand Mohit\nBansal.",
          "11": "pression Recognition. In Proceedings of the IEEE/CVF Conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "”TVLT:\nTextless Vision-Language\nTransformer.”\narXiv\npreprint",
          "11": "on Computer Vision and Pattern Recognition (pp. 17958-17968)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "arXiv:2209.14156 (2022).",
          "11": "[43] Lo, L., Ruan, B.K., Shuai, H.H. and Cheng, W.H., 2023. Model-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[23] Wang, Zhe, Yongxiong Wang, Chuanfei Hu, Zhong Yin, and Yu",
          "11": "ing Uncertainty for Low-Resolution Facial Expression Recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Song. ”Transformers for EEG-based emotion recognition: A hierar-",
          "11": "IEEE Transactions on Affective Computing."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[44] Ali, K.\nand Hughes, C.E.,\n2020. Face\nreenactment based facial",
          "12": "and-language tasks.” Advances\nin neural\ninformation processing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "expression recognition.\nIn Advances\nin Visual Computing:\n15th",
          "12": "systems 32 (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "International Symposium, ISVC 2020, San Diego, CA, USA, October",
          "12": "[66] Kim, Wonjae, Bokyung Son,\nand Ildoo Kim.\n”Vilt: Vision-and-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "5–7, 2020, Proceedings, Part\nI 15 (pp. 501-513). Springer\nInterna-",
          "12": "language transformer without convolution or region supervision.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tional Publishing.",
          "12": "In International Conference on Machine Learning, pp. 5583-5594."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[45] Russell,\nJ.A., 1980. A circumplex model of affect.\nJournal of per-",
          "12": "PMLR, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sonality and social psychology, 39(6), p.1161.",
          "12": "[67] Chen, Yen-Chun, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[46] Thayer, R.E., 1990. The biopsychology of mood and arousal. Ox-",
          "12": "Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. ”Uniter: Learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ford University Press.",
          "12": "universal image-text representations.” (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[68] Zadeh, AmirAli Bagher, Paul Pu Liang, Soujanya Poria, Erik Cam-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[47] Gunes, H.\nand Pantic, M.,\n2010. Automatic, dimensional\nand",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "bria, and Louis-Philippe Morency. ”Multimodal\nlanguage analysis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "continuous emotion recognition. International Journal of Synthetic",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "in the wild: Cmu-mosei dataset and interpretable dynamic fusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Emotions (IJSE), 1(1), pp.68-99.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "graph.” In Proceedings of\nthe 56th Annual Meeting of\nthe Associ-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[48] Kollias, D. and Zafeiriou, S., 2018. A multi-component CNN-RNN",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ation for Computational Linguistics (Volume 1: Long Papers), pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "approach for dimensional emotion recognition in-the-wild. arXiv",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "2236-2246. 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "preprint arXiv:1805.01452.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[69] Wu, Yi-Chiao, Li-Wen Chiu, Chun-Chih Lai, Bing-Fei Wu, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[49] Deng, D., Chen, Z.\nand Shi, B.E.,\n2020, November. Multitask",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Sunny SJ Lin.\n”Recognizing, Fast\nand Slow: Complex Emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition with incomplete labels.\nIn 2020 15th IEEE In-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Recognition with Facial Expression Detection and Remote Physi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ternational Conference on Automatic Face and Gesture Recognition",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ological Measurement.” IEEE Transactions on Affective Computing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(FG 2020) (pp. 592-599). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "(2023)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[50] Kuhnke, F., Rumberg, L.\nand Ostermann,\nJ.,\n2020, November.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[70] Liu, Xin, Josh Fromm, Shwetak Patel, and Daniel McDuff. ”Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Two-stream aural-visual affect analysis\nin the wild.\nIn 2020 15th",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "task temporal\nshift attention networks\nfor on-device\ncontactless"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE International Conference\non Automatic Face\nand Gesture",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "vitals measurement.” Advances in Neural\nInformation Processing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Recognition (FG 2020) (pp. 600-605). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Systems 33 (2020): 19400-19411."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[51] Zhang, Y.H., Huang, R., Zeng, J. and Shan, S., 2020, November. M",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[71] Yu, Zitong, Wei Peng, Xiaobai Li, Xiaopeng Hong, and Guoying"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "3 f: Multi-modal continuous valence-arousal estimation in the wild.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Zhao. ”Remote heart\nrate measurement\nfrom highly compressed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In 2020 15th IEEE International Conference on Automatic Face and",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "facial videos:\nan end-to-end deep learning solution with video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gesture Recognition (FG 2020) (pp. 632-636). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "enhancement.” In Proceedings of the IEEE/CVF International Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[52] Zeng, Z., Pantic, M. and Huang, T.S., 2009. Emotion recognition",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ference on Computer Vision, pp. 151-160. 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based on multimodal\ninformation.\nIn Affective information pro-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[72] Kingma, Diederik\nP\n.,\nand\nJimmy\nBa.\n”Adam: A method\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cessing (pp. 241-265). London: Springer London.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[53] Yin, Z., Zhao, M., Wang, Y., Yang,\nJ. and Zhang,\nJ., 2017. Recog-",
          "12": "[73]\nIlya Loshchilov and Frank Hutter. ”Decoupled weight decay reg-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition of emotions using multimodal physiological signals and an",
          "12": "ularization”. In ICLR, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ensemble deep learning model. Computer methods and programs",
          "12": "[74] Yoon, Hyun Joong, and Seong Youb Chung. ”EEG-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in biomedicine, 140, pp.93-110.",
          "12": "tion estimation using Bayesian weighted-log-posterior function and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[54] Katsigiannis, S. and Ramzan, N., 2017. DREAMER: A database",
          "12": "perceptron convergence\nalgorithm.” Computers\nin biology\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "for emotion recognition through EEG and ECG signals from wire-",
          "12": "medicine 43, no. 12 (2013): 2230-2237."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "less low-cost off-the-shelf devices.\nIEEE journal of biomedical and",
          "12": "[75] Atkinson, John, and Daniel Campos. ”Improving BCI-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "health informatics, 22(1), pp.98-107.",
          "12": "tion recognition by combining EEG feature selection and kernel"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[55]\nSantamaria-Granados,\nL.,\nMunoz-Organero,\nM.,\nRamirez-",
          "12": "classifiers.” Expert Systems with Applications 47 (2016): 35-41."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gonzalez, G., Abdulhay, E. and Arunkumar, N.J.I.A., 2018. Using",
          "12": "[76] Yu, Zitong, Xiaobai Li, and Guoying Zhao. ”Remote photoplethys-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "deep convolutional neural network for\nemotion detection on a",
          "12": "mograph\nsignal measurement\nfrom facial\nvideos using\nspatio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "physiological signals dataset (AMIGOS). IEEE Access, 7, pp.57-67.",
          "12": "temporal networks.” arXiv preprint arXiv:1905.02419 (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[56] Elalamy, R., Fanourakis, M.\nand Chanel, G.,\n2021,\nSeptember.",
          "12": "[77] Zhang, Kaipeng, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. ”Joint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Multi-modal emotion recognition using recurrence plots and trans-",
          "12": "face detection and alignment using multitask cascaded convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "fer learning on physiological signals. In 2021 9th International Con-",
          "12": "tional networks.” IEEE signal processing letters 23, no. 10 (2016):"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ference on Affective Computing and Intelligent\nInteraction (ACII)",
          "12": "1499-1503."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(pp. 1-7). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[57]\nSoleymani, M., Asghari-Esfeden, S., Fu, Y. and Pantic, M., 2015.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "is a postdoctoral\nre-\nKamran Ali Kamran Ali"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Analysis\nof EEG signals\nand facial\nexpressions\nfor\ncontinuous",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "search associate in the computer science de-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion detection. IEEE Transactions on Affective Computing, 7(1),",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "partment at\nthe University of Central Florida,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp.17-28.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Orlando, FL, 32816, USA. His research interests"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[58] Wang,\nS., Qu,\nJ., Zhang, Y.\nand Zhang, Y.,\n2023. Multimodal",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "include computer vision and machine learning."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Emotion Recognition From EEG Signals and Facial Expressions.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "He is the corresponding author of\nthis article."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Access, 11, pp.33061-33068.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Contact him at kamran.ali@ucf.edu."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[59] de\nSouza Neto, E.P., Custaud, M.A.,\nFrutoso,\nJ.,\nSomody, L.,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gharib, C. and Fortrat,\nJ.O., 2001. Smoothed pseudo Wigner–Ville",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "distribution as an alternative to Fourier\ntransform in rats. Auto-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nomic Neuroscience, 87(2-3), pp.258-267.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[60]\nScholl, Stefan. ”Fourier, Gabor, Morlet or Wigner: comparison of",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "time-frequency transforms.” arXiv preprint arXiv:2101.06707 (2021).",
          "12": "Charles E. Hughes Charles E. Hughes is a pe-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[61] Antoine, J-P., Pierre Carrette, Romain Murenzi, and Bernard Piette.",
          "12": "gasus professor of Computer Science at the Uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "”Image analysis with two-dimensional continuous wavelet\ntrans-",
          "12": "versity of Central Florida, Orlando, FL, 32816,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "form.” Signal processing 31, no. 3 (1993): 241-272.",
          "12": "USA. His research interests include virtual\nlearn-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[62] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ”Deep",
          "12": "ing environments, computer graphics, machine"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "residual learning for image recognition.” In Proceedings of the IEEE",
          "12": "learning, and visual programming systems. Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "conference on computer vision and pattern recognition, pp. 770-778.",
          "12": "tact him at charles.hughes@ucf.edu."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2016.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[63] He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P. and Girshick, R., 2022.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Masked autoencoders are scalable vision learners.\nIn Proceedings",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the IEEE/CVF conference on computer vision and pattern recog-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition (pp. 16000-16009).",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[64] Tan, Hao, and Mohit Bansal. ”Lxmert: Learning cross-modality",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "encoder\nrepresentations\nfrom\ntransformers.”\narXiv\npreprint",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "arXiv:1908.07490 (2019).",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[65] Lu,\nJiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. ”Vilbert:",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Pretraining task-agnostic visiolinguistic representations for vision-",
          "12": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[44] Ali, K.\nand Hughes, C.E.,\n2020. Face\nreenactment based facial",
          "12": "and-language tasks.” Advances\nin neural\ninformation processing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "expression recognition.\nIn Advances\nin Visual Computing:\n15th",
          "12": "systems 32 (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "International Symposium, ISVC 2020, San Diego, CA, USA, October",
          "12": "[66] Kim, Wonjae, Bokyung Son,\nand Ildoo Kim.\n”Vilt: Vision-and-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "5–7, 2020, Proceedings, Part\nI 15 (pp. 501-513). Springer\nInterna-",
          "12": "language transformer without convolution or region supervision.”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "tional Publishing.",
          "12": "In International Conference on Machine Learning, pp. 5583-5594."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[45] Russell,\nJ.A., 1980. A circumplex model of affect.\nJournal of per-",
          "12": "PMLR, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "sonality and social psychology, 39(6), p.1161.",
          "12": "[67] Chen, Yen-Chun, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[46] Thayer, R.E., 1990. The biopsychology of mood and arousal. Ox-",
          "12": "Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. ”Uniter: Learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ford University Press.",
          "12": "universal image-text representations.” (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[68] Zadeh, AmirAli Bagher, Paul Pu Liang, Soujanya Poria, Erik Cam-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[47] Gunes, H.\nand Pantic, M.,\n2010. Automatic, dimensional\nand",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "bria, and Louis-Philippe Morency. ”Multimodal\nlanguage analysis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "continuous emotion recognition. International Journal of Synthetic",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "in the wild: Cmu-mosei dataset and interpretable dynamic fusion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Emotions (IJSE), 1(1), pp.68-99.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "graph.” In Proceedings of\nthe 56th Annual Meeting of\nthe Associ-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[48] Kollias, D. and Zafeiriou, S., 2018. A multi-component CNN-RNN",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ation for Computational Linguistics (Volume 1: Long Papers), pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "approach for dimensional emotion recognition in-the-wild. arXiv",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "2236-2246. 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "preprint arXiv:1805.01452.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[69] Wu, Yi-Chiao, Li-Wen Chiu, Chun-Chih Lai, Bing-Fei Wu, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[49] Deng, D., Chen, Z.\nand Shi, B.E.,\n2020, November. Multitask",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Sunny SJ Lin.\n”Recognizing, Fast\nand Slow: Complex Emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion recognition with incomplete labels.\nIn 2020 15th IEEE In-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Recognition with Facial Expression Detection and Remote Physi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ternational Conference on Automatic Face and Gesture Recognition",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ological Measurement.” IEEE Transactions on Affective Computing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(FG 2020) (pp. 592-599). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "(2023)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[50] Kuhnke, F., Rumberg, L.\nand Ostermann,\nJ.,\n2020, November.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[70] Liu, Xin, Josh Fromm, Shwetak Patel, and Daniel McDuff. ”Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Two-stream aural-visual affect analysis\nin the wild.\nIn 2020 15th",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "task temporal\nshift attention networks\nfor on-device\ncontactless"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE International Conference\non Automatic Face\nand Gesture",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "vitals measurement.” Advances in Neural\nInformation Processing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Recognition (FG 2020) (pp. 600-605). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Systems 33 (2020): 19400-19411."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[51] Zhang, Y.H., Huang, R., Zeng, J. and Shan, S., 2020, November. M",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[71] Yu, Zitong, Wei Peng, Xiaobai Li, Xiaopeng Hong, and Guoying"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "3 f: Multi-modal continuous valence-arousal estimation in the wild.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Zhao. ”Remote heart\nrate measurement\nfrom highly compressed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "In 2020 15th IEEE International Conference on Automatic Face and",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "facial videos:\nan end-to-end deep learning solution with video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gesture Recognition (FG 2020) (pp. 632-636). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "enhancement.” In Proceedings of the IEEE/CVF International Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[52] Zeng, Z., Pantic, M. and Huang, T.S., 2009. Emotion recognition",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "ference on Computer Vision, pp. 151-160. 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "based on multimodal\ninformation.\nIn Affective information pro-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "[72] Kingma, Diederik\nP\n.,\nand\nJimmy\nBa.\n”Adam: A method\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "cessing (pp. 241-265). London: Springer London.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[53] Yin, Z., Zhao, M., Wang, Y., Yang,\nJ. and Zhang,\nJ., 2017. Recog-",
          "12": "[73]\nIlya Loshchilov and Frank Hutter. ”Decoupled weight decay reg-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition of emotions using multimodal physiological signals and an",
          "12": "ularization”. In ICLR, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ensemble deep learning model. Computer methods and programs",
          "12": "[74] Yoon, Hyun Joong, and Seong Youb Chung. ”EEG-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "in biomedicine, 140, pp.93-110.",
          "12": "tion estimation using Bayesian weighted-log-posterior function and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[54] Katsigiannis, S. and Ramzan, N., 2017. DREAMER: A database",
          "12": "perceptron convergence\nalgorithm.” Computers\nin biology\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "for emotion recognition through EEG and ECG signals from wire-",
          "12": "medicine 43, no. 12 (2013): 2230-2237."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "less low-cost off-the-shelf devices.\nIEEE journal of biomedical and",
          "12": "[75] Atkinson, John, and Daniel Campos. ”Improving BCI-based emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "health informatics, 22(1), pp.98-107.",
          "12": "tion recognition by combining EEG feature selection and kernel"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[55]\nSantamaria-Granados,\nL.,\nMunoz-Organero,\nM.,\nRamirez-",
          "12": "classifiers.” Expert Systems with Applications 47 (2016): 35-41."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gonzalez, G., Abdulhay, E. and Arunkumar, N.J.I.A., 2018. Using",
          "12": "[76] Yu, Zitong, Xiaobai Li, and Guoying Zhao. ”Remote photoplethys-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "deep convolutional neural network for\nemotion detection on a",
          "12": "mograph\nsignal measurement\nfrom facial\nvideos using\nspatio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "physiological signals dataset (AMIGOS). IEEE Access, 7, pp.57-67.",
          "12": "temporal networks.” arXiv preprint arXiv:1905.02419 (2019)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[56] Elalamy, R., Fanourakis, M.\nand Chanel, G.,\n2021,\nSeptember.",
          "12": "[77] Zhang, Kaipeng, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. ”Joint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Multi-modal emotion recognition using recurrence plots and trans-",
          "12": "face detection and alignment using multitask cascaded convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "fer learning on physiological signals. In 2021 9th International Con-",
          "12": "tional networks.” IEEE signal processing letters 23, no. 10 (2016):"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "ference on Affective Computing and Intelligent\nInteraction (ACII)",
          "12": "1499-1503."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "(pp. 1-7). IEEE.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[57]\nSoleymani, M., Asghari-Esfeden, S., Fu, Y. and Pantic, M., 2015.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "is a postdoctoral\nre-\nKamran Ali Kamran Ali"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Analysis\nof EEG signals\nand facial\nexpressions\nfor\ncontinuous",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "search associate in the computer science de-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "emotion detection. IEEE Transactions on Affective Computing, 7(1),",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "partment at\nthe University of Central Florida,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "pp.17-28.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Orlando, FL, 32816, USA. His research interests"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[58] Wang,\nS., Qu,\nJ., Zhang, Y.\nand Zhang, Y.,\n2023. Multimodal",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "include computer vision and machine learning."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Emotion Recognition From EEG Signals and Facial Expressions.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "He is the corresponding author of\nthis article."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "IEEE Access, 11, pp.33061-33068.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "",
          "12": "Contact him at kamran.ali@ucf.edu."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[59] de\nSouza Neto, E.P., Custaud, M.A.,\nFrutoso,\nJ.,\nSomody, L.,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Gharib, C. and Fortrat,\nJ.O., 2001. Smoothed pseudo Wigner–Ville",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "distribution as an alternative to Fourier\ntransform in rats. Auto-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nomic Neuroscience, 87(2-3), pp.258-267.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[60]\nScholl, Stefan. ”Fourier, Gabor, Morlet or Wigner: comparison of",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "time-frequency transforms.” arXiv preprint arXiv:2101.06707 (2021).",
          "12": "Charles E. Hughes Charles E. Hughes is a pe-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[61] Antoine, J-P., Pierre Carrette, Romain Murenzi, and Bernard Piette.",
          "12": "gasus professor of Computer Science at the Uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "”Image analysis with two-dimensional continuous wavelet\ntrans-",
          "12": "versity of Central Florida, Orlando, FL, 32816,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "form.” Signal processing 31, no. 3 (1993): 241-272.",
          "12": "USA. His research interests include virtual\nlearn-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[62] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ”Deep",
          "12": "ing environments, computer graphics, machine"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "residual learning for image recognition.” In Proceedings of the IEEE",
          "12": "learning, and visual programming systems. Con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "conference on computer vision and pattern recognition, pp. 770-778.",
          "12": "tact him at charles.hughes@ucf.edu."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "2016.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[63] He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P. and Girshick, R., 2022.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Masked autoencoders are scalable vision learners.\nIn Proceedings",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "of the IEEE/CVF conference on computer vision and pattern recog-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "nition (pp. 16000-16009).",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[64] Tan, Hao, and Mohit Bansal. ”Lxmert: Learning cross-modality",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "encoder\nrepresentations\nfrom\ntransformers.”\narXiv\npreprint",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "arXiv:1908.07490 (2019).",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "[65] Lu,\nJiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. ”Vilbert:",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015": "Pretraining task-agnostic visiolinguistic representations for vision-",
          "12": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Quadcopter control in three-dimensional space using a noninvasive motor imagery-based brain-computer interface",
      "authors": [
        "Karl Lafleur",
        "Kaitlin Cassady",
        "Alexander Doud",
        "Kaleb Shades"
      ],
      "year": "2013",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "2",
      "title": "Noninvasive brain-actuated control of a mobile robot by human EEG",
      "authors": [
        "Jd Millán",
        "Frederic Renkens",
        "Josep Mourino",
        "Wulfram Gerstner"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on biomedical Engineering"
    },
    {
      "citation_id": "3",
      "title": "Affective computing in the era of contemporary neurophysiology and health informatics",
      "authors": [
        "Panagiotis Bamidis",
        "Christos Papadelis",
        "Chrysoula Kourtidou-Papadeli",
        "Costas Pappas",
        "Ana Vivas"
      ],
      "year": "2004",
      "venue": "Interacting with Computers"
    },
    {
      "citation_id": "4",
      "title": "Bio-sensing textiles-wearable chemical biosensors for health monitoring",
      "authors": [
        "Shirley Coyle",
        "Yanzhe Wu",
        "King-Tong Lau",
        "Sarah Brady",
        "Gordon Wallace",
        "Dermot Diamond"
      ],
      "year": "2007",
      "venue": "4th International Workshop on Wearable and Implantable Body Sensor Networks (BSN 2007)"
    },
    {
      "citation_id": "5",
      "title": "Affective interactions using virtual reality: the link between presence and emotions",
      "authors": [
        "Giuseppe Riva",
        "Fabrizia Mantovani",
        "Samantha Claret",
        "Alessandra Capideville",
        "Francesca Preziosa",
        "Daniela Morganti",
        "Andrea Villani",
        "Gaggioli"
      ],
      "year": "2007",
      "venue": "Cyberpsychology and behavior"
    },
    {
      "citation_id": "6",
      "title": "Facial expression recognition by using a disentangled identity-invariant expression representation",
      "authors": [
        "Kamran Ali",
        "Charles Hughes"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "7",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "Tzyy-Ping Jung",
        "Terrence Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "Ali Fard",
        "Mohammad Pourramezan",
        "Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "B Allison",
        "A Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "12",
      "title": "A Review and Meta-Analysis of Multimodal Affect Detection",
      "authors": [
        "J Kory"
      ],
      "venue": "A Review and Meta-Analysis of Multimodal Affect Detection"
    },
    {
      "citation_id": "13",
      "title": "Modeling physiological data with deep belief networks",
      "authors": [
        "Dan Wang",
        "Yi Shang"
      ],
      "year": "2013",
      "venue": "International journal of information and education technology (IJIET)"
    },
    {
      "citation_id": "14",
      "title": "Amigos: A dataset for mood, personality and affect research on individuals and groups",
      "authors": [
        "J Miranda",
        "M Khomami Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2017",
      "venue": "IEEE T Affect Comput"
    },
    {
      "citation_id": "15",
      "title": "Fusion of facial expressions and EEG for implicit affective tagging",
      "authors": [
        "Sander Koelstra",
        "Ioannis Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "16",
      "title": "Comparing features from ECG pattern and HRV analysis for emotion recognition system",
      "authors": [
        "Hany Ferdinando",
        "Tapio Seppänen",
        "Esko Alasaarela"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition by heart rate variability",
      "authors": [
        "Hany Ferdinando",
        "Liang Ye",
        "Tapio Seppänen",
        "Esko Alasaarela"
      ],
      "year": "2014",
      "venue": "Australian Journal of Basic and Applied Science"
    },
    {
      "citation_id": "18",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Multi-Label Multimodal Emotion Recognition With Transformer-Based Fusion and Emotion-Level Representation Learning",
      "authors": [
        "Hoai-Duy Le",
        "Guee-Sang Lee",
        "Soo-Hyung Kim",
        "Seungwon Kim",
        "Hyung-Jeong Yang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Multimodal emotion recognition using cross modal audio-video fusion with attention and deep metric learning",
      "authors": [
        "Bogdan Mocanu",
        "Ruxandra Tapu",
        "Titus Zaharia"
      ],
      "year": "2023",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "21",
      "title": "Hierarchical interactive multimodal transformer for aspect-based multimodal sentiment analysis",
      "authors": [
        "Jianfei Yu",
        "Kai Chen",
        "Rui Xia"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "TVLT: Textless Vision-Language Transformer",
      "authors": [
        "Zineng Tang",
        "Jaemin Cho",
        "Yixin Nie",
        "Mohit Bansal"
      ],
      "year": "2022",
      "venue": "TVLT: Textless Vision-Language Transformer",
      "arxiv": "arXiv:2209.14156"
    },
    {
      "citation_id": "23",
      "title": "Transformers for EEG-based emotion recognition: A hierar-chical spatial information learning model",
      "authors": [
        "Zhe Wang",
        "Yongxiong Wang",
        "Chuanfei Hu",
        "Zhong Yin",
        "Yu Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "24",
      "title": "Heart rate estimation from facial videos using a spatiotemporal representation with convolutional neural networks",
      "authors": [
        "Rencheng Song",
        "Senle Zhang",
        "Chang Li",
        "Yunfei Zhang",
        "Juan Cheng",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "25",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "26",
      "title": "Deep learning with time-frequency representation for pulse estimation from facial videos",
      "authors": [
        "Hsu",
        "Arulmurugan Gee-Sern",
        "Ming-Shiang Ambikapathi",
        "Chen"
      ],
      "year": "2017",
      "venue": "2017 IEEE international joint conference on biometrics (IJCB)"
    },
    {
      "citation_id": "27",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "Hao Lu",
        "Hu Han",
        "Kevin Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "Smith Khare",
        "Varun Bajaj"
      ],
      "year": "2020",
      "venue": "Time-frequency representation and convolutional neural network-based emotion recognition"
    },
    {
      "citation_id": "29",
      "title": "A deep learning approach to recognize cognitive load using ppg signals",
      "authors": [
        "Francesca Gasparini",
        "Alessandra Grossi",
        "Stefania Bandini"
      ],
      "year": "2021",
      "venue": "The 14th PErvasive Technologies Related to Assistive Environments Conference"
    },
    {
      "citation_id": "30",
      "title": "Photoplethysmography and deep learning: enhancing hypertension risk stratification",
      "authors": [
        "Yongbo Liang",
        "Zhencheng Chen",
        "Rabab Ward",
        "Mohamed Elgendi"
      ],
      "year": "2018",
      "venue": "Biosensors"
    },
    {
      "citation_id": "31",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "32",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "A circumplex model of affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "34",
      "title": "Improving Distinguishability of Photoplethysmography in Emotion Recognition Using Deep Convolutional Generative Adversarial Networks",
      "authors": [
        "Sung Yu",
        "Shao-Wei Nien",
        "Yu Wang",
        "Chang Ping"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "A comparison of emotion recognition system using electrocardiogram (ECG) and photoplethysmogram (PPG)",
      "authors": [
        "Sharifah Ismail",
        "Masidayu Noor",
        "Nor Sayed",
        "Azlina Ab",
        "Siti Aziz",
        "Ibrahim Zainab"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "36",
      "title": "Fast emotion recognition based on single pulse PPG signal with convolutional neural network",
      "authors": [
        "M Lee",
        "Y Lee",
        "D Pae",
        "M Lim",
        "D Kim",
        "T Kang"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition from ECG signals using wavelet scattering and machine learning",
      "authors": [
        "A Sep Úlveda",
        "F Castillo",
        "C Palma",
        "M Rodriguez-Fernandez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "38",
      "title": "Self-supervised ECG representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "CNN-LSTM for automatic emotion recognition using contactless photoplythesmographic signals",
      "authors": [
        "W Mellouk",
        "W Handouzi"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition by using a disentangled identity-invariant expression representation",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "41",
      "title": "Towards semi-supervised deep facial expression recognition with an adaptive confidence margin",
      "authors": [
        "H Li",
        "N Wang",
        "X Yang",
        "X Wang",
        "X Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Modeling Uncertainty for Low-Resolution Facial Expression Recognition",
      "authors": [
        "L Lo",
        "B Ruan",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Face reenactment based facial expression recognition",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2020",
      "venue": "Advances in Visual Computing: 15th International Symposium, ISVC 2020"
    },
    {
      "citation_id": "45",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "46",
      "title": "The biopsychology of mood and arousal",
      "authors": [
        "R Thayer"
      ],
      "year": "1990",
      "venue": "The biopsychology of mood and arousal"
    },
    {
      "citation_id": "47",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "48",
      "title": "A multi-component CNN-RNN approach for dimensional emotion recognition in-the-wild",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-component CNN-RNN approach for dimensional emotion recognition in-the-wild",
      "arxiv": "arXiv:1805.01452"
    },
    {
      "citation_id": "49",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "50",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "51",
      "title": "M 3 f: Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Y Zhang",
        "R Huang",
        "J Zeng",
        "S Shan"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition based on multimodal information",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "T Huang"
      ],
      "year": "2009",
      "venue": "Affective information processing"
    },
    {
      "citation_id": "53",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "54",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "55",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-Gonzalez",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "56",
      "title": "Multi-modal emotion recognition using recurrence plots and transfer learning on physiological signals",
      "authors": [
        "R Elalamy",
        "M Fanourakis",
        "G Chanel"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "57",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Multimodal Emotion Recognition From EEG Signals and Facial Expressions",
      "authors": [
        "S Wang",
        "J Qu",
        "Y Zhang",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "59",
      "title": "Smoothed pseudo Wigner-Ville distribution as an alternative to Fourier transform in rats",
      "authors": [
        "E De Souza Neto",
        "M Custaud",
        "J Frutoso",
        "L Somody",
        "C Gharib",
        "J Fortrat"
      ],
      "year": "2001",
      "venue": "Neuroscience"
    },
    {
      "citation_id": "60",
      "title": "Fourier, Gabor, Morlet or Wigner: comparison of time-frequency transforms",
      "authors": [
        "Stefan Scholl"
      ],
      "year": "2021",
      "venue": "Fourier, Gabor, Morlet or Wigner: comparison of time-frequency transforms",
      "arxiv": "arXiv:2101.06707"
    },
    {
      "citation_id": "61",
      "title": "Image analysis with two-dimensional continuous wavelet transform",
      "authors": [
        "J-P Antoine",
        "Romain Pierre Carrette",
        "Bernard Murenzi",
        "Piette"
      ],
      "year": "1993",
      "venue": "Signal processing"
    },
    {
      "citation_id": "62",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "63",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "64",
      "title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "Lxmert: Learning cross-modality encoder representations from transformers",
      "arxiv": "arXiv:1908.07490"
    },
    {
      "citation_id": "65",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "66",
      "title": "Vilt: Vision-andlanguage transformer without convolution or region supervision",
      "authors": [
        "Wonjae Kim",
        "Bokyung Son",
        "Ildoo Kim"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "67",
      "title": "Uniter: Learning universal image-text representations",
      "authors": [
        "Yen Chen",
        "Linjie Chun",
        "Licheng Li",
        "Ahmed Yu",
        "Faisal Kholy",
        "Zhe Ahmed",
        "Yu Gan",
        "Jingjing Cheng",
        "Liu"
      ],
      "year": "2019",
      "venue": "Uniter: Learning universal image-text representations"
    },
    {
      "citation_id": "68",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Zadeh",
        "Paul Bagher",
        "Soujanya Liang",
        "Erik Poria",
        "Louis-Philippe Cambria",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "69",
      "title": "Recognizing, Fast and Slow: Complex Emotion Recognition with Facial Expression Detection and Remote Physiological Measurement",
      "authors": [
        "Yi-Chiao Wu",
        "Li-Wen Chiu",
        "Chun-Chih Lai",
        "Bing-Fei Wu",
        "Sunny Sj Lin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Multitask temporal shift attention networks for on-device contactless vitals measurement",
      "authors": [
        "Xin Liu",
        "Josh Fromm",
        "Shwetak Patel",
        "Daniel Mcduff"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "71",
      "title": "Remote heart rate measurement from highly compressed facial videos: an end-to-end deep learning solution with video enhancement",
      "authors": [
        "Zitong Yu",
        "Wei Peng",
        "Xiaobai Li",
        "Xiaopeng Hong",
        "Guoying Zhao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "72",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "Diederik Kingma",
        "Jimmy Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "73",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "74",
      "title": "EEG-based emotion estimation using Bayesian weighted-log-posterior function and perceptron convergence algorithm",
      "authors": [
        "Hyun Yoon",
        "Seong Joong",
        "Chung Youb"
      ],
      "year": "2013",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "75",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "John Atkinson",
        "Daniel Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "76",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatiotemporal networks",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2019",
      "venue": "Remote photoplethysmograph signal measurement from facial videos using spatiotemporal networks",
      "arxiv": "arXiv:1905.02419"
    },
    {
      "citation_id": "77",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "78",
      "title": "Ali is a postdoctoral research associate in the computer science department at the University of Central Florida",
      "authors": [
        "Kamran Ali"
      ],
      "venue": "Ali is a postdoctoral research associate in the computer science department at the University of Central Florida"
    },
    {
      "citation_id": "79",
      "title": "His research interests include virtual learning environments, computer graphics, machine learning, and visual programming systems",
      "authors": [
        "Charles Hughes Charles"
      ],
      "venue": "His research interests include virtual learning environments, computer graphics, machine learning, and visual programming systems"
    }
  ]
}