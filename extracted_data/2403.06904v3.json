{
  "paper_id": "2403.06904v3",
  "title": "Human Pose Descriptions And Subject-Focused Attention For Improved Zero-Shot Transfer In Human-Centric Classification Tasks",
  "published": "2024-03-11T16:56:37Z",
  "authors": [
    "Muhammad Saif Ullah Khan",
    "Muhammad Ferjad Naeem",
    "Federico Tombari",
    "Luc Van Gool",
    "Didier Stricker",
    "Muhammad Zeshan Afzal"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present a novel LLM-based pipeline for creating contextual descriptions of human body poses in images using only auxiliary attributes. This approach facilitates the creation of the MPII Pose Descriptions dataset, which includes natural language annotations for 17,367 images containing people engaged in 410 distinct activities. We demonstrate the effectiveness of our pose descriptions in enabling zeroshot human-centric classification using CLIP. Moreover, we introduce the FocusCLIP framework, which incorporates Subject-Focused Attention (SFA) in CLIP for improved textto-image alignment. Our models were pretrained on the MPII Pose Descriptions dataset and their zero-shot performance was evaluated on five unseen datasets covering three tasks. FocusCLIP outperformed the baseline CLIP model, achieving an average accuracy increase of 8.61% (33.65% compared to CLIP's 25.04%). Notably, our approach yielded improvements of 3.98% in activity recognition, 14.78% in age classification, and 7.06% in emotion recognition. These results highlight the potential of integrating detailed pose descriptions and subject-level guidance into general pretraining frameworks for enhanced performance in downstream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Pretraining techniques that leverage multiple modalities have transformed deep learning, enabling models to capture intricate patterns from vast, unlabeled datasets  [16, 17, 31, 34] . This approach is pivotal for zero-shot capabilities, which allow models to recognize concepts not seen during training  [30, 55] . Within this evolving field, CLIP  [34]  emerged as a significant advancement in language-image pretraining. It demonstrates impressive performance on various visual tasks by contrastively aligning images and Our LLM pipeline creates grounded pose descriptions for images of people using only auxiliary attributes (activity labels and 2D keypoint coordinates) obtained from dataset annotations or extracted from the images using pretrained models.\n\ntext in a shared embedding space. CLIP-like methods  [12, 19, 23, 52, 55]  are effective but require a lot of data. The original CLIP model used 400 million image-text pairs because of its broad pretraining approach. Recent works  [27]  have attempted to narrow the pretraining objective by using known priors. Following this, we propose using specialized text and a mechanism to limit the learning space when we are only interested in a related set of tasks, such as human-centric classification. This allows for enhanced zero-shot performance on unseen datasets using fewer pretraining samples compared to naive contrastive alignment. We use the pipeline in Fig.  1  to curate the MPII Human Pose Descriptions dataset. Our domain-specific text data contains around 14k samples with detailed natural lan- guage descriptions of activities and body postures. Moreover, we draw inspiration from the human vision to further enhance the CLIP framework. The human gaze is characterized by rapid movements and fixations, where high-acuity vision is concentrated on limited spatial regions during fixation  [6, 15] . We create FocusCLIP by adding Subject-Focused Attention (SFA) to the vision side of the CLIP framework. This imitates the fixation stage of human vision, allowing the model to concentrate on task-relevant image regions. We demonstrate the effectiveness of our proposed pipeline comprising pose descriptions and SFA by performing a zero-shot evaluation of the pretrained Fo-cusCLIP on several unseen human-centric tasks. Our main contributions include:\n\n• Introduction of a single-shot, structured LLMprompting method for describing images in datasets by leveraging both class and image-level annotations.\n\n• Public release of the MPII Pose Descriptions dataset comprising high-quality textual descriptions of human body posture and activities.\n\n• Integrating subject-focused attention in generic contrastive pretraining via ROI heatmaps, establishing a novel paradigm for focused embedding learning.\n\n• Superior zero-shot performance on three humancentric tasks compared to CLIP, including singleimage activity recognition, age group classification, and emotion recognition.\n\nOur approach emphasizes task-related image features while retaining zero-shot capabilities, providing a promising direction for enhancing performance in applications requiring specialized knowledge with less pretraining data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Vision-Language Models (VLMs) Multimodal learning  [5, 12, 19, 23, 30, 31, 51, 52, [54] [55] [56]  has seen significant advancements, primarily focused on aligning and integrating textual and visual data. Despite the promising results of CLIP-like models  [12, 14, 23, 34, 50, 52, 55] , which map images and text into a shared embedding space, these models struggle with specialized tasks due to their reliance on generic pretraining data  [27] . Furthermore, their learning objective is global, with no mechanism for fine-grained alignment between image and text regions  [29, 30] . Several works have attempted to address this limitation by introducing image-to-text cross-attention  [30] , random token masking  [23] , attentive token masking  [52] , and masked selfdistillation  [12] . Our work, FocusCLIP, explores another approach for improving fine-grained alignment between image and text by introducing an ROI heatmap during pretraining, explicitly guiding the focus toward task-relevant areas. This novel approach adds a degree of supervision to the self-supervised CLIP pretraining and enhances the alignment between text and visual data.\n\nAttention using Heatmaps Many VLMs develop an overreliance on superficial language priors  [33, 37, 40] . Additionally, vision networks often focus on image areas that do not correlate with the regions humans look at when performing the same tasks  [10, 39] . To address this, HINT  [40]  used human-generated attention heatmaps to guide model focus. These heatmaps served as explicit hints, showing which parts of an image humans considered essential for the task. The technique effectively improved the model performance on several tasks by providing a more reliable basis for grounding the predictions. Similarly,  [36]  also uses heatmaps of human gaze to highlight image regions humans deemed meaningful for bird classification. Notably, this use of heatmaps is distinct from the attention mechanisms commonly found in Transformer models  [48] . Using explicit human guidance to tune the model's focus has been similarly explored in various Visual Question Answering studies  [26, 35] . Our work introduces a conceptually similar attention map through the heatmap input. Integrating it into the CLIP framework sets our work apart, broadening applicability to multiple zero-shot tasks. Furthermore, unlike  [40] , where heatmaps always need to be created manually, our proposed method does not require additional manual effort by human annotators in cases where keypoint annotations are already available.\n\nLarge Language Models (LLMs) as Annotators In  [43] , the authors explored the possibility of leveraging few-shot learning abilities of LLMs  [2, 4, 9, 32, 38, 46, 47]  to generate new text datasets. Concurrently, other works demonstrated their capability to automate laborious annotation tasks  [22, 45] . Recently,  [29]  introduced a few-shot prompting strategy for writing natural language descriptions for images of animals and birds using LLMs. While  [29]  did not provide a framework for validating the text quality, they successfully used LLM-text to improve model performance. Drawing on this, we develop a prompting strategy in Section 3 for effectively describing images.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Standing On A Rock, Holding A Sling",
      "text": "There is one person in the image who is standing on a rock, holding a sling. This activity is characterized by positioning on one's feet with one arm extended. The person in the center of the image is standing, which is evident from the location of the hips, knees, and ankles. The distribution of weight appears balanced, as both ankles are planted on the ground. The right leg of this person is slightly bent at the knee, with the ankle firmly on the ground, suggesting the leg might be bearing more weight. The left leg is also slightly bent at the knee, with the ankle grounded too, providing balance and support.\n\nThe torso seems upright with a small inclination to the right, possibly for stability. The neck and head are erect, suggesting the individual could be focused on some target. The right arm is bent at an almost 90-degree angle at the elbow, with the wrist closer to the waist, implying the person is holding something, perhaps the sling's pouch. The left arm is markedly bent at the elbow, with a raised wrist level, suggesting the person might be reaching out or preparing to launch something with the sling. The head is looking straight forward or possibly slightly downward, indicating focus on the task at hand.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Posing For A Painting, Smiling",
      "text": "There is one person in the image who is posing for a painting, smiling. The activity is characterized by a relaxed stance, with the person's body fairly upright and their expression visibly happy. The person is centrally positioned in the image. They are in a semi-seated pose, with their lower half obscured, implying they're sitting. The torso is slightly tilted to the right from center. Their right arm is bent at the elbow, extending down from the shoulder with the wrist located close to the hip near the pelvis. They may be supporting themselves in the seated position using their right hand. The left arm is outstretched away from the torso, forming a slight curve with a bent elbow. The wrist of their left hand is also at the same level as their right hand, which suggests it might be resting on a surface like an armrest.\n\nThere's no visible information about the legs, as the keypoints for the knees and ankles return the \"-1\" value, hence they may be out of frame or obscured.\n\nThe head is straight and upright with the neck-less curved. This, in conjunction with their activity, indicates that they might be looking directly towards the viewer, enhancing their smiling pose for the painting.\n\nFigure  3 . Sample Pose Description. We use our pipeline to generate pose descriptions for two famous artworks, the Statue of David and the Mona Lisa. The LLM was provided body keypoints obtained using an off-the-shelf pose estimation network and manual activity labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section describes our novel pose descriptions dataset and the proposed FocusCLIP framework, which integrates Subject-Focused Attention (SFA) into CLIP  [34] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pose Descriptions Dataset",
      "text": "We introduce the MPII Pose Descriptions Dataset to improve zero-shot human classification, a collection of natural language annotations describing human body poses and activities. This dataset is derived from the MPII Human Pose dataset  [1] , which provides detailed keypoint annotations for various human activities.\n\nUsing a novel LLM-driven pipeline (Fig.  1 ), we automatically generated contextual descriptions of human poses based on auxiliary attributes such as 2D keypoint coordinates and activity labels. These attributes can either be obtained from the dataset annotations or extracted from images using pretrained models. By feeding this structured data into LLMs, we produced natural language descriptions that link body postures to human actions, enabling a more intuitive understanding of the data. We provide further details about prompt design in Appendix A.\n\nThe dataset contains 17,367 images, with 14,644 training and 2,723 validation samples. Each image includes up to four unique pose descriptions generated by state-of-theart LLMs, providing diverse annotations for each sample. These descriptions (Fig.  3  and 4 ) are grounded in image content and capture subtle posture and context variations beyond traditional keypoint annotations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Applications And Impact",
      "text": "This dataset designed to bridge the gap between visual pose estimation and natural language understanding, making it valuable for various human-centric applications: Multimodal Learning: The dataset facilitates joint learning between visual and textual modalities, enabling models to better align human pose with contextual descriptions. Activity Recognition: By incorporating rich textual descriptions of human activities, the dataset enhances models' ability to recognize and classify actions from image and text inputs, improving zero-shot generalization. Fine-Grained Pose Analysis: The detailed descriptions help models distinguish subtle differences in body postures, supporting tasks like behavior analysis, sports activity classification, and human-object interaction recognition. Text-Enhanced Keypoint Estimation: These contextual descriptions offer additional information for improving the precision and interpretability of pose estimation models, especially in ambiguous scenarios.\n\nBeyond pose estimation, the dataset can also be used for zero-shot human classification tasks by providing a deeper understanding of human behavior from visual and linguistic perspectives. We demonstrate this with our FocusCLIP framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Focusclip Framework",
      "text": "our training dataset, containing N samples. In this dataset, I i represents the input image, T i denotes the corresponding textual description of the human pose, and H i is the associated ROI heatmap for each i-th sample. Let ϕ I : I → E I , ϕ H : H → E H , and ϕ T : T → E T be encoding functions GPT-3.5: There are two people in the image who are playing music, specifically drums, while sitting.\n\nGPT-3.5: They have their right arm bent at the elbow, holding a paintbrush near their head.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gpt-3.5:",
      "text": "There are 2 people in the image who are playing doubles tennis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gpt-3.5:",
      "text": "The torso appears to be slightly twisted to the right side while maintaining balance during climbing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gpt-3.5:",
      "text": "The right person is also standing with their left arm slightly bent and holding onto a fishing rod with their left hand.\n\nGPT-3.5: These limb positions suggest that this person might be preparing or executing a shot during their racquetball game.\n\nGPT-4: The left leg seems to be bearing most of their weight, as it's straightened and firmly planted on the ground.\n\nLLaMa-2: Their left arm is extended overhead, with their hand holding a tool or brush near the top of the wall.\n\nLLaMa-2: The person's left leg is extended behind them, with their foot pointing towards the ground.\n\nGPT-4: The person's right arm seems to be extended outwards and slightly downwards, possibly holding onto a tool such as a shovel handle.\n\nGPT-4: Lastly, their head seems to be tilted downwards possibly focusing on maintaining balance during this complex exercise routine.\n\nGPT-3.5: The second person is positioned towards the right side of the image, with a larger scale.\n\nGPT-4: Their right arm is extended downwards with elbow bent at an angle greater than 90 degrees, suggesting that it's possibly resting or pressing against something -likely the double bass.  that map the image, heatmap, and text, respectively, into an embedding space. The objective is to learn a function f : (E I , E H ) → E T that maps the visual embeddings E I and E H to the textual embedding E T . We minimize a dual contrastive loss function L SFA that quantifies the distance between matched and unmatched (E I , E H , E T ) triplets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Subject-Focused Attention (Sfa)",
      "text": "The SFA mechanism enhances FocusCLIP by incorporating an additional encoder (Fig.  5a ). This encoder leverages Region of Interest (ROI) heatmaps to highlight salient regions-such as human figures-within the input images. This mechanism aims to improve model attention to taskrelevant areas while maintaining alignment with corresponding textual descriptions.\n\nGiven an input image I, its ROI heatmap H is generated based on keypoint annotations (Fig.  5b ). The heatmap is applied through element-wise multiplication to form a masked image I H = I ⊙ H, selectively emphasizing important regions. The visual encoder ϕ I , the text encoder ϕ T , and the ROI encoder ϕ R transform the image I, the text T , and the masked image I H , respectively, into their corresponding embeddings E I , E T , and E H .\n\nThe SFA mechanism optimizes a dual contrastive loss that jointly aligns both the image and masked image embeddings with the text embeddings. The loss for the original image-text pair is defined as:\n\nwhere s(u, v) = u ⊤ v/(∥u∥∥v∥) is the cosine similar-ity, E V is the embedding of the i-th image or heatmaphighlighted image, E T is the corresponding text embedding, N is the batch size, τ is the temperature parameter, and\n\nSimilarly, the loss for the heatmap-highlighted imagetext pair is given by:\n\nThe total loss is the average of the two:\n\nBy enforcing this dual alignment, SFA ensures that both the global context from the original image and the focused regions from the masked image are jointly considered, leading to improved performance on human-centric tasks. This mechanism effectively guides attention toward individuals in complex scenes (Fig.  5c ).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Zero-Shot Inference",
      "text": "During zero-shot prediction, the goal is to apply the learned function f to predict the class label ĉ for a new image Î without the aid of the heatmap. This is achieved by defining a set of potential class labels C = {c 1 , c 2 , . . . , c K } and generating a corresponding set of texts T = {T 1 , T 2 , . . . , T K } through a task-specific sentence template populated with each class label. These texts are then encoded into embeddings by f , producing Z = {E T1 , E T2 , . . . , E T K }. Concurrently, the embedding of the new image E Î is obtained. The class label c associated with the text embedding E Tc that has the highest cosine similarity with E Î is predicted as the class for Î, formalized as:\n\nThe model performance is measured on an unseen test dataset, which examines the model's ability to generalize to classes not seen during training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We perform two sets of experiments to optimize our pose descriptions dataset and assess our FocusCLIP framework in zero-shot human classification tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quality Evaluation Of Pose Descriptions Dataset",
      "text": "We designed a human evaluation scheme to evaluate the \"correctness\" of generated text, with results shown in Fig.  6 . Each image was annotated with pose descriptions written by three different LLMs, including GPT-3.5, GPT-4, and LLaMA-2. Descriptions comprise a variable number of sentences. We evaluated the sentence-level quality of these descriptions using human feedback. Selecting 100 samples randomly, we split the corresponding 300 descriptions into sentences. Evaluators were shown image-sentence pairs and asked to rank correctness on a five-point scale by comparing the sentence with the corresponding image. The results of this evaluation are illustrated in Fig.  8 . We also take a closer look at one complete sample with assigned correctness scores by evaluators in Fig.  9 . The human evaluation further confirmed GPT-4's superiority in creating contextually relevant and interpretive descriptions, ranking highest among human evaluators. We also used these evaluations to tune our LLM prompt to obtain high-quality descriptions.\n\nWe also compared LLM-written descriptions with a baseline of human-written text-specifically scientific abstracts from arXiv. We used several metrics to assess the quality of the descriptions, including Flesch-Kincaid Grade Level for readability level, number of grammatical errors for text integrity, MTLD for lexical diversity, and a 3-gram repetition score for text uniqueness. Moreover, we used the CLIP Score metric with OpenAI's CLIP-ViT-L/14 for image-description correlation. Tab. 1 shows that GPT-4 generated descriptions offer greater readability, fewer grammatical errors, enhanced linguistic diversity, and lower repetition than other models. This suggests GPT-4's proficient use of an extensive vocabulary and nuanced understanding of depicted activities, surpassing even human-generated text in diversity (Fig.  7 ). GPT-4 provides valuable insights in addition to limb locations and orientations. For instance, when describing a person playing football with one leg straight and the other bent (Fig.  4 ), only GPT-4 reasoned that the person's weight must be more on the straight leg.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation On Human Classification Tasks",
      "text": "In this set of experiments, we evaluate the zero-shot classification abilities of FocusCLIP, which was trained on MPII Human Pose dataset  [1]  images and our pose descriptions. We apply the model to three human-centric tasks, using five previously unseen datasets. Our main baseline is vanilla CLIP training  [34]  from scratch on the same imagetext pairs as FocusCLIP for a fair comparison. We also report a lower bound for accuracy by simulating a random guess averaged over three runs. Implementation details. We initialize the visual encoder with ImageNet pre-trained weights and freeze it, in line with recommendations by  [44, 55] . Our models are trained using a contrastive loss, with a fixed temperature value of 0.5, consistent with the approach used in SimCLR  [8] . We utilize a Stochastic Gradient Descent (SGD) optimizer, with a learning rate of 0.001 and momentum of 0.9, and train all models for 64 epochs. This process takes approximately 12 hours on an A100-40GB GPU. For all our experiments, we  Their left arm is holding a rope to tie down the load while their right arm is holding a heavy box. Their torso is slightly twisted towards the truck bed. Their head is facing forward.\n\nGPT-4: There is one person in the image who is working, specifically truck driving, loading and unloading a truck, tying down load, standing, walking and carrying heavy loads. This activity involves a lot of movement and physical exertion. The centrally located person is actively engaged in work with their limbs positioned to carry out tasks. Their right leg seems to be bent at the knee as it's visible from the hip to the knee but the ankle isn't visible suggesting it might be obscured behind an object or positioned towards the back. The left leg appears straighter with both hip and knee visible but like the right leg, its ankle isn't visible either. For their arms; their right arm appears extended outward with a bend at elbow while holding something given that both wrist and elbow are higher than shoulder level. Their left arm also seems to be bent at elbow but unlike right arm, it's likely hanging downwards since its wrist is lower than shoulder level. Their torso leans slightly towards right indicating they might be trying to balance something heavy they're lifting or carrying on their right side. Lastly for head position; since upper neck and head top keypoints are close together compared to other parts of body suggests they're looking straight ahead or slightly downwards which could mean they're focused on task at hand.\n\nLLaMA-2: There is one person in the image who is occupation, truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy loads. The person is standing with their feet shoulder-width apart, with their left foot slightly in front of their right foot. Their left leg is bent at the knee, and their right leg is straight. Their left arm is extended overhead, and their right arm is bent and holding onto something. Their torso is straight, and their head is facing forward. The person's left arm is positioned above their right shoulder, and their right arm is positioned below their left shoulder. Their left leg is positioned in front of their right leg, and their right leg is positioned behind their left leg. The person is wearing a jacket and pants, and they have a serious expression on their face. They are in a standing pose, with their weight evenly distributed on both feet. They are holding a rope in their right hand, and their left hand is on their hip. The person's body language suggests that they are focused on their task and are exerting effort to tie down the load. Their facial expression indicates that they are concentrating and may be slightly straining from the weight of the load. Overall, the person in the image is engaged in a physically demanding activity that requires strength, endurance, and attention to detail. They are dressed appropriately for the task, and their body language conveys a sense of determination and focus.\n\nFigure  9 . Qualitative Comparison of LLM Output. GPT-4 descriptions are more descriptive compared to LLaMA-2. They also contain a higher percentage of sentences rated correct by human evaluators. GPT-4 uses a diverse vocabulary with few repetitions. In comparison, LLaMA-2 output exhibits sentences with repeating patterns. It is also more often incorrect (i.e., not correlated to the image), and the contents of its descriptions are sometimes trivial. We also show the output of GPT-3.5 with an unrefined prompt.\n\nuse ViT-B/16  [13]  for visual and ROI encoders and BERT-Base  [11]  for the textual encoder.\n\nTab. 2 presents our quantitative results. We report the top-k accuracy for image-based activity classification, age classification, and emotion recognition. For datasets with numerical age labels, we categorize the age into groups such as adult, teenager, kid, etc., to transform it into a classification task. FocusCLIP outperforms the baseline CLIP by 3.98% with 10.47% accuracy on the Stanford40  [53]  dataset for activity recognition. Similarly, significant improvements are observed across various age classification datasets, including Emotic  [20]  and LAGENDA-Body  [21] , which contain full body images, and LAGENDA-Face  [21]  and UTKFace  [57] , which contain cropped facial images. The improvements range from 4.24% to 26.85%. In the emotion recognition task, there is a similar trend with 3.34% improvement for Emotic  [20]  and 10.79% improvement for FER+  [3] . The improvements in zero-shot classification performance across diverse human-centric tasks illustrate FocusCLIP's enhanced ability to understand and interpret human-centric features more effectively than the CLIP baseline. Notably, the improvements in age classification across various datasets highlight its capacity to grasp implicit human attributes, and significant gains in emotion recognition tasks emphasize its proficiency in identifying emotional cues from both body language (Emotic) and facial expressions (FER+). Furthermore, FocusCLIP's performance in the activity recognition task underscores its capability to contextualize human actions within a scene, suggesting that the model can extract meaningful information from both the foreground and background. As we show in our ablation studies, heatmaps that additionally highlight important scene elements further improve performance for this task. This supports our claim that heatmaps are a versatile and effective way to guide model focus.\n\nThese results suggest that combining pose descriptions with heatmap-based guidance in FocusCLIP effectively directs the model's focus toward relevant features within an image, enabling it to make more accurate predictions. This highlights the potential of specialized training for models that not only perform well in narrow, task-specific benchmarks but also exhibit a broader understanding of humancentric concepts in a more generalized, zero-shot context.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Ablations Studies",
      "text": "The major components of FocusCLIP include the ROI encoder and dual contrastive loss. Furthermore, we multiply the heatmap with the original image to create a highlighted image before feeding it into the ROI encoder, and we share the weights between both visual encoders. We analyze the value of each of these four components in Table Tab. 5. By removing each component separately and observing the impact on performance, we determine that all components work together to boost performance. The ROI encoder with heatmap input and the multiplication operation to highlight image regions (MIX) played important roles and notably impacted performance (rows 1 and 4). However, the most significant decrease in performance was observed when the two visual encoders did not share weights (row 3). This was due to the increased complexity of the learning objective and the doubling of model param-eters, resulting in overfitting on our small training data and inferior zero-shot performance on unseen datasets. The additional loss function (row 2) also contributes to the overall performance of our network (row 5). Impact of the ROI encoder. The primary contribution of our work is the heatmap-based attention provided by the ROI encoder. The second contribution is the pose descriptions dataset. As shown in Table  Tab . 5, when the model is trained with our pose descriptions, it achieves a task-wise mean accuracy of 25.04% (row 1). This is 4.16% above the random baseline at 20.88%, demonstrating the impact of our text dataset. When the person heatmaps are provided as an auxiliary input, the task-wise mean improves to 33.65% (row 5), an additional 8.61%. This confirms our initial hypothesis that heatmaps can provide subject-focused supervision, allowing FocusCLIP to learn feature representations better suited for the intended downstream tasks. Ablations over Prompting Method: Fig.  10  compares the statistics of responses generated by two different prompts: the one we proposed, which includes persona and multiple contextual cues, and an unrefined prompt that lacks these features. Quantitative ablations in Tab. 4 reveal that structured prompts, which utilize function-like syntax, yield a higher average performance across various tasks than plain prompts. The results show that our proposed prompt leads to higher-quality responses regarding language and correlation with the images. We present detailed qualitative results in Appendix B, illustrating how adding activity labels and persons enhances the contextual richness of responses and complements the keypoint data. Similarly, our human evaluations validate the hypothesis that embedding image-specific attributes and other contextual data in the prompt results in more precise generated descriptions. This can be explained by multiple contextual cues and image attributes that we add to the prompt. It reduces the expectations from LLM to extrapolate new information by instead asking it to parse structured data, mitigating hallucinations. This emphasizes the significance of prompt engineering when using LLMs as annotators. Impact of heatmap quality. We compare three heatmap sources: Gaussian ellipses within object bounding boxes, self-attention maps from a DINO model  [5] , and zero-shot heatmaps generated from TCL  [7] . Bounding box-based heatmaps, while less annotation-dependent, lack the shape detail provided by keypoint-based annotations. DINOderived maps eliminate manual annotation but offer limited control over shape or image regions. TCL heatmaps, generated through textual description, provide flexibility but have low resolution and contrast. As shown in Tab. 6, including any heatmap consistently enhances performance over a CLIP-only baseline. Our original fine-grained, keypointbased heatmaps excel, particularly in age and emotion recognition, achieving the highest average performance of  33.65. DINO-based heatmaps demonstrate a unique advantage in activity recognition, scoring 13.38, as they effectively highlight multiple significant regions within the scene, not just the subject. This contributes to their overall average score of 29.61. TCL-based heatmaps also show competitive performance with an average score of 28.74. However, bounding-box-based heatmaps offer the least improvement, with an average score of 25.70, reiterating their limitations in capturing object intricacies. Notably, DINO and TCL-based heatmaps surpass the no-heatmap baseline without manual annotation. This demonstrates the ability of FocusCLIP to benefit from heatmaps.\n\nImpact of LLM choice on performance. We examine how different LLMs influence zero-shot classification performance using our single-shot prompting method, as summarized in Tab. 7. GPT-4-generated captions enable our FocusCLIP model to achieve the best performance on the MPII dataset, closely followed by GPT-3.5. Conversely, captions generated by LLaMA-2 lag, likely due to its fewer parameters hindering the effective parsing of our JSONformatted prompts. These observations are consistent with quality evaluations in Fig.  8 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduced the MPII Pose Descriptions dataset, a novel contribution that combines annotated human pose images with detailed, context-aware captions generated by LLMs. The dataset enriches human-centric tasks such as keypoint estimation and activity recognition by providing semantically rich descriptions of human poses. We also presented FocusCLIP, a modification of the CLIP framework, which incorporates Subject-Focused Attention to enhance the model's understanding of humans. Combining fine-grained pose descriptions and focused attention mechanisms offers a practical and scalable solution to improve performance across various human-centered visual tasks. Our work establishes a foundation for future research, especially in integrating more sophisticated attention-based fusion strategies between multiple visual inputs and exploring the use of the dataset for tasks beyond zero-shot classification, such as fine-grained pose analysis and text-enhanced keypoint estimation.\n\nLimitations While our dataset holds significant potential, there are important considerations regarding the automatic generation of pose descriptions. Although we ask LLMs to use gender-neutral language to reduce activity-related bias, the dataset may still inadvertently reflect stereotypes inherent in the LLMs. Furthermore, the correctness of the generated descriptions was validated on only a small subset of the data, which may lead to inaccuracies or inconsistencies. Future iterations can explore improved fusion techniques and bias mitigation strategies. Despite these challenges, the dataset and model contribute valuable resources to advancing human motion and activity understanding research.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Llm Prompt",
      "text": "The section describes the prompt structure, its rationale, and design process. The specific prompt used for data generation is as follows:\n\nYou are an expert human activity and pose analyzer with deep understanding of MPII Human Pose dataset, which has 16 keypoints in order: 0 -right ankle, 1 -right knee, 2 -right hip, 3 -left hip, 4 -left knee, 5 -left ankle, 6 -pelvis, 7 -thorax, 8 -upper neck, 9 -head top, 10 -right wrist, 11 -right elbow, 12 -right shoulder, 13 -left shoulder, 14 -left elbow, 15 -left wrist. Given a set of 2D keypoint coordinates from MPII dataset as (x,y) with -1 for invisible joints, you will precisely describe body poses in terms of relative limb locations. Your descriptions will follow this template: \"There are [num2word($count)] people in image who are [getVerb($activity) parse-Name($activity)]. [General attributes describing $activity in keypoints context.]\" For each person in image: \"The [parseLocation($center,$scale)] person is [predictStateFromContext()] with their [limb]...\" For each limb (left leg, right leg, left arm, right arm, torso, head): \"[Describe how these limbs are positioned relative to other limbs, bend angles, and other similar pose information.]\" Use concise, precise, and gender-neutral language.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.1. Design Decisions",
      "text": "We used insights from contemporary prompt engineering works  [25, 28, 49]  to design a prompt that can describe humans in images using an LLM. In particular, our generic template prompt relies on persona specification  [28]  and the ability of LLMs to parse structured data  [18, 24] .\n\nThe opening sentence establishes the LLM role:\n\nYou are an experienced {role}, with a deep understanding of {dataset name} dataset, which has {dataset description}. This is inspired from the persona patterns in  [41, 42, 49] . In our case, the role is \"human pose estimation expert\", dataset name is \"MPII Human Pose\", and the description includes an explanation of available keypoints and other annotations. Including this effectively morphs the LLM into a knowledgeable entity about the dataset. Inspired by the experiments in  [28] , we interface this persona specification with LLM system prompt to further enhance context-aware responsiveness.\n\nNext, we define the required content in the generated descriptions, including a captioning objective for additional context.\n\nGiven the following annotations of an image from the {dataset name} dataset, describe the {target} in the image in terms of {required content}, and any other discriminatory attributes necessary for {captioning objective}: This is followed by the raw image-specific attributes from the dataset annotations as key-value pairs. We use the activity label, people count, keypoint locations, visibility, center, and scale for each annotated person. This is our main difference from previous works  [28, 29]  who harness the few-shot learning capabilities of LLMs to generate data, providing a handful of examples and prompting LLMs to synthesize analogous data. In our approach, we instead leverage the well-documented ability of LLMs to parse structured data  [18, 24] .\n\nThe next section of our prompt defines the required response format. In our experiments, we ask the LLM to fill in a semi-standard template: Your response should {response format} This syntax effectively acts as semantic tagging, guiding the LLM to treat certain portions of the JSON string as specialized entities, facilitating a more effective and nuanced parsing of the annotations.\n\nLastly, we reinforce the persona by repeating it and informing the LLM about the intended usage of the generated data. We also include any response restrictions, which tell LLMs the kind of language to use.\n\nDraw on your professional expertise as a {role}, image-specific features mentioned in the annotation, general facts known about the {target}, and any other relevant knowledge that can be used to teach {task}. {response restrictions}.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "A.2. Qualitative Ablations For The Prompt",
      "text": "This section systematically evaluates the impact of different components of our prompt. As described in the paper, our prompt has several essential elements. We look at the response of the GPT-4 model with different prompt variations using the reference images in 1a and 2a. We manually labeled the 16 MPII keypoints for each individual in the reference images and also defined activity labels for each image, which can be found in the respective caption. There is one person in the image who is holding a torch aloft. The person is depicted in a classic torch-bearer pose, with their arm raised, suggesting a sense of victory or celebration.\n\nThe person occupying the central portion of the image is standing straight with their right arm held high. The right leg is slightly bent at the knee, with the ankle in front of the right hip, suggesting a slight forward movement. The left leg is straight and aligned with the left hip, providing stable support to the body. The right arm is positioned in an elevated and bent pose, with the wrist being slightly higher than the shoulder, indicative of holding a torch aloft. The left arm appears relaxed and slightly bent forward at the elbow, hanging naturally by the side. The alignment of the pelvis, thorax, and upper neck suggests an upright posture for the torso. The head is also upright, in line with the neck, indicating a forward gaze. This overall body posture implies a sense of determination and focus. There is one person in the image who is standing upright. The person occupies the majority of the center and lower half of the image. The centrally placed person is standing straight with their limbs showing clear, natural bends and placements. Their right leg features the knee slightly bent forward, positioning the right ankle slightly ahead of the right hip. Conversely, their left leg is straight, with the left ankle is directly beneath the hip. Their right arm is elevated and bent at the elbow, with the wrist slightly higher than the shoulder. Conversely, the left arm hangs down naturally, with a slight forward bend at the elbow that positions the wrist slightly in front of the shoulder. The person's torso is upright, as shown by the alignment of the pelvis, thorax, and upper neck. The head is also aligned with the neck, suggesting a forward gaze.\n\n(c) Prompt: Modified prompt containing only keypoint data (i.e., no activity label).\n\nFigure  1 . Impact of activity labels. We manually labeled the keypoints on the statue and defined an activity name (a). Using the GPT-4 model, we compare the pose descriptions generated from activity labels and keypoint data (b) with those generated only from keypoint data (c). The additional contextual information included in the LLM response when using the activity label is bolded. These details are absent when the activity label is omitted.  When we ask the LLM to act as an expert pose analyzer (b), it makes fewer mistakes, uses more engaging language, and provides higherquality insights about the pose. Compared to this, when directly asked to describe pose without specifying a role (c), the LLM focuses on insignificant details (i.e., legs, which are not important to the activity), writes monotonic sentences, makes more mistakes, and does not provide useful insights about the interaction.\n\nImpact of activity labels. In the paper, we said that supplementing the raw keypoint coordinates with activity labels enhances the context-richness of the generated response. In some cases, it also leads to better parsing of the keypoints. This can be observed by comparing the default description in 1b with the description in 1c. The second description was generated using our complete prompt, with only the activity label missing from the image-specific attributes we provide as key-value pairs. As seen in 1c, the model response lacks the additional context provided by activity labels, which are marked bold in 1b. This context can enable a better understanding of both the activity and body posture.\n\nAlso, it is interesting to note that the activity label can help the LLM infer what objects the person might be interacting with. For example, consider the second image in the third-row of Paper Fig.  4 . It shows a person painting a wall with their left arm raised high, similar to the Statue of Liberty. The activity label for this image is \"painting a wall,\" which produces a widely different (yet accurate) description: \"Their left arm is extended overhead, with their hand holding a tool or brush near the top of the wall.\" Impact of personas. Another principal component of our prompt (A) is the persona specification using a role label. In 2, we compare the LLM response with and without role labels specifying the persona. When we ask the LLM to act as an expert pose analyzer, it makes fewer mistakes, uses more engaging language, and provides higher-quality insights about the pose. Compared to this, when directly asked to describe pose without specifying a role, the LLM focuses on insignificant details (i.e., legs, which are not important to the activity), writes monotonic sentences, makes more mistakes, and does not provide useful insights about the interaction.",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "B. Classification Tasks And Model Details",
      "text": "Given a query image and a set of possible class labels, a task-specific sentence template is populated by the class labels and corresponding text embeddings are computed. The query image is passed through the visual encoder to get the image embeddings. Then, we compute a similarity score between the image embedding and each text embeddings and select the class label corresponding to the most similar text embedding as the predicted class.\n\nWe populated a task-specific template sentence with all candidate classes to generate candidate sentences. These sentences were then passed through the pretrained text encoder to obtain text embeddings. Similarly, we passed the query image through the pretrained image encoder to get its image embedding. We used a distance metric to select the text embedding most similar to the image embedding, and we chose the corresponding class label as the predicted class. Below, we provide sentence templates for each task: Action recognition aims to predict the action category from images of people performing actions. We use the sentence template: \"a photo of a person [activity verb-ing]\". Age classification classifies people into a discrete age category using the sentence template: \"a photo of a [age group] person\". Emotion recognition predicts the emotion category from images of cropped faces or people. For facial emotion, the sentence template \"a photo of a/an [emotion adjective] looking face\" is used. For body images, the sentence template \"a photo of a person who is feeling [emotion noun]\" is used.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B.1. Model Hyperparameters",
      "text": "We provide detailed hyperparameters used to train our models in 1.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our LLM pipeline creates grounded pose descriptions",
      "page": 1
    },
    {
      "caption": "Figure 1: to curate the MPII Hu-",
      "page": 1
    },
    {
      "caption": "Figure 2: FocusCLIP outperforms the baseline CLIP model on",
      "page": 2
    },
    {
      "caption": "Figure 3: Sample Pose Description. We use our pipeline to generate pose descriptions for two famous artworks, the Statue of David and",
      "page": 3
    },
    {
      "caption": "Figure 1: ), we auto-",
      "page": 3
    },
    {
      "caption": "Figure 3: and 4) are grounded in image",
      "page": 3
    },
    {
      "caption": "Figure 4: Sample sentences from generated pose descriptions reflect the LLM’s nuanced understanding of activities and contextual",
      "page": 4
    },
    {
      "caption": "Figure 5: The FocusCLIP framework. (a) We add SFA in CLIP, highlighting important areas for downstream tasks, and train the model",
      "page": 4
    },
    {
      "caption": "Figure 5: a). This encoder leverages",
      "page": 4
    },
    {
      "caption": "Figure 5: b). The heatmap is ap-",
      "page": 4
    },
    {
      "caption": "Figure 6: Each image was annotated with pose descriptions written",
      "page": 5
    },
    {
      "caption": "Figure 8: We also take a",
      "page": 5
    },
    {
      "caption": "Figure 9: The human evaluation",
      "page": 5
    },
    {
      "caption": "Figure 7: ). GPT-4 provides valuable insights in ad-",
      "page": 5
    },
    {
      "caption": "Figure 4: ), only GPT-4 reasoned that the",
      "page": 5
    },
    {
      "caption": "Figure 6: GPT-4’s descriptions cor-",
      "page": 6
    },
    {
      "caption": "Figure 7: GPT-4 surpassed human-written text in",
      "page": 6
    },
    {
      "caption": "Figure 8: Human evaluation of LLM outputs.",
      "page": 6
    },
    {
      "caption": "Figure 9: Qualitative Comparison of LLM Output. GPT-4 descriptions are more descriptive compared to LLaMA-2. They also contain",
      "page": 6
    },
    {
      "caption": "Figure 10: Using contextual cues and",
      "page": 8
    },
    {
      "caption": "Figure 8: 6. Conclusion",
      "page": 8
    },
    {
      "caption": "Figure 1: Impact of activity labels. We manually labeled the keypoints on the statue and defined an activity name (a). Using the GPT-4",
      "page": 13
    },
    {
      "caption": "Figure 2: Impact of personas. We manually defined an activity name and labeled keypoints for two people in a sample image (a) and",
      "page": 13
    },
    {
      "caption": "Figure 4: It shows a person painting",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Quality of pose descriptions using automated metrics. *Using shorter re- 60",
      "data": [
        {
          "ing,specificallytruckdrivin": "truck,tyingdownload,stan"
        },
        {
          "ing,specificallytruckdrivin": "heavyloads.Thisactivityinvo"
        },
        {
          "ing,specificallytruckdrivin": "physicalexertion.Thecentral"
        },
        {
          "ing,specificallytruckdrivin": "engagedinworkwiththeirli"
        },
        {
          "ing,specificallytruckdrivin": "tasks.Theirrightlegseemst"
        },
        {
          "ing,specificallytruckdrivin": "visiblefromthehiptothekne"
        },
        {
          "ing,specificallytruckdrivin": "suggestingitmightbeobscur"
        },
        {
          "ing,specificallytruckdrivin": "sitionedtowardstheback.Th"
        },
        {
          "ing,specificallytruckdrivin": "withbothhipandkneevisible"
        },
        {
          "ing,specificallytruckdrivin": "kleisn’tvisibleeither.Forthe"
        },
        {
          "ing,specificallytruckdrivin": "pearsextendedoutwardwith"
        },
        {
          "ing,specificallytruckdrivin": "ingsomethinggiventhatboth"
        },
        {
          "ing,specificallytruckdrivin": "thanshoulderlevel.Theirleft"
        },
        {
          "ing,specificallytruckdrivin": "elbowbutunlikerightarm,it’"
        },
        {
          "ing,specificallytruckdrivin": "sinceitswristislowerthan"
        },
        {
          "ing,specificallytruckdrivin": "leansslightlytowardsrightin"
        },
        {
          "ing,specificallytruckdrivin": "ingtobalancesomethingheav"
        },
        {
          "ing,specificallytruckdrivin": "ontheirrightside.Lastlyfor"
        },
        {
          "ing,specificallytruckdrivin": "neckandheadtopkeypointsa"
        },
        {
          "ing,specificallytruckdrivin": "tootherpartsofbodysugge"
        },
        {
          "ing,specificallytruckdrivin": "aheadorslightlydownwards"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Quality of pose descriptions using automated metrics. *Using shorter re- 60",
      "data": [
        {
          "tion,truckdriving,loadingand": "load,standing,walkingandcarr"
        },
        {
          "tion,truckdriving,loadingand": "isstandingwiththeirfeetshould"
        },
        {
          "tion,truckdriving,loadingand": "footslightlyinfrontoftheirrig"
        },
        {
          "tion,truckdriving,loadingand": "attheknee,andtheirrightleg"
        },
        {
          "tion,truckdriving,loadingand": "extended overhead, and their ri"
        },
        {
          "tion,truckdriving,loadingand": "ontosomething.Theirtorsoiss"
        },
        {
          "tion,truckdriving,loadingand": "ingforward.Theperson’slefta"
        },
        {
          "tion,truckdriving,loadingand": "rightshoulder,andtheirrightarm"
        },
        {
          "tion,truckdriving,loadingand": "shoulder.Theirleftlegispositio"
        },
        {
          "tion,truckdriving,loadingand": "andtheirrightlegispositioned"
        },
        {
          "tion,truckdriving,loadingand": "soniswearingajacketandpant"
        },
        {
          "tion,truckdriving,loadingand": "pressionontheirface.Theyare"
        },
        {
          "tion,truckdriving,loadingand": "weightevenlydistributedonboth"
        },
        {
          "tion,truckdriving,loadingand": "intheirrighthand,andtheirleft"
        },
        {
          "tion,truckdriving,loadingand": "son’sbodylanguagesuggeststha"
        },
        {
          "tion,truckdriving,loadingand": "andareexertingefforttotiedow"
        },
        {
          "tion,truckdriving,loadingand": "sionindicatesthattheyareconc"
        },
        {
          "tion,truckdriving,loadingand": "strainingfromtheweightofthe"
        },
        {
          "tion,truckdriving,loadingand": "theimageisengagedinaphysi"
        },
        {
          "tion,truckdriving,loadingand": "requiresstrength,endurance,an"
        },
        {
          "tion,truckdriving,loadingand": "dressedappropriatelyforthetask"
        },
        {
          "tion,truckdriving,loadingand": "veysasenseofdeterminationan"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Quality of pose descriptions using automated metrics. *Using shorter re- 60",
      "data": [
        {
          "whoisloadingandunlo": "downtheload,standing,"
        },
        {
          "whoisloadingandunlo": "ingaheavyload.Theper"
        },
        {
          "whoisloadingandunlo": "theirlegsslightlyapart"
        },
        {
          "whoisloadingandunlo": "tendedtowardsthetruck"
        },
        {
          "whoisloadingandunlo": "slightlybentwhiletheir"
        },
        {
          "whoisloadingandunlo": "Theirleftarmisholding"
        },
        {
          "whoisloadingandunlo": "theloadwhiletheirrig"
        },
        {
          "whoisloadingandunlo": "heavybox.Theirtorsois"
        },
        {
          "whoisloadingandunlo": "wardsthetruckbed.The"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: FocusCLIP improves CLIP’s top-k accuracy by 8.61% eters,resultinginoverfittingonoursmalltrainingdataand",
      "data": [
        {
          "Ours": "10.47"
        },
        {
          "Ours": "41.80"
        },
        {
          "Ours": "59.44"
        },
        {
          "Ours": "71.41"
        },
        {
          "Ours": "35.13"
        },
        {
          "Ours": "13.73"
        },
        {
          "Ours": "63.35"
        },
        {
          "Ours": "33.65"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Our structured prompt cre-",
      "data": [
        {
          "10.47": "51.94\n38.54"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Our structured prompt cre-",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Activity Age Emotion": "6.49 37.16 31.48\n3.47 49.28 39.68\n4.66 40.96 26.68\n8.06 46.32 21.52",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "Column_1": "✓",
          "Column_2": "✓",
          "Column_3": "✓",
          "Column_4": "✓",
          "Activity Age Emotion": "10.47",
          "Column_6": "51.94",
          "Column_7": "38.54",
          "Column_8": "33.65"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Our structured prompt cre-",
      "data": [
        {
          "10.47": "51.94"
        },
        {
          "10.47": "38.54"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Our structured prompt cre-",
      "data": [
        {
          "10.47": "51.94"
        },
        {
          "10.47": "38.54"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Therearetwopeopleintheimage"
        },
        {
          "Column_1": "Bothindividualsappeartobestan"
        },
        {
          "Column_1": "Theirarmsareextendedtowards"
        },
        {
          "Column_1": "ofholdinghands.Theirlegsandt"
        },
        {
          "Column_1": "sition,suggestingarelaxedstate."
        },
        {
          "Column_1": "Thepersonontheleftsideofthe"
        },
        {
          "Column_1": "theirbodyslightlytiltedtowardst"
        },
        {
          "Column_1": "isextendedtowardsthepersono"
        },
        {
          "Column_1": "theelbow,indicatingtheyarehol"
        },
        {
          "Column_1": "attheelbowwiththewristpositio"
        },
        {
          "Column_1": "straightwithaslightbendatthek"
        },
        {
          "Column_1": "Thepersonontherightsideofthe"
        },
        {
          "Column_1": "withasimilarbodytilttowardst"
        },
        {
          "Column_1": "towardsthepersonontheleft,m"
        },
        {
          "Column_1": "Therightarmisbentattheelbo"
        },
        {
          "Column_1": "thetorso.Thelegsarestraight"
        },
        {
          "Column_1": "mirroringtherelaxedstanceofth"
        },
        {
          "Column_1": "slightlyinclinedtowardseachoth"
        },
        {
          "Column_1": "orengagementinconversation.The"
        },
        {
          "Column_1": "comfortableandrelaxedinteraction"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Therearetwopeopleintheima": "doors.Theindividualsarestand"
        },
        {
          "Therearetwopeopleintheima": "theirhandsareconnected,indic"
        },
        {
          "Therearetwopeopleintheima": "The leftmost person is standin"
        },
        {
          "Therearetwopeopleintheima": "slightlytiltedtowardstheright"
        },
        {
          "Therearetwopeopleintheima": "theankle,knee,andhipaligned"
        },
        {
          "Therearetwopeopleintheima": "straight,butitisslightlyangled"
        },
        {
          "Therearetwopeopleintheima": "uprightandslightlyturnedtot"
        },
        {
          "Therearetwopeopleintheima": "lookingforward.Therightarm"
        },
        {
          "Therearetwopeopleintheima": "handisextendedtowardstheo"
        },
        {
          "Therearetwopeopleintheima": "slightlybentattheelbow,and"
        },
        {
          "Therearetwopeopleintheima": "wardstheotherperson,indicati"
        },
        {
          "Therearetwopeopleintheima": "Therightmostpersonisalsostan"
        },
        {
          "Therearetwopeopleintheima": "slightlytiltedtowardstheleft."
        },
        {
          "Therearetwopeopleintheima": "theankle,knee,andhipaligned"
        },
        {
          "Therearetwopeopleintheima": "straight,butitisslightlyangled"
        },
        {
          "Therearetwopeopleintheima": "uprightandslightlyturnedto"
        },
        {
          "Therearetwopeopleintheima": "lookingforward.Therightarm"
        },
        {
          "Therearetwopeopleintheima": "andthehandisextendedtowar"
        },
        {
          "Therearetwopeopleintheima": "armisbentattheelbow,and"
        },
        {
          "Therearetwopeopleintheima": "wardstheotherperson,indicatin"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "2d human pose estimation: New benchmark and state of the art analysis",
      "authors": [
        "Mykhaylo Andriluka",
        "Leonid Pishchulin",
        "Peter Gehler",
        "Bernt Schiele"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "Palm 2 technical report",
      "authors": [
        "Rohan Anil",
        "Andrew Dai",
        "Orhan Firat",
        "Melvin Johnson",
        "Dmitry Lepikhin",
        "Alexandre Passos",
        "Siamak Shakeri",
        "Emanuel Taropa",
        "Paige Bailey",
        "Zhifeng Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "arxiv": "arXiv:2305.10403"
    },
    {
      "citation_id": "3",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "4",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "5",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "Mathilde Caron",
        "Hugo Touvron",
        "Ishan Misra",
        "Hervé Jégou",
        "Julien Mairal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "6",
      "title": "Visual attention: The past 25 years",
      "authors": [
        "Marisa Carrasco"
      ],
      "year": "2011",
      "venue": "Vision research"
    },
    {
      "citation_id": "7",
      "title": "Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs",
      "authors": [
        "Junbum Cha",
        "Jonghwan Mun",
        "Byungseok Roh"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "9",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "arxiv": "arXiv:2204.02311"
    },
    {
      "citation_id": "10",
      "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions?",
      "authors": [
        "Abhishek Das",
        "Harsh Agrawal",
        "Larry Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2017",
      "venue": "Human attention in visual question answering: Do humans and deep networks look at the same regions?"
    },
    {
      "citation_id": "11",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "12",
      "title": "Maskclip: Masked selfdistillation advances contrastive language-image pretraining",
      "authors": [
        "Xiaoyi Dong",
        "Jianmin Bao",
        "Yinglin Zheng",
        "Ting Zhang",
        "Dongdong Chen",
        "Hao Yang",
        "Ming Zeng",
        "Weiming Zhang",
        "Lu Yuan",
        "Dong Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "14",
      "title": "Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Yuxin Fang",
        "Wen Wang",
        "Binhui Xie",
        "Quan Sun",
        "Ledell Wu",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao",
        "Eva"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Eye movements and their functions in everyday tasks",
      "authors": [
        "Tom Foulsham"
      ],
      "year": "2015",
      "venue": "Eye"
    },
    {
      "citation_id": "16",
      "title": "Vision-language pre-training:: Basics, recent advances, and future trends",
      "authors": [
        "Zhe Gan",
        "Linjie Li",
        "Chunyuan Li",
        "Lijuan Wang",
        "Zicheng Liu",
        "Jianfeng Gao"
      ],
      "year": "2022",
      "venue": "Foundations and Trends® in Computer Graphics and Vision"
    },
    {
      "citation_id": "17",
      "title": "Self-supervised pretraining of visual features in the wild",
      "authors": [
        "Priya Goyal",
        "Mathilde Caron",
        "Benjamin Lefaudeux",
        "Min Xu",
        "Pengchao Wang",
        "Vivek Pai",
        "Mannat Singh",
        "Vitaliy Liptchinsky",
        "Ishan Misra",
        "Armand Joulin"
      ],
      "year": "2021",
      "venue": "Self-supervised pretraining of visual features in the wild",
      "arxiv": "arXiv:2103.01988"
    },
    {
      "citation_id": "18",
      "title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
      "authors": [
        "Jiayan Guo",
        "Lun Du",
        "Hengyu Liu"
      ],
      "year": "2023",
      "venue": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
      "arxiv": "arXiv:2305.15066"
    },
    {
      "citation_id": "19",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham",
        "Quoc Le",
        "Yun-Hsuan Sung",
        "Zhen Li",
        "Tom Duerig"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Emotic: Emotions in context dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "21",
      "title": "Mivolo: Multiinput transformer for age and gender estimation",
      "authors": [
        "Maksim Kuprashevich",
        "Irina Tolstykh"
      ],
      "year": "2023",
      "venue": "Mivolo: Multiinput transformer for age and gender estimation"
    },
    {
      "citation_id": "22",
      "title": "Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification",
      "authors": [
        "Taja Kuzman",
        "Igor Mozetic",
        "Nikola Ljubešic"
      ],
      "year": "2023",
      "venue": "Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification"
    },
    {
      "citation_id": "23",
      "title": "Scaling language-image pre-training via masking",
      "authors": [
        "Yanghao Li",
        "Haoqi Fan",
        "Ronghang Hu",
        "Christoph Feichtenhofer",
        "Kaiming He"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Lost in the middle: How language models use long contexts",
      "authors": [
        "Kevin Nelson F Liu",
        "John Lin",
        "Ashwin Hewitt",
        "Michele Paranjape",
        "Fabio Bevilacqua",
        "Percy Petroni",
        "Liang"
      ],
      "year": "2023",
      "venue": "Lost in the middle: How language models use long contexts",
      "arxiv": "arXiv:2307.03172"
    },
    {
      "citation_id": "25",
      "title": "Jailbreaking chatgpt via prompt engineering: An empirical study",
      "authors": [
        "Yi Liu",
        "Gelei Deng",
        "Zhengzi Xu",
        "Yuekang Li",
        "Yaowen Zheng",
        "Ying Zhang",
        "Lida Zhao",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "year": "2023",
      "venue": "Jailbreaking chatgpt via prompt engineering: An empirical study",
      "arxiv": "arXiv:2305.13860"
    },
    {
      "citation_id": "26",
      "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "authors": [
        "Jiasen Lu",
        "Caiming Xiong",
        "Devi Parikh",
        "Richard Socher"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Prior knowledge-guided attention in self-supervised vision transformers",
      "authors": [
        "Kevin Miao",
        "Akash Gokul",
        "Raghav Singh",
        "Suzanne Petryk",
        "Joseph Gonzalez",
        "Kurt Keutzer",
        "Trevor Darrell"
      ],
      "year": "2022",
      "venue": "Prior knowledge-guided attention in self-supervised vision transformers",
      "arxiv": "arXiv:2209.03745"
    },
    {
      "citation_id": "28",
      "title": "Is a prompt and a few samples all you need? using gpt-4 for data augmentation in low-resource classification tasks",
      "authors": [
        "Giovanni Anders",
        "Jacob Møller",
        "Arianna Aarup Dalsgaard",
        "Luca Maria Pera",
        "Aiello"
      ],
      "year": "2023",
      "venue": "Is a prompt and a few samples all you need? using gpt-4 for data augmentation in low-resource classification tasks",
      "arxiv": "arXiv:2304.13861"
    },
    {
      "citation_id": "29",
      "title": "I2mvformer: Large language model generated multi-view document supervision for zero-shot image classification",
      "authors": [
        "Muhammad Ferjad",
        "Muhammad Gul Zain",
        "Ali Khan",
        "Yongqin Xian",
        "Muhammad Zeshan Afzal",
        "Didier Stricker",
        "Luc Van Gool",
        "Federico Tombari"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "I2dformer: Learning image to document attention for zero-shot image classification",
      "authors": [
        "Muhammad Ferjad Naeem",
        "Yongqin Xian",
        "Luc Gool",
        "Federico Tombari"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Silc: Improving vision language pretraining with self-distillation",
      "authors": [
        "Muhammad Ferjad Naeem",
        "Yongqin Xian",
        "Xiaohua Zhai",
        "Lukas Hoyer",
        "Luc Van Gool",
        "Federico Tombari"
      ],
      "year": "2023",
      "venue": "Silc: Improving vision language pretraining with self-distillation",
      "arxiv": "arXiv:2310.13355"
    },
    {
      "citation_id": "32",
      "title": "OpenAI. Gpt-4 technical report",
      "year": "2023",
      "venue": "OpenAI. Gpt-4 technical report"
    },
    {
      "citation_id": "33",
      "title": "An empirical study on the language modal in visual question answering",
      "authors": [
        "Daowan Peng",
        "Wei Wei",
        "Xian-Ling Mao",
        "Yuanyuan Fu",
        "Dangyang Chen"
      ],
      "year": "2023",
      "venue": "An empirical study on the language modal in visual question answering",
      "arxiv": "arXiv:2305.10143"
    },
    {
      "citation_id": "34",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Overcoming language priors in visual question answering with adversarial regularization",
      "authors": [
        "Aishwarya Sainandan Ramakrishnan",
        "Stefan Agrawal",
        "Lee"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Human attention in fine-grained classification",
      "authors": [
        "Wenjia Yao Rong",
        "Zeynep Xu",
        "Enkelejda Akata",
        "Kasneci"
      ],
      "year": "2021",
      "venue": "British Machine Vision Conference (BMVC 2021)"
    },
    {
      "citation_id": "37",
      "title": "Are vision-language transformers learning multimodal representations? a probing perspective",
      "authors": [
        "Emmanuelle Salin",
        "Badreddine Farah",
        "Stéphane Ayache",
        "Benoit Favre"
      ],
      "year": "2022",
      "venue": "Pro-ceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "A 176b-parameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "Sasha Luccioni",
        "Matthias Franc ¸ois Yvon",
        "Gallé"
      ],
      "year": "2022",
      "venue": "A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "39",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "40",
      "title": "Taking a hint: Leveraging explanations to make vision and language models more grounded",
      "authors": [
        "Stefan Ramprasaath R Selvaraju",
        "Yilin Lee",
        "Hongxia Shen",
        "Shalini Jin",
        "Larry Ghosh",
        "Dhruv Heck",
        "Devi Batra",
        "Parikh"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "41",
      "title": "How does prompt engineering affect chatgpt performance on unsupervised entity resolution",
      "year": "2023",
      "venue": "How does prompt engineering affect chatgpt performance on unsupervised entity resolution",
      "arxiv": "arXiv:2310.06174"
    },
    {
      "citation_id": "42",
      "title": "An empirical evaluation of prompting strategies for large language models in zero-shot clinical natural language processing",
      "authors": [
        "Sonish Sivarajkumar",
        "Mark Kelley",
        "Alyssa Samolyk-Mazzanti",
        "Shyam Visweswaran",
        "Yanshan Wang"
      ],
      "year": "2023",
      "venue": "An empirical evaluation of prompting strategies for large language models in zero-shot clinical natural language processing",
      "arxiv": "arXiv:2309.08008"
    },
    {
      "citation_id": "43",
      "title": "Selective annotation makes language models better few-shot learners",
      "authors": [
        "Hongjin Su",
        "Jungo Kasai",
        "Chen Wu",
        "Weijia Shi",
        "Tianlu Wang",
        "Jiayi Xin",
        "Rui Zhang",
        "Mari Ostendorf",
        "Luke Zettlemoyer",
        "Noah Smith"
      ],
      "year": "2022",
      "venue": "Selective annotation makes language models better few-shot learners",
      "arxiv": "arXiv:2209.01975"
    },
    {
      "citation_id": "44",
      "title": "Eva-clip: Improved training techniques for clip at scale",
      "authors": [
        "Quan Sun",
        "Yuxin Fang",
        "Ledell Wu",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2023",
      "venue": "Eva-clip: Improved training techniques for clip at scale",
      "arxiv": "arXiv:2303.15389"
    },
    {
      "citation_id": "45",
      "title": "Leveraging large language models and weak supervision for social media data annotation: an evaluation using covid-19 self-reported vaccination tweets",
      "authors": [
        "Ramya Tekumalla",
        "Juan Banda"
      ],
      "year": "2023",
      "venue": "Leveraging large language models and weak supervision for social media data annotation: an evaluation using covid-19 self-reported vaccination tweets",
      "arxiv": "arXiv:2309.06503"
    },
    {
      "citation_id": "46",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "47",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "48",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "49",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "Jules White",
        "Quchen Fu",
        "Sam Hays",
        "Michael Sandborn",
        "Carlos Olea",
        "Henry Gilbert",
        "Ashraf Elnashar",
        "Jesse Spencer-Smith",
        "Douglas Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "arxiv": "arXiv:2302.11382"
    },
    {
      "citation_id": "50",
      "title": "Demystifying clip data",
      "authors": [
        "Hu Xu",
        "Saining Xie",
        "Ellen Xiaoqing",
        "Po-Yao Tan",
        "Russell Huang",
        "Vasu Howes",
        "Shang-Wen Sharma",
        "Gargi Li",
        "Luke Ghosh",
        "Christoph Zettlemoyer",
        "Feichtenhofer"
      ],
      "year": "2023",
      "venue": "Demystifying clip data",
      "arxiv": "arXiv:2309.16671"
    },
    {
      "citation_id": "51",
      "title": "Vision-language pre-training with triple contrastive learning",
      "authors": [
        "Jinyu Yang",
        "Jiali Duan",
        "Son Tran",
        "Yi Xu",
        "Sampath Chanda",
        "Liqun Chen",
        "Belinda Zeng",
        "Trishul Chilimbi",
        "Junzhou Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Attentive mask clip",
      "authors": [
        "Yifan Yang",
        "Weiquan Huang",
        "Yixuan Wei",
        "Houwen Peng",
        "Xinyang Jiang",
        "Huiqiang Jiang",
        "Fangyun Wei",
        "Yin Wang",
        "Han Hu",
        "Lili Qiu"
      ],
      "year": "2022",
      "venue": "Attentive mask clip",
      "arxiv": "arXiv:2212.08653"
    },
    {
      "citation_id": "53",
      "title": "Human action recognition by learning bases of action attributes and parts",
      "authors": [
        "Bangpeng Yao",
        "Xiaoye Jiang",
        "Aditya Khosla",
        "Andy Lai Lin",
        "Leonidas Guibas",
        "Li Fei-Fei"
      ],
      "year": "2011",
      "venue": "ICCV"
    },
    {
      "citation_id": "54",
      "title": "Sigmoid loss for language image pre-training",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2023",
      "venue": "Sigmoid loss for language image pre-training",
      "arxiv": "arXiv:2303.15343"
    },
    {
      "citation_id": "55",
      "title": "Lit: Zero-shot transfer with locked-image text tuning",
      "authors": [
        "Xiaohua Zhai",
        "Xiao Wang",
        "Basil Mustafa",
        "Andreas Steiner",
        "Daniel Keysers",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2005",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Revisiting visual representations in vision-language models",
      "authors": [
        "Pengchuan Zhang",
        "Xiujun Li",
        "Xiaowei Hu",
        "Jianwei Yang",
        "Lei Zhang",
        "Lijuan Wang",
        "Yejin Choi",
        "Jianfeng Gao",
        "Vinvl"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "57",
      "title": "Age progression/regression by conditional adversarial autoencoder",
      "authors": [
        "Zhifei Zhang",
        "Yang Song",
        "Hairong Qi"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}