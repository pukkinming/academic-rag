{
  "paper_id": "2407.12274v2",
  "title": "Mdpe: A Multimodal Deception Dataset With Personality And Emotional Characteristics",
  "published": "2024-07-17T02:44:26Z",
  "authors": [
    "Cong Cai",
    "Shan Liang",
    "Xuefei Liu",
    "Kang Zhu",
    "Zhengqi Wen",
    "Jianhua Tao",
    "Heng Xie",
    "Jizhou Cui",
    "Yiming Ma",
    "Zhenhua Cheng",
    "Hanzhe Xu",
    "Ruibo Fu",
    "Bin Liu",
    "Yongwei Li"
  ],
  "keywords": [
    "multimodal dataset",
    "affective computing",
    "deception detection",
    "personality",
    "emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deception detection has garnered increasing attention in recent years due to the significant growth of digital media and heightened ethical and security concerns. It has been extensively studied using multimodal methods, including video, audio, and text. In addition, individual differences in deception production and detection are believed to play a crucial role. Although some studies have utilized individual information such as personality traits to enhance the performance of deception detection, current systems remain limited, partly due to a lack of sufficient datasets for evaluating performance. To address this issue, we introduce a multimodal deception dataset MDPE 1 . Besides deception features, this dataset also includes individual differences information in personality and emotional expression characteristics. It can explore the impact of individual differences on deception behavior. It comprises over 104",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Generally, deception refers to the act of misleading, tricking, or deceiving others  [9] . It involves hiding the truth or presenting false information to create an impression that is not accurate. Deception can take many forms, including both verbal and nonverbal information  [5] . And it also occurs in various contexts, such as interpersonal relationships, business, politics, and entertainment. Deception is often considered unethical and can have serious consequences for trust and relationships.\n\nAs deception has expanded to other fields such as social media, interviews, online transactions, the need arises for a reliable and efficient system to aid the task of detecting deceptive behavior. Many machine learning approaches have been proposed in order to improve the reliability of deception detection systems  [23] . In particular, physiological, psychological, visual, linguistic, acoustic, and thermal modalities have been analyzed in order to detect discriminative features and clues to identify deceptive behavior  [16, 26, 39, 48] . Video-based deception detection is a current priority in deception research, because behavioral cues can be extracted from videos in a cheaper, faster, and non-invasive manner  [6] , which is preferable to invasive approaches that extract clues through devices attached to human bodies (e.g., polygraphs). Visual clues of deception include facial emotions, expression intensity, hands and body movements, and microexpressions. These features were shown to be capable of discriminating between deceptive and truthful behavior  [15, 40] . Recently, multimodal analysis has gained a lot of attention due to their superior performance compared to the use of unimodal modalities. In the deception detection field, several multimodal approaches  [31, 37, 42, 53]  have been suggested to improve deception detection by integrating features from different modalities. This integration created a more reliable system that is not susceptible to factors affecting sole modalities and polygraph tests.\n\nSubstantial empirical evidence indicates significant individual differences in both deception production and detection capabilities  [32, 35, 50] . These differences encompass cognitive processing, personality traits, psychological characteristics, and emotional expressivity. Empirical studies confirm that personality factors and emotional cues critically influence subjects' capacity to deceive and detect deception  [19, 32] . Emotion-a fundamental dimension of human communication-interacts with cognition to guide social behavior across both interpersonal and human-computer interactions  [20, 36] . This relationship is particularly relevant to deception, as deceptive acts can elicit distinctive emotional states that manifest as behavioral clues  [15, 58] . However, leveraging emotional features to improve deception detection accuracy remains challenging  [25] . A primary complicating factor is that emotional expression itself constitutes a core component of deception, making it difficult to discern whether a deceiver's displayed emotions are genuine or strategically fabricated.\n\nTo address this issue, we propose a multimodal deception dataset MDPE. It not only collects subjects' deception information, but also personality information and emotional information. Each subject was required to conduct another emotional experiment in addition to engaging in deception, in order to obtain their true emotional expression. Although our research was conducted in the laboratory to provide clear and comparable conversations, we provided subjects with effective monetary incentives to detect and generate effective deceptive behavior  [32] . To our knowledge, this is the largest multimodal deception dataset in the released dataset and the only deception detection dataset with personality and emotional characteristics.\n\nTo sum up, our contributions are threefold:\n\n‚Ä¢ We propose a novel multimodal deception dataset MDPE with personality and emotional characteristics, composed of facial video, and audio recordings and transcript. And an easily replicable experimental protocol has also been provided to researchers. ‚Ä¢ We provide a benchmark for deception detection from multimodal signals, and discussed the impact of personality traits and emotional cues on deception detection. ‚Ä¢ We offer new possibilities to facilitate further affective computing research, encourage the development of new methods that utilize individual differences for deception detection, as well as for tasks such as personality recognition and emotion recognition.   [4, 17, 38] , dilation of pupils  [11, 34] , and facial muscle movements  [28, 46]  have been found to distinguish between deceptive and truthful behavior. Vocal cues can be indicative of deception, with deceptive speakers tending to speak with higher and more varied pitch  [9, 64] , shorter utterances, and less fluency  [51, 56]  than truthful speakers. Deception also correlates with verbal attributes of speech, with deceivers tending to communicate with less cognitive complexity, fewer self-references, and more words indicative of negative emotions  [39, 62] . Mohamed et al.  [1]  explored a multimodal deception detection approach and integrates multiple physiological, linguistic, and thermal features. They used a decision tree model, to gain insights into the features that are most effective in detecting deceit. Leena Mathur et al.  [37]  analyzed the discriminative power of features from visual, vocal, and verbal modalities affect for deception detection. They experimented with unimodal Support Vector Machines (SVM) and SVM-based multimodal fusion methods to identify effective features for detecting deception.\n\nIndividual Difference Deception Some studies confirm that some of the five NEO-FFI (Neuroticism-Extraversion-Openness Five-Factor Inventory) dimensions are related to deception  [29, 49] . Sarah Ita Levitan et al.  [32]  reported the role of personality factors derived from the NEO-FFI and of gender, ethnicity and confidence ratings on subjects' ability to deceive and to detect deception. Justyna Sarzy≈Ñska et al.  [52]  reports correlations between the ability to lie and extraversion, as well as conscientiousness. Personality characteristics are a promising set of information for deception detection, and similarly, emotional characteristics are also important. Joseph P. Gaspar et al.  [18]  integrate prior theory and research on emotions, emotional intelligence, and deception and introduce a theoretical model. This model explores the interplay between emotional intelligence (the ability to perceive emotions, use emotions, understand emotions, and regulate emotions; and deception. Mircea Zloteanu et al hold strong beliefs about the role of emotional cues in detecting deception, and explored how decoders' emotion recognition ability and senders' emotions influence veracity judgements  [63] . Joseph P. Gaspar et al.  [19]  believe that emotions are both an antecedent and a consequence of deception, and they introduce the emotion deception model to represent these relationships. This model broadens their understanding of deception in negotiations and accounts for the important role of emotions in the deception decision process. To our knowledge, MDPE is the only deception detection dataset with personality and emotional characteristics.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Dataset 3.1 Experimental Setup",
      "text": "Equipment: The dataset was collected using a GoPro Hero9 sports camera configured to record video at a resolution of 1920√ó1080 pixels and a frame rate of 60 frames per second (fps). Audio data were synchronously captured via the camera's built-in microphone. During the emotional experiment, subjects were provided with a ThinkPad laptop to watch emotion-induction stimuli videos. Data collection took place in a controlled professional recording studio to minimize environmental interference. Only the participant and the interviewer remained in the room during the recordings.\n\nEmotion-Induction Videos: During the emotional induction experiment, subjects were shown a series of emotion-inducing videos designed to evoke specific emotional states, including sadness, happiness, relaxation, surprise, fear, disgust, anger, and neutrality. A total of 39 videos were utilized, with 17 collected from the Chinese Emotional Video System (CEVS)  [41] . Each video segment in the CEVS has been professionally labeled and evaluated to ensure its effectiveness in eliciting the corresponding emotional responses. However, the CEVS only includes six emotions: sadness, happiness, fear, disgust, anger, and neutrality, excluding relaxation and surprise. Additionally, some of the CEVS videos failed to reliably evoke the intended emotional responses during our pre-experiment. This may be attributed to shifts in aesthetic preferences over time, resulting in reduced emotional resonance among contemporary viewers. To address this problem, an additional 22 videos were collected from online sources. These videos were annotated by 12 independent data annotators according to the same criteria and annotation methods as those used in the CEVS. The results showed that each video successfully elicited strong emotional responses.\n\nDeception Questions: During the deception experiment, participants were asked a set of 24 \"deception questions\". These questions were developed by a panel of five psychology researchers, each with over five years of experience, drawing upon theoretical frameworks such as the Fraud Triangle Theory and Rational Choice Theory. To ensure comprehensiveness, the questions were designed to integrate interdisciplinary perspectives from psychology, criminology, and sociology, thereby capturing diverse dimensions of deceptive behavior. Some questions were specifically designed to reflect emerging trends in deceptive practices, addressing aspects potentially overlooked by conventional methodologies. The initial question set underwent a preliminary round of testing, during which participant feedback was collected and incorporated into subsequent revisions. The finalized version of the deception questions is provided in Appendix C.\n\nPersonnel: The study involved two distinct personnel roles: the interviewer and the Data Collection Coordinator (DCC). Interviewers were researchers with at least three years of experience in psychology and had received specialized training in deception detection techniques. Their responsibilities included conducting interviews, making real-time judgments regarding participants' truthfulness, and completing the Interviewer Judgment Scale. The data collection coordinator will assist with the execution of the study, including the preparation of materials and other related tasks.\n\nOther Material: The Interviewer Judgment Scale assessed the interviewer's perception of each subject's response credibility, rated on a 5-point Likert scale (1 = definitely true, 5 = definitely false). After the interview, subjects completed the Confidence Scale for Lying to self-assess their deceptive performance, also using a 5-point Likert scale (1 = I definitely deceived successfully, 5 = I definitely did not deceive successfully). During the emotional experiment, subjects rated their experience of eight specific emotions on an Emotional Scale, using a 5-point scale (1 = no such emotion, 5 = strongest emotion). Details can be found in Appendix B.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Procedure",
      "text": "Each subject was required to complete the following three experiments: personality, emotion and deception experiment.\n\nPersonality Characteristics Collection: Subjects were required to complete a Big Five personality questionnaire  [60] , which consists of 60 items. Each item was rated on a Likert scale from 1 to 5, with 1 indicating strong disagreement and 5 indicating strong agreement, reflecting the degree to which each statement matched the subject's own characteristics. Based on the scoring methodology, the Big Five personality traits-openness, conscientiousness, extraversion, agreeableness, and neuroticism-were assessed for each subject. The full questionnaire and scoring methodology are provided in Appendix A. Emotional Experiment: The DCC randomly selected 16 induction videos (ensure 2 videos for each emotion) for the subjects to watch. After watching each video, subjects were required to describe their feelings and then fill out an Emotional Scale.\n\nDeception Experiment: The deception data collection process follows DDPM  [55] . The DCC randomly selected 9 questions that must lie and hand them over to the subject (the interviewer does not know which 9 questions). The first 3 questions will not be selected, which means that the first three \"warm up\" questions were always to be answered honestly. They allowed the subject to get settled, and gave the interviewer an idea of the subject's demeanor when answering a question honestly. The subject have a maximum of 15 minutes to prepare, and during the preparation process, they must remember these 9 questions and think about how to deceive in the upcoming interview process. During the interview process, when asked these 9 questions, the subject must lie, and when asked the remaining 15 questions, they must tell the truth. Subjects were motivated to deceive successfully through two levels of bonus compensation: if they were able to deceive the interviewer in five or six of the nine deceptive responses, they were given a 150% of a base incentive payment; the base payment was doubled if they were successfully deceptive in seven or more questions. In order to collect more indistinguishable deception answers, we encourage subjects to incorporate some truth into lies when answering these deceptive questions.\n\nDuring the interview process, the interviewer asked 24 questions in random order, and provide their judgment of truthful or deceptive answers to each question. And the interviewer filled out the Interviewer Judgment Scale. And after the interview, the subject also filled out the Subject Lie Confidence Scale.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Statistic And Analysis",
      "text": "A total of 193 subjects took part in this study, comprising 130 females and 63 males, all of whom were native Chinese speakers. Their ages ranged from 18 to 69 years, and their occupations included students, laborers, teachers, retirees, and others. Each subject contributed responses to 24 deception questions, yielding a total of 1737 deceptive and 2895 truthful responses. In addition, each participant provided 16 emotion-inducing videos, resulting in a total of 3088 emotional video recordings. Following data collection, the raw video recordings were segmented. The duration of individual deceptive video clips ranged from 4 to 27 minutes, while emotional videos ranged from 19 to 38 minutes, which included the time spent watching emotion-induction materials. In total, 1808 minutes of deceptive video and 4401 minutes of emotional video were obtained, amounting to 6209 minutes of video content.\n\nA preliminary analysis of the data reveals the distribution of successful deception attempts across all subjects (Figure  1 (a) ). The majority of participants had success rates in the 3-6 times range. Figure  1  (b) displays the average number of successful deception by personality category. Subjects were categorized into high and low groups for each Big Five personality trait based on mean scores. Analysis indicates no significant difference in deception success rates between high/low Neuroticism or Openness. However, significantly higher deception success rates were observed among individuals with low Extraversion, high Agreeableness, and low Conscientiousness. The finding regarding low Extraversion may be attributed to a tendency towards greater caution and reduced likelihood of revealing vulnerabilities through exaggeration. Individuals scoring high in Agreeableness, characterized by cooperativeness and trustfulness, may more easily gain trust and goodwill, potentially lowering interviewer vigilance. Similarly, those with low Conscientiousness typically place less emphasis on rules, obligations, and social norms. These patterns align with established perspectives from disciplines such as self-control theory and general crime theory  [8, 10, 21] . Figure  1 (c ) presents the average intensity of each emotion expressed during the emotion experiment, comparing subjects with high versus low deception success rates. Individuals exhibiting higher deception success rates demonstrated heightened emotional expressivity. This may reflect a greater capacity among successful deceivers to understand and simulate others' emotions, resulting in a more sensitive and reactive emotional system. This observation is consistent with existing theories linking emotion and cognition  [2, 22] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Ethics Review And License",
      "text": "Before the experiment began, the subjects were informed of all experimental procedures. The subjects explicitly consented to record their conversation and publish the video data in a scientific conference or journal. And we do not publish any privacy-sensitive data, and the anonymity of participants will be guaranteed. All data were collected under a protocol approved by the authors'institution's Human Subjects Institutional Review Board.\n\nAdditionally, we restrict the use of this dataset under the license of CC BY-NC-SA-4.0, requiring researchers to use our dataset responsibly. And commercial usage is prohibited.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Benchmark 4.1 Data Preprocessing",
      "text": "For the visual modality, raw videos are standardized to 30 fps. Faces are cropped and aligned using the DLib Toolkit  [1] . Frame-level features are extracted using visual encoders and compressed to video-level via average pooling. For audio, the track is separated using FFmpeg, standardized to 16kHz mono, and acoustic features extracted. For text, transcripts are generated using the Paraformer ASR toolkit  [2] . For each sample ùë• ùëñ , this yields acoustic features ùëì ùëé ùëñ ‚àà R ùëë ùëé , textual features ùëì ùëô ùëñ ‚àà R ùëë ùëô , and visual features ùëì ùë£ ùëñ ‚àà R ùëë ùë£ , where {ùëë ùëö } ùëö‚àà {ùëé,ùëô,ùë£ } is the feature dimension for each modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extraction",
      "text": "Feature selection significantly impacts model performance. To guide feature selection, we evaluate distinct feature types under consistent experimental conditions. For visual modality, compared with handcrafted features, deep features extracted from supervised models are useful for facial expression recognition  [33] . CLIP  [47]  is a multimodal model based on contrastive learning, where training utilizes text and images to construct positive and negative sample pairs. The Vision Transformer (VIT)  [13]  is a transformer encoder model, pre-trained in a supervised manner on a large dataset of images. For acoustic modality, the Wav2vec2  [3]  masks speech inputs in the latent space and addresses a contrastive task defined on quantized latent representations. It has been widely applied to downstream speech tasks. HUBERT  [27]  utilizes offline clustering steps to provide aligned target labels for prediction losses. To better distinguish between speakers, a sentence mixture training strategy WavLM  [7]  is proposed, allowing for the unsupervised creation and merging of additional overlapping sentences during the training process. For text modality, we extract the sbert-chinese-general-v2 features, which is based on the bert-base-chinese version of the BERT model. ChatGLM  [14]  is an open-source conversational language model, based on the General Language Model (GLM) architecture. It demonstrats exceptional contextual understanding and more efficient inference capabilities. Baichuan  [59]  is an opensource large-scale model with 13 billion parameters. It features a larger size, more extensive training data, and more efficient inference capabilities.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Structure",
      "text": "For unimodal features, we utilize the fully-connected layers to extract hidden representations and predict deception:\n\nwhere ‚Ñé ùëö ùëñ ‚àà R ‚Ñé is the hidden feature for each modality, ùëë ùëñ ‚àà R 2 is the estimated deception probabilities. For multimodal features, different modalities contribute differently to deception detection. Therefore, we compute importance scores ùõº ùëñ ‚àà R 3√ó1 for each modality and exploit weighted fusion to obtain multimodal features:\n\nSimilarly, for personality traits and emotional expression features, feature fusion is achieved through concatenation. Personality features are derived directly from personality scale scores. Emotional expression features, however, are obtained by first training a dedicated emotion recognition model. Subsequently, all emotional expression samples are processed through this model, and the activations from the last fully connected layer are extracted. These activations then undergo average pooling to form the final emotion expression feature vector. It is noted that the model architecture employed for both personality recognition and emotion recognition tasks remains identical.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "The dimension of latent representations is selected from {64, 128, 256}. We employ the Adam optimizer  [30]  with a learning rate chosen from 10 -3 , 10 -4 , a weight decay of 10 -5 , and a maximum of 300 training epochs. To mitigate overfitting, dropout  [57]  is applied, with rates selected from {0.2, 0.3, 0.4, 0.5}. The cross-entropy loss function is used for optimization. For deception detection tasks, each sample comprises 24 answers. We randomly select 5 answers per sample (3 truthful, 2 deceptive) for validation, reserving the remaining 19 for training. All experiments are repeated five times with randomized initializations, and results report average performance to ensure statistical reliability. For personality and emotion recognition tasks, the dataset is split into 133 training samples, 40 validation samples, and 40 test samples; these tasks utilize root mean square error (RMSE) as the loss function.\n\nFor deception detection, accuracy was selected as the evaluation metric. For personality recognition, we employed the mean accuracy (A), defined as follows:  This metric is widely adopted in personality recognition tasks  [45, 61] . For emotion recognition, the root mean square error (RMSE) was used.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Results",
      "text": "This section establishes the benchmark for MDPE, designed to guide feature selection and inform the development of robust feature extractors.\n\nUnimodal deception detection results (Table  2 ) indicate that the textual modality significantly outperforms visual and acoustic modalities. This suggests that textual cues in our dataset provide more salient deceptive indicators. Subsequent multimodal fusion experiments (Table  3 ) reveal that integrating complementary modalities consistently enhances performance. This improvement aligns with the premise that deception manifests across multiple channels, enabling models to better comprehend video content and detect deception more accurately. Notably, while most unimodal features benefit from fusion, combining visual and acoustic features yields negligible gains or performance degradation. This implies that textual features stabilize model performance, a finding consistent with human deception judgment, where content-based (textual) cues typically dominate over visual or acoustic signals. Further exploration of deceptive indicators within visual and acoustic modalities is warranted.\n\nIncorporating personality features consistently improves deception detection performance, underscoring their importance for the task. While emotion features also enhance performance, their contribution is less pronounced than personality features and occasionally detrimental. This discrepancy may arise because personality traits serve as direct indicators, whereas emotion features depend on the quality of upstream emotion recognition models. Future work should explore more effective methods for leveraging emotional expression features. Combining both personality and emotion features achieves the highest unimodal deception detection performance, confirming their relevance and demonstrating the viability of individual-difference-based modeling.\n\nFor personality recognition (Table  3 ), acoustic features surpass visual/textual features in isolation, highlighting the predictive value of vocal cues. Full multimodal fusion achieves optimal performance, indicating cross-modal complementarity. In emotion recognition, textual features provide the strongest cues, and multimodal integration significantly outperforms unimodal approaches, emphasizing the necessity of cross-modal fusion for robust emotion understanding.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Limitations",
      "text": "Firstly, although the subjects were required that they must lie about the deception questions, and verified the deceptive questions and content with the Interviewer after the deception experiment, we do not know whether the subjects have actually deceived on the deception questions. Secondly, relying on self-assessment scales for data annotation is a subjective process for subjects, which may lead to bias in subsequent analysis. Different subjects may have significant differences in their perception of emotions. In addition, MDPE only collects native Chinese speakers, there may be cultural differences in deception detection. Finally, gender imbalance among subjects in MPDE is a common issue in human data collection  [12, 44] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce the Multimodal Deception Detection Dataset (MDPE), comprising video, audio, and textual modalities, supplemented with personality and emotion annotations. This dataset enables crossmodal analysis to investigate complementary relationships between modalities, thereby advancing robust deception detection methodologies relevant to societal security. Furthermore, MDPE facilitates research into the influence of personality traits and emotional states on deceptive behavior. Beyond its primary application, the dataset supports auxiliary tasks such as personality and emotion recognition, as well as joint analyses of deception-personality-emotion interactions. Benchmark experiments are provided to ensure reproducibility and establish a foundation for future work. By publicly releasing MDPE, we aim to stimulate progress in this critical area of affective computing.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Statistical and analytical results of MDPE.",
      "page": 4
    },
    {
      "caption": "Figure 1: (a)). The",
      "page": 4
    },
    {
      "caption": "Figure 1: (b) displays the average number of successful deception by",
      "page": 4
    },
    {
      "caption": "Figure 1: (c) presents the average intensity of each",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Number": "1 2 3 4 5 6 7 8 9",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "10",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "11",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "12",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "13",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "14",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "15",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        },
        {
          "Video Number": "16",
          "Sadness": "",
          "Relax": "",
          "Happiness": "",
          "Surprise": "",
          "Fear": "",
          "Angry": "",
          "Disgust": "",
          "Neutral": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Detecting deceptive behavior via integration of discriminative features from multiple modalities",
      "authors": [
        "Mohamed Abouelenien",
        "Ver√≥nica P√©rez-Rosas",
        "Rada Mihalcea",
        "Mihai Burzo"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "2",
      "title": "Emotional intelligence, Machiavellianism and emotional manipulation: Does EI have a dark side?",
      "authors": [
        "Elizabeth J Austin",
        "Daniel Farrelly",
        "Carolyn Black",
        "Helen Moore"
      ],
      "year": "2007",
      "venue": "Personality and individual differences"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Lie to me: Deceit detection via online behavioral learning",
      "authors": [
        "Nisha Bhaskaran",
        "Ifeoma Nwogu",
        "Mark Frank",
        "Venu Govindaraju"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Valerie Judee K Burgoon",
        "Laura Manusov",
        "Guerrero"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "Multimodal deception detection",
      "authors": [
        "Mihai Burzo",
        "Mohamed Abouelenien",
        "Veronica Perez-Rosas",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition"
    },
    {
      "citation_id": "7",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Personality, integrity, and white collar crime: A construct validity study",
      "authors": [
        "M Judith",
        "Frank Collins",
        "Schmidt"
      ],
      "year": "1993",
      "venue": "Personnel psychology"
    },
    {
      "citation_id": "9",
      "title": "Cues to deception",
      "authors": [
        "James Bella M Depaulo",
        "Brian Lindsay",
        "Laura Malone",
        "Kelly Muhlenbruck",
        "Harris Charlton",
        "Cooper"
      ],
      "year": "2003",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "10",
      "title": "Moral disengagement in ethical decision making: a study of antecedents and outcomes",
      "authors": [
        "Linda James R Detert",
        "Vicki Trevi√±o",
        "Sweitzer"
      ],
      "year": "2008",
      "venue": "Journal of applied psychology"
    },
    {
      "citation_id": "11",
      "title": "Differentiation of deception using pupillary responses as an index of cognitive processing",
      "authors": [
        "Daphne Dionisio",
        "Eric Granholm",
        "William Hillix",
        "William Perrine"
      ],
      "year": "2001",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "12",
      "title": "Exclusion of females in autism research: Empirical evidence for a \"leaky\" recruitment-to-research pipeline",
      "authors": [
        "M D' Anila",
        "Isabelle Mello",
        "Cindy Frosch",
        "Annie Li",
        "John Cardinaux",
        "Gabrieli"
      ],
      "year": "2022",
      "venue": "Autism Research"
    },
    {
      "citation_id": "13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "14",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2021",
      "venue": "Glm: General language model pretraining with autoregressive blank infilling",
      "arxiv": "arXiv:2103.10360"
    },
    {
      "citation_id": "15",
      "title": "Telling lies: Clues to deceit in the marketplace, politics, and marriage",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2009",
      "venue": "Telling lies: Clues to deceit in the marketplace, politics, and marriage"
    },
    {
      "citation_id": "16",
      "title": "Syntactic stylometry for deception detection",
      "authors": [
        "Song Feng",
        "Ritwik Banerjee",
        "Yejin Choi"
      ],
      "year": "2012",
      "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Eye blinks: new indices for the detection of deception",
      "authors": [
        "Kyosuke Fukuda"
      ],
      "year": "2001",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "18",
      "title": "Emotional intelligence and deception: A theoretical model and propositions",
      "authors": [
        "Redona Joseph P Gaspar",
        "Maurice Methasani",
        "Schweitzer"
      ],
      "year": "2022",
      "venue": "Journal of Business Ethics"
    },
    {
      "citation_id": "19",
      "title": "The emotion deception model: A review of deception in negotiation and the role of emotion in deception",
      "authors": [
        "P Joseph",
        "Maurice Gaspar",
        "Schweitzer"
      ],
      "year": "2013",
      "venue": "Negotiation and Conflict Management Research"
    },
    {
      "citation_id": "20",
      "title": "Affective personalization of a social robot tutor for children's second language skills",
      "authors": [
        "Goren Gordon",
        "Samuel Spaulding",
        "Jacqueline Westlund",
        "Jin Lee",
        "Luke Plummer",
        "Marayna Martinez",
        "Madhurima Das",
        "Cynthia Breazeal"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "21",
      "title": "A general theory of crime",
      "authors": [
        "Travis Michael R Gottfredson",
        "Hirschi"
      ],
      "year": "1990",
      "venue": "A general theory of crime"
    },
    {
      "citation_id": "22",
      "title": "When \"the show must go on\": Surface acting and deep acting as determinants of emotional exhaustion and peer-rated service delivery",
      "authors": [
        "Alicia Grandey"
      ],
      "year": "2003",
      "venue": "Academy of management Journal"
    },
    {
      "citation_id": "23",
      "title": "A new theoretical perspective on deception detection: On the psychology of instrumental mind-reading",
      "authors": [
        "P√§r Anders",
        "Maria Hartwig"
      ],
      "year": "2008",
      "venue": "Psychology, Crime & Law"
    },
    {
      "citation_id": "24",
      "title": "Bag-of-lies: A multimodal dataset for deception detection",
      "authors": [
        "Viresh Gupta",
        "Mohit Agarwal",
        "Manik Arora",
        "Tanmoy Chakraborty",
        "Richa Singh",
        "Mayank Vatsa"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "25",
      "title": "Lie detection from multiple cues: A meta-analysis",
      "authors": [
        "Maria Hartwig",
        "F Charles",
        "Bond"
      ],
      "year": "2014",
      "venue": "Applied Cognitive Psychology"
    },
    {
      "citation_id": "26",
      "title": "Distinguishing deceptive from non-deceptive speech",
      "authors": [
        "Julia Bell Hirschberg",
        "Stefan Benus",
        "Jason Brenier",
        "Frank Enos",
        "Sarah Friedman",
        "Sarah Gilman",
        "Cynthia Girand",
        "Martin Graciarena",
        "Andreas Kathol",
        "Laura Michaelis"
      ],
      "year": "2005",
      "venue": "Distinguishing deceptive from non-deceptive speech"
    },
    {
      "citation_id": "27",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Executing facial control during deception situations",
      "authors": [
        "M Carolyn",
        "Mark Hurley",
        "Frank"
      ],
      "year": "2011",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "29",
      "title": "The dark triad and normal personality traits",
      "authors": [
        "Sharon Jakobwitz",
        "Vincent Egan"
      ],
      "year": "2006",
      "venue": "Personality and Individual differences"
    },
    {
      "citation_id": "30",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "31",
      "title": "A deep learning approach for multimodal deception detection",
      "authors": [
        "Gangeshwar Krishnamurthy",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "International Conference on Computational Linguistics and Intelligent Text Processing"
    },
    {
      "citation_id": "32",
      "title": "Cross-cultural production and detection of deception from speech",
      "authors": [
        "Guzhen Sarah I Levitan",
        "Mandi An",
        "Gideon Wang",
        "Julia Mendels",
        "Michelle Hirschberg",
        "Andrew Levine",
        "Rosenberg"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on workshop on multimodal deception detection"
    },
    {
      "citation_id": "33",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "Pupillary size in response to a visual guilty knowledge test: New technique for the detection of deception",
      "authors": [
        "Ofer Lubow",
        "Fein"
      ],
      "year": "1996",
      "venue": "Journal of Experimental Psychology: Applied"
    },
    {
      "citation_id": "35",
      "title": "Deep learning-based document modeling for personality detection from text",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "36",
      "title": "Voice Emotion Games: Language and Emotion in the Voice of Children with Autism Spectrum Conditio",
      "authors": [
        "Erik Marchi",
        "Bj√∂rn Schuller",
        "Simon Baron-Cohen",
        "Amandine Lassalle",
        "O' Helen",
        "Delia Reilly",
        "Ofer Pigat",
        "S Golan",
        "Shahar Friedenson",
        "Tal",
        "Bolte"
      ],
      "year": "2015",
      "venue": "Proceedings of the 3rd International Workshop on Intelligent Digital Games for Empowerment and Inclusion (IDGEI 2015) as part of the 20th ACM International Conference on Intelligent User Interfaces, IUI 2015. 9-pages"
    },
    {
      "citation_id": "37",
      "title": "Introducing representations of facial affect in automated multimodal deception detection",
      "authors": [
        "Leena Mathur",
        "Maja Matariƒá"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "38",
      "title": "A comparison of different features for automatic eye blinking detection with an application to analysis of deceptive behavior",
      "authors": [
        "Kyrii Minkov",
        "Stefanos Zafeiriou",
        "Maja Pantic"
      ],
      "year": "2012",
      "venue": "2012 5th International Symposium on Communications, Control and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Lying words: Predicting deception from linguistic styles",
      "authors": [
        "James Matthew L Newman",
        "Diane Pennebaker",
        "Jane Berry",
        "Richards"
      ],
      "year": "2003",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "40",
      "title": "The design and development of a lie detection system using facial micro-expressions",
      "authors": [
        "Michel Owayjan",
        "Ahmad Kashour",
        "Nancy Haddad",
        "Mohamad Fadel",
        "Ghinwa Souki"
      ],
      "year": "2012",
      "venue": "2012 2nd international conference on advances in computational tools for engineering applications (ACTEA)"
    },
    {
      "citation_id": "41",
      "title": "Preliminary preparation and evaluation of China emotional image material library",
      "authors": [
        "Yuxia Huang",
        "Pengfei Xu",
        "Yuejia Luo"
      ],
      "year": "2010",
      "venue": "Chinese mental health psychology Journal"
    },
    {
      "citation_id": "42",
      "title": "Verbal and nonverbal clues for real-life deception detection",
      "authors": [
        "Ver√≥nica P√©rez-Rosas",
        "Mohamed Abouelenien",
        "Rada Mihalcea",
        "Yao Xiao",
        "Mihai Cj Linton",
        "Burzo"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "43",
      "title": "A Multimodal Dataset for Deception Detection",
      "authors": [
        "Ver√≥nica P√©rez-Rosas",
        "Rada Mihalcea",
        "Alexis Narvaez",
        "Mihai Burzo"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "44",
      "title": "Dementia clinical trials over the past decade: are women fairly represented?",
      "authors": [
        "Ana-Catarina Pinho-Gomes",
        "Jessica Gong",
        "Katie Harris",
        "Mark Woodward",
        "Cheryl Carcel"
      ],
      "year": "2022",
      "venue": "BMJ Neurology Open"
    },
    {
      "citation_id": "45",
      "title": "Chalearn lap 2016: First round challenge on first impressions-dataset and results",
      "authors": [
        "V√≠ctor Ponce-L√≥pez",
        "Baiyu Chen",
        "Marc Oliu",
        "Ciprian Corneanu",
        "Albert Clap√©s",
        "Isabelle Guyon",
        "Xavier Bar√≥",
        "Hugo Escalante",
        "Sergio Escalera"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops: Amsterdam"
    },
    {
      "citation_id": "46",
      "title": "Leakage\" in deceptive facial expressions relates to psychopathy and emotional intelligence",
      "authors": [
        "Stephen Porter",
        "Leanne Ten Brinke",
        "Alysha Baker",
        "Brendan Wallace"
      ],
      "year": "2011",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "47",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "48",
      "title": "Thermal facial analysis for deception detection",
      "authors": [
        "A Bashar",
        "Reyer Rajoub",
        "Zwiggelaar"
      ],
      "year": "2014",
      "venue": "IEEE transactions on information forensics and security"
    },
    {
      "citation_id": "49",
      "title": "Revised NEO Personality Inventory profiles of Machiavellian and non-Machiavellian people",
      "authors": [
        "Anupama Nerella V Ramanaiah",
        "Fred Byravan",
        "Detwiler"
      ],
      "year": "1994",
      "venue": "Psychological Reports"
    },
    {
      "citation_id": "50",
      "title": "A sentimentaware deep learning approach for personality detection from text",
      "authors": [
        "Zhancheng Ren",
        "Qiang Shen",
        "Xiaolei Diao",
        "Hao Xu"
      ],
      "year": "2021",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "51",
      "title": "The voice of deceit: Refining and expanding vocal cues to deception",
      "authors": [
        "Patricia Rockwell",
        "David Buller",
        "Judee Burgoon"
      ],
      "year": "1997",
      "venue": "Communication Research Reports"
    },
    {
      "citation_id": "52",
      "title": "More intelligent extraverts are more likely to deceive",
      "authors": [
        "Justyna Sarzy≈Ñska",
        "Marcel Falkiewicz",
        "Monika Riegel",
        "Justyna Babula",
        "Edward Daniel S Margulies",
        "Anna Nƒôcka",
        "Iwona Grabowska",
        "Szatkowska"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "53",
      "title": "Multimodal deception detection using real-life trial data",
      "authors": [
        "Veronica Umut ≈ûen",
        "Berrin Perez-Rosas",
        "Mohamed Yanikoglu",
        "Mihai Abouelenien",
        "Rada Burzo",
        "Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Box of lies: Multimodal deception detection in dialogues",
      "authors": [
        "Felix Soldner",
        "Ver√≥nica P√©rez-Rosas",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "55",
      "title": "Deception detection and remote physiological monitoring: A dataset and baseline experimental results",
      "authors": [
        "Jeremy Speth",
        "Nathan Vance",
        "Adam Czajka",
        "Kevin Bowyer",
        "Diane Wright",
        "Patrick Flynn"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "56",
      "title": "Paraverbal indicators of deception: A meta-analytic synthesis",
      "authors": [
        "Siegfried Ludwig",
        "Barbara Schwandt"
      ],
      "year": "2006",
      "venue": "Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition"
    },
    {
      "citation_id": "57",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "58",
      "title": "Detecting lies and deceit: Pitfalls and opportunities",
      "authors": [
        "Aldert Vrij"
      ],
      "year": "2008",
      "venue": "Detecting lies and deceit: Pitfalls and opportunities"
    },
    {
      "citation_id": "59",
      "title": "Open largescale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open largescale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "60",
      "title": "The big five inventory-2 in China: A comprehensive psychometric evaluation in four diverse samples",
      "authors": [
        "Bo Zhang",
        "Ming Li",
        "Jian Li",
        "Jing Luo",
        "Yonghao Ye",
        "Lu Yin",
        "Zhuosheng Chen",
        "Christopher Soto",
        "Oliver John"
      ],
      "year": "2022",
      "venue": "The big five inventory-2 in China: A comprehensive psychometric evaluation in four diverse samples"
    },
    {
      "citation_id": "61",
      "title": "PersEmoN: a deep network for joint analysis of apparent personality, emotion and their relationship",
      "authors": [
        "Le Zhang",
        "Songyou Peng",
        "Stefan Winkler"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Automating linguistics-based cues for detecting deception in text-based asynchronous computer-mediated communications",
      "authors": [
        "Lina Zhou",
        "Jay Judee K Burgoon",
        "Doug Nunamaker",
        "Twitchell"
      ],
      "year": "2004",
      "venue": "Automating linguistics-based cues for detecting deception in text-based asynchronous computer-mediated communications"
    },
    {
      "citation_id": "63",
      "title": "Veracity judgement, not accuracy: Reconsidering the role of facial expressions, empathy, and emotion recognition training on deception detection",
      "authors": [
        "Mircea Zloteanu",
        "Peter Bull",
        "Eva Krumhuber",
        "Daniel C Richardson"
      ],
      "year": "2021",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "64",
      "title": "Verbal and nonverbal communication of deception",
      "authors": [
        "Bella Miron Zuckerman",
        "Robert Depaulo",
        "Rosenthal"
      ],
      "year": "1981",
      "venue": "Advances in experimental social psychology"
    }
  ]
}