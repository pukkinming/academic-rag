{
  "paper_id": "2409.14221v1",
  "title": "Strong Alone, Stronger Together: Synergizing Modality-Binding Foundation Models With Optimal Transport For Non-Verbal Emotion Recognition",
  "published": "2024-09-21T18:58:10Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Mohd Mujtaba Akhtar",
    "Girish",
    "Swarup Ranjan Behera",
    "Sishir Kalita",
    "Arun Balaji Buduru",
    "Rajesh Sharma",
    "S. R Mahadeva Prasanna"
  ],
  "keywords": [
    "Non-Verbal Emotion Recognition",
    "Multimodal Foundation Models",
    "LanguageBind",
    "ImageBind"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we investigate multimodal foundation models (MFMs) for emotion recognition from non-verbal sounds. We hypothesize that MFMs, with their joint pre-training across multiple modalities, will be more effective in non-verbal sounds emotion recognition (NVER) by better interpreting and differentiating subtle emotional cues that may be ambiguous in audio-only foundation models (AFMs). To validate our hypothesis, we extract representations from state-of-the-art (SOTA) MFMs and AFMs and evaluated them on benchmark NVER datasets. We also investigate the potential of combining selected foundation model representations to enhance NVER further inspired by research in speech recognition and audio deepfake detection. To achieve this, we propose a framework called MATA (Intra-Modality Alignment through Transport Attention). Through MATA coupled with the combination of MFMs: LanguageBind and ImageBind, we report the topmost performance with accuracies of 76.47%, 77.40%, 75.12% and F1-scores of 70.35%, 76.19%, 74.63% for ASVP-ESD, JNV, and VIVAE datasets against individual FMs and baseline fusion techniques and report SOTA on the benchmark datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition plays a critical role in understanding human behavior, affecting decision-making, interpersonal relationships, and well-being. While emotions can be identified through multiple channels -such as facial expressions, physiological signals, and vocal cues -non-verbal sounds offer a unique and often underexplored perspective. Non-verbal vocalizations, including laughter, cries, and sighs, convey a broad spectrum of emotions that enhance communication in daily life. Recognizing emotions from these non-verbal vocal cues has applications in diverse areas, such as healthcare, human-computer interaction, customer service, and security. In this study, we focus specifically on non-verbal emotion recognition (NVER).\n\nHowever, recent research in emotion recognition has largely centered around verbal speech, employing both handcrafted spectral features  [1]  and more recently, audio foundation models (AFMs)  [2] . AFMs, such as WavLM  [3] , wav2vec2  [4] , and HuBERT  [5] , have shown considerable promise in capturing emotional cues in speech. These founda-tion models (FMs) are typically fine-tuned or used as feature extractors for downstream emotion recognition tasks. While significant progress has been made, non-verbal vocalizations remain underrepresented in the field except a few notable ones  [6] ,  [7] ,  [8] . Furthermore, multimodal foundation models (MFMs) remain largely unexplored for NVER despite their potential for more nuanced emotional interpretation.\n\nIn this paper, we aim to address this gap by exploring the use of MFMs for NVER. We hypothesize that MFMs, are better equipped for NVER due to their multimodal pre-training that enhances their contextual understanding, enabling the model to better interpret and differentiate subtle emotional cues in non-verbal sounds that may be ambiguous in AFMs. To test this hypothesis, we conduct a comparative study of state-of-the-art (SOTA) MFMs (LanguageBind and ImageBind) and AFMs (WavLM, Unispeech-SAT, and Wav2vec2) by extracting their representations and building a simple downstream CNN model on benchmark NVER datasets (ASVP-ESD, JNV, and VIVAE).\n\nFurthermore, inspired by research in related areas, such as speech recognition  [9]  and audio deepfake detection  [10] , which have demonstrated the effectiveness of combining FMs due to their complementary behavior, we take the first step in NVER toward this direction. For this purpose, we propose MATA (Intra-Modality Alignment through Transport Attention) framework for the effective fusion of FMs. MATA introduces a novel fusion mechanism leveraging optimal transport to align and integrate representations from FMs.\n\nOur study shows that MATA with the fusion of ImageBind and LanguageBind outperform all the individual FMs as well as baseline fusion techniques and leads to SOTA results across NVER benchmarks. Our contributions are summarized as follows:\n\n• We conduct the first comprehensive comparative study of SOTA MFMs and AFMs, demonstrating the superior performance of MFMs for NVER, surpassing unimodal AFMs. • We introduce a novel fusion framework, MATA, that effectively combines FMs representations. With MATA, we achieve the highest reported performance across multiple NVER benchmark datasets, outperforming both individual FMs and baseline fusion techniques. We will share the models and codes curated as part of this research after the review process.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Foundation Models",
      "text": "In this section, we provide an overview of the SOTA MFMs and AFMs considered in our study. These models are selected due to their SOTA performance across various benchmarks in their respective domains.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Foundation Models",
      "text": "ImageBind 1 (IB)  [11]  learns from images, audio, text, IMU, depth, and thermal data, aligning other modality representations to image representations. It uses InfoNCE-based optimization and transformer architecture and support zero-shot capability. It associates modality pairs without paired training data and demonstrating strong cross-modal generalization. LanguageBind 2 (LB)  [12]  uses language as the anchor modality due to its rich contextual knowledge. It aligns video, depth, audio, and infrared data to a frozen language encoder through contrastive learning. Pre-trained on the VIDAL-10M dataset, LanguageBind achieves SOTA performance across several benchmarks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Audio Foundation Models",
      "text": "We select the AFMs that has shown SOTA performance in SUPERB  [13]  and pre-trained on large scale diverse speech data. WavLM 3  [14]  combines masked speech modeling and denoising during pre-training and uses 94k hours of data from VoxPopuli, LibriLight, and GigaSpeech datasets. UniSpeech-SAT 4  [15]  uses contrastive utternace-wise loss, speaker-aware learning for SOTA performance in speech processing and trained on 94k hours of Gigaspeech, Voxpopuli, and LibriVox datasets. Wav2vec2 5  [16]  doesn't shows SOTA performance like WavLM and Unispeech-SAT in SUPERB. However, we use it due to its performance in speech emotion recognition  [4] . It is trained in a self-supervised fashion that masks speech inputs at the latent level and optimizing via contrastive learning.\n\nWe resample all the audios to 16kHz before passing to the MFMs and AFMs. The representations are extracted using average pooling from the last hidden layer of the FMs, resulting in dimensions of 1024 for ImageBind and 768 for LanguageBind, WavLM, UniSpeech-SAT, and Wav2vec2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Modeling",
      "text": "In this section, we discuss the downstream modeling used for individual FMs and the proposed framework, MATA for fusing FMs. Fig.  1 : MATA framework: OT and FCN stand for Optimal Transport and Fully Connected Network, respectively. FM1 and FM2 refer to Foundation Model 1 and 2; U11 and U22 represent features from individual FM branches, while U12 and U21 represent features transported from FM2 to the FM1 network and from FM1 to the FM2 network, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Individual Foundation Models",
      "text": "The extracted representations from each FM are passed through two convolutional blocks. We experiment with CNN due to its capability shown in related emotion recognition research  [17] . Each convolutional block comprises a 1D convolutional layer followed by max-pooling. The first convolutional block uses 64 filters with a kernel size of 3x3, while the second block employs 128 filters with the same size as the first block. The features are then flattened and passed through a dense layer with 128 neurons. Finally, an output layer with softmax activation predicts the emotion classes, matching the number of output neurons to the number of target classes. The training parameters of the downstream models for different FM representations range from 6.2M to 8.3M.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Modality Alignment Through Transport Attention (Mata)",
      "text": "The architecture of MATA is shown in Figure  1 . For each FM, the extracted representations are passed through two convolutional blocks with the same modeling as used in the individual models above. However, the number of filters used in 1D-CNN in two convolution blocks are 32 and 64. Then, it is flattened, followed by linear projection to 120-dimension. The projection to lower dimensions is due to computational constraints. Then, the features of each network block from individual FMs are passed through the fusion block, which encompasses the optimal transport (OT) distance M for effective fusion  [18]  of FMs. M between the feature matrices, x 1 and x 2 from two FMs, computed via normalized Euclidean distance: To align the features, we apply the Sinkhorn algorithm to obtain the optimal transport plan γ, where: γ = Sinkhorn(M ). Using γ, we transport features between FMs networks, producing x 2 → x 1 and\n\nThese transported features are concatenated with the original features from FMs to form the fused representations:\n\nThese fused features are then concatenated with the original features from the opposite FM, as shown in Figure  1 , and the resultant features are finally concatenated and passed to the Multi-Head Attention (MHA) block. The MHA block ensures further better feature interaction due to its selfattention mechanism. The attention output is computed as:\n\nwhere Q and K are the query and key matrices derived from the final concatenated features. V represents the feature vectors that are attended to. Here, we are using multiple attention heads, and the number of heads is 8. The number of training parameters of MATA with different combinations of FMs ranges from 4M to 4.5M.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Benchmark Datasets",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Asvp-Esd [19]:",
      "text": "This dataset includes thousands of highquality audio recordings labeled with 12 emotions and an additional class breath. The recordings were captured in natural environments with diverse speakers comprising speech and non-speech emotional sounds. We use only the nonspeech part in our experiments. The audio samples were gathered from various sources, including films, TV programs, YouTube channels, and other online platforms.\n\nJNV  [20] : It features 420 audio clips from four native Japanese speakers (two male, two female) expressing six emotions: anger, disgust, fear, happiness, sadness, and surprise. Recorded at 48 kHz in an anechoic chamber, the dataset includes both predefined and spontaneous vocalizations. VIVAE  [21] : It includes 1,085 audio files from eleven speakers expressing three positive (achievement, pleasure, surprise) and three negative emotions (anger, fear, pain) at",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Training Details",
      "text": "We trained our models for 50 epochs with a learning rate of 1e-3 and Adam as the optimizer. We use cross-entropy as the loss function and batch size of 32. Early stopping and dropout are employed to prevent overfitting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Results And Discussion",
      "text": "We present the results of the experiments in Table  I . We first evaluated individual FMs. LB achieved the highest performance across all the NVER datasets, with an accuracy of 75.55% and an F1-score of 67.55% on ASVP-ESD, 73.65% accuracy and an F1-score of 72.51% on JNV, and 70.05% accuracy and an F1-score of 69.80% on VIVAE, significantly outperforming all other FMs. In summary, the MFMs perform better than the AFMs for NVER, thus proving our hypothesis that MFMs capture complex emotional nuances due to their multimodal pre-training that may be ambiguous to AFMs. The t-SNE plots of the raw representations from the FMs are shown in Figure  2 . We observe better clusters across emotions for MFMs in comparison to the AFMs.\n\nWhen combining the FMs through MATA, we obtain the topmost performance against all the individual FMs and the baseline concatenation-based fusion technique. In concatenation-based fusion, we use the same architectural components as MATA. This shows the observable complementary behavior of the MFMs as well as the effectiveness of MATA in performing effective fusion of the MFMs. With MATA, we also observe that the fusion of MFMs and AFMs gives comparatively better results than individual FMs as well as the baseline concatenation-based fusion technique. The confusion matrices of MATA, with Unispeech-SAT + WavLM and LanguagebIND + ImageBind are shown in Figure  3 . We also provide an ablation study of MATA without the MHA block (Table  I : Fusion with OT); we observe better results than the individual FMs, comparative results, and sometimes better performance with some pairs of FMs. Additional Experiments: To show the generalizability of the proposed framework, MATA, we also experimented on a benchmark speech emotion recognition (SER) dataset, CREMA-D  [22] . It consists of 7,442 clips from 91 actors (48 male, 43 female) expressing six basic emotions: happiness, sadness, anger, fear, disgust, and neutral. Rated by 2,443 participants across audio-only, visual-only, and audio-visual modalities. Due to the diversity of the speakers, CREMA-D serve as essential benchmark for emotion recognition systems. From Table  I , we observe that MFMs show better performance than AFMs. However, we achieve the topmost performance with MATA with the combination of Lan-guageBind and ImageBind representations, thus showing the effectiveness of the proposed framework.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "Our study demonstrates the effectiveness of MFMs for NVER. This performance can be attributed to their joint pretraining across multiple modalities that provide better contextual understanding and excel in capturing subtle emotional cues that AFMs may miss. Through extensive evaluation of the benchmark NVER datasets, we confirm the superior performance of MFMs (LanguageBind and ImageBind) in comparison to AFMs such as WavLM, Unispeech-SAT and Wav2vec2. We show more improved performance through the fusion of the FMs by proposing MATA for effective fusion. With MATA, we achieve top performance against all the individual FMs as well as baseline fusion techniques, thus achieving SOTA performance across the NVER benchmark datasets under consideration. Our study provides valuable insights for future research in selecting optimal representations for NVER and usage of MFMs. It also opens pathways for developing effective fusion techniques for the fusion of FMs.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: MATA framework: OT and FCN stand for Optimal",
      "page": 2
    },
    {
      "caption": "Figure 1: , and the resultant features are finally concatenated and",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE plots of raw representations from FMs on the",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion Matrix of MATA: UNI, WA, LB, and",
      "page": 4
    },
    {
      "caption": "Figure 2: We observe better clusters across",
      "page": 4
    },
    {
      "caption": "Figure 3: We also provide an ablation study of MATA",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‡Contributed equally": "orchidp@iiitd.ac.in"
        },
        {
          "‡Contributed equally": "Abstract—In this\nstudy, we\ninvestigate multimodal\nfounda-"
        },
        {
          "‡Contributed equally": "tion models\n(MFMs)\nfor emotion recognition from non-verbal"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "sounds. We hypothesize that MFMs, with their joint pre-training"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "across multiple modalities, will be more effective in non-verbal"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "sounds emotion recognition (NVER) by better interpreting and"
        },
        {
          "‡Contributed equally": "differentiating subtle\nemotional\ncues\nthat may be ambiguous"
        },
        {
          "‡Contributed equally": "in\naudio-only\nfoundation models\n(AFMs).\nTo\nvalidate\nour"
        },
        {
          "‡Contributed equally": "hypothesis, we\nextract\nrepresentations\nfrom state-of-the-art"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "(SOTA) MFMs and AFMs and evaluated them on benchmark"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "NVER datasets. We also investigate the potential of combining"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "selected foundation model\nrepresentations\nto\nenhance NVER"
        },
        {
          "‡Contributed equally": "further\ninspired by research in speech recognition and audio"
        },
        {
          "‡Contributed equally": "deepfake detection. To achieve\nthis, we propose a framework"
        },
        {
          "‡Contributed equally": "called MATA (Intra-Modality Alignment\nthrough Transport"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "Attention). Through MATA coupled with the\ncombination of"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "MFMs: LanguageBind and ImageBind, we report\nthe topmost"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "performance with accuracies of 76.47%, 77.40%, 75.12% and"
        },
        {
          "‡Contributed equally": "F1-scores\nof\n70.35%,\n76.19%,\n74.63% for ASVP-ESD,\nJNV,"
        },
        {
          "‡Contributed equally": "and VIVAE datasets against individual FMs and baseline fusion"
        },
        {
          "‡Contributed equally": "techniques and report SOTA on the benchmark datasets."
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "Index Terms: Non-Verbal Emotion Recognition, Multi-"
        },
        {
          "‡Contributed equally": "modal Foundation Models, LanguageBind, ImageBind"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "I.\nINTRODUCTION"
        },
        {
          "‡Contributed equally": ""
        },
        {
          "‡Contributed equally": "Emotion recognition plays a critical\nrole in understanding"
        },
        {
          "‡Contributed equally": "human behavior, affecting decision-making,\ninterpersonal re-"
        },
        {
          "‡Contributed equally": "lationships, and well-being. While emotions can be identified"
        },
        {
          "‡Contributed equally": "through multiple channels - such as facial expressions, phys-"
        },
        {
          "‡Contributed equally": "iological\nsignals, and vocal cues\n- non-verbal\nsounds offer"
        },
        {
          "‡Contributed equally": "a unique\nand often underexplored perspective. Non-verbal"
        },
        {
          "‡Contributed equally": "vocalizations,\nincluding laughter, cries, and sighs, convey a"
        },
        {
          "‡Contributed equally": "broad spectrum of emotions that enhance communication in"
        },
        {
          "‡Contributed equally": "daily life. Recognizing emotions from these non-verbal vocal"
        },
        {
          "‡Contributed equally": "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,"
        },
        {
          "‡Contributed equally": "human-computer\ninteraction, customer service, and security."
        },
        {
          "‡Contributed equally": "In this\nstudy, we\nfocus\nspecifically on non-verbal\nemotion"
        },
        {
          "‡Contributed equally": "recognition (NVER)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "human-computer\ninteraction, customer service, and security.",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "across NVER benchmarks."
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "In this\nstudy, we\nfocus\nspecifically on non-verbal\nemotion",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "Our contributions are summarized as follows:"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "recognition (NVER).",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "• We conduct\nthe first comprehensive comparative study"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "However,\nrecent\nresearch\nin\nemotion\nrecognition\nhas",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "of SOTA MFMs and AFMs, demonstrating the superior"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "largely centered around verbal speech, employing both hand-",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "performance of MFMs for NVER, surpassing unimodal"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "crafted spectral\nfeatures\n[1] and more recently, audio foun-",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "AFMs."
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "dation models\n(AFMs)\n[2]. AFMs,\nsuch\nas WavLM [3],",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "• We\nintroduce\na novel\nfusion framework, MATA,\nthat"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "wav2vec2 [4],\nand HuBERT [5], have\nshown considerable",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "effectively combines FMs representations. With MATA,"
        },
        {
          "cues has\napplications\nin diverse\nareas,\nsuch as healthcare,": "promise in capturing emotional cues in speech. These founda-",
          "as\nbaseline\nfusion\ntechniques\nand\nleads\nto\nSOTA results": "we\nachieve\nthe\nhighest\nreported\nperformance\nacross"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "Fusion Block"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "Input Audio"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "Fig. 1: MATA framework: OT and FCN stand for Optimal"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "Transport and Fully Connected Network,\nrespectively. FM1"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "and FM2 refer\nto Foundation Model 1 and 2; U11 and U22"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "represent\nfeatures from individual FM branches, while U12"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "and U21 represent features transported from FM2 to the FM1"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "network and from FM1 to the FM2 network,\nrespectively."
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "A.\nIndividual Foundation Models"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "The\nextracted representations\nfrom each FM are passed"
        },
        {
          "FM1\nFM2": "through two convolutional blocks. We experiment with CNN"
        },
        {
          "FM1\nFM2": "due\nto its\ncapability shown in related emotion recognition"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "research\n[17]. Each\nconvolutional\nblock\ncomprises\na\n1D"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "convolutional\nlayer\nfollowed by max-pooling. The first con-"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "volutional block uses 64 filters with a kernel\nsize of 3x3,"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "while the second block employs 128 filters with the same size"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "as the first block. The features are then flattened and passed"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "through a dense layer with 128 neurons. Finally, an output"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "layer with softmax activation predicts\nthe\nemotion classes,"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "matching the number of output neurons\nto the number of"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "target\nclasses. The\ntraining parameters of\nthe downstream"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "models for different FM representations range from 6.2M to"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "8.3M."
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "B. Modality Alignment\nthrough Transport Attention (MATA)"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "The\narchitecture\nof MATA is\nshown\nin Figure\n1. For"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "each FM,\nthe\nextracted representations\nare passed through"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "two convolutional blocks with the\nsame modeling as used"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "in\nthe\nindividual models\nabove. However,\nthe\nnumber\nof"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "filters used in 1D-CNN in two convolution blocks\nare 32"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "and 64. Then,\nit\nis flattened,\nfollowed by linear projection"
        },
        {
          "FM1\nFM2": "to\n120-dimension. The\nprojection\nto\nlower\ndimensions\nis"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "due to computational constraints. Then,\nthe features of each"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "network block from individual FMs are passed through the"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "fusion block, which encompasses the optimal\ntransport (OT)"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "distance M for effective fusion [18] of FMs. M between the"
        },
        {
          "FM1\nFM2": "from two FMs,\ncomputed via\nfeature matrices, x1\nand x2"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "normalized Euclidean distance:"
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "∥x1 − x2∥2"
        },
        {
          "FM1\nFM2": "M ="
        },
        {
          "FM1\nFM2": ""
        },
        {
          "FM1\nFM2": "max(∥x1 − x2∥2)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LanguageBind, ImageBind, Unispeech-SAT, WavLM, and Wav2vec2, respectively. F1-Score is the macro-average F1-Score.",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "Features",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "Accuracy"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "69.12"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "55.30"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "36.87"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "35.94"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "46.54"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+IB",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "67.48"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "61.75"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "65.44"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "60.83"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "53.00"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "58.53"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "56.22"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "44.70"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "49.31"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "WA+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "48.85"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+IB",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "70.05"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "61.29"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "62.21"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "66.82"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "57.14"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "57.14"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "59.91"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "41.01"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "46.54"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "WA+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "49.77"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": ""
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+IB",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "75.12"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "65.44"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "69.12"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "LB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "69.59"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+UNI",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "60.37"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "61.75"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "IB+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "60.37"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WA",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "45.16"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "UNI+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "48.85"
        },
        {
          "TABLE I: Evaluation Scores: Scores are in % and represent": "WA+WAV2",
          "the average of 5 folds. LB, IB, UNI, WA, and WAV2 stands for": "50.69"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "WA+WAV2\n62.36\n52.29\n59.63",
          "55.02\n48.85\n47.58\n70.99\n71.06": "57.71\n50.69\n49.24\n67.36\n67.49"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "To\nalign\nthe\nfeatures, we\napply\nthe\nSinkhorn\nalgo-",
          "55.02\n48.85\n47.58\n70.99\n71.06": "of training parameters of MATA with different combinations"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "rithm to obtain the optimal\ntransport plan γ, where: γ =",
          "55.02\n48.85\n47.58\n70.99\n71.06": "of FMs ranges from 4M to 4.5M."
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "Sinkhorn(M ). Using γ, we transport\nfeatures between FMs",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "IV. EXPERIMENTS"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "networks, producing x2 → x1\nand x1 → x2: x2 → x1 =",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "transported features\nγ · x2,\nx1 → x2 = γT\n· x1. These",
          "55.02\n48.85\n47.58\n70.99\n71.06": "A. Benchmark Datasets"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "are\nconcatenated with\nthe\noriginal\nfeatures\nfrom FMs\nto",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "ASVP-ESD [19]: This dataset\nincludes\nthousands of high-"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "form the\nfused\nrepresentations:\nfused1 = Concat(x2 →",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "quality audio recordings\nlabeled with 12 emotions\nand an"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "x1, x1), fused2 = Concat(x1 → x2, x2).",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "additional class breath. The recordings were captured in nat-"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "These fused features are then concatenated with the orig-",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "ural environments with diverse speakers comprising speech"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "inal\nfeatures\nfrom the\nopposite FM,\nas\nshown\nin Figure",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "and\nnon-speech\nemotional\nsounds. We\nuse\nonly\nthe\nnon-"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "1,\nand\nthe\nresultant\nfeatures\nare finally\nconcatenated\nand",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "speech\npart\nin\nour\nexperiments. The\naudio\nsamples were"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "passed to the Multi-Head Attention (MHA) block. The MHA",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "gathered from various sources, including films, TV programs,"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "block ensures further better feature interaction due to its self-",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "YouTube channels, and other online platforms."
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "attention mechanism. The attention output\nis computed as:",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "JNV [20]:\nIt\nfeatures\n420\naudio\nclips\nfrom four\nnative"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "(cid:19)",
          "55.02\n48.85\n47.58\n70.99\n71.06": "Japanese\nspeakers\n(two male,\ntwo\nfemale)\nexpressing\nsix"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "(cid:18) QK T",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "√\nV\nAttention(Q, K, V ) = softmax",
          "55.02\n48.85\n47.58\n70.99\n71.06": "emotions: anger, disgust,\nfear, happiness,\nsadness, and sur-"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "dk",
          "55.02\n48.85\n47.58\n70.99\n71.06": ""
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "",
          "55.02\n48.85\n47.58\n70.99\n71.06": "prise. Recorded at 48 kHz in an anechoic chamber, the dataset"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "where Q and K are\nthe\nquery\nand\nkey matrices\nderived",
          "55.02\n48.85\n47.58\n70.99\n71.06": "includes both predefined and spontaneous vocalizations."
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "from the final concatenated features. V represents the feature",
          "55.02\n48.85\n47.58\n70.99\n71.06": "VIVAE\n[21]:\nIt\nincludes\n1,085\naudio\nfiles\nfrom eleven"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "vectors\nthat\nare\nattended\nto. Here, we\nare\nusing multiple",
          "55.02\n48.85\n47.58\n70.99\n71.06": "speakers\nexpressing\nthree\npositive\n(achievement,\npleasure,"
        },
        {
          "61.43\n52.89\n53.57\nUNI+WAV2": "attention heads, and the number of heads is 8. The number",
          "55.02\n48.85\n47.58\n70.99\n71.06": "surprise) and three negative emotions\n(anger,\nfear, pain) at"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "that MFMs capture complex emotional nuances due to their"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "multimodal pre-training that may be ambiguous\nto AFMs."
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "The\nt-SNE plots of\nthe\nraw representations\nfrom the FMs"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "are\nshown\nin Figure\n2. We\nobserve\nbetter\nclusters\nacross"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "emotions for MFMs in comparison to the AFMs."
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "When\ncombining\nthe\nFMs\nthrough MATA, we\nobtain"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "the\ntopmost\nperformance\nagainst\nall\nthe\nindividual\nFMs"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "and\nthe\nbaseline\nconcatenation-based\nfusion\ntechnique.\nIn"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "concatenation-based\nfusion, we\nuse\nthe\nsame\narchitectural"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "components as MATA. This\nshows\nthe observable comple-"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "mentary behavior of\nthe MFMs as well as the effectiveness"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "of MATA in performing effective fusion of the MFMs. With"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "MATA, we also observe that\nthe fusion of MFMs and AFMs"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "gives\ncomparatively\nbetter\nresults\nthan\nindividual FMs\nas"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "well\nas\nthe baseline\nconcatenation-based fusion technique."
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "The\nconfusion matrices of MATA, with Unispeech-SAT +"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "WavLM and LanguagebIND +\nImageBind\nare\nshown\nin"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "Figure\n3. We\nalso\nprovide\nan\nablation\nstudy\nof MATA"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "without\nthe MHA block\n(Table\nI:\nFusion with OT); we"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "observe better\nresults\nthan the individual FMs, comparative"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "results, and sometimes better performance with some pairs"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "of FMs."
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "Additional Experiments: To show the generalizability of"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "the proposed framework, MATA, we also experimented on"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "a\nbenchmark\nspeech\nemotion\nrecognition\n(SER)\ndataset,"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "CREMA-D [22]. It consists of 7,442 clips from 91 actors (48"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "male, 43 female) expressing six basic emotions: happiness,"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "sadness,\nanger,\nfear,\ndisgust,\nand\nneutral. Rated\nby\n2,443"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "participants across audio-only, visual-only, and audio-visual"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "modalities. Due to the diversity of\nthe speakers, CREMA-"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "D serve\nas\nessential\nbenchmark\nfor\nemotion\nrecognition"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "systems. From Table I, we observe that MFMs show better"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "performance than AFMs. However, we achieve the topmost"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "performance with MATA with\nthe\ncombination\nof\nLan-"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "guageBind and ImageBind representations,\nthus showing the"
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": "effectiveness of\nthe proposed framework."
        },
        {
          "better than the AFMs for NVER, thus proving our hypothesis": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "nonverbal expressions,” IEEE Access, 2024."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[9] A. Arunkumar, V. N.\nSukhadia,\nand\nS. Umesh,\n“Investigation\nof"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "ensemble features of self-supervised pretrained models for automatic"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "speech recognition,” ArXiv, vol. abs/2206.05518, 2022."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[10] O. C. Phukan, G. S. Kashyap, A. B. Buduru, and R. Sharma, “Het-"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "erogeneity over homogeneity:\nInvestigating multilingual\nspeech pre-"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "trained models for detecting audio deepfake,” in NAACL-HLT, 2024."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[11] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "and I. Misra, “Imagebind: One embedding space to bind them all,” in"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "CVPR, 2023."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[12] B. Zhu, B. Lin, M. Ning, Y. Yan,\nJ. Cui, W. HongFa, Y. Pang,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "W. Jiang, J. Zhang, Z. Li, C. W. Zhang, Z. Li, W. Liu, and L. Yuan,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "“Languagebind: Extending video-language pretraining to n-modality"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "by language-based semantic alignment,” 2023."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[13]\nS.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. Lai, K. Lakhotia, Y. Y. Lin,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T. hsien Huang, W.-C. Tseng,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "A.\nrahman Mohamed,\nand H.\nyi Lee,\n“Superb: Speech\nprocessing"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "universal performance benchmark,” in Interspeech, 2021."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[14]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-supervised pre-"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "training for\nfull\nstack speech processing,” IEEE Journal of Selected"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[15]\nS. Chen, Y. Wu, C. Wang,\nZ. Chen,\nZ. Chen,\nS.\nLiu,\nJ. Wu,"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "Y\n. Qian, F. Wei,\nJ. Li, and X. Yu, “Unispeech-sat: Universal\nspeech"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "representation learning with speaker aware pre-training,” ICASSP 2022"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "- 2022 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "Processing (ICASSP), pp. 6152–6156, 2021."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[16] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "A framework for\nself-supervised learning of\nspeech representations,”"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "Advances\nin\nneural\ninformation\nprocessing\nsystems,\nvol.\n33,\npp."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "12 449–12 460, 2020."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[17] O. C. Phukan, A. B. Buduru,\nand R. Sharma,\n“Transforming\nthe"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "embeddings: A lightweight\ntechnique for speech emotion recognition"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "tasks,” in Interspeech, 2023."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[18]\nS. Pramanick, A. B. Roy, and V. M. Patel, “Multimodal\nlearning using"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "optimal\ntransport\nfor sarcasm and humor detection,” 2022 IEEE/CVF"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "Winter Conference on Applications of Computer Vision (WACV), pp."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "546–556, 2021."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[19] D. Landry, Q. He, H. Yan, and Y. Li, “Asvp-esd: A dataset and its"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "benchmark for emotion recognition using both speech and non-speech"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "utterances,” Global Scientific Journals, vol. 8, pp. 1793–1798, 2020."
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "[20] D. Xin, S. Takamichi, and H. Saruwatari, “Jnv corpus: A corpus of"
        },
        {
          "“Jvnv: A corpus of japanese emotional speech with verbal content and": "japanese nonverbal vocalizations with diverse phrases and emotions,”"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "vocalizations of affect and emotion (vivae) corpus prompts new per-"
        },
        {
          "REFERENCES": "[1] K. V. K. Kishore and P. K. Satish, “Emotion recognition in speech us-",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "spective on nonspeech perception.” Emotion, vol. 22, no. 1, p. 213,"
        },
        {
          "REFERENCES": "ing mfcc and wavelet features,” 2013 3rd IEEE International Advance",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "2022."
        },
        {
          "REFERENCES": "Computing Conference (IACC), pp. 842–847, 2013.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "[22] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "REFERENCES": "[2]\nL.-W. Chen and A.\nI. Rudnicky, “Exploring wav2vec 2.0 fine tuning",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal actors"
        },
        {
          "REFERENCES": "for improved speech emotion recognition,” ICASSP 2023 - 2023 IEEE",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "dataset,” IEEE transactions on affective computing, vol. 5, no. 4, pp."
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": "377–390, 2014."
        },
        {
          "REFERENCES": "(ICASSP), pp. 1–5, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[3] D. Diatlova, A. Udalov, V. Shutov, and E. Spirin, “Adapting wavlm",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "for speech emotion recognition,” ArXiv, vol. abs/2405.04485, 2024.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[4]\nL. Pepino, P. E. Riera,\nand L. Ferrer,\n“Emotion\nrecognition\nfrom",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "speech using wav2vec 2.0 embeddings,” ArXiv, vol. abs/2104.03502,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "pp. 3400–3404, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[5]\nE.\nda\nSilva Morais, R. Hoory, W. Zhu,\nI. Gat, M. Damasceno,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "and H. Aronowitz, “Speech emotion recognition using self-supervised",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "ICASSP 2022\n-\n2022\nIEEE International Conference\non\nfeatures,”",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP), pp. 6922–6926,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "2022.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[6]\nJ.-H. Hsu, M.-H. Su, C.-H. Wu,\nand Y.-H. Chen,\n“Speech emotion",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "recognition\nconsidering\nnonverbal\nvocalization\nin\naffective\nconver-",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "IEEE/ACM Transactions on Audio, Speech, and Language\nsations,”",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Processing, vol. 29, pp. 1675–1686, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[7]\nP. Tzirakis, A. Baird, J. A. Brooks, C. Gagne, L. Kim, M. Opara, C. B.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Gregory, J. Metrick, G. Boseck, V. R. Tiruvadi, B. Schuller, D. Keltner,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "and A. S. Cowen, “Large-scale nonverbal vocalization detection using",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "transformers,” ICASSP 2023 - 2023 IEEE International Conference on",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5, 2023.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[8] D. Xin, J. Jiang, S. Takamichi, Y. Saito, A. Aizawa, and H. Saruwatari,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "“Jvnv: A corpus of japanese emotional speech with verbal content and",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "nonverbal expressions,” IEEE Access, 2024.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[9] A. Arunkumar, V. N.\nSukhadia,\nand\nS. Umesh,\n“Investigation\nof",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "ensemble features of self-supervised pretrained models for automatic",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "speech recognition,” ArXiv, vol. abs/2206.05518, 2022.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[10] O. C. Phukan, G. S. Kashyap, A. B. Buduru, and R. Sharma, “Het-",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "erogeneity over homogeneity:\nInvestigating multilingual\nspeech pre-",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "trained models for detecting audio deepfake,” in NAACL-HLT, 2024.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[11] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "and I. Misra, “Imagebind: One embedding space to bind them all,” in",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "CVPR, 2023.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[12] B. Zhu, B. Lin, M. Ning, Y. Yan,\nJ. Cui, W. HongFa, Y. Pang,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "W. Jiang, J. Zhang, Z. Li, C. W. Zhang, Z. Li, W. Liu, and L. Yuan,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "“Languagebind: Extending video-language pretraining to n-modality",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "by language-based semantic alignment,” 2023.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[13]\nS.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. Lai, K. Lakhotia, Y. Y. Lin,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T. hsien Huang, W.-C. Tseng,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "A.\nrahman Mohamed,\nand H.\nyi Lee,\n“Superb: Speech\nprocessing",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "universal performance benchmark,” in Interspeech, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[14]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-supervised pre-",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "training for\nfull\nstack speech processing,” IEEE Journal of Selected",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[15]\nS. Chen, Y. Wu, C. Wang,\nZ. Chen,\nZ. Chen,\nS.\nLiu,\nJ. Wu,",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Y\n. Qian, F. Wei,\nJ. Li, and X. Yu, “Unispeech-sat: Universal\nspeech",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "representation learning with speaker aware pre-training,” ICASSP 2022",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "- 2022 IEEE International Conference on Acoustics, Speech and Signal",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Processing (ICASSP), pp. 6152–6156, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[16] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "A framework for\nself-supervised learning of\nspeech representations,”",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Advances\nin\nneural\ninformation\nprocessing\nsystems,\nvol.\n33,\npp.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "12 449–12 460, 2020.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[17] O. C. Phukan, A. B. Buduru,\nand R. Sharma,\n“Transforming\nthe",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "embeddings: A lightweight\ntechnique for speech emotion recognition",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "tasks,” in Interspeech, 2023.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[18]\nS. Pramanick, A. B. Roy, and V. M. Patel, “Multimodal\nlearning using",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "optimal\ntransport\nfor sarcasm and humor detection,” 2022 IEEE/CVF",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Winter Conference on Applications of Computer Vision (WACV), pp.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "546–556, 2021.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[19] D. Landry, Q. He, H. Yan, and Y. Li, “Asvp-esd: A dataset and its",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "benchmark for emotion recognition using both speech and non-speech",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "utterances,” Global Scientific Journals, vol. 8, pp. 1793–1798, 2020.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "[20] D. Xin, S. Takamichi, and H. Saruwatari, “Jnv corpus: A corpus of",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "japanese nonverbal vocalizations with diverse phrases and emotions,”",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        },
        {
          "REFERENCES": "Speech Commun., vol. 156, p. 103004, 2023.",
          "[21] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The variably intense": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in speech using mfcc and wavelet features",
      "authors": [
        "K Kishore",
        "P Satish"
      ],
      "year": "2013",
      "venue": "2013 3rd IEEE International Advance Computing Conference (IACC)"
    },
    {
      "citation_id": "2",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Adapting wavlm for speech emotion recognition",
      "authors": [
        "D Diatlova",
        "A Udalov",
        "V Shutov",
        "E Spirin"
      ],
      "year": "2024",
      "venue": "ArXiv"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Da Silva Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition considering nonverbal vocalization in affective conversations",
      "authors": [
        "J.-H Hsu",
        "M.-H Su",
        "C.-H Wu",
        "Y.-H Chen"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Large-scale nonverbal vocalization detection using transformers",
      "authors": [
        "P Tzirakis",
        "A Baird",
        "J Brooks",
        "C Gagne",
        "L Kim",
        "M Opara",
        "C Gregory",
        "J Metrick",
        "G Boseck",
        "V Tiruvadi",
        "B Schuller",
        "D Keltner",
        "A Cowen"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Jvnv: A corpus of japanese emotional speech with verbal content and nonverbal expressions",
      "authors": [
        "D Xin",
        "J Jiang",
        "S Takamichi",
        "Y Saito",
        "A Aizawa",
        "H Saruwatari"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Investigation of ensemble features of self-supervised pretrained models for automatic speech recognition",
      "authors": [
        "A Arunkumar",
        "V Sukhadia",
        "S Umesh"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "10",
      "title": "Heterogeneity over homogeneity: Investigating multilingual speech pretrained models for detecting audio deepfake",
      "authors": [
        "O Phukan",
        "G Kashyap",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2024",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "11",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar",
        "A El-Nouby",
        "Z Liu",
        "M Singh",
        "K Alwala",
        "A Joulin",
        "I Misra"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment",
      "authors": [
        "B Zhu",
        "B Lin",
        "M Ning",
        "Y Yan",
        "J Cui",
        "W Hongfa",
        "Y Pang",
        "W Jiang",
        "J Zhang",
        "Z Li",
        "C Zhang",
        "Z Li",
        "W Liu",
        "L Yuan"
      ],
      "year": "2023",
      "venue": "Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment"
    },
    {
      "citation_id": "13",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Rahman Mohamed",
        "H Yi Lee"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "14",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Unispeech-sat: Universal speech representation learning with speaker aware pre-training",
      "authors": [
        "S Chen",
        "Y Wu",
        "C Wang",
        "Z Chen",
        "Z Chen",
        "S Liu",
        "J Wu",
        "Y Qian",
        "F Wei",
        "J Li",
        "X Yu"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "O Phukan",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks"
    },
    {
      "citation_id": "18",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Patel"
      ],
      "year": "2021",
      "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "19",
      "title": "Asvp-esd: A dataset and its benchmark for emotion recognition using both speech and non-speech utterances",
      "authors": [
        "D Landry",
        "Q He",
        "H Yan",
        "Y Li"
      ],
      "year": "2020",
      "venue": "Global Scientific Journals"
    },
    {
      "citation_id": "20",
      "title": "Jnv corpus: A corpus of japanese nonverbal vocalizations with diverse phrases and emotions",
      "authors": [
        "D Xin",
        "S Takamichi",
        "H Saruwatari"
      ],
      "year": "2023",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "21",
      "title": "The variably intense vocalizations of affect and emotion (vivae) corpus prompts new perspective on nonspeech perception",
      "authors": [
        "N Holz",
        "P Larrouy-Maestri",
        "D Poeppel"
      ],
      "year": "2022",
      "venue": "Emotion"
    },
    {
      "citation_id": "22",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    }
  ]
}