{
  "paper_id": "2205.02455v1",
  "title": "Cogmen: Contextualized Gnn Based Multimodal Emotion Recognition",
  "published": "2022-05-05T05:54:24Z",
  "authors": [
    "Abhinav Joshi",
    "Ashwani Bhat",
    "Ayush Jain",
    "Atin Vikram Singh",
    "Ashutosh Modi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-theart (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are intrinsic to humans and guide their behavior and are indicative of the underlying thought process  (Minsky, 2007) . Consequently, understanding and recognizing emotions is vital for developing AI technologies (e.g., personal digital assistants) that interact directly with humans. During a conversation between a number of people, there is a constant ebb and flow of emotions experienced and expressed by each person. The task of multimodal emotion recognition addresses the problem of monitoring the emotions expressed (via various modalities, e.g., video (face), audio (speech)) by individuals in different settings such as conversations.\n\nEmotions are physiological, behavioral, and communicative reactions to cognitively processed stimuli  (Planalp et al., 2018) . Emotions are often a result of internal physiological changes, and these physiological reactions may not be noticeable by others and are therefore intra-personal. For example, in a conversational setting, an emotion may be a communicative reaction that has its origin in a sentence spoken by another person, acting as a stimulus. The emotional states expressed in utterances correlate with the context directly; for example, if the underlying context is about a happy topic like celebrating a festival or description of a vacation, there will be more positive emotions like joy and surprise. Consider the example shown in Figure  1 , where the context depicts an exciting conversation. Speaker-1 being excited about his admission affects the flow of emotions in the entire context. The emotion states of Speaker-2 show the dependency on Speaker-1 in U 2 , U 4 and U 6 , and maintains intra-personal state depicted in U 8 and U 10 by being curious about the responses of Speaker-1. The example conversation portrays the effect of global information as well as inter and intra dependency of speakers on the emotional states of the utterances. Moreover, emotions are a multimodal phenomenon; a person takes cues from different modalities (e.g., audio, video) to infer the emotions of others, since, very often, the in-formation in different modalities complement each other. In this paper, we leverage these intuitions and propose COGMEN: COntextualized Graph neural network based Multimodal Emotion recog-nitioN architecture that addresses both, the effects of context on the utterances and inter and intra dependency for predicting the per-utterance emotion of each speaker during the conversation. There has been a lot of work on unimodal (using text only) prediction, but our focus is on multimodal emotion prediction. As is done in literature on multimodal emotion prediction, we do not focus on comparison with unimodal models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is an actively researched problem in NLP  (Sharma and Dhall, 2021; Sebe et al., 2005) . The broad applications ranging from emotion understanding systems, opinion mining from a corpus to emotion generation have attracted active research interest in recent years  (Dhuheir et al., 2021; Franzen et al., 2021; Vinola and Vimaladevi, 2015; Kołakowska et al., 2014; Colombo et al., 2019; Janghorbani et al., 2019; Goswamy et al., 2020; Singh et al., 2021a; Agarwal et al., 2021; Singh et al., 2021b) . Availability of bench-mark multimodal datasets, such as CMU-MOSEI  (Zadeh et al., 2018b) , and IEMOCAP  (Busso et al., 2008) , have accelerated the progress in the area. Broadly speaking, most of the existing work in this area can be categorized mainly into two areas: unimodal approaches and multimodal approaches.\n\nUnimodal approaches tend to consider the text as a prominent mode of communication and solve the emotion recognition task using only text modality. In contrast, multimodal approaches are more naturalistic and consider multiple modalities (au-dio+video+text) and fuse them to recognize emotions. In this paper, we propose a multimodal approach to emotion recognition. Nevertheless, we briefly outline some of the prominent unimodal approaches as some of the techniques are applicable to our setting.\n\nUnimodal Approaches: COSMIC  (Yu et al., 2019 ) performs text only emotion classification problem by leveraging commonsense knowledge. DialogXL  (Shen et al., 2021a)  uses XLnet  (Yang et al., 2019)  as architecture in dialogue feature extraction. CESTa  (Wang et al., 2020)  captures the emotional consistency in the utterances using Conditional Random Fields  (Lafferty et al., 2001)  for boosting the performance of emotion classification. Other popular approaches parallel to our work use graph-based neural networks as their baseline and solve the context propagation issues in RNN-based architectures, including DialogueGCN  (Ghosal et al., 2019) , RGAT  (Ishiwatari et al., 2020) , ConGCN  (Zhang et al., 2019) , and SumAgg-Gin  (Sheng et al., 2020) . Some of the recent approaches like DAG-ERC  (Shen et al., 2021b)  combine the strengths of conventional graph-based neural models and recurrence-based neural models.\n\nMultimodal Approaches: Due to the high correlation between emotion and facial cues  (Ekman, 1993) , fusing modalities to improve emotion recognition has drawn considerable interest  (Sebe et al., 2005) . Some of the initial approaches include  Datcu and Rothkrantz (2014) , who fused acoustic information with visual cues for emotion recognition.  Wollmer et al. (2010)  use contextual information for emotion recognition in a multimodal setting. In the past decade, the growth of deep learning has motivated a wide range of approaches in multimodal settings. The Memory Fusion network (MFN)  (Zadeh et al., 2018a)  proposes synchronizing multimodal sequences using multi-view gated memory storing intra-view and cross-view interac-tions through time.  Graph-MFN (Bagher Zadeh et al., 2018)  extends the idea of MFN and introduces Dynamic Fusion Graph (DFG), which learns to model the n-modal interactions and alter its structure dynamically to choose a fusion graph based on the importance of each n-modal dynamics during inference. Conversational memory network (CMN)  (Hazarika et al., 2018b ) leverages contextual information from the conversation history and uses gated recurrent units to model past utterances of each speaker into memories. Tensor fusion Network (TFN)  (Zadeh et al., 2017)  uses an outer product of the modalities. Other popular approaches include DialogueRNN  (Majumder et al., 2019)  that proposes an attention mechanism over the different utterances and models emotional dynamics by its party GRU and global GRU. B2+B4  (Kumar and Vepa, 2020) , use a conditional gating mechanism to learn cross-modal information. bc-LSTM  (Poria et al., 2017)  proposes an LSTM-based model that captures contextual information from the surrounding utterances. Multilogue-Net  (Shenoy and Sardana, 2020)  proposes a solution based on a context-aware RNN and uses pairwise attention as a fusion mechanism for all three modalities (audio, video, and text). Recently,  Delbrouck et al. (2020)  proposed TBJE, a transformer-based architecture with modular co-attention  (Yu et al., 2019)  to encode multiple modalities jointly. CONSK-GCN  (Fu et al., 2021)  uses graph convolutional network (GCN) with knowledge graphs.  Lian et al. (2020)  use GNN based architecture for Emotion Recognition using text and speech modalities. Af-CAN  (Wang et al., 2021a)  proposes RNN based on contextual attention for modeling the transaction and dependence between speakers.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Model",
      "text": "In a conversation involving different speakers, there is a continuous ebb and flow in the emotions of each of the speakers, usually triggered by the context and reactions of other speakers. Inspired by this intuition, we propose a multimodal emotion prediction model that leverages contextual information, inter-speaker and intra-speaker relations in a conversation.\n\nIn our model, we leverage both the context of dialogue and the effect of nearby utterances. We model these two sources of information via two means: 1) Global Information: How to capture the impact of underlying context on the emotional state of an utterance? 2) Local information: How to establish relations between the nearby utterances that preserve both inter-speaker and intra-speaker dependence on utterances in a dialogue? Global Information: We want to have a unified model that can capture the underlying context and handle its effect on each utterance present in the dialogue. A transformer encoder  (Vaswani et al., 2017)  architecture is a suitable choice for this goal. Instead of following the conventional sequential encoding by adding positional encodings to the input, in our approach, a simple transformer encoder without any positional encodings leverages the entire context to generate distributed representations (features) efficiently corresponding to each utterance. The transformer facilitates the flow of information from all utterances when predicting emotion for a particular utterance.\n\nLocal Information: The emotion expressed in an utterance is often triggered by the information in neighboring utterances. We establish relations between the nearby utterances in a way that is capable of capturing both inter-speaker and intra-speaker effects of stimulus over the emotion state of an utterance. Our approach comes close to DialogueGCN  (Ghosal et al., 2019) , and we define a graph where each utterance is a node, and directed edges represent various relations. We define relations (directed edges) between nodes R ij = u i → u j , where the direction of the arrow represents the spoken order of utterances. We categorize the directed relations into two types, for self-dependent relations between the utterances spoken by the same speaker R intra , and interrelations between the utterances spoken by different speakers R inter . We propose to use Relational GCN  (Schlichtkrull et al., 2018)  followed by a GraphTransformer  (Shi et al., 2021)  to capture dependency defined by the relations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Overall Architecture",
      "text": "Figure  2  shows the detailed architecture. The input utterances go as input to the Context Extractor module, which is responsible for capturing the global context. The features extracted for each utterance by the context extractor form a graph based on interactions between the speakers. The graph goes as input to a Relational GCN, followed by GraphTransformer, which uses the formed graph to capture the inter and intra-relations between the utterances. Finally, two linear layers acting as an emotion classifier use the features obtained for all  the utterances to predict the corresponding emotions.\n\nContext Extractor: Context Extractor takes concatenated features of multiple modalities (audio, video, text) as input for each dialogue utterance (u i ; i = 1, . . . , n) and captures the context using a transformer encoder. The feature vector for an utterance u i with the input features corresponding to available modalities, audio (u\n\nThe combined features matrix for all utterances in a dialogue is given by:\n\nWe define a Query, a Key, and a Value vector for encoding the input features X ∈ R n×d as follows:\n\nThe attention mechanism captures the interaction between the Key and Query vectors to output an attention map α (h) , where σ j denotes the softmax function over the row vectors indexed by j:\n\nwhere α (h) ∈ R n×n represents the attention weights for a single attention head (h). The obtained attention map is used to compute a weighted sum of the values for each utterance:\n\nwhere, W o ∈ R kH×d and H represents the total number of heads in multi-head attention. Note U ∈ R n×d . We add residual connection X and apply LayerNorm, followed by a feed forward and Add & Norm layer:\n\nwhere,\n\nGraph Formation: A graph captures inter and intra-speaker dependency between utterances. Every utterance acts as a node of a graph that is connected using directed relations (past and future relations). We define relation types as speaker to speaker. Formally, consider a conversation between M speakers defined as a dialogue D = {U S 1 , U S 2 , . . . , U S M }, where U S 1 = {u\n\nn } represent the set of utterances spoken by speaker-1. We define intra relations between the utterances spoken by the same speaker, R intra ∈ {U S i → U S i }, and inter relations between the utterances spoken by different speakers, R inter ∈ {U S i → U S j } i =j . We further consider a window size and use P and F as hyperparameters to form relations between the past P utterances and future F utterances for every utterance in a dialogue. For instance, R intra and R inter for utterance u (S 1 ) i (spoken by speaker-1) are defined as:\n\nwhere ← and → represent the past and future relation type respectively (example in Appendix F). Relational Graph Convolutional Network (RGCN): The vanilla RGCN  (Schlichtkrull et al., 2018)  helps accumulate relation-specific transformations of neighboring nodes depending on the type and direction of edges present in the graph through a normalized sum. In our case, it captures the inter-speaker and intra-speaker dependency on the connected utterances.\n\nwhere N r (i) denotes the set of neighbor indices of node i under relation r ∈ R, Θ root and Θ r denote the learnable parameters of RGCN, |N r (i)| is the normalization constant and z j is the utterance level feature coming from the transformer. GraphTransformer: For extracting rich representation from the node features, we use a GraphTransformer  (Shi et al., 2021) . GraphTransformer adopts the vanilla multi-head attention into graph learning by taking into account nodes connected via edges.\n\nGiven node features H = x 1 , x 2 , . . . , x n obtained from RGCN,\n\nwhere the attention coefficients α i,j are computed via multi-head dot product attention: Emotion Classifier: A linear layer over the features extracted by GraphTransformer (h i ) predicts the emotion corresponding to the utterance.\n\nwhere ŷi is the emotion label predicted for the utterance u i .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We experiment for the Emotion Recognition task on the two widely used datasets: IEMOCAP  (Busso et al., 2008)  and MOSEI  (Zadeh et al., 2018b) . The dataset statistics are given in Table  1 . IEMOCAP is a dyadic multimodal emotion recognition dataset where each utterance in a dialogue is labeled with one of the six emotion categories: anger, excited, sadness, happiness, frustrated, and neutral. In literature, two IEMOCAP settings are used for testing, one with 4 emotions (anger, sadness, happiness, neutral) and one with 6 emotions.\n\nWe experiment with both of these settings. MOSEI is a multimodal emotion recognition dataset annotated with 7 sentiments (-3 (highly negative) to +3 (highly positive)) and 6 emotion labels (happiness, sadness, disgust, fear, surprise, and anger). Note that the emotion labels differ across the datasets. We use weighted F1-score and Accuracy as evaluation metrics (details in Appendix C).\n\nImplementation Details: For IEMOCAP, audio features (size 100) are extracted using OpenSmile  (Eyben et al., 2010) , video features (size 512) are taken from  Baltrusaitis et al. (2018) , and text features (size 768) are extracted using sBERT  (Reimers and Gurevych, 2019)  We fuse the features of all the available modalities (A(audio)+T(text)+V(video): ATV) via concatenation. We also explored other fusion mechanisms (Appendix G.1). However, concatenation gave the best performance. We conduct a hyper-parameter search for our proposed model using Bayesian optimization techniques (details in Appendix A).\n\nBaselines: We do a comprehensive evaluation of COGMEN by comparing it with a number of baseline models. For IEMOCAP, we compare our model with the existing multimodal frameworks (Table  2 ), which includes DialogueRNN  (Majumder et al., 2019) , bc-LSTM  (Poria et al., 2017) , CHFusion  (Majumder et al., 2018) , memnet  (Sukhbaatar et al., 2015) , TFN  (Zadeh et al., 2017) , MFN  (Zadeh et al., 2018a) , CMN  (Hazarika et al., 2018b) , ICON  (Hazarika et al., 2018a) , and Af-CAN  (Wang et al., 2021b) . For MOSEI, COGMEN is compared (    with number of utterances present in a dialogue (more details on effect of window size in Appendix G.2). This experiment helps understand the importance of context in a dialogue. Moreover, it points towards challenges in developing a real-time system (details in §6). We test the local information hypothesis by removing the GNN module and directly passing the context extracted features to the emotion classifier. Table  6  shows the drop in performance across modalities when the GNN component is removed from the architecture, making our local information hypothesis more concrete.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effect Of Relation Types:",
      "text": "We also test the effect of inter and intra-relations in the dialogue graph by making all relations of the same type and training the architecture. We observe a drop in performance (Table  6 ) when the relations are kept the same in the graph formation step. The explicit relation formation helps capture the local dependencies present in the dialogue.\n\nEffect of Modalities: The focus of this work is multimodal emotion recognition. However, just for the purpose of comparison, we also compare with unimodal (text only) approaches. We compare (Table  7 ) with EmoBERTa  (Kim and Vossen, 2021) , DAG-ERC  (Shen et al., 2021b) , CESTa  (Wang et al., 2020) , SumAggGIN  (Sheng et al., 2020) , DialogueCRN  (Hu et al., 2021) , DialogXL  (Shen et al., 2021a)  and DialogueGCN  (Ghosal et al., 2019) . Text-based models are specifically optimized for text modalities and incorporate changes to architectures to cater to text. It is not fair to compare with our multimodal approach from that perspective. As shown in results, COGMEN, being a fairly generic architecture, still gives better (for IEMOCAP (4-way)) or comparable performance with respect to the SOTA unimodal architectures.\n\nIn the case of our model, adding more information via other modalities helps to improve the performance. Results on different modality combinations are in Appendix D. Error Analysis: After analysing the predictions made across the datasets, we find that our model falls short in distinguishing between similar emotions, such as happiness vs excited and anger vs frustration (Figure  3 ). This issue also exists in previous methods as reported in  Shen et al. (2021b), and Ghosal et al. (2019) . We also find that our model misclassifies the other emotion labels as neutral because of a more significant proportion of neutral labeled examples. Moreover, we observe the accuracy of our model when classifying examples having emotion shift is 53.6% compared to 74.2% when the emotion remains the same (more details in Appendix B).\n\nEfficacy of the GNN Layer: For observing the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Before After",
      "text": "Happiness Sadness Neutral Anger effect of the GNN component in our architecture, we also visualize the features before and after the GNN component. Figure  4  clearly shows the better formation of emotion clusters depicting the importance of capturing local dependency in utterances for better performance in emotion recognition (more in Appendix E and Appendix Figure  -9 ). Importance of utterances: To verify the effect of utterances and their importance in a prediction for a dialogue, we infer the trained model on dialogues by masking one utterance at a time and calculating the F1-score for prediction. Figure  5  shows the obtained results for a dialogue (Appendix Table 10) instance taken randomly from IEMOCAP (4-way) (more in Appendix E). For the first 4 utterances, emotions state being neutral, the effect of masking the utterances is significantly less. In contrast, masking the utterances with emotion shift (9, 10, 11) completely drops the dialogue's F1-score, showing that our architecture captures the effects of emotions present in the utterances.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Comparison with Baselines: Emotion recognition in a multimodal conversation setting comes with two broadly portrayed research challenges  (Poria et al., 2019) , first, the ability of a model to capture global and local context present in the dialogues, and second, the ability to maintain self and interpersonal dependencies among the speakers. All the popular baselines like Dialogue-GCN  (Ghosal et al., 2019) , DialogueRNN  (Majumder et al., 2019) , bc-LSTM  (Poria et al., 2017)  Af-CAN  (Wang et al., 2021a) , etc., try to address these challenges by proposing various architectures. bc-LSTM (bi-directional contextual LSTM  (Poria et al., 2017) ) uses LSTM to capture the contextual information and maintain long relations between the utterances from the past and future. Another contemporary architecture Af-CAN  (Wang et al., 2021a)  utilizes recurrent neural networks based on contextual attention to model the interaction and dependence between speakers and uses bi-directional GRU units to capture the global features from past and future. We propose to address these issues using a unified architecture that captures the effect of context on utterances while maintaining the states for self and interpersonal dependencies. We make use of transformers for encoding the global context and make use of GraphTransformers to capture the self and interpersonal dependencies. Our way of forming relational graphs between the utterances comes close to DialogueGCN (unimodal architecture). We further use a shared Emotion classifier for predicting emotions from all the obtained utterance level features. Moreover, our unified architecture handles multiple modalities effectively and shows an increase in performance after adding information from other modalities. Limitations (Offline Setting): A noteworthy limitation of all the proposed Emotion Recognition approaches (including the current one) is that they use global context from past and future utterances to predict emotions. However, baseline systems compared in this paper are also offline systems.\n\nFor example, bc-LSTM (bi-directional contextual LSTM) and Af-CAN use utterances from the past and future to predict emotions. Other popular baselines like DialogueGCN and DialogueRNN (BiDi-alogueRNN) also peek into the future, assuming the presence of all the utterances during inference (offline setting). All such systems that depend on future information can only be used in an offline setting to process and tag the dialogue. An Emotion Recognition system that could work in an online setting exhibits another line of future work worth exploring due to its vast use cases in live telecasting and telecommunication. A possible approach to maintain the context in an online setting would be to take a buffer of smaller context size, where the model can predict emotions taking not the complete dialogue but a smaller subset of it as input in real-time. We tried exploring this setting for our architecture with an online buffer of maintaining a smaller context window. For experimenting with it, we created a sub-dataset using the IEMOCAP (4-way) setting by splitting each dialogue into n utterances and training our architecture. Our results in Table  5  show the decrease in performance with the number of utterances present in a dialogue depicting the importance of context in a conversation. Performance improvements in these settings where the system can work in real-time are worth exploring and are an interesting direction for future research.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "We present a novel approach of using GNNs for multimodal emotion recognition and propose COGMEN: COntextualized GNN based Multimodal Emotion recognitioN. We test COGMEN on two widely known multimodal emotion recognition datasets, IEMOCAP and MOSEI. COGMEN outperforms the existing state-of-the-art methods in multimodal emotion recognition by a significant margin (i.e., 7.7% F1-score increase for IEMO-CAP (4-way)). By comprehensive analysis and ablation studies over COGMEN, we show the importance of different modules. COGMEN fuses information effectively from multiple modalities to improve the performance of emotion prediction tasks. We perform a detailed error analysis and observe that the misclassifications are mainly between the similar classes and emotion shift cases. We plan to address this in future work, where the focus will be to incorporate a component for capturing the emotional shifts for fine-grained emotion prediction. We use PyTorch  (Paszke et al., 2019)  for training our architecture and PyG (PyTorch Geometric)  (Fey and Lenssen, 2019)  for the GNN component in our architecture. We use comet  (Com, 2021)  for logging all our experiments and its Bayesian optimizer for hyperparameter tuning. Our architecture trained on the IEMOCAP dataset has 55,932,052 parameters and takes around 7 minutes to train for 50 epochs on the NVIDIA Tesla K80 GPU. Comparison of the model with baselines in terms of the number of parameters is challenging, as the baselines parameters vary depending on the hyperparameter setting. Moreover, many baselines do not provide information about the number of parameters.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Dataset Analysis",
      "text": "We study IEMOCAP dataset in detail for error analysis of our model. We observe the emotion transition at Utterance level (Figure  6 ) and Speaker level (Figure  7 ). We find a high percentage of transitions between similar emotions, causing the models to confuse between the similar classes of emotion. Considering the emotion transition between states that are opposite, like from happy to sad, we deduce the poor performance of emotion recognition architectures for such cases. We plan to address this issue in future work where we target a model which performs better in fine-grained emotion recognition and is robust towards the shifts in emotions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C Evaluation Metrics",
      "text": "Weighted F1 Score: The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\nFor weighted F1 score, we calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). Accuracy: It is defined as the percentage of correct predictions in the test set.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D Results On Modality Combinations",
      "text": "Table  11  shows results on the IEMOCAP dataset for all the modality combinations for our architectures. Figure  8  shows the confusion matrix for prediction on IEMOCAP 4-way dataset.    obtained results for a dialogue instance taken randomly from IEMOCAP 4-way. For the first 15 utterances, emotions state being sadness, the effect of masking the utterances is more negligible for the first 5 utterances. This drop depicts the",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "F Graph Formation",
      "text": "To give a clear picture of the graph formation procedure, we describe the process for utterances spoken in a dialogue. As an illustration, let's consider two speakers, S 1 and S 2 , present in a conversation of 7 utterances. Features corresponding to each utterance is shown as a node in Figure  11 . Speaker 1 speaks utterances u i-3 , u i-1 , u i+1 , u i+3 and Speaker 2 speaks u i-2 , u i , u i+2 . After creating the graphs with relations, the constructed graph would look like shown in Figure  11 , and the corresponding relations for each instance would be as shown in Table  12 . Since there are two speakers in the conversation (S N = 2), the total number of unique relations would be:\n\nTable  13  shows the number of possible unique relations for a conversation between two speakers.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "G Discussion",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "G.1 Modality Fusing Mechanisms",
      "text": "While experimenting with the model architecture, we explored various mechanisms for mixing information from multiple modalities. Some of the mechanisms include pairwise attention inspired from  Ghosal et al. (2018) , bimodal attention present in Multilogue-Net  (Shenoy and Sardana, 2020) , and crossAttention layer proposed in HKT  (Hasan et al., 2021) . However, in our case, none of these fusing mechanisms shows significant performance improvement over simple concatenation. Moreover, all these fusing mechanisms require extra computation steps for fusing information. In contrast, a simple concatenation of modality features works well with no additional computational overhead.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "G.2 Effect Of Window Size In Graph Formation",
      "text": "To explore the effect of window size in the Graph Formation module of our architecture, we conduct experiments with multiple window sizes. The obtained results are present in Table  14 . The window size can be treated as a hyperparameter that could be adjusted while training our architecture. Moreover, the freedom of setting the window size makes our architecture more flexible in terms of usage. A larger window size would result in better performance for cases where the inter and intra speaker dependencies are maintained for longer sequences.\n\nIn contrast, setting a lower window size would be better in a use case where the topic frequently changes in dialogues and speakers are less affected by another speaker. In the future, we plan to explore a dynamic and automatic selection of window size depending on the dialogue instance.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example conversation between two speak-",
      "page": 1
    },
    {
      "caption": "Figure 1: , where the context depicts an excit-",
      "page": 1
    },
    {
      "caption": "Figure 2: shows the detailed architecture. The in-",
      "page": 3
    },
    {
      "caption": "Figure 2: The proposed model (COGMEN) architecture.",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion Matrix for IEMOCAP (6-way)",
      "page": 8
    },
    {
      "caption": "Figure 3: ). This issue also exists in pre-",
      "page": 8
    },
    {
      "caption": "Figure 4: UMAP (Becht et al., 2019) representation of",
      "page": 8
    },
    {
      "caption": "Figure 4: clearly shows the better",
      "page": 8
    },
    {
      "caption": "Figure 5: Importance of utterances in IEMOCAP (4-",
      "page": 8
    },
    {
      "caption": "Figure 6: ) and Speaker level",
      "page": 14
    },
    {
      "caption": "Figure 7: ). We ﬁnd a high percentage of transitions",
      "page": 14
    },
    {
      "caption": "Figure 6: Utterance-level Emotion transition for IEMO-",
      "page": 14
    },
    {
      "caption": "Figure 7: Speaker-level Emotion transition for IEMO-",
      "page": 14
    },
    {
      "caption": "Figure 8: shows the confusion matrix for",
      "page": 14
    },
    {
      "caption": "Figure 5: Modalities",
      "page": 15
    },
    {
      "caption": "Figure 8: Confusion Matrix for IEMOCAP 4-Way clas-",
      "page": 15
    },
    {
      "caption": "Figure 9: ) explained in section 5.",
      "page": 15
    },
    {
      "caption": "Figure 10: shows the",
      "page": 15
    },
    {
      "caption": "Figure 9: UMAP (Becht et al., 2019) representation of",
      "page": 15
    },
    {
      "caption": "Figure 10: Importance of utterances in IEMOCAP clas-",
      "page": 15
    },
    {
      "caption": "Figure 11: Graph formation process in (COGMEN) architecture.",
      "page": 16
    },
    {
      "caption": "Figure 11: , where relations with past utterances are denoted by (←) and",
      "page": 16
    },
    {
      "caption": "Figure 11: Speaker 1 speaks utterances ui−3, ui−1, ui+1, ui+3",
      "page": 16
    },
    {
      "caption": "Figure 11: , and the corre-",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 2: ), which includes DialogueRNN emotionlabelwhereaseparatemodelistrainedfor",
      "data": [
        {
          "bc-LSTM": "memnet",
          "35.6": "33.0",
          "69.2": "69.3",
          "53.5": "55.0",
          "66.3": "66.1",
          "61.1": "62.3",
          "62.4": "63.0",
          "59.8": "59.9",
          "59.0": "59.5"
        },
        {
          "bc-LSTM": "TFN",
          "35.6": "33.7",
          "69.2": "68.6",
          "53.5": "55.1",
          "66.3": "64.2",
          "61.1": "62.4",
          "62.4": "61.2",
          "59.8": "58.8",
          "59.0": "58.5"
        },
        {
          "bc-LSTM": "MFN",
          "35.6": "34.1",
          "69.2": "70.5",
          "53.5": "52.1",
          "66.3": "66.8",
          "61.1": "62.1",
          "62.4": "62.5",
          "59.8": "60.1",
          "59.0": "59.9"
        },
        {
          "bc-LSTM": "CMN",
          "35.6": "32.6",
          "69.2": "72.9",
          "53.5": "56.2",
          "66.3": "64.6",
          "61.1": "67.9",
          "62.4": "63.1",
          "59.8": "61.9",
          "59.0": "61.4"
        },
        {
          "bc-LSTM": "ICON",
          "35.6": "32.8",
          "69.2": "74.4",
          "53.5": "60.6",
          "66.3": "68.2",
          "61.1": "68.4",
          "62.4": "66.2",
          "59.8": "64.0",
          "59.0": "63.5"
        },
        {
          "bc-LSTM": "DialogueRNN",
          "35.6": "32.8",
          "69.2": "78.0",
          "53.5": "59.1",
          "66.3": "63.3",
          "61.1": "73.6",
          "62.4": "59.4",
          "59.8": "63.3",
          "59.0": "62.8"
        },
        {
          "bc-LSTM": "CAN",
          "35.6": "31.8",
          "69.2": "71.9",
          "53.5": "60.4",
          "66.3": "66.7",
          "61.1": "68.5",
          "62.4": "66.1",
          "59.8": "63.2",
          "59.0": "62.4"
        },
        {
          "bc-LSTM": "Af-CAN",
          "35.6": "37.0",
          "69.2": "72.1",
          "53.5": "60.7",
          "66.3": "67.3",
          "61.1": "66.5",
          "62.4": "66.1",
          "59.8": "64.6",
          "59.0": "63.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Results on MOSEI dataset. For emotion classification, a weighted F1-score is used. For Sentiment",
      "data": [
        {
          "Model": "Multilogue-Net T + A + V",
          "2 Class": "82.88",
          "7 Class Happiness Sadness Angry Fear Disgust Surprise Happiness Sadness Angry Fear Disgust Surprise": "67.84"
        },
        {
          "Model": "T\nTBJE\nA + T\nT + A + V",
          "2 Class": "81.9\n82.4\n81.5",
          "7 Class Happiness Sadness Angry Fear Disgust Surprise Happiness Sadness Angry Fear Disgust Surprise": "-\n65.91\n-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Results on MOSEI dataset. For emotion classification, a weighted F1-score is used. For Sentiment",
      "data": [
        {
          "Model\nModality\nF1-score (%)": "4-way"
        },
        {
          "Model\nModality\nF1-score (%)": "DialogueGCN\nT\n71.58"
        },
        {
          "Model\nModality\nF1-score (%)": "DialogXL\nT\n73.02"
        },
        {
          "Model\nModality\nF1-score (%)": "DAG-ERC\nT\n78.08"
        },
        {
          "Model\nModality\nF1-score (%)": "81.55\nT\nCOGMEN\n84.50\nA+T+V"
        },
        {
          "Model\nModality\nF1-score (%)": "6-way"
        },
        {
          "Model\nModality\nF1-score (%)": "EmoBERTa"
        },
        {
          "Model\nModality\nF1-score (%)": "DAG-ERC"
        },
        {
          "Model\nModality\nF1-score (%)": "CESTa"
        },
        {
          "Model\nModality\nF1-score (%)": "SumAggGIN"
        },
        {
          "Model\nModality\nF1-score (%)": "DialogueCRN"
        },
        {
          "Model\nModality\nF1-score (%)": "DialogXL"
        },
        {
          "Model\nModality\nF1-score (%)": "DialogueGCN"
        },
        {
          "Model\nModality\nF1-score (%)": "COGMEN"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker": "M",
          "Utterance Text": "’Why does that bother you?’",
          "Emotion": "neutral"
        },
        {
          "Speaker": "F",
          "Utterance Text": "\"She’s been in New York three and a half years. Why all of the sudden?\"",
          "Emotion": "neutral"
        },
        {
          "Speaker": "M",
          "Utterance Text": "’Well maybe. Maybe she just wanted to see her again.’",
          "Emotion": "neutral"
        },
        {
          "Speaker": "F",
          "Utterance Text": "\"What did you mean? He lived next door to the girl all of his life,\nwhy wouldn’t he want to see her again? Don’t look at me like that,\nhe didn’t tell me any more than he told you.\"",
          "Emotion": "neutral"
        },
        {
          "Speaker": "M",
          "Utterance Text": "\"She’s not his girl. She knows she’s not.\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "F",
          "Utterance Text": "\"I want you to pretend like he’s coming back!\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "M",
          "Utterance Text": "\"Because if he’s not coming back, then I’ll kill myself.\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "F",
          "Utterance Text": "’Laugh. Laugh at me, but what happens the night that she goes to sleep in his bed,\nand his memorial breaks in pieces?\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "M",
          "Utterance Text": "’Only last week, another boy turned up in Detroit,\nbeen missing longer than Larry,\nyou read it yourself, ’",
          "Emotion": "angry"
        },
        {
          "Speaker": "F",
          "Utterance Text": "\"You’ve got to believe. You’ve got to–\"",
          "Emotion": "sad"
        },
        {
          "Speaker": "M",
          "Utterance Text": "\"What do you mean me above all? Look at you. You’re shaking!\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "F",
          "Utterance Text": "\"I can’t help it!\"",
          "Emotion": "angry"
        },
        {
          "Speaker": "M",
          "Utterance Text": "’What have I got to hide? What the hell is the matter with you, Kate?’",
          "Emotion": "angry"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modalities": "a",
          "IEMOCAP-4way\nF1 Score (%)": "63.58",
          "IEMOCAP-6way\nF1 Score (%)": "47.57"
        },
        {
          "Modalities": "t",
          "IEMOCAP-4way\nF1 Score (%)": "81.55",
          "IEMOCAP-6way\nF1 Score (%)": "66.00"
        },
        {
          "Modalities": "v",
          "IEMOCAP-4way\nF1 Score (%)": "43.85",
          "IEMOCAP-6way\nF1 Score (%)": "37.58"
        },
        {
          "Modalities": "at",
          "IEMOCAP-4way\nF1 Score (%)": "81.59",
          "IEMOCAP-6way\nF1 Score (%)": "65.42"
        },
        {
          "Modalities": "av",
          "IEMOCAP-4way\nF1 Score (%)": "64.48",
          "IEMOCAP-6way\nF1 Score (%)": "52.20"
        },
        {
          "Modalities": "tv",
          "IEMOCAP-4way\nF1 Score (%)": "81.52",
          "IEMOCAP-6way\nF1 Score (%)": "62.19"
        },
        {
          "Modalities": "atv",
          "IEMOCAP-4way\nF1 Score (%)": "84.50",
          "IEMOCAP-6way\nF1 Score (%)": "67.63"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 12: Since there are two speakers",
      "data": [
        {
          "Figure": "(a)",
          "Central Node": "u(S1)\ni−3",
          "Rintra": "u(S1)\ni−3 ← u(S1)\ni−3 , u(S1)\ni−3 → u(S1)\ni−1 , u(S1)\ni−3 → u(S1)\ni+1 , u(S1)\ni−3 → u(S1)",
          "Rinter": "u(S1)\n, u(S1)\ni−3 → u(S2)\ni−2 , u(S1)\ni−3 → u(S2)\ni−3 → u(S2)"
        },
        {
          "Figure": "(b)",
          "Central Node": "u(S2)\ni−2",
          "Rintra": "u(S2)\n, u(S2)\ni−2 ← u(S2)\ni−2 , u(S2)\ni−2 → u(S2)\ni−2 → u(S2)",
          "Rinter": "u(S2)\ni−2 ← u(S1)\ni−3 , u(S2)\ni−2 → u(S1)\ni−1 , u(S2)\ni−2 → u(S1)\ni+1 , u(S2)\ni−2 → u(S1)"
        },
        {
          "Figure": "(c)",
          "Central Node": "u(S1)\ni−1",
          "Rintra": "u(S1)\ni−1 ← u(S1)\ni−3 , u(S1)\ni−1 ← u(S1)\ni−1 , u(S1)\ni−1 → u(S1)\ni+1 , u(S1)\ni−1 → u(S1)",
          "Rinter": "u(S1)\n, u(S1)\ni−1 ← u(S2)\ni−2 , u(S1)\ni−1 → u(S2)\ni−1 → u(S2)"
        },
        {
          "Figure": "(d)",
          "Central Node": "u(S2)\ni",
          "Rintra": "u(S2)\n← u(S2)\n← u(S2)\n, u(S2)\n→ u(S2)\ni\ni−2 , u(S2)\ni\ni\ni+2",
          "Rinter": "u(S2)\n← u(S1)\n← u(S1)\n→ u(S1)\n→ u(S1)\ni\ni−3 , u(S2)\ni−1 , u(S2)\ni+1 , u(S2)\ni+3"
        },
        {
          "Figure": "(e)",
          "Central Node": "u(S1)\ni+1",
          "Rintra": "u(S1)\ni+1 ← u(S1)\ni−3 , u(S1)\ni+1 ← u(S1)\ni−1 , u(S1)\ni+1 ← u(S1)\ni+1 , u(S1)\ni+1 → u(S1)",
          "Rinter": "u(S1)\n, u(S1)\ni+1 ← u(S2)\ni−2 , u(S1)\ni+1 ← u(S2)\ni+1 → u(S2)"
        },
        {
          "Figure": "(f)",
          "Central Node": "u(S2)\ni+2",
          "Rintra": "u(S2)\n, u(S2)\ni+2 ← u(S2)\ni−2 , u(S2)\ni+2 ← u(S2)\ni+2 ← u(S2)",
          "Rinter": "u(S2)\ni+2 ← u(S1)\ni−3 , u(S2)\ni+2 ← u(S1)\ni−1 , u(S2)\ni+2 ← u(S1)\ni+1 , u(S2)\ni+2 → u(S1)"
        },
        {
          "Figure": "(g)",
          "Central Node": "u(S1)\ni+3",
          "Rintra": "u(S1)\ni+3 ← u(S1)\ni−3 , u(S1)\ni+3 ← u(S1)\ni−1 , u(S1)\ni+3 ← u(S1)\ni+1 , u(S1)\ni+3 ← u(S1)",
          "Rinter": ", u(S1)\nu(S1)\ni+3 ← u(S2)\ni−2 , u(S1)\ni+3 ← u(S2)\ni+3 ← u(S2)"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 12: Since there are two speakers",
      "data": [
        {
          "Relation Type": "1",
          "Node A": "u(S1)",
          "Node B": "u(S1)",
          "Relation Causality": "Past",
          "Relation": "u(S1) ← u(S1)"
        },
        {
          "Relation Type": "2",
          "Node A": "u(S1)",
          "Node B": "u(S2)",
          "Relation Causality": "Past",
          "Relation": "u(S1) ← u(S2)"
        },
        {
          "Relation Type": "3",
          "Node A": "u(S2)",
          "Node B": "u(S1)",
          "Relation Causality": "Past",
          "Relation": "u(S2) ← u(S1)"
        },
        {
          "Relation Type": "4",
          "Node A": "u(S2)",
          "Node B": "u(S2)",
          "Relation Causality": "Past",
          "Relation": "u(S2) ← u(S2)"
        },
        {
          "Relation Type": "5",
          "Node A": "u(S1)",
          "Node B": "u(S1)",
          "Relation Causality": "Future",
          "Relation": "u(S1) → u(S1)"
        },
        {
          "Relation Type": "6",
          "Node A": "u(S1)",
          "Node B": "u(S2)",
          "Relation Causality": "Future",
          "Relation": "u(S1) → u(S2)"
        },
        {
          "Relation Type": "7",
          "Node A": "u(S2)",
          "Node B": "u(S1)",
          "Relation Causality": "Future",
          "Relation": "u(S2) → u(S1)"
        },
        {
          "Relation Type": "8",
          "Node A": "u(S2)",
          "Node B": "u(S2)",
          "Relation Causality": "Future",
          "Relation": "u(S2) → u(S2)"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 14: Results for various window sizes for graph",
      "data": [
        {
          "Modalities": "atv",
          "Window Past": "1",
          "Window future": "1",
          "F1 Score (%)": "81.72"
        },
        {
          "Modalities": "atv",
          "Window Past": "2",
          "Window future": "2",
          "F1 Score (%)": "83.21"
        },
        {
          "Modalities": "atv",
          "Window Past": "4",
          "Window future": "4",
          "F1 Score (%)": "84.08"
        },
        {
          "Modalities": "atv",
          "Window Past": "5",
          "Window future": "5",
          "F1 Score (%)": "83.19"
        },
        {
          "Modalities": "atv",
          "Window Past": "6",
          "Window future": "6",
          "F1 Score (%)": "82.49"
        },
        {
          "Modalities": "atv",
          "Window Past": "7",
          "Window future": "7",
          "F1 Score (%)": "82.28"
        },
        {
          "Modalities": "atv",
          "Window Past": "9",
          "Window future": "9",
          "F1 Score (%)": "82.77"
        },
        {
          "Modalities": "atv",
          "Window Past": "10",
          "Window future": "10",
          "F1 Score (%)": "84.50"
        },
        {
          "Modalities": "atv",
          "Window Past": "11",
          "Window future": "11",
          "F1 Score (%)": "83.93"
        },
        {
          "Modalities": "atv",
          "Window Past": "15",
          "Window future": "15",
          "F1 Score (%)": "83.78"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "References 2021. Comet.ML home page",
      "venue": "References 2021. Comet.ML home page"
    },
    {
      "citation_id": "2",
      "title": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "authors": [
        "Harsh Agarwal",
        "Keshav Bansal",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "year": "2021",
      "venue": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts"
    },
    {
      "citation_id": "3",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "4",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)",
      "doi": "10.1109/FG.2018.00019"
    },
    {
      "citation_id": "5",
      "title": "Dimensionality reduction for visualizing single-cell data using umap",
      "authors": [
        "Etienne Becht",
        "Leland Mcinnes",
        "John Healy",
        "Charles-Antoine Dutertre",
        "Immanuel Kwok",
        "Lai Ng",
        "Florent Ginhoux",
        "Evan Newell"
      ],
      "year": "2019",
      "venue": "Nature biotechnology"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Affect-driven dialog generation",
      "authors": [
        "Pierre Colombo",
        "Wojciech Witon",
        "Ashutosh Modi",
        "James Kennedy",
        "Mubbasir Kapadia"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1374"
    },
    {
      "citation_id": "8",
      "title": "Semantic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analysis approach",
      "authors": [
        "Dragos Datcu"
      ],
      "year": "2014",
      "venue": "Semantic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analysis approach"
    },
    {
      "citation_id": "9",
      "title": "A transformerbased joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.1"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition for healthcare surveillance systems using neural networks: A survey",
      "authors": [
        "Marwan Dhuheir",
        "Abdullatif Albaseer",
        "Emna Baccour",
        "Aiman Erbad",
        "Mohamed Abdallah",
        "Mounir Hamdi"
      ],
      "year": "2021",
      "venue": "Emotion recognition for healthcare surveillance systems using neural networks: A survey"
    },
    {
      "citation_id": "11",
      "title": "Facial expression and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "12",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "13",
      "title": "Fast graph representation learning with pytorch geometric",
      "authors": [
        "Matthias Fey",
        "Jan Eric Lenssen"
      ],
      "year": "2019",
      "venue": "Fast graph representation learning with pytorch geometric"
    },
    {
      "citation_id": "14",
      "title": "Developing emotion recognition for video conference software to support people with autism",
      "authors": [
        "Marc Franzen",
        "Stephan Michael",
        "Tobias Gresser",
        "Prof Müller",
        "Dr",
        "Sebastian Mauser"
      ],
      "year": "2021",
      "venue": "Developing emotion recognition for video conference software to support people with autism"
    },
    {
      "citation_id": "15",
      "title": "Consk-gcn: Conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Yahui Fu",
        "Shogo Okada",
        "Longbiao Wang",
        "Lili Guo",
        "Yaodong Song",
        "Jiaxing Liu",
        "Jianwu Dang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME51207.2021.9428438"
    },
    {
      "citation_id": "16",
      "title": "Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Chauhan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1382"
    },
    {
      "citation_id": "17",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "18",
      "title": "Adapting a language model for controlled affective text generation",
      "authors": [
        "Tushar Goswamy",
        "Ishika Singh",
        "Ahsan Barkati",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.251"
    },
    {
      "citation_id": "19",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "M Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "20",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "21",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "22",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP (1)"
    },
    {
      "citation_id": "23",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Domain authoring assistant for intelligent virtual agent. AAMAS '19",
      "authors": [
        "Sepehr Janghorbani",
        "Ashutosh Modi",
        "Jakob Buhmann",
        "Mubbasir Kapadia"
      ],
      "year": "2019",
      "venue": "International Foundation for Autonomous Agents and Multiagent Systems"
    },
    {
      "citation_id": "25",
      "title": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. arXiv e-prints",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. arXiv e-prints",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition and its applications",
      "authors": [
        "Agata Kołakowska",
        "Agnieszka Landowska",
        "Mariusz Szwoch",
        "Wioleta Szwoch",
        "Michal Wrobel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3"
    },
    {
      "citation_id": "27",
      "title": "Gated mechanism for attention based multi modal sentiment analysis",
      "authors": [
        "Ayush Kumar",
        "Jithendra Vepa"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Pereira"
      ],
      "year": "2001",
      "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01"
    },
    {
      "citation_id": "29",
      "title": "Conversational emotion recognition using self-attention mechanisms and graph neural networks",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang",
        "Zhanlei Yang",
        "Rongjun Li"
      ],
      "year": "2020",
      "venue": "In INTERSPEECH"
    },
    {
      "citation_id": "30",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems"
    },
    {
      "citation_id": "31",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python"
    },
    {
      "citation_id": "33",
      "title": "The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind",
      "authors": [
        "Marvin Minsky"
      ],
      "year": "2007",
      "venue": "The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind"
    },
    {
      "citation_id": "34",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Kopf",
        "Edward Yang",
        "Zachary Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "36",
      "title": "The Roles of Emotion in Relationships, 2 edition, Cambridge Handbooks in Psychology",
      "authors": [
        "Sally Planalp",
        "Julie Fitness",
        "Beverley Fehr"
      ],
      "year": "2018",
      "venue": "The Roles of Emotion in Relationships, 2 edition, Cambridge Handbooks in Psychology",
      "doi": "10.1017/9781316417867.021"
    },
    {
      "citation_id": "37",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "39",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1410"
    },
    {
      "citation_id": "40",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "Michael Schlichtkrull",
        "Thomas Kipf",
        "Peter Bloem",
        "Rianne Van Den",
        "Ivan Berg",
        "Max Titov",
        "Welling"
      ],
      "year": "2018",
      "venue": "The Semantic Web"
    },
    {
      "citation_id": "41",
      "title": "Multimodal approaches for emotion recognition: A survey",
      "authors": [
        "Nicu Sebe",
        "Ira Cohen",
        "Theo Gevers",
        "Thomas Huang"
      ],
      "year": "2005",
      "venue": "Proceedings of SPIE-IS and T Electronic Imaging -Internet Imaging VI ; Conference",
      "doi": "10.1117/12.600746"
    },
    {
      "citation_id": "42",
      "title": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
      "authors": [
        "Garima Sharma",
        "Abhinav Dhall"
      ],
      "year": "2021",
      "venue": "A Survey on Automatic Multimodal Emotion Recognition in the Wild",
      "doi": "10.1007/978-3-030-51870-7_3"
    },
    {
      "citation_id": "43",
      "title": "2021a. Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "44",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "45",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "46",
      "title": "Multiloguenet: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "47",
      "title": "Masked label prediction: Unified massage passing model for semi-supervised classification",
      "authors": [
        "Yunsheng Shi",
        "Zhengjie Huang",
        "Wenjin Wang",
        "Hui Zhong",
        "Shikun Feng",
        "Yu Sun"
      ],
      "year": "2021",
      "venue": "IJCAI"
    },
    {
      "citation_id": "48",
      "title": "2021a. An end-to-end network for emotion-cause pair extraction",
      "authors": [
        "Aaditya Singh",
        "Shreeshail Hingane",
        "Saim Wani",
        "Ashutosh Modi"
      ],
      "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "49",
      "title": "2021b. Fine-Grained Emotion Prediction by Modeling Emotion Definitions",
      "authors": [
        "Gargi Singh",
        "Dhanajit Brahma",
        "Piyush Rai",
        "Ashutosh Modi"
      ],
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "50",
      "title": "End-to-end memory networks",
      "authors": [
        "Sainbayar Sukhbaatar",
        "Arthur Szlam",
        "Jason Weston",
        "Rob Fergus"
      ],
      "year": "2015",
      "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "51",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "A survey on human emotion recognition approaches, databases and applications",
      "authors": [
        "C Vinola",
        "Vimaladevi"
      ],
      "year": "2015",
      "venue": "ELCVIA Electronic Letters on Computer Vision and Image Analysis"
    },
    {
      "citation_id": "53",
      "title": "2021a. A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "Tana Wang",
        "Yaqing Hou",
        "Dongsheng Zhou",
        "Qiang Zhang"
      ],
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN52387.2021.9533718"
    },
    {
      "citation_id": "54",
      "title": "2021b. A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "Tana Wang",
        "Yaqing Hou",
        "Dongsheng Zhou",
        "Qiang Zhang"
      ],
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "55",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "56",
      "title": "Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling",
      "authors": [
        "Martin Wollmer",
        "Angeliki Metallinou",
        "Florian Eyben",
        "Bjorn Schuller",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. INTERSPEECH 2010"
    },
    {
      "citation_id": "57",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "Deep modular co-attention networks for visual question answering",
      "authors": [
        "Zhou Yu",
        "Jun Yu",
        "Yuhao Cui",
        "Dacheng Tao",
        "Qi Tian"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "59",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "60",
      "title": "Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multiview sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria"
      ],
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "62",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2019/752"
    }
  ]
}