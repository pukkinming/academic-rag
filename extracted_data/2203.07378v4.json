{
  "paper_id": "2203.07378v4",
  "title": "Recognition: Closing The Valence Gap *",
  "published": "2022-03-14T13:21:47Z",
  "authors": [
    "Johannes Wagner",
    "Andreas Triantafyllopoulos",
    "Hagen Wierstorf",
    "Maximilian Schmitt",
    "Felix Burkhardt",
    "Florian Eyben",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Affective Computing",
    "Speech Emotion Recognition",
    "Transformers Augmentation SNR Arousal Dominance Valence / sentiment MSP-Podcast IEMOCAP MSP-Podcast IEMOCAP MSP-Podcast IEMOCAP MOSI Natural soundscapes 0 dB -.145 -.109 -.081 -.119 -.221 -.183 -.210 10 dB -.038 -.018 +.001 -.035 -.051 -.064 -.071 20 dB -.019 -.009 +.012 -.018 -.003 -.015 -.025 Human, non-speech 0 dB -.254 -.205 -.195 -.167 -.278 -.214"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances in transformer-based architectures have shown promise in several machine learning tasks. In the audio domain, such architectures have been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Our investigations reveal that transformerbased architectures are more robust compared to a CNN-based baseline and fair with respect to gender groups, but not towards individual speakers. Finally, we show that their success on valence is based on implicit linguistic information, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. To make our findings reproducible, we release the best performing model to the community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic speech emotion recognition (SER) is a key enabling technology for facilitating better human-to-machine interactions  [1] . SER research is dominated by two conceptual paradigms: discrete emotions  [2]  and emotional dimensions  [3] . The first investigates emotional categories like happy or sad, while the latter focuses on the dimensions of arousal, valence, and dominance  [3] .\n\nA SER system achieves this through the linguistic (what has been said) or the paralinguistic (how it has been said) stream  [1, 4, 5] . The linguistic stream is better suited for valence recognition  [6, 7]  and can draw from recent advances in automatic speech recognition (ASR) and natural language processing (NLP)  [8] , but might be limited to a single language. Paralinguistics works better for arousal and dominance  [6, 7]  and has the potential to generalise across different languages. Both paradigms can be combined in bimodal architectures  [4] , which require to execute several different models. Instead, we aim towards a model that only implicitly utilises the linguistic information stream during deployment, and does not require access to ASR and NLP frontends.\n\nAlthough the field has seen tremendous progress in the last decades  [1] , three major challenges remain for real-world paralinguistics-based SER applications: a) improving on its inferior valence performance  [7, 9] , b) overcoming issues of generalisation and robustness  [10, 11] , and c) alleviating individual-and group-level fairness concerns, which is a prerequisite for ethical emotion recognition technology  [12, 13] . Previous works have attempted to tackle these issues in isolation, but combining them is not straightforward.\n\nIn recent years, the artificial intelligence (AI) field is undergoing a major paradigm shift, moving from specialised architectures trained for a given task to general-purpose foundation models that can be adapted to several use-cases  [14] . Such models have seen tremendous success in computer vision  [15, 16] , NLP  [17] , and computer audition  [18, 19] , including SER  [20, 21] . Among others, wav2vec 2.0  [18]  and HuBERT  [19]  have emerged as foundation model candidates for speech-related applications. We evaluate several publicly-available pre-trained variants of those models for dimensional SER, and show that they can achieve state-of-the art results for valence. We further analyze the influence of the model architecture, the pre-training data, how well the models generalise, their robustness, fairness, and efficiency. Moreover, we make our best performing model publicly available  [22] . To our best knowledge this is the first transformer-based dimensional SER model released to the community. For an introduction on how to use it, please visit: https://github.com/audeering/w2v2-how-to.\n\nThe remainder of this paper is organised as follows. Section 2 discusses related work, Section 3 presents the models, databases, and evaluation methods. Section 4 shows the results and investigates why transformer models are able to close the valence gap and improve performance with respect to robustness and fairness. Section 5 investigates efficiency improvements, before Section 6 summarises the results, and Section 7 concludes the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Table  1 : State-of-the-art 4-class emotion recognition performance on IEMOCAP using transformer-based architectures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the base (b) or large (L) architecture was used as well as whether the pre-trained model was fine-tuned for speech recognition (FT-SR). The column FT-D marks if the transformer layers were further fine-tuned during the down-stream classification task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Work",
      "text": "Model L FT-SR FT-D UAR WAR 1  [23]  w2v2-L ✓ 60.0 2\n\n[24] * w2v2-L ✓ 62.5 62.6 3  [20]  w2v2-b 63.4 4  [25]  w2v2-b 63.4 5  [26]  w2v2-b ✓ 63.8 6  [20]  hubert-b 64.9 7  [25]  hubert-b 64.9 8  [20]  w2v2-L ✓ 65.6 9  [25]  w2v2-L ✓ 65.  6 10 [26]  w2v2-b 67.2 11  [20]  hubert-L ✓ 67.6 12  [25]  hubert-L ✓ 67.6 13  [27]  w2v2-b ✓ 69.9 14  [28]  w2v2-L ✓ 70.7 15  [20]  w2v2-b ✓ ✓ 73.8 (68.3) ** 16  [27]  w2v2-b ✓ 74.3 17  [20]  hubert-b ✓ 76.6 (69.7) ** 18  [20]  w2v2-L ✓ ✓ ✓ 76.8 (69.1) ** 19  [20]  w2v2-b ✓ 77.0 (71.0) ** 20  [20]  w2v2-L ✓ ✓ 77.5 (71.0) ** 21  [20]  hubert-L ✓ ✓ ✓ 79.0 (73.0) ** 22  [20]  hubert-L ✓ ✓ 79.6 (73.0) **",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprint",
      "text": "The focus of our work is the recognition of emotional dimensions. However, most related studies target emotional categories. Since the approaches are closely related, we consider both in this section.\n\nIn Table  1 , we provide a summary of recent works based on wav2vec 2.0 and HuBERT on the IEMOCAP dataset  [29] , on which most prior works have focused. Results are ranked by unweighted average recall (UAR) / weighted average recall (WAR) on the four emotional categories of anger (1103 utterances), happiness (+ excitement) (1636), sadness (1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply leave-one-session-out cross validation (5 folds), except Yuan et al.  [24] , using leave-one-speaker-out cross validation (10 folds), and Wang et al.  [20] , who do not explicitly mention which folds they used. Even though authors have used different head architectures and training procedures in their studies, we can draw some general observations:\n\n1. Fine-tuning pre-trained weights yields a 10% boost.\n\n2. Additional ASR fine-tuning does not help with SER (e. g. row 15 vs row 19 -3.2%).\n\n3. The large architecture is typically better than the base one (e. g. row 17 vs row 22 +3.0%), but differences can be quite small (e. g. row 19 vs row 20 +.5% ).\n\n4. HuBERT outperforms wav2vec 2.0 (e. g. row 22 vs row 20: +2.1%).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5.",
      "text": "When performing a fine-tuning of the transformer layers, a simple average pooling in combination with a linear classifier built over wav2vec 2.0 or HuBERT as proposed by Wang et al.  [20]  seems sufficient and shows best performance in the ranking. However, some of the more complex models like the cross-representation encoder-decoder model proposed by Makiuchi et al.  [28]  only report results without fine-tuning the pretrained model during the down-stream task.\n\nWhile the aforementioned studies have focused on emotional categories, there also exist several ones which concentrate on dimensions. The most comparable to ours is that of Srinivasan et al.  [30] , who fine-tuned wav2vec 2.0 / HuBERT on arousal, dominance, and valence. Their results show that pre-trained models are particularly good in predicting valence. When additionally joining audio embeddings from the fine-tuned models and text representations obtained with a pre-trained BERT model, they got a concordance correlation coefficient (CCC) for valence of .683 on the MSP-Podcast corpus  [31] . Furthermore, they were able to distill the multi-model system to an audio-only model using student-teacher transfer learning, while still reaching a concordance correlation coefficient (CCC) of .627 (a massive improvement compared to the previous state-of-the-art performance of only .377  [32] ). However, this improvement was the result of cross-modal transfer learning, and it remains unclear whether speech-based architectures are by themselves able to reach such performance level -a fact we further explore in our work.\n\nThe presented results demonstrate the great potential of wav2vec 2.0 and HuBERT for emotion recognition. However, the influence of pre-training data quantity and domain remains unclear. For instance, even though the large model shows consistently better performance, it is unclear if that can be attributed to the additional layers or to an 60 fold increase of training data compared to the base model. Likewise there is little understanding on the impact of language, as previous work focused in pre-training on English speech data. In this contribution, we present a systematic comparison of different models pre-trained under various conditions (e. g. including noisy speech) and evaluate them on several datasets (in-domain and cross-corpus).\n\nMoreover, it is important to show that SER models work well under noisy conditions. Oates et al.  [10] , Triantafyllopoulos et al.  [11] , Jaiswal and Provost  [33] , and Pappagari et al.  [34]  have shown that previous SER models suffer from robustness issues. We systematically investigate robustness of transformer-based models against a variety of augmentations that do not change the human perception of the underlying emotion  [33] .\n\nFinally, we consider fairness an important, but challenging topic for machine learning models. Discussions in the speech processing community focus mainly on group fairness, e. g. gender  [35] . For SER models, only a few evaluations are available. Gorrostieta et al.  [36]  found a decrease in CCC for females compared to males for arousal in MSP-Podcast (v1.3) of .234. Besides group fairness, this contribution investigates individual fairness by estimating the influence of the speaker on the model performance, which is a known problem for speaker verification models  [37] .\n\n3 Experimental setup",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Pre-Trained Models",
      "text": "Throughout the paper, we discuss results obtained with transformer-based models pre-trained on large amounts of unlabelled data. We investigate two main variants: wav2vec 2.0  [18]  and HuBERT  [19] . The network architecture of both models is the same. As input, it expects a raw waveform normalised to have zero mean and unit variance, LibriSpeech 960 eng R w2v2-L  [18]  LibriSpeech * 60k eng R hubert-L  [19]  Libri-Light 60k eng R w2v2-L-robust  [38]  Libri which is fed into a feature encoder consisting of 7 convolutional layers that extracts feature vectors over time, with a dimensionality of 512 and a step size of 20 ms. These features are projected to a higher dimension (768 or 1024 hidden units, see below) and then fed into the encoder. The encoder is a series of transformer layers, each of them consisting of a multi-head self-attention module and several fully-connected layers. In order to inject temporal information, the output of a convolutional layer is added at the input of the encoder.\n\nThe only difference between the main variants is the way they are pre-trained on unlabelled data. In wav2vec 2.0, the features of a certain ratio of time steps are masked, by replacing them with a learnt fixed feature vector at the input of the encoder. A contrastive loss between the encoder outputs and a quantised version of the input features is then minimised  [18] . In order to avoid learning too simple representations, the quantisation is done using a codebook, whose diversity loss is minimised as well. In contrast, HuBERT minimises a cross-entropy loss for the masked time steps, where the targets are not trained simultaneously with the model. The pre-training is performed in several steps, where in the first step, clusters obtained by k-means clustering of MFCCs are employed as targets and in later steps, clusters of the outputs of certain transformer layers are taken into account  [19] . In following these strategies, the models try to learn the structure of speech, resulting in a reduced need for labelled task-specific training data.\n\nBoth wav2vec 2.0 and HuBERT exist in two forms: a base architecture with 12 transformer layers of 768 hidden units each (95M parameters), and a large architecture with 24 transformer layers of 1024 hidden units each (317M parameters). Apart from that, we further distinguish them by the data used for pre-training. We included the four models found in previous work (cf. Section 2), which are pre-trained on English audiobooks, namely wav2vec2base (w2v2-b), hubert-base-ls960 (hubert-b), wav2vec2-large (w2v2-L), hubert-large-ll60k (hubert-L); the wav2vec2large-robust model (w2v2-L-robust), additionally trained on telephone speech; the wav2vec2-large-100k-voxpopuli model (w2v2-L-vox), trained only on parliamentary speech in multiple languages; and the wav2vec2-xls-r-300m model (w2v2-L-xls-r), trained on more than 400k hours across all domains and multiple languages. Compare Table  2  for citations and an overview of the included data. We did not include models fine-tuned on speech recognition as previous work showed that this does not lead to better performance. Also note that we refer to their fine-tuned versions when we report results (cf. Section 3.2).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Architecture",
      "text": "Inspired by Wang et al.  [20]  we apply average pooling over the hidden states of the last transformer layer and feed the result through a hidden layer and a final output layer. For fine-tuning on the downstream task, we use the ADAM optimiser with CCC loss, which is the standard loss function used for dimensional SER  [9, 32, 41] , and a fixed learning rate of 1e-4. We run for 5 epochs with a batch size of 32 and keep the checkpoint with best performance on the development set.\n\nDuring training, we freeze the CNN layers but fine-tune the transformer ones. According to Wang et al.  [20] , such a partial fine-tuning yields better results. When using the term fine-tuning, we will henceforth refer to this partial fine-tuning. These models are trained using a single random seed, for which the performance is reported. We compare results to a 14-layer Convolutional Neural Network (CNN14) as a standard baseline we have been using for SER in previous work  [9, 42] . It follows the architecture proposed by Kong et al.  [43]  for audio pattern recognition. Different to the transformer-based models, which operate on the raw audio signal, this takes log-Mel spectrograms as input. CNN14 has 6 convolutional blocks with two layers each, each followed by max pooling. Convolution layers have a 3 × 3 kernel and a stride of 1 × 1, whereas max pooling layers use a stride of 2 × 2. After the last convolution layer, features are pooled using both mean and max pooling, and subsequently fed into two linear layers. Dropout with a probability of 0.2 is applied after every each convolution block. Log-Mel spectrograms are computed with 64 Mel bins, a window size of 32 ms, and a hop size of 10 ms. Note that the CNN14 model is not pre-trained, i. e. it is always trained from scratch in our experiments. We train for 60 epochs, with a learning rate of .01, and a batch size of 64 using stochastic gradient descent (SGD) with a Nesterov momentum of .9. We select the model that performs best on the validation set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We used the MSP-Podcast corpus  [31]  (v1.7) to run multitask training on the three dimensions of arousal, dominance, and valence for speech from podcast recordings. The original labels cover a range from 1 to 7, which we map into the interval of 0 to 1. Its train split contains 62 hours of recordings. In-domain results are reported on the test-1 split, which contains 21 hours of audio provided by 12, 902 samples (54% female / 46% male) from 60 speakers (30 female / 30 male). The samples per speaker vary between 42 and 912.\n\nWe report cross-domain results IEMOCAP (Interactive Emotional Dyadic Motion Capture) dataset  [29] , which contains 12 hours of scripted and improvised dialogues by ten speakers (5 female / 5 male). It provides the same dimensional labels as MSP-Podcast, but in a range of 1 to 5, which we map to the interval 0 to 1. Since we use the dataset only during evaluation, we do not apply the usual speaker cross-validation, but treat the corpus as a whole. It includes 10, 039 samples (49% female / 51% male).\n\nFinally, we report cross-corpus results for valence on the test set of the Multimodal Opinion Sentiment Intensity (MOSI)  [44]  corpus. The dataset is a collection of YouTube movie review videos spoken by 41 female and 48 male speakers. They are annotated for sentiment on a 7-point Likert scale ranging from -3 to 3, which we map to the interval 0 to 1. The test set contains 1 hour audio recordings given as 685 samples (51% female / 49% male), annotated for sentiment. As the gender labels are not part of the distributed database, we re-annotated them ourselves  [45] .\n\nWhile sentiment is a different concept than valence, as the former corresponds to an attitude held towards a specific object and the latter more generally characterises a person's feeling  [46] , there is evidence that sentiment annotations can be decomposed to two constituents: intensity and polarity  [47] , which roughly correspond to arousal and valence. We therefore expect some correlation between (predicted) valence and (annotated) sentiment scores. PREPRINT",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "Machine learning models for speech emotion recognition are expected to work under different acoustic conditions and for different speakers. To cover this, we evaluate them for correctness, robustness, and fairness  [48] .\n\nCorrectness measures how well predictions match the ground truth. The concordance correlation coefficient (CCC) provides an estimate of how well the predicted distribution matches the ground truth one  [49] , and is the typical measure for evaluating dimensional SER models  [50] .\n\nRobustness (cf. Section 4.8) measures how model performance is affected by changes to the input signals such as adding background noise. Applying changes to the input signals must be carefully done for SER, as they might affect the ground truth label  [33, 51] . We focus on testing the robustness of the models against data augmentations that do not change the human perception of the underlying emotion. We select the following five augmentations from Jaiswal and Provost  [33]  to enable direct comparison with previous results: Natural soundscapes adds a randomly selected sample from the natural class of the ESC-50 dataset  [52]  with a signal-to-noise ratio (SNR) of 0 dB, 10 dB or 20 dB; Human, non-speech adds a randomly selected sample from the human class of the ESC-50 dataset with a SNR of 0 dB, 10 dB or 20 dB; Interior/domestic adds a randomly selected sample from the interior class of the ESC-50 dataset with a SNR of 0 dB, 10 dB or 20 dB; Speed up segment selects a random segment of 10% to 20% length within the utterance and increases its speed by 1.25; Fade-in/fade-out decreases or increases the amplitude of the signal by 2% every second.\n\nFairness (cf. Section 4.9) evaluates if the model predictions show biases for certain protected attributes like race, gender, or age  [53] . We focus on gender due to the lack of sufficient available information and/or datasets for other attributes. For regression problems, there is no clear definition how to measure fairness, but most approaches try to achieve an equal average expected outcome for population A and B  [54] . We measure fairness by estimating the gender fairness score as the difference in the correctness metric (CCC) between female and male groups. A positive gender fairness score indicates a better performance of the model for female speakers.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "We begin our investigation with a thorough evaluation of transformer-based models. We show that valence is the primary beneficiary of pre-training as it enables the models to implicitly learn linguistic information during the finetuning of the transformer layers. Utilising a comprehensive testing scheme, we attempt to identify how different aspects of foundation models impact performance and generalisation. We place particular emphasis on robustness and fairness, which are critical considerations for SER systems targeted to real-world applications.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Can Foundation Models Close The Performance Gap For Valence?",
      "text": "Answer: The best models achieve a similar performance for arousal and dominance as non-transformer architectures  [32] , but improve the CCC score for valence by .26 and close the performance gap for valence.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Details:",
      "text": "In Figure  2 , we show in-domain and cross-domain CCC performance for different wav2vec 2.0 and HuBERT models as well as for the CNN14 baseline.\n\nWe first focus on arousal and dominance  Embeddings from the already fine-tuned models are concatenated with BERT embeddings extracted from automatic transcriptions, whereupon a two-layer feed-forward neural network is trained. We show the difference to results with the fine-tuned (ft) models from Figure  2 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Details:",
      "text": "To evaluate whether adding linguistic information improves the predictions, the following experiment is conducted: a regression head is pre-trained, using as input pooled BERT embeddings in addition to the pooled states of the fine-tuned transformer models.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a transformer model for natural language, pretrained on English language corpora consisting of more than 3 billion words  [55] . The BERT embeddings have a dimensionality of 768 and are extracted from the transcriptions generated by the wav2vec2-base-960h speech recognition model 2  . The fusion is done by concatenating the representations of both modalities. As regression head, exactly the same architecture as for the fine-tuning of wav2vec\n\nFigure  4 : CCC performance for valence on the original and synthetic files on MSP-Podcast. We see that models with a high performance on the original files are more sensitive to sentiment (cf. left and center section). To prove that a fine-tuning of the transformer layers is required to learn linguistic content, we additionally show the correlation for models where the transformer layers were frozen (frz) during training (cf. Section 4.4).\n\nIn Figure  3 , we report deviations from the results achieved with the fine-tuned acoustic models alone (cf. Figure  2 ). We can see that a fusion with embeddings from the text domain helps with valence, but not with arousal and dominance, where performance actually deteriorates. This is in line with our previous findings, where we also found that introducing linguistic information sometimes hampered performance for those two dimensions on MSP-Podcast  [9] . What is interesting, though, are the relatively large differences between the models, and that, especially, our best models hubert-L and w2v2-L-robust do not improve. The models that benefit most are the two multi-lingual models w2v2-L-vox and w2v2-L-xls-r, showing that models pre-trained on multiple languages gain from a fusion with text features from the test set domain language.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Do The Models Implicitly Learn Linguistic Information?",
      "text": "Answer: The models implicitly capture linguistic information from the audio signal. The extent in which they learn sentiment during fine-tuning depends on the data used for pre-training (e. g. multi-lingual data makes it more difficult). Generally, we see that valence performance correlates with a model's ability to predict sentiment.\n\nDetails: Previous findings suggest that during fine-tuning, the models implicitly learn linguistic information. To asses how sensitive the models are to linguistic content, we generated a synthesised version of a subset of the test set from the transcriptions of MSP-Podcast.  3  In Figure  4 , we finally show CCC performance for valence on the original and synthesised files for all models. We see that performance gaps between the models in Figure  2  are directly linked with their ability to predict sentiment. Models reaching a high performance on the original files also do so on their synthetic versions and vice versa. However, to learn linguistic content, a fine-tuning of the transformer layers is essential. If we predict the synthetic test set with models where the transformer layers were frozen during training (cf. Section 4.4), correlation drops to almost zero.\n\nThis finding is also important for works doing in-domain training on IEMOCAP, as parts of the conversations are scripted which results in a leakage of text information that may result in overoptimistic results  [56]  when that text information is exploited by transformer models. Furthermore, our models may inherit similar biases as those found in NLP models  [57] .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "How Important Is A Fine-Tuning Of The Transformer Layers?",
      "text": "Answer: Fine-tuning the transformer layers is necessary to obtain state-of-the-art performance, in particular for the valence dimension. The highest gain is observed for hubert-L and w2v2-L-robust, which are the models that benefit the least from a fusion with text.\n\nDetails: So far, we have fine-tuned all transformer layers along with the added output layer. However, practitioners often choose to use a pre-trained model as a frozen feature extractor, and subsequently train just an output layer on the generated embeddings. Nevertheless, prior studies have shown that it is necessary to fine-tune several or all layers on the target task to get good downstream performance  [20, 42, 43, 58] . In this sub-section, we experiment with training only the last output layer and keeping all others frozen. This is compared to our previous experiments where we jointly fine-tune the last layer and the transformer layers.  For the frozen results, we keep all transformer layers frozen and simply train the output head. Results show that finetuning the transformer layer is worth the computational cost it incurs.\n\nFigure  5  shows the difference between CCC values for the fine-tuned and frozen models. We observe performance gains when fine-tuning in all cases, demonstrating that fine-tuning of the transformer layers is necessary. Moreover, the models that see the biggest performance gain are hubert-L and w2v2-L-robust. In Section 4.2, these models were found to benefit less from additional text information. These findings indicate that a fine-tuning of the transformer layers enables the models to capture the linguistic information needed to perform well on valence.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Do The Models Generalise Better Across Different Domains?",
      "text": "Answer: Transformer-based models generalise better than a non-transformer baseline.\n\nDetails: As we see a similar trend for different transformer-based models between in-domain and cross-corpus results in Figure  2 , we focus on the best-performing one (w2v2-L-robust). The drop in CCC between in-domain and cross-corpus results for w2v2-L-robust on IEMOCAP is 11% for arousal, 21% for dominance, and 30% for valence on IEMOCAP, and 15% for sentiment on MOSI. For CNN14, the drop in CCC is 34% for arousal, and 52% for dominance, while for valence, we do not estimate the drop in cross-domain performance as the in-domain CCC is already very low. The drop in CCC is smaller for w2v2-L-robust for arousal and dominance, indicating that transformer-based models generalise better. For valence, we cannot derive a final conclusion, but the trend we see for sentiment in MOSI seems very promising.\n\n4.6 Does more data during pre-training lead to better performance?\n\nAnswer: For arousal and dominance, all tested models perform equally well, whereas with respect to valence / sentiment the data used for pre-training has a strong effect. Mixing data from several domains leads to a considerable improvement for w2v2-L-robust compared to w2v2-L, which is only trained on clean speech. However, hubert-L, which uses the same pre-training data as w2v2-L, which is also trained on clean speech, still performs as good as w2v2-L-robust. For models pre-trained on multi-lingual data, we see a performance drop when tested on English speech.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Details:",
      "text": "To understand what influence the size and domain of the pre-training data have on downstream performance, we included several wav2vec 2.0 models with same large architecture but different pre-training (see Table  2 ).\n\nThe results in Figure  2  show only differences in terms of CCC between the transformer models for valence and sentiment, not for arousal or dominance. Previous studies uniformly report that HuBERT outperforms wav2vec 2.0 which is replicated by our results with w2v2-b showing a smaller CCC than hubert-b for the valence task on MSP-Podcast and IEMOCAP, and for the sentiment task on MOSI. The increase in performance for w2v2-L-robust compared to hubert-L is most likely explained by the additional 3k hours of telephone conversations used for pre-training. However, by comparing w2v2-L-vox and w2v2-L-xls-r, it also becomes clear that more data does not necessarily lead to better results. Though both models are trained on significantly more data than hubert-L and w2v2-L-robust (100k and 463k vs 63k hours), they perform clearly worse. Notably, both were pre-trained on multiple languages. Since the databases we use for evaluation contain only English speakers, this could be a disadvantage to models that are exclusively pre-trained on English.\n\n0.0 0.5",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Does A Larger Architecture Lead To Better Performance?",
      "text": "Answer: A larger architecture does not lead to better performance per se. Larger architectures using different data during pre-training might perform worse than smaller architectures.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Details:",
      "text": "We can draw some conclusions about the influence that the size of the architecture has on performance based on Figure  2 . The size of the architecture, i. e. base vs large, seems not to be the decisive point: w2v2-b and w2v2-L show very similar performance. In addition, the small models w2v2-b and hubert-b have comparable performance to the large models w2v2-L, w2v2-L-vox, and w2v2-L-xls-r for arousal and dominance, both in-and cross-domain.\n\nFor valence, the small models outperform w2v2-L, w2v2-L-vox, and w2v2-L-xls-r in most cases for MSP-Podcast and MOSI, and achieve a similar performance on IEMOCAP.\n\n4.8 Are the models robust against changes to the input signals?\n\nAnswer: The tested models are reasonably robust against changes to the input signals, with w2v2-L-robust showing the highest and hubert-b the lowest robustness.\n\nDetails: Figure  6  summarises the average CCC scores of the models averaged over all augmentations described in Section 4.8. All models show a drop in CCC compared to the CCC scores for the clean data from Figure  2 . w2v2-L-robust has now the highest CCC score for all datasets and all dimensions. The average change in CCC for w2v2-L-robust is -0.068. The model with the highest average change in CCC is hubert-b (-0.108). The model with the lowest average change in CCC is CNN14 (-0.047), which is mostly due to its results for IEMOCAP for which it shows no impairment of its relatively low performance by augmentations.\n\nTable  3  shows changes in CCC for single augmentations for each dataset and dimension for the best performing model w2v2-L-robust. The performance of the model is only sligthly affected (absolute change in CCC score below .05) for added background sounds with a SNR of 20 dB or a fade-in/fade-out of the signal. When speeding up parts of the signal or adding background sounds with more severe SNRs the change in CCC can be up to -.278. The model investigated on the same augmentations by Jaiswal and Provost  [33]  shows an equal drop in unweighted average recall (UAR) when adding background sounds with 0 dB, 10 dB, 20 dB SNR of at least -.  The gender fairness score is given by CCC female -CCC male . A positive value indicates that the model under test performs better for female speaker and a negative value that it performs better for male speaker. A model with desired equal performance would have a gender fairness score of 0.\n\nthe most affected when adding human, non-speech sounds (average drop in CCC of -.103), and the least when adding interior/domestic sounds (average drop in CCC of -.055).\n\n4.9 Are the models fair regarding the gender of the speaker?\n\nAnswer: Models are more fair for arousal and dominance than for valence. For valence, most models show a higher CCC for females than for males.\n\nDetails: Figure  7  shows gender fairness scores for the speakers in MSP-Podcast, IEMOCAP, and MOSI. As introduced in Section 3.4, the gender fairness score is expressed by the difference in CCC between female and male speakers with positive values indicating higher values of the underlying metric for females. For MSP-Podcast, nearly all models show a slightly worse female CCC for arousal and dominance. For IEMOCAP, nearly all models show a slightly better female CCC for arousal and dominance. Speakers have been ordered according to the mean CCC over all dimensions and models.\n\nFor valence in MSP-Podcast and IEMOCAP, most models show a better CCC for female speakers than male oneswith the exception of CNN14. For sentiment in MOSI, the CNN14 model shows a bias towards better performance for male speaker, whereas all other models show very small biases in the different direction.\n\nAveraging over all databases and dimensions the model with the best gender fairness score is w2v2-L with .007, followed by w2v2-L-vox with .015, w2v2-L-xls-r with .018, w2v2-L-robust, with .019, hubert-b with .025, hubert-L with .027, and w2v2-b with .029 up to CNN14 with -.043.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Is Performance Equal Across All Speakers?",
      "text": "Answer: Performance for the best foundation models is similar between most speakers in MSP-Podcast, but can deteriorate to low CCC values for some speakers.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Details:",
      "text": "The performance of speech processing is dependent on individual speaker characteristics  [37] . This has led several prior SER works to target personalisation to different speakers  [59, 60, 61] . To investigate this phenomenon for transformer-based models, we examine the per-speaker performance, where instead of computing a global CCC value over all test set values, we compute one for each speaker. As discussed (cf. Section 3.3), the MSP-Podcast test set consists of 12902 samples from 60 speakers; however, the samples are not equally distributed across them (minimum samples: 41, maximum samples 912). In order to make our subsequent analysis more robust, we only keep speakers with more than 200 samples, resulting in 19 speakers. We use bootstrapping, where we randomly sample (with replacement) 200 samples from each speaker to compute the CCC. This process is repeated 1000 times, and we report the mean value.\n\nOur results are presented in Figure  8 . For visualisation purposes, we ordered speakers based on the average CCC value over all models and across arousal, dominance, and valence. For arousal and dominance models perform well for most speakers, and show similar performance. For speakers 7 and 931 all models show a low CCC, whereas for speaker 931 the CNN14 model performs worse than the others. For valence, CCC values per speaker differ between models replicating the findings of Figure  2 . The best model (w2v2-L-robust) performs relatively similar for most of the speaker groups and shows only a drop for speaker 7, a similar result as for valence and dominance.\n\nDifferent models broadly, but not perfectly, agree on 'good' and 'bad' speakers, with pairwise Spearman correlations ranging from .960 to .725 for arousal, .972 to .825 for dominance, and .947 to .333 for valence. This could be a manifestation of the underspecification phenomenon plaguing machine learning architectures  [62] , as models which have similar performance on the entire test set, nevertheless behave differently across different subsets of it.\n\nPREPRINT 0.0 0.5 .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": ".448 .539",
      "text": "Figure  9 : CCC performance of randomly-initialised wav2vec 2.0 model (w2v2-L-w/o-pretrain) on in-domain and cross-corpus arousal, dominance, valence / sentiment prediction. We compare the performance with that of CNN14 and w2v2-L-robust. We observe that valence and sentiment benefit massively from pre-training, without which wav2vec 2.0 performs worse than a classic CNN approach.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Why Do Foundation Models Generalise So Well?",
      "text": "Answer: Even without pre-training, the latent space provided by the transformer architecture generalises better than CNN14, as it abstracts away domain and speaker. Pre-training marginally improves arousal and dominance performance but is critical for valence.\n\nDetails: So far, we were able to confirm the superiority of transformer-based models. However, even though pretraining seems important, it remains unclear to what extent the transformer architecture itself contributes to that success. To shed more light into this, we trained wav2vec 2.0 from a random initialisation. As our architecture, we chose the large wav2vec 2.0 architecture, which is also used by the best performing model w2v2-L-robust. In the following, we will refer to this model as w2v2-L-w/o-pretrain.\n\nWe trained the model for 50 epochs and selected the best checkpoint according to the performance on the development set (epoch 17).  4  In Figure  9 , we compare in-and cross-domain performance with CNN14 and w2v2-L-robust. We see that especially valence / sentiment detection benefits massively from pre-training (both in-domain and cross-domain), and that without pre-training wav2vec 2.0 performs in most cases worse than CNN14.\n\nIn the introduction of wav2vec 2.0, Baevski et al.  [18]  postulate that pre-training helps learn more general representations that abstract away from speaker or background information. However, it is not entirely clear if these benefits are a result of pre-training or are a consequence of the specific inductive biases introduced by the architecture. To investigate this, we compare embeddings extracted with CNN14, w2v2-L-w/o-pretrain, and w2v2-L-robust,  5  which are shown in Figure  10 . The embeddings are projected to two dimensions using t-SNE  [63]  and different information is chromatically superimposed. sentations. In contrast, the latent space of both wav2vec 2.0 models shows no clusters for domain, gender, or speaker. The architecture itself seems to introduce specific inductive biases which are well-suited to learning robust representations. Nevertheless, only the pre-trained model (w2v2-L-robust) shows a smooth transition from low to high valence scores, showing that pre-training is still necessary for good downstream performance. Moreover, the strong speaker dependency presented in Section 4.10 shows that the two dimensional t-SNE visualisations help comparing generalisation abilities between models, but are not necessarily sufficient for deriving conclusions w. r. t. generalisation over different factors.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Efficiency",
      "text": "For our last experimental evaluation, we focus on efficiency. We concentrate on three facets: optimisation stability, computational complexity, and data efficiency. Details: To balance the effects of randomness (either in the initialisation of network weights or the data sampling), it is a common strategy to perform several runs with different random seeds. Starting from pre-trained weights, however, we expect less volatility  [64, 65] . Figure  11  shows the mean and standard deviation over the performance on the development set across three trials for CNN14 and w2v2-b. CNN14 shows a constant jittering across all 60 epochs, whereas w2v2-b converges faster and we can reduce the number of epochs to 5.   Figure  12 : CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust and pruned versions. The legend shows the number of bottom layers kept during fine-tuning. We see that half of the layers can be removed without any loss in performance.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "How Many Transformer Layers Do We Really Need?",
      "text": "Answer: We can reduce the number of transformer layers to 12 without a degradation in performance. With less than 12 layers we begin to see a negative effect on valence.\n\nDetails: In Section 4.7, we mentioned that w2v2-b and hubert-b outperform some of the large models. From that, we concluded that the size of the architecture seems less important, but it is rather the data used for pre-training that determines success. If this is really the case, we should be able to partially reduce the size of a model without losing performance.\n\nSajjad et al.  [66]  investigated different layer pruning strategies and identified top-layer dropping as the best strategy offering a good trade-off between accuracy and model size. Inspired by their findings, we set up an experiment where we successively removed transformer layers from the top of the original pre-trained model before fine-tuning.\n\nIn Figure  12 , we report the effect on CCC for w2v2-L-robust (our overall best performing model). Results show that half of the layers can be removed without a loss in performance. We denote the resulting 12-layer model as w2v2-L-robust-12. Only with 10 or less layers we actually begin to see a drop for valence / sentiment on IEMOCAP and MOSI. For arousal and dominance, we still achieve good performance with only 8 layers.  Figure  13  shows CCC for arousal, dominance, valence / sentiment on MSP-Podcast, IEMOCAP and MOSI. For efficiency, we start from the reduced 12-layer architecture and therefore compare results to w2v2-L-robust-12 (cf. Section 5.2). There is no noteworthy degradation for arousal and dominance when keeping close to the entire training set. The only exception is dominance on IEMOCAP, where we achieve best results with just 75% of the data. For these dimensions, however, performance already saturates at 25% yielding a loss of less than .02 on MSP-Podcast, whereas for IEMOCAP, even 12.5% of the training samples seem sufficient to stay within a margin of .05.\n\nOnce again, it is a different story for valence. For MSP-Podcast, we see a constant improvement that only begins to decrease when reaching 75% of the data. For MOSI, we even see a boost in CCC of almost .1 for the remaining 25%. However, in light of our findings from Section 4.3, this does not come as a surprise. Providing more linguistic diversity makes it more likely a model can detect associations between key words and emotional context. What is a surprise, though, is that on IEMOCAP, using just 7.5% of the data, results in a drop of less than .05. A possible explanation is that the vocabulary of IEMOCAP does not resemble that of MSP-Podcast and that, therefore, the impact of linguistic information is limited. This would also explain why the differences in valence performance are less pronounced for IEMOCAP (cf. Figure  2 ).",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Summary",
      "text": "We explored the use of (pre-trained) transformer-based architectures for speech emotion recognition. In the previous sections, we dealt with several questions in isolation. We now attempt a unified summary by collectively considering all findings.\n\nEffect of pre-training: pre-training is essential to get good performance (Section 4.6), especially for the valence dimension. This is particularly evident when training wav2vec 2.0 from a random initialisation (Section 4.11): the model PREPRINT performs substantially worse on all three dimensions, and its embeddings are unable to capture valence information. In addition, pre-training serves as a form of regularisation which helps stabilise the training (Section 5.1), thus resulting in models which require less iterations, and less data to train on (Section 5.3). However, we were unable to determine a clear relationship of the form 'more pre-training data leads to better performance'. In fact, downstream performance can be negatively impacted by the introduction of more data, as seen by the comparison between w2v2-L-vox and w2v2-L-xls-r, which differ only in the fact that w2v2-L-xls-r has been trained on more (and more diverse) data, yet performs worse on all three dimensions.\n\nGeneralisation: transformer-based models show very good cross-corpus generalisation (Section 4.6), robustness (Section 4.8), and appear invariant to domain, speaker, and gender characteristics (Section 4.11). These are all very important traits for any model that is intended for production use in realistic environments. However, they seem to stem primarily from the architecture rather than the pre-training as they are also evident in models initialised from random weights (Section 4.11). We also showed that several self-attention layers can be removed without hampering downstream performance (Section 5.2), though they might still be necessary for successful pre-training.\n\nFairness: fairness remains a challenging topic for contemporary machine learning architectures. Community discussions primarily concern the issue of group fairness. In the present, we investigate this for the only group variable available in our datasets: gender (Section 4.9), where we observe that transformer-based architectures are more fair than the CNN14 baseline. However, we argue that individual fairness is important for SER. This refers to how models perform across different speakers; a feat which proves challenging even for the top-performing models investigated here (Section 4.10). We consider this an important topic which has not been sufficiently investigated for SER, though it is long known to impact other speech analysis models  [35, 37] .\n\nIntegration of linguistic and paralinguistic streams: finally, one of our most intriguing findings is that transformers seem capable of integrating both information streams of the voice signal. This is evident in how well-performing valence prediction models retain their effectiveness for synthesised speech lacking emotional intonation (Section 4.3) and fail to benefit from fusion with explicit textual information (cf. Section 4.2). Interestingly, this is only possible when fine-tuning the self-attention layers (Section 4.4), as keeping them frozen results to complete failure for synthesised speech (Section 4.3). This draws attention to an under investigated aspect of fine-tuning, namely, how it qualitatively affects the nature of internal representations. Common understanding sees it as a mechanism through which to obtain better performance, but our analysis shows that it leads to a fundamental change in how the underlying signal is represented (moving from almost no sensitivity to linguistic content to increased reactivity to it). This mechanism may be crucial in the pursuit of paralinguistic and linguistic integration which is key to a holistic understanding of human communication. However, this integration might prove problematic in cases where the two modalities disagree, e. g. in cases of irony  [67] . Our results also highlight that good valence performance might be language dependent as models pre-trained on a variety of languages perform worse for valence compared with comparable models pre-trained only for English (Section 4.1).",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "Transformers have already revolutionised a very diverse set of artificial intelligence tasks, including speech emotion recognition. The present contribution goes beyond previous works that already established their effectiveness for SER by conducting a thorough evaluation and analysis of prominent transformer-based speech models for dimensional emotion recognition. We obtain state-of-the-art valence recognition performance on MSP-Podcast of .638 without using explicit linguistic information, and manage to attribute this exceptional result to implicit linguistic information learnt through a fine-tuning of the self-attention layers. We release our best performing model (w2v2-L-robust-12) to the community  [22] .  6  Transformer architectures are more robust to small perturbations, fair on the (gender) group-if not on the individual-level, and generalise across different domains. Our findings demonstrate that a new era is dawning in speech emotion recognition: that of pre-trained, transformer-based foundation models, which can finally lead to the coveted integration of the two dominant information streams of spoken language, linguistics, and paralinguistics.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed architecture built on wav2vec 2.0 / HuBERT.",
      "page": 5
    },
    {
      "caption": "Figure 2: , we show in-domain and cross-domain CCC performance for different wav2vec 2.0 and HuBERT",
      "page": 6
    },
    {
      "caption": "Figure 2: CCC scores for arousal, dominance, valence (MSP-Podcast / IEMOCAP), and sentiment (MOSI). All mod-",
      "page": 7
    },
    {
      "caption": "Figure 3: Text & audio fusion results for arousal, dominance, and valence prediction on MSP-Podcast. Embeddings",
      "page": 7
    },
    {
      "caption": "Figure 2: Details: To evaluate whether adding linguistic information improves the predictions, the following experiment is",
      "page": 7
    },
    {
      "caption": "Figure 4: CCC performance for valence on the original and synthetic files on MSP-Podcast. We see that models with",
      "page": 8
    },
    {
      "caption": "Figure 3: , we report deviations from the results achieved with the fine-tuned acoustic models alone (cf. Figure 2).",
      "page": 8
    },
    {
      "caption": "Figure 4: , we finally show CCC performance for valence on the original and",
      "page": 8
    },
    {
      "caption": "Figure 2: are directly linked with",
      "page": 8
    },
    {
      "caption": "Figure 5: Difference of fine-tuned (ft) to frozen (frz) CCC performance for arousal, dominance, and valence prediction",
      "page": 9
    },
    {
      "caption": "Figure 2: , where transformer and output layers are jointly trained.",
      "page": 9
    },
    {
      "caption": "Figure 5: shows the difference between CCC values for the fine-tuned and frozen models. We observe performance",
      "page": 9
    },
    {
      "caption": "Figure 2: , we focus on the best-performing one (w2v2-L-robust). The drop in CCC between in-domain and",
      "page": 9
    },
    {
      "caption": "Figure 2: show only differences in terms of CCC between the transformer models for valence and senti-",
      "page": 9
    },
    {
      "caption": "Figure 6: CCC scores for arousal, dominance, valence (MSP-Podcast/IEMOCAP), and sentiment (MOSI) when aug-",
      "page": 10
    },
    {
      "caption": "Figure 2: The size of the architecture, i. e. base vs large, seems not to be the decisive point: w2v2-b and w2v2-L",
      "page": 10
    },
    {
      "caption": "Figure 6: summarises the average CCC scores of the models averaged over all augmentations described",
      "page": 10
    },
    {
      "caption": "Figure 2: w2v2-L-robust has now the highest CCC score for all datasets and all dimensions. The average change in CCC for",
      "page": 10
    },
    {
      "caption": "Figure 7: Gender fairness scores for arousal, dominance, valence (MSP-Podcast / IEMOCAP), and sentiment (MOSI).",
      "page": 11
    },
    {
      "caption": "Figure 7: shows gender fairness scores for the speakers in MSP-Podcast, IEMOCAP, and MOSI. As in-",
      "page": 11
    },
    {
      "caption": "Figure 8: Speaker-level performance (CCC) on MSP-Podcast for the different models. We only use speakers with at",
      "page": 12
    },
    {
      "caption": "Figure 8: For visualisation purposes, we ordered speakers based on the average CCC",
      "page": 12
    },
    {
      "caption": "Figure 2: The best model (w2v2-L-robust) performs relatively similar for most of",
      "page": 12
    },
    {
      "caption": "Figure 9: CCC performance of randomly-initialised wav2vec 2.0 model (w2v2-L-w/o-pretrain) on in-domain and",
      "page": 13
    },
    {
      "caption": "Figure 9: , we compare in- and cross-domain performance with CNN14 and w2v2-L-robust. We see",
      "page": 13
    },
    {
      "caption": "Figure 10: The embeddings are projected to two dimensions using t-SNE [63] and different information is",
      "page": 13
    },
    {
      "caption": "Figure 10: Visualisation of embeddings extracted with different models overlayed with meta information for a com-",
      "page": 14
    },
    {
      "caption": "Figure 11: shows the mean and standard deviation over the performance on the",
      "page": 14
    },
    {
      "caption": "Figure 11: Mean and standard deviation of development set performance on MSP-Podcast across three training runs.",
      "page": 15
    },
    {
      "caption": "Figure 12: CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust and pruned versions. The",
      "page": 15
    },
    {
      "caption": "Figure 12: , we report the effect on CCC for w2v2-L-robust (our overall best performing model). Results show",
      "page": 15
    },
    {
      "caption": "Figure 13: CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust on sparse training data.",
      "page": 16
    },
    {
      "caption": "Figure 13: shows CCC for arousal, dominance, valence / sentiment on MSP-Podcast, IEMOCAP and MOSI. For",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "September 11, 2023": "ABSTRACT"
        },
        {
          "September 11, 2023": "Recent advances in transformer-based architectures have shown promise in several machine learning"
        },
        {
          "September 11, 2023": "tasks.\nIn the audio domain, such architectures have been successfully utilised in the field of speech"
        },
        {
          "September 11, 2023": "emotion recognition (SER). However, existing works have not evaluated the influence of model size"
        },
        {
          "September 11, 2023": "and pre-training data on downstream performance, and have shown limited attention to generalisa-"
        },
        {
          "September 11, 2023": "tion, robustness,\nfairness, and efficiency. The present contribution conducts a thorough analysis of"
        },
        {
          "September 11, 2023": "these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the"
        },
        {
          "September 11, 2023": "dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP"
        },
        {
          "September 11, 2023": "and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top per-"
        },
        {
          "September 11, 2023": "formance for valence prediction without use of explicit\nlinguistic information, with a concordance"
        },
        {
          "September 11, 2023": "correlation coefficient (CCC) of .638 on MSP-Podcast. Our investigations reveal\nthat\ntransformer-"
        },
        {
          "September 11, 2023": "based architectures are more robust compared to a CNN-based baseline and fair with respect\nto"
        },
        {
          "September 11, 2023": "gender groups, but not towards individual speakers. Finally, we show that their success on valence is"
        },
        {
          "September 11, 2023": "based on implicit linguistic information, which explains why they perform on-par with recent multi-"
        },
        {
          "September 11, 2023": "modal approaches that explicitly utilise textual information. To make our findings reproducible, we"
        },
        {
          "September 11, 2023": "release the best performing model to the community."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "1\nIntroduction"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "Automatic speech emotion recognition (SER) is a key enabling technology for facilitating better human-to-machine"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "interactions [1]. SER research is dominated by two conceptual paradigms: discrete emotions [2] and emotional di-"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "mensions [3]. The first investigates emotional categories like happy or sad, while the latter focuses on the dimensions"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "of arousal, valence, and dominance [3]."
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "A SER system achieves this through the linguistic (what has been said) or the paralinguistic (how it has been said)"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "stream [1, 4, 5]. The linguistic stream is better suited for valence recognition [6, 7] and can draw from recent advances"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "in automatic speech recognition (ASR) and natural\nlanguage processing (NLP) [8], but might be limited to a single"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "language.\nParalinguistics works better\nfor arousal and dominance [6, 7] and has the potential\nto generalise across"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "different\nlanguages. Both paradigms can be combined in bimodal architectures [4], which require to execute several"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "different models. Instead, we aim towards a model that only implicitly utilises the linguistic information stream during"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "deployment, and does not require access to ASR and NLP frontends."
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "∗Citation: Wagner, J., Triantafyllopoulos, A., Wierstorf, H., Schmitt, M., Burkhardt, F., Eyben, F., & Schuller, B. W."
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "(2023). Dawn of the transformer era in speech emotion recognition: closing the valence gap. IEEE Transactions on Pattern"
        },
        {
          "Keywords Affective Computing · Speech Emotion Recognition · Transformers.": "Analysis and Machine Intelligence. 10.1109/TPAMI.2023.3263585"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Although the field has seen tremendous progress in the last decades [1], three major challenges remain for real-world"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "paralinguistics-based SER applications: a) improving on its inferior valence performance [7, 9], b) overcoming issues"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "of generalisation and robustness [10, 11], and c) alleviating individual- and group-level fairness concerns, which is a"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "prerequisite for ethical emotion recognition technology [12, 13]. Previous works have attempted to tackle these issues"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "in isolation, but combining them is not straightforward."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "In recent years, the artificial intelligence (AI) field is undergoing a major paradigm shift, moving from specialised ar-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "chitectures trained for a given task to general-purpose foundation models that can be adapted to several use-cases [14]."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Such models have seen tremendous success in computer vision [15, 16], NLP [17], and computer audition [18, 19],"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "including SER [20, 21]. Among others, wav2vec 2.0 [18] and HuBERT [19] have emerged as foundation model can-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "didates for speech-related applications. We evaluate several publicly-available pre-trained variants of those models for"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "dimensional SER, and show that they can achieve state-of-the art results for valence. We further analyze the influence"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "of the model architecture,\nthe pre-training data, how well\nthe models generalise,\ntheir robustness, fairness, and effi-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ciency. Moreover, we make our best performing model publicly available [22]. To our best knowledge this is the first"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "transformer-based dimensional SER model released to the community. For an introduction on how to use it, please"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "visit: https://github.com/audeering/w2v2-how-to."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "The remainder of this paper is organised as follows. Section 2 discusses related work, Section 3 presents the models,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "databases, and evaluation methods.\nSection 4 shows the results and investigates why transformer models are able"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "base (b) or large (L) architecture was used as well as whether the pre-trained model was fine-tuned for speech recog-"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "nition (FT-SR). The column FT-D marks if"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "1"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "2"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "3"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "4"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "5"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "6"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "7"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "8"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "9"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "10"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "11"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "12"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "13"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "14"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "15"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "16"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "17"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "18"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "19"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "20"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "21"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": "22"
        },
        {
          "tures ranked by unweighted average recall (UAR) / weighted average recall (WAR). The table encodes whether the": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dawn of the transformer era in speech emotion recognition": "The focus of our work is the recognition of emotional dimensions. However, most related studies target emotional",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "categories. Since the approaches are closely related, we consider both in this section.",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "In Table 1, we provide a summary of recent works based on wav2vec 2.0 and HuBERT on the IEMOCAP dataset [29],",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "on which most prior works have focused. Results are ranked by unweighted average recall (UAR) / weighted average",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "recall (WAR) on the four emotional categories of anger (1103 utterances), happiness (+ excitement) (1636), sadness",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "leave-one-session-out cross validation (5 folds), except Yuan et al. [24], using leave-one-speaker-out cross validation",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "(10 folds), and Wang et al. [20], who do not explicitly mention which folds they used. Even though authors have used",
          "PREPRINT": ""
        },
        {
          "Dawn of the transformer era in speech emotion recognition": "different head architectures and training procedures in their studies, we can draw some general observations:",
          "PREPRINT": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "leave-one-session-out cross validation (5 folds), except Yuan et al. [24], using leave-one-speaker-out cross validation"
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "(10 folds), and Wang et al. [20], who do not explicitly mention which folds they used. Even though authors have used"
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "different head architectures and training procedures in their studies, we can draw some general observations:"
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "1. Fine-tuning pre-trained weights yields a 10% boost."
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": ""
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": ""
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "can be quite small (e. g. row 19 vs row 20 +.5% )."
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "4. HuBERT outperforms wav2vec 2.0 (e. g. row 22 vs row 20: +2.1%)."
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": ""
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": ""
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": ""
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "encoder-decoder model proposed by Makiuchi et al."
        },
        {
          "(1084), and neutral (1708), which is the typical categorical SER formulation for IEMOCAP. Most of the works apply": "trained model during the down-stream task."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "comprised of two architecture designs (wav2vec 2.0 and HuBERT), each with two different variants (base and large)."
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "For each model, we list included dataset(s), total number of hours (h), number of languages (eng if only English), and"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "covered domains (Read speech, Telephone conversions, Parliamentary speech, Youtube)."
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "Model"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "w2v2-b [18]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "hubert-b [19]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "w2v2-L [18]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "hubert-L [19]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "w2v2-L-robust [38]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "w2v2-L-vox [39]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": "w2v2-L-xls-r [40]"
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        },
        {
          "Table 2: Transformer-based models included in this study and details on the data used during pre-training. Models": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fine-tune\nFreeze\nWAV": "Figure 1: Proposed architecture built on wav2vec 2.0 / HuBERT."
        },
        {
          "Fine-tune\nFreeze\nWAV": "We compare results to a 14-layer Convolutional Neural Network (CNN14) as a standard baseline we have been using"
        },
        {
          "Fine-tune\nFreeze\nWAV": "for SER in previous work [9, 42]. It follows the architecture proposed by Kong et al. [43] for audio pattern recognition."
        },
        {
          "Fine-tune\nFreeze\nWAV": "Different to the transformer-based models, which operate on the raw audio signal, this takes log-Mel spectrograms as"
        },
        {
          "Fine-tune\nFreeze\nWAV": "input. CNN14 has 6 convolutional blocks with two layers each, each followed by max pooling. Convolution layers"
        },
        {
          "Fine-tune\nFreeze\nWAV": "have a 3 × 3 kernel and a stride of 1 × 1, whereas max pooling layers use a stride of 2 × 2. After the last convolution"
        },
        {
          "Fine-tune\nFreeze\nWAV": "layer,\nfeatures are pooled using both mean and max pooling, and subsequently fed into two linear layers. Dropout"
        },
        {
          "Fine-tune\nFreeze\nWAV": "with a probability of 0.2 is applied after every each convolution block. Log-Mel spectrograms are computed with 64"
        },
        {
          "Fine-tune\nFreeze\nWAV": "Mel bins, a window size of 32 ms, and a hop size of 10 ms.\nNote that\nthe CNN14 model\nis not pre-trained,\ni. e.\nit\nis"
        },
        {
          "Fine-tune\nFreeze\nWAV": "always trained from scratch in our experiments. We train for 60 epochs, with a learning rate of .01, and a batch size of"
        },
        {
          "Fine-tune\nFreeze\nWAV": "64 using stochastic gradient descent (SGD) with a Nesterov momentum of .9. We select the model that performs best"
        },
        {
          "Fine-tune\nFreeze\nWAV": "on the validation set."
        },
        {
          "Fine-tune\nFreeze\nWAV": "3.3\nDatasets"
        },
        {
          "Fine-tune\nFreeze\nWAV": "We used the MSP-Podcast corpus [31] (v1.7) to run multitask training on the three dimensions of arousal, dominance,"
        },
        {
          "Fine-tune\nFreeze\nWAV": "and valence for speech from podcast recordings. The original\nlabels cover a range from 1 to 7, which we map into"
        },
        {
          "Fine-tune\nFreeze\nWAV": "the interval of 0 to 1.\nIts train split contains 62 hours of recordings.\nIn-domain results are reported on the test-1 split,"
        },
        {
          "Fine-tune\nFreeze\nWAV": "which contains 21 hours of audio provided by 12, 902 samples (54% female / 46% male) from 60 speakers (30 female"
        },
        {
          "Fine-tune\nFreeze\nWAV": "/ 30 male). The samples per speaker vary between 42 and 912."
        },
        {
          "Fine-tune\nFreeze\nWAV": "We report cross-domain results IEMOCAP (Interactive Emotional Dyadic Motion Capture) dataset [29], which con-"
        },
        {
          "Fine-tune\nFreeze\nWAV": "tains 12 hours of scripted and improvised dialogues by ten speakers (5 female / 5 male).\nIt provides the same dimen-"
        },
        {
          "Fine-tune\nFreeze\nWAV": "sional labels as MSP-Podcast, but in a range of 1 to 5, which we map to the interval 0 to 1. Since we use the dataset"
        },
        {
          "Fine-tune\nFreeze\nWAV": "only during evaluation, we do not apply the usual speaker cross-validation, but treat the corpus as a whole. It includes"
        },
        {
          "Fine-tune\nFreeze\nWAV": "10, 039 samples (49% female / 51% male)."
        },
        {
          "Fine-tune\nFreeze\nWAV": "Finally, we report cross-corpus results for valence on the test set of\nthe Multimodal Opinion Sentiment\nIntensity"
        },
        {
          "Fine-tune\nFreeze\nWAV": "(MOSI) [44] corpus. The dataset\nis a collection of YouTube movie review videos spoken by 41 female and 48 male"
        },
        {
          "Fine-tune\nFreeze\nWAV": "speakers. They are annotated for sentiment on a 7-point Likert scale ranging from −3 to 3, which we map to the interval"
        },
        {
          "Fine-tune\nFreeze\nWAV": "0 to 1. The test set contains 1 hour audio recordings given as 685 samples (51% female / 49% male), annotated for"
        },
        {
          "Fine-tune\nFreeze\nWAV": "sentiment. As the gender labels are not part of the distributed database, we re-annotated them ourselves [45]."
        },
        {
          "Fine-tune\nFreeze\nWAV": "While sentiment\nis a different concept\nthan valence, as the former corresponds to an attitude held towards a specific"
        },
        {
          "Fine-tune\nFreeze\nWAV": "object and the latter more generally characterises a person’s feeling [46], there is evidence that sentiment annotations"
        },
        {
          "Fine-tune\nFreeze\nWAV": "can be decomposed to two constituents:\nintensity and polarity [47], which roughly correspond to arousal and valence."
        },
        {
          "Fine-tune\nFreeze\nWAV": "We therefore expect some correlation between (predicted) valence and (annotated) sentiment scores."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "3.4\nEvaluation"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Machine learning models for speech emotion recognition are expected to work under different acoustic conditions and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "for different speakers. To cover this, we evaluate them for correctness, robustness, and fairness [48]."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Correctness measures how well predictions match the ground truth. The concordance correlation coefficient (CCC)"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "provides an estimate of how well\nthe predicted distribution matches the ground truth one [49], and is the typical"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "measure for evaluating dimensional SER models [50]."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Robustness (cf. Section 4.8) measures how model performance is affected by changes to the input signals such as"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "adding background noise. Applying changes to the input signals must be carefully done for SER, as they might affect"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the ground truth label [33, 51]. We focus on testing the robustness of the models against data augmentations that do not"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "change the human perception of the underlying emotion. We select the following five augmentations from Jaiswal and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Provost [33] to enable direct comparison with previous results: Natural soundscapes adds a randomly selected sample"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "from the natural class of the ESC-50 dataset [52] with a signal-to-noise ratio (SNR) of 0 dB, 10 dB or 20 dB; Human,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "non-speech adds a randomly selected sample from the human class of the ESC-50 dataset with a SNR of 0 dB, 10 dB"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "or 20 dB; Interior/domestic adds a randomly selected sample from the interior class of the ESC-50 dataset with a SNR"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "of 0 dB, 10 dB or 20 dB; Speed up segment selects a random segment of 10% to 20% length within the utterance and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "increases its speed by 1.25; Fade-in/fade-out decreases or increases the amplitude of the signal by 2% every second."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Fairness (cf. Section 4.9) evaluates if\nthe model predictions show biases for certain protected attributes like race,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "gender, or age [53]. We focus on gender due to the lack of sufficient available information and/or datasets for other"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "attributes.\nFor\nregression problems,\nthere is no clear definition how to measure fairness, but most approaches try"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "to achieve an equal average expected outcome for population A and B [54]. We measure fairness by estimating the"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "gender fairness score as the difference in the correctness metric (CCC) between female and male groups. A positive"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "gender fairness score indicates a better performance of the model for female speakers."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".482",
          "0.5\n1.0\n0.0\n.564": ".629",
          "0.5\n1.0\n.658": ".717"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.027",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".532",
          "0.5\n1.0\n0.0\n.564": ".621",
          "0.5\n1.0\n.658": ".711"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.057",
          "0.2\n0.4\nw2v2-b": "hubert-b",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".421",
          "0.5\n1.0\n0.0\n.564": ".633",
          "0.5\n1.0\n.658": ".725"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "Arousal\n-.061",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".636",
          "0.5\n1.0\n0.0\n.564": ".655",
          "0.5\n1.0\n.658": ".739"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.049",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".635",
          "0.5\n1.0\n0.0\n.564": ".634",
          "0.5\n1.0\n.658": ".745"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.065",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "w2v2-L",
          "0.0\n0.5\n1.0\n0.0\n.248": ".448",
          "0.5\n1.0\n0.0\n.564": ".650",
          "0.5\n1.0\n.658": ".739"
        },
        {
          "of the fine-tuned transformer models.": "Details: To evaluate whether adding linguistic information improves the predictions,",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.028",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".359",
          "0.5\n1.0\n0.0\n.564": ".632",
          "0.5\n1.0\n.658": ".723"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.054",
          "0.2\n0.4\nw2v2-b": "hubert-L",
          "0.0\n0.5\n1.0\n0.0\n.248": ".259",
          "0.5\n1.0\n0.0\n.564": ".272",
          "0.5\n1.0\n.658": ".431"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".409",
          "0.5\n1.0\n0.0\n.564": ".488",
          "0.5\n1.0\n.658": ".602"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.044",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "conducted: a regression head is pre-trained, using as input pooled BERT embeddings in addition to the pooled states",
          "(ft) models from Figure 2.": "whereupon a two-layer feed-forward neural network is trained. We show the difference to results with the fine-tuned",
          "0.0\n-.023": "Figure 3: Text & audio fusion results for arousal, dominance, and valence prediction on MSP-Podcast. Embeddings",
          "0.2\n0.4\nw2v2-b": "evaluated on its test set (in-domain), as well as to the test set of MOSI and the entire IEMOCAP dataset (cross-corpus).",
          "0.0\n0.5\n1.0\n0.0\n.248": ".413",
          "0.5\n1.0\n0.0\n.564": ".487",
          "0.5\n1.0\n.658": ".630"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.106",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".384",
          "0.5\n1.0\n0.0\n.564": ".478",
          "0.5\n1.0\n.658": ".654"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "Dominance\n-.101",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".425",
          "0.5\n1.0\n0.0\n.564": ".465",
          "0.5\n1.0\n.658": ".639"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.066",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "w2v2-L-robust",
          "0.0\n0.5\n1.0\n0.0\n.248": ".448",
          "0.5\n1.0\n0.0\n.564": ".518",
          "0.5\n1.0\n.658": ".663"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.111",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": "",
          "0.5\n1.0\n0.0\n.564": "",
          "0.5\n1.0\n.658": ""
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".412",
          "0.5\n1.0\n0.0\n.564": ".496",
          "0.5\n1.0\n.658": ".658"
        },
        {
          "of the fine-tuned transformer models.": "",
          "(ft) models from Figure 2.": "",
          "0.0\n-.023": "-.087",
          "0.2\n0.4\nw2v2-b": "",
          "0.0\n0.5\n1.0\n0.0\n.248": ".399",
          "0.5\n1.0\n0.0\n.564": ".496",
          "0.5\n1.0\n.658": ".657"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Original files\nSynthetic files\nSynthetic files (frz)"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Figure 4: CCC performance for valence on the original and synthetic files on MSP-Podcast. We see that models with"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "a high performance on the original files are more sensitive to sentiment (cf.\nleft and center section). To prove that a"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "fine-tuning of\nthe transformer layers is required to learn linguistic content, we additionally show the correlation for"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "models where the transformer layers were frozen (frz) during training (cf. Section 4.4)."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "In Figure 3, we report deviations from the results achieved with the fine-tuned acoustic models alone (cf. Figure 2)."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "We can see that a fusion with embeddings from the text domain helps with valence, but not with arousal and domi-"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "nance, where performance actually deteriorates. This is in line with our previous findings, where we also found that"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "introducing linguistic information sometimes hampered performance for those two dimensions on MSP-Podcast [9]."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "What\nis interesting,\nthough, are the relatively large differences between the models, and that, especially, our best"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "models hubert-L and w2v2-L-robust do not\nimprove. The models that benefit most are the two multi-lingual models"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "w2v2-L-vox and w2v2-L-xls-r, showing that models pre-trained on multiple languages gain from a fusion with text"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "features from the test set domain language."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "4.3\nDo the models implicitly learn linguistic information?"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Answer: The models implicitly capture linguistic information from the audio signal. The extent\nin which they learn"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "sentiment during fine-tuning depends on the data used for pre-training (e. g. multi-lingual data makes it more difficult)."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Generally, we see that valence performance correlates with a model’s ability to predict sentiment."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Details: Previous findings suggest that during fine-tuning, the models implicitly learn linguistic information. To asses"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "how sensitive the models are to linguistic content, we generated a synthesised version of a subset of the test set from"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "the transcriptions of MSP-Podcast.3\nIn Figure 4, we finally show CCC performance for valence on the original and"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "synthesised files for all models. We see that performance gaps between the models in Figure 2 are directly linked with"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "their ability to predict sentiment. Models reaching a high performance on the original files also do so on their synthetic"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "versions and vice versa. However, to learn linguistic content, a fine-tuning of the transformer layers is essential. If we"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "predict\nthe synthetic test set with models where the transformer layers were frozen during training (cf. Section 4.4),"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "correlation drops to almost zero."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "This finding is also important\nfor works doing in-domain training on IEMOCAP, as parts of\nthe conversations are"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "scripted which results in a leakage of\ntext\ninformation that may result\nin overoptimistic results [56] when that\ntext"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "information is exploited by transformer models. Furthermore, our models may inherit similar biases as those found in"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "NLP models [57]."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "4.4\nHow important is a fine-tuning of the transformer layers?"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Answer: Fine-tuning the transformer layers is necessary to obtain state-of-the-art performance,\nin particular for the"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "valence dimension. The highest gain is observed for hubert-L and w2v2-L-robust, which are the models that benefit"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "the least from a fusion with text."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "Details: So far, we have fine-tuned all\ntransformer layers along with the added output\nlayer. However, practitioners"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "often choose to use a pre-trained model as a frozen feature extractor, and subsequently train just an output layer on the"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "generated embeddings. Nevertheless, prior studies have shown that it is necessary to fine-tune several or all layers on"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "the target task to get good downstream performance [20, 42, 43, 58].\nIn this sub-section, we experiment with training"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "only the last output layer and keeping all others frozen. This is compared to our previous experiments where we jointly"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "fine-tune the last layer and the transformer layers."
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "3Partial audio transcripts are available with MSP-Podcast v1.9 and cover 55% of\nthe test-1 split\nfrom v1.7 we used for our"
        },
        {
          "-.003\n.045\n.031\n-.003\n.006\n.071\n-.021\n.100\n.021\n.007\n-.001\n0.0": "experiments."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Arousal\nDominance\nValence"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Figure 5: Difference of fine-tuned (ft) to frozen (frz) CCC performance for arousal, dominance, and valence prediction"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "on MSP-Podcast. The fine-tuned results are from Figure 2, where transformer and output\nlayers are jointly trained."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "For the frozen results, we keep all transformer layers frozen and simply train the output head. Results show that fine-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "tuning the transformer layer is worth the computational cost it incurs."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Figure 5 shows the difference between CCC values for the fine-tuned and frozen models. We observe performance"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "gains when fine-tuning in all cases, demonstrating that fine-tuning of the transformer layers is necessary. Moreover,"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "the models that see the biggest performance gain are hubert-L and w2v2-L-robust.\nIn Section 4.2, these models were"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "found to benefit\nless from additional\ntext\ninformation. These findings indicate that a fine-tuning of the transformer"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "layers enables the models to capture the linguistic information needed to perform well on valence."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "4.5\nDo the models generalise better across different domains?"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Answer: Transformer-based models generalise better than a non-transformer baseline."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Details: As we see a similar\ntrend for different\ntransformer-based models between in-domain and cross-corpus re-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "sults in Figure 2, we focus on the best-performing one (w2v2-L-robust). The drop in CCC between in-domain and"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "cross-corpus results for w2v2-L-robust on IEMOCAP is 11% for arousal, 21% for dominance, and 30% for valence"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "on IEMOCAP, and 15% for sentiment on MOSI. For CNN14, the drop in CCC is 34% for arousal, and 52% for domi-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "nance, while for valence, we do not estimate the drop in cross-domain performance as the in-domain CCC is already"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "very low. The drop in CCC is smaller for w2v2-L-robust for arousal and dominance, indicating that transformer-based"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "models generalise better. For valence, we cannot derive a final conclusion, but the trend we see for sentiment in MOSI"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "seems very promising."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "4.6\nDoes more data during pre-training lead to better performance?"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Answer: For arousal and dominance, all\ntested models perform equally well, whereas with respect\nto valence / sen-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "timent\nthe data used for pre-training has a strong effect. Mixing data from several domains leads to a considerable"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "improvement\nfor w2v2-L-robust compared to w2v2-L, which is only trained on clean speech. However, hubert-L,"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "which uses the same pre-training data as w2v2-L, which is also trained on clean speech, still performs as good as"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "w2v2-L-robust.\nFor models pre-trained on multi-lingual data, we see a performance drop when tested on English"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "speech."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "Details: To understand what influence the size and domain of the pre-training data have on downstream performance,"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "we included several wav2vec 2.0 models with same large architecture but different pre-training (see Table 2)."
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "The results in Figure 2 show only differences in terms of CCC between the transformer models for valence and senti-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "ment, not for arousal or dominance. Previous studies uniformly report that HuBERT outperforms wav2vec 2.0 which"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "is replicated by our results with w2v2-b showing a smaller CCC than hubert-b for the valence task on MSP-Podcast"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "and IEMOCAP, and for the sentiment\ntask on MOSI. The increase in performance for w2v2-L-robust compared to"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "hubert-L is most\nlikely explained by the additional 3k hours of telephone conversations used for pre-training. How-"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "ever, by comparing w2v2-L-vox and w2v2-L-xls-r,\nit also becomes clear that more data does not necessarily lead to"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "better\nresults.\nThough both models are trained on significantly more data than hubert-L and w2v2-L-robust\n(100k"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "and 463k vs 63k hours),\nthey perform clearly worse. Notably, both were pre-trained on multiple languages. Since"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "the databases we use for evaluation contain only English speakers,\nthis could be a disadvantage to models that are"
        },
        {
          "CCCft\n.047\n.034\n.042\n.034\n0.0": "exclusively pre-trained on English."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Arousal"
        },
        {
          "CNN14": "1.0",
          "w2v2-b": ".505\n.618\n.604",
          "hubert-b": ".654\n.677\n.659",
          "w2v2-L": "",
          "hubert-L": ".538\n.559\n.599",
          "w2v2-L-robust": ".616\n.595\n.599",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.5",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": ".442",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Dominance"
        },
        {
          "CNN14": "1.0",
          "w2v2-b": ".541\n.540",
          "hubert-b": ".584\n.591\n.582",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "CCC\n0.5",
          "w2v2-b": ".467",
          "hubert-b": "",
          "w2v2-L": ".283",
          "hubert-L": ".417\n.429\n.430",
          "w2v2-L-robust": ".462\n.439\n.444",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Valence / sentiment"
        },
        {
          "CNN14": "1.0",
          "w2v2-b": "",
          "hubert-b": ".535\n.552",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.5",
          "w2v2-b": ".171\n.373\n.378",
          "hubert-b": ".378",
          "w2v2-L": ".201",
          "hubert-L": ".300\n.281\n.267",
          "w2v2-L-robust": ".367\n.331\n.295",
          "w2v2-L-vox": ".079\n.218\n.304\n.141",
          "w2v2-L-xls-r": ".386\n.439\n.253\n.131"
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "MOSI",
          "w2v2-L-xls-r": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "0.0"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "MSP-Podcast\nIEMOCAP\nMOSI"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Figure 6: CCC scores for arousal, dominance, valence (MSP-Podcast/IEMOCAP), and sentiment (MOSI) when aug-"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "menting the test data. The scores are averaged over all eleven different augmented versions of the test data."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "4.7\nDoes a larger architecture lead to better performance?"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Answer: A larger architecture does not\nlead to better performance per se. Larger architectures using different data"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "during pre-training might perform worse than smaller architectures."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Details: We can draw some conclusions about the influence that the size of the architecture has on performance based"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "on Figure 2. The size of the architecture,\ni. e. base vs large, seems not\nto be the decisive point: w2v2-b and w2v2-L"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "show very similar performance.\nIn addition,\nthe small models w2v2-b and hubert-b have comparable performance"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "to the large models w2v2-L, w2v2-L-vox, and w2v2-L-xls-r for arousal and dominance, both in- and cross-domain."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "For valence, the small models outperform w2v2-L, w2v2-L-vox, and w2v2-L-xls-r in most cases for MSP-Podcast and"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "MOSI, and achieve a similar performance on IEMOCAP."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "4.8\nAre the models robust against changes to the input signals?"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Answer: The tested models are reasonably robust against changes to the input signals, with w2v2-L-robust showing"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "the highest and hubert-b the lowest robustness."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Details:\nFigure 6 summarises\nthe average CCC scores of\nthe models averaged over all augmentations described"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "in Section 4.8.\nAll models show a drop in CCC compared to the CCC scores for\nthe clean data from Figure 2."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "w2v2-L-robust has now the highest CCC score for all datasets and all dimensions. The average change in CCC for"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "w2v2-L-robust\nis −0.068. The model with the highest average change in CCC is hubert-b (−0.108). The model with"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "the lowest average change in CCC is CNN14 (−0.047), which is mostly due to its results for IEMOCAP for which it"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "shows no impairment of its relatively low performance by augmentations."
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "Table 3 shows changes in CCC for single augmentations for each dataset and dimension for the best performing model"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "w2v2-L-robust. The performance of\nthe model\nis only sligthly affected (absolute change in CCC score below .05)"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "for added background sounds with a SNR of 20 dB or a fade-in/fade-out of the signal. When speeding up parts of"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "the signal or adding background sounds with more severe SNRs the change in CCC can be up to −.278. The model"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "investigated on the same augmentations by Jaiswal and Provost [33] shows an equal drop in unweighted average recall"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "(UAR) when adding background sounds with 0 dB, 10 dB, 20 dB SNR of at least −.30. w2v2-L-robust is more robust"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "when adding background sounds with a moderate SNR. It shows a drop in CCC of up to to −.28 for 0 dB SNR, but only"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "a drop in CCC of up to −.036 for 20 dB SNR. Whereas the model investigated by Jaiswal and Provost [33] is similar"
        },
        {
          ".171\n.201\n.079\n.218\n.141\n.253\n.131\n0.5": "affected by adding human, non-speech, interior/domestic, or natural sounds as background sounds, w2v2-L-robust\nis"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Augmentation"
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Natural soundscapes"
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Human, non-speech"
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Interior/domestic"
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": ""
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Speed up segment"
        },
        {
          "Table 3: Change in CCC for w2v2-L-robust predictions on augmented data compared to its predictions on clean data.": "Fade-in/fade-out"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Arousal"
        },
        {
          "CNN14": "0.2",
          "w2v2-b": ".020\n-.003",
          "hubert-b": "-.017\n-.028\n-.016",
          "w2v2-L": "",
          "hubert-L": "-.010\n-.012\n-.005",
          "w2v2-L-robust": "-.004\n.002\n-.003",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.2",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Dominance"
        },
        {
          "CNN14": "0.2",
          "w2v2-b": "-.020\n-.025",
          "hubert-b": "-.016\n-.015\n-.032",
          "w2v2-L": "",
          "hubert-L": "-.027\n-.024\n-.028",
          "w2v2-L-robust": "-.020\n-.028\n-.032",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.2",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Valence / sentiment"
        },
        {
          "CNN14": "0.2",
          "w2v2-b": ".054\n.083",
          "hubert-b": ".062\n.028\n.038",
          "w2v2-L": "",
          "hubert-L": ".020\n.029\n.032",
          "w2v2-L-robust": ".018\n.014\n.022",
          "w2v2-L-vox": "-.203\n.040\n-.000\n-.045\n.036",
          "w2v2-L-xls-r": ".020\n-.030\n.019"
        },
        {
          "CNN14": "0.0",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.2",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "MOSI",
          "w2v2-L-xls-r": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Arousal"
        },
        {
          "CNN14": "1.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.75",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.50",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.25",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Dominance"
        },
        {
          "CNN14": "1.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.75",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "Speaker-level CCC\n0.50",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.25",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": "Valence"
        },
        {
          "CNN14": "1.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.75",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.50",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.25",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "0.00",
          "w2v2-b": "",
          "hubert-b": "",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "",
          "w2v2-L-vox": "",
          "w2v2-L-xls-r": ""
        },
        {
          "CNN14": "",
          "w2v2-b": "7",
          "hubert-b": "931 968 170 159 867 160 151 138 148",
          "w2v2-L": "",
          "hubert-L": "",
          "w2v2-L-robust": "8\n131\n94\n13",
          "w2v2-L-vox": "152\n22\n59",
          "w2v2-L-xls-r": "130\n23"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "7\n931 968 170 159 867 160 151 138 148\n8\n131\n94\n13\n152\n22\n59\n130\n23"
        },
        {
          "0.00": "Speaker ID"
        },
        {
          "0.00": "for the different models. We only use speakers with at\nFigure 8: Speaker-level performance (CCC) on MSP-Podcast"
        },
        {
          "0.00": "least 200 test set samples for robust CCC estimates. All models show low CCC for at least one speaker on all 3 tasks."
        },
        {
          "0.00": "Speakers have been ordered according to the mean CCC over all dimensions and models."
        },
        {
          "0.00": "For valence in MSP-Podcast and IEMOCAP, most models show a better CCC for female speakers than male ones –"
        },
        {
          "0.00": "with the exception of CNN14. For sentiment in MOSI, the CNN14 model shows a bias towards better performance for"
        },
        {
          "0.00": "male speaker, whereas all other models show very small biases in the different direction."
        },
        {
          "0.00": "Averaging over all databases and dimensions the model with the best gender\nfairness score is w2v2-L with .007,"
        },
        {
          "0.00": "followed by w2v2-L-vox with .015, w2v2-L-xls-r with .018, w2v2-L-robust, with .019, hubert-b with .025, hubert-L"
        },
        {
          "0.00": "with .027, and w2v2-b with .029 up to CNN14 with −.043."
        },
        {
          "0.00": "4.10\nIs performance equal across all speakers?"
        },
        {
          "0.00": "Answer:\nPerformance for\nthe best\nfoundation models is similar between most speakers in MSP-Podcast, but can"
        },
        {
          "0.00": "deteriorate to low CCC values for some speakers."
        },
        {
          "0.00": "Details: The performance of speech processing is dependent on individual speaker characteristics [37]. This has led"
        },
        {
          "0.00": "several prior SER works to target personalisation to different speakers [59, 60, 61]. To investigate this phenomenon"
        },
        {
          "0.00": "for transformer-based models, we examine the per-speaker performance, where instead of computing a global CCC"
        },
        {
          "0.00": "value over all\ntest set values, we compute one for each speaker. As discussed (cf. Section 3.3),\nthe MSP-Podcast"
        },
        {
          "0.00": "test set consists of 12902 samples from 60 speakers; however,\nthe samples are not equally distributed across them"
        },
        {
          "0.00": "(minimum samples: 41, maximum samples 912). In order to make our subsequent analysis more robust, we only keep"
        },
        {
          "0.00": "speakers with more than 200 samples,\nresulting in 19 speakers. We use bootstrapping, where we randomly sample"
        },
        {
          "0.00": "(with replacement) 200 samples from each speaker to compute the CCC. This process is repeated 1000 times, and we"
        },
        {
          "0.00": "report the mean value."
        },
        {
          "0.00": "Our\nresults are presented in Figure 8.\nFor visualisation purposes, we ordered speakers based on the average CCC"
        },
        {
          "0.00": "value over all models and across arousal, dominance, and valence. For arousal and dominance models perform well"
        },
        {
          "0.00": "for most speakers, and show similar performance. For speakers 7 and 931 all models show a low CCC, whereas for"
        },
        {
          "0.00": "speaker 931 the CNN14 model performs worse than the others. For valence, CCC values per speaker differ between"
        },
        {
          "0.00": "models replicating the findings of Figure 2. The best model (w2v2-L-robust) performs relatively similar for most of"
        },
        {
          "0.00": "the speaker groups and shows only a drop for speaker 7, a similar result as for valence and dominance."
        },
        {
          "0.00": "Different models broadly, but not perfectly, agree on ‘good’ and ‘bad’ speakers, with pairwise Spearman correlations"
        },
        {
          "0.00": "ranging from .960 to .725 for arousal,\n.972 to .825 for dominance, and .947 to .333 for valence. This could be a"
        },
        {
          "0.00": "manifestation of the underspecification phenomenon plaguing machine learning architectures [62], as models which"
        },
        {
          "0.00": "have similar performance on the entire test set, nevertheless behave differently across different subsets of it."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": ".745",
          "w2v2-L-w/o-pretrain": ".531",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": ".431",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": ".634",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": ".272\n.381",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": ".635",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": ".259\n.183",
          "w2v2-L-robust": ".046"
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "",
          "w2v2-L-robust": ""
        },
        {
          "CNN14": "",
          "w2v2-L-w/o-pretrain": "IEMOCAP",
          "w2v2-L-robust": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "0.0"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "MSP-Podcast\nIEMOCAP\nMOSI"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "(w2v2-L-w/o-pretrain) on in-domain and\nFigure 9: CCC performance of randomly-initialised wav2vec 2.0 model"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "cross-corpus arousal, dominance, valence / sentiment prediction. We compare the performance with that of CNN14"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "and w2v2-L-robust.\nWe observe\nthat\nvalence and sentiment benefit massively\nfrom pre-training, without which"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "wav2vec 2.0 performs worse than a classic CNN approach."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "4.11\nWhy do foundation models generalise so well?"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "Answer: Even without pre-training,\nthe latent space provided by the transformer architecture generalises better than"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "CNN14, as it abstracts away domain and speaker. Pre-training marginally improves arousal and dominance perfor-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "mance but is critical for valence."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "Details: So far, we were able to confirm the superiority of\ntransformer-based models. However, even though pre-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "training seems important,\nit remains unclear to what extent\nthe transformer architecture itself contributes to that suc-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "cess. To shed more light into this, we trained wav2vec 2.0 from a random initialisation. As our architecture, we chose"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "the large wav2vec 2.0 architecture, which is also used by the best performing model w2v2-L-robust.\nIn the following,"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "we will refer to this model as w2v2-L-w/o-pretrain."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "We trained the model for 50 epochs and selected the best checkpoint according to the performance on the development"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "set (epoch 17).4\nIn Figure 9, we compare in- and cross-domain performance with CNN14 and w2v2-L-robust. We see"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "that especially valence / sentiment detection benefits massively from pre-training (both in-domain and cross-domain),"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "and that without pre-training wav2vec 2.0 performs in most cases worse than CNN14."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "In the introduction of wav2vec 2.0, Baevski et al. [18] postulate that pre-training helps learn more general represen-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "tations that abstract away from speaker or background information. However,\nit\nis not entirely clear if these benefits"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "are a result of pre-training or are a consequence of the specific inductive biases introduced by the architecture. To"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "investigate this, we compare embeddings extracted with CNN14, w2v2-L-w/o-pretrain, and w2v2-L-robust,5 which are"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "shown in Figure 10. The embeddings are projected to two dimensions using t-SNE [63] and different\ninformation is"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "chromatically superimposed."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "For CNN14, two main clusters almost perfectly separate the two data sources MSP-Podcast and IEMOCAP, whereas"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "several smaller blobs represent gender groups and individual speakers.\nIn fact, speaker and domain are more pro-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "nounced than valence information. Hence, similar emotional content can translate into entirely different latent repre-"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "4Even though we used the same data (MSP-Podcast) for fine-tuning, we expected it would take longer for the model to convert"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "if we start from scratch. Also,\nthis time we trained all encoder layers (including the CNN ones). Apart from that we followed the"
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "methodology described in Section 3.2."
        },
        {
          ".248\n.123\n.183\n.046\n.015\n0.5": "5We use average pooling on the output of the last CNN layer for CNN14 and the last transformer layer for wav2vec 2.0."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "High": "Figure 10: Visualisation of embeddings extracted with different models overlayed with meta information for a com-"
        },
        {
          "High": "bined dataset of MSP-Podcast and IEMOCAP. We observe that the latent space of wav2vec 2.0 offers a better abstrac-"
        },
        {
          "High": "tion from domain, gender, and speaker compared to the CNN14 baseline – even without pre-training. However, only"
        },
        {
          "High": "a pre-trained model\nis able to separate low from high valence. To reduce the dimensionality of\nthe latent space, we"
        },
        {
          "High": "applied T-SNE [63]."
        },
        {
          "High": "sentations. In contrast, the latent space of both wav2vec 2.0 models shows no clusters for domain, gender, or speaker."
        },
        {
          "High": "The architecture itself seems to introduce specific inductive biases which are well-suited to learning robust represen-"
        },
        {
          "High": "tations. Nevertheless, only the pre-trained model (w2v2-L-robust) shows a smooth transition from low to high valence"
        },
        {
          "High": "scores, showing that pre-training is still necessary for good downstream performance. Moreover,\nthe strong speaker"
        },
        {
          "High": "dependency presented in Section 4.10 shows that\nthe two dimensional\nt-SNE visualisations help comparing general-"
        },
        {
          "High": "isation abilities between models, but are not necessarily sufficient for deriving conclusions w. r. t. generalisation over"
        },
        {
          "High": "different factors."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.6\n w2v2-b"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "(5 epochs)"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.5"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "CCC\n  CNN14"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "(60 epochs)"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.4"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.3"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Epoch"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Figure 11: Mean and standard deviation of development set performance on MSP-Podcast across three training runs."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Compared to CNN14, w2v2-b converges earlier and shows less fluctuation."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "6\n8\n10\n12\n24 (original)"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Arousal"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": ".730\n.721\n.734\n.744\n.745\n.644\n.645\n.645\n.660\n.663\n1.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.5"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Dominance"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": ".614\n.648\n.647\n.655\n.634\n.496\n.502\n.518\n1.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "CCC\n.441\n.486\n0.5"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Valence / sentiment"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": ".598\n.638\n.635\n.540\n.539\n1.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": ".416\n.450\n.431\n.371\n.462\n.478\n.448\n.218\n.292\n.478\n0.5"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "0.0"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "MSP-Podcast\nIEMOCAP\nMOSI"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Figure 12: CCC scores for arousal, dominance, and valence / sentiment for w2v2-L-robust and pruned versions. The"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "legend shows the number of bottom layers kept during fine-tuning. We see that half of\nthe layers can be removed"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "without any loss in performance."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "5.2\nHow many transformer layers do we really need?"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Answer: We can reduce the number of transformer layers to 12 without a degradation in performance. With less than"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "12 layers we begin to see a negative effect on valence."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Details:\nIn Section 4.7, we mentioned that w2v2-b and hubert-b outperform some of the large models. From that,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "we concluded that\nthe size of the architecture seems less important, but\nit\nis rather the data used for pre-training that"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "determines success.\nIf this is really the case, we should be able to partially reduce the size of a model without losing"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "performance."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Sajjad et al. [66] investigated different\nlayer pruning strategies and identified top-layer dropping as the best strategy"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "offering a good trade-off between accuracy and model size.\nInspired by their findings, we set up an experiment"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "where we successively removed transformer layers from the top of the original pre-trained model before fine-tuning."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "In Figure 12, we report\nthe effect on CCC for w2v2-L-robust\n(our overall best performing model).\nResults show"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "that half of\nthe layers can be removed without a loss in performance. We denote the resulting 12-layer model as"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "w2v2-L-robust-12. Only with 10 or less layers we actually begin to see a drop for valence / sentiment on IEMOCAP"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "and MOSI. For arousal and dominance, we still achieve good performance with only 8 layers."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "15"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": "Arousal"
        },
        {
          "5%": ".690\n.674",
          "7.5%": ".725\n.737\n.735",
          "10%": ".744",
          "12.5%": ".587\n.614\n.606",
          "15%": ".625\n.639",
          "25%": ".643\n.660",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": "Dominance"
        },
        {
          "5%": ".603\n.613",
          "7.5%": ".641\n.648\n.647",
          "10%": ".655",
          "12.5%": "",
          "15%": "",
          "25%": ".503",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": ".340\n.388\n.370",
          "15%": ".444\n.466",
          "25%": ".486",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": "Valence / sentiment"
        },
        {
          "5%": "",
          "7.5%": ".558\n.606\n.629",
          "10%": ".638",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": ".498\n.540"
        },
        {
          "5%": ".441\n.480",
          "7.5%": "",
          "10%": "",
          "12.5%": ".360\n.430\n.462",
          "15%": ".440\n.460",
          "25%": ".466\n.478",
          "50%": ".069",
          "75%": ".125\n.291\n.287",
          "100%": ".329\n.427\n.460"
        },
        {
          "5%": "",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": ""
        },
        {
          "5%": "MSP-Podcast",
          "7.5%": "",
          "10%": "",
          "12.5%": "",
          "15%": "",
          "25%": "",
          "50%": "",
          "75%": "",
          "100%": "MOSI"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ".240\n.069\n.125\n0.5": "0.0"
        },
        {
          ".240\n.069\n.125\n0.5": "MSP-Podcast\nIEMOCAP\nMOSI"
        },
        {
          ".240\n.069\n.125\n0.5": "for w2v2-L-robust on sparse training data.\nFigure 13: CCC scores for arousal, dominance, and valence / sentiment"
        },
        {
          ".240\n.069\n.125\n0.5": "The legend shows the fraction of data used for fine-tuning. Please note that steps are not linear."
        },
        {
          ".240\n.069\n.125\n0.5": "5.3\nCan we reduce the training data without a loss in performance?"
        },
        {
          ".240\n.069\n.125\n0.5": "Answer: A reduction of training samples without loss in performance is only possible for arousal and dominance."
        },
        {
          ".240\n.069\n.125\n0.5": "Details: Reducing the amount of\ntraining data offers another way to speed up model building.\nTo find out what"
        },
        {
          ".240\n.069\n.125\n0.5": "effect\nthe removal of training samples has, we conducted an experiment where we fine-tuned several versions of the"
        },
        {
          ".240\n.069\n.125\n0.5": "same pre-trained model with different fractions of the training set (MSP-Podcast). We leave development and test set"
        },
        {
          ".240\n.069\n.125\n0.5": "untouched."
        },
        {
          ".240\n.069\n.125\n0.5": "Figure 13 shows CCC for arousal, dominance, valence / sentiment on MSP-Podcast,\nIEMOCAP and MOSI.\nFor"
        },
        {
          ".240\n.069\n.125\n0.5": "efficiency, we start\nfrom the reduced 12-layer architecture and therefore compare results to w2v2-L-robust-12 (cf."
        },
        {
          ".240\n.069\n.125\n0.5": "Section 5.2). There is no noteworthy degradation for arousal and dominance when keeping close to the entire training"
        },
        {
          ".240\n.069\n.125\n0.5": "set. The only exception is dominance on IEMOCAP, where we achieve best results with just 75% of the data. For"
        },
        {
          ".240\n.069\n.125\n0.5": "these dimensions, however, performance already saturates at 25% yielding a loss of less than .02 on MSP-Podcast,"
        },
        {
          ".240\n.069\n.125\n0.5": "whereas for IEMOCAP, even 12.5% of the training samples seem sufficient to stay within a margin of .05."
        },
        {
          ".240\n.069\n.125\n0.5": "Once again,\nit\nis a different story for valence. For MSP-Podcast, we see a constant\nimprovement\nthat only begins to"
        },
        {
          ".240\n.069\n.125\n0.5": "decrease when reaching 75% of the data. For MOSI, we even see a boost in CCC of almost .1 for the remaining 25%."
        },
        {
          ".240\n.069\n.125\n0.5": "However, in light of our findings from Section 4.3, this does not come as a surprise. Providing more linguistic diversity"
        },
        {
          ".240\n.069\n.125\n0.5": "makes it more likely a model can detect associations between key words and emotional context. What\nis a surprise,"
        },
        {
          ".240\n.069\n.125\n0.5": "though, is that on IEMOCAP, using just 7.5% of the data, results in a drop of less than .05. A possible explanation is"
        },
        {
          ".240\n.069\n.125\n0.5": "that the vocabulary of IEMOCAP does not resemble that of MSP-Podcast and that, therefore, the impact of linguistic"
        },
        {
          ".240\n.069\n.125\n0.5": "information is limited. This would also explain why the differences in valence performance are less pronounced for"
        },
        {
          ".240\n.069\n.125\n0.5": "IEMOCAP (cf. Figure 2)."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "performs substantially worse on all three dimensions, and its embeddings are unable to capture valence information. In"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "addition, pre-training serves as a form of regularisation which helps stabilise the training (Section 5.1), thus resulting"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "in models which require less iterations, and less data to train on (Section 5.3). However, we were unable to determine"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "a clear relationship of the form ‘more pre-training data leads to better performance’. In fact, downstream performance"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "can be negatively impacted by the introduction of more data, as seen by the comparison between w2v2-L-vox and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "w2v2-L-xls-r, which differ only in the fact\nthat w2v2-L-xls-r has been trained on more (and more diverse) data, yet"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "performs worse on all three dimensions."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Generalisation: transformer-based models show very good cross-corpus generalisation (Section 4.6), robustness (Sec-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "tion 4.8), and appear invariant\nto domain, speaker, and gender characteristics (Section 4.11). These are all very im-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "portant\ntraits for any model\nthat\nis intended for production use in realistic environments. However,\nthey seem to"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "stem primarily from the architecture rather than the pre-training as they are also evident\nin models initialised from"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "random weights (Section 4.11). We also showed that several self-attention layers can be removed without hampering"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "downstream performance (Section 5.2), though they might still be necessary for successful pre-training."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Fairness:\nfairness remains a challenging topic for contemporary machine learning architectures. Community discus-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "sions primarily concern the issue of group fairness.\nIn the present, we investigate this for\nthe only group variable"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "available in our datasets: gender (Section 4.9), where we observe that\ntransformer-based architectures are more fair"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "than the CNN14 baseline. However, we argue that individual fairness is important for SER. This refers to how models"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "perform across different speakers; a feat which proves challenging even for the top-performing models investigated"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "here (Section 4.10). We consider this an important topic which has not been sufficiently investigated for SER, though"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "it is long known to impact other speech analysis models [35, 37]."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Integration of linguistic and paralinguistic streams: finally, one of our most intriguing findings is that transformers"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "seem capable of integrating both information streams of the voice signal. This is evident in how well-performing va-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "lence prediction models retain their effectiveness for synthesised speech lacking emotional intonation (Section 4.3) and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "fail to benefit from fusion with explicit textual information (cf. Section 4.2).\nInterestingly,\nthis is only possible when"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "fine-tuning the self-attention layers (Section 4.4), as keeping them frozen results to complete failure for synthesised"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "speech (Section 4.3). This draws attention to an under investigated aspect of fine-tuning, namely, how it qualitatively"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "affects the nature of internal representations. Common understanding sees it as a mechanism through which to obtain"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "better performance, but our analysis shows that it leads to a fundamental change in how the underlying signal is rep-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "resented (moving from almost no sensitivity to linguistic content\nto increased reactivity to it). This mechanism may"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "be crucial in the pursuit of paralinguistic and linguistic integration which is key to a holistic understanding of human"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "communication. However, this integration might prove problematic in cases where the two modalities disagree, e. g. in"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "cases of irony [67]. Our results also highlight that good valence performance might be language dependent as models"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "pre-trained on a variety of languages perform worse for valence compared with comparable models pre-trained only"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "for English (Section 4.1)."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "the coveted integration of the two dominant information streams of spoken language, linguistics, and paralinguistics."
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "8\nReferences"
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "[1]\nB. Schuller, “Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends,” Communications of"
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "the ACM, vol. 61, no. 5, pp. 90–99, 2018."
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "[2]\nP. Ekman, “An argument for basic emotions,” Cognition & emotion, vol. 6, no. 3-4, pp. 169–200, 1992."
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "[3]\nJ. A. Russell and A. Mehrabian, “Evidence for a three-factor theory of emotions,” Journal of research in Personality, vol. 11,"
        },
        {
          "ing in speech emotion recognition:\nthat of pre-trained, transformer-based foundation models, which can finally lead to": "no. 3, pp. 273–294, 1977."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Speech Communication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 3302–3306."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben, and B. W. Schuller, “Multistage linguistic conditioning of"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "convolutional layers for speech emotion recognition,” arXiv preprint arXiv:2110.06650, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller, “Robust speech emotion recognition under different encod-"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "ing conditions,” in Proceedings of the Annual Conference of the International Speech Communication Association (INTER-"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "SPEECH), Graz, Austria: ISCA, 2019, pp. 3935–3939."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "A. Triantafyllopoulos, G. Keren, J. Wagner,\nI. Steiner, and B. W. Schuller, “Towards robust speech emotion recognition"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "using deep residual networks for speech enhancement,” in Proceedings of the Annual Conference of the International Speech"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Communication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 1691–1695."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "A. Batliner, S. Hantke, and B. W. Schuller, “Ethics and good practice in computational paralinguistics,” IEEE Transactions"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "on Affective Computing, 2020."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "J. Cheong, S. Kalkan, and H. Gunes, “The hitchhiker’s guide to bias and fairness\nin facial affective signal processing:"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Overview and techniques,” IEEE Signal Processing Magazine, vol. 38, no. 6, pp. 39–49, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "R. Bommasani et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,”"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "in Proceedings of\nthe International Conference on Machine Learning (ICML), Vienna, Austria (virtual), 2020, pp. 1597–"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "1607."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, “A"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "survey on vision transformer,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "need,” in Advances in Neural Information Processing Systems (NeurIPS), Long Beach, CA, USA, 2017, pp. 5998–6008."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "sentations,” in Advances in Neural Information Processing Systems (NeurIPS), Vancouver, BC, Canada, 2020, pp. 12 449–"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "12 460."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Y\n. Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec 2.0/hubert benchmark for\nspeech emotion recognition,"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "speaker verification and spoken language understanding,” arXiv preprint arXiv:2111.02735, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and B. Schuller, “Survey of deep representation learning for speech emotion"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "recognition,” IEEE Transactions on Affective Computing, vol. 12, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "for Dimensional Speech\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Eyben, and B. W. Schuller, Model"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Emotion Recognition based on Wav2vec 2.0, 2022. DOI: 10.5281/zenodo.6221127."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "D. N. Krishna, “Using large pre-trained models with cross-modal attention for multi-modal emotion recognition,” arXiv"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "preprint arXiv:2108.09669, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "J. Yuan, X. Cai, R. Zheng, L. Huang, and K. Church, “The role of phonetic units in speech emotion recognition,” arXiv"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "preprint arXiv:2108.01132, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "W.-C. Tseng, K.-t. Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H.-y. Lee, Superb: Speech"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "processing universal performance benchmark, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "the\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech using wav2vec 2.0 embeddings,” Proceedings of"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Annual Conference of the International Speech Communication Association (INTERSPEECH), pp. 3400–3404, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "L.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition,” arXiv preprint"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "arXiv:2110.06309, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal emotion recognition with high-level speech and text features,” arXiv"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "preprint arXiv:2111.10202, 2021."
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International": "Interactive emotional dyadic motion capture database,” Language resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "B. T. Atmaja, A. Sasou, and M. Akagi, “Survey on bimodal speech emotion recognition from acoustic and linguistic infor-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "mation fusion,” Speech Communication, 2022."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, “A survey of affect recognition methods: Audio, visual, and spontaneous"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "expressions,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 39–58, 2009."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "R. A. Calvo and S. D’Mello, “Affect detection: An interdisciplinary review of models, methods, and their applications,”"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "IEEE Transactions on Affective Computing, vol. 1, no. 1, pp. 18–37, 2010."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. Kossaifi, R. Walecki, Y. Panagakis, J. Shen, M. Schmitt, F. Ringeval, J. Han, V. Pandit, A. Toisoul, B. Schuller, K. Star,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "E. Hajiyev, and M. Pantic, “SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild,” IEEE"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 3, pp. 1022–1040, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S. Sahu, V. Mitra, N. Seneviratne, and C. Y. Espy-Wilson, “Multi-modal learning for speech emotion recognition: An analysis"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "and comparison of asr outputs with ground truth transcription,” in Proceedings of the Annual Conference of the International"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Speech Communication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 3302–3306."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Triantafyllopoulos, U. Reichel, S. Liu, S. Huber, F. Eyben, and B. W. Schuller, “Multistage linguistic conditioning of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "convolutional layers for speech emotion recognition,” arXiv preprint arXiv:2110.06650, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller, “Robust speech emotion recognition under different encod-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ing conditions,” in Proceedings of the Annual Conference of the International Speech Communication Association (INTER-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "SPEECH), Graz, Austria: ISCA, 2019, pp. 3935–3939."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Triantafyllopoulos, G. Keren, J. Wagner,\nI. Steiner, and B. W. Schuller, “Towards robust speech emotion recognition"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "using deep residual networks for speech enhancement,” in Proceedings of the Annual Conference of the International Speech"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Communication Association (INTERSPEECH), Graz, Austria: ISCA, 2019, pp. 1691–1695."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Batliner, S. Hantke, and B. W. Schuller, “Ethics and good practice in computational paralinguistics,” IEEE Transactions"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "on Affective Computing, 2020."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. Cheong, S. Kalkan, and H. Gunes, “The hitchhiker’s guide to bias and fairness\nin facial affective signal processing:"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Overview and techniques,” IEEE Signal Processing Magazine, vol. 38, no. 6, pp. 39–49, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "R. Bommasani et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,”"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "in Proceedings of\nthe International Conference on Machine Learning (ICML), Vienna, Austria (virtual), 2020, pp. 1597–"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "1607."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, “A"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "survey on vision transformer,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "need,” in Advances in Neural Information Processing Systems (NeurIPS), Long Beach, CA, USA, 2017, pp. 5998–6008."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "sentations,” in Advances in Neural Information Processing Systems (NeurIPS), Vancouver, BC, Canada, 2020, pp. 12 449–"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "12 460."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Y\n. Wang, A. Boumadane, and A. Heba, “A fine-tuned wav2vec 2.0/hubert benchmark for\nspeech emotion recognition,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "speaker verification and spoken language understanding,” arXiv preprint arXiv:2111.02735, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and B. Schuller, “Survey of deep representation learning for speech emotion"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "recognition,” IEEE Transactions on Affective Computing, vol. 12, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "for Dimensional Speech\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Eyben, and B. W. Schuller, Model"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Emotion Recognition based on Wav2vec 2.0, 2022. DOI: 10.5281/zenodo.6221127."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "D. N. Krishna, “Using large pre-trained models with cross-modal attention for multi-modal emotion recognition,” arXiv"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "preprint arXiv:2108.09669, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. Yuan, X. Cai, R. Zheng, L. Huang, and K. Church, “The role of phonetic units in speech emotion recognition,” arXiv"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "preprint arXiv:2108.01132, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "W.-C. Tseng, K.-t. Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H.-y. Lee, Superb: Speech"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "processing universal performance benchmark, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the\nL. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech using wav2vec 2.0 embeddings,” Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Annual Conference of the International Speech Communication Association (INTERSPEECH), pp. 3400–3404, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "L.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition,” arXiv preprint"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "arXiv:2110.06309, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal emotion recognition with high-level speech and text features,” arXiv"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "preprint arXiv:2111.10202, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Interactive emotional dyadic motion capture database,” Language resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2008."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation learning through cross-modal conditional teacher-student train-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ing for speech emotion recognition,” arXiv preprint arXiv:2112.00158, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "R. Lotfian and C. Busso, “Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "existing podcast recordings,” IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471–483, 2019."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "M. Li, B. Yang, J. Levy, A. Stolcke, V. Rozgic, S. Matsoukas, C. Papayiannis, D. Bone, and C. Wang, “Contrastive unsuper-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "vised learning for speech emotion recognition,” in Proceedings of the IEEE International Conference on Acoustics, Speech,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "and Signal Processing (ICASSP), Toronto, ON, Canada: IEEE, 2021, pp. 6329–6333."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "M. Jaiswal and E. M. Provost, “Best practices for noise-based augmentation to improve the performance of emotion recog-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "nition ”in the wild”,” arXiv preprint arXiv:2104.08806, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "˙\nR. Pappagari, J. Villalba, P.\nZelasko, L. Moro-Velazquez, and N. Dehak, “Copypaste: An augmentation method for speech"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the IEEE International Conference on Acoustics, Speech and Signal Processing\nemotion recognition,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "(ICASSP), IEEE, 2021, pp. 6324–6328."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S. S. Rajan, S. Udeshi, and S. Chattopadhyay, “Aequevox: Automated fairness testing of speech recognition systems,” arXiv"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "preprint arXiv:2110.09843, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "C. Gorrostieta, R. Lotfian, K. Taylor, R. Brutti, and J. Kane, “Gender de-biasing in speech emotion recognition,” in Proceed-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ings of\nthe Annual Conference of\nthe International Speech Communication Association (INTERSPEECH), Graz, Austria:"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ISCA, 2019, pp. 2823–2827."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "G. Doddington, W. Liggett, A. Martin, M. Przybocki, and D. A. Reynolds, “Sheep, goats,\nlambs and wolves: A statistical"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the 5th International\nanalysis of speaker performance in the nist 1998 speaker recognition evaluation,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Conference on Spoken Language Processing (ICSLP), Sydney, Australia: ISCA, 1998, pp. 1–4."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "M. Auli, “Robust wav2vec 2.0: Analyzing domain shift\nin self-supervised pre-training,” arXiv preprint arXiv:2104.01027,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, “Voxpopuli: A large-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Natural Language Processing (Volume 1: Long Papers), Bangkok, Thailand, 2021, pp. 993–1003."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf,\nJ. Pino, A. Baevski,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Conneau, and M. Auli, “Xls-r: Self-supervised cross-lingual speech representation learning at scale,” arXiv preprint"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "arXiv:2111.09296, 2021."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "G. Trigeorgis, F. Ringeval, R. Br¨uckner, E. Marchi, M. Nicolaou, B. Schuller, and S. Zafeiriou, “Adieu features? end-to-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the IEEE International\nend speech emotion recognition using a deep convolutional recurrent network,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Conference on Acoustics, Speech, and Signal Processing (ICASSP), Shanghai, China: IEEE, 2016, pp. 5200–5204."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Triantafyllopoulos and B. W. Schuller, “The role of task and acoustic similarity in audio transfer learning: Insights from"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the speech emotion recognition case,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Processing (ICASSP), Toronto, ON, Canada: IEEE, 2021, pp. 7268–7272."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “Panns: Large-scale pretrained audio neural networks for"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "audio pattern recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880–2894,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2020."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal sentiment intensity analysis in videos: Facial gestures and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "verbal messages,” IEEE Intelligent Systems, vol. 31, no. 6, pp. 82–88, 2016."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Intensity dataset\nH. Wierstorf, Gender annotations for Multimodal Opinion-level Sentiment\n(MOSI), Zenodo, 2023. DOI:"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "10.5281/zenodo.7554349."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "M. Munezero, C. S. Montero, E. Sutinen, and J. Pajunen, “Are they different? affect, feeling, emotion, sentiment, and opinion"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "detection in text,” IEEE Transactions on Affective Computing, vol. 5, no. 2, pp. 101–111, 2014."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "L. Tian, C. Lai, and J. Moore, “Polarity and intensity: The two aspects of sentiment analysis,” in Proceedings of the Grand"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Challenge and Workshop on Human Multimodal Language, Melbourne, Australia: ACL, 2018, pp. 40–47."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Machine learning testing: Survey, landscapes and horizons,” IEEE Transactions"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "on Software Engineering, vol. 48, no. 1, pp. 1–36, 2020."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "L. I.-K. Lin, “A concordance correlation coefficient\nto evaluate reproducibility,” Biometrics, vol. 45, no. 1, pp. 255–268,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "1989."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "F. Ringeval, B. Schuller, M. Valstar, R. Cowie, H. Kaya, M. Schmitt, S. Amiriparian, N. Cummins, D. Lalanne, A. Michaud,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the\net al., “Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2018 on audio/visual emotion challenge and workshop, 2018, pp. 3–13."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins, S. Hantke, and B. Schuller, “The perception of emotions in nois-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the Annual Conference of\nthe International Speech Communication Association\nified nonsense speech,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "(INTERSPEECH), Stockholm, Sweden: ISCA, Aug. 2017, pp. 3246–3250."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the 23rd Annual ACM Conference\nK. J. Piczak, “ESC: Dataset for Environmental Sound Classification,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "on Multimedia, Brisbane, Australia: ACM Press, Oct. 13, 2015, pp. 1015–1018,\nISBN: 978-1-4503-3459-4. DOI: 10.1145/"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2733373.2806390. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2733373.2806390."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "S. Corbett-Davies and S. Goel, “The measure and mismeasure of fairness: A critical review of fair machine learning,” arXiv"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "preprint arXiv:1808.00023, 2018."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. K. Fitzsimons, A. A. Ali, M. A. Osborne, and S. J. Roberts, “A general framework for fair regression,” Entropy, vol. 21,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "p. 741, 8 2019."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional\ntransformers for\nlanguage"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "understanding,” in Proceedings of NAACL-HLT, Minneapolis, MN, USA: Association for Computational Linguistics (ACL),"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "2019, pp. 4171–4186."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "L. Pepino, P. Riera, L. Ferrer, and A. Gravano, “Fusion approaches for emotion recognition from speech using acoustic"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "and text-based features,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "(ICASSP), IEEE, Barcelona, Spain, 2020, pp. 6484–6488."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Triantafyllopoulos, J. Wagner, H. Wierstorf, M. Schmitt, U. Reichel, F. Eyben, F. Burkhardt, and B. W. Schuller, “Probing"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the Annual Conference of\nthe Interna-\nspeech emotion recognition transformers for linguistic knowledge,” Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "tional Speech Communication Association (INTERSPEECH), pp. 146–150, 2022."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, “Mockingjay: Unsupervised speech representation learning with"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "the IEEE International Conference on Acoustics, Speech, and\ndeep bidirectional\ntransformer encoders,” in Proceedings of"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Signal Processing (ICASSP), Barcelona, Spain: IEEE, 2020, pp. 6419–6423."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "O. Rudovic, J. Lee, M. Dai, B. Schuller, and R. W. Picard, “Personalized machine learning for robot perception of affect and"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "engagement in autism therapy,” Science Robotics, vol. 3, no. 19, pp. 1–11, 2018."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. Triantafyllopoulos, S. Liu, and B. W. Schuller, “Deep speaker conditioning for speech emotion recognition,” in Proceed-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ings of the IEEE International Conference on Multimedia and Expo (ICME), Shenzhen, China: IEEE, 2021, pp. 1–6."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "K. Sridhar and C. Busso, “Unsupervised personalization of an emotion recognition system: The unique properties of\nthe"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "externalization of valence in speech,” arXiv preprint arXiv:2201.07876, 2022."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "A. D’Amour et al., “Underspecification presents challenges for credibility in modern machine learning,” arXiv preprint"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "arXiv:2011.03395, 2020."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research (JMLR), vol. 9,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "pp. 2579–2605, 2008."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "D. Erhan, A. Courville, Y. Bengio, and P. Vincent, “Why does unsupervised pre-training help deep learning?” In Proceed-"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "ings of\nthe 13th International Conference on Artificial Intelligence and Statistics (AISTATS), Sardinia, Italy: PMLR, 2010,"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "pp. 201–208."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "B. Neyshabur, H. Sedghi, and C. Zhang, “What is being transferred in transfer learning?” In Advances in Neural Information"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Processing Systems (NeurIPS), Vancouver, BC, Canada, 2020, pp. 512–523."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "H. Sajjad, F. Dalvi, N. Durrani, and P. Nakov, “Poor man’s BERT: smaller and faster transformer models,” arXiv preprint"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "arXiv:2004.03844, 2020."
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "F. Burkhardt, B. Weiss, F. Eyben, J. Deng, and B. Schuller, “Detecting vocal\nirony,” in Language Technologies for the"
        },
        {
          "PREPRINT\nDawn of the transformer era in speech emotion recognition": "Challenges of the Digital Age, G. Rehm and T. Declerck, Eds., Cham: Springer International Publishing, 2018, pp. 11–22."
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "3",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "4",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller",
        "K Star",
        "E Hajiyev",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Multi-modal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "9",
      "title": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "U Reichel",
        "S Liu",
        "S Huber",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "arxiv": "arXiv:2110.06650"
    },
    {
      "citation_id": "10",
      "title": "Robust speech emotion recognition under different encoding conditions",
      "authors": [
        "C Oates",
        "A Triantafyllopoulos",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTER-SPEECH)"
    },
    {
      "citation_id": "11",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "12",
      "title": "Ethics and good practice in computational paralinguistics",
      "authors": [
        "A Batliner",
        "S Hantke",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "The hitchhiker's guide to bias and fairness in facial affective signal processing: Overview and techniques",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "14",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "15",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "16",
      "title": "A survey on vision transformer",
      "authors": [
        "K Han",
        "Y Wang",
        "H Chen",
        "X Chen",
        "J Guo",
        "Z Liu",
        "Y Tang",
        "A Xiao",
        "C Xu",
        "Y Xu",
        "Z Yang",
        "Y Zhang",
        "D Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "18",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "21",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Model for Dimensional Speech Emotion Recognition based on Wav",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Model for Dimensional Speech Emotion Recognition based on Wav",
      "doi": "10.5281/zenodo.6221127"
    },
    {
      "citation_id": "23",
      "title": "Using large pre-trained models with cross-modal attention for multi-modal emotion recognition",
      "authors": [
        "D Krishna"
      ],
      "year": "2021",
      "venue": "Using large pre-trained models with cross-modal attention for multi-modal emotion recognition",
      "arxiv": "arXiv:2108.09669"
    },
    {
      "citation_id": "24",
      "title": "The role of phonetic units in speech emotion recognition",
      "authors": [
        "J Yuan",
        "X Cai",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "The role of phonetic units in speech emotion recognition",
      "arxiv": "arXiv:2108.01132"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "P.-H Yang",
        "Y.-S Chi",
        "C.-I Chuang",
        "K Lai",
        "Y Lakhotia",
        "A Lin",
        "J Liu",
        "X Shi",
        "G.-T Chang",
        "T.-H Lin",
        "W.-C Huang",
        "K Tseng",
        "D.-R -T. Lee",
        "Z Liu",
        "S Huang",
        "S.-W Dong",
        "S Li",
        "A Watanabe",
        "H.-Y Mohamed",
        "Lee"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "28",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Multimodal emotion recognition with high-level speech and text features",
      "arxiv": "arXiv:2111.10202"
    },
    {
      "citation_id": "30",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "31",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2021",
      "venue": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "arxiv": "arXiv:2112.00158"
    },
    {
      "citation_id": "32",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Best practices for noise-based augmentation to improve the performance of emotion recognition \"in the wild",
      "authors": [
        "E Provost"
      ],
      "year": "2021",
      "venue": "Best practices for noise-based augmentation to improve the performance of emotion recognition \"in the wild",
      "arxiv": "arXiv:2104.08806"
    },
    {
      "citation_id": "35",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Aequevox: Automated fairness testing of speech recognition systems",
      "authors": [
        "S Rajan",
        "S Udeshi",
        "S Chattopadhyay"
      ],
      "year": "2021",
      "venue": "Aequevox: Automated fairness testing of speech recognition systems",
      "arxiv": "arXiv:2110.09843"
    },
    {
      "citation_id": "37",
      "title": "Gender de-biasing in speech emotion recognition",
      "authors": [
        "C Gorrostieta",
        "R Lotfian",
        "K Taylor",
        "R Brutti",
        "J Kane"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "38",
      "title": "Sheep, goats, lambs and wolves: A statistical analysis of speaker performance in the nist 1998 speaker recognition evaluation",
      "authors": [
        "G Doddington",
        "W Liggett",
        "A Martin",
        "M Przybocki",
        "D Reynolds"
      ],
      "year": "1998",
      "venue": "Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP)"
    },
    {
      "citation_id": "39",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "40",
      "title": "Voxpopuli: A largescale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Riviere",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "42",
      "title": "Adieu features? end-toend speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brückner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "43",
      "title": "The role of task and acoustic similarity in audio transfer learning: Insights from the speech emotion recognition case",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
      "authors": [
        "Q Kong",
        "Y Cao",
        "T Iqbal",
        "Y Wang",
        "W Wang",
        "M Plumbley"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "46",
      "title": "Gender annotations for Multimodal Opinion-level Sentiment Intensity dataset (MOSI), Zenodo",
      "authors": [
        "H Wierstorf"
      ],
      "year": "2023",
      "venue": "Gender annotations for Multimodal Opinion-level Sentiment Intensity dataset (MOSI), Zenodo",
      "doi": "10.5281/zenodo.7554349"
    },
    {
      "citation_id": "47",
      "title": "Are they different? affect, feeling, emotion, sentiment, and opinion detection in text",
      "authors": [
        "M Munezero",
        "C Montero",
        "E Sutinen",
        "J Pajunen"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Polarity and intensity: The two aspects of sentiment analysis",
      "authors": [
        "L Tian",
        "C Lai",
        "J Moore"
      ],
      "year": "2018",
      "venue": "Proceedings of the Grand Challenge and Workshop on Human Multimodal Language"
    },
    {
      "citation_id": "49",
      "title": "Machine learning testing: Survey, landscapes and horizons",
      "authors": [
        "J Zhang",
        "M Harman",
        "L Ma",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Software Engineering"
    },
    {
      "citation_id": "50",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "-K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "51",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "52",
      "title": "The perception of emotions in noisified nonsense speech",
      "authors": [
        "E Parada-Cabaleiro",
        "A Baird",
        "A Batliner",
        "N Cummins",
        "S Hantke",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "53",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "K Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia",
      "doi": "10.1145/2733373.2806390"
    },
    {
      "citation_id": "54",
      "title": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "authors": [
        "S Corbett-Davies",
        "S Goel"
      ],
      "year": "2018",
      "venue": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "arxiv": "arXiv:1808.00023"
    },
    {
      "citation_id": "55",
      "title": "A general framework for fair regression",
      "authors": [
        "J Fitzsimons",
        "A Ali",
        "M Osborne",
        "S Roberts"
      ],
      "venue": "Entropy"
    },
    {
      "citation_id": "56",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "57",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "59",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S -W. Yang",
        "P.-H Chi",
        "P.-C Hsu",
        "H.-Y Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "60",
      "title": "Personalized machine learning for robot perception of affect and engagement in autism therapy",
      "authors": [
        "O Rudovic",
        "J Lee",
        "M Dai",
        "B Schuller",
        "R Picard"
      ],
      "year": "2018",
      "venue": "Science Robotics"
    },
    {
      "citation_id": "61",
      "title": "Deep speaker conditioning for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "62",
      "title": "Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech",
      "arxiv": "arXiv:2201.07876"
    },
    {
      "citation_id": "63",
      "title": "Underspecification presents challenges for credibility in modern machine learning",
      "authors": [
        "A D'amour"
      ],
      "year": "2020",
      "venue": "Underspecification presents challenges for credibility in modern machine learning",
      "arxiv": "arXiv:2011.03395"
    },
    {
      "citation_id": "64",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research (JMLR)"
    },
    {
      "citation_id": "65",
      "title": "Why does unsupervised pre-training help deep learning?",
      "authors": [
        "D Erhan",
        "A Courville",
        "Y Bengio",
        "P Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)"
    },
    {
      "citation_id": "66",
      "title": "What is being transferred in transfer learning?",
      "authors": [
        "B Neyshabur",
        "H Sedghi",
        "C Zhang"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "67",
      "title": "Poor man's BERT: smaller and faster transformer models",
      "authors": [
        "H Sajjad",
        "F Dalvi",
        "N Durrani",
        "P Nakov"
      ],
      "year": "2020",
      "venue": "Poor man's BERT: smaller and faster transformer models",
      "arxiv": "arXiv:2004.03844"
    },
    {
      "citation_id": "68",
      "title": "Detecting vocal irony",
      "authors": [
        "F Burkhardt",
        "B Weiss",
        "F Eyben",
        "J Deng",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Detecting vocal irony"
    }
  ]
}