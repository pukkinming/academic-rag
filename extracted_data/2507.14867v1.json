{
  "paper_id": "2507.14867v1",
  "title": "Hybrid-Supervised Hypergraph-Enhanced Transformer For Micro-Gesture Based Emotion Recognition",
  "published": "2025-07-20T08:27:56Z",
  "authors": [
    "Zhaoqiang Xia",
    "Hexiang Huang",
    "Haoyu Chen",
    "Xiaoyi Feng",
    "Guoying Zhao"
  ],
  "keywords": [
    "Micro-gesture",
    "Emotion Recognition",
    "Hypergraph-enhanced Transformer",
    "Hybrid-supervised Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-gestures are unconsciously performed body gestures that can convey the emotion states of humans and start to attract more research attention in the fields of human behavior understanding and affective computing as an emerging topic. However, the modeling of human emotion based on microgestures has not been explored sufficiently. In this work, we propose to recognize the emotion states based on the micro-gestures by reconstructing the behavior patterns with a hypergraphenhanced Transformer in a hybrid-supervised framework. In the framework, hypergraph Transformer based encoder and decoder are separately designed by stacking the hypergraph-enhanced self-attention and multiscale temporal convolution modules. Especially, to better capture the subtle motion of micro-gestures, we construct a decoder with additional upsampling operations for a reconstruction task in a self-supervised learning manner. We further propose a hypergraph-enhanced self-attention module where the hyperedges between skeleton joints are gradually updated to present the relationships of body joints for modeling the subtle local motion. Lastly, for exploiting the relationship between the emotion states and local motion of micro-gestures, an emotion recognition head from the output of encoder is designed with a shallow architecture and learned in a supervised way. The end-to-end framework is jointly trained in a one-stage way by comprehensively utilizing self-reconstruction and supervision information. The proposed method is evaluated on two publicly available datasets, namely iMiGUE and SMG, and achieves the best performance under multiple metrics, which is superior to the existing methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M ICRO-GESTURE (MiG) becomes an emerging re- search topic in recent years as it can reveal human beings' hidden emotional status spontaneously. They are a specific type of gestures (body movements) that are spontaneously and unconsciously elicited by inner feelings, which usually induce subtle body movements and are easily ignored by ordinary people  [1] ,  [2] . The movements of MiG can be roughly categorized as self-adaptor movement such as scratching the head and rubbing the hand, defensive-behavior Z. Xia is with School of Electronics and Information at Northwestern Polytechnical University, xi'an, Shaanxi 710072, China, and with Innovation Center NPU Chongqing, Chongqing 401120, China. Email: zxia@nwpu.edu.cn.\n\nH. Huang and X. Feng are with School of Electronics and Information at Northwestern Polytechnical University, xi'an, Shaanxi 710072, China. Email: huanghexiang@mail.nwpu.edu.cn, fengxiao@nwpu.edu.cn.\n\nH. Chen and G. Zhao are with the University of Oulu, Email: chen.haoyu@oulu.fi, guoying.zhao@oulu.fi. movement such as folding arms or moving legs, and objectrelated movement such as playing with clothes. Unlike ordinary body movements of the hands, head and other parts of the body that allow individuals to exhibit emotions explicitly, artistically and abstractly  [3] , micro-gestures can be used to explore the emotional states that people express intentionally and refer to some specific gestural behaviors, which are usually related to partial regions of human body and have subtle movements. So ordinary body movements are used to facilitate non-verbal communications and are helpful to be understood by others, while the MiG is an important clue to analyze the people's hidden emotion states. Therefore, the automatic emotion recognition with MiGs would be beneficial in the fields of social media, human-computer interaction, public safety and health care.\n\nTo analyze hidden emotions from MiGs, the approaches of emotional artificial intelligence for ordinary body movements might be applied to obtain the representations of local and subtle body movements. Most technologies in this field  [3] ,  [4]  focus on recognizing the communicative or illustrative body movements and usually extract different feature representation from the full-body  [3]  or upper-body  [5]  for classifying gestures into 6 emotional categories, i.e., sadness, anger, happiness, disgust, fear, and surprise. The representation for ordinary body movements can be static or dynamic with the appearance or geometrical information. To cite a few, the geometric movement qualities  [6] , normalized rotation values  [7]  and Fisher score movement  [8]  have been proposed as the representation features, and multilayer Perceptron (MLP)  [7] , support vector machine (SVM) and hidden Markov model (HMM)  [8]  have been used as emotion classifiers. Furthermore, various deep learning based methods like recurrent neural network (RNN)  [9] , convolutional neural network (CNN)  [10]  and semantic auto-encoder (SAE)  [11]  have also been introduced to analyze the temporal motion for emotions. However, these emotion-centered methods for ordinary body movements are dedicated to the substantial movements that are intentionally performed, while the local and subtle movements of MiGs cannot be represented very well and need to be further studied by exploring the relationship between the emotion states and MiGs.\n\nIn recent years, considerably less work has been done on the emotion recognition with MiGs  [2] . As a less explored topic, few efforts on MiGs have relatively been reported until the first dataset was published (the initial version of SMG  [12] ). To date, only two datasets focused on different scenarios (i.e., arXiv:2507.14867v1 [cs.CV] 20 Jul 2025 public interview and personal interview), namely iMiGUE  [1]  and SMG  [2] , have been publicly accessed. Within the interview, the positive or negative emotions from various subjects would be disclosed by their MiGs. For the sensitive reason of biometric data, these two datasets mainly provide the skeleton data and remove the identity information in RGB videos. In the original works of two datasets  [1] ,  [2] , the deep belief network (DBN) and long short-term memories (LSTMs) were separately utilized to estimate the hidden emotion states based on the micro-gestures. More recently, based on these two datasets, some methods  [13] ,  [14] ,  [15] ,  [16] ,  [17]  have been developed for recognizing the categories of MiGs, rather than the emotion states. The deep models such as variational autoencoder  [16] , the convolutional neural networks (CNNs)  [13] , graph convolution networks (GCNs)  [14] ,  [15]  and ensemble method  [17]  based on the skeleton data have been presented to capture the subtle movements of upper-body and classify emotions into 17 or 33 categories. Among these methods, the GCNs based methods  [14] ,  [15]  show promising performance for the MiG analysis, which utilize the graph or hypergraph constructed from the body joints to extract the subtle motion feature. But these methods did not further model the relationship between micro-gestures and emotion states and mainly focused on the classification of MiG categories. Some developments of employing dynamic temporal weights to the joints in a hypergraph  [18]  or global systemic dynamics  [?]  with Euclidean distance between each joint may be helpful to improve the representation ability. However, these extension methods utilize the temporal or global motion information but ignore the local spatial relationship between each body parts (joint group, i.e., left arm and right arm), which needs to be explored with more representative modeling by considering the interaction of body joints.\n\nTo explore the relationship between each body joints, we propose a Hybrid-supervised Hypergraph-enhanced trans-Former for micrO-gesture based emotion recognition (denoted as H2OFormer) by reconstructing the local and subtle body movements for skeleton data. In the H2OFormer framework, the Transformer architecture is used to construct one encoder and one decoder, which are separately designed by utilizing the hypergraph-enhanced self-attention and multiscale temporal convolution for extending the vanilla hypergraph Transformer  [19] . Incorporating a decoder with an encoder could introduce extra information such as the self-supervised information (i.e., the motion reconstruction), which would utilize asymmetrical blocks for the decoder compared to the encoder. Compared to the vanilla hypergraph Transformer, the hypergraph-enhanced self-attention in H2OFormer could update the relationships between hyperedges dynamically. Based on the reconstruction, a recognition head from the output of the encoder is constructed for modeling the relationship between micro-gestures and the emotion states in a supervised way, which would exploit the shallow architecture to connect the encoded feature (local motion) and the emotion states. All these modules such as the encoder, decoder and recognition head would be cascaded and jointly learned in a one-stage strategy. The main contributions of this work are summarized as follows.\n\n• We propose a hypergraph-enhanced self-attention module by dynamically exploring the hyperedges of body joints, which can be easily embedded into the vanilla hypergraph Transformer framework for better capturing the local motion. • We design an asymmetrical encoder and decoder architecture based on hypergraph-enhanced Transformer, which accurately models the subtle movement for skeleton data based micro-gestures in a self-supervised way of motion reconstruction.\n\n• We employ a one-stage learning strategy using hybridsupervised information to integrate the self-supervised reconstruction task with supervised recognition task, efficiently exploring more information for sample-limited model training.\n\n• We perform extensive experiments to evaluate the performance of the proposed method and achieve the best results on two datasets, which shows its discrimination ability on micro-gesture based emotion recognition. The rest of the paper is organized as follows. Section II presents the related works on the emotional analysis and category classification of MiGs, and Section III describes the preliminaries of hypergraph. Section IV introduces the deep approach in detail. Section V provides several experiments to verify the effectiveness of the proposed deep method and discusses the characteristics of the proposed method. Section VI summarizes the entire paper and analyzes the future work briefly.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Automatic emotion recognition has attracted the interest of artificial intelligence research community for the past two decades and has developed numerous approaches by leveraging the face, body, speech or gaze of humans. Since few works that have been discussed above focused on the emerging MiG based emotion recognition, we extend the discussion to the approaches that would potentially be used for emotion recognition based on body movements and the self-supervised learning methods for emotional or skeleton data, which could be helpful to develop better models for recognizing MiG based emotion states. More comprehensive surveys can be revisited by  [3] ,  [5] ,  [20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Body Movement Based Emotion Recognition",
      "text": "According to  [3] , the body movements are usually categorized as the communicative, functional, artistic, and abstract movements, and the first type attracts most attention in past years. Except the data captured by motion and pressure sensors, the computer vision techniques due to their high availability are the focus in the context.\n\nAs one of pioneering works, Gunes et al.  [21]  proposed to estimate the emotional categories from body movements in static frames of videos. The propositional gestures were captured by body action units tracked with Camshift algorithm and described by orientation features. By utilizing other features from faces, the emotion states were obtained by exploiting BayesNet classifiers. Then, the non-propositional movement qualities (e.g., amplitude, speed and fluidity of movement) combined with dynamic time warping based onenearest-neighbor were used to recognize the body movements with four acted emotion states  [6] . With the motion capture system  [22] , professional actors were asked to simulate emotion states and 3D motion-capture data were collected for further analyzing by representing a feature vector for each kinematics sample. Although these preliminary works can recognize the emotional gestures, they mainly aim to explore the acted gestures and usually have huge differences with the real-world scenarios.\n\nTo achieve the non-acted (spontaneous) body postures, in  [7] , the skeleton data in a body-movement based video game was collected by a motion capture system. Low-level movement feature description was analyzed by using normalized rotation values and then the MLP was used to recognize emotion labels defined by the authors. Similar to  [7] , the data collected in a Nintendo sport game was used to analyze the time-related features and exploit RNNs to surpass human observers' benchmarks  [9] . In  [8] , a full-body dataset was collected using a Vicon motion capture system and a stochastic model for modeling the movement dynamics was built on HMMs. A Fisher score movement representation with Hilbert-Schmidt independence criterion was then used to recognize affective movements using SVM. Furthermore, a group of features from the full body  [23]  were utilized to describe movements on various levels and employ random forest to predict eight emotion states in various daily actions such as walking, sitting or knocking. Senecal et al.  [24]  proposed to recognize continuous emotional behaviors by using Laban movement analysis mapped onto Russell circumplex model.\n\nApart from the professional movement-captured devices, other cameras like Microsoft Kinect camera are also exploited to capture the movements of the whole body. By recording the gaits with Kinect  [25] , features from 3D coordinates of 14 main body joints were extracted and then processed by Fourier transformation and principal component analysis. Naive Bayes, random forests and SVM were further used as classifiers to analyze various emotions. Besides, with the geometric features, Piana et al.  [26]  proposed to combine psychological theories, such as impulsiveness and contraction index, to implement the tasks of emotion recognition and emotion expression. Moreover, multimodal information has also been explored to discover the interrelation between face or speech and body gestures for emotion recognition. In  [27] , prosody and audio spectral features for modeling the interaction dynamics of speech referred to three types of body representations, while the facial expression, gesture and acoustic features were used within an automatic system based on a Bayesian classifier  [28] .\n\nIn more recent studies, deep learning based techniques begin to dominate this field. Kosti et al.  [10]  presented to recognize emotion states based on static RGB images by using CNNs with low-rank filters. With CNNs, the person and the whole scene were jointly explored to analyze the emotion states by considering the continuous dimensions valence, rousal and dominance. In  [29] , an attention-based channel-wise CNN was presented to retain the independent characteristics of each body joint and learn key body gesture features in RGB videos. Besides CNNs, other deep learning models such as SAE  [11] , LSTM  [30] , and two-branch fusion framework  [31]  have also been employed to obtain good representation for sequential actions in the task of emotion recognition in videos. These deep models are borrowed from the general-purpose video understanding and directly used for the gesture based emotion recognition, which can preferably capture the motion feature by an encoder for ordinary body movements.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Self-Supervised Learning For Skeleton Data And Emotion Recognition",
      "text": "Since the encoder-decoder architecture we designed in this work refers to the self-supervised learning (SSL), we summarize several SSL methods in this section. SSL usually can learn discriminated features from various data without relying on human-annotated labels  [32]  and achieves great development in the past decade  [33] . On one hand, SSL based on one modality, i.e., skeleton data, has been applied to various tasks like action recognition  [34] ,  [35] ,  [36] ,  [37] ,  [38] ,  [39] , which has the similar input of MiG based emotion recognition. On the other hand, as most datasets in the field of emotion recognition contain insufficient samples and usually limit the feature learning, SSL has also been applied to learn emotion representation for further analysis from various modalities, e.g., facial expression  [40] ,  [41] ,  [42] ,  [43] ,  [44] ,  [45] ,  [46] , speech  [47] ,  [48] , gait  [49] ,  [50] , electrocardiograms (ECG)  [51] ,  [52]  and Electroencephalography (EEG)  [53] ,  [54] . As limited SSL methods have been used for skeleton data based emotion recognition, we would revisit the SSL methods for skeleton data and emotion recognition, which may be helpful to design a method for skeleton based MiG analysis.\n\nFor utilizing the skeleton data to explore robust representations, some approaches in the task of action recognition have been presented with the SSL strategy inspired by the success of self-supervised learning in image and video tasks. As a primary work for skeleton data, MS 2 L [34] integrated reconstruction, classification, and projection tasks based on recurrent layers for self-learning and then obtained skeleton features from action recognition. In  [35] , the semantic invariance under the variant of distance and viewpoint was modeled by two separate transformations with contrastive learning of skeleton sequence to a given skeleton sequence, which maximized agreement by using the contrastive loss. Furthermore, a focalized contrastive view-invariant learning framework by gated recurrent unit based encoder and decoder  [36]  was proposed to maximize the mutual information between multi-view action pairs by adapting contrastive self-learning, which leveraged a dynamic-scaled focal loss based on the geometric distance of the contrastive representations. in  [37] , the query, extended query and key encoders were used to extract the temporal motion features and then trained by minimizing dual distributional divergence for skeleton sequences. Besides, the skeleton data could be transformed into the image-like data and the image based SSL method can be applied directly  [38] . A cross-stream contrastive learning model  [39]  was exploited by two GCNbased skeleton encoders to calculate the correspondence of intra-stream and inter-stream.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Het Het",
      "text": "Fig.  1 : The architecture overview of our proposed model (H2OFormer), which is consisted of four components, i.e., the Encoder, the Decoder, the Head and the Loss.\n\nFor recognizing facial expressions, the contrastive learning  [40] , a typical SSL strategy, was utilized as the first step for constructing a encoder to represent multiview facial data. Then this strategy was adequately revisited and extended into three operations, i.e., positives augmentation, hard-negatives increment, and false-negatives cancellation  [41] , or new feature space through feature transformation  [42]  for obtaining expression-aware representations. To further address the issue that the contrastive learning above tends to learn pose-invariant features, a pose-disentangled contrastive learning method  [43]  was presented for general self-supervised facial representation. Furthermore, the contrastive SSL strategy was extended to video based emotion recognition task and one 3D convolution based encoder was constructed for capturing the temporal motion of facial regions  [44] . With another SSL strategy, i.e., masked autoencoders (MAE), MAE-Face  [45]  based on images was developed to obtain robust visual representations for facial affect analysis and can be pretrained on a large-scale face image database  [46] .\n\nFor recognizing emotions from other non-visual modalities, the SSL strategy was also combined with the specific architectures being adaptive to the corresponding modality. To cite a few, the SSL method  [47]  was employed to learn better representation for predicting emotional cues from speech with using the energy variations and facial activation as the selfsupervision. Another shallow architecture with crossmodal attention fusion  [48]  was used for sentiment analysis and emotion recognition by fine-tuning two pretrained self-supervised learning models. In  [49] , two independent representation models like GCN and LSTM neural network were constructed and then used to contrastively learn pain-level representations from body movements by using a comparison network. Gait based emotion representation  [50]  was obtained by a crosscoordinate contrastive learning framework. In this framework, multiple encoders were learned by considering the semantic invariance with using ambiguity transform and hybrid samples memory bank. Moreover, an ECG based emotion recognition system  [51]  employed a signal transformation recognition network in the self-supervised learning step and then fed the transformed signal to an emotion recognition network. To exploit representations of time-series ECG signals, a Transformer based framework  [52]  by masking the ECG signals randomly as input was presented to overcome the relatively small size of datasets with emotional labels. Similar to EEG signals, to reduce the overfitting, a generative adversarial networkbased self-supervised data augmentation was incorporated to combine adversarial training with self-supervised learning for EEG based emotion recognition  [53] . To learn more general representation from multiple tasks, a graph-based multi-task self-supervised learning model  [54]  was built for EEG emotion recognition, which maps the transformed data into a common feature space with contrastive learning.\n\nThe above-mentioned SSL methods have been shown to improve the performance of skeleton data based action recognition and emotion recognition for 1D, 2D or 3D data. For skeleton data, most approaches construct various encoders in different views or models and perform the contrastive learning among these encoders, while few approaches try to reconstruct the actions due to the difficulty of designing decoders for skeleton sequences. For emotion tasks, most of them use the SSL strategy to obtain a pretrained model and then apply it for recognizing emotions directly in an unsupervised way (one-stage) or fine-tune it with limited supervised information (two-stage). However, these deep models that are used with SSL are mainly focused on the significant motion from the skeleton data, which usually do not incorporate the dynamical relationships between various body joints and temporal variations for the emotional motion. So the special architecture and SSL strategy need to be further studied in the task of MiG based emotion recognition with skeleton data, which usually refers to the subtle motions for some partial joints of body.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "In this paper, the MiG recognition framework is designed on the basis of hypergraph, which would be introduced in the following content for a start. Generally speaking, the hypergraph is a generalization of graph in which an edge can join any number of vertices (also called as hyperedge) for geometriclike data and has been applied in many vision-related tasks, such as data classification  [55]  and action recognition  [19] .\n\nFor the human-centric tasks, by considering each joint (keypoint) of human body as a vertex v and their potential connection as a hyperedge e in a graph, the hypergraph H is constructed by containing the vertices V and their hyperedges E, which is H = (V, E). With this representation, the spatialtemporal motion information can be extracted from this hypergraph by using multi-layer network with various architectures. The numbers of vertex and hyperedge are denoted as |V| and |E|. The incidence matrix H ∈ {0, 1}\n\n|V|×|E| can be used to quantitatively characterize the hypergraph H. Each entry in H is denoted as:\n\nThe people from the same scenario (i.e., from the same dataset) share the same incidence matrix. The degree of a vertex is calculated as d(v) = e H(v, e), while the degree of a hyperedge is calculated as d(e) = v H(v, e). The degree matrices D v ∈ R |V|×|V| and D e ∈ R |E|×|E| for vertices and hyperedges are then obtained by setting all the vertex degrees d(v) and all the edge degrees d(e) as their diagonal entries, respectively. In the initial stage of the hypergraph, one vertex only belongs to one hyperedge, which means D v (i) = 1, but one hyperedge may be connected to many vertices. In this case, all body joints are separately divided into |E| subsets.\n\nEach vertex is represented as a feature vector and the features for the hypergraph can be denoted X ∈ R |V|×D . D is the dimension of the vertex, which is equal to 2 or 3 if X represents the original spatial coordinates of the body joints. So the subset representations for graph based neural network could be expressed as\n\nwhere W e ∈ R D×|E| represents the learnable weight and could be assigned to each hyperedge for obtaining better representations. Based on the hypergraph, the task of emotion recognition turns to learn multiple weights for classifying the vertices on the hypergraph.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Methodology",
      "text": "In this section, the deep framework of H2OFormer for MiG based emotion recognition will be introduced. The entire architecture of this framework is described firstly in Fig.  1  and then some core components are explained in detail in the following sections.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Architecture Overview",
      "text": "The input of the proposed method is the skeleton data detected from individuals in a review scenario, which is implemented by using the OpenPose algorithm  [56]  on RGB videos. Usually, the skeleton sequence from videos would be divided into several clips and each clip would be fed into the deep model. The skeleton data in t-th frame in one clip for input could be denoted as X t o = {m 1 ; m 2 ; . . . ; m K } ∈ R |V|×3 , where total K body joints are detected by OpenPose in one frame. For each joint of body joints, the 3D spatial coordinates are recorded in a 3D vector by m k = (x, y, z) k . For one sequence of MiGs with T frames, the frames having K-joint motion could be combined together to be the input tensor\n\n. This tensor X o as an entire input would be fed into the H2OFormer and obtain a reconstructed sample Xo ∈ R T ×|V|×3 as well as one emotion state c ∈ Z.\n\nThe framework mainly consists of four core components, i.e., one Transformer encoder, one Transformer decoder, one recognition head, and the integrated loss function. The encoder and decoder by stacking many Transformer blocks are combined together to reconstruct the skeletal input in a selfsupervised way, obtaining Xo from X o . With the motion reconstruction, the local and subtle movements could be roughly captured by the encoder's representation vector for depicting MiGs. However, the local and subtle movements may not reflect the real changes of hidden emotion states. So one emotion recognition head obtaining state c is also appended into this framework and used to rectify the motion extraction from one MiG sequence by exploiting the supervised information. Different from the way that learns the encoder first and the head secondly, we integrate them in a one-stage way and learn the deep model in one time. To learn the parameters of the deep model, the jointly integrated loss function is designed to consider the reconstruction loss and recognition loss. The encoder, decoder and the recognition head are learned by the integrated loss in one-stage learning.\n\nIn the components of encoder and decoder, the hypergraphenhanced Transformer module (HET) for each block is designed to construct the multilayer architecture. In each block, the HET is used as the building block and stacked directly block by block, which will be explained in detail in Section IV-A. For the encoder, various blocks of HET can be employed to capture the local and subtle movements according to the complexity of the human skeleton data. For instance, the persons in SMG  [2]  have full-body movements with more complexity, which needs to choose more blocks (e.g., 10 blocks are cascaded in the encoder), while the persons in iMiGUE  [1]  have upper-body movements with less complexity, which can decrease the number of blocks (e.g., 6 blocks in the encoder). For the decoder, the configuration is similar to the encoder but the motivation is very different, which is used to reconstruct the local and subtle motion. The number of blocks in decoder is quite different from the encoder, which adopts less blocks for decoder in the context (asymmetrical block configuration compared to the encoder). Apart from the HET, the upsampling operation is also necessary to the decoder for the gesture reconstruction. In this work, we choose to utilize the linear projection layer which could be inserted into the HET module easily as the upsampling layer for linearly transforming the dimensionality. The upsampling operation could increase the dimensional size of features.\n\nFor the task of emotion recognition, only one Transformer structure similar to the vanilla architecture  [57]  is used for prediction task as it does not need to extract much more information by considering the hypergraph, while it needs to build up the relationship between MiGs and emotion states. It becomes not very necessary to enhance the hyperedges in the prediction task and may limit the architecture complexity for head branch  [58] . So the encoded feature from the encoder is utilized to predict the emotion states directly with a shallow architecture. In this context, one multi-head attention, one batch normalization, one MLP, one average pooling and one fully-connected layer are incorporated in the head to recognize the emotion states. The output of the prediction head is c ∈ {0, 1}, where 1 represents the positive state and 0 the negative state.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Hypergraph-Enhanced Transformer Module",
      "text": "The vanilla hypergraph Transformer (Hyperformer)  [19]  is originally used for action recognition by introducing the hyperedge into the Transformer. However, the hyperedge is initially captured from the input skeleton data and fixed through the entire network learning. Whereas, as the same emotion state may contain different movements from various people, the interaction degree of body joints would be changed with various people and the connection intensity behind the body joints might be different. It means that the connection of hyperedges needs to be dynamically updated according to the different skeletal input of various MiGs. Motivated by this, we design a dynamic strategy for updating hyperedges, which is shown in Fig.  2 .\n\nHyperedge Representation. Based on the incidence matrix H and the original feature X t (for the input of our H2OFormer, X t = X t o ), the augmented feature representation E t for hyperedges can be further computed as:\n\nX t ∈ R |V|×D represents the t-th frame of input feature tensor X ∈ R T ×|V|×D for each block (the output from previous block), which is equal to X t o at first block or X t f at f -th intermediate block. D represents the feature dimension of each joint (keypoint) in the hypergraph. The product of incidence matrix H and the feature of joints X t can obtain the basic representation feature of hyperedges. The incidence matrix and degree matrix of hyperedges are used to normalize the weight and dimension of the hyperedges according to the original hypergraph. The learnable matrix W e could improve the representation ability by changing the dimension of input feature. The final matrix E t represents the joints with the hyperedge representation for the t-th frame in a MiG sequence.\n\nSelf-attention Computation. Based on the input X t and hyperedge representation E t , the self-attention for Transformer encoder is then computed to improve the representation ability of input feature for this block. Slightly different from the hypergraph attention calculated in  [19] , we extend the attention computation by containing four parts as follows:\n\nThe Q, K and V are obtained by linear projection from the input representation X t . Similarly, E Q , E K and E V with various entities are calculated by linear projection from the same hyperedge representation E t , which can represent different connection weights of hyperedges for body joints. The part a represents the sequential information of human skeleton. R ϕ in part a is the k-Hop relative positional embedding for sequential data, which is indexed from a learnable parameter table by the shortest path distance between the i th and j th joints  [19] . The part b represents the joint-to-joint attention, which is same to the original Transformer. The part c represents the joint-tohyperedge attention, which is induced by hypergraph attention  [19] . The part d is newly added by our proposed method for implicitly measuring the hyperedge-to-hyperedge attention. In each block, multihead structure is also used for the attention module. So the spatial motion information can be enhanced by using the attention feature.\n\nIn part a, the relative position encoding matrix R ϕ ∈ R |V|×|V|×D of |V| key-points is calculated from H ops ∈ R |V|×|V| , which is given by\n\nHere, I |V| is the identity matrix of order |V|, and A G is the adjacency matrix of the graph. Define a matrix\n\nwhere m is the maximum value of the H ops matrix plus one, i.e., m = max(H ops ) + 1, the vector ⃗ r m ∈ R D , and R ∈ R m×D . The relative position encoding matrix R ϕ can be obtained as follows\n\nwhere hops N N is the value corresponding to the |V|-th row and |V|-th column in the H ops matrix. On one side, the calculated attention A is further used to compute the enhanced feature X t a by multiplying with the input as X t a = Sof tmax(A) • V . On the other side, the hyperedge-to-hyperedge attention (part d) continues to be used to dynamically explore the hyperedge representation E t , which can further be used as the input of next block. The updated hyperedge representation E ′ t is calculated as\n\nIn  [19] , the hyperedge representation is only calculated in the first block and repetitively used in next blocks, while the hyperedge representation in this work could be updated in one block and passed to the next block (dynamical hyperedge representation). So the hyperedge representation in each block is enhanced by the hyperedge-to-hyperedge attention, which is influenced by the hyperedge with more relationships having larger attention weights. Multiscale Temporal Convolution. Based on the attentive feature X t a , we further utilize the temporal convolution to extract the sequential motion information from the entire sequence X a = X 1 a ; . . . ; X t a ; . . . ; X T a . As different people may have various gestures with different changing times, we employ multiscale paths in each block for multiple temporal movements. Motivated by the dilated convolution for microexpression recognition  [59] , we use 1D dilated convolution with different sizes, i.e., 1 × 1, 3 × 1 and 5 × 1. By arranging various dilated convolutions in different paths, the multiscale temporal motion information is obtained and finally concatenated into one feature presentation by adding the input feature to obtain the output feature of this block (e.g., X f ).\n\nIdentity-Mapping/Upsampling. In our hypergraph-enhanced Transformer module, multiple types of shortcut connections, i.e., identity-mapping and upsampling, have been used from the input to the output as well as the input for temporal convolution in each block, which are shown in Fig.  2 . In the encoder, the goal is to extract the motion feature, so that the identity mapping connections are chosen as the shortcut connection like the ResNet, which feed the input to next layer. In the decoder, the task is to reconstruct the local and subtle movement, so that the upsampling connections are used to increase the dimension of the input feature, which is implemented by a linear projection. The connection type would be chosen by the block location in encoder or decoder, while they are inserted in the same position of the deep architecture for each block.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Model Learning",
      "text": "Different from the previous works, which usually used a two-stage learning strategy for self-supervised learning, we propose to learn the above-mentioned model with a one-stage learning strategy under hybrid supervision. Totally two types of tasks, i.e., reconstruction task and recognition task, are considered in the context. The reconstruction task utilizes the self-supervision to capture the subtle local motion of MiGs, while the recognition task uses the labels of emotion states to connect the MiG motion with the emotion states. Two types of losses are employed for two tasks, respectively.\n\nFor the reconstruction task, the difference between the original MiG sequence X o and the reconstructed sequence Xo is considered. For N sequences with T frames, the reconstruction loss L rec is calculated as\n\nwhere Xt o (i) and X t o (i) represents the i-th sequence in the training set. For the recognition task, as the hidden emotion states are binary, the cross-entropy loss for emotion state classification is considered. It is calculated as\n\nwhere the probability of being positive state is denoted as\n\nand the probability of being negative state p(\n\nrepresents the recognition head with the encoder. To learn a model in one stage, these two losses are combined linearly by using L = λ 1 L rec + λ 2 L cls , where λ 1 and λ 2 are the hyperparameters for final loss. To guarantee the one-stage learning effective, these two hyperparameters are used to ensure the same order of magnitude of two types of losses. Besides, different from the popularly used strategy, i.e., masking, for RGB data, we choose the original skeleton data as the input of our deep model without any masking strategy by considering the structure characteristic of skeletal joints. Each joint extracted from RGB based video frames has been used to remove the redundant information for depicting the motion, which is greatly different from the RGB data. This strategy is verified in Section V-B.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Experiment And Discussion",
      "text": "In this section, we will introduce the datasets we used, the metrics for evaluation, the results of ablation study and comparison to the state of the art (SOTA) methods for the hidden emotion recognition with MiGs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets And Implementation Detail",
      "text": "In the experiment, we choose to utilize the iMiGUE  [1]  and SMG  [2]  datasets, which are recently emerging micro-gesture datasets for hidden emotion recognition.  SMG Dataset. In this dataset, eliciting tasks by telling and repeating stories are designed to observe the MiGs when the participants need to prove that they knew the story content, respectively, no matter they are assigned with a real story or an empty story. The participants have more emotional involvement for making up a fake story and exhibit two emotional stress states as hidden emotions. A total of 40 sequences are collected from 40 subjects with an average age of 25 years, hailing from various countries. Each sequence lasts approximated 15 minutes, and the entire dataset comprises 821,056 frames, which exceeds 8 hours of recorded data. The dataset utilizes skeleton data from 25 human body joints, with each keypoint represented by 11 numerical values, which are the 3D spatial coordinates, the 4D rotation coordinates, 2D spatial coordinates for higher-resolution images, and 2D spatial coordinates for lower-resolution images, respectively. It has an average length of 51.3 frames for MiG. Additionally, it includes 2 categories of emotions, with 71 instances of relaxed emotion state (positive) and 71 instances of stressed emotion state (negative).\n\nImplementation. In our work, the parameter settings are configured as follows. The training epochs, batch size, initial learning rate and learning rate decay rate for model learning are set to 100, 64, 0.0005, and 0.1, respectively. The number of stacked encoder blocks L and the number of stacked decoder blocks M are asymmetrically configured to different values according the different data complexity from two datasets, i.e., L = 6, M = 2 for iMiGUE, and L = 10, M = 4 for SMG. This would be further discussed with other configurations in the ablation-study experiment for observing the influences of encoder-decoder blocks. The dimension size D for the intermediate blocks is set to 216, and the temporal length T for each clip is set to 52. In the self-attention module, 9 heads are used for each block. The stochastic gradient descent (SGD) is used as the optimizer for training our H2OFormer. The weights and biases of the convolution and other learnable parameters designed in H2OFormer are initialized to ensure that the gradients during the training process do not vanish or explode too quickly. Each skeleton sequence is fed into the H2OFormer and is then performed with the hypergraph convolution layer by layer. The constructed sequence and emotion category are obtained from the network and input into the loss functions for computing the gradients and updating the learnable parameters. All experiments are performed on one NVIDIA GeForce RTX 4090. The code of the implementation is available on Github (https://github.com/xiazhaoqiang/H2OFormer-MicroGestureRec).\n\nProtocol. Following  [1] ,  [2] , the training set and test set are randomly chosen from all subjects, and the subjectindependent protocol is used to evaluate the proposed method, which uses the data from the separate subjects (participants) for the training and test sets. The division between training and test for subjects are kept consistent with the original datasets, which can be found in  [1] ,  [2] . Within the protocol, the metrics of accuracy and F1-score are further employed to evaluate the performance of the proposed method. The accuracy is calculated as acc = 1",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Ablation Study",
      "text": "Here, we choose to evaluate the important components of our proposed framework, e.g., the hypergraph used, the enhanced hyperedge, the decoder for reconstruction, and the way of training. Furthermore, the influences of various parameter configurations, e.g., the blocks of encoder and decoder and the  number of hyperedges are also verified. The visualization for the hyperedge representation is also shown in this section.\n\nVarious Components of H2OFormer. In order to verify the effectiveness of important components in our proposed model, we conduct a series of ablation study experiments. The vanilla Transformer without using any hypergraph and reconstruction task are used as the baseline (denoted as BL). Four core components are sequentially added into the BL method. The component of using hypergraph (HG) is evaluated by adding the HG with BL (denoted as BL+HG). The component of using enhanced hyperedge with HG is verified by adding the enhanced module with BL+HG (denoted as BL+HG+EH) while the component of merely using decoder branch is denoted as BL+HG+DB, which would be trained with onestage learning strategy. To observe the effectiveness of using one-stage learning strategy, we combine the EH and DB together (denoted as BL+HG+EH+DB) in a two-stage training strategy, which trains the encoder-decoder model firstly and then fine-tunes the encoder and the recognition head with the loss L rec and L cls , respectively. The two-stage training is performed on the same dataset without any additional data. Besides, as the masking strategy has been used frequently for RGB images  [60] , we also investigate the masking strategy on skeleton data, which is denoted as H2OFormer (Masked). In this method, the spatial coordinates of several joints are randomly chosen and set to zeros, in which 30% joints have been masked following  [60] . The decoder will refill these missing joints with our proposed one-stage H2OFormer. Our method utilizes all the components above and has the same configuration with these baseline methods. All the ablation study methods are performed on two datasets with two kinds of metrics, i.e., accuracy and F1-score.\n\nThe experimental results of using various components are summarized in Table  I . Take iMiGUE dataset for example. From the experiments of each component, it can be seen that the performance of using hypergraph can be improved as the hyperedges could focus on the local movements. The accuracy of the model can further be improved by 2% when the decoder is added to reconstruct the data, while it is also improved by 1% when the enhanced hyperedge is incorporated. It illustrates that the component of enhanced hyperedge could promote the ability of representation and the component of decoder could make the model focusing on the characteristics of MiG motion. It is worth noting that by jointly using the decoder of reconstructed task with the iterative hyperedge enhancement and one-stage training, the performance of the basic model is greatly improved by 8%, much better than using them individually. Another thing that the masking strategy does not work very well on skeleton data needs to be focused. That might because the skeleton data has more compact representation as each joint will be usefully for recognizing emotion states while some regions in RGB data may be removed due to the information redundancy. The similar conclusion can be observed on SMG datasets. Only one difference on SMG dataset is that using decoder module seems more important than other modules.\n\nVarious Blocks of H2OFormer. To observe the influences of using different blocks for encoder and decoder, some experiments are performed on two datasets under the metric of accuracy, which is shown in Fig.  4 . In this figure, the encoder and decoder with various blocks are employed. On these two datasets, the numbers of blocks are selected as 6, 8 and 10, respectively, with the decoders starting from 2 to 10 blocks. From the results, different patterns can be observed on two datasets. On iMiGUE dataset, the matching between the encoder and the decoder becomes important. With different blocks in encoder and decoder, the accuracy changes obviously. The models with larger complexity on iMiGUE may not achieve obvious performance improvement by adding blocks in encoder and decoder directly as the skeleton data over this dataset only contain upper-body joints. By considering the performance and the complexity of the proposed model, 6 blocks for encoder and 2 blocks for decoder are chosen as the final structure for other experiments, even if this is not the best result reported in the figure. On SMG dataset, the performance of the proposed models having various blocks changes differently. As the subjects in SMG dataset have fullbody joints (more keypoints) than the iMiGUE dataset, the model having more blocks may be better than the simpler one (i.e., the model on iMiGUE dataset). But it does mean that the more the blocks, the better the performance. The encoders with 10 blocks is matched with the decoder with 4 blocks, which also considers the performance and the complexity. However, these results also reflect that the generalization capability of H2OFormer is also slightly affected by various blocks. It would be better to know the data complexity before choosing the optimal number of blocks for our proposed H2OFormer.\n\nVarious Hyperedges of H2OFormer. The hyperedges in our proposed method have important impact on the representation ability of deep model, so we investigate the number of hyperedges on two datasets, which is shown in Fig.  5 (a) . From the figure, we can see that using too less or too many hyperedges cannot achieve good performance. On one hand, choosing too less hyperedges means that one joint or keypoint can only be connected to limited joints or keypoints. Take the movements from iMiGUE dataset for example. If choosing too less hyperedges, the keypoints on left arm and hand could not be connected to the keypoints of right arm and hand, which cannot model the joint motion of left and right arms effectively. In other words, the left arm and hand may move with the right arm and hand simultaneously for the upper body. So the joints in left arm and hand could be connected with the joints in right arm and hand. On the other hand, choosing too many hyperedges means that one joint or keypoint can be connected to almost arbitrary joints or keypoints. For example, the keypoints on left arm and hand could be connected to the keypoints of abdomen, which has a large redundancy for the model learning. Slightly different from iMiGUE dataset, the configuration for the connection of body joints could be changed on the SMG dataset, as the SMG collects the fullbody data. Overall, the best configuration of hyperedges seems to be consistent with the prior structure of human body.\n\nOther Hyperparameters of H2OFormer. Here, we further evaluate the impact of using various hidden sizes, different number of attention heads for the model and the usage of dropout learning strategy for model learning. As shown in Fig.  5 (b) , various hidden sizes can affect the recognition performance. Observed from the results on two datasets, when the hidden size is relatively small, the performance on the iMiGUE dataset is better. As the hidden size increases, the accuracy continuously decreases. On the contrary, on the SMG dataset, as the hidden size increases, the accuracy continuously increases. For the Fig.  5 (c ), different number of attention heads can affect the recognition performance. Observed from the results on two datasets, 9 heads used in our implementation part are the best choice for learning a good model. From the Fig.  5 (d) , in which the dropout rate ranges from 0.0 to 0.5, it can be concluded that the dropout strategy is not suitable for the MiGs based emotion recognition. The reason may be that the body joints are connected to each other and densely modeled by the graph convolution, which could not be set to zero by the dropout operation. The Visualization of Hyperedge Representation. As the attention module with dynamic hyperedges are important in the proposed H2OFormer, we illustrate out four attention parts in Eq. 4. The four parts are shown in Fig.  6 , which selectively shows three levels of the beginning, intermediate and final blocks in the encoder. Among four parts, we can see that different-level attention has been explored, which has progressively coarser levels of attention sequentially from part a to part d. All the weights for the attention parts have been changed during the model learning on two datasets. Besides, the attention weights are also dynamically changed with various blocks within the encoder. Similar to the attention module, the hyperedges are updated dynamically and have the obvious visual changes by comparing the first block and last block. The visual results on two datasets can be directly observed in Fig.  7 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Comparison To Sota Methods",
      "text": "As currently few studies focus on the recognition of emotion states based on MiGs, we utilize the results as the baseline methods reported from two datasets  [12] ,  [1]  for performance comparison. Except that, we also implement one of the best MiG recognition method (i.e., PoseC3D  [13] ) and the vanilla hypergraph Transformer for action recognition method (i.e., Hyperformer  [19] ) in the task of MiG based emotion recognition. All these methods are reported with the same protocol and metric. In order to compare these methods fairly, the metric of accuracy used in  [12] ,  [1]  is chosen in the experiment.\n\nOn iMiGUE dataset, deep models with various architectures are utilized to recognize the emotion states, which could be observed in Table  II . The CNN, RNN and GCN architectures (identified in column 2 of Table  II ) are separately used to extract the feature of local motion based on skeleton data. All these methods use the spatial coordinates as the input and predict the emotion states in an end-to-end way. The macrogesture based GCN (i.e., ST-GCN  [64]  and MS-G3D  [65] ) and CNN (i.e., PoseC3D  [13] ) can achieve the almost similar performance, which would be surpassed by modeling the temporal motion with LSTM (i.e., U-SVAE+LSTM and TSM+LSTM  [1] ) or Transformer (i.e., Hyperformer  [19] ). With the enhancement of hyperedges and decoder, our proposed method can achieve better results than LSTM and Transformer based  methods as the local and subtle movements can be modeled more precisely. In addition to the skeleton data, the data with RGB modality in original datasets are also used to recognize the emotion states by exploring CNN based architectures (i.e., TRN  [61] , TSM  [62]  and I3D  [63] ) while the privacy issue directly using RGB data becomes severe. Besides, the performance of skeleton data based methods has been superior to the RGB data based methods. So our proposed H2OFormer outperforms all other methods on iMiGUE dataset. On SMG dataset, the handcrafted or deep features with neural networks (i.e., BayesianNet  [12] , WSGN  [2] , L2GCN  [67]  or BGCN  [67] ) are initially used to recognize the emotion state, shown in Table  III . Different from the deep models  [64] ,  [65] ,  [68]  in an end-to-end way, these models  [67] ,  [67] ,  [12] ,  [2]  recognized MiGs firstly and then the emotion states based on MiGs, which exploits a two-stage learning strategy to obtain the model. In these methods, using GCN based models could not achieve performance as GCN is more suitable to extract features, rather than performing the inference. The classifiers like BayesianNet and WSGN would achieve better results as the direct inference has been performed. In one word, these two-stage recognition pipeline could simplify the recognition procedure while the performance would be limited by the recognition of MiGs even the recognition of MiGs is improved greatly recently  [17] . For the methods in an end-toend way which learn the features and classifiers jointly like  [64] ,  [65] ,  [68] , it becomes difficult to learn representative features from the whole body joints. The samples from SMG has larger variation for the full body having more meaningless gestures like shaking legs. So the reconstruction of MiGs is more difficult and the model would focus more on the recognition of emotion states by using different loss weights compared to the iMiGUE dataset. With the enhanced hyperedges, the performance of our proposed H2OFormer on SMG dataset is greatly improved, and it outperforms other methods.\n\nReading the emotions from the MiGs is not only difficult to the deep models but also challenging for the human beings. Investigated by  [2]  on SMG dataset, the ordinary college students and university staff without any related knowledge were recruited and evaluated as common people, while another three university staff with psychological knowledge were trained to recognize MiGs as human experts. In the existing works, the learning based methods could purse the similar emotion recognition ability of common people, while the experts achieve higher performance with a clear gap than learning based methods. In this work, with using the hyperedge representation, the performance of deep learning based method could reach the level of human experts. From the results of Table  III , it can be concluded that it is the first time for the learning based method to achieve the performance comparable to human experts. However, the human experts trend to multiple cues such as facial expressions and even overall impressions from appearance based data (RGB data) to determine the emotional stress states  [2] . So the cognition level of learning based methods has been promoted from the common people to the professional experts by our proposed H2OFormer.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "D. Discussion",
      "text": "From the comparison and ablation results, it can be observed that our proposed method by leveraging the hyperedge representation can achieve the best performance of 0.70+ accuracy beyond all existing learning based methods in different scenarios. The existing methods based on GCNs for skeleton data have explored the temporal motion information, multi-scale information and the architecture search strategy to continuously improve the recognition performance, while our work tries a different way for further considering the relationships of body joints. It shows that the combination of the dynamic hyperedge based self-attention and the one-stage learning in an encoderdecoder-like architecture could improve the recognition ability for revealing the hidden emotions. Especially, the usage of effective hyperedge representation provides a new perspective to explore the motion information by considering the relationships of various body joints. Besides, this work implies that the deep learning based technique by incorporating more modules may improve the recognition ability of hidden emotions. With continued progress of recognition accuracy from 0.4+ to 0.7+ in recent years, our vision based method has caught up with the level of humans. This will greatly enhance confidence in the use of the technology with being helpful to humans.\n\nAlthough our proposed model achieves promising performance for the MiG based emotion recognition, some disadvantages still exist by observing the ablation study experiments. Firstly, the architecture of the deep model is not very robust. With various blocks of encoder and decoder in H2OFormer, the performance still changes with a certain degree of randomness. It has not show the obvious increase or decrease rules with adding or reducing more blocks. It will become tricky to choose a suitable number of blocks for new application scenarios. Secondly, our proposed method promotes the recognition ability by introducing more computation with designing a dynamic hyperedge representation in each block compared to the existing hypergraph based models. It obviously increases requirements for hardware resources and training time.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "This paper proposed hypergraphenhanced Transformer for micro-gesture based hidden emotion recognition. In the proposed method, the Transformer based encoder and decoder were separately designed by utilizing the hypergraph-enhanced self-attention for reconstructing the local and subtle MiGs. In the Transformer, the hyperedges between body joints were literally updated to enhancing the representation of the relationships of local motion. Based on the motion reconstruction, a recognition head from the encoder was further constructed for modeling the microgestures and the emotion states. The results on ablation-study and comparison to SOTAs showed that the proposed method achieved the best performance on two publicly well-known datasets. In the future work, we will explore a more effective module for measuring the relationship of different body joints and design a robust architecture based on the action generation technique.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture overview of our proposed model (H2OFormer), which is consisted of four components, i.e., the Encoder,",
      "page": 4
    },
    {
      "caption": "Figure 1: and then some core components are explained in detail in the",
      "page": 5
    },
    {
      "caption": "Figure 2: The hypergraph-enhanced Transformer (HET) module",
      "page": 6
    },
    {
      "caption": "Figure 2: Hyperedge Representation. Based on the incidence ma-",
      "page": 6
    },
    {
      "caption": "Figure 2: In the encoder, the goal is to extract the motion feature,",
      "page": 7
    },
    {
      "caption": "Figure 3: Examples (chosen frames from the entire sequence) of",
      "page": 8
    },
    {
      "caption": "Figure 4: Performance comparison of different encoder blocks on iMiGUE and SMG dataset.",
      "page": 9
    },
    {
      "caption": "Figure 5: Performance of using different numbers of hyperedges (a), attention heads (b), dropout rates (c) and hidden sizes (d)",
      "page": 10
    },
    {
      "caption": "Figure 4: In this figure, the",
      "page": 10
    },
    {
      "caption": "Figure 6: The visualization of four attention parts a, b, c and d",
      "page": 11
    },
    {
      "caption": "Figure 5: (b), various hidden sizes can affect the recognition per-",
      "page": 11
    },
    {
      "caption": "Figure 5: (c), different number of attention",
      "page": 11
    },
    {
      "caption": "Figure 5: (d), in which the dropout rate ranges from 0.0 to 0.5,",
      "page": 11
    },
    {
      "caption": "Figure 7: The visualization of hyperedge features in the first and",
      "page": 11
    },
    {
      "caption": "Figure 7: C. Comparison to SOTA Methods",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "movement such as": "related movement s",
          "folding arms or mov": "uch as playing with",
          "ing legs, a": "clothes. U",
          "nd object": "nlike ordi",
          "-": "-"
        },
        {
          "movement such as": "nary body moveme",
          "folding arms or mov": "nts of the hands, hea",
          "ing legs, a": "d and oth",
          "nd object": "er parts of",
          "-": ""
        },
        {
          "movement such as": "the body that allow",
          "folding arms or mov": "individuals to exhibi",
          "ing legs, a": "t emotions",
          "nd object": "explicitly,",
          "-": ""
        },
        {
          "movement such as": "artistically and abst",
          "folding arms or mov": "ractly [3], micro-ges",
          "ing legs, a": "tures can",
          "nd object": "be used to",
          "-": ""
        },
        {
          "movement such as": "explore the emotion",
          "folding arms or mov": "al states that people",
          "ing legs, a": "express in",
          "nd object": "tentionally",
          "-": ""
        },
        {
          "movement such as": "andrefertosomesp",
          "folding arms or mov": "ecificgesturalbehavi",
          "ing legs, a": "ors,which",
          "nd object": "areusually",
          "-": ""
        },
        {
          "movement such as": "related to partial r",
          "folding arms or mov": "egions of human bo",
          "ing legs, a": "dy and h",
          "nd object": "ave subtle",
          "-": ""
        },
        {
          "movement such as": "movements.Soordi",
          "folding arms or mov": "narybodymovements",
          "ing legs, a": "areusedt",
          "nd object": "ofacilitate",
          "-": ""
        },
        {
          "movement such as": "non-verbal commun",
          "folding arms or mov": "ications and are help",
          "ing legs, a": "ful to be",
          "nd object": "understood",
          "-": ""
        },
        {
          "movement such as": "by others, while th",
          "folding arms or mov": "e MiG is an impor",
          "ing legs, a": "tant clue",
          "nd object": "to analyze",
          "-": ""
        },
        {
          "movement such as": "the people’s hidden",
          "folding arms or mov": "",
          "ing legs, a": "",
          "nd object": "",
          "-": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "movementsandusu",
          "g the communicative": "",
          "or illustr": "",
          "ative body": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "happiness, disgust,",
          "otional categories,": "",
          "i.e., sadne": "",
          "ss, anger,": ""
        },
        {
          "Column_1": "ordinary body mov",
          "otional categories,": "",
          "i.e., sadne": "",
          "ss, anger,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "bei",
          "Column_2": "ngs’",
          "Column_3": "",
          "Column_4": "hidde",
          "Column_5": "n emo",
          "Column_6": "tional",
          "Column_7": "statu",
          "Column_8": "s spo",
          "s it": "ntaneously",
          "can": "",
          "reveal": ". They",
          "human": "are",
          "Column_13": "a"
        },
        {
          "Column_1": "spe",
          "Column_2": "cific",
          "Column_3": "",
          "Column_4": "type",
          "Column_5": "of ges",
          "Column_6": "tures (",
          "Column_7": "body",
          "Column_8": "move",
          "s it": "ments) th",
          "can": "",
          "reveal": "at are s",
          "human": "ponta",
          "Column_13": "-"
        },
        {
          "Column_1": "neously",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "and u",
          "Column_5": "ncons",
          "Column_6": "ciously",
          "Column_7": "elicited b",
          "Column_8": "",
          "s it": "y in",
          "can": "ner fe",
          "reveal": "elings,",
          "human": "which",
          "Column_13": ""
        },
        {
          "Column_1": "usu",
          "Column_2": "ally",
          "Column_3": "",
          "Column_4": "induce",
          "Column_5": "subtl",
          "Column_6": "e body",
          "Column_7": "move",
          "Column_8": "ment",
          "s it": "s and",
          "can": "are",
          "reveal": "easily ig",
          "human": "nored",
          "Column_13": ""
        },
        {
          "Column_1": "by",
          "Column_2": "or",
          "Column_3": "di",
          "Column_4": "nary",
          "Column_5": "people",
          "Column_6": "[1], [",
          "Column_7": "2]. T",
          "Column_8": "he m",
          "s it": "ovem",
          "can": "ents",
          "reveal": "of Mi",
          "human": "G can",
          "Column_13": ""
        },
        {
          "Column_1": "be",
          "Column_2": "roughly c",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "ategor",
          "Column_6": "ized a",
          "Column_7": "s self",
          "Column_8": "-adap",
          "s it": "tor",
          "can": "move",
          "reveal": "ment su",
          "human": "ch as",
          "Column_13": ""
        },
        {
          "Column_1": "scratchi",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "ng the",
          "Column_5": "head",
          "Column_6": "and ru",
          "Column_7": "bbing",
          "Column_8": "the h",
          "s it": "and,",
          "can": "defen",
          "reveal": "sive-be",
          "human": "havior",
          "Column_13": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nhancedself-attentionmodule": "the hyperedges of body joints,",
          "Column_2": ""
        },
        {
          "nhancedself-attentionmodule": "dedintothevanillahypergraph",
          "Column_2": ""
        },
        {
          "nhancedself-attentionmodule": "rbettercapturingthelocalmo",
          "Column_2": "-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "encoderanddecoderarchitec": "enhanced Transformer, which",
          "-": ""
        },
        {
          "encoderanddecoderarchitec": "le movement for skeleton data",
          "-": ""
        },
        {
          "encoderanddecoderarchitec": "self-supervised way of motion",
          "-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "beliefnetwork(DBN)andlon",
          "oftwodatasets[1],[2],thedeep": "gshort-termmemories(LSTMs)"
        },
        {
          "Column_1": "were separately utilized to es",
          "oftwodatasets[1],[2],thedeep": "timate the hidden emotion states"
        },
        {
          "Column_1": "based on the micro-gestures.",
          "oftwodatasets[1],[2],thedeep": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "arning strategy using hybrid": "integrate the self-supervised",
          "-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "methods, the GCNs based me": "performance for the MiG a",
          "thods [14], [15] show promising": "nalysis, which utilize the graph"
        },
        {
          "methods, the GCNs based me": "or hypergraph constructed fro",
          "thods [14], [15] show promising": "m the body joints to extract the"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Some developments of emplo": "tothejointsinahypergraph[",
          "ying dynamic temporal weights": "18]orglobalsystemicdynamics"
        },
        {
          "Some developments of emplo": "[?]withEuclideandistanceb",
          "ying dynamic temporal weights": "etweeneachjointmaybehelpful"
        },
        {
          "Some developments of emplo": "toimprovetherepresentation",
          "ying dynamic temporal weights": "ability.However,theseextension"
        },
        {
          "Some developments of emplo": "methodsutilizethetemporal",
          "ying dynamic temporal weights": "orglobalmotioninformationbut"
        },
        {
          "Some developments of emplo": "ignore the local spatial relati",
          "ying dynamic temporal weights": "onship between each body parts"
        },
        {
          "Some developments of emplo": "(joint group, i.e., left arm an",
          "ying dynamic temporal weights": "d right arm), which needs to be"
        },
        {
          "Some developments of emplo": "explored with more represen",
          "ying dynamic temporal weights": "tative modeling by considering"
        },
        {
          "Some developments of emplo": "the interaction of body joints",
          "ying dynamic temporal weights": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "learning methods for emotional",
          "ementsandtheself-supervised": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Transformer architecture": "andonedecoder,whicharese",
          "is used to construct one encoder": "paratelydesignedbyutilizingthe"
        },
        {
          "the Transformer architecture": "hypergraph-enhanced self-att",
          "is used to construct one encoder": "ention and multiscale temporal"
        },
        {
          "the Transformer architecture": "convolution for extending the",
          "is used to construct one encoder": "vanilla hypergraph Transformer"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "rized as the communicative, fu",
          "According to [3], the body": "",
          "movements are usually catego": "nctional, artistic, and abstract",
          "-": ""
        },
        {
          "Column_1": "movements, and the first type",
          "According to [3], the body": "",
          "movements are usually catego": "attracts most attention in past",
          "-": ""
        },
        {
          "Column_1": "years. Except the data captured",
          "According to [3], the body": "",
          "movements are usually catego": "by motion and pressure sen",
          "-": "-"
        },
        {
          "Column_1": "sors, the computer vision techn",
          "According to [3], the body": "",
          "movements are usually catego": "iques due to their high avail",
          "-": "-"
        },
        {
          "Column_1": "abi",
          "According to [3], the body": "lity are the focus in the cont",
          "movements are usually catego": "",
          "-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the vanilla hypergraph Transf": "self-attentioninH2OFormerc",
          "ormer, the hypergraph-enhanced": "ouldupdatetherelationshipsbe",
          "Column_3": "-"
        },
        {
          "the vanilla hypergraph Transf": "tweenhyperedgesdynamicall",
          "ormer, the hypergraph-enhanced": "",
          "Column_3": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "by an encoder for ordinary bo",
          "bly capture the motion feature": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "work refers to the self-supervis",
          "rchitecture we designed in this": "ed learning (SSL), we summa",
          "Column_3": "-"
        },
        {
          "Column_1": "rizeseveralSSLmethodsinthi",
          "rchitecture we designed in this": "ssection.SSLusuallycanlearn",
          "Column_3": ""
        },
        {
          "Column_1": "discriminatedfeaturesfromvar",
          "rchitecture we designed in this": "iousdatawithoutrelyingonhu",
          "Column_3": "-"
        },
        {
          "Column_1": "man-annotated labels [32] and",
          "rchitecture we designed in this": "achieves great development in",
          "Column_3": ""
        },
        {
          "Column_1": "thepastdecade[33].Ononeha",
          "rchitecture we designed in this": "nd,SSLbasedononemodality,",
          "Column_3": ""
        },
        {
          "Column_1": "i.e.,skeletondata,hasbeenapp",
          "rchitecture we designed in this": "liedtovarioustaskslikeaction",
          "Column_3": ""
        },
        {
          "Column_1": "recognition [34], [35], [36], [",
          "rchitecture we designed in this": "37], [38], [39], which has the",
          "Column_3": ""
        },
        {
          "Column_1": "similar input of MiG based em",
          "rchitecture we designed in this": "otion recognition. On the other",
          "Column_3": ""
        },
        {
          "Column_1": "hand,asmostdatasetsinthefi",
          "rchitecture we designed in this": "eldofemotionrecognitioncon",
          "Column_3": "-"
        },
        {
          "Column_1": "taininsufficientsamplesandus",
          "rchitecture we designed in this": "uallylimitthefeaturelearning,",
          "Column_3": ""
        },
        {
          "Column_1": "SSL has also been applied to l",
          "rchitecture we designed in this": "earn emotion representation for",
          "Column_3": ""
        },
        {
          "Column_1": "furtheranalysisfromvariousm",
          "rchitecture we designed in this": "odalities,e.g.,facialexpression",
          "Column_3": ""
        },
        {
          "Column_1": "[40], [41], [42], [43], [44], [45",
          "rchitecture we designed in this": "], [46], speech [47], [48], gait",
          "Column_3": ""
        },
        {
          "Column_1": "[49],[50],electrocardiograms(",
          "rchitecture we designed in this": "ECG)[51],[52]andElectroen",
          "Column_3": "-"
        },
        {
          "Column_1": "cephalography(EEG)[53],[54",
          "rchitecture we designed in this": "].AslimitedSSLmethodshave",
          "Column_3": ""
        },
        {
          "Column_1": "been used for skeleton data b",
          "rchitecture we designed in this": "ased emotion recognition, we",
          "Column_3": ""
        },
        {
          "Column_1": "would revisit the SSL methods",
          "rchitecture we designed in this": "for skeleton data and emotion",
          "Column_3": ""
        },
        {
          "Column_1": "recognition, which may be h",
          "rchitecture we designed in this": "elpful to design a method for",
          "Column_3": ""
        },
        {
          "Column_1": "skeleton based MiG analysis.",
          "rchitecture we designed in this": "",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "rchitecture we designed in this": "ta to explore robust representa",
          "Column_3": "-"
        },
        {
          "Column_1": "tions, some approaches in the",
          "rchitecture we designed in this": "task of action recognition have",
          "Column_3": ""
        },
        {
          "Column_1": "been presented with the SSL s",
          "rchitecture we designed in this": "trategy inspired by the success",
          "Column_3": ""
        },
        {
          "Column_1": "of self-supervised learning in",
          "rchitecture we designed in this": "image and video tasks. As",
          "Column_3": "a"
        },
        {
          "Column_1": "primary work for skeleton dat",
          "rchitecture we designed in this": "a, MS2L [34] integrated recon",
          "Column_3": "-"
        },
        {
          "Column_1": "struction, classification, and pr",
          "rchitecture we designed in this": "ojection tasks based on recur",
          "Column_3": "-"
        },
        {
          "Column_1": "rentlayersforself-learningand",
          "rchitecture we designed in this": "thenobtainedskeletonfeatures",
          "Column_3": ""
        },
        {
          "Column_1": "fromactionrecognition.In[35",
          "rchitecture we designed in this": "],thesemanticinvarianceunder",
          "Column_3": ""
        },
        {
          "Column_1": "thevariantofdistanceandview",
          "rchitecture we designed in this": "pointwasmodeledbytwosep",
          "Column_3": "-"
        },
        {
          "Column_1": "arate transformations with cont",
          "rchitecture we designed in this": "rastive learning of skeleton se",
          "Column_3": "-"
        },
        {
          "Column_1": "quencetoagivenskeletonseq",
          "rchitecture we designed in this": "uence,whichmaximizedagree",
          "Column_3": "-"
        },
        {
          "Column_1": "ment by using the contrastive",
          "rchitecture we designed in this": "loss. Furthermore, a focalized",
          "Column_3": ""
        },
        {
          "Column_1": "contrastive view-invariant learn",
          "rchitecture we designed in this": "ing framework by gated recur",
          "Column_3": "-"
        },
        {
          "Column_1": "rent unit based encoder and",
          "rchitecture we designed in this": "decoder [36] was proposed to",
          "Column_3": ""
        },
        {
          "Column_1": "maximize the mutual informat",
          "rchitecture we designed in this": "ion between multi-view action",
          "Column_3": ""
        },
        {
          "Column_1": "pairs by adapting contrastive",
          "rchitecture we designed in this": "self-learning, which leveraged",
          "Column_3": ""
        },
        {
          "Column_1": "a dynamic-scaled focal loss ba",
          "rchitecture we designed in this": "sed on the geometric distance",
          "Column_3": ""
        },
        {
          "Column_1": "of the contrastive representatio",
          "rchitecture we designed in this": "ns. in [37], the query, extended",
          "Column_3": ""
        },
        {
          "Column_1": "queryandkeyencoderswereu",
          "rchitecture we designed in this": "sedtoextractthetemporalmo",
          "Column_3": "-"
        },
        {
          "Column_1": "tionfeaturesandthentrainedb",
          "rchitecture we designed in this": "yminimizingdualdistributional",
          "Column_3": ""
        },
        {
          "Column_1": "divergence for skeleton sequen",
          "rchitecture we designed in this": "ces. Besides, the skeleton data",
          "Column_3": ""
        },
        {
          "Column_1": "could be transformed into the",
          "rchitecture we designed in this": "image-like data and the image",
          "Column_3": ""
        },
        {
          "Column_1": "basedSSLmethodcanbeappli",
          "rchitecture we designed in this": "eddirectly[38].Across-stream",
          "Column_3": ""
        },
        {
          "Column_1": "contrastive learning model [39",
          "rchitecture we designed in this": "] was exploited by two GCN",
          "Column_3": "-"
        },
        {
          "Column_1": "based skeleton encoders to ca",
          "rchitecture we designed in this": "lculate the correspondence of",
          "Column_3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "three": "in",
          "Column_2": "cre",
          "op": "ment,",
          "era": "",
          "tions, i.e": "and false",
          "., positives augmentati": "-negatives cancellation",
          "on, hard-negatives": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "prove the performance of ske",
          "methodshavebeenshowntoim": "leton data based action recog",
          "Column_3": "",
          "-": "-"
        },
        {
          "Column_1": "nition and emotion recognitio",
          "methodshavebeenshowntoim": "n for 1D, 2D or 3D data. For",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "skeleton data, most approache",
          "methodshavebeenshowntoim": "s construct various encoders",
          "Column_3": "in",
          "-": ""
        },
        {
          "Column_1": "differentviewsormodelsand",
          "methodshavebeenshowntoim": "performthecontrastivelearning",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "among these encoders, while",
          "methodshavebeenshowntoim": "few approaches try to recon",
          "Column_3": "",
          "-": "-"
        },
        {
          "Column_1": "struct the actions due to the",
          "methodshavebeenshowntoim": "difficulty of designing decoders",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "for skeleton sequences. For e",
          "methodshavebeenshowntoim": "motion tasks, most of them use",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "the SSL strategy to obtain a p",
          "methodshavebeenshowntoim": "retrained model and then apply",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "it for recognizing emotions d",
          "methodshavebeenshowntoim": "irectly in an unsupervised way",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "(one-stage) or fine-tune it with",
          "methodshavebeenshowntoim": "limited supervised information",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "(two-stage). However, these d",
          "methodshavebeenshowntoim": "eep models that are used with",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "SSL are mainly focused on t",
          "methodshavebeenshowntoim": "he significant motion from the",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "skeletondata,whichusuallyd",
          "methodshavebeenshowntoim": "onotincorporatethedynamical",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "relationships between various",
          "methodshavebeenshowntoim": "body joints and temporal vari",
          "Column_3": "a",
          "-": "-"
        },
        {
          "Column_1": "tionsfortheemotionalmotion",
          "methodshavebeenshowntoim": ".Sothespecialarchitectureand",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "SSL strategy need to be furt",
          "methodshavebeenshowntoim": "her studied in the task of MiG",
          "Column_3": "",
          "-": ""
        },
        {
          "Column_1": "based emotion recognition wi",
          "methodshavebeenshowntoim": "th skeleton data, which usually",
          "Column_3": "",
          "-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "els",
          "Column_2": "li",
          "Column_3": "ke",
          "Column_4": "GCN and",
          "Column_5": "",
          "49],twoindependentr": "LSTM neural network",
          "epresentationmod": "were constructed",
          "Column_8": "",
          "-": ""
        },
        {
          "Column_1": "and t",
          "Column_2": "",
          "Column_3": "hen",
          "Column_4": "used to con",
          "Column_5": "",
          "49],twoindependentr": "trastively learn pain-le",
          "epresentationmod": "vel representations",
          "Column_8": "",
          "-": ""
        },
        {
          "Column_1": "from",
          "Column_2": "",
          "Column_3": "body",
          "Column_4": "",
          "Column_5": "moveme",
          "49],twoindependentr": "nts by using a compari",
          "epresentationmod": "son network. Gait",
          "Column_8": "",
          "-": ""
        },
        {
          "Column_1": "based",
          "Column_2": "",
          "Column_3": "emo",
          "Column_4": "",
          "Column_5": "tion repre",
          "49],twoindependentr": "sentation [50] was ob",
          "epresentationmod": "tained by a cross",
          "Column_8": "",
          "-": "-"
        },
        {
          "Column_1": "co",
          "Column_2": "ord",
          "Column_3": "inate",
          "Column_4": "",
          "Column_5": "contrastiv",
          "49],twoindependentr": "e learning framework.",
          "epresentationmod": "In this framework,",
          "Column_8": "",
          "-": ""
        },
        {
          "Column_1": "multi",
          "Column_2": "",
          "Column_3": "ple",
          "Column_4": "en",
          "Column_5": "coders w",
          "49],twoindependentr": "ere learned by consid",
          "epresentationmod": "ering the seman",
          "Column_8": "tic",
          "-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "thebasisofhypergraph,which",
          "nitionframeworkisdesignedon": "wouldbeintroducedinthefol-"
        },
        {
          "Column_1": "lowingcontentforastart.Gen",
          "nitionframeworkisdesignedon": "erallyspeaking,thehypergraph"
        },
        {
          "Column_1": "is a generalization of graph i",
          "nitionframeworkisdesignedon": "n which an edge can join any"
        },
        {
          "Column_1": "number of vertices (also calle",
          "nitionframeworkisdesignedon": "d as hyperedge) for geometric-"
        },
        {
          "Column_1": "like data and has been applie",
          "nitionframeworkisdesignedon": "d in many vision-related tasks,"
        },
        {
          "Column_1": "such as data classification [55]",
          "nitionframeworkisdesignedon": ""
        },
        {
          "Column_1": "",
          "nitionframeworkisdesignedon": "byconsideringeachjoint(key-"
        },
        {
          "Column_1": "point) of human body as a ve",
          "nitionframeworkisdesignedon": "rtex v and their potential con-"
        },
        {
          "Column_1": "nection as a hyperedge e in",
          "nitionframeworkisdesignedon": "a graph, the hypergraph H is"
        },
        {
          "Column_1": "constructed by containing the",
          "nitionframeworkisdesignedon": "vertices V and their hyperedges"
        },
        {
          "Column_1": "E, which is H=(V,E). With",
          "nitionframeworkisdesignedon": "this representation, the spatial-"
        },
        {
          "Column_1": "temporalmotioninformationc",
          "nitionframeworkisdesignedon": "anbeextractedfromthishyper-"
        },
        {
          "Column_1": "graphbyusingmulti-layernet",
          "nitionframeworkisdesignedon": "workwithvariousarchitectures."
        },
        {
          "Column_1": "The numbers of vertex and hy",
          "nitionframeworkisdesignedon": "peredge are denoted as |V| and"
        },
        {
          "Column_1": "|E|. The incidence matrix H",
          "nitionframeworkisdesignedon": "∈{0,1}|V|×|E| can be used to"
        },
        {
          "Column_1": "quantitatively characterize the",
          "nitionframeworkisdesignedon": "hypergraph H. Each entry in"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "cap",
          "Column_2": "tured",
          "Column_3": "",
          "Column_4": "by",
          "Column_5": "the",
          "lo": "en",
          "cal": "coder’s",
          "and": "",
          "sub": "rep",
          "tle": "re",
          "move": "sen",
          "Column_12": "ta",
          "Column_13": "tion",
          "ments": "",
          "Column_15": "",
          "Column_16": "vec",
          "could": "tor",
          "Column_18": "",
          "Column_19": "fo",
          "be": "r d",
          "roughly": "epict",
          "Column_22": "ing"
        },
        {
          "Column_1": "MiGs.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "lo": "",
          "cal": "",
          "and": "",
          "sub": "",
          "tle": "",
          "move": "",
          "Column_12": "",
          "Column_13": "",
          "ments": "",
          "Column_15": "",
          "Column_16": "",
          "could": "",
          "Column_18": "",
          "Column_19": "",
          "be": "",
          "roughly": "",
          "Column_22": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "dataset)sharethesameinciden",
          "scenario (i.e., from the same": "cematrix.Thedegreeofaver-"
        },
        {
          "Column_1": "(cid:80)\ntex is calculated as d(v)=",
          "scenario (i.e., from the same": "H(v,e), while the degree of\ne"
        },
        {
          "Column_1": "a hyperedge is calculated as d",
          "scenario (i.e., from the same": "(cid:80)\n(e)= H(v,e). The degree\nv"
        },
        {
          "Column_1": "matrices D ∈R|V|×|V| and\nv",
          "scenario (i.e., from the same": "D ∈R|E|×|E| for vertices and\ne"
        },
        {
          "Column_1": "hyperedges are then obtained b",
          "scenario (i.e., from the same": "y setting all the vertex degrees"
        },
        {
          "Column_1": "d(v) and all the edge degrees",
          "scenario (i.e., from the same": "d(e) as their diagonal entries,"
        },
        {
          "Column_1": "respectively. In the initial stag",
          "scenario (i.e., from the same": "e of the hypergraph, one vertex"
        },
        {
          "Column_1": "only belongs to one hyperedge",
          "scenario (i.e., from the same": ", which means D (i)=1, but\nv"
        },
        {
          "Column_1": "one hyperedge may be conne",
          "scenario (i.e., from the same": "cted to many vertices. In this"
        },
        {
          "Column_1": "case, all body joints are separa",
          "scenario (i.e., from the same": ""
        },
        {
          "Column_1": "",
          "scenario (i.e., from the same": "s a feature vector and the fea-"
        },
        {
          "Column_1": "tures for the hypergraph can",
          "scenario (i.e., from the same": "be denoted X ∈R|V|×D. D is"
        },
        {
          "Column_1": "the dimension of the vertex,",
          "scenario (i.e., from the same": "which is equal to 2 or 3 if X"
        },
        {
          "Column_1": "represents the original spatial",
          "scenario (i.e., from the same": "coordinates of the body joints."
        },
        {
          "Column_1": "So the subset representations f",
          "scenario (i.e., from the same": "or graph based neural network"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "whereW ∈RD×|E|represent\ne": "beassignedtoeachhyperedge",
          "sthelearnableweightandcould": "forobtainingbetterrepresenta-"
        },
        {
          "whereW ∈RD×|E|represent\ne": "tions. Based on the hypergrap",
          "sthelearnableweightandcould": "h, the task of emotion recogni-"
        },
        {
          "whereW ∈RD×|E|represent\ne": "tionturnstolearnmultiplewei",
          "sthelearnableweightandcould": "ghtsforclassifyingthevertices"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "uti",
          "Column_2": "lize",
          "Column_3": "",
          "Column_4": "the l",
          "Column_5": "inear",
          "Column_6": "projectio",
          "Column_7": "",
          "Column_8": "n layer w",
          "Column_9": "",
          "this": "hich",
          "work": "could",
          "Column_12": "",
          "Column_13": "",
          ",": "be i",
          "we": "",
          "choose": "nserted i",
          "Column_17": "",
          "Column_18": "",
          "to": "nto"
        },
        {
          "Column_1": "the",
          "Column_2": "",
          "Column_3": "HET m",
          "Column_4": "",
          "Column_5": "odul",
          "Column_6": "e",
          "Column_7": "easily",
          "Column_8": "as th",
          "Column_9": "e up",
          "this": "samp",
          "work": "ling",
          "Column_12": "",
          "Column_13": "layer",
          ",": "",
          "we": "",
          "choose": "for",
          "Column_17": "",
          "Column_18": "line",
          "to": "arly"
        },
        {
          "Column_1": "trans",
          "Column_2": "",
          "Column_3": "formin",
          "Column_4": "",
          "Column_5": "g th",
          "Column_6": "e",
          "Column_7": "dimens",
          "Column_8": "ional",
          "Column_9": "",
          "this": "",
          "work": "",
          "Column_12": "",
          "Column_13": "",
          ",": "",
          "we": "",
          "choose": "",
          "Column_17": "",
          "Column_18": "",
          "to": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "utilized to predict the emotion states directly with a shallow\narchitecture. In this context, one multi-head attention, one\nbatch normalization, one MLP, one average pooling and one Xt ∈R|V|×\nfully-connectedlayerareincorporatedintheheadtorecognize\nX ∈RT×|V\nthe emotion states. The output of the prediction head is\nblock), whi\nc ∈ {0,1}, where 1 represents the positive state and 0 the\nintermediate\nnegative state.\njoint (keypo\nmatrix H a\nrepresentatio\n \nand degree\nweight and\nConcat original hyp\nthe represen\nConv Conv MaxPool\ndilation=1 dilation=2 feature. The\n5×1 5×1\n3×1 hyperedgere\nConv Conv Conv Conv\n1×1 1×1 1×1 1×1 Self-atten\nUpSample/Identity UpSample/Identity UpSample/Identity UpSample/Identity\nhyperedgere\nencoderisth\nUpSample/Identity of input fea\nhypergrapha\n    ′\ncomputation\nMatMul MatMul\nSoftmax Softmax\n               \nThe Q, K a\nV K Q\ninputreprese\n     \nious entities\nLinear Linear hyperedge r\nconnection\n1       −1        representsth\n[  ,…,  ,…,  ] partaisthe\nFig. 2: The hypergraph-enhanced Transformer (HET) module\ndata,which\nfor each block. The upsampling operation is used only in": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "par",
          "Column_17": "t a"
        },
        {
          "utilized to predict the emotion states directly with a shallow\narchitecture. In this context, one multi-head attention, one\nbatch normalization, one MLP, one average pooling and one Xt ∈R|V|×\nfully-connectedlayerareincorporatedintheheadtorecognize\nX ∈RT×|V\nthe emotion states. The output of the prediction head is\nblock), whi\nc ∈ {0,1}, where 1 represents the positive state and 0 the\nintermediate\nnegative state.\njoint (keypo\nmatrix H a\nrepresentatio\n \nand degree\nweight and\nConcat original hyp\nthe represen\nConv Conv MaxPool\ndilation=1 dilation=2 feature. The\n5×1 5×1\n3×1 hyperedgere\nConv Conv Conv Conv\n1×1 1×1 1×1 1×1 Self-atten\nUpSample/Identity UpSample/Identity UpSample/Identity UpSample/Identity\nhyperedgere\nencoderisth\nUpSample/Identity of input fea\nhypergrapha\n    ′\ncomputation\nMatMul MatMul\nSoftmax Softmax\n               \nThe Q, K a\nV K Q\ninputreprese\n     \nious entities\nLinear Linear hyperedge r\nconnection\n1       −1        representsth\n[  ,…,  ,…,  ] partaisthe\nFig. 2: The hypergraph-enhanced Transformer (HET) module\ndata,which\nfor each block. The upsampling operation is used only in": "",
          "Column_2": "repre",
          "Column_3": "sentsth",
          "Column_4": "esequenti",
          "Column_5": "",
          "Column_6": "alin",
          "Column_7": "for",
          "Column_8": "mation",
          "Column_9": "ofhu",
          "Column_10": "man",
          "Column_11": "skele",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "ton.",
          "Column_15": "",
          "Column_16": "R\nϕ",
          "Column_17": "in"
        },
        {
          "utilized to predict the emotion states directly with a shallow\narchitecture. In this context, one multi-head attention, one\nbatch normalization, one MLP, one average pooling and one Xt ∈R|V|×\nfully-connectedlayerareincorporatedintheheadtorecognize\nX ∈RT×|V\nthe emotion states. The output of the prediction head is\nblock), whi\nc ∈ {0,1}, where 1 represents the positive state and 0 the\nintermediate\nnegative state.\njoint (keypo\nmatrix H a\nrepresentatio\n \nand degree\nweight and\nConcat original hyp\nthe represen\nConv Conv MaxPool\ndilation=1 dilation=2 feature. The\n5×1 5×1\n3×1 hyperedgere\nConv Conv Conv Conv\n1×1 1×1 1×1 1×1 Self-atten\nUpSample/Identity UpSample/Identity UpSample/Identity UpSample/Identity\nhyperedgere\nencoderisth\nUpSample/Identity of input fea\nhypergrapha\n    ′\ncomputation\nMatMul MatMul\nSoftmax Softmax\n               \nThe Q, K a\nV K Q\ninputreprese\n     \nious entities\nLinear Linear hyperedge r\nconnection\n1       −1        representsth\n[  ,…,  ,…,  ] partaisthe\nFig. 2: The hypergraph-enhanced Transformer (HET) module\ndata,which\nfor each block. The upsampling operation is used only in": "",
          "Column_2": "part",
          "Column_3": "aisthe",
          "Column_4": "k-Hop",
          "Column_5": "rela",
          "Column_6": "tive",
          "Column_7": "pos",
          "Column_8": "itional",
          "Column_9": "embe",
          "Column_10": "dding",
          "Column_11": "",
          "Column_12": "for",
          "Column_13": "",
          "Column_14": "se",
          "Column_15": "quent",
          "Column_16": "",
          "Column_17": "ial"
        },
        {
          "utilized to predict the emotion states directly with a shallow\narchitecture. In this context, one multi-head attention, one\nbatch normalization, one MLP, one average pooling and one Xt ∈R|V|×\nfully-connectedlayerareincorporatedintheheadtorecognize\nX ∈RT×|V\nthe emotion states. The output of the prediction head is\nblock), whi\nc ∈ {0,1}, where 1 represents the positive state and 0 the\nintermediate\nnegative state.\njoint (keypo\nmatrix H a\nrepresentatio\n \nand degree\nweight and\nConcat original hyp\nthe represen\nConv Conv MaxPool\ndilation=1 dilation=2 feature. The\n5×1 5×1\n3×1 hyperedgere\nConv Conv Conv Conv\n1×1 1×1 1×1 1×1 Self-atten\nUpSample/Identity UpSample/Identity UpSample/Identity UpSample/Identity\nhyperedgere\nencoderisth\nUpSample/Identity of input fea\nhypergrapha\n    ′\ncomputation\nMatMul MatMul\nSoftmax Softmax\n               \nThe Q, K a\nV K Q\ninputreprese\n     \nious entities\nLinear Linear hyperedge r\nconnection\n1       −1        representsth\n[  ,…,  ,…,  ] partaisthe\nFig. 2: The hypergraph-enhanced Transformer (HET) module\ndata,which\nfor each block. The upsampling operation is used only in": "",
          "Column_2": "data,",
          "Column_3": "which",
          "Column_4": "isindexed",
          "Column_5": "",
          "Column_6": "from",
          "Column_7": "a",
          "Column_8": "learna",
          "Column_9": "blepar",
          "Column_10": "am",
          "Column_11": "eter",
          "Column_12": "",
          "Column_13": "ta",
          "Column_14": "ble",
          "Column_15": "",
          "Column_16": "byt",
          "Column_17": "he"
        },
        {
          "utilized to predict the emotion states directly with a shallow\narchitecture. In this context, one multi-head attention, one\nbatch normalization, one MLP, one average pooling and one Xt ∈R|V|×\nfully-connectedlayerareincorporatedintheheadtorecognize\nX ∈RT×|V\nthe emotion states. The output of the prediction head is\nblock), whi\nc ∈ {0,1}, where 1 represents the positive state and 0 the\nintermediate\nnegative state.\njoint (keypo\nmatrix H a\nrepresentatio\n \nand degree\nweight and\nConcat original hyp\nthe represen\nConv Conv MaxPool\ndilation=1 dilation=2 feature. The\n5×1 5×1\n3×1 hyperedgere\nConv Conv Conv Conv\n1×1 1×1 1×1 1×1 Self-atten\nUpSample/Identity UpSample/Identity UpSample/Identity UpSample/Identity\nhyperedgere\nencoderisth\nUpSample/Identity of input fea\nhypergrapha\n    ′\ncomputation\nMatMul MatMul\nSoftmax Softmax\n               \nThe Q, K a\nV K Q\ninputreprese\n     \nious entities\nLinear Linear hyperedge r\nconnection\n1       −1        representsth\n[  ,…,  ,…,  ] partaisthe\nFig. 2: The hypergraph-enhanced Transformer (HET) module\ndata,which\nfor each block. The upsampling operation is used only in": "",
          "Column_2": "short",
          "Column_3": "estpath",
          "Column_4": "distance",
          "Column_5": "",
          "Column_6": "between",
          "Column_7": "",
          "Column_8": "theit",
          "Column_9": "h and",
          "Column_10": "jth",
          "Column_11": "joints",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "[19",
          "Column_15": "",
          "Column_16": "",
          "Column_17": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xt ∈R|V|×": "X ∈RT×|V"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "R ∈R|V|×|V|×D\nϕ",
          "part": "",
          "a, t": "",
          "he": "of",
          "rela": "|V|",
          "tiv": "k",
          "e pos": "ey-poi",
          "ition": "nts i",
          "en": "s c",
          "cod": "al",
          "Column_11": "cu",
          "in": "lated",
          "g": "",
          "mat": "from",
          "rix": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "used",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "to",
          "Column_5": "in",
          "Column_6": "crease",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "the",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "di",
          "Column_13": "men",
          "Column_14": "",
          "Column_15": "sion",
          "Column_16": "",
          "up": "of",
          "sam": "the",
          "pling": "in",
          "Column_20": "put",
          "con": "",
          "Column_22": "fea",
          "nec": "ture,",
          "tions": "",
          "Column_25": "",
          "Column_26": "which",
          "are": ""
        },
        {
          "Column_1": "is",
          "Column_2": "",
          "Column_3": "im",
          "Column_4": "ple",
          "Column_5": "mented",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "by",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "a",
          "Column_12": "lin",
          "Column_13": "",
          "Column_14": "ear",
          "Column_15": "pro",
          "Column_16": "ject",
          "up": "",
          "sam": "",
          "pling": "",
          "Column_20": "",
          "con": "",
          "Column_22": "",
          "nec": "",
          "tions": "",
          "Column_25": "",
          "Column_26": "",
          "are": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Here, I": "is",
          "Column_2": "the",
          "is the identit\n|V|": "adjacency matrix",
          "y matrix of order |V|": "of the graph. Define",
          ", and A\nG": "a matrix"
        },
        {
          "Here, I": "R=[⃗r ,⃗r ,··· ,⃗r ]T,wh\n1 2 m",
          "Column_2": "",
          "is the identit\n|V|": "",
          "y matrix of order |V|": "eremisthemaximumv",
          ", and A\nG": "alueofthe"
        },
        {
          "Here, I": "H m\nops",
          "Column_2": "",
          "is the identit\n|V|": "atrix plus one, i.e.",
          "y matrix of order |V|": ", m=max(H )+1,\nops",
          ", and A\nG": "the vector"
        },
        {
          "Here, I": "⃗r ∈R\nm",
          "Column_2": "",
          "is the identit\n|V|": "D, and R∈Rm×",
          "y matrix of order |V|": "D. The relative position",
          ", and A\nG": "encoding"
        },
        {
          "Here, I": "matrix",
          "Column_2": "",
          "is the identit\n|V|": "R can be obtained\nϕ",
          "y matrix of order |V|": "",
          ", and A\nG": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where h": "and |V|-",
          "ops is the valu\nNN": "th column in the",
          "e corresponding to the": "",
          "|V|-th row": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "peredge",
          "Column_2": "",
          "Column_3": "representation in t",
          "Column_4": "his work could be upda",
          "Column_5": "ted in one",
          "-": ""
        },
        {
          "Column_1": "blockan",
          "Column_2": "",
          "Column_3": "dpassedtothene",
          "Column_4": "xtblock(dynamicalhyp",
          "Column_5": "eredgerep",
          "-": "-"
        },
        {
          "Column_1": "re",
          "Column_2": "sentati",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "-": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "hanced",
          "Column_2": "Transformer modu",
          "Column_3": "le, multiple types of sh",
          "rgraph-en": "ortcut con",
          "-": "-"
        },
        {
          "Column_1": "nections",
          "Column_2": ", i.e., identity-ma",
          "Column_3": "",
          "rgraph-en": "",
          "-": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "shortcut",
          "Column_2": "connection like th",
          "ng connections are cho": "e ResNet, which feed t",
          "sen as the": "he input to"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "are": "2D",
          "the 3D spatial coordinate": "spatial coordinates for hig",
          "s, the 4D rotation coordin": "her-resolution images, and",
          "ates,": "2D"
        },
        {
          "are": "spat",
          "the 3D spatial coordinate": "ialcoordinatesforlower-re",
          "s, the 4D rotation coordin": "solutionimages,respective",
          "ates,": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "use",
          "Column_2": "dastheoptimizerfortraini",
          "astic gradient descent (SG": "ngourH2OFormer.Theweights",
          "D) is": "",
          "Column_5": ""
        },
        {
          "Column_1": "and",
          "Column_2": "biases of the convolution",
          "astic gradient descent (SG": "and other learnable param",
          "D) is": "eters",
          "Column_5": ""
        },
        {
          "Column_1": "desi",
          "Column_2": "gned in H2OFormer are",
          "astic gradient descent (SG": "initialized to ensure that",
          "D) is": "the",
          "Column_5": ""
        },
        {
          "Column_1": "gra",
          "Column_2": "dients during the training p",
          "astic gradient descent (SG": "rocess do not vanish or exp",
          "D) is": "lode",
          "Column_5": ""
        },
        {
          "Column_1": "too",
          "Column_2": "quickly.Eachskeletonsequ",
          "astic gradient descent (SG": "enceisfedintotheH2OFormer",
          "D) is": "",
          "Column_5": ""
        },
        {
          "Column_1": "and",
          "Column_2": "is then performed with th",
          "astic gradient descent (SG": "e hypergraph convolution l",
          "D) is": "ayer",
          "Column_5": ""
        },
        {
          "Column_1": "by",
          "Column_2": "layer. The constructed sequ",
          "astic gradient descent (SG": "ence and emotion category",
          "D) is": "are",
          "Column_5": ""
        },
        {
          "Column_1": "obt",
          "Column_2": "ained from the network an",
          "astic gradient descent (SG": "d input into the loss funct",
          "D) is": "ions",
          "Column_5": ""
        },
        {
          "Column_1": "for",
          "Column_2": "computing the gradients",
          "astic gradient descent (SG": "and updating the learnable",
          "D) is": "pa",
          "Column_5": "-"
        },
        {
          "Column_1": "ram",
          "Column_2": "",
          "astic gradient descent (SG": "",
          "D) is": "",
          "Column_5": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "sce",
          "Column_2": "nari",
          "Column_3": "os of",
          "Column_4": "“post-match p",
          "s dataset, several video": "ress” are collected by r",
          "s containing": "ecording an",
          "Column_7": ""
        },
        {
          "Column_1": "ath",
          "Column_2": "lete’",
          "Column_3": "inter",
          "Column_4": "views from jo",
          "s dataset, several video": "urnalists and reporters",
          "s containing": "over match",
          "Column_7": "-"
        },
        {
          "Column_1": "rel",
          "Column_2": "ated",
          "Column_3": "ques",
          "Column_4": "tions & answe",
          "s dataset, several video": "rs. The athletes have",
          "s containing": "no time to",
          "Column_7": ""
        },
        {
          "Column_1": "pre",
          "Column_2": "pare",
          "Column_3": "and",
          "Column_4": "should respon",
          "s dataset, several video": "d rapidly, which would",
          "s containing": "potentially",
          "Column_7": ""
        },
        {
          "Column_1": "lead to",
          "Column_2": "",
          "Column_3": "posit",
          "Column_4": "ive or negative",
          "s dataset, several video": "emotion states of the",
          "s containing": "interviewed",
          "Column_7": ""
        },
        {
          "Column_1": "playera",
          "Column_2": "",
          "Column_3": "shid",
          "Column_4": "denemotions.",
          "s dataset, several video": "",
          "s containing": "",
          "Column_7": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "able",
          "Column_2": "on Github (https://github.",
          "of the implementation is a": "com/xiazhaoqiang/H2OFormer",
          "vail": "",
          "-": "-"
        },
        {
          "Column_1": "Mic",
          "Column_2": "",
          "of the implementation is a": "",
          "vail": "",
          "-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ind",
          "Column_2": "ependentprotocolisusedt",
          "Column_3": "oevaluatetheproposedmethod,",
          "ject": "",
          "-": ""
        },
        {
          "Column_1": "which uses the data from the",
          "Column_2": "",
          "Column_3": "separate subjects (particip",
          "ject": "ants)",
          "-": ""
        },
        {
          "Column_1": "for",
          "Column_2": "",
          "Column_3": "",
          "ject": "",
          "-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "rep",
          "Column_2": "eatin",
          "Column_3": "g sto",
          "Column_4": "ries are design",
          "taset, eliciting tasks by": "ed to observe the MiG",
          "telling and": "s when the",
          "Column_7": ""
        },
        {
          "Column_1": "pa",
          "Column_2": "rticip",
          "Column_3": "ants",
          "Column_4": "need to prove",
          "taset, eliciting tasks by": "that they knew the st",
          "telling and": "ory content,",
          "Column_7": ""
        },
        {
          "Column_1": "res",
          "Column_2": "pecti",
          "Column_3": "vely,",
          "Column_4": "no matter the",
          "taset, eliciting tasks by": "y are assigned with a r",
          "telling and": "eal story or",
          "Column_7": ""
        },
        {
          "Column_1": "an",
          "Column_2": "emp",
          "Column_3": "tystory.Thepartici",
          "Column_4": "",
          "taset, eliciting tasks by": "pantshavemoreemotio",
          "telling and": "nalinvolve",
          "Column_7": "-"
        },
        {
          "Column_1": "ment fo",
          "Column_2": "",
          "Column_3": "r ma",
          "Column_4": "king up a fak",
          "taset, eliciting tasks by": "e story and exhibit tw",
          "telling and": "o emotional",
          "Column_7": ""
        },
        {
          "Column_1": "stress s",
          "Column_2": "",
          "Column_3": "tates",
          "Column_4": "as hidden em",
          "taset, eliciting tasks by": "",
          "telling and": "",
          "Column_7": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ran": "been",
          "domly": "",
          "Column_3": "",
          "Column_4": "masked",
          "Column_5": "",
          "cho": "",
          "sen": "fol",
          "and": "low",
          "Column_9": "ing",
          "set": "",
          "Column_11": "",
          "to": "[60]",
          "Column_13": "",
          "ze": "",
          "ros,": "",
          "Column_16": "",
          "in": "",
          "Column_18": "",
          "which": "",
          "Column_20": "",
          "30%": "",
          "Column_22": "",
          "joints": "",
          "Column_24": "",
          "have": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "per",
          "Column_2": "formed",
          "Column_3": "",
          "Column_4": "on",
          "Column_5": "",
          "Column_6": "the",
          "Column_7": "",
          "Column_8": "same",
          "Column_9": "dataset",
          "Column_10": "",
          "Column_11": "with",
          "The": "",
          "Column_13": "out",
          "two-": "any",
          "stage": "",
          "Column_16": "ad",
          "Column_17": "di",
          "train": "tional",
          "ing": "",
          "Column_20": "data.",
          "Column_21": "",
          "is": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "on",
          "Column_2": "iMiGUE",
          "Per": "",
          "for": "",
          "Column_5": "and",
          "mance": "",
          "Column_7": "SMG",
          "Column_8": "",
          "of": "",
          "us": "dataset",
          "ing": "",
          "Column_12": "",
          "dif": "",
          "fer": "",
          "ent": "",
          "Column_16": "",
          "num": "",
          "bers": "",
          "Column_19": "",
          "Column_21": "",
          "hy": "",
          "per": "",
          "edges": "",
          "Column_25": "",
          "(a), attention heads (b), drop": "",
          "out rates (c) and hidden size": "",
          "s (d)": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "body joints (more keypoint",
          "ubjects in SMG dataset have": "s) than the iMiGUE dataset,",
          "full-": "the"
        },
        {
          "Column_1": "modelhavingmoreblocksm",
          "ubjects in SMG dataset have": "aybebetterthanthesimpler",
          "full-": "one"
        },
        {
          "Column_1": "(i.e.,themodeloniMiGUE",
          "ubjects in SMG dataset have": "",
          "full-": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "these results also reflect tha",
          "Column_2": "t the generalization capabilit",
          "ever,": "y of"
        },
        {
          "Column_1": "H2OFormer is also slightly",
          "Column_2": "affected by various block",
          "ever,": "s. It"
        },
        {
          "Column_1": "would be better to know the",
          "Column_2": "data complexity before choo",
          "ever,": "sing"
        },
        {
          "Column_1": "the optimal number of block",
          "Column_2": "s for our proposed H2OFor",
          "ever,": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "choosingtoolesshyperedges",
          "Column_2": "meansthatonejointorkeyp",
          "and,": "oint"
        },
        {
          "Column_1": "can only be connected to lim",
          "Column_2": "ited joints or keypoints. Take",
          "and,": "the"
        },
        {
          "Column_1": "movements from iMiGUE",
          "Column_2": "dataset for example. If choo",
          "and,": "sing"
        },
        {
          "Column_1": "too less hyperedges, the key",
          "Column_2": "points on left arm and hand c",
          "and,": "ould"
        },
        {
          "Column_1": "not be connected to the ke",
          "Column_2": "ypoints of right arm and h",
          "and,": "and,"
        },
        {
          "Column_1": "which cannot model the joi",
          "Column_2": "nt motion of left and right",
          "and,": "arms"
        },
        {
          "Column_1": "effectively. In other words, t",
          "Column_2": "he left arm and hand may m",
          "and,": "ove"
        },
        {
          "Column_1": "withtherightarmandhands",
          "Column_2": "imultaneouslyfortheupperb",
          "and,": "ody."
        },
        {
          "Column_1": "So the joints in left arm an",
          "Column_2": "d hand could be connected",
          "and,": "with"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.20": "0.10\n0.00\n-0.10"
        },
        {
          "0.20": "-0.20\n-0.30"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.50": "0.00"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "been",
          "Column_2": "",
          "Column_3": "changed",
          "Column_4": "",
          "Column_5": "dur",
          "ll": "ing",
          "t": "",
          "he": "th",
          "weights": "e",
          "Column_10": "mode",
          "for": "l",
          "Column_12": "learn",
          "the": "",
          "Column_14": "ing",
          "a": "",
          "tten": "on",
          "tion": "",
          "Column_18": "tw",
          "parts": "o",
          "Column_20": "datasets.",
          "have": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the joints in right arm and hand": "too many hyperedges means th",
          ". On the other hand, choosing": "at one joint or keypoint can be"
        },
        {
          "the joints in right arm and hand": "connectedtoalmostarbitraryjo",
          ". On the other hand, choosing": "intsorkeypoints.Forexample,"
        },
        {
          "the joints in right arm and hand": "the keypoints on left arm and",
          ". On the other hand, choosing": "hand could be connected to"
        },
        {
          "the joints in right arm and hand": "the keypoints of abdomen, whi",
          ". On the other hand, choosing": "ch has a large redundancy for"
        },
        {
          "the joints in right arm and hand": "the model learning. Slightly di",
          ". On the other hand, choosing": "fferent from iMiGUE dataset,"
        },
        {
          "the joints in right arm and hand": "the configuration for the conne",
          ". On the other hand, choosing": "ction of body joints could be"
        },
        {
          "the joints in right arm and hand": "changed on the SMG dataset,",
          ". On the other hand, choosing": "as the SMG collects the full-"
        },
        {
          "the joints in right arm and hand": "bodydata.Overall,thebestcon",
          ". On the other hand, choosing": "figurationofhyperedgesseems"
        },
        {
          "the joints in right arm and hand": "to be consistent with the prior",
          ". On the other hand, choosing": ""
        },
        {
          "the joints in right arm and hand": "",
          ". On the other hand, choosing": "H2OFormer.Here,wefurther"
        },
        {
          "the joints in right arm and hand": "evaluate the impact of using v",
          ". On the other hand, choosing": "arious hidden sizes, different"
        },
        {
          "the joints in right arm and hand": "number of attention heads for",
          ". On the other hand, choosing": "the model and the usage of"
        },
        {
          "the joints in right arm and hand": "dropout learning strategy for",
          ". On the other hand, choosing": "model learning. As shown in"
        },
        {
          "the joints in right arm and hand": "Fig. 5 (b), various hidden sizes",
          ". On the other hand, choosing": "can affect the recognition per-"
        },
        {
          "the joints in right arm and hand": "formance. Observed from the",
          ". On the other hand, choosing": "results on two datasets, when"
        },
        {
          "the joints in right arm and hand": "the hidden size is relatively s",
          ". On the other hand, choosing": "mall, the performance on the"
        },
        {
          "the joints in right arm and hand": "iMiGUE dataset is better. As",
          ". On the other hand, choosing": "the hidden size increases, the"
        },
        {
          "the joints in right arm and hand": "accuracycontinuouslydecrease",
          ". On the other hand, choosing": "s.Onthecontrary,ontheSMG"
        },
        {
          "the joints in right arm and hand": "dataset,asthehiddensizeincrea",
          ". On the other hand, choosing": "ses,theaccuracycontinuously"
        },
        {
          "the joints in right arm and hand": "increases. For the Fig. 5 (c),",
          ". On the other hand, choosing": "different number of attention"
        },
        {
          "the joints in right arm and hand": "heads can affect the recognitio",
          ". On the other hand, choosing": "n performance. Observed from"
        },
        {
          "the joints in right arm and hand": "theresultsontwodatasets,9he",
          ". On the other hand, choosing": "adsusedinourimplementation"
        },
        {
          "the joints in right arm and hand": "part are the best choice for lea",
          ". On the other hand, choosing": "rning a good model. From the"
        },
        {
          "the joints in right arm and hand": "Fig. 5 (d), in which the dropou",
          ". On the other hand, choosing": "t rate ranges from 0.0 to 0.5,"
        },
        {
          "the joints in right arm and hand": "it can be concluded that the d",
          ". On the other hand, choosing": "ropout strategy is not suitable"
        },
        {
          "the joints in right arm and hand": "for the MiGs based emotion re",
          ". On the other hand, choosing": "cognition. The reason may be"
        },
        {
          "the joints in right arm and hand": "that the body joints are connec",
          ". On the other hand, choosing": "ted to each other and densely"
        },
        {
          "the joints in right arm and hand": "modeled by the graph convolut",
          ". On the other hand, choosing": "ion, which could not be set to"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "(iden",
          "Column_2": "ti",
          "Column_3": "fied",
          "Column_4": "",
          "Column_5": "in",
          "Column_6": "",
          "Column_7": "colu",
          "Column_8": "mn",
          "Column_9": "2",
          "CNN,": "of",
          "Column_11": "",
          "Column_12": "T",
          "Column_13": "abl",
          "RNN": "e",
          "Column_15": "II)",
          "and": "",
          "GCN": "",
          "ar": "",
          "ch": "",
          "itec": "",
          "tures": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[64], [65], [68], it becomes d": "features from the whole body",
          "ifficult to learn representative": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "to the deep models but also",
          "the MiGs is not only difficult": "challenging for the human be",
          "Column_3": "-"
        },
        {
          "Column_1": "ings.Investigatedby[2]onSM",
          "the MiGs is not only difficult": "Gdataset,theordinarycollege",
          "Column_3": ""
        },
        {
          "Column_1": "students and university staff",
          "the MiGs is not only difficult": "without any related knowledge",
          "Column_3": ""
        },
        {
          "Column_1": "were recruited and evaluated",
          "the MiGs is not only difficult": "as common people, while an",
          "Column_3": "-"
        },
        {
          "Column_1": "otherthreeuniversitystaffwith",
          "the MiGs is not only difficult": "psychologicalknowledgewere",
          "Column_3": ""
        },
        {
          "Column_1": "trained to recognize MiGs as",
          "the MiGs is not only difficult": "human experts. In the existing",
          "Column_3": ""
        },
        {
          "Column_1": "works, the learning based me",
          "the MiGs is not only difficult": "thods could purse the similar",
          "Column_3": ""
        },
        {
          "Column_1": "emotion recognition ability of",
          "the MiGs is not only difficult": "common people, while the ex",
          "Column_3": "-"
        },
        {
          "Column_1": "perts achieve higher performan",
          "the MiGs is not only difficult": "ce with a clear gap than learn",
          "Column_3": "-"
        },
        {
          "Column_1": "ing based methods. In this w",
          "the MiGs is not only difficult": "ork, with using the hyperedge",
          "Column_3": ""
        },
        {
          "Column_1": "representation,theperformance",
          "the MiGs is not only difficult": "ofdeeplearningbasedmethod",
          "Column_3": ""
        },
        {
          "Column_1": "could reach the level of huma",
          "the MiGs is not only difficult": "n experts. From the results of",
          "Column_3": ""
        },
        {
          "Column_1": "Table III, it can be concluded",
          "the MiGs is not only difficult": "that it is the first time for the",
          "Column_3": ""
        },
        {
          "Column_1": "learningbasedmethodtoachie",
          "the MiGs is not only difficult": "vetheperformancecomparable",
          "Column_3": ""
        },
        {
          "Column_1": "to human experts. However, th",
          "the MiGs is not only difficult": "e human experts trend to mul",
          "Column_3": "-"
        },
        {
          "Column_1": "tiple cues such as facial expres",
          "the MiGs is not only difficult": "sions and even overall impres",
          "Column_3": "-"
        },
        {
          "Column_1": "sionsfromappearancebasedda",
          "the MiGs is not only difficult": "ta(RGBdata)todeterminethe",
          "Column_3": ""
        },
        {
          "Column_1": "emotional stress states [2]. So",
          "the MiGs is not only difficult": "the cognition level of learning",
          "Column_3": ""
        },
        {
          "Column_1": "based methods has been prom",
          "the MiGs is not only difficult": "oted from the common people",
          "Column_3": ""
        },
        {
          "Column_1": "to the professional experts by",
          "the MiGs is not only difficult": "",
          "Column_3": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "that our proposed method by l",
          "ationresults,itcanbeobserved": "everaging the hyperedge repre",
          "Column_24": "-"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "sentationcanachievethebest",
          "ationresults,itcanbeobserved": "performanceof0.70+accuracy",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "beyond all existing learning b",
          "ationresults,itcanbeobserved": "ased methods in different sce",
          "Column_24": "-"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "narios.Theexistingmethodsb",
          "ationresults,itcanbeobserved": "asedonGCNsforskeletondata",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "haveexploredthetemporalmot",
          "ationresults,itcanbeobserved": "ioninformation,multi-scalein",
          "Column_24": "-"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "formation and the architecture",
          "ationresults,itcanbeobserved": "search strategy to continuously",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "improve the recognition perfor",
          "ationresults,itcanbeobserved": "mance, while our work tries",
          "Column_24": "a"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "differentwayforfurtherconsid",
          "ationresults,itcanbeobserved": "eringtherelationshipsofbody",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "joints.Itshowsthatthecombin",
          "ationresults,itcanbeobserved": "ationofthedynamichyperedge",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "based self-attention and the on",
          "ationresults,itcanbeobserved": "e-stage learning in an encoder",
          "Column_24": "-"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "Diff",
          "Column_11": "er",
          "Column_12": "ent",
          "Column_13": "from",
          "Column_14": "the",
          "Column_15": "",
          "Column_16": "deep",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "mod",
          "Column_20": "",
          "Column_21": "els",
          "Column_22": "decoder-likearchitecturecould",
          "ationresults,itcanbeobserved": "improvetherecognitionability",
          "Column_24": ""
        },
        {
          "Column_1": "[64], [65], [68]",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "in",
          "Column_7": "an",
          "Column_8": "end-",
          "Column_9": "to",
          "Column_10": "-end",
          "Column_11": "way,",
          "Column_12": "",
          "Column_13": "these",
          "Column_14": "mod",
          "Column_15": "",
          "Column_16": "els",
          "Column_17": "[67], [67],",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "for revealing the hidden emot",
          "ationresults,itcanbeobserved": "ions. Especially, the usage of",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "rec",
          "Column_5": "ogn",
          "Column_6": "ized",
          "Column_7": "",
          "Column_8": "MiG",
          "Column_9": "s",
          "Column_10": "firstl",
          "Column_11": "y",
          "Column_12": "and",
          "Column_13": "then",
          "Column_14": "the",
          "Column_15": "",
          "Column_16": "emo",
          "Column_17": "tion",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "states",
          "Column_21": "",
          "Column_22": "effective hyperedge representat",
          "ationresults,itcanbeobserved": "ion provides a new perspective",
          "Column_24": ""
        },
        {
          "Column_1": "based",
          "Column_2": "",
          "Column_3": "on",
          "Column_4": "MiGs,",
          "Column_5": "",
          "Column_6": "whic",
          "Column_7": "",
          "Column_8": "hex",
          "Column_9": "ploitsa",
          "Column_10": "",
          "Column_11": "two-",
          "Column_12": "",
          "Column_13": "stagel",
          "Column_14": "earn",
          "Column_15": "",
          "Column_16": "ing",
          "Column_17": "strat",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "egy",
          "Column_21": "to",
          "Column_22": "toexplorethemotioninformat",
          "ationresults,itcanbeobserved": "ionbyconsideringtherelation",
          "Column_24": "-"
        },
        {
          "Column_1": "ob",
          "Column_2": "tain",
          "Column_3": "the",
          "Column_4": "model.",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "shipsofvariousbodyjoints.Be",
          "ationresults,itcanbeobserved": "sides,thisworkimpliesthatthe",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "deeplearningbasedtechnique",
          "ationresults,itcanbeobserved": "byincorporatingmoremodules",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "may improve the recognition a",
          "ationresults,itcanbeobserved": "bility of hidden emotions. With",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "continued progress of recogniti",
          "ationresults,itcanbeobserved": "on accuracy from 0.4+ to 0.7+",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "in recent years, our vision bas",
          "ationresults,itcanbeobserved": "ed method has caught up with",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "the level of humans. This will",
          "ationresults,itcanbeobserved": "greatly enhance confidence in",
          "Column_24": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "the use of the technology with",
          "ationresults,itcanbeobserved": "",
          "Column_24": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "mancefo",
          "ugh our proposed model ac": "rtheMiGbasedemotionre",
          "hieves promising perfor": "cognition,somedisadvan",
          "-": "-"
        },
        {
          "Column_1": "tages sti",
          "ugh our proposed model ac": "ll exist by observing the ab",
          "hieves promising perfor": "lation study experiments.",
          "-": ""
        },
        {
          "Column_1": "Firstly, t",
          "ugh our proposed model ac": "he architecture of the deep",
          "hieves promising perfor": "model is not very robust.",
          "-": ""
        },
        {
          "Column_1": "With var",
          "ugh our proposed model ac": "ious blocks of encoder and",
          "hieves promising perfor": "decoder in H2OFormer,",
          "-": ""
        },
        {
          "Column_1": "theperfo",
          "ugh our proposed model ac": "rmancestillchangeswitha",
          "hieves promising perfor": "certaindegreeofrandom",
          "-": "-"
        },
        {
          "Column_1": "ness. It",
          "ugh our proposed model ac": "has not show the obvious i",
          "hieves promising perfor": "ncrease or decrease rules",
          "-": ""
        },
        {
          "Column_1": "with add",
          "ugh our proposed model ac": "ing or reducing more bloc",
          "hieves promising perfor": "ks. It will become tricky",
          "-": ""
        },
        {
          "Column_1": "to choos",
          "ugh our proposed model ac": "e a suitable number of bl",
          "hieves promising perfor": "ocks for new application",
          "-": ""
        },
        {
          "Column_1": "scenario",
          "ugh our proposed model ac": "s.Secondly,ourproposedm",
          "hieves promising perfor": "ethodpromotestherecog",
          "-": "-"
        },
        {
          "Column_1": "nitionab",
          "ugh our proposed model ac": "ilitybyintroducingmoreco",
          "hieves promising perfor": "mputationwithdesigning",
          "-": ""
        },
        {
          "Column_1": "adynam",
          "ugh our proposed model ac": "ichyperedgerepresentationi",
          "hieves promising perfor": "neachblockcomparedto",
          "-": ""
        },
        {
          "Column_1": "the exist",
          "ugh our proposed model ac": "ing hypergraph based mode",
          "hieves promising perfor": "ls. It obviously increases",
          "-": ""
        },
        {
          "Column_1": "requirem",
          "ugh our proposed model ac": "ents for hardware resources",
          "hieves promising perfor": "",
          "-": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "module",
          "In the future work, we will": "for measuring the relatio",
          "explore a more effective": "nship of different body"
        },
        {
          "Column_1": "joints an",
          "In the future work, we will": "d design a robust architec",
          "explore a more effective": "ture based on the action"
        },
        {
          "Column_1": "generatio",
          "In the future work, we will": "",
          "explore a more effective": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "2",
      "title": "SMG: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "H Chen",
        "H Shi",
        "X Liu",
        "X Li",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "Body movements for affective expression: A survey of automatic recognition and generation",
      "authors": [
        "M Karg",
        "A.-A Samadani",
        "R Gorbet",
        "K Kühnlenz",
        "J Hoey",
        "D Kulić"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion expression in human body posture and movement: A survey on intelligible motion factors, quantification and validation",
      "authors": [
        "M.-A Mahfoudi",
        "A Meyer",
        "T Gaudin",
        "A Buendia",
        "S Bouakaz"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Recognising human emotions from body movement and gesture dynamics",
      "authors": [
        "G Castellano",
        "S Villalba",
        "A Camurri"
      ],
      "year": "2007",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "7",
      "title": "Automatic recognition of non-acted affective postures",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze",
        "A Steed"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B"
    },
    {
      "citation_id": "8",
      "title": "Affective movement recognition based on generative and discriminative stochastic dynamic models",
      "authors": [
        "A Samadani",
        "R Gorbet",
        "D Kulić"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "9",
      "title": "Continuous recognition of playerś affective body expression as dynamic quality of aesthetic experience",
      "authors": [
        "N Savva",
        "A Scarinzi",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Álvarez",
        "A Recasens",
        "À Lapedriza"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "11",
      "title": "Generalized zero-shot emotion recognition from body gestures",
      "authors": [
        "J Wu",
        "Y Zhang",
        "S Sun",
        "Q Li",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Analyze spontaneous gestures for emotional stress state recognition: A micro-gesture dataset and analysis with deep learning",
      "authors": [
        "H Chen",
        "X Liu",
        "X Li",
        "H Shi",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "13",
      "title": "Joint skeletal and semantic embedding loss for micro-gesture classification",
      "authors": [
        "K Li",
        "D Guo",
        "G Chen",
        "X Peng",
        "M Wang"
      ],
      "venue": "IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)"
    },
    {
      "citation_id": "14",
      "title": "Representation learning for topologyadaptive micro-gesture recognition and analysis",
      "authors": [
        "A Shah",
        "H Chen",
        "G Zhao"
      ],
      "venue": "IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)"
    },
    {
      "citation_id": "15",
      "title": "Micro-gesture online recognition with graph-convolution and multiscale transformers for long sequence",
      "authors": [
        "X Guo",
        "W Peng",
        "H Huang",
        "Z Xia"
      ],
      "venue": "IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)"
    },
    {
      "citation_id": "16",
      "title": "MSTCN-VAE: An unsupervised learning method for micro-gesture recognition based on skeleton modality",
      "authors": [
        "W Yuan",
        "S He",
        "J Dou"
      ],
      "venue": "IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)"
    },
    {
      "citation_id": "17",
      "title": "Micro-gesture classification based on ensemble hypergraph-convolution transformer",
      "authors": [
        "H Huang",
        "X Guo",
        "W Peng",
        "Z Xia"
      ],
      "venue": "IJCAI Workshop on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)"
    },
    {
      "citation_id": "18",
      "title": "Dynamic hypergraph convolutional networks for skeleton-based action recognition",
      "authors": [
        "J Wei",
        "Y Wang",
        "M Guo",
        "P Lv",
        "X Yang",
        "M Xu"
      ],
      "year": "2021",
      "venue": "arXiv"
    },
    {
      "citation_id": "19",
      "title": "Hypergraph transformer for skeleton-based action recognition",
      "authors": [
        "Y Zhou",
        "C Li",
        "Z.-Q Cheng",
        "Y Geng",
        "X Xie",
        "M Keuper"
      ],
      "year": "2022",
      "venue": "arXiv"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "W Wang",
        "Y Guo",
        "V Leung"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "21",
      "title": "Affect recognition from face and body: early fusion vs. late fusion",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Systems, Man and Cybernetics (SMC)"
    },
    {
      "citation_id": "22",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "G Venture",
        "H Kadone",
        "T Zhang",
        "J Grèzes",
        "A Berthoz",
        "H Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "23",
      "title": "Multi-level classification of emotional body expression",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2015",
      "venue": "IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "24",
      "title": "Continuous body emotion recognition system during theater performances",
      "authors": [
        "S Senecal",
        "L Cuel",
        "A Aristidou",
        "N Magnenat-Thalmann"
      ],
      "year": "2016",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition using kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "26",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "27",
      "title": "Analysis of emotional effect on speechbody gesture interplay",
      "authors": [
        "Z Yang",
        "S Narayanan"
      ],
      "year": "2014",
      "venue": "Analysis of emotional effect on speechbody gesture interplay"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
      "authors": [
        "L Kessous",
        "G Castellano",
        "G Caridakis"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "29",
      "title": "Time-dependent body gesture representation for video emotion recognition",
      "authors": [
        "J Wei",
        "X Yang",
        "Y Dong"
      ],
      "year": "2021",
      "venue": "International Conference on Multimedia Modeling (MMM)"
    },
    {
      "citation_id": "30",
      "title": "SwarMan: Anthropomorphic swarm of drones avatar with body tracking and deep learning-based gesture recognition",
      "authors": [
        "A Baza",
        "A Gupta",
        "E Dorzhieva",
        "A Fedoseev",
        "D Tsetserukou"
      ],
      "year": "2022",
      "venue": "IEEE Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "31",
      "title": "Coupled multimodal emotional feature analysis based on broad-deep fusion networks in human-robot interaction",
      "authors": [
        "L Chen",
        "M Li",
        "M Wu",
        "W Pedrycz",
        "K Hirota"
      ],
      "year": "2023",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "32",
      "title": "A survey on self-supervised learning: Algorithms, applications, and future trends",
      "authors": [
        "J Gui",
        "T Chen",
        "J Zhang",
        "Q Cao",
        "Z Sun",
        "H Luo",
        "D Tao"
      ],
      "year": "2023",
      "venue": "A survey on self-supervised learning: Algorithms, applications, and future trends"
    },
    {
      "citation_id": "33",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "MS2L: Multi-task self-supervised learning for skeleton based action recognition",
      "authors": [
        "L Lin",
        "S Song",
        "W Yang",
        "J Liu"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Contrastive self-supervised learning for skeleton action recognition",
      "authors": [
        "X Gao",
        "Y Yang",
        "S Du"
      ],
      "year": "2020",
      "venue": "NeurIPS Workshop on Pre-registration in Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "Focalized contrastive view-invariant learning for self-supervised skeleton-based action recognition",
      "authors": [
        "Q Men",
        "E Ho",
        "H Shum",
        "H Leung"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "37",
      "title": "Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition",
      "authors": [
        "T Guo",
        "H Liu",
        "Z Chen",
        "M Liu",
        "T Wang",
        "R Ding"
      ],
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "38",
      "title": "Self-supervised video interaction classification using image representation of skeleton data",
      "authors": [
        "F Askari",
        "R Jiang",
        "Z Li",
        "J Niu",
        "Y Shi",
        "J Clark"
      ],
      "year": "2023",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "39",
      "title": "Cross-stream contrastive learning for self-supervised skeleton-based action recognition",
      "authors": [
        "D Li",
        "Y Tang",
        "Z Zhang",
        "W Zhang"
      ],
      "year": "2023",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "40",
      "title": "Self-supervised contrastive learning of multiview facial expressions",
      "authors": [
        "S Roy",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "41",
      "title": "Revisiting self-supervised contrastive learning for facial expression recognition",
      "authors": [
        "Y Shu",
        "X Gu",
        "G Yang",
        "B Lo"
      ],
      "year": "2022",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "42",
      "title": "Emotion-aware multi-view contrastive learning for facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "43",
      "title": "Posedisentangled contrastive learning for self-supervised facial representation",
      "authors": [
        "Y Liu",
        "W Wang",
        "Y Zhan",
        "Z Chen",
        "S Feng",
        "L.-Y Liu"
      ],
      "year": "2023",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "SS-VAERR: Self-supervised apparent emotional reaction recognition from video",
      "authors": [
        "M Jegorova",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "45",
      "title": "A unified approach to facial affect analysis: the mae-face visual representation",
      "authors": [
        "B Ma",
        "W Zhang",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "46",
      "title": "Multi-modal facial affective analysis based on masked autoencoder",
      "authors": [
        "W Zhang",
        "B Ma",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "47",
      "title": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks"
    },
    {
      "citation_id": "48",
      "title": "Exploiting fine-tuning of self-supervised learning models for improving bi-modal sentiment analysis and emotion recognition",
      "authors": [
        "W Yang",
        "S Fukayama",
        "P Heracleous",
        "J Ogata"
      ],
      "year": "2022",
      "venue": "Exploiting fine-tuning of self-supervised learning models for improving bi-modal sentiment analysis and emotion recognition"
    },
    {
      "citation_id": "49",
      "title": "Movement representation learning for pain level classification",
      "authors": [
        "T Olugbade",
        "A De C Williams",
        "N Gold",
        "N Bianchi-Berthouze"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "See your emotion from gait using unlabeled skeleton data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "51",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Transformer-based self-supervised learning for emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "53",
      "title": "GANSER: A self-supervised data augmentation framework for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "GMSS: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Hgnn+: General hypergraph neural networks",
      "authors": [
        "Y Gao",
        "Y Feng",
        "S Ji",
        "R Ji"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Z Cao",
        "T Simon",
        "S.-E Wei",
        "Y Sheikh"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "57",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems (NeurPIS)"
    },
    {
      "citation_id": "58",
      "title": "Micro-expression spotting with multi-scale local transformer in long videos",
      "authors": [
        "X Guo",
        "X Zhang",
        "L Li",
        "Z Xia"
      ],
      "year": "2023",
      "venue": "Pattern Recognition Letter"
    },
    {
      "citation_id": "59",
      "title": "Revealing the invisible with model and data shrinking for composite-database micro-expression recognition",
      "authors": [
        "Z Xia",
        "W Peng",
        "H.-Q Khor",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "60",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "61",
      "title": "Temporal relational reasoning in videos",
      "authors": [
        "B Zhou",
        "A Andonian",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "62",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "J Lin",
        "C Gan",
        "S Han"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "63",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "64",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "65",
      "title": "Learning graph convolutional network for skeleton-based human action recognition by neural searching",
      "authors": [
        "W Peng",
        "X Hong",
        "H Chen",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "66",
      "title": "L2-gcn: Layer-wise and learned efficient training of graph convolutional networks",
      "authors": [
        "Y You",
        "T Chen",
        "Z Wang",
        "Y Shen"
      ],
      "year": "2020",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "67",
      "title": "Bayesian graph convolutional neural networks for semi-supervised classification",
      "authors": [
        "Y Zhang",
        "S Pal",
        "M Coates",
        "D Üstebay"
      ],
      "year": "2019",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "68",
      "title": "Disentangling and unifying graph convolutions for skeleton-based action recognition",
      "authors": [
        "Z Liu",
        "H Zhang",
        "Z Chen",
        "Z Wang",
        "W Ouyang"
      ],
      "year": "2020",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}