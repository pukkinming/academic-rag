{
  "paper_id": "2007.04364v1",
  "title": "Temporal Aggregation Of Audio-Visual Modalities For Emotion Recognition",
  "published": "2020-07-08T18:44:15Z",
  "authors": [
    "Andreea Birhala",
    "Catalin Nicolae Ristea",
    "Anamaria Radoi",
    "Liviu Cristian Dutu"
  ],
  "keywords": [
    "asynchronous data",
    "convolutional neural network",
    "data fusion",
    "emotion recognition",
    "multimodal information",
    "spectrogram"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition has a pivotal role in affective computing and in human-computer interaction. The current technological developments lead to increased possibilities of collecting data about the emotional state of a person. In general, human perception regarding the emotion transmitted by a subject is based on vocal and visual information collected in the first seconds of interaction with the subject. As a consequence, the integration of verbal (i.e., speech) and non-verbal (i.e., image) information seems to be the preferred choice in most of the current approaches towards emotion recognition. In this paper, we propose a multimodal fusion technique for emotion recognition based on combining audio-visual modalities from a temporal window with different temporal offsets for each modality. We show that our proposed method outperforms other methods from the literature and human accuracy rating. The experiments are conducted over the open-access multimodal dataset CREMA-D.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automatic detection of human emotions has become an important area of research due to the technological development that occurred in the human-computer interaction domain (e.g., social robots  [1] ,  [2] , monitoring systems for car drivers' condition  [3] ). In order to increase the accuracy of emotion recognition systems, most of the currently developed methods incorporate multimodal information (e.g., facial and speech features)  [4] ,  [5] ,  [6] ,  [7] . Facial expressions represent one of the most important modes of communication through which people express their emotions and intentions. In addition to facial expressions, people also express their feelings through speech; e.g, speech inflection, vocal intensity are characteristics that contain information about the emotional state of a subject.\n\nEach person is unique and can express emotions in their own characteristic way, depending on their culture, age, gender or previous life experiences  [8] . Nevertheless, there are common characteristics that can be exploited in order to obtain an accurate classification system. In general, most recognition systems consider only 6 types of emotions (e.g., anger, happiness, surprise, disgust, contempt, anxiety)  [9] . According to the Facial Action Coding System (FACS), each human emotion This work has been partially supported by the Ministry of Innovation and Research, UEFISCDI, project SPIA-VA, agreement 2SOL/2017, grant PN-III-P2-2.1-SOL-2016-02-0002. can be described through a combination of several Facial Action Units (FAUs)  [10] . More precisely, the FACS refer to a combined set of facial muscle movements that correspond to a displayed emotion. The basic element in this coding system is Action Unit (AU) and each AU is related to the contraction of one or more facial muscles.\n\nEmotion recognition systems using only visual information (i.e., video frames) can be mainly classified into static and dynamic methods depending on the feature representations. In static-based methods, the features are encoded with spatial information from singular frames without taking into consideration the temporal extent, whilst dynamic-based methods consider the temporal relation between continuous frames from the input sequence. In the case of static-based methods, state-of-the-art deep neural networks architectures (e.g., VGG  [11] , ResNet  [12] ) have been proposed for feature extraction, whilst the classification into emotion categories is performed using a Support Vector Machine (SVM) module  [13] .\n\nDue to the increased interest in developing real-world scenarios datasets and also the increased computer processing capabilities, recent approaches are based on deep learning techniques that are able to extract both facial and audio discriminant information. In a recent paper  [4] , we have shown that not very deep convolutional neural network (CNN) architectures are able to extract meaningful information regarding emotion categories from both video frames and spectrograms of audio signals. By combining the audio-video information, we managed to achieve an increase of almost 7 % compared to the case when only video data is considered.\n\nConsidering the behavioral differences between people and the diverse modes of communicating their feelings, methods addressing person-specific affective understanding have been also developed. In  [14] , Grow-When-Required Networks and personalized affective memories are used to learn individualized aspects of emotions. However, the complexity of the proposed model limits the real-time usage of the proposed solution.\n\nIn order to include temporal dynamic characteristics between video frames, Beard et. al proposed a recurrent multiattention (RMA) mechanism with shared external memory that is updated over multiple iterations of analysis  [6] . This approach allows relevant memories to persists over multiple hops. The method achieved a maximum accuracy of 65 % on the CREMA-D dataset, comparable to the human rating accuracy reported for this dataset (i.e., 63.6 %  [15] ).\n\nIn an attempt to exploit the complementary information brought by diverse modalities (i.e., audio and video), a Multimodal Emotion Recognition Metric Learning (MERML) was defined in  [16] . The learned metric was further used by SVM with Radial Basis Function (RBF) kernel.\n\nIn this paper, we propose a novel multimodal architecture that combines visual and audio features extracted from random selections of analysis windows within individual temporal segments of the input video. Thus, the temporal aggregation of audio and video allows for asynchronous inputs of the two considered modalities. We tested our solution on the CREMA-D  [15] , a widely used audio-visual dataset in the multimodal emotion recognition field.\n\nThe rest of the paper is organized as follows. Section II introduces the multimodal model architecture. Section III describes the dataset used for experiments, whereas section IV presents the experimental results. Finally, section V concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Approach",
      "text": "Inspired by the solution for action recognition presented in  [17]  and by how the human brain processes audio-visual information, we propose a temporal aggregation mechanism in order to combine modalities within a range of temporal offsets. In this mechanism we explore the fusion between audio and visual inputs within a temporal window, that will allow the model to be trained with asynchronous inputs from both modalities. Our proposed temporal aggregation mechanism is shown in Fig 1 . In the following, we denote by r v the sampling rate of the video sequence and by r a (different from r v ) the sampling rate of the audio signal.\n\nThe input video sequence is divided into N temporal segments of equal length. For each temporal segment, we randomly select a video frame j and we randomly choose the center c of the audio signal window between [j/r v -b, j/r v +b] seconds. The audio signal used in the analysis of the current temporal segment is considered between [c-d/2, c+d/2] seconds (i.e., samples between r a (c -d/2) and r a (c + d/2) ). Further, the video frame and the spectrogram of the audio signal are fed into an audio-visual network corresponding to the current temporal segment. It is worth noting that N independent audio-visual networks are needed to build the entire emotion recognition architecture.\n\nFollowing a similar approach to the method that has been recently proposed by the authors in  [4] , the core audio-visual network is composed of a sequence of convolutional blocks, which extracts the audio and visual features. After the concatenation of the audio and video feature vectors, the resulting feature vector is considered as input for a Fully Connected (FC) layer, followed by a SoftMax activation function which yields the class probabilities for the considered emotion categories. The solution proposed in  [4]  achieved approximately the human rating performance accuracy at low computational costs. However, in order to accelerate the training process and to increase the stability of the core audio-visual network, we inserted a Batch Normalization layer after each convolutional layer  [18] . The core audio-visual network architecture, which processes asynchronous multimodal information, is shown in Fig.  2 .\n\nAfter aggregating the emotion class probabilities for all the temporal segments composing the video, the class label for the entire video is the one for which the maximum score is achieved. As shown in Fig.  1 , the final score is obtaining by summing up the emotion class probabilities over all the temporal segments.\n\nWe mention that the frames of the original videos are preprocessed using the MTCNN algorithm  [19] , whose aim is to perform face detection and to remove the unnecessary information (i.e., background) with respect to the emotion recognition task.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Database",
      "text": "Over the past years, several databases for the emotion recognition task have been proposed and the research in the affective computing domain focused on mixing different sources of information to achieve better performance. The CREMA-D multimodal database was published in 2015  [15]  and contains 7442 clips of 91 actors (48 male and 43 female) with different ethnic backgrounds. The actors were asked to convey particular emotions while producing, with different intonations, 12 particular sentences that evoke the target emotions. Six labels have been used to discriminate among different emotion categories (i.e., neutral, happy, anger, disgust, fear, sad) with four different intensity levels (i.e.,  low, medium, high, unspecified). The labels corresponding to each recording were collected using crowd-sourcing. More precisely, 2443 participants were asked to label the perceived emotion and its intensity. The human accuracy achieved for this task was, on average, 63.6%. It is worth mentioning that human training was achieved through participants' previous experience.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, the results achieved on the CREMA-D data set, as well as the experimental setup used in our approach, are provided. For all our experiments, we use an user-independent  10-fold cross validation technique to split the dataset into train and validation subsets. We divide the dataset into 10 different folds (i.e., none of the actors is introduced in more than one fold for generalization reasons) and compute the mean accuracy over all the folds.\n\nVarious evaluation metrics are used to assess the per-   [15]  63.6 CNN-based approach  [4]  55.8 RMA  [6]  65.0 MERML  [16]  66.5 Proposed method 68.4\n\nformance of the proposed solution for emotion recognition, namely mean overall accuracy, loss values and confusion matrix. These performance measures are shown in Fig.  3  and Fig.  4  and are achieved for a temporal aggregation of 10 segments extracted from the video. As shown in Fig.  5 , the overall accuracy increases with the number of segments considered. However, the time required for training a model and the inference time increase almost linearly with the number of segments, i.e., from 3 hours for a model with 3 segments to approximately 5.5 hours for a model with 10 segments. Moreover, the accuracy does not increase substantially for more than 8 segments. Furthermore, the overall accuracy is compared with the performances achieved by other methods from the literature (e.g., CNN-based approach  [4] , RMA  [6] ). It is worth mentioning that the experiments using the approach proposed in  [4]  followed the same 10-fold cross validation technique.\n\nThe input frames were cut at 224 × 224 pixels around the detected faces  [19] , whereas the spectrograms were resized to 192 × 120. The length of the audio signal d was set to 1.28 seconds, whereas the offset b was set to 0.01 seconds. In order to train the models, we used the cross entropy loss and the stochastic gradient descent optimization method with 0.9 momentum. For each epoch, the learning rate was initially set to 1e-3 and decayed by a factor of 10 every 50 training steps. The batch size was set to 16.\n\nOur solution outperformed the baseline approach proposed in  [4]  by 12.6%, human accuracy rating by 4.8% and also methods based on recurrent multi-attention  [6] . The proposed method of early combining features and temporal aggregation of partial results leads to a better performance, which suggests that combining information coming from different sources in an asynchronous manner proves to be beneficial in the process of emotion understanding.\n\nWe mention that all the experiments were conducted over an Intel Xeon E5-1680v3, 8 cores @3.2 GHz, equipped with NVIDIA Quadro M4000 GPU with 8 GB RAM.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we proposed a new method to incorporate multimodal information to discriminate among categories of emotions. The methodology benefits from combining the audio and visual information in an asynchronous manner which allows a certain degree of flexibility between the analysis of the two modalities. Using a simple audio-visual neural network as core architecture for predicting the emotional states, the temporal aggregation of the multimodal information from various segments leads to a substantial increase in the accuracy of the recognition system and outperforms other approaches from the literature.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In the following, we denote by rv the sampling",
      "page": 2
    },
    {
      "caption": "Figure 1: Our proposed temporal aggregation mechanism. FC label is a 6-",
      "page": 2
    },
    {
      "caption": "Figure 2: After aggregating the emotion class probabilities for all the",
      "page": 2
    },
    {
      "caption": "Figure 1: , the ﬁnal score is obtaining",
      "page": 2
    },
    {
      "caption": "Figure 2: Our proposed audio-visual network used to retrieve emotion class probabilities for each temporal segment of the analysed video.",
      "page": 3
    },
    {
      "caption": "Figure 3: Performance over the training and validation sets.",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion matrix for best performance.",
      "page": 3
    },
    {
      "caption": "Figure 5: Variation of accuracy values with respect to the number of segments",
      "page": 3
    },
    {
      "caption": "Figure 4: and are achieved for a temporal aggregation of 10",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Humanaccuracy[15]\nCNN-basedapproach[4]\nRMA[6]\nMERML[16]\nProposedmethod",
          "Accuracy[%]": "63.6\n55.8\n65.0\n66.5\n68.4"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion modelling for social robotics applications: a review",
      "authors": [
        "F Cavallo",
        "F Semeraro",
        "L Fiorini",
        "G Magyar",
        "P Sinčák",
        "P Dario"
      ],
      "year": "2018",
      "venue": "Journal of Bionic Engineering"
    },
    {
      "citation_id": "2",
      "title": "Extending assisted audio capabilities of tiago service robot",
      "authors": [
        "L Grama",
        "C Rusu"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "3",
      "title": "Toward emotion recognition in car-racing drivers: A biosignal processing approach",
      "authors": [
        "C Katsis",
        "N Katertsidis",
        "G Ganiatsas",
        "D Fotiadis"
      ],
      "year": "2008",
      "venue": "IEEE Trans. on Systems, Man, and Cybernetics -Part A: Systems and Humans"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition system from speech and visual information based on convolutional neural networks",
      "authors": [
        "N.-C Ristea",
        "L Dut",
        "A Radoi"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "5",
      "title": "Avec 2013: The continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "M Valstar",
        "B Schuller",
        "K Smith",
        "F Eyben",
        "B Jiang",
        "S Bilakhia",
        "S Schnieder",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "ACM International Workshop on Audio/Visual Emotion Challenge, ser. AVEC"
    },
    {
      "citation_id": "6",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "year": "2018",
      "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning"
    },
    {
      "citation_id": "7",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "10",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in the wild from videos using images",
      "authors": [
        "S Bargal",
        "E Barsoum",
        "C Ferrer",
        "C Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction ICMI 2016"
    },
    {
      "citation_id": "14",
      "title": "A personalized affective memory neural model for improving emotion recognition",
      "authors": [
        "P Barros",
        "G Parisi",
        "S Wermter"
      ],
      "year": "1904",
      "venue": "ArXiv"
    },
    {
      "citation_id": "15",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Metric learning based multimodal audio-visual emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Epic-fusion: Audio-visual temporal binding for egocentric action recognition",
      "authors": [
        "E Kazakos",
        "A Nagrani",
        "A Zisserman",
        "D Damen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "arxiv": "arXiv:1502.03167"
    },
    {
      "citation_id": "19",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    }
  ]
}