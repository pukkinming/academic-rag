{
  "paper_id": "2210.14501v1",
  "title": "Effect Of Different Splitting Criteria On The Performance Of Speech Emotion Recognition",
  "published": "2022-10-26T06:16:09Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Akira Sasou"
  ],
  "keywords": [
    "Speech emotion recognition",
    "data partition",
    "text-independent",
    "speaker-independent",
    "splitting criteria"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional speech emotion recognition (SER) evaluations have been performed merely on a speaker-independent condition; some of them even did not evaluate their result on this condition. This paper highlights the importance of splitting training and test data for SER by script, known as sentenceopen or text-independent criteria. The results show that employing sentence-open criteria degraded the performance of SER. This finding implies the difficulties of recognizing emotion from speech in different linguistic information embedded in acoustic information. Surprisingly, text-independent criteria consistently performed worse than speaker+text-independent criteria. The full order of difficulties for splitting criteria on SER performances from the most difficult to the easiest is textindependent, speaker+text-independent, speaker-independent, and speaker+text-dependent. The gap between speaker+textindependent and text-independent was smaller than other criteria, strengthening the difficulties of recognizing emotion from speech in different sentences.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) is one topic of interest in automatic speech recognition and understanding. In contrast to automatic speech recognition (ASR) which attempts to obtain linguistic information from speech, SER attempts to obtain non-linguistic information from speech. In more concrete, SER aims to infer the affective state of the speaker from solely speech data.\n\nSER can be designed to recognize discrete emotion, continuous emotion, or both emotion models. Recent research suggested that emotion is ordinal by nature  [1] , which is closer to a categorical than the continuous model. In the categorical model, several emotion categories exist, from the simplest two categories with positive and negative emotions to 27 categories  [2] . The choice of emotion model in SER depends on the availability of the labels in the dataset.\n\nData-driven methods, in which most SER systems employ this kind of approach, rely on the configuration or selection of the data to build the model. In SER, it is common to split the data by evaluating different speakers for training and test partitions. This approach, known as speaker-independent criteria, is a gold standard to build SER model that minimizes speaker variability in the training phase. *Corresponding author, on leave from Department of Engineering Physics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia (email: bagus@ep.its.ac.id).\n\nRecent research in SER, particularly by fusing acoustic and linguistic information, has found that different splitting criteria in splitting data for training will result in different performances  [3] . In the fusion of acoustic and linguistic information, it is sound that the model needs to be trained in different scripts, known as sentence-open or textindependent. This strategy was intended to avoid the effect of having the same linguistic information for predicting emotion under the same sentences for both training and test partitions. Since linguistic information is extracted via text or script, this splitting condition is necessary to evaluate such discrepancies of using different features or types of information. Using merely acoustic features for SER, one may argue that this evaluation is unnecessary since no linguistic features are involved in building the SER model.\n\nFujisaki in 2002 proposed a scheme in which various types of information are manifested in the segmental and suprasegmental features of speech  [4] . One of the types of information includes emotion. Referencing this argument that emotional information is manifested directly in speech without a need to convert speech into text, there is a possibility that different sentences will yield different SER performances under the same acoustic-only system. The current research on SER showed no evaluation of the differences of splitting criteria, particularly comparing data with and without linguistic information.\n\nThe contribution of this paper is an evaluation of the effect of splitting criteria into the training data on SER performance. As argued previously, linguistic information is embedded in acoustic features; hence, evaluating textindependent criteria, i.e., different sentences for training and test, is necessary to observe such effects. We evaluated four splitting criteria: speaker-dependent (including text-dependent data), speaker-independent, text-independent, and speaker+text-independent criteria, and traced their SER performances. We experimented with these criteria in three different experiments. The results of three different experiments show a consistent pattern of difficulties for four different splitting criteria. Text-independent criteria obtained the worst result followed by speaker+text-independent, speakerindependent, and speaker-dependent criteria.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Research on the evaluation of the effect of data selection on speech processing is not new. Different data for training will produce different results on the same algorithm. A model generalization is a challenging task to minimize the variance among different data inputs for the same model.\n\nIn  [5] , the authors evaluated the effect of training data selection for speech recognition of emotional speech. Emotional speech showed lower error rates than neutral speech. Using their proposed selection criteria based on the entropy of diphones, they improved the error rate of speech recognition by transferring the model from emotional speech to non-emotional speech data.\n\nThe authors of  [3]  pointed to the different results of splitting data by random folds, speaker folds, and script folds on the speech emotion recognition by fusing acoustic and linguistic information [Fig.  2 ]. The results suggested that splitting by both speakers and scripts is more difficult than splitting by random folds only or splitting by speaker only. No information is available on comparing the difficulties of splitting by both speaker and script to splitting by script only.\n\nThe author of  [6]  evaluated Japanese Twitter-based emotional speech (JTES) dataset with speaker-open and sentenceclosed conditions. The author achieved an accuracy of 81.44 % by utilizing a common normalization. Using two other corpora as training data and JTES as test data resulted in a degraded score to 80.66%, highlighting the difficulty of crosscorpus evaluation  [7] . On the same JTES dataset but different criteria, speaker-open and sentence-open, the authors of  [8]  achieved an accuracy of 73.4 % by utilizing multi-stream attention-based bidirectional LSTM (BLSTM) with feature segmentation. This result is the closest machine performance to human performance on the JTES dataset, in which human evaluators scored 75.5% from acoustic subjective evaluation  [9] . However, again, there is no evaluation comparing the effect of splitting by speakers (text-independent) and other criteria.\n\nThis research fills the gap in the existing research on evaluating SER with different splitting criteria. We evaluated speaker-independent (including text-independent), speakerdependent, text-independent, and speaker+text-independent data. This paper denotes an insight to add information missing in the previous papers to gain new insight on the effect of splitting data based on the different criteria on SER performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "This paper makes use of the phonetically and prosodically balanced Japanese Twitter-based Emotional Speech corpus (JTES)  [10] . The main reason to choose this emotional speech dataset is due to the richness of sentences (200 sentences: 50 sentences for each emotion) and speakers (100 speakers: 50 male and 50 female), which can be evaluated with different criteria and conditions. Japanese speech emotion recognition also lacks exploration compared to English, Chinese, and European languages.\n\nThe JTES dataset consists of 20000 utterances from four emotion categories: joy, anger, sadness, and neutral. For each emotion, there are 500 raw sentences collected from Twitter data. The Twitter text data are modified to reflect Japanese culture when they are spoken in the recordings. For instance, sentences that end with nouns were removed due to emotional independence. The labeling of the emotion categories was performed by matching emotion-related words. Selection criteria based on entropy reduce the number into 50 sentences for each emotion category.\n\nAll data in the JTES dataset were involved in the experiments. However, due to the naturalness of this research's aims, each splitting criteria resulted in a different number of data for each experiment. For instance, splitting by speakers and by sentences will allocate 19600 samples for training, and the rest 400 samples for test. However, splitting by both speakers and sentences only allocates 14400 samples for training due to overlap between both speaker and text. To avoid the effect of different sizes of training data, we performed an evaluation on the same amount of data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Feature Extraction",
      "text": "The first step to obtaining input data for the SER system from the raw dataset is by extracting features of speech. For this purpose, we utilized opensmile feature extraction tool to extract a set of acoustic features that represent emotion information in speech. The version of opensmile used in this experiment is 3.0. The feature set is 'emo large' with default configuration emo_large.conf in the config/misc/ directory.\n\nFor this emo large feature set, we extracted only statistical functions per utterance. The size was a 6552-dimensional feature from 56 low-level descriptors with their deltas and deltas-deltas (total 168 features) multiplied by 39 statistics. For the feature data, first, we extracted each audio file (.wav) into corresponding .csv files. Then we extracted related rows in .csv files (rows 6559 to end) and saved them in the form of Numpy arrays (.npy files). The first 6558 rows only consist of header names of the corresponding features, which are not used in a classification process. Table  I  shows detail of the acoustic features in emo large setup. The top rows show fifty-six LLDs. Adding this 56 LLD with their deltas (∆) and deltas-deltas (∆∆) sums up 168 features. For each LLD, 39 statistics were computed, resulting in 6652 features in total. Aside from emotion recognition, this feature set is reported to be effective for classifying dogs' barking  [11] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Classifiers",
      "text": "We evaluated multi-layer perceptron (MLP), also known as fully-connected or dense network, for the classifier of the SER system. The previous research on speech emotion recognition has shown the effectiveness of these fully-connected (FC) or dense networks to overcome the SER problem  [12] . The structure of FC networks follow the previous research on the different layers and dataset  [13] . Three dense layers were stacked, followed by a four-unit dense layer as the final layer.\n\nTable  II  shows the detail of hyperparameters employed in the MLP networks. The choice of the values is based on some references  [13] ,  [14] . The same architecture was employed to evaluate four different conditions in three experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Experiments",
      "text": "Since the goal of this research is to evaluate the performance of SER under different training criteria, we split our experiment into four criteria, namely speaker-dependent (SD), speaker-independent (SI), text-independent (TI), and speaker+text-independent (STI). The splitting criteria were designed to match the number of STI the test partition, i.e., 400 samples, following the previous research  [8] ,  [14] . Fig.  2  depicts the configuration of this splitting criteria. Note that the first SD criterion also contains text-dependent data since the data are randomly shuffled. For instance, we only selected the first 19600 utterances for training and the rest 400 utterances for the test for both first and second experiments. Both number of data are obtained after shuffling the data. In the second, third, and fourth criteria, we selected the samples such that the condition of each criterion is fulfilled, i.e., the samples in the training and test partitions are different based on these criteria. Three different experiments were conducted to gain an overall conclusion among these splitting criteria.   III .\n\nIn the first experiment, we ran 30 trials and calculated an average number of accuracy (weighted accuracy, WA) along with its standard errors (SE). This method is similar to the method reported in  [8] . While they only ran ten trials, we extended it into 30 trials for more statistical confidence. This first experiment is conducted to attain an initial result and to compare the trend with the later methods (crossvalidation with same and different number of data). The number of 30 trials was chosen since the results showed unstable performance when the model was trained with less than this number of trials.\n\nThe second experiment replaced an average score from a number of trials with cross-validation. In this experiment, we divided each training data (SD, SI, TI, STI) into five-folds and shuffle the training and validation data in different five splits. The model generated by cross-validation with this 5fold is then used to predict the test data. As shown in Table  III , both the first and second experiments employed the same amount of data. For SD, SI, and TI, 19600 samples were used for training while the rest 400 samples were kept for the test. For STI, which overlaps between speaker and textindependent data, only 14400 can be used for training with the same number of test data, i.e., 400 samples.\n\nTo take into account the effect of different numbers of training data, we reduced the number of training samples in the previous cross-validation experiment. In this third experiment, all criteria have the same number of data for both training and test partitions. A larger number of training data is reported to be more effective for deep learning than smaller data  [15] . Hence, it is necessary to train the model under the same number of data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Evaluation Metric",
      "text": "We evaluated our results, i.e., classification performance, with a single metric weighted accuracy (WA). This metric is also known as overall accuracy  [16] . Since the dataset is balanced, i.e., the number of samples for each emotion class is same, various metrics (accuracy, precision, recall, F1) result in similar performance. Hence, we only used WA as a measure of total correct predicted samples divided by the number of all samples irrespective of emotion classes. This metric is also known as weighted average recall  [17, p. 268] .\n\nIn addition to WA, we calculated error bars using the standard error (SE) of the mean. This SE \"reflect the uncertainty in the mean and its dependency on the sample size, n\"  [18] . It can be formulated as,\n\nwhere SD is the standard deviation. As shown in the formula, SE depends on the number of samples, n. In our experiment, n = 30 is for experiment 1 and n = 5 is for both experiment 2 and experiment 3.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Results",
      "text": "Table  IV  shows the result of experiments under different conditions. As noted in the Methods section, one of the purposes of conducting experiments in different conditions (average of trials, cross-validation, and same amount data) is to check the consistency and pattern of the evaluated criteria. Hence, we summarized all results of these experiments in a single table to show these behaviors.\n\nTable  IV  shows results of three SER experiment conditions in terms of accuracy (weighted accuracy, WA). There is consistency among different splitting criteria across different experiments. The SD condition always gains the highest performance, as expected. Contrary to expectation, the textindependent condition always obtains the lowest performance among others. We first assumed that the speaker+textindependent condition would have the worst result since this condition is the most difficult among others. These results suggest that the order of difficulties across different splitting criteria is text-independent, speaker+text-independent, speaker-independent, and speaker-dependent.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Discussion",
      "text": "We came to this research idea from the previous findings on SER research where the recognition performance has increased when acoustic information is combined with linguistic information  [3] ,  [12] . If linguistic information is involved, it makes sense that the performance will improve.\n\nIn other words, the SER performance will decrease without linguistic information. While the previous linguistic information is extracted from text, Fujisaki  [4]  argued that linguistic information itself is also manifested in acoustic features, meaning that there is also linguistic information in acoustic features. This argument leads to the following presumption.\n\nIf we train SER with different splitting criteria with regard to linguistic information, the performance also changes. Implementing splitting criteria with different linguistic information can be achieved by separating training and test sets into different sentences. The spoken sentences (or utterances) used for the test partition can be set to be different from the sentences used for the training. One may argue that this separation only works if linguistic information from text is involved. However, based on a solid argument from the previous literature  [4] , we believe that there will be differences in SER performance by splitting training and test partitions into different sentences.\n\nTo test our hypothesis, we split the samples in the dataset into four criteria: speaker-dependent, speaker-independent, text-independent, speaker+text-independent. We experimented with these different splitting criteria on three different conditions: average trials, cross-validation, and the same amount of data. The results show consistency among different splitting criteria and experiment conditions.\n\nAmong three experiments, the cross-validation condition obtained the highest scores among the other two conditions. Shuffling different training and validation data seems to be useful for generalizing the model. Using five-fold shows slightly higher scores than an average of 30 trials. The results may also be used to justify that reporting performance from average trials may be sufficient to gain an insight into the order of difficulties among different splitting criteria. One exception in guaranteeing this justification is that the test set for four splitting criteria is different for various criteria in one experiment condition. It is impossible to have the same test set for SD, SI, TI, and STI. Indeed, the test sets are the same for each criterion across different experiments.\n\nThe last experiment 3 using the same amount of data for all splitting criteria seems to be the most relevant result in this research. As shown in a literature  [19] , the size of data greatly influences the performance of deep learning, particularly on the SER task. To avoid bias due to differences in the amount of training data, we forced all data in the third experiment to have the same size. In this scenario, the results show consistency with the previous experiments with smaller performance scores. The result for STI was the same for both experiment 2 and experiment 3 since both experiments used the same data for training and test.\n\nOne interesting finding in this research is that the textindependent criterion is more difficult than speaker+textindependent criteria. A possible explanation for this phenomenon might be that the model learns more information in speaker+text-independent than in text-independent only. To test that hypothesis, it is suggested for future research to explore the data inside the model using such a tool, e.g., t-SNE  [20] . Testing the experimental setup used in this research on other datasets might be useful to generalize the finding, particularly on the order of difficulties across different splitting criteria.\n\nThis research challenges the previously reported results on SER research, where most evaluations are performed under speaker-independent only. One may argue that building larger datasets that cover more spoken words may be sufficient to tackle this limitation. However, in the currently evaluated datasets in SER community, there are limited numbers of samples available. There is a necessity to overcome this sentence-open problem in SER with such strategies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "This paper reports experimental research on the effect of different splitting criteria for speech emotion recognition tasks. Since linguistic information is manifested in acoustic features, it is hypothesized that different splitting criteria with regard to linguistic information will lead to different performances on the SER task. Four splitting criteria were evaluated, focusing on the differences between splitting criteria with the same and different linguistic information. The first is known as text-dependent, while the latter is known as text-independent. Along with splitting criteria by speakers, we conducted experiments on three conditions. The results show a consistency that text-independent condition is the most difficult condition among others.\n\nTo tackle the limitation of text-independent condition, one may utilize a larger dataset to cover more spoken words for training SER. Under limited or small dataset, it is necessary to find a strategy to learn information in different sentences in training and test partitions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Acknowledgment",
      "text": "This paper is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO), Japan.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: ]. The results suggested that",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the ﬂow diagram of this research. Each",
      "page": 2
    },
    {
      "caption": "Figure 1: Flow diagram of research",
      "page": 3
    },
    {
      "caption": "Figure 2: depicts the conﬁguration of this splitting criteria. Note that",
      "page": 3
    },
    {
      "caption": "Figure 2: Data partition on each splitting criteria; the number of samples on",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "a-sasou@aist.go.jp"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Abstract—Traditional speech emotion recognition (SER) eval-\nRecent\nresearch in SER, particularly by fusing acoustic"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "uations have been performed merely on a speaker-independent"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "and linguistic information, has found that different splitting"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "condition;\nsome of\nthem even did not evaluate their result on"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "criteria in splitting data for\ntraining will\nresult\nin different"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "this condition. This paper highlights the importance of splitting"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "performances\n[3].\nIn\nthe\nfusion\nof\nacoustic\nand\nlinguis-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "training and test data for SER by script, known as\nsentence-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "tic\ninformation,\nit\nis\nsound\nthat\nthe model\nneeds\nto\nbe\nopen or\ntext-independent\ncriteria. The\nresults\nshow that\nem-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "ploying\nsentence-open\ncriteria\ndegraded\nthe\nperformance\nof\ntrained in different scripts, known as sentence-open or\ntext-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "SER. This ﬁnding implies the difﬁculties of recognizing emotion"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "independent. This strategy was intended to avoid the effect of"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "from speech in different\nlinguistic\ninformation embedded in"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "having the same linguistic information for predicting emotion"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "acoustic\ninformation.\nSurprisingly,\ntext-independent\ncriteria"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "under the same sentences for both training and test partitions."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "consistently\nperformed worse\nthan\nspeaker+text-independent"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Since linguistic information is extracted via text or script, this\ncriteria. The\nfull order of difﬁculties\nfor\nsplitting criteria on"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "SER performances from the most difﬁcult to the easiest is text-\nsplitting condition is necessary to evaluate such discrepancies"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "independent,\nspeaker+text-independent,\nspeaker-independent,"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "of using different\nfeatures or\ntypes of\ninformation. Using"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "and speaker+text-dependent. The\ngap between speaker+text-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "merely acoustic features\nfor SER, one may argue that\nthis"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "independent\nand\ntext-independent was\nsmaller\nthan\nother"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "evaluation\nis\nunnecessary\nsince\nno\nlinguistic\nfeatures\nare"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "criteria,\nstrengthening\nthe difﬁculties\nof\nrecognizing\nemotion"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "involved in building the SER model.\nfrom speech in different sentences."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Fujisaki in 2002 proposed a scheme in which various types\nIndex Terms—Speech\nemotion\nrecognition,\ndata\npartition,"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "text-independent, speaker-independent, splitting criteria\nof information are manifested in the segmental and supraseg-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "mental features of speech [4]. One of the types of information"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "includes emotion. Referencing this argument\nthat emotional\nI.\nINTRODUCTION"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "information is manifested directly in speech without a need to"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Speech emotion recognition (SER) is one topic of interest"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "convert speech into text,\nthere is a possibility that different"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "in automatic speech recognition and understanding.\nIn con-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "sentences will yield different SER performances under\nthe"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "trast\nto automatic speech recognition (ASR) which attempts"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "same\nacoustic-only\nsystem. The\ncurrent\nresearch\non SER"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "to obtain linguistic information from speech, SER attempts"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "showed no evaluation of\nthe differences of\nsplitting crite-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "to obtain non-linguistic\ninformation from speech.\nIn more"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "ria, particularly comparing data with and without\nlinguistic"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "concrete, SER aims to infer the affective state of the speaker"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "information."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "from solely speech data."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "The\ncontribution\nof\nthis\npaper\nis\nan\nevaluation\nof\nthe"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "SER can be designed to recognize discrete emotion, con-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "effect\nof\nsplitting\ncriteria\ninto\nthe\ntraining\ndata\non\nSER"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "tinuous emotion, or both emotion models. Recent\nresearch"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "performance. As\nargued\npreviously,\nlinguistic\ninformation"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "suggested that\nemotion is ordinal by nature\n[1], which is"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "is\nembedded\nin\nacoustic\nfeatures;\nhence,\nevaluating\ntext-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "closer\nto a\ncategorical\nthan the\ncontinuous model.\nIn the"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "independent\ncriteria,\ni.e.,\ndifferent\nsentences\nfor\ntraining"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "categorical model, several emotion categories exist, from the"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "and\ntest,\nis\nnecessary\nto\nobserve\nsuch\neffects. We\neval-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "simplest\ntwo categories with positive and negative emotions"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "uated\nfour\nsplitting\ncriteria:\nspeaker-dependent\n(including"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "to 27 categories\n[2]. The choice of emotion model\nin SER"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "text-dependent data), speaker-independent,\ntext-independent,"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "depends on the availability of\nthe labels in the dataset."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "and speaker+text-independent criteria, and traced their SER"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Data-driven methods,\nin which most SER systems employ"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "performances. We experimented with these criteria in three"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "this kind of approach,\nrely on the conﬁguration or selection"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "different experiments. The results of\nthree different experi-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "of\nthe data\nto build the model.\nIn SER,\nit\nis\ncommon to"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "ments show a consistent pattern of difﬁculties for four differ-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "split the data by evaluating different speakers for training and"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "ent\nsplitting criteria. Text-independent criteria obtained the"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "test partitions. This approach, known as speaker-independent"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "worst\nresult\nfollowed by speaker+text-independent, speaker-"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "criteria, is a gold standard to build SER model that minimizes"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "independent, and speaker-dependent criteria."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "speaker variability in the training phase."
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "II. RELATED WORK"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "*Corresponding\nauthor,\non\nleave\nfrom Department\nof\nEngineering"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Research on the evaluation of\nthe effect of data selection"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "Physics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia (email:"
        },
        {
          "National\nInstitute of Advanced Industrial Science and Technology, Japan": "bagus@ep.its.ac.id).\non speech processing is not new. Different data for\ntraining"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "will produce different results on the same algorithm. A model": "generalization is a challenging task to minimize the variance",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "culture when they are spoken in the recordings. For instance,"
        },
        {
          "will produce different results on the same algorithm. A model": "among different data inputs for\nthe same model.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "sentences\nthat end with nouns were removed due to emo-"
        },
        {
          "will produce different results on the same algorithm. A model": "In [5],\nthe\nauthors\nevaluated the\neffect of\ntraining data",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "tional\nindependence. The labeling of\nthe emotion categories"
        },
        {
          "will produce different results on the same algorithm. A model": "selection for speech recognition of emotional speech. Emo-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "was performed by matching emotion-related words. Selection"
        },
        {
          "will produce different results on the same algorithm. A model": "tional speech showed lower error\nrates than neutral speech.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "criteria based on entropy reduce the number into 50 sentences"
        },
        {
          "will produce different results on the same algorithm. A model": "Using their proposed selection criteria based on the entropy",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "for each emotion category."
        },
        {
          "will produce different results on the same algorithm. A model": "of diphones,\nthey improved the error\nrate of\nspeech recog-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "All data in the JTES dataset were involved in the exper-"
        },
        {
          "will produce different results on the same algorithm. A model": "nition by transferring the model\nfrom emotional\nspeech to",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "iments. However, due\nto the naturalness of\nthis\nresearch’s"
        },
        {
          "will produce different results on the same algorithm. A model": "non-emotional speech data.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "aims, each splitting criteria resulted in a different number of"
        },
        {
          "will produce different results on the same algorithm. A model": "The\nauthors\nof\n[3]\npointed\nto\nthe\ndifferent\nresults\nof",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "data for each experiment. For instance, splitting by speakers"
        },
        {
          "will produce different results on the same algorithm. A model": "splitting data by random folds, speaker folds, and script folds",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "and by sentences will allocate 19600 samples\nfor\ntraining,"
        },
        {
          "will produce different results on the same algorithm. A model": "on the\nspeech emotion recognition by fusing acoustic\nand",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "and\nthe\nrest\n400\nsamples\nfor\ntest. However,\nsplitting\nby"
        },
        {
          "will produce different results on the same algorithm. A model": "linguistic\ninformation\n[Fig.\n2]. The\nresults\nsuggested\nthat",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "both speakers\nand sentences only allocates 14400 samples"
        },
        {
          "will produce different results on the same algorithm. A model": "splitting by both speakers and scripts is more difﬁcult\nthan",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "for\ntraining due to overlap between both speaker and text."
        },
        {
          "will produce different results on the same algorithm. A model": "splitting by random folds only or splitting by speaker only.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "To avoid the\neffect of different\nsizes of\ntraining data, we"
        },
        {
          "will produce different results on the same algorithm. A model": "No information is available on comparing the difﬁculties of",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "performed an evaluation on the same amount of data."
        },
        {
          "will produce different results on the same algorithm. A model": "splitting by both speaker and script to splitting by script only.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "B. Feature Extraction"
        },
        {
          "will produce different results on the same algorithm. A model": "The author of\n[6] evaluated Japanese Twitter-based emo-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "tional speech (JTES) dataset with speaker-open and sentence-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "The ﬁrst step to obtaining input data for\nthe SER system"
        },
        {
          "will produce different results on the same algorithm. A model": "closed conditions. The author achieved an accuracy of 81.44",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "from the raw dataset\nis by extracting features of speech. For"
        },
        {
          "will produce different results on the same algorithm. A model": "% by utilizing a\ncommon normalization. Using two other",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "this purpose, we utilized opensmile\nfeature\nextraction tool"
        },
        {
          "will produce different results on the same algorithm. A model": "corpora as training data and JTES as test data resulted in a de-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "to extract a set of acoustic features\nthat\nrepresent emotion"
        },
        {
          "will produce different results on the same algorithm. A model": "graded score to 80.66%, highlighting the difﬁculty of cross-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "information in speech. The version of opensmile used in this"
        },
        {
          "will produce different results on the same algorithm. A model": "corpus evaluation [7]. On the same JTES dataset but different",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "large’ with default\nexperiment is 3.0. The feature set is ’emo"
        },
        {
          "will produce different results on the same algorithm. A model": "criteria, speaker-open and sentence-open,\nthe authors of\n[8]",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "conﬁguration emo_large.conf in the config/misc/"
        },
        {
          "will produce different results on the same algorithm. A model": "achieved an accuracy of 73.4 % by utilizing multi-stream",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "directory."
        },
        {
          "will produce different results on the same algorithm. A model": "attention-based bidirectional LSTM (BLSTM) with feature",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "For this emo\nlarge feature set, we extracted only statistical"
        },
        {
          "will produce different results on the same algorithm. A model": "segmentation. This result\nis the closest machine performance",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "functions per utterance. The\nsize was\na 6552-dimensional"
        },
        {
          "will produce different results on the same algorithm. A model": "to human performance on the JTES dataset,\nin which human",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "feature from 56 low-level descriptors with their deltas and"
        },
        {
          "will produce different results on the same algorithm. A model": "evaluators scored 75.5% from acoustic subjective evaluation",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "deltas-deltas (total 168 features) multiplied by 39 statistics."
        },
        {
          "will produce different results on the same algorithm. A model": "[9]. However,\nagain,\nthere\nis no evaluation comparing the",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "For the feature data, ﬁrst, we extracted each audio ﬁle (.wav)"
        },
        {
          "will produce different results on the same algorithm. A model": "effect of\nsplitting by speakers\n(text-independent) and other",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "into corresponding .csv ﬁles. Then we extracted related rows"
        },
        {
          "will produce different results on the same algorithm. A model": "criteria.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "in .csv ﬁles (rows 6559 to end) and saved them in the form of"
        },
        {
          "will produce different results on the same algorithm. A model": "This\nresearch ﬁlls\nthe\ngap\nin\nthe\nexisting\nresearch\non",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Numpy arrays (.npy ﬁles). The ﬁrst 6558 rows only consist"
        },
        {
          "will produce different results on the same algorithm. A model": "evaluating SER with different splitting criteria. We evaluated",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "of header names of the corresponding features, which are not"
        },
        {
          "will produce different results on the same algorithm. A model": "speaker-independent\n(including\ntext-independent),\nspeaker-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "used in a classiﬁcation process."
        },
        {
          "will produce different results on the same algorithm. A model": "dependent,\ntext-independent,\nand\nspeaker+text-independent",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "data.\nThis\npaper\ndenotes\nan\ninsight\nto\nadd\ninformation",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "TABLE I"
        },
        {
          "will produce different results on the same algorithm. A model": "missing in the previous papers\nto gain new insight on the",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "LOW-LEVEL DESCRIPTORS (LLD) IN EMO LARGE CONFIGURATION"
        },
        {
          "will produce different results on the same algorithm. A model": "effect of splitting data based on the different criteria on SER",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Feat. number\nLLD"
        },
        {
          "will produce different results on the same algorithm. A model": "performance.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "1\nLog energy"
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "2-14\nMFCC (0-12)"
        },
        {
          "will produce different results on the same algorithm. A model": "III. METHODS",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "15-40\nMelspec (0-25)"
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "41\nZero Crossing Rate (ZCR)"
        },
        {
          "will produce different results on the same algorithm. A model": "Fig.\n1\nshows\nthe ﬂow diagram of\nthis\nresearch. Each",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Voice probability\n42"
        },
        {
          "will produce different results on the same algorithm. A model": "component on that diagram is explained below.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "44-44\nfo, fo env"
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "45\nSpectral energy: 0-250 Hz"
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Spectral energy: 0-250 Hz\n46"
        },
        {
          "will produce different results on the same algorithm. A model": "A. DATASET",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "47\nSpectral energy: 0-650 Hz"
        },
        {
          "will produce different results on the same algorithm. A model": "This paper makes use of the phonetically and prosodically",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "48\nSpectral energy: 1000-4000 Hz"
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Spectral RollOff\n(25%, 50%, 75%, 90%)\n49-52"
        },
        {
          "will produce different results on the same algorithm. A model": "balanced Japanese Twitter-based Emotional Speech corpus",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "53-54\nSpectral Flux, Spectral Centroid"
        },
        {
          "will produce different results on the same algorithm. A model": "(JTES)\n[10]. The main\nreason\nto\nchoose\nthis\nemotional",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "55-56\nSpectral MaxPos, Spectral MinPos"
        },
        {
          "will produce different results on the same algorithm. A model": "speech\ndataset\nis\ndue\nto\nthe\nrichness\nof\nsentences\n(200",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "∆ + ∆∆\n57-168"
        },
        {
          "will produce different results on the same algorithm. A model": "sentences: 50 sentences for each emotion) and speakers (100",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": ""
        },
        {
          "will produce different results on the same algorithm. A model": "speakers: 50 male and 50 female), which can be evaluated",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "Table I shows detail of the acoustic features in emo\nlarge"
        },
        {
          "will produce different results on the same algorithm. A model": "with different criteria and conditions. Japanese speech emo-",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "setup. The\ntop rows\nshow ﬁfty-six LLDs. Adding this 56"
        },
        {
          "will produce different results on the same algorithm. A model": "tion recognition also lacks exploration compared to English,",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "LLD with their deltas (∆) and deltas-deltas (∆∆) sums up"
        },
        {
          "will produce different results on the same algorithm. A model": "Chinese, and European languages.",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "168 features. For\neach LLD, 39 statistics were\ncomputed,"
        },
        {
          "will produce different results on the same algorithm. A model": "The JTES dataset consists of 20000 utterances from four",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "resulting\nin\n6652\nfeatures\nin\ntotal. Aside\nfrom emotion"
        },
        {
          "will produce different results on the same algorithm. A model": "emotion categories: joy, anger, sadness, and neutral. For each",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "recognition,\nthis\nfeature\nset\nis\nreported to be\neffective\nfor"
        },
        {
          "will produce different results on the same algorithm. A model": "emotion,\nthere are 500 raw sentences collected from Twitter",
          "data. The Twitter\ntext data are modiﬁed to reﬂect\nJapanese": "classifying dogs’ barking [11]."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "In the ﬁrst"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "an\naverage\nnumber"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "along with its"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "we extended it"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "This ﬁrst experiment"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "and\nto\ncompare"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "validation with"
        },
        {
          "each criteria is shown in Table III.": ""
        },
        {
          "each criteria is shown in Table III.": "number of 30 trials was"
        },
        {
          "each criteria is shown in Table III.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JTES": "Fig. 1.\nFlow diagram of"
        },
        {
          "JTES": "C. Classiﬁers"
        },
        {
          "JTES": "We evaluated multi-layer perceptron (MLP), also known"
        },
        {
          "JTES": ""
        },
        {
          "JTES": ""
        },
        {
          "JTES": "as fully-connected or dense network, for the classiﬁer of the"
        },
        {
          "JTES": "SER system. The previous research on speech emotion recog-"
        },
        {
          "JTES": "nition has\nshown the effectiveness of\nthese fully-connected"
        },
        {
          "JTES": ""
        },
        {
          "JTES": ""
        },
        {
          "JTES": "(FC) or dense networks to overcome the SER problem [12]."
        },
        {
          "JTES": "The structure of FC networks\nfollow the previous\nresearch"
        },
        {
          "JTES": "on the different\nlayers and dataset\n[13]. Three dense layers"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "were stacked, followed by a four-unit dense layer as the ﬁnal"
        },
        {
          "JTES": "layer."
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Table II shows the detail of hyperparameters employed in"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "the MLP networks. The choice of the values is based on some"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "references\n[13],\n[14]. The same architecture was employed"
        },
        {
          "JTES": "to evaluate four different conditions in three experiments."
        },
        {
          "JTES": ""
        },
        {
          "JTES": "TABLE II"
        },
        {
          "JTES": "HYPERPARAMETERS OF THE CLASSIFIER"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Parameter\nValue"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Networks/layers\nFC"
        },
        {
          "JTES": "Layer activation\nReLU"
        },
        {
          "JTES": "Units\n(256, 256, 256)"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Optimizer\nAdam"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Learning rate\n0.001"
        },
        {
          "JTES": "Epoch\n25"
        },
        {
          "JTES": "Batch size\n1024"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Validation split\n20%"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Output\nlayer\n(units)\nFC (4)"
        },
        {
          "JTES": "Output activation\nSoftmax"
        },
        {
          "JTES": ""
        },
        {
          "JTES": ""
        },
        {
          "JTES": "D. Experiments"
        },
        {
          "JTES": ""
        },
        {
          "JTES": ""
        },
        {
          "JTES": "Since\nthe\ngoal\nof\nthis\nresearch\nis\nto\nevaluate\nthe\nper-"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "formance of SER under different\ntraining criteria, we split"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "our experiment\ninto four criteria, namely speaker-dependent"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "(SD),\nspeaker-independent\n(SI),\ntext-independent\n(TI),\nand"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "speaker+text-independent\n(STI). The\nsplitting criteria were"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "designed to match the number of STI\nthe test partition,\ni.e.,"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "400 samples,\nfollowing the previous research [8],\n[14]. Fig."
        },
        {
          "JTES": ""
        },
        {
          "JTES": "2 depicts the conﬁguration of this splitting criteria. Note that"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "the ﬁrst SD criterion also contains text-dependent data since"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "the data are randomly shufﬂed. For instance, we only selected"
        },
        {
          "JTES": ""
        },
        {
          "JTES": "the\nﬁrst\n19600\nutterances\nfor\ntraining\nand\nthe\nrest\n400"
        },
        {
          "JTES": "utterances for the test for both ﬁrst and second experiments."
        },
        {
          "JTES": "Both number of data are obtained after shufﬂing the data. In"
        },
        {
          "JTES": "the second, third, and fourth criteria, we selected the samples"
        },
        {
          "JTES": "such that\nthe condition of each criterion is fulﬁlled,\ni.e.,\nthe"
        },
        {
          "JTES": "samples in the training and test partitions are different based"
        },
        {
          "JTES": "on these criteria. Three different experiments were conducted"
        },
        {
          "JTES": "to gain an overall conclusion among these splitting criteria."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "TI\n64.36 ± 0.08\n65.04 ± 0.90\n62.35 ± 0.93"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "STI\n69.56 ± 0.09\n70.65 ± 0.44\n70.65 ± 0.44"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "involved,\nit makes sense that\nthe performance will\nimprove."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "In other words,\nthe SER performance will decrease without"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "linguistic information. While the previous linguistic informa-"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "tion is extracted from text, Fujisaki [4] argued that\nlinguistic"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "information\nitself\nis\nalso manifested\nin\nacoustic\nfeatures,"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "meaning that\nthere is also linguistic information in acoustic"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "features. This argument\nleads to the following presumption."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "If we train SER with different splitting criteria with regard"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "to linguistic information,\nthe performance also changes."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "Implementing\nsplitting\ncriteria with\ndifferent\nlinguistic"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "information\ncan\nbe\nachieved\nby\nseparating\ntraining\nand"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "test\nsets\ninto different\nsentences. The spoken sentences\n(or"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "utterances) used for the test partition can be set to be different"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "from the\nsentences used for\nthe\ntraining. One may argue"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "that\nthis separation only works if linguistic information from"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "text\nis\ninvolved. However, based on a solid argument\nfrom"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "the\nprevious\nliterature\n[4], we\nbelieve\nthat\nthere will\nbe"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "differences in SER performance by splitting training and test"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "partitions into different sentences."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "To test our hypothesis, we split\nthe samples in the dataset"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "into\nfour\ncriteria:\nspeaker-dependent,\nspeaker-independent,"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "text-independent,\nspeaker+text-independent.\nWe\nexperi-"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "mented with these different\nsplitting criteria on three dif-"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "ferent\nconditions:\naverage\ntrials,\ncross-validation,\nand\nthe"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "same amount of data. The results\nshow consistency among"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "different splitting criteria and experiment conditions."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "Among three experiments,\nthe cross-validation condition"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "obtained the highest scores among the other two conditions."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "Shufﬂing different\ntraining and validation data seems\nto be"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "useful\nfor\ngeneralizing\nthe model. Using ﬁve-fold\nshows"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "slightly higher scores than an average of 30 trials. The results"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "may also be used to justify that\nreporting performance from"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "average trials may be sufﬁcient\nto gain an insight\ninto the"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "order of difﬁculties\namong different\nsplitting criteria. One"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "exception in guaranteeing this justiﬁcation is that\nthe test set"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "for\nfour\nsplitting criteria is different\nfor various criteria in"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "one experiment condition.\nIt\nis impossible to have the same"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "test set\nfor SD, SI, TI, and STI.\nIndeed,\nthe test sets are the"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "same for each criterion across different experiments."
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "The last experiment 3 using the same amount of data for"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "all\nsplitting\ncriteria\nseems\nto\nbe\nthe most\nrelevant\nresult"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "in this\nresearch. As\nshown in a literature [19],\nthe size of"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "data\ngreatly\ninﬂuences\nthe\nperformance\nof\ndeep\nlearning,"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "particularly on the SER task. To avoid bias due to differences"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": ""
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "in the amount of training data, we forced all data in the third"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "experiment to have the same size. In this scenario, the results"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "show consistency with the previous experiments with smaller"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "performance scores. The result for STI was the same for both"
        },
        {
          "SI\n87.88 ± 0.09\n88.85 ± 0.49\n86.64 ± 0.63": "experiment 2 and experiment 3 since both experiments used"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "ICASSP 2019\n-\nRecognition\nacross Heterogeneous Languages,”\nin"
        },
        {
          "the same data for\ntraining and test.": "One\ninteresting ﬁnding in this\nresearch is\nthat\nthe\ntext-",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2019 IEEE Int. Conf. Acoust. Speech Signal Process.\nIEEE, may"
        },
        {
          "the same data for\ntraining and test.": "independent\ncriterion\nis more\ndifﬁcult\nthan\nspeaker+text-",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2019, pp. 5881–5885."
        },
        {
          "the same data for\ntraining and test.": "independent\ncriteria. A possible\nexplanation\nfor\nthis\nphe-",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[7] ——,\n“Domain Generalization with\nTriplet Network\nfor\nCross-"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2021\nIEEE Spok. Lang.\nCorpus Speech Emotion Recognition,”\nin"
        },
        {
          "the same data for\ntraining and test.": "nomenon might be that\nthe model\nlearns more information",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Technol. Work.\nIEEE,\njan 2021, pp. 389–396.\n[Online]. Available:"
        },
        {
          "the same data for\ntraining and test.": "in speaker+text-independent\nthan in text-independent only.",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "https://ieeexplore.ieee.org/document/9383534/"
        },
        {
          "the same data for\ntraining and test.": "To test\nthat hypothesis,\nit\nis\nsuggested for\nfuture\nresearch",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[8] Y. Chiba, T. Nose, and A. Ito, “Multi-Stream Attention-Based BLSTM"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "with\nFeature\nSegmentation\nfor\nSpeech\nEmotion Recognition,”\nin"
        },
        {
          "the same data for\ntraining and test.": "to\nexplore\nthe\ndata\ninside\nthe model\nusing\nsuch\na\ntool,",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Interspeech 2020, vol. 2020-Octob.\nISCA: ISCA, oct 2020, pp. 3301–"
        },
        {
          "the same data for\ntraining and test.": "e.g.,\nt-SNE [20]. Testing\nthe\nexperimental\nsetup\nused\nin",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "3305."
        },
        {
          "the same data for\ntraining and test.": "this research on other datasets might be useful\nto generalize",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[9] Y. Mai, T. Nose, Y. Chiba, and A. Ito, “Labeling of Perceived Emotion"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "for Large-Scale Emotional Speech Database,” in 2020 RISP Int. Work."
        },
        {
          "the same data for\ntraining and test.": "the ﬁnding, particularly on the order of difﬁculties\nacross",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Nonlinear Circuits, Commun. Signal Process., 2020, pp. 230–233."
        },
        {
          "the same data for\ntraining and test.": "different splitting criteria.",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[10]\nE. Takeishi, T. Nose, Y. Chiba, and A. Ito, “Construction and analysis"
        },
        {
          "the same data for\ntraining and test.": "This research challenges the previously reported results on",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "of phonetically and prosodically balanced emotional speech database,”"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2016 Conf. Orient. Chapter\nInt. Comm. Coord.\nStand.\nSpeech\nin"
        },
        {
          "the same data for\ntraining and test.": "SER research, where most evaluations are performed under",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Databases Assess. Tech. O-COCOSDA 2016.\nBali:\nIEEE, 2016, pp."
        },
        {
          "the same data for\ntraining and test.": "speaker-independent only. One may argue that building larger",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "16–21."
        },
        {
          "the same data for\ntraining and test.": "datasets that cover more spoken words may be sufﬁcient\nto",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[11] H. P´erez-Espinosa and A. A. Torres-Garc´ıa, “Evaluation of quantitative"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "and qualitative\nfeatures\nfor\nthe\nacoustic\nanalysis of domestic dogs"
        },
        {
          "the same data for\ntraining and test.": "tackle\nthis\nlimitation. However,\nin the\ncurrently evaluated",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "vocalizations,” J.\nIntell. Fuzzy Syst., vol. 36, no. 5, pp. 5051–5061,"
        },
        {
          "the same data for\ntraining and test.": "datasets\nin SER community,\nthere\nare\nlimited numbers of",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2019."
        },
        {
          "the same data for\ntraining and test.": "samples\navailable. There\nis\na\nnecessity\nto\novercome\nthis",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[12] B. T. Atmaja\nand M. Akagi,\n“Deep Multilayer Perceptrons\nfor Di-"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "mensional Speech Emotion Recognition,” in 2020 Asia-Paciﬁc Signal"
        },
        {
          "the same data for\ntraining and test.": "sentence-open problem in SER with such strategies.",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Inf. Process. Assoc. Annu. Summit Conf. APSIPA ASC 2020 - Proc.,"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Auckland, 2020, pp. 325–331."
        },
        {
          "the same data for\ntraining and test.": "V. CONCLUSIONS",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[13] ——, “Improving Valence Prediction in Dimensional Speech Emotion"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Recognition Using Linguistic Information,” in Proc. 2020 23rd Conf."
        },
        {
          "the same data for\ntraining and test.": "This\npaper\nreports\nexperimental\nresearch\non\nthe\neffect",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Orient. COCOSDA Int. Comm. Co-ord.\nStand.\nSpeech Databases"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Assess. Tech. O-COCOSDA 2020.\nIEEE, nov 2020, pp. 166–171."
        },
        {
          "the same data for\ntraining and test.": "of different splitting criteria for speech emotion recognition",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[14] Y. Haneda, M. Katoh, and T. Kosaka, “Basic research on emotional"
        },
        {
          "the same data for\ntraining and test.": "tasks. Since linguistic information is manifested in acoustic",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "recognition for\nJapanese emotional\nspeech corpus\nJTES,” Yamagata"
        },
        {
          "the same data for\ntraining and test.": "features,\nit\nis\nhypothesized\nthat\ndifferent\nsplitting\ncriteria",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "University, Tech. Rep., 2019."
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[15] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness"
        },
        {
          "the same data for\ntraining and test.": "with regard to linguistic\ninformation will\nlead to different",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "of data,” IEEE Intell. Syst., vol. 24, no. 2, pp. 8–12, 2009."
        },
        {
          "the same data for\ntraining and test.": "performances on the SER task. Four\nsplitting criteria were",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[16] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient Emotion Recognition"
        },
        {
          "the same data for\ntraining and test.": "evaluated,\nfocusing on the differences between splitting cri-",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "from Speech Using Deep Learning on Spectrograms,” in Interspeech"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "2017, vol. 2017-Augus.\nISCA:\nISCA, aug 2017, pp. 1089–1093."
        },
        {
          "the same data for\ntraining and test.": "teria with the same and different\nlinguistic information. The",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[17] B. Schuller and A. Batliner, Computational Paralinguistics: Emotion,"
        },
        {
          "the same data for\ntraining and test.": "ﬁrst\nis known as text-dependent, while the latter is known as",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Affect\nand\nPersonality\nin\nSpeech\nand\nLanguage\nProcessing."
        },
        {
          "the same data for\ntraining and test.": "text-independent. Along with splitting criteria by speakers,",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Wiley,\n2013.\n[Online]. Available:\nhttps://books.google.co.jp/books?"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "id=aaTyAAAAQBAJ"
        },
        {
          "the same data for\ntraining and test.": "we conducted experiments on three conditions. The results",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[18] M. Krzywinski and N. Altman, “Points of Signiﬁcance: Error bars,”"
        },
        {
          "the same data for\ntraining and test.": "show a\nconsistency\nthat\ntext-independent\ncondition\nis\nthe",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Nat. Methods, vol. 10, no. 10, pp. 921–922, 2013."
        },
        {
          "the same data for\ntraining and test.": "most difﬁcult condition among others.",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[19] B. T. Atmaja, K. Shirai, and M. Akagi, “Deep Learning-based Cate-"
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "gorical and Dimensional Emotion Recognition for Written and Spoken"
        },
        {
          "the same data for\ntraining and test.": "To tackle the limitation of text-independent condition, one",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Text,” in IPTEK J. Proc. Ser., 2019."
        },
        {
          "the same data for\ntraining and test.": "may utilize a larger dataset\nto cover more spoken words for",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "[20]\nL. van der Maaten and G. Hinton, “Visualizing Data using t-SNE,” J."
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "Mach. Learn. Res., vol. 164, pp. 2579–2605, 2008. [Online]. Available:"
        },
        {
          "the same data for\ntraining and test.": "training SER. Under limited or small dataset,\nit\nis necessary",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        },
        {
          "the same data for\ntraining and test.": "",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": "https://lvdmaaten.github.io/publications/papers/JMLR{ }2008.pdf"
        },
        {
          "the same data for\ntraining and test.": "to ﬁnd a strategy to learn information in different sentences",
          "[6]\nS.-w. Lee, “The Generalization Effect for Multilingual Speech Emotion": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Technology Development Organization (NEDO), Japan.": "REFERENCES"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "[1] G. N. Yannakakis, R. Cowie,\nand C. Busso,\n“The Ordinal Nature"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "of Emotions: An Emerging Approach,” IEEE Trans. Affect. Comput.,"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "vol. 12, no. 1, pp. 16–35, 2021."
        },
        {
          "Technology Development Organization (NEDO), Japan.": "[2] A. S. Cowen and D. Keltner, “Self-report captures 27 distinct cate-"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "gories of emotion bridged by continuous gradients,” Proc. Natl. Acad."
        },
        {
          "Technology Development Organization (NEDO), Japan.": "Sci. U. S. A., vol. 114, no. 38, pp. E7900–E7909, 2017."
        },
        {
          "Technology Development Organization (NEDO), Japan.": "[3]\nL. Pepino, P. Riera, L. Ferrer, and A. Gravano, “Fusion Approaches"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "for Emotion Recognition from Speech Using Acoustic and Text-Based"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "Features,”\nin ICASSP 2020 - 2020 IEEE Int. Conf. Acoust. Speech"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "Signal Process.\nIEEE, may 2020, pp. 6484–6488."
        },
        {
          "Technology Development Organization (NEDO), Japan.": "[4] H. Fujisaki, “Prosody,\nInformation, and Modeling with Emphasis on"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "Tonal Features of Speech,” in Work. Spok. Lang. Process., 2003."
        },
        {
          "Technology Development Organization (NEDO), Japan.": "[5] Y. Yamada, Y. Chiba, T. Nose, and A.\nIto, “Effect of Training Data"
        },
        {
          "Technology Development Organization (NEDO), Japan.": "Selection for Speech Recognition of Emotional Speech,” Int. J. Mach."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Ordinal Nature of Emotions: An Emerging Approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "2",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "3",
      "title": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "4",
      "title": "Prosody, Information, and Modeling with Emphasis on Tonal Features of Speech",
      "authors": [
        "H Fujisaki"
      ],
      "year": "2003",
      "venue": "Work. Spok. Lang. Process"
    },
    {
      "citation_id": "5",
      "title": "Effect of Training Data Selection for Speech Recognition of Emotional Speech",
      "authors": [
        "Y Yamada",
        "Y Chiba",
        "T Nose",
        "A Ito"
      ],
      "year": "2021",
      "venue": "Int. J. Mach. Learn. Comput"
    },
    {
      "citation_id": "6",
      "title": "The Generalization Effect for Multilingual Speech Emotion Recognition across Heterogeneous Languages",
      "authors": [
        "S.-W Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "7",
      "title": "Domain Generalization with Triplet Network for Cross-Corpus Speech Emotion Recognition",
      "year": "2021",
      "venue": "IEEE Spok. Lang. Technol. Work. IEEE"
    },
    {
      "citation_id": "8",
      "title": "Multi-Stream Attention-Based BLSTM with Feature Segmentation for Speech Emotion Recognition",
      "authors": [
        "Y Chiba",
        "T Nose",
        "A Ito"
      ],
      "year": "2020",
      "venue": "Interspeech 2020"
    },
    {
      "citation_id": "9",
      "title": "Labeling of Perceived Emotion for Large-Scale Emotional Speech Database",
      "authors": [
        "Y Mai",
        "T Nose",
        "Y Chiba",
        "A Ito"
      ],
      "year": "2020",
      "venue": "RISP Int. Work. Nonlinear Circuits, Commun. Signal Process"
    },
    {
      "citation_id": "10",
      "title": "Construction and analysis of phonetically and prosodically balanced emotional speech database",
      "authors": [
        "E Takeishi",
        "T Nose",
        "Y Chiba",
        "A Ito"
      ],
      "year": "2016",
      "venue": "2016 Conf. Orient. Chapter Int. Comm. Coord. Stand. Speech Databases Assess"
    },
    {
      "citation_id": "11",
      "title": "Evaluation of quantitative and qualitative features for the acoustic analysis of domestic dogs vocalizations",
      "authors": [
        "H Pérez-Espinosa",
        "A Torres-García"
      ],
      "year": "2019",
      "venue": "J. Intell. Fuzzy Syst"
    },
    {
      "citation_id": "12",
      "title": "Deep Multilayer Perceptrons for Dimensional Speech Emotion Recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "2020 Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. APSIPA ASC 2020 -Proc"
    },
    {
      "citation_id": "13",
      "title": "Improving Valence Prediction in Dimensional Speech Emotion Recognition Using Linguistic Information",
      "year": "2020",
      "venue": "Proc. 2020 23rd Conf. Orient. COCOSDA Int. Comm. Co-ord. Stand. Speech Databases Assess"
    },
    {
      "citation_id": "14",
      "title": "Basic research on emotional recognition for Japanese emotional speech corpus JTES",
      "authors": [
        "Y Haneda",
        "M Katoh",
        "T Kosaka"
      ],
      "year": "2019",
      "venue": "Basic research on emotional recognition for Japanese emotional speech corpus JTES"
    },
    {
      "citation_id": "15",
      "title": "The unreasonable effectiveness of data",
      "authors": [
        "A Halevy",
        "P Norvig",
        "F Pereira"
      ],
      "year": "2009",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "16",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "ISCA: ISCA"
    },
    {
      "citation_id": "17",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Wiley"
      ],
      "year": "2013",
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "Points of Significance: Error bars",
      "authors": [
        "M Krzywinski",
        "N Altman"
      ],
      "year": "2013",
      "venue": "Nat. Methods"
    },
    {
      "citation_id": "20",
      "title": "Deep Learning-based Categorical and Dimensional Emotion Recognition for Written and Spoken Text",
      "authors": [
        "B Atmaja",
        "K Shirai",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "in IPTEK J. Proc. Ser"
    },
    {
      "citation_id": "21",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}