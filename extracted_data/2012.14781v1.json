{
  "paper_id": "2012.14781v1",
  "title": "A Hierarchical Transformer With Speaker Modeling For Emotion Recognition In Conversation",
  "published": "2020-12-29T14:47:35Z",
  "authors": [
    "Jiangnan Li",
    "Zheng Lin",
    "Peng Fu",
    "Qingyi Si",
    "Weiping Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) is a more challenging task than conventional text emotion recognition. It can be regarded as a personalized and interactive emotion recognition task, which is supposed to consider not only the semantic information of text but also the influences from speakers. The current method models speakers' interactions by building a relation between every two speakers. However, this fine-grained but complicated modeling is computationally expensive, hard to extend, and can only consider local context. To address this problem, we simplify the complicated modeling to a binary version: Intra-Speaker and Inter-Speaker dependencies, without identifying every unique speaker for the targeted speaker. To better achieve the simplified interaction modeling of speakers in Transformer, which shows excellent ability to settle long-distance dependency, we design three types of masks and respectively utilize them in three independent Transformer blocks. The designed masks respectively model the conventional context modeling, Intra-Speaker dependency, and Inter-Speaker dependency. Furthermore, different speaker-aware information extracted by Transformer blocks diversely contributes to the prediction, and therefore we utilize the attention mechanism to automatically weight them. Experiments on two ERC datasets indicate that our model is efficacious to achieve better performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Nowadays, intelligent machines to precisely capture speakers' emotions in conversations are gaining popularity, thus driving the development of Emotion Recognition in Conversation (ERC). ERC is a task to predict the emotion of the current utterance expressed by a specific speaker according to the context  (Poria et al. 2019b) , which is more challenging than the conventional emotion recognition only considering semantic information of an independent utterance.\n\nTo precisely predict the emotion of a targeted utterance, both the semantic information of the utterance and the information provided by utterances in the context are critical. Nowadays, a number of works  (Hazarika et al. 2018a,b; Majumder et al. 2019; Ghosal et al. 2019)  demonstrate that the interactions between speakers can facilitate extracting information from contextual utterances. We denote this kind of information with modeling speakers' interactions as (c). Relational graph for u3, u4 and u5 in simplified dependencies. Only 2 relations are involved.  speaker-aware contextual information. To capture speakeraware contextual information, the state-of-the-art model Di-alogueGCN  (Ghosal et al. 2019 ) introduces Self and Inter-Speaker dependencies, which capture the influences from different speakers. As illustrated in Fig.  1  (a), Self and Inter-Speaker dependencies establish a specific relation between every two speakers and construct a fully connected relational graph. And then a Relational Graph Convolutional Network (RGCN)  (Schlichtkrull et al. 2018 ) is applied to process such a graph. Although DialogueGCN can achieve excellent performance with Self and Inter-Speaker dependencies, this speaker modeling is easy to be complicated with the number of speakers increasing. As shown in Fig.  1 (b ), for a conversation clip with two speakers, the considered relations reach to 7. The number can drastically increase with more speakers involved. Thus this complicated speaker modeling is hard to deal with the condition that the number of speakers dynamically changes, and not flexible to be deployed in other models. In addition, RGCN processing the fully connected graph with multiple relations requires tremendous consumption of computation. This limitation leads to Dia-logueGCN only considering the local context in a conversation  (Ghosal et al. 2019) . Therefore, it is appealing to introduce a simple and general speaker modeling, which is easy to extend in all scenes and realize in other models so that long-distance context can be available.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Simplified Dependencies",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self And Inter-Speaker Dependencies",
      "text": "To address the above problem, we propose a TRans-forMer with Speaker Modeling (TRMSM). First, we simplify the Self and Inter-speaker dependencies to a binary version, which only contains two relations: Intra-Speaker dependency and Inter-Speaker dependency. As illustrated in Fig.  1 , for the speaker of the targeted utterance, Intra-Speaker dependency focuses on the influence from the same speaker, and Inter-Speaker dependency treats other speakers as a whole group instead of building a relation between every two speakers. In this way, our simplified modeling can be easy to extend in other models and deal with the scene with the dynamical number of speakers without introducing new relations between speakers.\n\nFurthermore, with the ability to settle long-distance dependency, Transformer  (Vaswani et al. 2017 ) achieves excellent performance among a great number of Natural Language Processing (NLP) problems. To better model the longdistance contextual utterances in a conversation, we utilize a hierarchical Transformer with two levels: sentence level and dialogue level. In the sentence level, BERT  (Devlin et al. 2019 ) encodes the semantic representation for a targeted utterance, and in the dialogue level, Transformer is used to capture the information from contextual utterances. To better model our simplified dependencies in the dialogue-level Transformer, we design three masks: Conventional Mask for conventional context modeling, Intra-Speaker Mask for Intra-Speaker dependency, and Inter-Speaker Mask for Inter-Speaker dependency. To realize the functions of masks, we deploy three independent Transformer blocks in dialogue level and the designed masks are respectively used in these Transformer blocks. With different speaker-aware contextual information extracted by these Transformer blocks, whose contributions to the final prediction are diverse, we utilize the attention mechanism to automatically weight and fuse them. Besides, we also apply two other simple fusing methods: Add and Concatenation to demonstrate the advancement of the attention.\n\nSpecifically, our contributions are concluded as follows:\n\n• We simplify Self and Inter-speaker dependencies to a binary version, so that the speaker interaction modeling can be extended in hierarchical Transformer and the longdistance context can be considered.\n\n• We design three types of masks to achieve speakers' interactions modeling in Transformer and utilize the attention mechanism to automatically pick up the important speaker-aware contextual information.\n\n• We conduct experiments on two ERC datasets: IEMO-CAP and MELD. Our method achieves state-of-the-art performance on both datasets on average.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Two aspects are strongly related to our work: Emotion recognition in conversation and Utilization of mask in Transformer.\n\nEmotion recognition in conversation Hierarchical structure based on RNN  (Jiao, Lyu, and King 2020; Jiao et al. 2019)  or Transformer  (Zhong, Wang, and Miao 2019; Li et al. 2020 ) is leveraged in ERC to capture contextual information. Except contextual information, speaker information is proven to be important to ERC. Speakers can be regarded as objects related to utterances or additional information for utterances. As objects, speakers are involved in the graph of conversation as nodes to interact with utterances  (Zhang et al. 2019) . As additional information, speaker information is modeled via utterances. Specifically,  Hazarika et al. (2018b,a)  employ GRUs and Memory Network (Memnet)  (Sukhbaatar et al. 2015)  to model speakers' interactions in the dyadic conversation, which is difficult to extend to multi-speaker conditions. Therefore,  Majumder et al. (2019)  generalize speakers as parties, track them by GRU, and utilize attention mechanism to gather interactive information in multi-speaker conversations. Even so,  Ghosal et al. (2019)  argue that  Majumder et al. (2019)  ignored the influences from other speakers and propose Self and Inter-Speaker dependencies to formalize interactions within and between speakers. However, the complicated modeling of speakers' interactions is difficult to apply in other models, thus requiring a simplified version.\n\nUtilization of mask in Transformer Masks in Transformer are utilized to mask the unattended elements in selfattention. Recently, masks are well-designed and leveraged in language modeling  (Dong et al. 2019; Devlin et al. 2019; Radford et al. 2018 ) and conversation structure modeling  (Zhu et al. 2020) . Masks are flexible and convenient to be implemented and we choose them to model the interactions of speakers in Transformer.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we will elaborate on the task definition and the structure of TRMSM which is illustrated in Fig.  2 . Our model contains 4 parts: Sentence-Level Encoder, Dialogue-Level Encoder, Fusing Method, and Classifier.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Definition",
      "text": "within is sequentially formed by L n words. Particularly, M speakers, whose set is SP K = {spk 1 , spk 2 , ..., spk M }, participate in the conversation. For each utterance u n , a emotion label e n ∈ E and a speaker annotation p n ∈ SP K are assigned. ERC task aims to predict the emotion of every utterance in C with the information provided above.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sentence-Level Encoder",
      "text": "To encode a more informative and context-aware representation of a single utterance based on Transformer, we utilize\n\nwhere W ∈ R Ln×dw is the output of the top layer of BERT and d w is the dimension of the word representation. To obtain an utterance representation for u n , a max-pooling operation followed by a projection is deployed:\n\nwhere u n ∈ R du represents the utterance and d u is the dimension of utterance representation. By processing every utterance in a conversation, we finally obtain the representation matrix C ∈ R N ×du .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dialogue-Level Encoder",
      "text": "In the dialogue level, we utilize three transformer blocks: Conventional Blocks for conventional context modeling, Intra-Speaker Blocks for Intra-Speaker dependency, and Inter-Speaker Blocks for Inter-Speaker dependency. Due to the same structures of all Transformer blocks, we simply introduce the general process of the first layer of Transformer blocks.\n\nGiven the conversation matrix C processed by the sentence-level encoder, to avoid the absence of positional information in C, an Absolute Positional Embedding is added to every representation in C:\n\nwhere P E(0 : N ) is in the same dimension as C. Self-attention intuitively provides an interactive pattern for contextual modeling of conversations. Taking advantage of the mechanism of self-attention, the targeted utterances can be parallelly processed. Therefore, the targeted utterances are regarded as a query matrix, and the contextual utterances act as a key matrix, so that every utterance simultaneously assesses how much information shall be obtained from every contextual utterance. In this way, C is projected to query matrix Q ∈ R N ×da , key matrix K ∈ R N ×da , and value matrix V ∈ R N ×da by linear projections without bias:\n\nwhere [] is the concatenating operation. Self-attention is calculated by:\n\nwhere * denotes element-wise multiplication; M ∈ R N ×N is the utilized mask which is a square matrix whose noninfinite elements equal 1. We will introduce different masks used by diverse blocks later.\n\nO 1 C acts as the input of the second transformer layer, and by this analogy, we obtain the final output O C ∈ R N ×du after multiple layers. Therefore, the outputs of our 3 blocks can be denoted as:\n\nfor Intra-Speaker Block, and O ER C for Inter-Speaker Block. Due to the limited space in this paper, more details about Transformer can be reviewed in  Vaswani et al. (2017) .\n\nMasks can prompt Transformer blocks to realize their different functions, and we introduce how to form these 3 masks:\n\nConventional Mask sets all the elements of itself to 1, which means that every targeted utterance can get access to all the contextual utterances. Conventional Mask is applied in the multi-head attention of Conventional Blocks and is illustrated in Fig.  2 (a) . We annotate Conventional Mask as M C .\n\nIntra-Speaker Mask only considers those contextual utterances tagged with p n , which is the speaker tag of the targeted utterance. Therefore, based on M C , Intra-Speaker Mask M RA sets positions representing other speakers to -INF. Intra-Speaker Mask is illustrated in Fig.  2",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "(B).",
      "text": "Inter-Speaker Mask regards other speakers different from the one of the targeted utterance as one unit due to our simplification. Therefore, based on M C , Inter-Speaker Mask M ER sets positions whose speaker is the same as the speaker tag of the targeted utterance to -INF. Inter-Speaker Mask is illustrated in Fig.  2 (c ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fusing Method",
      "text": "As blocks produce different outputs that carry various speaker-aware contextual information, we utilize 3 simple methods to fuse the information.\n\nAdd As illustrated in Fig.  2  (i), Add equally regards the contributions of all outputs of blocks. Therefore, the fusing representation is:\n\nConcatenation Concatenation (illustrated in Fig.  2  (ii)) is also a simple but effective method to combine different information. Different from Add operation, Concatenation can implicitly choose the information which is important for the final prediction due to the following linear projection of classifier. Therefore, the fusing representation R ∈ R N ×3du is:\n\n) Attention As the contributions of different speaker parties are diversely weighted, it is feasible that the model automatically chooses the more important information. Therefore, we utilize the widely used attention  (Lian et   to achieve this goal. Attention mechanism takes 3 block outputting representations as inputs and produces an attention score for each representation. For simplicity, we take representations\n\nof utterance i as an example. Therefore, the attention score and fusing representation are computed as:\n\nwhere O i ∈ R 3×du is the concatenated representations, α ∈ R 1×3 is the attention score, w F ∈ R 1×du is a trainable parameter, and R i ∈ R 1×du is the fusing representation. Finally, all fusing representations of utterances are concatenated as R ∈ R N ×du .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classifier",
      "text": "With the sentence-level and dialogue-level contextual information fully modeled by encoders, the dialogue-level output is fed to a classifier which predicts the final emotion distributions:\n\nwhere\n\nand Ŷ is the matrix of emotion distributions of all utterances in conversation C. The model is trained by a cross-entropy loss function, which is calculated as:\n\nwhere y i is the one-hot vector denoting the emotion label of utterance i in a conversation, e denotes the dimension of each emotion, N l denotes the length of l-th conversation, and T denotes the number of conversations in a dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup Datasets",
      "text": "We evaluate our models on two datasets: IEMOCAP  (Busso et al. 2008) , MELD  (Poria et al. 2019a) , and both of them are multi-modal datasets that contain three modalities. We solely consider the textual modality following",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Compared Methods",
      "text": "To distinguish our models with different fusing methods, we construct 3 model variants: TRMSM-Add, TRMSM-Cat, and TRMSM-Att. To show the importance of the speakerrelated information, we construct our model without Intra-Speaker Blocks and Inter-Speaker Blocks, which we denote it as TRM. Besides, our models are compared with the baselines below:\n\n• CMN  (Hazarika et al. 2018b ) CMN is proposed to model dyadic conversations using two sets of RNNs and Memnets to respectively track different speakers. To retain the positional information from its hierarchical structure, a GRU is constructed for attention mechanism. • DialogueGCN  (Ghosal et al. 2019)  To fully model the interactive information between speakers, DialogueGCN models detailed dependencies between speakers using a Relational GCN. • KET  (Zhong, Wang, and Miao 2019)  KET introduces the Transformer structure to model context in conversations. It also proposes an effective graph attention to extract information from commonsense knowledge bases. • BERT  (Devlin et al. 2019 ) A vanilla BERT followed by a classifier is fine-tuned to show the importance of context. • Other baselines Both based on CNN to extract semantic information, scLSTM  (Poria et al. 2017)  utilizes LSTM  (Hochreiter and Schmidhuber 1997)  and Memnet  (Sukhbaatar et al. 2015)  utilizes memory network to model conversational context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation",
      "text": "For BERT and sentence-level encoder, an uncased BERTbase 1  model is adopted. For the dialogue-level encoder, the dimension of dialogue-level representation is set to 300 for IEMOCAP and 200 for MELD; the number of transformer layers is set to 6 for IEMOCAP and 1 for MELD; the number of heads is set 6 for IEMOCAP and 4 for MELD; dropout rate is set to 0.1. Additionally, models are trained using AdamW  (Kingma and Ba 2015; Loshchilov and Hutter 2019)  for 10000 steps with 1000 steps for warming up, and the learning rate linearly decaying after the warm-up is set to 1e-5 for IEMOCAP and 8e-6 for MELD. Due to the parallel prediction of utterances in one conversation, batch size is set to 1 following  Jiao, Lyu, and King (2020) . Besides, DialogueGCN is trained in the setting of 90:10 data split on IEMOCAP, and for a fair comparison, we re-run DialogueGCN with 80:20 data split using the open-source code 2  . All of our results reported are the average values of 5 runs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussions",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overall Results",
      "text": "For IEMOCAP, weighted-F1 (wF1) score is used as the metric. However, the data proportion of MELD is in a severely imbalanced condition. Therefore, the weighted-F1 score is not that proper and enough for MELD. To balance the contributions of large classes and small classes, we follow  Zhang et al. (2020)  and also use the average value of macro F1 score and micro F1 score as one metric, which is calculated by mF 1 = (F 1 macro + F 1 micro )/2. For IEMOCAP, as shown in Tab. 2, BERT attains wF1 of 54.01 which is substantially worse than our models and most state-of-the-art models considering the dialogue-level context. This result may indicate that IEMOCAP contains considerable utterances that cannot be predicted only depending on the semantic information, which is out of the Table  3 : The Results of our models on MELD. MELD uses weighted-F1 (wF1) score, and the average value (mF1) of Macro-F1 and Micro-F1, as the metrics. ♦ means referring from  Jiao, Lyu, and King (2020) . conversational context. Furthermore, TRMSM-Att outperforms AGHMN by 2.24 wF1 and DialogueGCN (80: 20) by 3.28 wF1, which benefits from the powerful Transformer and our speaker modeling with long-distance information considered. For emotions, TRMSM-Att achieves the best F1 on Frustrated, and TRMSM-Add achieves the best F1 on Neutral. Besides, our models can attain second or third higher results among other emotions. This demonstrates that our models are competitive to achieve comprehensive performance.\n\nFor MELD, as shown in Tab. 3, BERT outperforms other state-of-the-art models by a great margin, which indicates the importance of external knowledge brought by BERT. Compared with BERT, TRM attains marginally better results, which may be attributed to the limited conversational contextual information in MELD. To confirm this, comparing the results of CNN 3 and scLSTM (based on CNN), we can notice that the improvement is also limited. Although MELD provides limited contextual information in conversations, TRMSM-Att still outperforms BERT by 1.29 wF1 and 1.17 mF1, which indicates the effectiveness of our model to capture such information. For emotions, BERT beats other state-of-the-art models by a great margin in such an imbalanced circumstance, and TRMSM-Att attains the best F1 on 4 emotions including large classes Joy, Anger, Surprise, and the small class Disgust. This demonstrates that BERT can alleviate data imbalance and our model can take advantage of such a feature.\n\nFor both datasets, all TRMSM variants outperform TRM to show the importance of speaker-aware contex-3 CNN achieves 55.02 wF1 on MELD by  Poria et al. (2019b) .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Analysis",
      "text": "Ablation Study To better understand the influences of masks on our models, we report the results of the models removing the Transformer blocks with different masks on IEMOCAP and MELD. In this part, we denote Convention Mask as CM and Intra-Speaker, Inter-Speaker Masks as SM. Accordingly, TRMSM w/o SM is equivalent to TRM.\n\nAs seen in Fig.  3 , on both datasets, TRMSM w/o CM (solely applying SM) can achieve better performance than TRMSM w/o SM (solely applying CM). We attribute it to that speaker modeling does not drop the contextual information from conversations, and on the contrary, speaker modeling can guide the model to extract more effective information to the final prediction. Furthermore, TRMSM outperforms both TRMSM w/o CM and TRMSM w/o SM, which demonstrates that all of our designed masks are critical to achieving better performance.\n\nEffect of Range of Context To find out the influence of the range of context on our model, we train TRMSM-Att with different ranges of available context on IEMOCAP and refer the results from  Ghosal et al. (2019)  for DialogueGCN. We utilize different windows (-x, y) to limit the context, where x, y is respectively the number of utterances in prior context and post context. As illustrated in Fig.  4 , with the window widened, the performance increases as shown in both models. For DialogueGCN, (-10, 10) is the max window of context and therefore it cannot get access to the long-Figure  5 : Heatmaps of attention from fusing method and self-attention of Intra-, Inter-Speaker Blocks for the targeted utterances (whose speakers are marked in yellow). Labels of utterances are tagged below the utterances. Predictions of TRMSM and TRM are marked in green for correctness and red for mistake. distance context. On the contrary, the performance is further improved by TRMSM-Att with all context available. This indicates that the local contextual information is critical for the prediction and the long-distance information is also important for contextual modeling to further improve the performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Number Of Layers",
      "text": "We study the effect of the number of layers to our model on different datasets. Fig.  6  illustrates the radar graphs for the F1 scores of emotions in IEMOCAP and MELD by TRM and TRMSM-Att. As the number of layers increasing, the F1 scores of emotions in IEMOCAP normally expand. While in MELD, increasing the number of layers gradually hurts the performance to be 0 of F1 on emotions Fear and Disgust which are classes with the fewest data. We think the reason may be that MELD suffers from data imbalance and increasing the number of layers leads to severer overfitting on small classes. For data imbalance, methods like re-balance can be applied to alleviate it. Re-balance is out of the scope of this paper and our future work will study data imbalance of ERC.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Case Study",
      "text": "To better understand how our model captures Intra-Speaker and Inter-Speaker dependencies, we illustrate two conversation clips ending with the targeted utterances so that the targeted utterances can only refer to the prior context. We choose TRMSM-Att without Conventional Blocks so that only speaker information related blocks are considered. Specifically, we illustrate heatmaps of attention from fusing method and self-attention 4  in Transformer blocks. For simplicity, we denote attention from fusing method as FAtt.\n\nIn the scene of Fig.  5  (a), the speaker M keeps in frustration through the conversation and F in a neutral state has few influences on M. Therefore, FAtt pays more attention to Intra-Speaker dependency so that Intra-Speaker Blocks can extract information from M himself. We can see from the heatmap that the targeted utterance yeah grades the highest score to the farthest contextual utterance whose emotion is also frustration, which is out of the range of context that Dia-logueGCN can refer. In a sense, this indicates the importance of long-distance information.\n\nAs the condition in Fig.  5  (b), speakers in this conversation basically keep in a neutral state except that Chandler shows other emotions like anger and surprise before the targeted utterance. Although the targeted utterance with speaker Chandler shows slight sadness from the semantic view, it is supposed to be predicted as neutral according to the conversational context. Specifically, FAtt grades Inter-Speaker Blocks with higher score and self-attention in Inter-Speaker Blocks extracts information from the neutral utterances of other speakers. This case indicates the effectiveness of our model to extract inter-speaker information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we simplify the Self and Inter-Speaker dependencies to a binary version. To achieve the simplified modeling of speakers' interactions, we design three masks: Conventional Mask, Intra-Speaker Mask, and Inter-Speaker Mask. These masks are utilized in the self-attention modules of the second-level Transformer blocks of a hierarchical Transformer. As the speaker-aware information extracted by different masks diversely contributes to the prediction, attention mechanism is utilized to weight and fuse them. Finally, our model achieves state-of-the-art results on 2 ERC datasets and further analysis shows that our model is efficacious for ERC.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) illustrates a conversation clip of 3 speakers and",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), Self and Inter-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), for a con-",
      "page": 1
    },
    {
      "caption": "Figure 1: , for the speaker of the targeted utterance, Intra-",
      "page": 2
    },
    {
      "caption": "Figure 2: The structure of our proposed model, which is based on Transformer structure. Our proposed masks are utilized",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). We annotate Conventional Mask as",
      "page": 4
    },
    {
      "caption": "Figure 2: (i), Add equally regards the",
      "page": 4
    },
    {
      "caption": "Figure 3: The results of models with different blocks. wF1",
      "page": 6
    },
    {
      "caption": "Figure 4: wF1 of TRMSM-Att and DialogueGCN with dif-",
      "page": 6
    },
    {
      "caption": "Figure 3: , on both datasets, TRMSM w/o CM",
      "page": 6
    },
    {
      "caption": "Figure 4: , with the",
      "page": 6
    },
    {
      "caption": "Figure 5: Heatmaps of attention from fusing method and self-attention of Intra-, Inter-Speaker Blocks for the targeted utterances",
      "page": 7
    },
    {
      "caption": "Figure 6: The F1 score on every emotion class by TRMSM-",
      "page": 7
    },
    {
      "caption": "Figure 6: illustrates the radar graphs for the F1 scores of emotions",
      "page": 7
    },
    {
      "caption": "Figure 5: (a), the speaker M keeps in frus-",
      "page": 7
    },
    {
      "caption": "Figure 5: (b), speakers in this conver-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The results of our models on IEMOCAP. Weighted-F1 score (wF1) is used as the metric. † means referring from",
      "data": [
        {
          "Memnet(Sukhbaatar et al. 2015)†\nCMN(Hazarika et al. 2018b)†\nDialogueRNN(Majumder et al. 2019)\nKET(Zhong, Wang, and Miao 2019)\nAGHMN(Jiao, Lyu, and King 2020)\nDialogueGCN(Ghosal et al. 2019)": "DialogueGCN(80:20)\nBERT(Devlin et al. 2019)",
          "25.72\n33.53\n25.00\n30.38\n25.69\n33.18\n-\n-\n52.10\n48.30\n40.62\n42.75": "52.83\n42.47\n42.19\n39.05",
          "55.53\n61.77\n55.92\n62.41\n78.80\n75.10\n-\n-\n68.30\n73.30\n89.14\n84.54": "79.43\n77.26\n60.45\n59.91",
          "58.12\n52.84\n52.86\n52.39\n58.59\n59.21\n-\n-\n61.60\n58.40\n61.92\n63.54": "60.93\n58.48\n49.11\n52.24",
          "59.32\n55.39\n61.76\n59.83\n65.28\n64.71\n-\n-\n57.50\n61.90\n67.53\n64.19": "61.89\n57.82\n55.14\n54.82",
          "51.50\n58.30\n55.52\n60.25\n80.27\n71.86\n-\n-\n68.10\n69.70\n65.46\n63.08": "74.91\n66.85\n64.22\n55.97",
          "67.20\n59.00\n71.13\n60.69\n61.15\n58.91\n-\n-\n67.10\n62.30\n64.18\n66.99": "56.28\n56.82\n54.26\n55.88",
          "55.72\n55.10\n56.56\n56.13\n63.40\n62.75\n-\n59.56\n63.50\n63.50\n65.25\n64.18": "63.23\n62.46\n54.06\n54.01"
        },
        {
          "Memnet(Sukhbaatar et al. 2015)†\nCMN(Hazarika et al. 2018b)†\nDialogueRNN(Majumder et al. 2019)\nKET(Zhong, Wang, and Miao 2019)\nAGHMN(Jiao, Lyu, and King 2020)\nDialogueGCN(Ghosal et al. 2019)": "TRM\nTRMSM-Add\nTRMSM-Cat\nTRMSM-Att",
          "25.72\n33.53\n25.00\n30.38\n25.69\n33.18\n-\n-\n52.10\n48.30\n40.62\n42.75": "43.08\n42.42\n43.53\n48.53\n48.71\n49.46\n43.36\n50.22",
          "55.53\n61.77\n55.92\n62.41\n78.80\n75.10\n-\n-\n68.30\n73.30\n89.14\n84.54": "77.27\n74.54\n76.15\n76.74\n76.84\n77.02\n81.23\n75.82",
          "58.12\n52.84\n52.86\n52.39\n58.59\n59.21\n-\n-\n61.60\n58.40\n61.92\n63.54": "58.24\n60.88\n64.37\n66.06\n64.44\n63.00\n66.11\n64.15",
          "59.32\n55.39\n61.76\n59.83\n65.28\n64.71\n-\n-\n57.50\n61.90\n67.53\n64.19": "65.00\n60.22\n56.24\n60.6\n57.03\n60.72\n60.39\n60.97",
          "51.50\n58.30\n55.52\n60.25\n80.27\n71.86\n-\n-\n68.10\n69.70\n65.46\n63.08": "68.37\n66.98\n75.72\n68.49\n76.07\n70.33\n77.46\n72.70",
          "67.20\n59.00\n71.13\n60.69\n61.15\n58.91\n-\n-\n67.10\n62.30\n64.18\n66.99": "61.48\n59.84\n63.23\n61.18\n60.74\n62.09\n63.45\n62.16",
          "55.72\n55.10\n56.56\n56.13\n63.40\n62.75\n-\n59.56\n63.50\n63.50\n65.25\n64.18": "62.25\n62.11\n64.21\n64.45\n64.72\n64.82\n65.74\n65.34"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scLSTM(Poria et al. 2017)♦\nDialogueRNN(Majumder et al. 2019)♦\nAGHMN(Jiao, Lyu, and King 2020)\nKET(Zhong, Wang, and Miao 2019)\nDialogueGCN(Ghosal et al. 2019)": "BERT(Devlin et al. 2019)",
          "78.40\n73.80\n72.10\n73.5\n83.40\n76.40\n-\n-\n-\n-": "74.85\n76.57",
          "46.80\n47.70\n54.40\n49.40\n49.10\n49.70\n-\n-\n-\n-": "53.26\n56.25",
          "3.80\n5.40\n1.60\n1.20\n9.20\n11.50\n-\n-\n-\n-": "21.94\n21.89",
          "22.40\n25.10\n23.90\n23.80\n21.60\n27.00\n-\n-\n-\n-": "35.33\n33.01",
          "51.60\n51.30\n52.00\n50.70\n52.40\n52.40\n-\n-\n-\n-": "53.82\n57.02",
          "4.30\n5.20\n1.50\n1.70\n12.20\n14.00\n-\n-\n-\n-": "36.64\n27.67",
          "36.70\n38.40\n41.90\n41.50\n34.90\n39.40\n-\n-\n-\n-": "50.70\n42.42",
          "57.50\n55.90\n46.40\n56.10\n55.90\n45.30\n60.30\n58.10\n49.45\n-\n58.18\n-\n-\n58.10\n-": "61.82\n61.07\n53.40"
        },
        {
          "scLSTM(Poria et al. 2017)♦\nDialogueRNN(Majumder et al. 2019)♦\nAGHMN(Jiao, Lyu, and King 2020)\nKET(Zhong, Wang, and Miao 2019)\nDialogueGCN(Ghosal et al. 2019)": "TRM\nTRMSM-Add\nTRMSM-Cat\nTRMSM-Att",
          "78.40\n73.80\n72.10\n73.5\n83.40\n76.40\n-\n-\n-\n-": "76.57\n76.62\n77.71\n75.88\n75.69\n77.26\n75.48\n77.56",
          "46.80\n47.70\n54.40\n49.40\n49.10\n49.70\n-\n-\n-\n-": "55.53\n55.67\n53.40\n56.49\n52.64\n56.33\n57.25\n55.90",
          "3.80\n5.40\n1.60\n1.20\n9.20\n11.50\n-\n-\n-\n-": "23.8\n23.9\n24.67\n21.14\n25.66\n22.73\n25.91\n20.38",
          "22.40\n25.10\n23.90\n23.80\n21.60\n27.00\n-\n-\n-\n-": "36.43\n32.34\n36.39\n31.56\n34.37\n38.41\n36.82\n32.9",
          "51.60\n51.30\n52.00\n50.70\n52.40\n52.40\n-\n-\n-\n-": "52.07\n57.46\n53.97\n57.83\n57.81\n58.07\n58.66\n55.55",
          "4.30\n5.20\n1.50\n1.70\n12.20\n14.00\n-\n-\n-\n-": "29.46\n25.13\n35.07\n22.62\n35.61\n22.57\n28.63\n38.31",
          "36.70\n38.40\n41.90\n41.50\n34.90\n39.40\n-\n-\n-\n-": "50.68\n44.64\n52.36\n45.95\n48.34\n45.90\n45.95\n52.11",
          "57.50\n55.90\n46.40\n56.10\n55.90\n45.30\n60.30\n58.10\n49.45\n-\n58.18\n-\n-\n58.10\n-": "61.80\n61.30\n53.45\n62.93\n62.06\n53.96\n62.76\n62.01\n54.04\n62.36\n54.57\n63.23"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "2",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. of NAACL-HLT"
    },
    {
      "citation_id": "3",
      "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
      "authors": [
        "L Dong",
        "N Yang",
        "W Wang",
        "F Wei",
        "X Liu",
        "Y Wang",
        "J Gao",
        "M Zhou",
        "H Hon"
      ],
      "year": "2019",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "4",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP-IJCNLP"
    },
    {
      "citation_id": "5",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "6",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proc. of NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "Long Short-Term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "8",
      "title": "Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "9",
      "title": "Hi-GRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Proc. of NAACL-HLT"
    },
    {
      "citation_id": "10",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical Transformer Network for Utterance-level Emotion Recognition",
      "authors": [
        "Q Li",
        "C Wu",
        "K Zheng",
        "Z Wang"
      ],
      "year": "2020",
      "venue": "Hierarchical Transformer Network for Utterance-level Emotion Recognition",
      "arxiv": "arXiv:2002.07551"
    },
    {
      "citation_id": "12",
      "title": "Conversational Emotion Analysis via Attention Mechanisms",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "14",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "15",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency",
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2017",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "18",
      "title": "Modeling Relational Data with Graph Convolutional Networks",
      "authors": [
        "M Schlichtkrull",
        "T Kipf",
        "P Bloem",
        "R Van Den Berg",
        "I Titov",
        "M Welling"
      ],
      "year": "2018",
      "venue": "Proc. of ESWC"
    },
    {
      "citation_id": "19",
      "title": "End-To-End Memory Networks",
      "authors": [
        "S Sukhbaatar",
        "A Szlam",
        "J Weston",
        "R Fergus"
      ],
      "year": "2015",
      "venue": "Proc. of NeuIPS"
    },
    {
      "citation_id": "20",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. of NeuIPS"
    },
    {
      "citation_id": "21",
      "title": "Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge",
      "authors": [
        "B Zhang",
        "M Yang",
        "X Li",
        "Y Ye",
        "X Xu",
        "K Dai"
      ],
      "year": "2020",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "22",
      "title": "Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "Proc. of IJCAI"
    },
    {
      "citation_id": "23",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP-IJCNLP"
    },
    {
      "citation_id": "24",
      "title": "Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer",
      "authors": [
        "H Zhu",
        "F Nan",
        "Z Wang",
        "R Nallapati",
        "B Xiang"
      ],
      "year": "2020",
      "venue": "Proc. of AAAI"
    }
  ]
}