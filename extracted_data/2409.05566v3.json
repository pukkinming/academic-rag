{
  "paper_id": "2409.05566v3",
  "title": "Leveraging Content And Acoustic Representations For Speech Emotion Recognition",
  "published": "2024-09-09T12:46:32Z",
  "authors": [
    "Soumya Dutta",
    "Sriram Ganapathy"
  ],
  "keywords": [
    "Speech-text alignment",
    "representation learning",
    "self-supervised learning",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER), the task of identifying the expression of emotion from spoken content, is challenging due to the difficulty in extracting representations that capture emotional attributes. The scarcity of labeled datasets further complicates the challenge where large models are prone to over-fitting. In this paper, we propose CARE (Content and Acoustic Representations of Emotions), where we design a dual encoding scheme which emphasizes semantic and acoustic factors of speech. While the semantic encoder is trained using distillation from utterance-level text representations, the acoustic encoder is trained to predict low-level frame-wise features of the speech signal. The proposed dual encoding scheme is a base-sized model trained only on unsupervised raw speech. With a simple lightweight classification model trained on the downstream task, we show that the CARE embeddings provide effective emotion recognition on a variety of datasets. We compare the proposal with several other self-supervised models as well as recent largelanguage model based approaches. In these evaluations, the proposed CARE is shown to be the best performing model based on average performance across 8 diverse datasets. We also conduct several ablation studies to analyze the importance of various design choices.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH Emotion Recognition (SER) focuses on detecting the speaker's emotional state from the audio signal. Recognizing emotions in speech has significant applications across diverse fields, including human-computer interaction  [1] , social media analysis  [2] , customer service call centers  [3] , and mental health monitoring systems  [4] . However, despite considerable progress, SER continues to pose challenges due to the complexity of human emotions and the inherent difficulties in effectively capturing them from limited labeled datasets.\n\nTraditionally, SER systems have relied on various acoustic properties of speech signals. Lieberman et al.  [5]  emphasize the role of pitch contour in emotion analysis, while additional acoustic features, including energy, intensity, and speaking rate, were recognized as indicators of emotional class  [6] . The features identified through the Interspeech para-linguistic challenges were rich in emotional properties while being highdimensional  [7] ,  [8] . Eyben et al.  [9]  introduced a minimalist feature set to address this dimensionality issue.\n\nIn recent years, the network architectures in SER commonly include convolutional neural networks (CNN)  [10] ,  [11] , long short-term memory (LSTM) networks  [12] , and transformer models  [13] . While these models perform well on the specific datasets, they often struggle to generalize across diverse datasets. In such settings, self-supervised learning (SSL) models have emerged as a promising solution. Notable examples of SSL approaches include wav2vec 2.0  [14] , HuBERT  [15] , and WavLM  [16] . These models are engineered to capture speech patterns similar to textual models like BERT  [17] . Although trained on neutral speech data, these models have demonstrated encouraging results in emotion recognition tasks  [18] ,  [19] . The emotion recognition performance may be further enhanced by training these models with emotion-aware selfsupervised objectives. Two recent examples are Vesper  [20]  and emotion2vec  [21] . However, emotion in speech is also shaped by its semantic content  [22] . For instance, identifying emotions from text transcripts is often more effective than interpreting them from raw audio  [19] . The integration of speech content during the pre-training phase of SER models remains an under-explored yet promising area of research.\n\nIn this work, we introduce a self-supervised model for speech emotion recognition (SER) called Content and Acoustic Representations of Emotions (CARE). To the best of our knowledge, our approach is the first effort to pretrain a self-supervised model that integrates both semantic and acoustic components of speech. CARE leverages a dual encoding framework for processing speech signals: a semantic encoder, which aligns speech representations with sentencelevel transcripts, and a non-semantic encoder, which aligns speech representations with low-level acoustic features from the PASE+ model  [23] . The outputs of both encoders are combined, and a lightweight classification head is then trained to perform emotion recognition. The key contributions are :-\n\n• Proposing a novel self-supervised model for speech emotion recognition (SER) consisting of dual encoders: a semantic encoder and an acoustic encoder. • Developing an adaptation strategy for aligning pre-trained text models with speech inputs by convolutional adapters. • Experimenting on 8 benchmark speech datasets with diverse tasks, showcasing the effectiveness of CARE. • Identifying the individual and collective impact of semantic and acoustic representations for emotion recognition.\n\ninclude the SincNet architecture by Ravanelli et al.  [24]  and interpretable Gaussian filters by Agrawal et al.  [25] . The LEAF front-end  [26] , was utilized by Dutta et al.  [11]  for speech emotion classification. Typically, these models require endto-end training of both feature extractors and classifiers. In contrast, the proposed CARE architecture is a self-supervised model designed to generalize across diverse datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Self-Supervision For Ser",
      "text": "One of the earliest self-supervised model for the task of speech emotion recognition was proposed by Pascual et al.  [27] . This consisted of processing a speech signal by the SincNet model  [24]  followed by trainable convolutional blocks to predict a number of speech features such as the waveform, mel-frequency cepstral coefficients (MFCCs), pitch etc. Ravanelli et al.  [23]  further modified this model by adding more self-supervised tasks such as predicting FBANK and Gammatone features  [28]  to develop the PASE+ model.\n\nAmong the general purpose speech SSL models that were proposed over the years, WavLM  [16] , was shown to outperform other models such as HuBERT  [15]  and wav2vec2.0  [14]  for emotion recognition. Vesper  [20]  used a modified masking strategy to emphasize high pitch/energy regions of speech-known indicators of emotion-and derived targets for these masked regions from a WavLM teacher model. A similar strategy was employed by Ma et al. in emo-tion2vec  [21] , which utilized a pre-trained data2vec model as the teacher. Emotion2vec  [21]  also learns a global embedding to enhance SER performance. In contrast, the proposed CARE model integrates semantic content along with acoustic features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Multimodal Emotion Recognition",
      "text": "The use of speech signals alongside text transcripts for multimodal emotion recognition has been explored in several prior works  [11] ,  [29] ,  [30] . These approaches typically involve separate modeling of the two modalities, followed by a fusion stage. In contrast, CARE is designed to model the semantic and acoustic properties of speech with the uni-modal input.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Speech-Text Aligned Representations",
      "text": "The alignment of speech and text modalities has received renewed attention for speech representation learning. The SONAR model  [31]  aligns a speech encoder with textual representations at the utterance level. With the increasing prominence of large language models (LLMs), recent approaches have integrated speech encoders with LLMs. Notably, the SALMONN model by Tang et al.  [32]  introduced an audio encoder consisting of Whisper model and a music encoder along with the LLaMA language model  [33] . Hu et al.  [34]  proposed WavLLM, combining Whisper and WavLM encoders with the LLaMA model. These LLM-based approaches harness aligned speech-text representations, enabling prompt-based applications. However, their substantial model sizes (e.g., 7B parameters for SALMONN) present significant computational demands for both training and inference. In contrast, CARE achieves superior performance on various downstream datasets . with a much smaller size of 160M parameters. Summary: The landscape of various SER methods is summarized in Fig.  1 . We highlight a clear gap in current modeling frameworks: models either prioritize efficiency with limited performance (those in the lower end of the x-axis), or focus on maximizing performance with increased memory and compute requirements (typically based on LLMs). To address this gap, we propose CARE, that combines the computational efficiency of smaller models with the high performance of large-scale systems, thereby providing a superior trade-off between efficiency and performance.\n\nIII. PROPOSED APPROACH A. Background 1) RoBERTa: One of the significant contributions in creating a text representation model was proposed by Devlin et al.  [17] . Liu et. al  [35]  trained this architecture on a larger corpus of textual data without the next sentence prediction task. This pre-trained model, known as robust optimized BERT approach (RoBERTa), was shown to outperform BERT in a number of downstream tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Care Model",
      "text": "We propose a dual encoding scheme (semantic and acoustic encoders) to process the speech signal through distinct supervisory signals suited to their respective objectives. The chosen supervision for each encoder is detailed as follows: Semantic supervision: We do not assume the availability of ground-truth text transcripts for the pre-training data. In such a scenario, pre-trained automatic speech recognition (ASR) systems (Whisper-large-v3  [36] ) offer an alternative for generating these transcripts. Typically, ASR systems have been shown to exhibit higher word error rates (WER) on emotional speech compared to neutral speech datasets  [22] . Podcast recordings, on the other hand, provide sufficiently long context and offer a broad content variety suitable for pre-training the semantic encoder. Specifically, we observe a WER of 12.53%, which may be reasonable for SER tasks.\n\nSince the semantic encoder's purpose is to align the speech signal with its content to facilitate emotion recognition, an ASR-style alignment loss could be applied. However, a sentence-level representation for text is more appropriate for the task of emotion recognition as established by Fan et al.  [37] . Therefore, we extract contextual word-level embeddings from the transcripts using a pre-trained RoBERTa model  [35]  and mean-pool these embeddings to obtain a single feature vector representing the entire transcript. These utterance-level embeddings serve as the supervisory signal, or \"teacher\", for the semantic encoder in our CARE model. We denote these utterance-level embeddings by y text . Acoustic Supervision: In prior works, mean-pooled representations have shown to encode characteristics like speaker identity, accent, and language  [38] . However, we speculate that emotion in speech is often contained in fine-grained acoustic attributes such as pitch, rhythm, and their modulations  [6] . Thus, a frame-level target is chosen for the acoustic encoder.\n\nA direct approach for the frame level acoustic targets would involve masking parts of the speech signal and reconstructing them. However, prior works show that random masking is less effective for emotion recognition than selectively masking high-energy or high-pitch regions, as demonstrated by Chen et al.  [20] . Based on these observations, we choose to predict PASE+ features, which encompass filter-bank energies, pitch, and other low-level descriptors essential for capturing emotion. Specifically, we use frame-level PASE+ features with 256 dimensions as targets for the acoustic encoder in our CARE model. These features are down-sampled by a factor of 2, producing target descriptors at a frequency of 50 Hz. We denote the acoustic targets from the PASE+ model by y pase .\n\n1) Model Architecture: The speech signal is first processed through a series of convolutional layers designed to produce frame representations every 20 ms. These are followed by a stack of six transformer layers, forming the common encoder that serves both the acoustic and semantic encoder pathways in the proposed model.\n\nThe semantic encoder is designed to align the speech representations with its corresponding generated transcript. This encoder consists of six transformer layers which are initialized with the weights from a pre-trained text representation model. Being trained with textual data, the transformer layers in the semantic encoder do not generalize to speech representations. To address this, we propose a novel adaptation strategy by introducing two 1D-convolutional blocks-one placed before and one after each transformer layer.\n\nThe first block adjusts the speech representations from the common encoder to align them with the internal representations expected by the text-based model. The second block refines these representations post-transformer processing. Additionally, following established practice for processing speech in text models  [32] ,  [39] ,  [40] , the time resolution of the speech sequence is reduced before processing by the transformer layers in the semantic encoder. Specifically, the convolutional block preceding each transformer layer downsamples the sequence length by a factor of three, while the block following it up-samples it by the same factor. Each convolutional block consists of a single convolutional layer, with a kernel size of 5, and input and output channels set to 768 in order to match the dimension of the pre-trained transformer layers. While adaptation of speech SSL models with convolution layers has been explored in prior works  [41] ,  [42] , adapting pre-trained text models for speech tasks, using convolutional adapters, is explored for the first time in this work. Importantly, the transformer layers themselves are not updated during training. Finally, the semantic encoder's output representations are average-pooled to produce an utterancelevel representation.\n\nThe acoustic encoder also consists of six transformer layers, with its output subsequently mapped to 256 dimensions, using a fully-connected layer, to match the PASE+ feature targets. Figure  2  provides a block diagram of the CARE model.\n\n2) Loss: A semantic loss, L sem and a frame-level acoustic loss, L acoust , are employed for training the semantic and acoustic encoders, respectively. Denoting the semantic supervision by y text and the output from the semantic encoder as ŷsem , the semantic loss is the mean square error (MSE) loss:\n\nwhere N denotes the batch size.\n\nFor the frame level loss, let ŷacoust ∈ R N ×T ×D denote the output of the acoustic encoder, where N , T and D denote the batch size, number of frames per utterance and the dimension of the representation, respectively. The loss is defined as:\n\nwhere y pase denotes the acoustic target. The total loss during pre-training is given by\n\nwhere λ is decided based on the validation performance.\n\n3) Inference: For evaluating the model across various downstream tasks, we adopt the paradigm proposed in the SUPERB benchmark  [43] . The outputs from each transformer layer of the acoustic encoder are concatenated with the outputs from the convolution block following each transformer layer in the semantic encoder. These are then combined with layerwise outputs from the common encoder and the convolutional feature extractor. This process yields a total of 13 layer representations-one from the convolutional feature extractor, six from the common encoder, and six from the concatenated semantic and acoustic encoders. A convex combination of these layer representations is then fed into a classification head. It is to be noted that, during inference, the fullyconnected layer in the acoustic encoder and the average pooling block in the semantic encoder are not used.\n\nIn this setup, the only learnable parameters for the downstream tasks are the weights for the convex combination and those of the lightweight classification head.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Pre-Training",
      "text": "The MSP-PODCAST corpus  [44]  is used for the task of pre-training. A total of 149, 307 samples amounting to 230 hours of emotional speech data are used. Out of these, 80% of the data is randomly chosen as the training set while the remaining 20% serves as the validation set. The Whisperlarge-v3 model is used for generating the transcripts (the WER observed is 12.53%), while the pre-trained RoBERTa model is used for encoding the transcripts. The common encoder is initialized with first 6 layers of the WavLM-base model, while the acoustic encoder is initialized with the last 6 layers of the same. The convolutional feature extractor is also initialized from the WavLM-base model. The 6 transformer layers of the semantic encoder are initialized with the weights of the last 6 layers of a pre-trained RoBERTa base model, while the convolutional adapters are randomly initialized.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Downstream Tasks",
      "text": "A summary of the different datasets used for evaluation is mentioned in Table  I .\n\n1) IEMOCAP: The IEMOCAP dataset consists of 151 video recordings split into 5 sessions. Each of these sessions is a conversation between a pair of subjects. Each recording is split into multiple utterances. There are a total of 10, 039 utterances, each of which is labeled by human annotators as belonging to one of the 10 emotions -\"angry\", \"happy\", \"sad\", \"neutral\", \"frustrated\", \"excited\", \"fearful\", \"surprised\", \"disgusted\" or \"other\". Keeping in line with previous works, we do a four-way classification task where we consider \"angry\", \"happy\", \"sad\", \"neutral\" and \"excited\" categories (with \"excited\" and \"happy\" categories merged). We also have a separate setting of 6 emotional classes  [52] . The first 6 of the 10 emotion classes are considered for this setting.\n\n2) MELD: The MELD dataset  [46]  is a dataset created from video clippings of the popular TV show, \"Friends\". A seven way classification task is performed on this dataset, with each utterance being labeled as one of the 7 emotions -\"angry\", \"sad\", \"joy\", \"neutral\", \"fear\", \"surprise\" or \"disgust\".\n\n3) CMU-MOSI: The CMU-MOSI dataset  [47]  has a total of 2199 utterances. Each utterance is labeled in the range of  [-3, 3] . Following previous works, we treat this as a binary classification problem with utterances having sentiment values in the range [-3, 0) being classified as negative sentiment and those with values in the range [0, 3] considered as positive sentiment. The dataset partitioning follows a prior work  [53] .\n\n4) DAIC-WOZ: The DAIC-WOZ dataset  [48]  is a benchmark dataset for depression detection, consisting of 189 clinical interviews between the patient and the interviewer. Out of these 189 interviews, 107 are part of the training set while 35 interviews are part of the development subset. The dataset suffers from a data imbalance problem, with only 30 interviews labeled as \"depressed\" in the train set . In order to increase the balance, we follow  [54]  and extract 100 utterances randomly from each interview, which is labeled as depressed, while only 39 utterances are selected for interviews classified as \"normal\". The utterances from each interview are chosen randomly for the 5 splits. Following prior work  [54] -  [57] , we report results on the development set of this dataset.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "5) Ravdess-Song:",
      "text": "The RAVDESS-Song dataset  [49]  has a total of 1012 song recordings by 23 different singers. Each recording in this dataset is sung in one of six different emotions, namely, \"neutral\", \"calm\", \"happy\", \"sad\", \"angry\" and \"fear\". We conduct a speaker independent evaluation for this dataset, and create 5 different splits. For each split, we keep recordings from 16 singers for training, while recordings from 3 separate singers are used for validation. The recordings from the remaining 4 speakers are used for evaluation.\n\n6) CaFE: The CaFE dataset  [50]  is a Canadian French emotional dataset consisting of 936 utterances spoken by 12 speakers. Each utterance in this dataset is categorized as one of the seven emotions -\"neutral\", \"angry\", \"disgust\", \"sad\", \"surprise\", \"fear\" and \"happy\". Similar to RAVDESS-Song, we create 5 speaker independent splits for this dataset. The utterances belonging to 8 speakers are used for the training, while the remaining 4 speakers are used for validation and testing equally. The speakers used for train, validation and test are chosen randomly for the 5 splits.\n\n7) EmoDB: The EmoDB dataset  [51]  has a total of 535 utterances spoken by 10 different speakers for the task of emotion recognition in German. Each utterance in this dataset is categorized as one of the seven emotions -\"neutral\", \"angry\", \"disgust\", \"sad\", \"boredom\", \"fear\" and \"joy\". Similar to RAVDESS-Song and CaFE, we create 5 different speaker independent splits for this dataset. The utterances belonging to 6 different speakers are chosen for training while the remaining 4 speakers are used for validation and testing equally. The speakers used for train, validation and testing are chosen randomly for each of the 5 splits.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Loss And Evaluation Metrics For Downstream Tasks",
      "text": "The cross-entropy loss is used for training the downstream model weights (the convex combination weights and the lightweight classification head parameters). For testing, we use the weighted F1-score as the evaluation metric as many of the datasets are class-imbalanced (Table  I ). Denoting the F1 score of class c with N c samples, by F 1 c , the weighted F1-score is\n\nWe also report the unweighted average recall (UAR) for all cases, which is the mean of the class-wise recall scores.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Implementation Details",
      "text": "1) Pre-training: During pre-training, all the speech utterances from MSP-PODCAST are padded or randomly cropped to a duration of 5 seconds. The model is trained with a learning rate of 1e-5 and a batch size of 128 with AdamW  [58]  as the optimizer. The model is trained for a total of 200, 000 steps and the best model parameters based on validation set performance are chosen for evaluation of downstream datasets.\n\nWe experiment with different values of λ (Eq. 3) to balance the two losses during pre-training. Setting λ = 0.1 results in degraded performance, while increasing it to λ = 10 does not yield any significant improvement over λ = 1. Therefore, we fix λ = 1 for all the subsequent experiments.\n\n2) Fine-tuning and evaluation: For the downstream task training, the speech signals are cropped to a maximum duration of 30 seconds or padded to a minimum duration of 1 second. For the depression detection dataset, DAIC-WOZ, each speech segment has a duration of 10 seconds  [54] .\n\nEach layer output in the common, semantic, and acoustic encoders has a dimensionality of T × 768, where T denotes the number of frames in the speech signal, sampled at 50Hz. For the CARE model, as outputs from the 6 semantic and acoustic encoder layers are concatenated, the combined output dimension is 6 × T × 1536. To align with this dimensionality, the output from the convolutional feature extractor and the common encoder's 6 layers are duplicated to yield features of dimension 7 × T × 1536. Representations from these 13 layers are combined through a convex combination approach with learnable weights producing features of dimension T × 1536. Following this, features are mean-pooled along the temporal dimension, producing a single 1536-dimensional vector per audio file. This is input into a classification head consisting of a two-layer feed-forward neural network that employs ReLU activation  [59] . Only the weights for the convex combination of layer representations and those in the two-layer fully connected classification head are trained on each downstream dataset, consistent with the SUPERB framework  [43] .\n\nWe use a batch size of 32 with a learning rate of 1e-4 and train the model for 50 epochs. The hidden dimension of the two-layer classification head is set to be 256. The AdamW optimizer is used here as well. All the models, including the CARE and the baseline systems, utilize the same classification backend. Thus, the design allows fair comparison of the different representations.  1",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Performance Of Care",
      "text": "The results on the 8 downstream datasets using representations from the proposed CARE model are shown in Table  II . These baseline models are categorized into two groups based on the number of parameters used during inference: base models (parameter size < 200M), and large models (> 500M), which also include LLM based models. The following observations are made for each category:\n\n1) Base models: We compare HuBERT  [15] , WavLM  [34] , data2vec  [60]  and emotion2vec  [21]  representations as the baseline models in this category. Among these baseline systems, the emotion2vec is also pre-trained on IEMOCAP and MELD datasets, partially explaining the improved results seen on the downstream tasks on these datasets. While CARE performs similar to emotion2vec on CMU-MOSI, it improves over all the base-sized models on other datasets. On the average, the proposed CARE achieves a relative improvement of 15.6% over the best baseline model (HuBERT). 2) Large models: SONAR  [31]  is selected as the speech encoder in this category. For the six English-based datasets, the pre-trained English speech encoder 2 is used, while the French and German speech encoders are utilized for the CaFE and EmoDB datasets, respectively. Similar to the CARE backend, the layer representations from the SONAR encoder are linearly combined and the classification head is trained on the downstream task. Although SONAR has nearly four times the parameter size of CARE, our proposed model outperforms SONAR across all datasets except CMU-MOSI.\n\n3) LLM based models: Two versions of SALMONN  [32]  (7B and 13B) 3 are considered as examples of LLM-based models. These are typically applied in a zero-shot setting; however, due to variability in emotion classes across datasets, their zero-shot performance is inconsistent. E.g. while SALMONN-13B model achieves 68.75% weighted F1-score on the IEMOCAP-4 dataset (on which it is trained), it achieves only 24.06% for MELD. Thus, for fair comparison, the same framework used in CARE and other baseline models is followed for the LLM based evaluations as well. The internal representations from all layers (41 layers for SALMONN 13B and 33 layers for SALMONN 7B) are aggregated using a convex combination, and the classification head (similar to CARE) is trained for each downstream dataset. Similar to emotion2vec, SALMONN includes IEMOCAP in its pre-training, leading to superior performance on IEMOCAP-4 and IEMOCAP-6 compared to CARE. The larger model size and extensive pretraining data allows SALMONN to outperform CARE by 10% and 34% (relative improvements) on the MELD and CMU-MOSI datasets, respectively. However, on the remaining four tasks, CARE surpasses the SALMONN models, achieving relative improvements of 17% and 24% on the RAVDESSsong and CaFE datasets, respectively. Notably, though music datasets are used to pre-train SALMONN, CARE emerges as the best model on the RAVDESS-Song dataset. Key takeaways: 1) On average, CARE emerges as the topperforming model across the eight datasets, surpassing even the SALMONN 13B model, which has nearly 80 times more parameters. Although LLM-based models show strengths in 2 https://dl.fbaipublicfiles.com/SONAR/spenc.eng.pt 3 https://huggingface.co/tsinghua-ee/SALMONN in-domain emotion recognition datasets, their performance declines on out-of-domain tasks, indicating limited generalizability across diverse tasks and multilingual emotional speech. 2) CARE's advantage over speech SSL models like WavLM, HuBERT, and data2vec is expected, given that these models are trained on non-emotional data (see Sec. V-G for a related experiment). 3) Notably, CARE outperforms the multilingual SONAR model on CaFE and EmoDB datasets although it is trained on English speech only. This showcases the generalizability of our pre-training technique to out-ofdomain tasks in SER.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Emotional Attribute Prediction",
      "text": "The emotion recognition can be posed as a regression problem, where valence, arousal and dominance of a particular utterance are predicted  [61] . We use the MSP-IMPROV  [62]  for this purpose. This is an audio-visual dataset that consists of 12 actors eliciting a set of sentences in different emotions. The dataset consists of 8438 utterances with valence, arousal and dominance values (ranging from 1 to 5). We split the dataset in 12 parts, where each part contains utterances corresponding to 10 training speakers, while speech from the two other speakers are used for validating and testing the model. The performance is measured as the average over these 12 parts.\n\nWe use the concordance correlation coefficient (CCC) as the metric. Denoting the mean, variance of ground truth by µ g , σ 2\n\ng and predicted scores by µ p , σ 2 p , the CCC is defined as\n\nIn Eq. 5, ρ is the Pearson's correlation coefficient between the ground truth and the predicted scores. For training the downstream model, the representations from CARE and other models are aggregated similar to the categorical datasets. This is followed by a two-layer regression head with 256 as the hidden dimension and 3 as the output dimension (1 for each of the three attributes). The objective is to increase the CCC between the ground truth and the predicted values for each of the dimensions of valence, arousal and dominance. The results for this dataset along with other baseline models are shown in Table .III. We note that for this task, the CARE embeddings achieve the best results in terms of the valence and dominance attributes, while the performance on arousal is marginally better for the SALMONN-7B model.\n\nV. DISCUSSION",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison With Baselines",
      "text": "Four baseline systems (Table  IV ) are considered:-PASE+: For each downstream dataset, PASE+ features are extracted and a classification network is trained to predict the emotion class of each utterance similar to CARE. The total number of parameters used during inference is 8M. Whisper: For each downstream dataset, the representations from the 33 encoder layers of the Whisper-large-v3 model  [36]  are linearly combined with learnable weights. A two-layer classification head is trained on top of these representations for the task of emotion recognition. The total number of parameters used during inference is 800M. Whisper+RoBERTa: The transcripts are generated using the Whisper-large-v3 model and subsequently processed by a pretrained RoBERTa model. The internal representations from RoBERTa are linearly combined by learnable weights, followed by training a two-layer classification head. This has a total of 1.6B parameters in use during inference. Teacher-fusion: The PASE+ and Whisper+RoBERTa representations are concatenated and a two-layer classification head is trained for each downstream dataset. This baseline also has a total of 1.6B parameters during inference. Key takeaways: 1) The performance of CARE surpasses that of the acoustic supervisory signal by 29.76% (relative) on average across the 8 datasets. This improvement can be attributed to the larger parameter size of CARE compared to the PASE+ model. 2) CARE is seen to outperform Whisper and Whisper+RoBERTa systems by 41.79% and 41.18% in relative terms. This indicates that, although the Whisperbased baselines are much larger in size, the combination of the acoustic and semantic information in CARE results in effective emotion recognition. 3) On MELD and CMU-MOSI, CARE is outperformed by the Whisper+RoBERTa baseline. For these datasets, text-based models are known to significantly outperform speech-only systems  [19] ,  [64] . In the Whisper+RoBERTa setup, the RoBERTa model is fine-tuned on transcripts generated by Whisper-large-v3 (1.6B sized model). In contrast, CARE is a smaller model (160M), and does not use directly use the ASR transcripts during inference.\n\nTo further elucidate the fairness in model-size, we replace Whisper-large-v3 with a Whisper-base model for the ASR, followed by the RoBERTa modeling. Then, the performance drops from 49.29% to 46.02% on MELD and from 75.14% to 71.91% on CMU-MOSI. This underscores the importance of accurate transcriptions and large model capacity in settings where the textual information is emotion rich. 4) While the teacher-fusion baseline is competitive for a number of datasets involving English speech, CARE outperforms this baseline on average by 5.24% absolute. This also motivates why CARE was pre-trained using knowledge distillation as it outperforms the fusion baseline with only 10% of the parameters.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Importance Of The Two Encoders",
      "text": "We present the performance of CARE when we use only one of acoustic and semantic encoders along with the common encoder for the downstream datasets in Table  IV . For evaluating the combination of the semantic and common encoders, we use the 768-dimensional representations from the convolutional feature extractor, the common encoder, and the semantic encoder, excluding outputs from the acoustic encoder. Similarly, the semantic encoder representations are disregarded during the evaluation of the acoustic-common encoder combination. Note that, while CARE has more number of parameters (160M) as compared to models like WavLM or emotion2vec, both these combinations have similar number of parameters during inference. While the semantic-common encoder combination has an inference time parameter size of 110M, the acoustic-common encoder has a total of 94M parameters during evaluation on each downstream dataset. Key takeaways: 1) The combination of the acoustic and common encoder representations outperforms the best performing SSL model (HuBERT) by 8.08% (relative) on average for the 8 datasets (Table  II ). Given the similar parameter count, this performance suggests an advantage of our pretraining approach. 2) For the three out-of-domain datasets, the acoustic-common combination fares better than its semantic counterpart. 3) Across all datasets, the combination of both encoders in CARE yields the highest performance, suggesting that while the individual encoder performances are comparable, they capture distinct characteristics of the speech signal.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Modifications In The Semantic Encoder",
      "text": "To evaluate the suitability of our design choices for the semantic encoder, we made three architectural modifications:\n\n1) CARE-No init.: Removing the convolutional adapters, the transformer layers in the semantic encoder are initialized randomly (instead of pre-trained RoBERTa weights). 2) CARE-Trans.: Removing the convolutional adapters while the RoBERTa transformer layers are updated. 3) CARE-FT: Keeping the convolutional adapters, we update all the parameters (conv. adapters and transformer weights) in the semantic encoder. The results for these modifications are shown in Table  V . Key takeaways: 1) Initializing the transformer weights with RoBERTa is essential for CARE's performance. Random initialization of the semantic encoder leads to performance drops Datasets PASE+  [23]  Whisper  [36]  Whisper  [36] + RoBERTa  [35]  Teacher 3) Updating the transformer layers in the semantic encoder decreases performance. Since RoBERTa is pre-trained on text, fine-tuning with speech data degrades its effectiveness.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Initialization Of Acoustic And Common Encoders",
      "text": "As indicated in Section III, the common and the acoustic encoders of CARE are initialized with the WavLM-base model weights. We present the results of our method when this initialization is modified to i) random, ii) HuBERT-base  [15]  or iii) data2vec-base  [60]  (Table  VI ). Key takeaways: 1) The model's performance decreases with data2vec initialization, likely due to data2vec's lower baseline performance compared to HuBERT and WavLM (see Table  II ). An exception is the CMU-MOSI dataset, where this initialization improves over the WavLM initialized model by 4.21% (relative).\n\n2) The HuBERT-initialized model performs best on the RAVDESS-Song and Emo-DB datasets. Notably, HuBERT outperforms WavLM for these two out-of-domain datasets (Table  II ). 3) Initialization impacts the acoustic and common encoders less than the semantic encoder, as the latter requires alignment with text representations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Choice Of Acoustic Targets",
      "text": "We run an experiment where the acoustic encoder is trained with targets based on eGeMAPS  [9]  features extracted from the openSMILE toolkit  [65] . The PASE+ targets of the acoustic encoder of the CARE model is replaced by the eGeMAPS features. The performance of this model, called CARE (eGeMAPS), is shown in Fig.  3 . Key takeaway: The baseline model using eGeMAPS input features performs worse than the baseline with PASE+ features, as expected, since eGeMAPS are handcrafted. Consequently, the average performance of CARE with eGeMAPS is also lower than that of CARE with PASE+ targets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Choice Of Semantic Targets",
      "text": "We conduct an experiment where the Whisper encoder representations serve as supervisory signals for the semantic encoder. We explore two variants of this: 1) We pool the Whisper representations to serve as semantic targets while pretraining. This model is called CARE (Whisper-pool). 2) We pre-train a model with the frame-level representations of Whisper as the targets. We call this model CARE (Whisper-frame).\n\n3) We also use the frame level alignments between speech and the RoBERTa tokens and use the frame-level RoBERTa representations as the semantic targets. We call this model CARE (RoBERTa-frame). The comparative performances of the different variants along with the proposed model-CARE (RoBERTa-pool) are shown in Fig.  4 . Key takeaways: 1) The performance of CARE (RoBERTapool) is seen to be superior to both variants trained with Whisper encoded representations.\n\n2) The performance of the systems when the semantic encoder is trained with the pooled targets is observed to be better than those trained with framelevel representations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "G. Continued Pre-Training Of Wavlm",
      "text": "Since all self-supervised learning (SSL) models are trained on neutral data, their ability to accurately discern emotions from speech signals is typically limited. The emotion recognition performance of these SSL models when pre-trained on emotion datasets thus becomes crucial. To explore the impact of pre-training setup in the proposed CARE, we continued the pre-training of the publicly available WavLM-base model, using the MSP-PODCAST dataset. This was done following the WavLM pre-training procedure, with masked language modeling loss, for an additional 200, 000 steps (similar to the CARE). The results of this experiment are shown in Fig.  5 , wherein the performance of the continually pre-trained WavLM model is denoted by WavLM-Cont. Pretrain. The performance of the combination of the common and acoustic encoders is also shown for comparison. Key takeaway: Continued pre-training improves WavLMbase performance on certain downstream tasks, like IEMO-CAP. However, except for IEMOCAP, WavLM performs worse than CARE's acoustic-common encoder combination. As all the models in Fig.  5  have the same size (94M) during inference, this experiment highlights the benefits of our proposed distillation-based pre-training.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "H. Multimodal Emotion Recognition",
      "text": "To assess CARE's utility in the multimodal speech-text setting, we design a model using speech (WavLM-base or CARE), and text (RoBERTa) fusion. After combining the layer representations in SUPERB style, we concatenate the representations and train a classification head. We experiment on IEMOCAP-4 and IEMOCAP-6 and observe that the weighted F1-score for CARE+RoBERTa improves from 73.02% to 75.19% for IEMOCAP-4 and from 60.41% to 62.21% for IEMOCAP-6 over WavLM-base+RoBERTa system. In addition to the uni-modal improvements reported in Table  IV , these results highlight that multi-modal speechtext emotion recognition systems can also benefit from the enhanced representations provided by CARE.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "I. Visualization Of Layers Of Care",
      "text": "In order to interpret the pre-trained model representations learnt by the acoustic and semantic encoder of CARE, we probe the representations from each encoder. We use the English part of the Emotional Speech Dataset (ESD)  [66]  for this analysis. We form pairs of utterances, where both the utterances of a pair have the same emotional label in all cases and they are derived from two different speakers. A total of 1750 pairs are considered and the cosine similarities of the pooled representations (for transformer layer 7) are shown in Fig.  6 . The figure on the left indicates the setting where the speech content in the two utterances is the same whereas the plot on the right indicates different speech content. We note that when the spoken content is different, the acoustic encoder has higher similarity than the semantic encoder, indicating that the acoustic encoder is beneficial when the emotion information cannot be reliably predicted from the textual content of the audio.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Summary",
      "text": "Key Highlights: In this paper, a pre-training technique for content and acoustic encoding of emotional speech is provided. The proposed architecture, termed CARE, learns an enriched representation of acoustic and semantic information. The acoustic encoder uses supervision from low-level descriptors of speech, while the semantic encoder is distilled using text representations of the speech transcripts. We also propose an adaptation strategy for text-based models in speech representation learning using convolutional neural network layers. The CARE model, with experiments on 8 downstream tasks, is seen to outperform models of comparable sizes on most of the datasets. Further, the CARE is also observed to generalize better than LLM based models with large parameter sizes. The importance of the different components of the proposed model, along with the different design choices, are established through ablation studies.\n\nLimitations and future scope: The MSP-PODCAST dataset is used for pre-training CARE, which has only 230 hours of emotional speech data. Another limitation of this work, is the relatively lower performance on some in-domain speech datasets compared to LLM-based models, like the CMU-MOSI. In future, we plan to extend the CARE approach to multi-modal speech-text emotion recognition tasks.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Scatter plot of inference model size (parameters in millions) versus",
      "page": 2
    },
    {
      "caption": "Figure 1: We highlight a clear gap in current modeling",
      "page": 2
    },
    {
      "caption": "Figure 2: Block diagram of the proposed CARE model. The acoustic encoder of the model is trained with PASE+ features as targets. Blocks in blue indicate",
      "page": 3
    },
    {
      "caption": "Figure 2: provides a block diagram of the CARE model.",
      "page": 4
    },
    {
      "caption": "Figure 3: Key takeaway: The baseline model using eGeMAPS input",
      "page": 8
    },
    {
      "caption": "Figure 3: Performance of CARE when different acoustic targets are used. The",
      "page": 9
    },
    {
      "caption": "Figure 4: Performance of CARE when different semantic targets are used. All",
      "page": 9
    },
    {
      "caption": "Figure 4: Key takeaways: 1) The performance of CARE (RoBERTa-",
      "page": 9
    },
    {
      "caption": "Figure 5: Comparison of the performance when WavLM is continually pre-",
      "page": 9
    },
    {
      "caption": "Figure 5: , wherein the performance of the continually pre-trained",
      "page": 9
    },
    {
      "caption": "Figure 5: have the same size (94M) during infer-",
      "page": 9
    },
    {
      "caption": "Figure 6: The figure on the left indicates the setting where the",
      "page": 9
    },
    {
      "caption": "Figure 6: Distribution of cosine similarities for layer 7 representations for",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#Train\nUtt.": "4425",
          "#Val.\nUtt.": "1102",
          "#Test\nUtt.": "1102",
          "#Classes": "4",
          "Class\nBal.": "(cid:35)"
        },
        {
          "#Train\nUtt.": "5947",
          "#Val.\nUtt.": "1487",
          "#Test\nUtt.": "1387",
          "#Classes": "6",
          "Class\nBal.": "(cid:35)"
        },
        {
          "#Train\nUtt.": "9988",
          "#Val.\nUtt.": "1108",
          "#Test\nUtt.": "2610",
          "#Classes": "7",
          "Class\nBal.": "(cid:35)"
        },
        {
          "#Train\nUtt.": "1188",
          "#Val.\nUtt.": "325",
          "#Test\nUtt.": "686",
          "#Classes": "2",
          "Class\nBal.": "(cid:33)"
        },
        {
          "#Train\nUtt.": "6003",
          "#Val.\nUtt.": "2097",
          "#Test\nUtt.": "2097",
          "#Classes": "2",
          "Class\nBal.": "(cid:33)"
        },
        {
          "#Train\nUtt.": "704",
          "#Val.\nUtt.": "132",
          "#Test\nUtt.": "176",
          "#Classes": "6",
          "Class\nBal.": "(cid:33)"
        },
        {
          "#Train\nUtt.": "624",
          "#Val.\nUtt.": "156",
          "#Test\nUtt.": "156",
          "#Classes": "7",
          "Class\nBal.": "(cid:33)"
        },
        {
          "#Train\nUtt.": "324",
          "#Val.\nUtt.": "105",
          "#Test\nUtt.": "106",
          "#Classes": "7",
          "Class\nBal.": "(cid:33)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavLM[16]\nParams:94M": "65.9±0.5(67.2)",
          "HuBERT[15]\nParams:94M": "65.0±0.2(68.0)",
          "data2vec[60]\nParams:94M": "62.7±0.7(64.0)",
          "emotion2vec[21]\nParams:94M": "67.5±0.6#(69.0)",
          "SONAR[31]\nParams:600M": "59.4±0.4(61.0)",
          "SALMONN[32]\nParams:7B Params:13B": "75.8±0.6#(76.9)",
          "Column_7": "72.9±2.3#(74.9)"
        },
        {
          "WavLM[16]\nParams:94M": "51.7±0.5(48.4)",
          "HuBERT[15]\nParams:94M": "50.7±0.9(46.5)",
          "data2vec[60]\nParams:94M": "46.0±0.4(42.3)",
          "emotion2vec[21]\nParams:94M": "54.1±0.6#(51.9)",
          "SONAR[31]\nParams:600M": "43.5±0.2(41.0)",
          "SALMONN[32]\nParams:7B Params:13B": "59.3±1.4#(55.7)",
          "Column_7": "58.1±1.6#(55.2)"
        },
        {
          "WavLM[16]\nParams:94M": "45.6±0.4(24.3)",
          "HuBERT[15]\nParams:94M": "45.3±0.6(24.0)",
          "data2vec[60]\nParams:94M": "41.9±0.5(23.1)",
          "emotion2vec[21]\nParams:94M": "47.6±0.3#(27.4)",
          "SONAR[31]\nParams:600M": "43.2±0.2(23.3)",
          "SALMONN[32]\nParams:7B Params:13B": "53.3±0.7(33.4)",
          "Column_7": "52.6±0.4(32.8)"
        },
        {
          "WavLM[16]\nParams:94M": "64.1±0.8(64.2)",
          "HuBERT[15]\nParams:94M": "62.5±0.6(62.5)",
          "data2vec[60]\nParams:94M": "59.7±0.4(58.9)",
          "emotion2vec[21]\nParams:94M": "66.5±0.6(65.9)",
          "SONAR[31]\nParams:600M": "74.6±0.3(73.9)",
          "SALMONN[32]\nParams:7B Params:13B": "78.0±0.7(77.0)",
          "Column_7": "72.8±1.0(72.0)"
        },
        {
          "WavLM[16]\nParams:94M": "63.2±1.5(61.5)",
          "HuBERT[15]\nParams:94M": "65.9±2.0(61.9)",
          "data2vec[60]\nParams:94M": "67.8±1.4(65.7)",
          "emotion2vec[21]\nParams:94M": "61.6±0.7(61.0)",
          "SONAR[31]\nParams:600M": "64.3±0.4(63.7)",
          "SALMONN[32]\nParams:7B Params:13B": "62.6±3.4(60.4)",
          "Column_7": "64.7±3.0(61.1)"
        },
        {
          "WavLM[16]\nParams:94M": "50.5±3.6(49.1)",
          "HuBERT[15]\nParams:94M": "53.5±1.1(55.7)",
          "data2vec[60]\nParams:94M": "38.5±5.2(40.8)",
          "emotion2vec[21]\nParams:94M": "48.5±1.0(51.0)",
          "SONAR[31]\nParams:600M": "11.8±2.0(10.8)",
          "SALMONN[32]\nParams:7B Params:13B": "50.2±1.3(54.2)",
          "Column_7": "51.9±3.6(53.4)"
        },
        {
          "WavLM[16]\nParams:94M": "66.6±2.6(69.0)",
          "HuBERT[15]\nParams:94M": "66.5±4.5(69.1)",
          "data2vec[60]\nParams:94M": "48.8±4.3(51.0)",
          "emotion2vec[21]\nParams:94M": "59.3±3.8(62.6)",
          "SONAR[31]\nParams:600M": "5.7±1.4(7.1)",
          "SALMONN[32]\nParams:7B Params:13B": "59.9±2.0(62.8)",
          "Column_7": "69.8±3.3(71.4)"
        },
        {
          "WavLM[16]\nParams:94M": "66.5±4.8(68.5)",
          "HuBERT[15]\nParams:94M": "66.9±3.9(68.2)",
          "data2vec[60]\nParams:94M": "48.9±3.1(49.9)",
          "emotion2vec[21]\nParams:94M": "64.4±2.6(66.7)",
          "SONAR[31]\nParams:600M": "10.2±2.0(12.6)",
          "SALMONN[32]\nParams:7B Params:13B": "82.8±2.9(85.3)",
          "Column_7": "82.2±4.0(84.1)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PASE+[23]\nParams:8M": "56.68",
          "Whisper[36]\nParams:800M": "56.40",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "61.97",
          "Teacher-\nfusion\nParams:1.6B": "69.49",
          "Semantic+\nCommonEnc.\nParams:110M": "66.44",
          "Acoustic+\nCommonEnc.\nParams:94M": "65.91"
        },
        {
          "PASE+[23]\nParams:8M": "41.38",
          "Whisper[36]\nParams:800M": "40.62",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "49.28",
          "Teacher-\nfusion\nParams:1.6B": "56.61",
          "Semantic+\nCommonEnc.\nParams:110M": "53.05",
          "Acoustic+\nCommonEnc.\nParams:94M": "52.09"
        },
        {
          "PASE+[23]\nParams:8M": "35.86",
          "Whisper[36]\nParams:800M": "40.11",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "49.29",
          "Teacher-\nfusion\nParams:1.6B": "49.72",
          "Semantic+\nCommonEnc.\nParams:110M": "47.37",
          "Acoustic+\nCommonEnc.\nParams:94M": "46.98"
        },
        {
          "PASE+[23]\nParams:8M": "50.69",
          "Whisper[36]\nParams:800M": "55.60",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "75.14",
          "Teacher-\nfusion\nParams:1.6B": "74.12",
          "Semantic+\nCommonEnc.\nParams:110M": "64.23",
          "Acoustic+\nCommonEnc.\nParams:94M": "64.17"
        },
        {
          "PASE+[23]\nParams:8M": "66.84",
          "Whisper[36]\nParams:800M": "62.08",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "64.04",
          "Teacher-\nfusion\nParams:1.6B": "67.63",
          "Semantic+\nCommonEnc.\nParams:110M": "66.32",
          "Acoustic+\nCommonEnc.\nParams:94M": "66.89"
        },
        {
          "PASE+[23]\nParams:8M": "46.05",
          "Whisper[36]\nParams:800M": "34.20",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "9.58",
          "Teacher-\nfusion\nParams:1.6B": "48.48",
          "Semantic+\nCommonEnc.\nParams:110M": "55.23",
          "Acoustic+\nCommonEnc.\nParams:94M": "56.17"
        },
        {
          "PASE+[23]\nParams:8M": "52.86",
          "Whisper[36]\nParams:800M": "19.22",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "13.59",
          "Teacher-\nfusion\nParams:1.6B": "53.42",
          "Semantic+\nCommonEnc.\nParams:110M": "69.23",
          "Acoustic+\nCommonEnc.\nParams:94M": "71.62"
        },
        {
          "PASE+[23]\nParams:8M": "62.59",
          "Whisper[36]\nParams:800M": "24.77",
          "Whisper[36]+\nRoBERTa[35]\nParams:1.6B": "14.98",
          "Teacher-\nfusion\nParams:1.6B": "66.75",
          "Semantic+\nCommonEnc.\nParams:110M": "75.42",
          "Acoustic+\nCommonEnc.\nParams:94M": "78.63"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Randominit.": "66.72",
          "HuBERTinit.": "67.65",
          "Data2vecinit.": "66.76"
        },
        {
          "Randominit.": "51.47",
          "HuBERTinit.": "51.40",
          "Data2vecinit.": "52.98"
        },
        {
          "Randominit.": "46.41",
          "HuBERTinit.": "46.97",
          "Data2vecinit.": "47.19"
        },
        {
          "Randominit.": "65.07",
          "HuBERTinit.": "66.26",
          "Data2vecinit.": "68.14"
        },
        {
          "Randominit.": "67.56",
          "HuBERTinit.": "65.19",
          "Data2vecinit.": "66.59"
        },
        {
          "Randominit.": "57.81",
          "HuBERTinit.": "60.30",
          "Data2vecinit.": "55.09"
        },
        {
          "Randominit.": "73.83",
          "HuBERTinit.": "72.23",
          "Data2vecinit.": "62.46"
        },
        {
          "Randominit.": "78.61",
          "HuBERTinit.": "86.51",
          "Data2vecinit.": "77.19"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CARE-Noinit.\nCARE-Trans.\nCARE-FT\nCARE": "CARE-Noinit.\nCARE-Trans.\nCARE-FT\nCARE"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "eGeMAPS PASE+ CARE (eGeMAPS) CARE (PASE+) WavLM-Base WavLM-Cont. Pr\n80\n70\nerocs-1F\n60\nerocs-1F 52.5\ndethgieW 40\ndethgieW 35 20\n0\n17.5 IEMOCAP-4 IEMOCAP-6 MELD\nCMU\n0\nIEMOCAP-4 IEMOCAP-6 MELD CMU-MOSI DAIC-WOZ Fig. 5. Comparison of the perfo\ntrainedonMSP-PODCAST.Thep\nand common encoders of CARE\nFig.3. PerformanceofCAREwhendifferentacoustictargetsareused.The referstotheRAVDESS-Songdata\nmodel with eGeMAPS as features is trained similarly to that of the PASE+": "baseline.Allnumbersareshownastheaverageof5randominitializations.\nusing the MSP-PODCAST\nthe WavLM pre-training p\nCARE (Whisper-frame) CARE (Whisper-pool)\nmodeling loss, for an add\nCARE (RoBERTa-frame) CARE (RoBERTa-pool)\n70",
          "WavLM-Base WavLM-Cont. Pr\n80\nerocs-1F\n60\ndethgieW 40\n20\n0\nIEMOCAP-4 IEMOCAP-6 MELD\nCMU\nFig. 5. Comparison of the perfo\ntrainedonMSP-PODCAST.Thep\nand common encoders of CARE\nhe referstotheRAVDESS-Songdata": "",
          "etrain CARE (Acoustic+Common Encoder)\n-MOSI DAIC RAVDESS CaFE EmoDB\nrmance when WavLM is continually pre-\nerformanceofthecombinationofacoustic\nis shown for reference. Here, RAVDESS\nset.": ""
        },
        {
          "eGeMAPS PASE+ CARE (eGeMAPS) CARE (PASE+) WavLM-Base WavLM-Cont. Pr\n80\n70\nerocs-1F\n60\nerocs-1F 52.5\ndethgieW 40\ndethgieW 35 20\n0\n17.5 IEMOCAP-4 IEMOCAP-6 MELD\nCMU\n0\nIEMOCAP-4 IEMOCAP-6 MELD CMU-MOSI DAIC-WOZ Fig. 5. Comparison of the perfo\ntrainedonMSP-PODCAST.Thep\nand common encoders of CARE\nFig.3. PerformanceofCAREwhendifferentacoustictargetsareused.The referstotheRAVDESS-Songdata\nmodel with eGeMAPS as features is trained similarly to that of the PASE+": "the CARE). The results o\nFig. 5, wherein the perform\nerocs-1F 52.5\nWavLM model is denoted\nperformance of the combin\ndethgieW 35 encoders is also shown for\nKey takeaway: Continued\n17.5\nbase performance on certai\nCAP.However,exceptforIE\n0\nthan CARE’s acoustic-com\nIEMOCAP-4 IEMOCAP-6 MELD CMU-MOSI DAIC-WOZ\nthe models in Fig. 5 have t\nence, this experiment highli\nFig.4. PerformanceofCAREwhendifferentsemantictargetsareused.All\ndistillation-based pre-trainin\nnumbersareshownastheaverageof5randominitializations.\nH. Multimodal Emotion Re\nencoder. We explore two variants of this: 1) We pool the\nTo assess CARE’s utilit\nWhisperrepresentationstoserveassemantictargetswhilepre-",
          "WavLM-Base WavLM-Cont. Pr\n80\nerocs-1F\n60\ndethgieW 40\n20\n0\nIEMOCAP-4 IEMOCAP-6 MELD\nCMU\nFig. 5. Comparison of the perfo\ntrainedonMSP-PODCAST.Thep\nand common encoders of CARE\nhe referstotheRAVDESS-Songdata": "",
          "etrain CARE (Acoustic+Common Encoder)\n-MOSI DAIC RAVDESS CaFE EmoDB\nrmance when WavLM is continually pre-\nerformanceofthecombinationofacoustic\nis shown for reference. Here, RAVDESS\nset.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Wavlm"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "Maja Pantic"
      ],
      "year": "2005",
      "venue": "ACM international conference on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "Emotion detection and analysis on social media",
      "authors": [
        "Bharat Gaind",
        "Varun Syal",
        "Sneha Padgalwar"
      ],
      "year": "2019",
      "venue": "Emotion detection and analysis on social media",
      "arxiv": "arXiv:1901.08458"
    },
    {
      "citation_id": "4",
      "title": "Acoustic and lexical sentiment analysis for customer service calls",
      "authors": [
        "Bryan Li",
        "Dimitrios Dimitriadis",
        "Andreas Stolcke"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "EmoKey: An emotion-aware smartphone keyboard for mental health monitoring",
      "authors": [
        "Surjya Ghosh"
      ],
      "year": "2019",
      "venue": "COMSNETS"
    },
    {
      "citation_id": "6",
      "title": "Some aspects of fundamental frequency and envelope amplitude as related to the emotional content of speech",
      "authors": [
        "Philip Lieberman",
        "Sheldon Michaels"
      ],
      "year": "1962",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "7",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "Valery Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering"
    },
    {
      "citation_id": "8",
      "title": "The INTERSPEECH 2009 emotion challenge",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2009",
      "venue": "The INTERSPEECH 2009 emotion challenge"
    },
    {
      "citation_id": "9",
      "title": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "10",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben"
      ],
      "year": "2015",
      "venue": "IEEE transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "Promod Yenigalla"
      ],
      "year": "2018",
      "venue": "Speech emotion recognition using spectrogram & phoneme embedding"
    },
    {
      "citation_id": "12",
      "title": "Multimodal transformer with learnable frontend and self attention for emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Effective attention mechanism in dynamic models for speech emotion recognition",
      "authors": [
        "Po-Wei Hsiao",
        "Chia-Ping Chen"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using CNN-LSTM and vision transformer",
      "authors": [
        "Ayush Cs",
        "Kumar"
      ],
      "year": "2022",
      "venue": "International Conference on Innovations in Bio-Inspired Computing and Applications"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "17",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "19",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "20",
      "title": "HCAM-Hierarchical Cross Attention Model for Multi-modal Emotion Recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2023",
      "venue": "HCAM-Hierarchical Cross Attention Model for Multi-modal Emotion Recognition",
      "arxiv": "arXiv:2304.06910"
    },
    {
      "citation_id": "21",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "23",
      "title": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Yuanchao Li"
      ],
      "year": "2023",
      "venue": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "arxiv": "arXiv:2305.16065"
    },
    {
      "citation_id": "24",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "Mirco Ravanelli"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Speaker recognition from raw waveform with SincNet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "SLT"
    },
    {
      "citation_id": "26",
      "title": "Interpretable representation learning for speech and audio signals based on relevance weighting",
      "authors": [
        "Purvi Agrawal",
        "Sriram Ganapathy"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "27",
      "title": "LEAF: A Learnable Frontend for Audio Classification",
      "authors": [
        "Neil Zeghidour"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "28",
      "title": "Learning problem-agnostic speech representations from multiple self-supervised tasks",
      "authors": [
        "Santiago Pascual"
      ],
      "year": "2019",
      "venue": "Learning problem-agnostic speech representations from multiple self-supervised tasks"
    },
    {
      "citation_id": "29",
      "title": "Gammatone features and feature combination for large vocabulary speech recognition",
      "authors": [
        "Ralf Schluter"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "30",
      "title": "Leveraging contrastive learning and self-training for multimodal emotion recognition with limited labeled samples",
      "authors": [
        "Qi Fan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "authors": [
        "Guimin Hu"
      ],
      "year": "2024",
      "venue": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "arxiv": "arXiv:2409.07388"
    },
    {
      "citation_id": "32",
      "title": "SONAR: sentence-level multimodal and language-agnostic representations",
      "authors": [
        "Paul-Ambroise Duquenne",
        "Holger Schwenk",
        "Benoît Sagot"
      ],
      "year": "2023",
      "venue": "SONAR: sentence-level multimodal and language-agnostic representations"
    },
    {
      "citation_id": "33",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang"
      ],
      "year": "2023",
      "venue": "SALMONN: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "34",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "35",
      "title": "WavLLM: Towards robust and adaptive speech large language model",
      "authors": [
        "Shujie Hu"
      ],
      "year": "2024",
      "venue": "WavLLM: Towards robust and adaptive speech large language model",
      "arxiv": "arXiv:2404.00656"
    },
    {
      "citation_id": "36",
      "title": "RoBERTa: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "37",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford"
      ],
      "year": "2023",
      "venue": "ICML"
    },
    {
      "citation_id": "38",
      "title": "Sentiment-aware word and sentence level pre-training for sentiment analysis",
      "authors": [
        "Fan Shuai"
      ],
      "year": "2022",
      "venue": "EMNLP"
    },
    {
      "citation_id": "39",
      "title": "Towards the next frontier in speech representation learning using disentanglement",
      "authors": [
        "Varun Krishna",
        "Sriram Ganapathy"
      ],
      "year": "2024",
      "venue": "Towards the next frontier in speech representation learning using disentanglement",
      "arxiv": "arXiv:2407.02543"
    },
    {
      "citation_id": "40",
      "title": "Connecting speech encoder and large language model for asr",
      "authors": [
        "Wenyi Yu"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "41",
      "title": "Prompting large language models with speech recognition abilities",
      "authors": [
        "Yassir Fathullah"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "42",
      "title": "Evaluating parameter-efficient transfer learning approaches on sure benchmark for speech understanding",
      "authors": [
        "Yingting Li"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "43",
      "title": "Convolution-augmented parameter-efficient fine-tuning for speech recognition",
      "authors": [
        "Kwangyoun Kim"
      ],
      "year": "2024",
      "venue": "Convolution-augmented parameter-efficient fine-tuning for speech recognition"
    },
    {
      "citation_id": "44",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "Yang Shu Wen"
      ],
      "year": "2021",
      "venue": "SUPERB: Speech Processing Universal PERformance Benchmark"
    },
    {
      "citation_id": "45",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "LREC"
    },
    {
      "citation_id": "47",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "48",
      "title": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh"
      ],
      "year": "2016",
      "venue": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "49",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "Jonathan Gratch"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "50",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "51",
      "title": "A canadian french emotional speech dataset",
      "authors": [
        "Philippe Gournay",
        "Olivier Lahaie",
        "Roch Lefebvre"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th ACM multimedia systems conference"
    },
    {
      "citation_id": "52",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "53",
      "title": "DialogueRNN: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "54",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "55",
      "title": "Self-supervised representations in speech-based depression detection",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "56",
      "title": "Automatic depression detection: An emotional audio-textual corpus and a GRU/BiLSTM-based model",
      "authors": [
        "Ying Shen",
        "Huiyu Yang",
        "Lin Lin"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "57",
      "title": "A step towards preserving speakers' identity while detecting depression via speaker disentanglement",
      "authors": [
        "Vijay Ravi"
      ],
      "year": "2022",
      "venue": "A step towards preserving speakers' identity while detecting depression via speaker disentanglement"
    },
    {
      "citation_id": "58",
      "title": "Climate and weather: Inspecting depression detection via emotion recognition",
      "authors": [
        "Wen Wu",
        "Mengyue Wu",
        "Kai Yu"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "59",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "60",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "Vinod Nair",
        "Geoffrey Hinton"
      ],
      "year": "2010",
      "venue": "ICML"
    },
    {
      "citation_id": "61",
      "title": "Data2vec: A general framework for selfsupervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski"
      ],
      "year": "2022",
      "venue": "ICML"
    },
    {
      "citation_id": "62",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "63",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "64",
      "title": "SimSensei Kiosk: A virtual human interviewer for healthcare decision support",
      "authors": [
        "David Devault"
      ],
      "year": "2014",
      "venue": "International conference on Autonomous agents and multi-agent systems"
    },
    {
      "citation_id": "65",
      "title": "SMIN: Semi-supervised Multimodal Interaction Network for Conversational Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "66",
      "title": "openSMILE: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "ACM international conference on Multimedia"
    },
    {
      "citation_id": "67",
      "title": "Emotional voice conversion: Theory, databases and ESD",
      "authors": [
        "Kun Zhou"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    }
  ]
}