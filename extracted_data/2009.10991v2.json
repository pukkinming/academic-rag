{
  "paper_id": "2009.10991v2",
  "title": "Attention Driven Fusion For Multi-Modal Emotion Recognition",
  "published": "2020-09-23T08:07:58Z",
  "authors": [
    "Darshana Priyasad",
    "Tharindu Fernando",
    "Simon Denman",
    "Clinton Fookes",
    "Sridha Sridharan"
  ],
  "keywords": [
    "Speech emotion recognition",
    "deep learning",
    "multi-modal fusion",
    "cross attention",
    "SincNet"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has emerged as a powerful alternative to hand-crafted methods for emotion recognition on combined acoustic and text modalities. Baseline systems model emotion information in text and acoustic modes independently using Deep Convolutional Neural Networks (DCNN) and Recurrent Neural Networks (RNN), followed by applying attention, fusion, and classification. In this paper, we present a deep learning-based approach to exploit and fuse text and acoustic data for emotion classification. We utilize a SincNet layer, based on parameterized sinc functions with band-pass filters, to extract acoustic features from raw audio followed by a DCNN. This approach learns filter banks tuned for emotion recognition and provides more effective features compared to directly applying convolutions over the raw speech signal. For text processing, we use two branches (a DCNN and a Bidirection RNN followed by a DCNN) in parallel where cross attention is introduced to infer the N-gram level correlations on hidden representations received from the Bi-RNN. Following existing state-of-the-art, we evaluate the performance of the proposed system on the IEMOCAP dataset. Experimental results indicate that the proposed system outperforms existing methods, achieving 3.5% 1 improvement in weighted accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the advance of technology, Human Computer Interaction (HCI) has become a major research area. Within this field, automatic emotion recognition is being pursued as a means to improve the level of user experience by tailoring responses to the emotional context, especially in humanmachine interactions  [1] . However, this remains challenging due to the ambiguity of expressed emotions. An utterance may contain subject dependent auditory clues regarding expressed emotions which are not captured through speech transcripts alone. With deep learning, architectures can extract 1 Note: this has been updated from the ICASSP published version due to a small error. Results have been updated to correct the error, but overall findings are unchanged.\n\nhigher level features and more robust features for accurate speech emotion recognition  [2, 3] . In this paper, we propose a model that combines acoustic and textual information for speech emotion recognition.\n\nRecently, multi-modal information has been used in emotion recognition in preference to uni-modal methods  [4] , since humans express emotion via multiple modes  [5, 6, 7, 8] . Most state-of-the-art methods for utterance level emotion recognition have used low-level (energy) and high-level acoustic features (such as Mel Frequency Cepstral Coefficients (MFCC)  [5, 9] ). However, when the emotion expressed through speech becomes ambiguous, the lexical content may provide complementary information that can address the ambiguity.\n\nTripathi et al.  [10]  has used a Long-Short Term Memory (LSTM) along with a DCNN to perform joint acoustic and textual emotion recognition. They have used features such as MFCC, Zero Crossing Rate and spectral entropy for acoustic data, while using Glove  [11]  embeddings to extract a feature vector from speech transcripts. However, the performance gain is minimal due to the lack of robustness in the acoustic features, and the sparse text feature vectors. Yenigalla et al.  [12]  proposed a spectral and phenoms-sequence based DCNN model which is capable of retaining the emotional content of the speech that is lost when converted to text. Yoon et al.  [5]  presented a framework using Bidirectional LSTMs to obtain hidden representations of acoustic and textual information. The resultant features are fused with multi-hop attention, where one modality is used to direct attention for the other mode. Higher performance has been achieved due to the attention and fusion which select relevant segments from the textual model, and complementary features from each mode. Yoon et al.  [6]  have also presented an encoder based method where the fusion of two recurrent encoders is used to combine features from audio and text. However, both methods use manually calculated audio features which limit their accuracy and the robustness of the acoustic model  [13, 14] .\n\nGu et al.  [7]  presented a multimodal framework where a hybrid deep multimodal structure that considers spatial and temporal information is employed. Features obtained from each model were fused using a DNN to classify the emotion. Li et al.  [8]  proposed a personalized attribute aware attention mechanism where an attention profile is learned based arXiv:2009.10991v2 [eess.AS] 10 Oct 2020 on acoustic and lexical behavior data. Mirsamadi et al.  [15]  used deep learning along with local attention to automatically extract relevant features where segment level acoustic features are aggregated for utterance level emotion representation. However, the accuracy could be further improved by fusing the audio features with another modality with complementary information.\n\nIn this paper, we present a multi-modal emotion recognition model with combines acoustic and textual information by using DCNNs, and both cross attention and self-attention. Experiments are performed on the IEMOCAP  [16]  dataset to enable fair comparison with state-of-the-art methods, and a performance gain of 3.5% in weighted accuracy is achieved.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Acoustic Feature Extraction",
      "text": "In our proposed model, we utilize a SincNet filter layer  [17]  to learn custom filter banks tuned for emotion recognition from speech audio. This layer is shown to have fast convergence and higher interpretability with a smaller number of parameters compared to conventional convolution layers. Formally, this layer can be defined as,\n\nwhere x[n], y[n], g, θ refers to the input signal, filtered output, filter-bank function, and the learnable parameters respectively. In SincNet filters, convolution operations are applied over a raw waveform with predefined functions. Each defined filter-bank is composed of rectangular band-pass filters which can be represented by two low-pass filters with learnable cutoff frequencies. The time-domain representation of the function g can be derived as follows  [17] ,\n\nwhere f 1 , f 2 refers to low and high cutoff frequencies and sinc = sin x/x. The resultant convolution layer outputs are passed through a DCNN which contains several \"Convolution1D\", \"Batch Normalization\" and \"fully connected layers\". During the initial training phase, a random 250ms chunk from the audio signal is selected as the input. During validation and testing, we obtain the final \"softmax\" response for each chunk and add them together to get the final classification scores, similar to  [17] . However, we retrieve a 2048-D feature vector from the final dense layer before the classification layer for each chunk of an utterance, and average these before fusing them with textual features from the corresponding transcript in a later step (see Section 2.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Textual Feature Extraction",
      "text": "In our proposed model, after the sequence vector is passed through a common embedding layer, we utilize two parallel branches for textual feature extraction as illustrated in Figure  1 . Bi-RNNs followed by DCNNs have been extensively used in textual emotion analysis  [18, 19] . As an alternative, a CNN based architecture which is capable of considering \"n\" words at a time (n-grams) can be used  [20] . Therefore we use two parallel branches, employing, one using Bi-RNNs with DC-NNs and the other DCNNs alone to increase the effectiveness of the learned features (see Figure  1 (B) ). The resultant feature vector from the Bi-RNN is passed through three convolutional layers with a filter sizes of 1 ,3 and 5; and convolutional layers with the same size filter are used in the parallel branch. We introduce cross-attention where we use convolution layers with the same filter size from the right branch as the attention for the left branch, as illustrated, and jointly train with the other components of the network. The cross-attention is calculated using\n\nwhere α i , b i,j , a i,j , H are the attention score, context vector from the right branch with a filter size of j, output of the convolution layer with filter size j in left branch, and the output. The convolution layers from both branches are concatenated together and passed through a DNN consisting of fully connected layers for textual emotion classification. We retrieve a 4800-D feature vector from the final dense layer before the classification layer for multi-modal feature fusion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Acoustic & Textual Feature Fusion",
      "text": "Mid-level fusion is used to fuse textual and acoustic features obtained from individual networks. A 2048-D feature vector from the acoustic network and a 4800-D feature vector from the textual network are concatenated as illustrated in Figure  1 (C) . A neural network with attention is used to identify informative segments in the feature vectors. We have explored using fusion without attention (F-I), attention after fusion (F-II) where self-attention is applied on concatenated features, and attention before fusion (F-III) where attention is applied on individual feature vectors. For F-III, we calculate attention weights and combine the vectors using  [21] ,\n\nc t , h t , β t , and q refer to the merged feature vector, neural network (which is randomly initialized and jointly trained with other components of the network) output, attention score and the output respectively. Finally the the utterance emotion is classified using a \"softmax\" activation over the final dense layer of the fusion network.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset And Experimental Setup",
      "text": "Experiments are conducted on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset which includes five sessions of utterances for 10 unique speakers. We follow the evaluation protocol of  [5, 6] , and select utterances annotated with four basic emotions \"anger\", \"happiness\", \"neutral\" and \"sadness\". Samples with \"excitement\" are merged with \"happiness\" as per  [5, 6] . The resultant dataset contains 5531 utterances {\"anger\":1103, \"happiness\":1636, \"neutral\":1708, \"sadness\":1084}.\n\nInitial training is carried out on both acoustic and textual networks separately before the fusion. The sampling rate of each utterance waveform is set to 16, 000Hz while a random segment of 250ms is used in training the acoustic network. During the evaluation, the cumulative sum of all the predictions with a window size and shift of 250ms and 10ms are considered. In the textual network, all the transcripts of the utterances are set to a maximum length of 100 and padded with 0s. Glove-300d embeddings are used to convert the word sequence to a vector of (100, 300). We utilize a 10-fold crossvalidation with an 8:1:1 split for training, validation, and test sets respectively for text model. We select an average performing split and use this split to train the acoustic and fusion networks (we use a single split due to the high computation time of the acoustic model), such that all networks (text, audio and fusion) use the same data splits. The learning rate and the batch size in each network are fixed at 0.001 and 64 respectively, and the Adam optimiser is used.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Performance Evaluation And Analysis",
      "text": "Following  [5, 6] , the performance of our system is measured in terms of weighted accuracy (WA) and unweighted accuracy (UA). Table  1  and Figure  2  present performance of our approach for emotion recognition compared with the state of the art methods.\n\nMDRE  [6]  has used two RNNs to encode both acoustic and textual data followed by a DNN for classification, while Evec-MCNN-LSTM  [22]  has used an RNN and a DCNN to encode both modalities followed by fusion and an SVM for  Our proposed model has achieved a substantial improvement in overall accuracy, with a 3.5% increase compared to MHA-2. We have utilized self-attention before (F-III) and after fusion (F-II) as illustrated in Figure  1 . Cross-modal attention has not been utilized after fusion since the dimensionality of the feature vectors from the two modalities are different. A slight increase in the classification accuracy has been obtained by applying self-attention compared to conventional feature fusion (F-I). Furthermore, the highest accuracy has been obtained by F-III, outperforming F-II by 0.5%. Given that F-I slightly outperforms MHA-2, we have compared the classification accuracy of the individual modes of MHA-2 with our individual modes in Table  2 .\n\nOur acoustic and textual models outperformed the corresponding individual modes of MHA-2  [5] , where a substantial improvement of learning and deriving customized filter banks tuned for emotion recognition. It has been successfully applied for speaker recognition  [17]  as an alternative to i-vectors. The confusion matrices for F-I, F-II, and F-III are illustrated in Figure  2 . A 10.6% relative improvement in classification accuracy can be observed when comparing the individual modalities with the fusion network in our model. Given that accuracy is approximately similar for both modalities, each modality has complemented the other to increase recognition accuracy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present an attention-based multi-modal emotion recognition model combining acoustic and textual data. The raw audio waveform is utilized in our method, rather than extracting hand-crafted features as done by baseline methods.\n\nCombining a DCNN with a SincNet layer, which learns suitable filter parameters over the waveform for emotion recognition, outperforms the hand-crafted feature-based audio emotion detection of the baselines. Cross attention is applied to text-based feature extraction to guide the features derived by RNNs using N-gram level features extracted by a parallel branch. We have used self-attention on both feature vectors obtained from two networks before the fusion, to attend to the informative segments from each feature vector. We have achieved a weighted accuracy of 79.22% on the IEMO-CAP database, which outperforms the existing state-of-the-art model by 3.5%.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Bi-RNNs followed by DCNNs have been extensively used",
      "page": 2
    },
    {
      "caption": "Figure 1: (B)). The resultant fea-",
      "page": 2
    },
    {
      "caption": "Figure 1: (C). A neural network with attention is used to identify in-",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed architecture - The system contains three main parts: the text network (A); the audio network (B); and the fusion",
      "page": 3
    },
    {
      "caption": "Figure 2: present performance of our",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrices of the proposed architecture for separate fusion methods calculated using a average performing",
      "page": 4
    },
    {
      "caption": "Figure 1: Cross-modal atten-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Recognition accuracy of individual modes of the",
      "data": [
        {
          "MHA-2 [5]\nMHA-2 [5]": "Ours\nOurs",
          "A\nT": "A\nT",
          "65.2%\n70.3%": "69.8%\n66.7%"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Recognition accuracy of individual modes of the",
      "data": [
        {
          "Evec-MCNN-LSTM [22]\nMDRE [6]\nMHA-2 [5]": "Ours - F-I\nOurs - F-II\nOurs - F-III",
          "A + T\nA + T\nA + T": "A + T\nA + T\nA + T",
          "64.9%\n71.8%\n76.5%": "77.85%\n78.98%\n79.22%",
          "65.9%\n−\n77.6%": "79.27%\n80.01%\n80.51%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Detection of human emotion from speech-tools and techniques",
      "authors": [
        "A Mohanta",
        "U Sharma"
      ],
      "year": "2018",
      "venue": "Speech and Language Processing for Human-Machine Communications"
    },
    {
      "citation_id": "3",
      "title": "End-toend speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "8",
      "title": "Deep mul timodal learning for emotion recognition in spoken language",
      "authors": [
        "Y Gu",
        "S Chen",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Attentive to individual: A multimodal emotion recognition network with personalized attention profile",
      "authors": [
        "J.-L Li",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "10",
      "title": "Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "12",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Convolutional attention networks for multimodal emotion recognition from speech and text data",
      "authors": [
        "C Lee",
        "K Song",
        "J Jeong",
        "W Choi"
      ],
      "year": "2018",
      "venue": "ACL 2018"
    },
    {
      "citation_id": "15",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "18",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "19",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "20",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "21",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification",
      "arxiv": "arXiv:1408.5882"
    },
    {
      "citation_id": "22",
      "title": "Learning salient features for multimodal emotion recognition with recurrent neural networks and attention based fusion",
      "authors": [
        "D Priyasad",
        "T Fernando",
        "S Denman",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2019",
      "venue": "15th International Conference on Auditory Visual Speech Processing (AVSP)"
    },
    {
      "citation_id": "23",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    }
  ]
}