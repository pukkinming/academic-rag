{
  "paper_id": "2507.15205v2",
  "title": "Long-Short Distance Graph Neural Networks And Improved Curriculum Learning For Emotion Recognition In Conversation",
  "published": "2025-07-21T03:12:54Z",
  "authors": [
    "Xinran Li",
    "Xiujuan Xu",
    "Jiaqi Qiao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) is a practical and challenging task. This paper proposes a novel multimodal approach, the Long-Short Distance Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it constructs a longdistance graph neural network and a short-distance graph neural network to obtain multimodal features of distant and nearby utterances, respectively. To ensure that long-and short-distance features are as distinct as possible in representation while enabling mutual influence between the two modules, we employ a Differential Regularizer and incorporate a BiAffine Module to facilitate feature interaction. In addition, we propose an Improved Curriculum Learning (ICL) to address the challenge of data imbalance. By computing the similarity between different emotions to emphasize the shifts in similar emotions, we design a \"weighted emotional shift\" metric and develop a difficulty measurer, enabling a training process that prioritizes learning easy samples before harder ones. Experimental results on the IEMOCAP and MELD datasets demonstrate that our model outperforms existing benchmarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC)  [9]  is a practical task with applications in areas like chatbots  [13]  and the analysis of opinions on social media  [4] . In ERC, the recognition of the target utterance's emotion considers the influence of other utterances, primarily from the speaker themselves and other participants. The speaker's emotions may change under the influence of other participants. Therefore, fully exploring the previous attitudes and emotions of participants and modeling the dialogue context is crucial to identify the emotion of the target utterance. Curriculum Learning  [2]  is a training strategy that simulates the human learning process by starting with simple samples and gradually increasing difficulty, aiming to improve the model's generalization ability and convergence speed. Current research mainly faces the following two issues:\n\n(1) Most existing studies employ Graph Neural Networks (GNN)  [26]  to model historical utterances, treating contextual utterances (past or future) as nodes in a graph, and encoding contextual features into the target utterance through graph aggregation mechanisms. However, current GNN-based methods tend to construct overly complex models, using different graphs to process short-and long-distance contextual features separately. Although this approach can extract rich contextual features, there is a high degree of similarity between short-distance and long-distance features, leading to feature redundancy. Additionally, redundant computations across different graphs not only increase computational complexity and resource consumption but may also hinder performance improvement.\n\n(2) At present, most methods focus mainly on feature extraction or model architecture optimization, with relatively little attention given to improving the training process. These approaches often introduce excessively complex model designs and lengthy feature extraction procedures, resulting in increased computational costs while offering limited performance gains. Moreover, ERC datasets generally suffer from severe class imbalance, making it difficult for models to adequately learn the characteristics of low-frequency categories during training, thereby affecting overall performance and generalization ability  [28] . Therefore, optimizing training strategies to enhance model performance on imbalanced datasets remains a crucial challenge that needs to be addressed.\n\nTo address these challenges, we propose a multimodal ERC model, Long-Short Distance Graph Neural Network (LSDGNN), along with an Improved Curriculum Learning (ICL) strategy. LS-DGNN leverages a Directed Acyclic Graph (DAG)  [24]  to integrate short-and long-distance contextual features while using a Differential Regularizer to enhance feature diversity and mitigate redundancy. To enable features exchange between the long-and short-distance modules and enhance their features mutually, we introduce the Bi-Affine Module. These improve the model's ability to capture subtle emotional nuances. ICL consists of a difficulty measurer and a training scheduler, which dynamically adjust training based on sample complexity  [19] . By assigning different transition weights to emotions based on similarity, the model focuses more on easily confused emotions, improving classification performance. Moreover, our experiments show that assigning higher difficulty weight to similar emotions yields better results than assigning higher difficulty weight to dissimilar emotions, which aligns with human intuition.\n\nExperiments on IEMOCAP and MELD show that our model outperforms existing approaches, with particularly strong improvements on IEMOCAP.\n\nOur contributions can be summarized as follows:\n\n(1) We propose LSDGNN, a novel and effective multimodal ERC model that enhances emotion recognition through long-short distance DAG-based feature fusion, Differential Regularizer, and BiAffine Module.\n\n(2) We introduce ICL, an improved curriculum learning training strategy based on Weighted Emotional Shifts based on similarity, which enhances emotion classification performance. This method is flexible, allowing for easy integration into any emotion recognition model to improve its generalization ability.\n\n(3) Our model achieves state-of-the-art performance and will serve as a benchmark for future ERC research. The code and data have been released publicly on GitHub. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "With the rapid development of natural language processing and human-computer interaction technologies, emotion recognition in dialogue (ERC) has gradually become a hot research topic. This section introduces the commonly used ERC methods and the application of curriculum learning in this field.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Commonly Used Methods In The Erc Field",
      "text": "In recent years, research in the ERC field has focused on three baseline models: Recurrent Neural Networks (RNN), Graph Convolutional Networks (GNN), and Transformer models.\n\nIn RNN-related research, ICON  [7]  uses hierarchical modeling to capture global context and employs GRU to handle temporal dependencies between utterances; DialogueCRN  [8]  introduces cognitive factors, enhancing the understanding of context at both the situational and speaker levels.\n\nIn GNN-based research, DialogueGCN  [6]  treats dialogue utterances as graph vertices, with edges constructed based on context. MMGCN  [10]  uses multimodal features as nodes, connecting three modalities within the same node while establishing connections among the same modality. DAG-ERC  [24]  takes into account the identity of the speaker and location attributes when building a directed acyclic graph neural network.\n\nIn transformer-related research, BERT-ERC  [22]  improves the performance through suggestion texts, fine-grained classification modules and two-stage training, showing strong generalizability.\n\nWith the rise of large language models (LLMs), methods such as InstructERC  [14] , BiosERC  [27] , and LaERC-S  [5]  have adopted generative architectures to address ERC tasks. However, considering the potential data leakage issues of LLMs on the MELD dataset, this paper compares the proposed method only with small-scale models.\n\nAlthough these techniques have achieved great success, data scarcity remains the biggest problem faced by ERC  [11] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Curriculum Learning",
      "text": "Curriculum Learning (CL)  [2] , a training strategy that simulates human learning processes, has recently been explored in the field of Emotion Recognition in Conversation (ERC).\n\nYang et al.  [28]  proposed the Hybrid Curriculum Learning framework, which focuses solely on textual features and combines conversation-level and utterance-level curriculum learning strategies. By using a difficulty measurer based on the frequency of \"emotion shifts\" and enhancing emotion similarity, this framework helps the model gradually learn complex emotional patterns.\n\nAdditionally, Nguyen et al.  [19]  introduced the MultiDAG+CL method, which integrates Directed Acyclic Graphs (DAG) to combine textual, acoustic, and visual features within a unified framework, enhanced by Curriculum Learning to address challenges related to emotional shifts and data imbalance.\n\n1 https://github.com/LiXinran6/LSDGNN_ICL These studies suggest that incorporating Curriculum Learning into ERC tasks aids models in better handling emotional variations and data imbalance, thereby improving the accuracy and robustness of emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "This section introduces the following five aspects. First, it presents the problem definition. Next, it discusses the multimodal feature extraction methods. Then, it explains the construction method of the simplified graph structure used in this paper. After that, it introduces the proposed Long-Short Distance Graph Neural Network. And finally, it presents the proposed Improved Curriculum Learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Definition",
      "text": "In ERC, given a conversation consisting of a sequence of utterances, it is defined as D = {u1, u2, u3, ..., uN }, where N represents the total number of utterances. Each utterance is spoken by a single speaker, defined as ui,s j , indicating that the speaker of the i th utterance is sj. The goal of ERC is to assign an emotion label y k ∈ Y , such as joy or sadness, to each utterance ui in the conversation. Y is a set of emotion labels.\n\nThe solution to this problem is to propose a function f that takes an utterance ui as input and outputs the predicted emotion label yi. The function f should model the context only by considering past utterances {u1, u2, ..., ui-1} and not using future utterances {ui+1, ui+2, ..., uN }.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "We use modality-specific encoders to extract features. For the textual modality, RoBERTa  [16]  is used to extract features, while a Fully Connected Network (FCN) is employed for processing the acoustic and visual modalities, as shown in Equation (  1 ):\n\nwhere FCNA and FCNV represent Fully Connected Networks for the audio and visual modalities, respectively, and RoBERTa is the feature extractor for the textual modality. These encoders generate context-aware raw feature encodings h a i , h v i , h t i . For a given utterance ui with available multimodal inputs, its multimodal feature vector shown in Equation (  2 ):\n\nwhere ⊕ denotes the feature concatenation operation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Constructing A Graph Through Conversation",
      "text": "Different researchers use various complex graphs to extract text features. However, overly complex ideas and methods often lead to redundancy in the extracted features. In this paper, we use a simple and efficient graph, the directed acyclic graph (DAG) . DAG is represented as G = (V, E, R), where the nodes, denoted by V = {u1, u2, ..., uN }, correspond to the utterances in the conversation. The edges (i, j, rij) ∈ E represent the feature propagation from ui to uj, with rij ∈ R indicating the type of relation associated with the edge. The set of relation types R = {0, 1} includes two categories: type 1 indicates that the connected utterances are spoken by the same speaker, while type 0 indicates otherwise. The process of constructing the graph includes the following steps. First, each utterance in the conversation is treated as a node in the graph, and edges between nodes are determined based on the speaker's identity. Starting from the second utterance, the algorithm sequentially checks previous utterances and adds edges with a relation type (1 or 0) depending on whether the speakers are the same. A maximum of ω utterances from the same speaker can be connected, and utterances from different speakers are also linked, ultimately generating a complete directed acyclic graph. The constructed graph is shown in the Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Long-Short Distance Graph Neural Network",
      "text": "In this subsection, we introduce the Long-Short Distance Graph Neural Network (LSDGNN) proposed in this paper. Based on DAGNN  [25] , this model constructs long-distance and short-distance DAGNNs separately to extract long-and short-distance features, respectively. Due to the redundancy between long-and short-distance features, we employ a Differential Regularizer to further ensure that these features are more distinct in representation. We also employ the BiAffine Module to facilitate feature interaction between the longand short-distance modules, allowing their representations to mutually enhance each other. The framework of the model is shown in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dagnn",
      "text": "DAGNN (Directed Acyclic Graph Neural Network) is a type of graph neural network designed to effectively handle directed acyclic graph data structures. It aggregates node features in temporal order, allowing the collection of features from multiple neighboring nodes at the same layer. Additionally, it permits feature propagation temporally within the same layer, enabling access to features from distant utterances. Each node's state update depends not only on its own state but also on the aggregated features from neighboring nodes. Thus, DAGNN effectively combines the advantages of graph neural networks (GNN) and recurrent neural networks (RNN). Its state update formula as shown in Equation  (3) .\n\nwhere H l i represents the representation of node i in layer l.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Layers Of Lsdgnn",
      "text": "The proposed LSDGNN is inspired by DAG-ERC  [24]  and adopts its feature aggregation operation. Due to the temporal nature of the DAG, nodes must be updated sequentially from the first to the last utterance. For each utterance ui, the hidden state from the previous layer (l -1) is used to compute attention weights on the hidden states of its predecessors at layer l. The attention weights are computed as follows, as shown in Equation (  4 ):\n\nwhere W l α are trainable parameters, and ∥ represents the concatenation operation. The model collects features through relation-aware transformations, enabling it to exploit different edge types. The aggregated features are then combined with the hidden state from the previous layer using a gated recurrent unit (GRU) to compute the current layer's hidden state. The aggregated features M l i are computed as shown in Equation (  5 ):\n\nwhere W l rij ∈ {W l 0 , W l 1 } are the learnable parameters for the relation-aware transformation. Independent learnable parameters are assigned to different edge types (type 1 or 0). Next, as shown in Equation (  6 ), the GRU processes the hidden state and aggregated features to compute Hl i :\n\nwhere H l-1 i and M l i are the inputs, and Hl i is the output of the GRU. Additionally, a contextual feature unit is designed by swapping the inputs to the GRU, allowing the hidden state from the previous layer to control the propagation of contextual features. This process is described as shown in Equation (  7 ):\n\nwhere C l i is the output of the contextual feature unit. Finally, the node's representation at layer l is the sum of the outputs from the GRU and the contextual feature unit, as described in Equation (  8 ):",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Fusion And Prediction",
      "text": "In a conversation, a speaker's emotion is influenced by previous utterances, which include both short-distance and long-distance utterances. Therefore, we design the LSDGNN model, which consists of two modules: one for long-distance and one for short-distance. For short distances, we set ω to 1, meaning the model traces back to the first self-uttered statement. Meanwhile, for long distances, we use a value greater than 1.\n\nThrough feature extraction, we obtain the multimodal fused feature H 0 . Then, we input H 0 into the long-distance and short-distance channels, with the features denoted as H 0 L and H 0 S , respectively. These two features are processed through multiple LSDGNN layers, and after each layer, the resulting features are denoted as H j L and H j S , where j indicates the j-th layer. Inspired by Li et al.  [15] , we use mutual BiAffine transformations as a bridge to effectively exchange relevant features between The original inputs H L 0 and H S 0 are actually the same. In LSDGNN, at each layer, long-distance and short-distance features are processed using the Differential Regularizer and BiAffine Module. Here, i represents the i-th utterance, and j represents the features from the j-th layer.\n\nthe long-distance and short-distance modules. Specifically, the transformations are shown in Equations (  9 ) and  (10) .\n\nwhere W1 and W2 are trainable parameters.\n\nFor the i-th utterance, by concatenating the features H j L ′ and H j S ′ extracted from each layer through the BiAffine module with the original features H 0 , the final feature representation H is obtained as shown in Equation  (11) .\n\nwhere j ranges from 1 to l, indicating the features of the j-th layer. l represents the total number of layers in the LSDGNN. The obtained features Hi of the utterance ui are fed through a feedforward neural network to predict the emotion, as shown in Equations (  12 ),  (13) , and  (14) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Regularizer",
      "text": "We expect the feature representations learned from the long-distance and short-distance modules to capture distinct characteristics. Therefore, we introduce a Differential Regularizer  [15]  between the adjacency matrices of the two modules, as shown in Equation  (15) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Function",
      "text": "Our training objective is to minimize the following total objective function shown in Equation (  16 ):\n\nwhere LC is the standard cross-entropy loss, and LR is the differential regularization loss. λ is the regularization coefficient, set to 0.1 here.\n\nIn ERC, the cross-entropy loss is formulated as shown in Equation (17):\n\nwhere M is the total number of samples in the training dataset. Ni denotes the number of time steps for the i-th sample. Pi,t[yi,t] is the predicted probability for the true label yi,t at time step t for the i-th sample.\n\nFigure  3 . Previous psychological research  [20]  suggested that emotions consist of two dimensions: Valence and Arousal, and emotions are described using a two-dimensional coordinate system similar to a wheel. Inspired by Jing et al.  [12] , we construct this diagram, which includes all the emotions from the standard ERC dataset. Each emotion label can be mapped to a point on the unit circle.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Improved Curriculum Learning",
      "text": "Curriculum Learning  [2]  is a training strategy that arranges training tasks in a progressively increasing order of difficulty, improving the model's learning efficiency and performance. Moreover, it has excellent scalability and can be directly transferred to other models for training and usage.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Difficulty Measure Function",
      "text": "Inspired by Yang et al.  [28] , we designed a Difficulty Measure Function based on the weighted emotional shift frequency in conversations, considering the emotional similarity between utterances. Our approach is simpler and more efficient. As shown in Figure  3 , we introduce a 2D arousal-valence emotion wheel, where each emotion label corresponds to a point on the unit circle. We calculate the similarity between emotion labels according to Equation  (18) :\n\nwhere sij represents the similarity between label i and label j, vi denotes the valence value of label i, θij is the angle between label i and label j. N is the total number of emotions in the dataset. The closer two emotions are, the higher their similarity value.\n\nWe have constructed a function to calculate the difficulty of a conversation based on the Weighted Emotional Shifts (WES). An emotional shift is defined as occurring when the emotions expressed in two consecutive utterances by the same speaker are different. A weighted emotional shift is defined as shown in Equation (  19 ):\n\nWe employed a linear transformation, where the term similarity refers to the degree of similarity between two emotions, k represents the weight and b denotes the bias. Thus, the difficulty of a conversation ci is defined as shown in Equation  (20) :\n\nwhere N shif t (ci) and Nu(ci) represent the number of emotional shifts in conversation ci and the total number of utterances in conversation ci, respectively. Nsp(ci) is the number of speakers appearing in conversation ci, acting as a smoothing factor. N W ES j is the Weighted Emotional Shifts at the j-th emotional shift. The proposed algorithm is presented as Algorithm 1. The time complexity of the algorithm is O(n+m log m+t), where n is the number of utterances, m is the number of conversations, and t is the number of training epochs. When n is large, the complexity is dominated by O(n). The space complexity is O(mn), with dataset storage being the main factor.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Scheduler",
      "text": "The training scheduler organizes the training process by dividing the dataset D into multiple bins {D1, D2, ..., D k } based on similar difficulty. Training starts with the easiest bin, and after several epochs, the next bin is gradually added. Once all bins have been used, additional training epochs are performed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "This section introduces the datasets and implementation details.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We evaluated our approach on two ERC datasets: IEMOCAP  [3]  and MELD  [21] . The detailed dataset statistics are shown in Table  1 . We compared our method with several state-of-the-art baselines, including both unimodal and multimodal approaches. Consistent with previous work, the evaluation metric of our model is based on the average weighted F1 score from five random runs on the test set. Additionally, we include accuracy and macro F1 for supplementary comparison.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Each training and testing process runs on a single RTX 3090 GPU.\n\nFor IEMOCAP, we set the number of LSDGNN layers to 4, dropout rate to 0.4, batch size to 16, embedding layer size to 2948, differential regularization loss to 0.1, the number of buckets in curriculum learning to 5, and the learning rate to 0.0005, with each epoch taking a maximum of 10 seconds. For MELD, we set the number of LS-DGNN layers to 2, dropout rate to 0.1, batch size to 64, embedding layer size to 1666, differential regularization loss to 0.1, the number of buckets in curriculum learning to 12, and the learning rate to 0.00001, with each epoch taking a maximum of 8 seconds.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis",
      "text": "This section presents the comparison between our model and stateof-the-art methods, the results of ablation studies, and the impact of different values of long-distance w, as well as different values of k and b in Weighted Emotional Shifts on model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State Of The Art",
      "text": "Table  2  and Table  3  present the performance of our model on the IEMOCAP and MELD datasets, respectively, with bold values indicating the best results among all models. We compare the models based on three evaluation metrics: weighted F1, accuracy, and macro F1. The results of other models are taken from their original papers, with \"-\" indicating missing values. Our experiments are conducted using five random seeds, and the reported results are the averaged values. Our approach, LSDGNN+ICL (which integrates the LSDGNN model with the Improved Curriculum Learning strategy), achieves SOTA performance on both the IEMOCAP and MELD datasets, outperforming previous methods by 0.84% on IEMOCAP and 0.07% on MELD.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Experiments",
      "text": "To investigate the importance and necessity of each module in our model, we conducted ablation experiments on both datasets. The results are presented in Table  4 , with each value averaged over five runs. All comparison metrics in this section are weighted F1 scores. From the results, it can be observed that removing any part of the model leads to a decline in performance, indicating that each module plays a significant role. When the Improved Curriculum Learning is removed, the performance on both datasets significantly drops. Similarly, when the long-distance module is removed, the performance also notably decreases. It can be seen that the Differential Regularizer and BiAffine Module contribute to improving the performance on long-distance tasks to some extent. When both ICL and the longdistance module are removed, the model essentially becomes equivalent to DAG-ERC.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Different Values W Of Long-Distance",
      "text": "ω refers to the number of previous nodes that have the same speaker as the current node when constructing the DAG. Specifically, the current node searches for ω nodes with the same speaker in the graph. Generally, a smaller ω searches for fewer previous utterances, while a larger ω searches for more previous utterances, connecting with a greater number of speech sentences. Our model consists of two modules: the long-distance module and the short-distance module. For the short-distance module, we default ω to 1, while different values of ω in the long-distance module have an impact on performance. Table  5  shows the effect of different ω values in the long-distance module on the model's performance. All comparison metrics in this section are weighted F1 scores, averaged over five random seeds. It can be observed that as the ω value of the long-distance module increases from small to large, the model's performance on both datasets gradually improves, reaching its best performance when ω is 5. After that, the performance starts to decline slightly. We also tested the case where the ω of the long-distance module is set to infinity, and the result showed a slight decrease in performance, not reaching the best. This indicates that excessively long historical context does not further improve the model's performance, but may instead introduce",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Impact Of Parameters On Weighted Emotional Shifts",
      "text": "From Equation  19 , we can see that after obtaining the similarity between emotional shifts, we apply a linear transformation, assigning different values to N W ES by controlling the slope k and the bias b. A larger N W ES indicates a higher difficulty in emotional shifts. Therefore, when k is positive, greater emotional similarity results in higher overall difficulty, leading the model to first learn samples with larger emotional differences before learning those with similar emotions. Conversely, when k is negative, greater emotional differences correspond to higher overall difficulty, causing the model to first learn samples with similar emotions before moving on to those with greater differences. As shown in Table  6 , we experiment with different values of k and b and ultimately determine the optimal parameter combination for both datasets. All comparison metrics in this section are weighted F1 scores, averaged over five random seeds. We can see that when k is positive, regardless of the value of b, the overall results are better than those when k is negative. This indicates that curriculum learning has indeed played a role, while also proving that the more similar the changing emotions are, the harder they are to learn, which aligns with human intuition. In addition, different values of b cause significant fluctuations in model performance, especially when k is -1. This further suggests that when k is negative, regardless of the value of b, the defined difficulty standard makes it harder for the model to learn, thereby weakening the effect of curriculum learning. When k is 1, all results show varying degrees of improvement regardless of the value of b, whereas when k is -1, some results exhibit a significant decline.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present a novel multimodal approach called the Long-Short Distance Graph Neural Network (LSDGNN) for Emotion Recognition in Conversation (ERC). LSDGNN combines both long-and short-distance graph neural networks based on a Directed Acyclic Graph (DAG) to extract multimodal features from distant and nearby utterances. To ensure effective representation of these features while maintaining mutual influence between the two modules, we introduce a Differential Regularizer and incorporate a Bi-Affine Module to enhance feature interaction. Additionally, we propose an Improved Curriculum Learning (ICL) approach to address the data imbalance issue by designing a \"weighted emotional shift\" metric, which emphasizes emotional shifts between similar emotions. This difficulty measurer enables the model to prioritize learning easier samples before more difficult ones. The method can also be directly transferred to other ERC tasks. Experimental results on the IEMOCAP and MELD datasets demonstrate that our model outperforms existing benchmarks, showcasing its effectiveness in addressing the complexities of ERC tasks.\n\nIn the future, we will explore more advanced transformation methods for weighted emotional shifts and investigate adaptive graph strategies for multi-party and low-resource scenarios.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A directed acyclic graph (DAG) constructed from a three-party",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.",
      "page": 4
    },
    {
      "caption": "Figure 3: Previous psychological research [20] suggested that emotions",
      "page": 5
    },
    {
      "caption": "Figure 3: , we introduce a 2D arousal-valence emotion",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "aSchool of Software Technology, Dalian University of Technology": "can extract rich contextual features, there is a high degree of similar-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ity between short-distance and long-distance features, leading to fea-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ture redundancy. Additionally, redundant computations across differ-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ent graphs not only increase computational complexity and resource"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "consumption but may also hinder performance improvement."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "(2) At present, most methods focus mainly on feature extraction or"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "model architecture optimization, with relatively little attention given"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "to improving the training process. These approaches often introduce"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "excessively complex model designs and lengthy feature extraction"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "procedures,\nresulting in increased computational costs while offer-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ing limited performance gains. Moreover, ERC datasets generally"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "suffer\nfrom severe class\nimbalance, making it difficult\nfor models"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "to adequately learn the characteristics of\nlow-frequency categories"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "during training,\nthereby affecting overall performance and general-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ization ability [28]. Therefore, optimizing training strategies to en-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "hance model performance on imbalanced datasets remains a crucial"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "challenge that needs to be addressed."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "To\naddress\nthese\nchallenges, we\npropose\na multimodal ERC"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "model, Long-Short Distance Graph Neural Network (LSDGNN),"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "along with an Improved Curriculum Learning (ICL)\nstrategy. LS-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "DGNN leverages a Directed Acyclic Graph (DAG) [24] to integrate"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "short- and long-distance contextual features while using a Differen-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "tial Regularizer to enhance feature diversity and mitigate redundancy."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "To enable features exchange between the long- and short-distance"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "modules and enhance their\nfeatures mutually, we introduce the Bi-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "Affine Module. These improve the model’s ability to capture subtle"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "emotional nuances. ICL consists of a difficulty measurer and a train-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "ing scheduler, which dynamically adjust\ntraining based on sample"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "complexity [19]. By assigning different\ntransition weights to emo-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "tions based on similarity, the model focuses more on easily confused"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "emotions,\nimproving classification performance. Moreover, our ex-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "periments\nshow that assigning higher difficulty weight\nto similar"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "emotions yields better results than assigning higher difficulty weight"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "to dissimilar emotions, which aligns with human intuition."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "Experiments on IEMOCAP and MELD show that our model out-"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "performs existing approaches, with particularly strong improvements"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "on IEMOCAP."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "Our contributions can be summarized as follows:"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "(1) We propose LSDGNN, a novel and effective multimodal ERC"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "model that enhances emotion recognition through long-short distance"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "DAG-based feature\nfusion, Differential Regularizer,\nand BiAffine"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "Module."
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": "(2) We introduce ICL, an improved curriculum learning training"
        },
        {
          "aSchool of Software Technology, Dalian University of Technology": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "which enhances emotion classification performance. This method is",
          "These studies suggest that incorporating Curriculum Learning into": "ERC tasks aids models in better handling emotional variations and"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "flexible, allowing for easy integration into any emotion recognition",
          "These studies suggest that incorporating Curriculum Learning into": "data imbalance,\nthereby improving the accuracy and robustness of"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "model to improve its generalization ability.",
          "These studies suggest that incorporating Curriculum Learning into": "emotion recognition."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "(3) Our model achieves state-of-the-art performance and will serve",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "as a benchmark for\nfuture ERC research. The code and data have",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "3\nMethodology"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "been released publicly on GitHub.1",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "This section introduces the following five aspects. First,\nit presents"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "the problem definition. Next, it discusses the multimodal feature ex-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "2\nRelated Work",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "traction methods. Then,\nit explains the construction method of\nthe"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "With\nthe\nrapid\ndevelopment\nof\nnatural\nlanguage\nprocessing\nand",
          "These studies suggest that incorporating Curriculum Learning into": "simplified graph structure used in this paper. After that, it introduces"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "human-computer interaction technologies, emotion recognition in di-",
          "These studies suggest that incorporating Curriculum Learning into": "the proposed Long-Short Distance Graph Neural Network. And fi-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "alogue (ERC) has gradually become a hot research topic. This section",
          "These studies suggest that incorporating Curriculum Learning into": "nally, it presents the proposed Improved Curriculum Learning."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "introduces the commonly used ERC methods and the application of",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "curriculum learning in this field.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "3.1\nProblem Definition"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "In ERC, given a conversation consisting of a sequence of utterances,"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "2.1\nCommonly Used Methods in the ERC Field",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "it\nis defined as D = {u1, u2, u3, ..., uN }, where N represents the"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "In recent years, research in the ERC field has focused on three base-",
          "These studies suggest that incorporating Curriculum Learning into": "total number of utterances. Each utterance is\nspoken by a single"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "line models: Recurrent Neural Networks\n(RNN), Graph Convolu-",
          "These studies suggest that incorporating Curriculum Learning into": "indicating that\nthe speaker of"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "speaker, defined as ui,sj ,"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "tional Networks (GNN), and Transformer models.",
          "These studies suggest that incorporating Curriculum Learning into": "terance is sj. The goal of ERC is to assign an emotion label yk ∈ Y ,"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "In RNN-related research, ICON [7] uses hierarchical modeling to",
          "These studies suggest that incorporating Curriculum Learning into": "in the conversation. Y is\nsuch as joy or sadness, to each utterance ui"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "capture global context and employs GRU to handle temporal depen-",
          "These studies suggest that incorporating Curriculum Learning into": "a set of emotion labels."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "dencies between utterances; DialogueCRN [8] introduces cognitive",
          "These studies suggest that incorporating Curriculum Learning into": "The solution to this problem is to propose a function f that\ntakes"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "factors, enhancing the understanding of context at both the situational",
          "These studies suggest that incorporating Curriculum Learning into": "as\ninput and outputs\nthe predicted emotion label\nan utterance ui"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "and speaker levels.",
          "These studies suggest that incorporating Curriculum Learning into": "should model\nthe context only by considering\nyi. The function f"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "In GNN-based research, DialogueGCN [6]\ntreats dialogue ut-",
          "These studies suggest that incorporating Curriculum Learning into": "past utterances {u1, u2, ..., ui−1} and not using future utterances"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "terances as graph vertices, with edges constructed based on con-",
          "These studies suggest that incorporating Curriculum Learning into": "{ui+1, ui+2, ..., uN }."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "text. MMGCN [10] uses multimodal\nfeatures as nodes, connecting",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "three modalities within the same node while establishing connec-",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "3.2\nFeature Extraction"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "tions among the same modality. DAG-ERC [24] takes into account",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "the identity of\nthe speaker and location attributes when building a",
          "These studies suggest that incorporating Curriculum Learning into": "We use modality-specific encoders to extract features. For the textual"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "directed acyclic graph neural network.",
          "These studies suggest that incorporating Curriculum Learning into": "modality, RoBERTa [16]\nis used to extract\nfeatures, while a Fully"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "In transformer-related research, BERT-ERC [22] improves the per-",
          "These studies suggest that incorporating Curriculum Learning into": "Connected Network (FCN) is employed for processing the acoustic"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "formance through suggestion texts, fine-grained classification mod-",
          "These studies suggest that incorporating Curriculum Learning into": "and visual modalities, as shown in Equation (1):"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "ules and two-stage training, showing strong generalizability.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "With the rise of large language models (LLMs), methods such as",
          "These studies suggest that incorporating Curriculum Learning into": "ha\nhv\nht\n(1)\ni = FCNA(ua\ni ),\ni = FCNV (uv\ni ),\ni = RoBERTa(ut\ni)"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "InstructERC [14], BiosERC [27], and LaERC-S [5] have adopted",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "represent Fully Connected Networks for\nwhere FCNA and FCNV"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "generative architectures to address ERC tasks. However, considering",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "the audio and visual modalities,\nrespectively, and RoBERTa is the"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "the potential data leakage issues of LLMs on the MELD dataset, this",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "feature extractor\nfor\nthe textual modality. These encoders generate"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "paper compares the proposed method only with small-scale models.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "context-aware raw feature encodings ha\ni.\ni , ht\ni , hv"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "Although\nthese\ntechniques\nhave\nachieved\ngreat\nsuccess,\ndata",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "For a given utterance ui with available multimodal inputs, its mul-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "scarcity remains the biggest problem faced by ERC [11].",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "timodal feature vector shown in Equation (2):"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "2.2\nCurriculum Learning",
          "These studies suggest that incorporating Curriculum Learning into": "H 0"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "(2)\ni = ha\ni ⊕ hv\ni ⊕ ht"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "Curriculum Learning (CL) [2], a training strategy that simulates hu-",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "where ⊕ denotes the feature concatenation operation."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "man learning processes, has recently been explored in the field of",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "Emotion Recognition in Conversation (ERC).",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "Yang\net\nal.\n[28]\nproposed\nthe Hybrid Curriculum Learning",
          "These studies suggest that incorporating Curriculum Learning into": "3.3\nConstructing a Graph through Conversation"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "framework, which focuses solely on textual\nfeatures and combines",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "Different researchers use various complex graphs to extract text fea-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "conversation-level and utterance-level curriculum learning strategies.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "tures. However, overly complex ideas and methods often lead to re-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "By using a difficulty measurer based on the frequency of \"emotion",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "dundancy in the extracted features. In this paper, we use a simple and"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "shifts\" and enhancing emotion similarity,\nthis framework helps the",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "efficient graph, the directed acyclic graph (DAG) ."
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "model gradually learn complex emotional patterns.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "DAG is represented as G = (V, E, R), where the nodes, denoted"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "Additionally, Nguyen et al.\n[19]\nintroduced the MultiDAG+CL",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "by V = {u1, u2, ..., uN }, correspond to the utterances in the con-"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "method, which integrates Directed Acyclic Graphs (DAG)\nto com-",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "versation. The edges (i, j, rij) ∈ E represent the feature propagation"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "bine textual, acoustic, and visual features within a unified framework,",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "from ui to uj, with rij ∈ R indicating the type of relation associated"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "enhanced by Curriculum Learning to address challenges related to",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "with the edge. The set of\nrelation types R = {0, 1} includes two"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "emotional shifts and data imbalance.",
          "These studies suggest that incorporating Curriculum Learning into": ""
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "",
          "These studies suggest that incorporating Curriculum Learning into": "categories: type 1 indicates that the connected utterances are spoken"
        },
        {
          "strategy based on Weighted Emotional Shifts based on similarity,": "1 https://github.com/LiXinran6/LSDGNN_ICL",
          "These studies suggest that incorporating Curriculum Learning into": "by the same speaker, while type 0 indicates otherwise."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.4.2\nLayers of LSDGNN": "The proposed LSDGNN is inspired by DAG-ERC [24] and adopts"
        },
        {
          "3.4.2\nLayers of LSDGNN": "its feature aggregation operation. Due to the temporal nature of the"
        },
        {
          "3.4.2\nLayers of LSDGNN": "DAG, nodes must be updated sequentially from the first\nto the last"
        },
        {
          "3.4.2\nLayers of LSDGNN": "the hidden state from the previous\nutterance. For each utterance ui,"
        },
        {
          "3.4.2\nLayers of LSDGNN": "layer (l −1) is used to compute attention weights on the hidden states"
        },
        {
          "3.4.2\nLayers of LSDGNN": "of its predecessors at layer l. The attention weights are computed as"
        },
        {
          "3.4.2\nLayers of LSDGNN": "follows, as shown in Equation (4):"
        },
        {
          "3.4.2\nLayers of LSDGNN": "(cid:17)\n(cid:16)"
        },
        {
          "3.4.2\nLayers of LSDGNN": "]\nW l\nαl\n(4)\nα[H l\nj∥H l−1\nij = Softmaxj∈Ni"
        },
        {
          "3.4.2\nLayers of LSDGNN": "where W l\nα are trainable parameters, and ∥ represents the concate-"
        },
        {
          "3.4.2\nLayers of LSDGNN": "nation operation. The model collects features through relation-aware"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "transformations, enabling it\nto exploit different edge types. The ag-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "gregated features are then combined with the hidden state from the"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "previous layer using a gated recurrent unit (GRU) to compute the cur-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "rent\nlayer’s hidden state. The aggregated features M l\ni are computed"
        },
        {
          "3.4.2\nLayers of LSDGNN": "as shown in Equation (5):"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "(cid:88)"
        },
        {
          "3.4.2\nLayers of LSDGNN": "M l\nαijW l"
        },
        {
          "3.4.2\nLayers of LSDGNN": "(5)\ni =\nrij H l"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "j∈Ni"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "∈\n{W l\nwhere W l\nthe\nlearnable parameters\nfor\nthe\n0, W l\n1} are\nrij"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "relation-aware transformation. Independent learnable parameters are"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "assigned to different edge types\n(type 1 or 0). Next, as\nshown in"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "Equation (6),\nthe GRU processes\nthe hidden state and aggregated"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "H l\nfeatures to compute\ni :"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "H l\n, M l\n(6)\ni = GRUl\nH (H l−1\ni )"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "where H l−1\nand M l\nis the output of the GRU.\ni are the inputs, and ˜H l\ni"
        },
        {
          "3.4.2\nLayers of LSDGNN": "Additionally, a contextual feature unit\nis designed by swapping the"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "inputs to the GRU, allowing the hidden state from the previous layer"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "to control the propagation of contextual features. This process is de-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "scribed as shown in Equation (7):"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "C l\n)\n(7)\ni = GRUl\nM (M l\ni , H l−1"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "where C l\nis\nthe output of\nthe contextual\nfeature unit. Finally,\nthe"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "l\nnode’s representation at\nlayer\nis the sum of\nthe outputs from the"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "GRU and the contextual feature unit, as described in Equation (8):"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "H l\n(8)\ni = ˜H l\ni + C l"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "3.4.3\nFeature Fusion and Prediction"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "In a conversation, a speaker’s emotion is influenced by previous ut-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "terances, which include both short-distance and long-distance utter-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "ances. Therefore, we design the LSDGNN model, which consists of"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "two modules: one for long-distance and one for short-distance. For"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "short distances, we set ω to 1, meaning the model traces back to the"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "first self-uttered statement. Meanwhile, for long distances, we use a"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "value greater than 1."
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "Through feature extraction, we obtain the multimodal fused fea-"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "ture H 0. Then, we input H 0 into the long-distance and short-distance"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "channels, with the features denoted as H 0\nrespectively.\nL and H 0\nS,"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "These two features are processed through multiple LSDGNN lay-"
        },
        {
          "3.4.2\nLayers of LSDGNN": "ers, and after each layer,\nthe resulting features are denoted as H j"
        },
        {
          "3.4.2\nLayers of LSDGNN": "L"
        },
        {
          "3.4.2\nLayers of LSDGNN": ""
        },
        {
          "3.4.2\nLayers of LSDGNN": "and H j\nS, where j indicates the j-th layer."
        },
        {
          "3.4.2\nLayers of LSDGNN": "Inspired by Li et al.\n[15], we use mutual BiAffine transforma-"
        },
        {
          "3.4.2\nLayers of LSDGNN": "tions as a bridge to effectively exchange relevant\nfeatures between"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "0 are actually the same. In LSDGNN, at each layer, long-distance and short-distance features are processed using the\nThe original inputs HL\n0 and HS"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "Differential Regularizer and BiAffine Module. Here, i represents the i-th utterance, and j represents the features from the j-th layer."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "the long-distance and short-distance modules. Specifically, the trans-\n3.4.4\nRegularizer"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "formations are shown in Equations (9) and (10)."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "We expect the feature representations learned from the long-distance"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "′\n(cid:16)"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "and short-distance modules to capture distinct characteristics. There-\nH j\nH j\nH j\n= softmax\n(9)\nL\nLW1(H j\nS)T (cid:17)\nS"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "fore, we introduce a Differential Regularizer [15] between the adja-"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "cency matrices of the two modules, as shown in Equation (15)."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "′\n(cid:16)"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "1\nH j\nH j\nH j\n= softmax\n(10)\nSW2(H j\nL)T (cid:17)"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "S\nL\n(15)\nRD ="
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "∥Ashort − Along∥F"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "where W1 and W2 are trainable parameters."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "′\n′"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "3.4.5\nLoss Function\nFor the i-th utterance, by concatenating the features H j\nand H j\nL\nS"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "extracted from each layer through the BiAffine module with the orig-"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "Our\ntraining objective is to minimize the following total objective"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "inal\nfeatures H 0,\nthe final\nfeature representation H is obtained as"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "function shown in Equation (16):"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "shown in Equation (11)."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "′\n′\n′\n′\n′\n′\n(16)\nL = LC + λLR"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "H = concat(H 1\n, ..., H j\n, ..., H l\n, H 1\n, ..., H j\n, ..., H l\n, H 0)\n(11)\nL\nL\nS\nL\nS"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "where LC is the standard cross-entropy loss, and LR is the differen-"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "where j ranges from 1 to l, indicating the features of the j-th layer. l\ntial regularization loss. λ is the regularization coefficient, set\nto 0.1"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "represents the total number of layers in the LSDGNN.\nhere."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "The obtained features Hi of\nthe utterance ui are fed through a\nIn ERC, the cross-entropy loss is formulated as shown in Equation"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "feedforward neural network to predict\nthe\nemotion,\nas\nshown in\n(17):"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "Equations (12), (13), and (14)."
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "Ni(cid:88)"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "M(cid:88) i\nL(θ) = −\n(17)\nlog Pi,t[yi,t]\n(12)\nzi = ReLU(WH Hi + bH )"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "t=1\n=1"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "where M is the total number of samples in the training dataset. Ni"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "(13)\nPi = Softmax(Wzzi + bz)"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "denotes the number of time steps for the i-th sample. Pi,t[yi,t] is the"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "predicted probability for the true label yi,t at time step t for the i-th"
        },
        {
          "Figure 2.\nThe architecture diagram of LSDGNN. The left channel processes long-distance features, and the right channel processes short-distance features.": "(14)\nsample.\nyi = Argmaxk∈S(Pi[k])"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": "(20)"
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": "the number of emotional"
        },
        {
          "We employed a linear transformation, where the term similarity": "shifts in conversation ci and the total number of utterances in con-"
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": "is the"
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        },
        {
          "We employed a linear transformation, where the term similarity": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "tional Shifts (WES)"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "Input: D - training dataset, M - training model, k - number"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "of buckets in the training scheduler, DIF - difficulty"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "measure function, t - number of epochs, n - number of"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "utterances, e - the emotion label of the utterances,"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "p(ui) - the speaker’s corresponding utterance ui, S -"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "Set containing the emotion sequence of speakers,"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": ""
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "S[p[i]] - the emotion in the i-th utterance of speaker p,"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": ""
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "W ES - the total weighted emotional shifts of speak p"
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": ""
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "in a conversation."
        },
        {
          "Algorithm 1: Training with DMF based on Weighted Emo-": "Output: M ∗ - the optimal model"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "S[p[i]] - the emotion in the i-th utterance of speaker p,"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "W ES - the total weighted emotional shifts of speak p"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "in a conversation."
        },
        {
          "Set containing the emotion sequence of speakers,": "Output: M ∗ - the optimal model"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "1 S ← ∅, N W ES ← 0, Nsp ← 0, Nu ← 0, W ES ← 0"
        },
        {
          "Set containing the emotion sequence of speakers,": "2\nfor i = 1 to n do"
        },
        {
          "Set containing the emotion sequence of speakers,": "3\nS[p[i]] ← S[p[i]] ∪ {e[i]}"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "4\nNu ← Nu + 1"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "5 Nsp ← number of unique speakers in the conversation"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "6\nfor p ∈ S do"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "7\nfor i = 1 to length(S[p]) − 1 do"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "8\nif S[p[i]] ̸= S[p[i + 1]] then"
        },
        {
          "Set containing the emotion sequence of speakers,": "9\nsimilarity ← get_similarity(S[p[i]], S[p[i + 1]])"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "10\nN W ES ← k × similarity + b"
        },
        {
          "Set containing the emotion sequence of speakers,": "11\nW ES ← W ES + N W ES"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "12 DIF ← W ES+Nsp"
        },
        {
          "Set containing the emotion sequence of speakers,": "Nu+Nsp"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "13 D′ ← sort(D, DIF )"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "14 D′ ← {D1, D2, ..., Dk} where"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "DIF (da) < DIF (db), ∀i < j, da ∈ Di, db ∈ Dj"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "15\nfor epoch = 1 to t do"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "16\nif epoch ≤ k then"
        },
        {
          "Set containing the emotion sequence of speakers,": "17\nDtrain ← Dtrain ∪ Di"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "18\nTRAIN(M, Dtrain)"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        },
        {
          "Set containing the emotion sequence of speakers,": "19\nreturn M ∗"
        },
        {
          "Set containing the emotion sequence of speakers,": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: , with each value averaged over five",
      "data": [
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "Table 4."
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "Model"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "LSDGNN+ICL"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "w/o ICL"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "w/o ICL + DR"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "w/o ICL + BM + DR"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": "w/o ICL + Long Distance"
        },
        {
          "runs. All comparison metrics in this section are weighted F1 scores.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , with each value averaged over five",
      "data": [
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "tional training epochs are performed.",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "To investigate the importance and necessity of each module in our"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "model, we conducted ablation experiments on both datasets. The re-"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "4\nExperimental Settings",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "sults are presented in Table 4, with each value averaged over five"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "runs. All comparison metrics in this section are weighted F1 scores."
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "This section introduces the datasets and implementation details.",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "Table 4.",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "4.1\nDatasets",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "Model\nIEMOCAP"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "We evaluated our approach on two ERC datasets: IEMOCAP [3] and",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "LSDGNN+ICL\n70.24"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "MELD [21]. The detailed dataset\nstatistics are shown in Table 1.",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "w/o ICL\n68.92 (↓ 1.32)"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "We compared our method with several state-of-the-art baselines, in-",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "w/o ICL + DR\n68.70 (↓ 1.54)"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "cluding both unimodal and multimodal approaches. Consistent with",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "w/o ICL + BM + DR\n68.55 (↓ 1.69)"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "previous work,\nthe evaluation metric of our model\nis based on the",
          "5.2": "",
          "Ablation Experiments": "w/o ICL + Long Distance\n68.08 (↓ 2.56)"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "average weighted F1 score from five random runs on the test set.",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "Additionally, we include accuracy and macro F1 for supplementary",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "From the results, it can be observed that removing any part of the"
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "comparison.",
          "5.2": "",
          "Ablation Experiments": ""
        },
        {
          "the next bin is gradually added. Once all bins have been used, addi-": "",
          "5.2": "",
          "Ablation Experiments": "model leads to a decline in performance, indicating that each module"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , with each value averaged over five",
      "data": [
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "of-the-art methods,"
        },
        {
          "This section presents the comparison between our model and state-": "different values of long-distance w, as well as different values of k"
        },
        {
          "This section presents the comparison between our model and state-": "and b in Weighted Emotional Shifts on model performance."
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "5.1\nComparison with the State of the Art"
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "Table 2 and Table 3 present"
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "IEMOCAP and MELD datasets,"
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "dicating the best results among all models. We compare the models"
        },
        {
          "This section presents the comparison between our model and state-": ""
        },
        {
          "This section presents the comparison between our model and state-": "based on three evaluation metrics: weighted F1, accuracy, and macro"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2.": "Neutral",
          "Experimental results on IEMOCAP dataset": "Angry"
        },
        {
          "Table 2.": "59.21",
          "Experimental results on IEMOCAP dataset": "65.28"
        },
        {
          "Table 2.": "60.60",
          "Experimental results on IEMOCAP dataset": "68.20"
        },
        {
          "Table 2.": "63.54",
          "Experimental results on IEMOCAP dataset": "64.19"
        },
        {
          "Table 2.": "61.73",
          "Experimental results on IEMOCAP dataset": "69.00"
        },
        {
          "Table 2.": "69.36",
          "Experimental results on IEMOCAP dataset": "66.67"
        },
        {
          "Table 2.": "63.80",
          "Experimental results on IEMOCAP dataset": "69.00"
        },
        {
          "Table 2.": "69.53",
          "Experimental results on IEMOCAP dataset": "70.33"
        },
        {
          "Table 2.": "60.75",
          "Experimental results on IEMOCAP dataset": "73.51"
        },
        {
          "Table 2.": "61.50",
          "Experimental results on IEMOCAP dataset": "72.10"
        },
        {
          "Table 2.": "71.00",
          "Experimental results on IEMOCAP dataset": "69.74"
        },
        {
          "Table 2.": "Table 3.",
          "Experimental results on IEMOCAP dataset": "Experimental results on MELD dataset"
        },
        {
          "Table 2.": "Fear",
          "Experimental results on IEMOCAP dataset": "Sadness"
        },
        {
          "Table 2.": "-",
          "Experimental results on IEMOCAP dataset": "-"
        },
        {
          "Table 2.": "1.20",
          "Experimental results on IEMOCAP dataset": "23.80"
        },
        {
          "Table 2.": "-",
          "Experimental results on IEMOCAP dataset": "-"
        },
        {
          "Table 2.": "-",
          "Experimental results on IEMOCAP dataset": "-"
        },
        {
          "Table 2.": "27.87",
          "Experimental results on IEMOCAP dataset": "37.78"
        },
        {
          "Table 2.": "-",
          "Experimental results on IEMOCAP dataset": "-"
        },
        {
          "Table 2.": "18.18",
          "Experimental results on IEMOCAP dataset": "37.94"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MMGCN [10]\n-\n-\n-\n-": "57.95\n27.87\nDAG-ERC [24]\n77.26\n37.78",
          "-\n-\n-\n-\n-\n58.65": "30.36\n60.48\n49.59\n63.98\n-\n63.63"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "MultiDAG+CL [19]\n-\n-\n-\n-",
          "-\n-\n-\n-\n-\n58.65": "-\n-\n-\n-\n-\n64.00"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "LSDGNN+ICL\n77.58\n37.94\n57.90\n18.18",
          "-\n-\n-\n-\n-\n58.65": "61.70\n51.88\n64.67\n47.84\n64.07\n29.70"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "too much redundant and irrelevant context, negatively affecting the",
          "-\n-\n-\n-\n-\n58.65": "the overall results are better than those when k is negative. This in-"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "model’s effectiveness.",
          "-\n-\n-\n-\n-\n58.65": "dicates that curriculum learning has indeed played a role, while also"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "proving that\nthe more similar the changing emotions are,\nthe harder"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "they are to learn, which aligns with human intuition.\nIn addition,"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "5.4\nThe impact of parameters on Weighted Emotional",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "different values of b cause significant fluctuations in model perfor-"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "Shifts",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "mance, especially when k is -1. This further suggests that when k is"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "From Equation 19, we can see that after obtaining the similarity be-",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "negative, regardless of the value of b,\nthe defined difficulty standard"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "tween emotional shifts, we apply a linear transformation, assigning",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "makes it harder for the model to learn, thereby weakening the effect"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "different values to N W ES by controlling the slope k and the bias",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "of curriculum learning. When k is 1, all results show varying degrees"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "b. A larger N W ES indicates a higher difficulty in emotional shifts.",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "of improvement regardless of the value of b, whereas when k is -1,"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "Therefore, when k is positive, greater emotional\nsimilarity results",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "some results exhibit a significant decline."
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "in higher overall difficulty,\nleading the model\nto first\nlearn samples",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "with larger emotional differences before learning those with similar",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "emotions. Conversely, when k is negative, greater emotional differ-",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "6\nConclusion"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "ences correspond to higher overall difficulty, causing the model\nto",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "first\nlearn samples with similar emotions before moving on to those",
          "-\n-\n-\n-\n-\n58.65": ""
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "with greater differences. As shown in Table 6, we experiment with",
          "-\n-\n-\n-\n-\n58.65": "In this paper, we present a novel multimodal approach called the"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "different values of k and b and ultimately determine the optimal pa-",
          "-\n-\n-\n-\n-\n58.65": "Long-Short Distance Graph Neural Network (LSDGNN)\nfor Emo-"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "rameter combination for both datasets. All comparison metrics in this",
          "-\n-\n-\n-\n-\n58.65": "tion Recognition in Conversation (ERC). LSDGNN combines both"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "section are weighted F1 scores, averaged over five random seeds.",
          "-\n-\n-\n-\n-\n58.65": "long- and short-distance graph neural networks based on a Directed"
        },
        {
          "MMGCN [10]\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65": "Acyclic Graph (DAG)\nto extract multimodal\nfeatures\nfrom distant"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "Acyclic Graph (DAG)\nto extract multimodal\nfeatures\nfrom distant"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "and nearby utterances. To ensure effective representation of\nthese"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "features while maintaining mutual\ninfluence between the two mod-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "values of k",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "ules, we introduce a Differential Regularizer and incorporate a Bi-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "Affine Module to enhance feature interaction. Additionally, we pro-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "pose an Improved Curriculum Learning (ICL) approach to address"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "1",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "the data imbalance issue by designing a \"weighted emotional shift\""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "metric, which emphasizes\nemotional\nshifts between similar\nemo-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "tions. This difficulty measurer enables the model\nto prioritize learn-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "ing easier samples before more difficult ones. The method can also be"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "directly transferred to other ERC tasks. Experimental results on the"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "IEMOCAP and MELD datasets demonstrate that our model outper-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "-1",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "forms existing benchmarks, showcasing its effectiveness in address-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "ing the complexities of ERC tasks."
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": ""
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "In the future, we will explore more advanced transformation meth-"
        },
        {
          "section are weighted F1 scores, averaged over five random seeds.": "",
          "long- and short-distance graph neural networks based on a Directed": "ods\nfor weighted emotional\nshifts\nand investigate\nadaptive graph"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Der-gcn: Dialog and event relationaware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": "10.1109/TNNLS.2024.3367940"
    },
    {
      "citation_id": "2",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09",
      "doi": "10.1145/1553374.1553380"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S19-2005"
    },
    {
      "citation_id": "5",
      "title": "LaERC-S: Improving LLM-based emotion recognition in conversation with speaker characteristics",
      "authors": [
        "Y Fu",
        "J Wu",
        "Z Wang",
        "M Zhang",
        "L Shan",
        "Y Wu",
        "B Liu"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "7",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "8",
      "title": "Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai",
        "Dialoguecrn"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "9",
      "title": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "authors": [
        "G Hu",
        "Y Xin",
        "W Lyu",
        "H Huang",
        "C Sun",
        "Z Zhu",
        "L Gui",
        "R Cai"
      ],
      "year": "2024",
      "venue": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "arxiv": "arXiv:2409.07388"
    },
    {
      "citation_id": "10",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "11",
      "title": "EmoTrans: Emotional transition-based model for emotion recognition in conversation",
      "authors": [
        "Z Jian",
        "A Wang",
        "J Su",
        "J Yao",
        "M Wang",
        "Q Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "12",
      "title": "An automatic speech discrete labels to dimensional emotional values conversion method",
      "authors": [
        "S Jing",
        "X Mao",
        "L Chen"
      ],
      "year": "2018",
      "venue": "IET Biometrics",
      "doi": "10.1049/iet-bmt.2018.5016"
    },
    {
      "citation_id": "13",
      "title": "Study on emotion recognition and companion chatbot using deep neural network",
      "authors": [
        "M.-C Lee",
        "S.-Y Chiang",
        "S.-C Yeh",
        "T.-F Wen"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "14",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "15",
      "title": "Dual graph convolutional networks for aspect-based sentiment analysis",
      "authors": [
        "R Li",
        "H Chen",
        "F Feng",
        "Z Ma",
        "X Wang",
        "E Hovy"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.494"
    },
    {
      "citation_id": "16",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "17",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "18",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence",
      "doi": "10.1109/TAI.2024.3445325"
    },
    {
      "citation_id": "19",
      "title": "Curriculum learning meets directed acyclic graph for multimodal emotion recognition",
      "authors": [
        "C.-V Nguyen",
        "C.-B Nguyen",
        "D.-T Le",
        "Q.-T Ha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "20",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1982",
      "venue": "Social Science Information",
      "doi": "10.1177/053901882021004003"
    },
    {
      "citation_id": "21",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "22",
      "title": "Bert-erc: fine-tuning bert is enough for emotion recognition in conversation",
      "authors": [
        "X Qin",
        "Z Wu",
        "T Zhang",
        "Y Li",
        "J Luan",
        "B Wang",
        "L Wang",
        "J Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI'23/IAAI'23/EAAI'23",
      "doi": "10.1609/aaai.v37i11.26582"
    },
    {
      "citation_id": "23",
      "title": "Lr-gcn: Latent relationaware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3117062"
    },
    {
      "citation_id": "24",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Directed acyclic graph neural networks",
      "authors": [
        "V Thost",
        "J Chen"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph neural networks",
      "arxiv": "arXiv:2101.07965"
    },
    {
      "citation_id": "26",
      "title": "ESCP: Enhancing emotion recognition in conversation with speech and contextual prefixes",
      "authors": [
        "X Xu",
        "X Shi",
        "Z Zhao",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "27",
      "title": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "authors": [
        "J Xue",
        "P Nguyen",
        "B Matheny",
        "L Nguyen"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v36i10.21413"
    }
  ]
}