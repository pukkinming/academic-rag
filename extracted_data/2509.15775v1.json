{
  "paper_id": "2509.15775v1",
  "title": "Emoq: Speech Emotion Recognition Via Speech-Aware Q-Former And Large Language Model",
  "published": "2025-09-19T09:02:03Z",
  "authors": [
    "Yiqing Yang",
    "Man-Wai Mak"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Multimodal Large Language Model",
    "Cross-modal Alignment ùëæ#",
    "%&'(((k)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The performance of speech emotion recognition (SER) is limited by the insufficient emotion information in unimodal systems and the feature alignment difficulties in multimodal systems. Recently, multimodal large language models (MLLMs) have made progress in SER. However, MLLMs still suffer from hallucination and misclassification problems in complex emotion reasoning. To address these problems, we propose an MLLM-based framework called EmoQ, which generates query embeddings that fuse multimodal information through an EmoQ-Former and uses multi-objective affective learning (MAL) to achieve co-optimization. The framework also provides a soft-prompt injection strategy to inject multimodal representations into the LLM. This end-to-end architecture achieves state-of-the-art performance on the IEMOCAP and MELD datasets, providing a new multimodal fusion paradigm for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) realizes automatic recognition of speakers' emotional states by parsing the emotional features in speech signals. However, the single modality approach has intrinsic limitations: due to the ambiguity in the definition of emotion, coupled with the complexity of linguistic expressions, it is difficult to adequately capture emotion cues at the semantic level by relying on acoustic information alone.\n\nAudio-based SER often lacks semantic reasoning, leading to erroneous judgments. In real-world scenarios, speech signals can be accompanied by text transcriptions through automatic speech recognition (ASR), providing a basis for multimodal fusion. Recent studies have shown that the audio-text multimodal architecture can improve the performance of emotion recognition  [11, 20] . Text information can provide semantic complements to audio features, and multimodal fusion has become a key to improve SER performance.\n\nDespite the performance gains from multimodal fusion, several critical challenges remain. First, cross-modal alignment presents a fundamental challenge  [2] . Audio signals exhibit continuous temporal dynamics, whereas transcribed texts comprise sequences of discrete tokens, creating temporal misalignment that impedes acousticsemantic modeling. Second, modal imbalance emerges during multimodal training, where one modality tends to dominate the optimization process. Third, feature space heterogeneity poses integration difficulties  [29] . How exciting.\n\nFig.  1 . The EmoQ framework employs an end-to-end multimodal learning approach that utilizes an LLM for emotion classification.\n\nAs multimodal large language models (MLLMs) have demonstrated their capabilities in multimodal reasoning  [2, 6, 30] , researchers have started to apply them to SER. For example, SALMO-NN  [27]  uses a dual-encoder architecture combined with Q-Former  [2]  and LoRA  [3]  to achieve audio-text alignment. SECap  [28]  further explores the possibility of using LLMs for speech emotion captioning. The authors of LLM-CL  [22]  proposed a zero-shot multilingual speech emotion recognition framework based on contrastive learning. These MLLMs achieve performance gains on SER benchmarks, demonstrating the potential of the MLLM architectures for speech emotion understanding.\n\nInspired by the aforementioned works, we propose EmoQ as shown in Fig.  1 . EmoQ addresses the challenges in SER through three innovations:\n\n1) EmoQ provides a cross-modal alignment mechanism, where the EmoQ-Former achieves speech-to-text feature fusion through learnable queries and attention masks to overcome temporal misalignment and feature space heterogeneity.\n\n2) EmoQ employs a multi-objective affective learning (MAL) framework that combines supervised contrastive learning  [9]  and focal loss  [4]  for co-optimization.\n\n3) EmoQ introduces a soft-prompt injection strategy to replace the <AUDIO> token in the prompt with fused representations, enabling the LLM to simultaneously utilize both language comprehension and cross-modal emotion cues for inference.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "EmoQ is based on the MLLM framework as shown in Fig.  2 . The framework consists of an audio encoder, a Q-Former-based bridgenet, and an LLM decoder. The EmoQ-Former is inspired by BLIP-2  [2] , and it is responsible for aligning acoustic and textual features to generate a fused representation. This representation is injected into the LLM as soft-prompts to determine the emotion state of an input utterance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Learned Queries Transcription",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "‚ùÑ üî•",
      "text": "Input Audio Prompt   (3) an LLM decoder that uses an LLM to map the fused multimodal representations and tokenized prompts to emotion categories.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emoq-Former",
      "text": "Before entering the EmoQ-Former, we preprocess the input for the audio and text modalities. For the audio modality a, we use the pretrained HuBERT model  [1]  to extract the audio embeddings Ea ‚àà R La√óda , where La is the number of audio frames and da is the feature dimension. We map the audio embeddings to E ‚Ä≤ a ‚àà R La√ód h by a linear projection layer, aligned with the hidden dimension d h of the query vectors. For the corresponding transcribed text t, we use a tokenizer to convert it to a text embedding sequence Et ‚àà R L t √ód h , where Lt is the number of tokens.\n\nWe concatenate the learned query vectors Q ‚àà R Nq √ód h with the text embeddings Et to form a joint sequence X:\n\nThis sequence is processed by a self-attention mechanism such that the query vectors and text embeddings can attend to each other, allowing the query vectors to absorb the semantic information. Specially, we have:\n\nwhere WQ, WK , and WV are the weight matrices of the selfattention mechanism, and d k is the dimension of the key vectors. Next, we feed the matrix Q ‚Ä≤ ‚àà R Nq √ód h , which is the query portion of the self-attended matrix Aself, into the cross-attention module, allowing the vectors in this portion to attend to the projected audio embeddings E ‚Ä≤ a :\n\nwhere WQ,cross, WK,cross, and WV,cross are the weight matrices of the cross-attention, and M ‚àà R Nq √óLa is an attention mask matrix to handle variable-length audio sequences. Specifically, M1:N q ,j = 1 if the time position j corresponds to valid audio content; otherwise, M1:N q ,j = 0 to accommodate padded time positions beyond the actual audio length. We then present the cross-attended vectors {Across,i} Nq i=1 ‚àà R d h to a feed forward network (FFN), followed by passing the resulting vectors to an attentive pooling layer with multiple attention heads to obtain eq. Finally, we apply L2-norm to normalize eq to obtain the final embedding e ‚Ä≤ q .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Objective Affective Learning (Mal)",
      "text": "We propose a multi-objective affective learning framework that combines supervised contrastive loss  [9]  and focal loss  [4]  to make the cross-modal embeddings more discriminative and to handle the class imbalance issue in speech emotion recognition. Supervised contrastive loss leverages the label information to reinforce the principle that samples of the same class will be close to each other in the representation space and that samples from different classes will be far apart. It enhances the quality of the feature representation by using a variant of InfoNCE loss:\n\nwhere ei denotes the L2-normalized cross-modal feature vector e ‚Ä≤ q in Fig.  2  for the i-th sample in a mini-batch of size N . P (i) contains the indexes of positive samples with the same emotion label as sample i, A(i) contains the indexes of all samples in the batch except sample i, and œÑ is a temperature parameter.\n\nTo solve the category imbalance problem in emotion classification, we use the focal loss to reduce the contribution of easyto-categorize samples and enhance the learning weights of difficult samples. For each sample i with embedding ei and true label yi ‚àà {1, ..., C} where C is the number of emotion classes, we compute the logits zi ‚àà R C through a classification head to obtain the posterior probability vector pi ‚àà R C . The focal loss is computed as:\n\nwhere pi,y i is the yi-th element of pi, Œ±y i ‚àà [0, 1] is the precomputed weight for class yi, and Œ≥ is the focus parameter.\n\nMAL achieves the enhancement of emotion information by multi-objective learning:\n\nwhere Œª is a balancing factor.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Mllm Instruction Tuning For Emotion Recognition",
      "text": "For each sample i, we map e ‚Ä≤ q to a representation e h ‚àà R d l that is consistent with the dimensionality of the LLM word-embedding space through a projector. We then design a structured prompt template that instructs the LLM to act as an emotion recognition system. The user input section contains the speaker information (if available), the transcribed text, and the multimodal features represented by a special <AUDIO> placeholder. The template also concludes with an assistant response format that begins with \"Emotion:\" to ensure consistent output generation. Fig.  2  (right panel) gives an example of the prompt template. Before feeding the text into the LLM, we replace the embedding vector of the <AUDIO> placeholder with the projected vector e h .\n\nWe define the fine-tuning process of the LLM as a standard autoregressive language modeling task. The goal of the model is to maximize the probability of generating a sequence of correct responses given the input context:\n\nwhere N is the batch size and L is the length of the target sequence.\n\nxi is the complete input sequence containing soft-prompts, e h , and the token embeddings, and ti,j is the true token of sample i at position j. P (ti,j|ti,<j, xi) is the conditional probability of generating the correct token. In order to make the model focus on learning emotion keywords, we introduce the weights wij's at the token level. When ti,j is a predefined emotion word, its weight is set to a high value. During inference, we extract the logits of the predefined emotion words from the hidden state hT,i of the last token of sample i's answer sequence. We compute the logits over the entire vocabulary of the LLM:\n\nwhere Wo is the output vocabulary projection matrix of the LLM, and |V | is the vocabulary size. We then extract the logits corresponding to the C emotion categories:\n\nwhere kc denotes the vocabulary index of the c-th emotion word, and qi contains posterior probabilities of the C emotion states given the i-th utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Process",
      "text": "We designed a two-stage training process. First, we focus on the training of EmoQ-Former to equip it with multimodal feature fusion capability. Second, we align the multimodal information with the semantic space of the LLM to fine-tune the whole system in an endto-end manner. The goal of the first phase is to train the EmoQ-Former to be able to fuse acoustic features E ‚Ä≤ a and text embeddings Et into a highly discriminative multimodal embedding e ‚Ä≤ q . At this stage, we freeze the parameters of the pre-trained HuBERT. The output e ‚Ä≤ q of the EmoQ-Former is fed into a lightweight auxiliary MLP for emotion classification. The EmoQ-Former and the auxiliary MLP head are optimized by our proposed MAL (Eq. 6). After training, we save the weights of EmoQ-Former and discard the auxiliary MLP head.\n\nIn the second stage, we load the EmoQ-Former's weights pretrained in the first stage. Then, the EmoQ-Former is connected to the projector and the LLM to constitute the complete end-to-end model in Fig.  2 . We use Low-Rank Adaptation (LoRA)  [3]  for fine-tuning of the LLM, where the parameters of the EmoQ-Former and projector are co-trained with the LoRA parameters of the LLM. The whole model (EmoQ-Former and LLM) uses the LCE in Eq. 7 as the optimization objective.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets Details",
      "text": "The IEMOCAP  [7]  dataset is an authoritative benchmark dataset in the field of multimodal affective computing, which consists of conversation clips recorded by 10 native English speakers in a controlled environment. We chose four main emotion categories: neutral, angry, sadness, and happy, covering 5531 utterances. Among them, the category of happy integrates the labels of happy and excited from the original dataset. The dataset is divided using speaker-independent five-fold cross-validation.\n\nThe MELD  [8]  dataset was derived from the dialog clips of the TV series Friends and contains 13,708 utterances. This dataset provides pre-partitioned training, validation, and test sets containing 9989, 1109, and 2610 samples, respectively. We designed two sets of experimental configurations: a four-category experiment with emotion categories neutral, angry, sadness, and joy; and a seven-category experiment that uses the complete emotion labels of the dataset, including neutral, angry, sadness, joy, surprise, disgust, and fear.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "The audio encoder used the pre-trained HuBERT-large model, and the parameters were kept frozen during the training process. The EmoQ-Former comes from the pre-trained RoBERTa-base  [5] . The LLM decoder used the Qwen2.5-7B-Instruct  [6] , and adopted the LoRA fine-tuning strategy, where the rank parameter r = 16 and dropout rate were set to 0.1. The temperature parameter œÑ for LSCL and the focus parameter for L focal were set to 0.07 and 2.0, respectively. The class weights Œ±'s in Eq. 5 were adaptively computed based on the training set's class distribution. The balancing factor Œª was set to 1 in LMAL. The optimizer was chosen to be AdamW  [10] , with learning rate set to 1 √ó 10 -5 and a batch size of 8. We utilized the officially provided ground-truth transcriptions as the text input. We use a Weighted Accuracy (WA), Unweighted Accuracy (UA), and weighted F1-score as evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Main Results And Analysis",
      "text": "On the IEMOCAP four-category task, as shown in Table  1 , EmoQ improves over the best model by 2.0 and 0.5 percentage points on WA and UA respectively. On the more challenging MELD dataset, EmoQ shows even greater advantages. As shown in Table  2 , our model achieves 73.6% WA and 71.6% WF1. In the seven-category task (Table  3 ), EmoQ leads by 14.1, 23.5, and 17.2 percentage points in WA, UA, and WF1, respectively.\n\nEmoQ shows significantly greater improvement on MELD than on IEMOCAP, a phenomenon that stems from two designs. In the \"-\" indicates the metric was not reported in the original paper.\n\nfirst stage of training, EmoQ-Former learns a highly discriminative cross-modal emotional representation via the auxiliary classification head, ensuring that the multimodal representation is suitable for the emotion recognition task and provides emotional cues for the LLM to provide correct responses. In the second stage of training, we provided the LLM with both the complete transcribed text and the emotional soft-prompt representation described above.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "We verified the effect of different input modalities on the performance of SER, as shown in Table  4  The complete Audio and Text with EmoQ-Former generated a fusion representation e ‚Ä≤ q via EmoQ-Former, which was mapped to e h by the projector and injected into LLM, obtaining 67.6% WA and 50.8% UA. This demonstrates the effectiveness of EmoQ-Former in cross-modal feature fusion.\n\nWe analyzed the contribution of each component in the MAL framework to the model performance, as detailed in Table  5 . Both focal loss and SCL individually improve performance over the baseline, with focal loss being particularly effective against the class imbalance of the MELD dataset. The full MAL configuration combining both losses achieves the best performance.\n\nThe effectiveness of the two-stage training strategy is verified by comparing the end-to-end training against the two-stage training, as shown in Table  6 . End-to-end training (1st row) directly optimizes the full model, whereas two-stage training (2nd row) pre-trains the EmoQ-Former via an auxiliary MLP head followed by LLM finetuning. Evidently, the results show that two-stage training is superior, as it allows the EmQ-Former to learn a high-quality cross-modal representation for the LLM model to produce correct responses for the input prompts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the EmoQ framework, which effectively fuses audio information and textual semantics through EmoQ-Former and leverages multi-objective affective learning. Experimental results show that EmoQ achieves state-of-the-art performance on both IEMOCAP and MELD datasets. This study demonstrates the potential of MLLMs in emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The EmoQ framework employs an end-to-end multimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: EmoQ addresses the challenges in SER through",
      "page": 1
    },
    {
      "caption": "Figure 2: The architecture of EmoQ. It adopts end-to-end multimodal learning and contains three core modules: (1) An audio encoder that",
      "page": 2
    },
    {
      "caption": "Figure 2: for the i-th sample in a mini-batch of size N. P(i) contains",
      "page": 2
    },
    {
      "caption": "Figure 2: (right panel) gives an ex-",
      "page": 3
    },
    {
      "caption": "Figure 2: We use Low-Rank Adaptation (LoRA) [3] for fine-tuning",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": "Query Part ùë∏‚Äô"
        },
        {
          "Feed Forward\nFeed Forward": "Cross-Attention ùë®%&‚Äô!!"
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": "Self-Attention ùë®!\"#$\nSelf-Attention\tùë®!\"#$"
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": "Joint Sequence ùëø"
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": ""
        },
        {
          "Feed Forward\nFeed Forward": "Learned Queries ùë∏\nTranscription Embedding ùë¨ùíï"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Emotion: ‚Äù"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Learned Queries ùë∏\nTranscription Embedding ùë¨ùíï"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "We'll have so much fun."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "This work has been submitted to the IEEE for possible publica-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "üî•\n‚ùÑ"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "tion. Copyright may be transferred without notice, after which",
          "utterance?\nProjector": "Large \nAudio\nEmoQ-Former"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Language\nüòêüòÑüò°üò≠"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "this version may no longer be accessible.",
          "utterance?\nProjector": "Encoder\n(Emotion Querying Transformer)\nModel"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Emotion Label"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "ABSTRACT",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "¬∑¬∑¬∑\nHow exciting.\n¬∑¬∑¬∑"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "The performance of\nspeech emotion recognition (SER)\nis\nlimited",
          "utterance?\nProjector": "EmoQ\nInput Audio\nLearned Queries\nTranscription\nPrompt"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "by the insufficient emotion information in unimodal\nsystems and",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "the feature alignment difficulties in multimodal systems. Recently,",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Fig. 1. The EmoQ framework employs an end-to-end multimodal"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "multimodal large language models (MLLMs) have made progress in",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "learning approach that utilizes an LLM for emotion classification."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "SER. However, MLLMs still suffer from hallucination and misclas-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "sification problems in complex emotion reasoning. To address these",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "As multimodal\nlarge language models (MLLMs) have demon-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "problems, we propose an MLLM-based framework called EmoQ,",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "strated their\ncapabilities\nin multimodal\nreasoning [2, 6, 30],\nre-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "which generates query embeddings that fuse multimodal information",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "searchers have started to apply them to SER. For example, SALMO-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "through an EmoQ-Former and uses multi-objective affective learn-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "NN [27] uses a dual-encoder architecture combined with Q-Former"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "ing (MAL) to achieve co-optimization. The framework also provides",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "[2] and LoRA [3]\nto achieve audio-text alignment.\nSECap [28]"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "a soft-prompt injection strategy to inject multimodal representations",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "further explores the possibility of using LLMs for speech emotion"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "into the LLM. This end-to-end architecture achieves state-of-the-art",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "captioning.\nThe\nauthors of LLM-CL [22] proposed a\nzero-shot"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "performance on the IEMOCAP and MELD datasets, providing a new",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "multilingual speech emotion recognition framework based on con-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "multimodal fusion paradigm for SER.",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "trastive learning. These MLLMs achieve performance gains on SER"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "Index\nTerms‚Äî Speech\nEmotion Recognition, Multimodal",
          "utterance?\nProjector": "benchmarks, demonstrating the potential of the MLLM architectures"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "Large Language Model, Cross-modal Alignment",
          "utterance?\nProjector": "for speech emotion understanding."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "Inspired by the aforementioned works, we propose EmoQ as"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "1.\nINTRODUCTION",
          "utterance?\nProjector": "shown in Fig.\n1. EmoQ addresses the challenges in SER through"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "three innovations:"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "Speech emotion recognition (SER)\nrealizes automatic recognition",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "1) EmoQ provides a cross-modal alignment mechanism, where"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "of speakers‚Äô emotional states by parsing the emotional\nfeatures in",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "the EmoQ-Former achieves\nspeech-to-text\nfeature fusion through"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "speech signals. However, the single modality approach has intrinsic",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "learnable queries and attention masks\nto overcome temporal mis-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "limitations: due to the ambiguity in the definition of emotion, cou-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "alignment and feature space heterogeneity."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "pled with the complexity of\nlinguistic expressions,\nit\nis difficult\nto",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "adequately capture emotion cues at the semantic level by relying on",
          "utterance?\nProjector": "2) EmoQ employs a multi-objective affective learning (MAL)"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "acoustic information alone.",
          "utterance?\nProjector": "framework that combines supervised contrastive learning [9] and fo-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "Audio-based SER often lacks semantic reasoning, leading to er-",
          "utterance?\nProjector": "cal loss [4] for co-optimization."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "roneous judgments.\nIn real-world scenarios, speech signals can be",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "3) EmoQ introduces a soft-prompt\ninjection strategy to replace"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "accompanied by text transcriptions through automatic speech recog-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "the <AUDIO> token in the prompt with fused representations, en-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "nition (ASR), providing a basis for multimodal fusion. Recent stud-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "abling the LLM to simultaneously utilize both language comprehen-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "ies have shown that\nthe audio-text multimodal architecture can im-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "sion and cross-modal emotion cues for inference."
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "prove the performance of emotion recognition [11, 20]. Text\ninfor-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "mation can provide semantic complements\nto audio features, and",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "multimodal fusion has become a key to improve SER performance.",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "Despite the performance gains from multimodal fusion, several",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "critical challenges remain. First, cross-modal alignment presents a",
          "utterance?\nProjector": "2. METHOD"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "fundamental challenge [2]. Audio signals exhibit continuous tempo-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "ral dynamics, whereas transcribed texts comprise sequences of dis-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "EmoQ is based on the MLLM framework as shown in Fig. 2. The"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "crete tokens, creating temporal misalignment that impedes acoustic-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "framework consists of an audio encoder, a Q-Former-based bridge-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "semantic modeling. Second, modal imbalance emerges during mul-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "net, and an LLM decoder. The EmoQ-Former is inspired by BLIP-"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "timodal\ntraining, where one modality tends\nto dominate the opti-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "2 [2], and it is responsible for aligning acoustic and textual features"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "mization process. Third, feature space heterogeneity poses integra-",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "to generate a fused representation.\nThis representation is injected"
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "tion difficulties [29].",
          "utterance?\nProjector": ""
        },
        {
          "1Department of Electrical and Electronic Engineering, The Hong Kong Polytechnic University": "",
          "utterance?\nProjector": "into the LLM as soft-prompts to determine the emotion state of an"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": "H ‚Äì Embedding ùíÜùíâ"
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": "Projector"
        },
        {
          "Token Embedding": ""
        },
        {
          "Token Embedding": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "What emotion is expressed?<|im_end|>": "<|im_start|>assistant\n¬∑¬∑¬∑"
        },
        {
          "What emotion is expressed?<|im_end|>": "Projector\n¬∑¬∑¬∑"
        },
        {
          "What emotion is expressed?<|im_end|>": "Emotion: ?<|im_end|>\nAudio Embedding ùë¨‚ÄôùíÇ"
        },
        {
          "What emotion is expressed?<|im_end|>": "Learned Queries ùë∏\nTranscription Embedding ùë¨ùíï"
        },
        {
          "What emotion is expressed?<|im_end|>": "Fig. 2. The architecture of EmoQ.\nIt adopts end-to-end multimodal\nlearning and contains three core modules:\n(1) An audio encoder\nthat"
        },
        {
          "What emotion is expressed?<|im_end|>": "employs HuBERT to encode the features of speech signals in the form of audio embeddings; (2) an EmoQ-Former that realizes cross-modal"
        },
        {
          "What emotion is expressed?<|im_end|>": "fusion of audio and text by learnable queries and an attention masking strategy; and (3) an LLM decoder that uses an LLM to map the fused"
        },
        {
          "What emotion is expressed?<|im_end|>": "multimodal representations and tokenized prompts to emotion categories."
        },
        {
          "What emotion is expressed?<|im_end|>": "We'll have so much fun.\nüî•\n‚ùÑ"
        },
        {
          "What emotion is expressed?<|im_end|>": "Large"
        },
        {
          "What emotion is expressed?<|im_end|>": "Audio\nEmoQ-Former"
        },
        {
          "What emotion is expressed?<|im_end|>": "Language\nüòêüòÑüò°üò≠"
        },
        {
          "What emotion is expressed?<|im_end|>": "Encoder\n(Emotion Querying Transformer)"
        },
        {
          "What emotion is expressed?<|im_end|>": "Model"
        },
        {
          "What emotion is expressed?<|im_end|>": "2.1. EmoQ-Former\nthe actual audio length. We then present\nthe cross-attended vectors"
        },
        {
          "What emotion is expressed?<|im_end|>": "to a feed forward network (FFN),\nfollowed\n{Across,i}Nq\ni=1 ‚àà Rdh"
        },
        {
          "What emotion is expressed?<|im_end|>": "Before entering the EmoQ-Former, we preprocess the input for the"
        },
        {
          "What emotion is expressed?<|im_end|>": "by passing the resulting vectors to an attentive pooling layer with"
        },
        {
          "What emotion is expressed?<|im_end|>": "audio and text modalities. For the audio modality a, we use the pre-"
        },
        {
          "What emotion is expressed?<|im_end|>": "¬∑¬∑¬∑\nHow exciting.\n¬∑¬∑¬∑\nmultiple attention heads to obtain eq. Finally, we apply L2-norm to"
        },
        {
          "What emotion is expressed?<|im_end|>": "trained HuBERT model [1] to extract\nthe audio embeddings Ea ‚àà"
        },
        {
          "What emotion is expressed?<|im_end|>": "normalize eq to obtain the final embedding e‚Ä≤"
        },
        {
          "What emotion is expressed?<|im_end|>": "q.\nEmoQ\nTranscription\nthe\nPrompt\nRLa√óda , where La\nthe number of audio frames and da"
        },
        {
          "What emotion is expressed?<|im_end|>": "feature dimension. We map the audio embeddings to E‚Ä≤\na ‚àà RLa√ódh"
        },
        {
          "What emotion is expressed?<|im_end|>": "2.2. Multi-objective Affective Learning (MAL)"
        },
        {
          "What emotion is expressed?<|im_end|>": "by a linear projection layer, aligned with the hidden dimension dh of"
        },
        {
          "What emotion is expressed?<|im_end|>": "the query vectors. For the corresponding transcribed text t, we use a"
        },
        {
          "What emotion is expressed?<|im_end|>": "We propose a multi-objective affective learning framework that com-"
        },
        {
          "What emotion is expressed?<|im_end|>": "tokenizer to convert it to a text embedding sequence Et ‚àà RLt√ódh ,"
        },
        {
          "What emotion is expressed?<|im_end|>": "bines supervised contrastive loss [9] and focal\nloss [4] to make the"
        },
        {
          "What emotion is expressed?<|im_end|>": "where Lt is the number of tokens."
        },
        {
          "What emotion is expressed?<|im_end|>": "cross-modal embeddings more discriminative and to handle the class"
        },
        {
          "What emotion is expressed?<|im_end|>": "We concatenate the learned query vectors Q ‚àà RNq √ódh with"
        },
        {
          "What emotion is expressed?<|im_end|>": "imbalance issue in speech emotion recognition.\nSupervised con-"
        },
        {
          "What emotion is expressed?<|im_end|>": "the text embeddings Et to form a joint sequence X:"
        },
        {
          "What emotion is expressed?<|im_end|>": "trastive loss leverages the label information to reinforce the principle"
        },
        {
          "What emotion is expressed?<|im_end|>": "that samples of the same class will be close to each other in the rep-"
        },
        {
          "What emotion is expressed?<|im_end|>": "(1)\nX = Concat(Q, Et) ‚àà R(Nq +Lt)√ódh ."
        },
        {
          "What emotion is expressed?<|im_end|>": "resentation space and that samples from different classes will be far"
        },
        {
          "What emotion is expressed?<|im_end|>": "apart.\nIt enhances the quality of the feature representation by using"
        },
        {
          "What emotion is expressed?<|im_end|>": "This sequence is processed by a self-attention mechanism such that"
        },
        {
          "What emotion is expressed?<|im_end|>": "a variant of InfoNCE loss:"
        },
        {
          "What emotion is expressed?<|im_end|>": "the query vectors and text embeddings can attend to each other, al-"
        },
        {
          "What emotion is expressed?<|im_end|>": "lowing the query vectors to absorb the semantic information. Spe-"
        },
        {
          "What emotion is expressed?<|im_end|>": "1\nexp(ei ¬∑ ej/œÑ )\n(cid:88)"
        },
        {
          "What emotion is expressed?<|im_end|>": "1 N\nN(cid:88) i\n,\nlog\n(4)\ncially, we have:\nLSCL = ‚àí"
        },
        {
          "What emotion is expressed?<|im_end|>": "(cid:80)"
        },
        {
          "What emotion is expressed?<|im_end|>": "k‚ààA(i) exp(ei ¬∑ ek/œÑ )\n=1"
        },
        {
          "What emotion is expressed?<|im_end|>": "j‚ààP (i)"
        },
        {
          "What emotion is expressed?<|im_end|>": "(cid:19)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "multi-objective learning:",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "tion classification. The EmoQ-Former and the auxiliary MLP head"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "are optimized by our proposed MAL (Eq. 6). After training, we save"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "(6)\nLMAL = LSCL + ŒªLfocal,",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "the weights of EmoQ-Former and discard the auxiliary MLP head."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "In the second stage, we load the EmoQ-Former‚Äôs weights pre-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "where Œª is a balancing factor.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "trained in the first stage. Then, the EmoQ-Former is connected to the"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "projector and the LLM to constitute the complete end-to-end model"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "2.3. MLLM Instruction Tuning for Emotion Recognition",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "in Fig. 2. We use Low-Rank Adaptation (LoRA) [3] for fine-tuning"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "of the LLM, where the parameters of the EmoQ-Former and projec-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "For each sample i, we map e‚Ä≤\nthat\nto a representation eh ‚àà Rdl",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "q",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "tor are co-trained with the LoRA parameters of the LLM. The whole"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "is consistent with the dimensionality of the LLM word-embedding",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "model (EmoQ-Former and LLM) uses the LCE in Eq. 7 as the opti-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "space through a projector. We then design a structured prompt tem-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "mization objective."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "plate that instructs the LLM to act as an emotion recognition system.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "The user\ninput\nsection contains\nthe speaker\ninformation (if avail-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "able),\nthe transcribed text, and the multimodal features represented",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "3. EXPERIMENTS"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "by a special <AUDIO> placeholder.\nThe template also concludes",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "3.1. Datasets details"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "with an assistant response format that begins with ‚ÄúEmotion:‚Äù to en-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "sure consistent output generation. Fig. 2 (right panel) gives an ex-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "The IEMOCAP [7] dataset\nis an authoritative benchmark dataset\nin"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "ample of the prompt template. Before feeding the text into the LLM,",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "the field of multimodal affective computing, which consists of con-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "we replace the embedding vector of the <AUDIO> placeholder with",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "versation clips recorded by 10 native English speakers in a controlled"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "the projected vector eh.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "environment. We chose four main emotion categories: neutral, an-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "We define the fine-tuning process of the LLM as a standard au-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "gry, sadness, and happy, covering 5531 utterances. Among them, the"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "toregressive language modeling task.\nThe goal of\nthe model\nis to",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "category of happy integrates the labels of happy and excited from the"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "maximize the probability of generating a sequence of correct\nre-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "original dataset. The dataset\nis divided using speaker-independent"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "sponses given the input context:",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "five-fold cross-validation."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "The MELD [8] dataset was derived from the dialog clips of the"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "TV series Friends and contains 13,708 utterances. This dataset pro-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "1 N\nN(cid:88) i\nL(cid:88) j\n(7)\nwij log P (ti,j|ti,<j, xi),\nLCE = ‚àí",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "=1\n=1",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "vides pre-partitioned training, validation,\nand test\nsets containing"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "9989, 1109, and 2610 samples, respectively. We designed two sets of"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "where N is the batch size and L is the length of the target sequence.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "experimental configurations: a four-category experiment with emo-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "xi\nis the complete input sequence containing soft-prompts, eh, and",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "tion categories neutral, angry, sadness, and joy; and a seven-category"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "is the true token of sample i at po-\nthe token embeddings, and ti,j",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "experiment\nthat uses the complete emotion labels of the dataset,\nin-"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "sition j. P (ti,j|ti,<j, xi) is the conditional probability of generat-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "cluding neutral, angry, sadness, joy, surprise, disgust, and fear."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "ing the correct token.\nIn order to make the model focus on learning",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "emotion keywords, we introduce the weights wij‚Äôs at the token level.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "3.2. Experiment Setup"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "is a predefined emotion word,\nits weight\nis set\nto a high\nWhen ti,j",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "value. During inference, we extract the logits of the predefined emo-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "The audio encoder used the pre-trained HuBERT-large model, and"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "tion words from the hidden state hT,i of the last token of sample i‚Äôs",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "the parameters were kept\nfrozen during the training process.\nThe"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "answer sequence. We compute the logits over the entire vocabulary",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "EmoQ-Former comes from the pre-trained RoBERTa-base [5]. The"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "of the LLM:",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "LLM decoder used the Qwen2.5-7B-Instruct\n[6], and adopted the"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "(8)\nli = hT,iWo ‚àà R|V |,",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "LoRA fine-tuning strategy, where the rank parameter r = 16 and"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "dropout rate were set\nto 0.1. The temperature parameter œÑ for LSCL"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "where Wo is the output vocabulary projection matrix of the LLM,",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "to 0.07 and 2.0, respec-\nand the focus parameter for Lfocal were set"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "and |V | is the vocabulary size. We then extract the logits correspond-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "tively.\nThe class weights Œ±‚Äôs in Eq. 5 were adaptively computed"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "ing to the C emotion categories:",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "based on the training set‚Äôs class distribution. The balancing factor Œª"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "was set to 1 in LMAL. The optimizer was chosen to be AdamW [10],"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "(9)\nqi = Softmax([li,k1 , li,k2 , ..., li,kC ]) ‚àà RC ,",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": ""
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "with learning rate set to 1 √ó 10‚àí5 and a batch size of 8. We utilized"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "where kc denotes the vocabulary index of the c-th emotion word, and",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "the officially provided ground-truth transcriptions as the text\ninput."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "qi contains posterior probabilities of the C emotion states given the",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "We use a Weighted Accuracy (WA), Unweighted Accuracy (UA),"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "i-th utterance.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "and weighted F1-score as evaluation metrics."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "2.4. Training Process",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "3.3. Main Results and Analysis"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "We designed a two-stage training process.\nFirst, we focus on the",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "On the IEMOCAP four-category task, as shown in Table 1, EmoQ"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "training of EmoQ-Former to equip it with multimodal feature fusion",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "improves over\nthe best model by 2.0 and 0.5 percentage points on"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "capability.\nSecond, we align the multimodal\ninformation with the",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "WA and UA respectively. On the more challenging MELD dataset,"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "semantic space of the LLM to fine-tune the whole system in an end-",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "EmoQ shows even greater advantages. As shown in Table 2, our"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "to-end manner.",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "model achieves 73.6% WA and 71.6% WF1.\nIn the seven-category"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "The goal of\nthe first phase is to train the EmoQ-Former\nto be",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "task (Table 3), EmoQ leads by 14.1, 23.5, and 17.2 percentage points"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "able to fuse acoustic features E‚Ä≤\ninto a\na and text embeddings Et",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "in WA, UA, and WF1, respectively."
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "highly discriminative multimodal embedding e‚Ä≤\nthis stage, we\nq. At",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "EmoQ shows significantly greater improvement on MELD than"
        },
        {
          "MAL achieves\nthe\nenhancement\nof\nemotion\ninformation\nby": "freeze the parameters of the pre-trained HuBERT. The output e‚Ä≤\nq of",
          "the EmoQ-Former is fed into a lightweight auxiliary MLP for emo-": "on IEMOCAP, a phenomenon that stems from two designs.\nIn the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Performance comparison on MELD‚Äôs 7-class emotion",
      "data": [
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "recognition task",
          "Table 3.\nPerformance": "recognition task",
          "comparison on MELD‚Äôs 7-class": "",
          "emotion": ""
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "Model",
          "Table 3.\nPerformance": "Model",
          "comparison on MELD‚Äôs 7-class": "Year",
          "emotion": "WF1 (%)"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "HuBERT large [12]",
          "Table 3.\nPerformance": "MSTR [15]",
          "comparison on MELD‚Äôs 7-class": "2023",
          "emotion": "46.2"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "Co-attention [13]",
          "Table 3.\nPerformance": "SpeechFormer++ [14]",
          "comparison on MELD‚Äôs 7-class": "2023",
          "emotion": "47.0"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "SpeechFormer++ [14]",
          "Table 3.\nPerformance": "DST [24]",
          "comparison on MELD‚Äôs 7-class": "2023",
          "emotion": "48.8"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "MSTR [15]",
          "Table 3.\nPerformance": "DWFormer [25]",
          "comparison on MELD‚Äôs 7-class": "2023",
          "emotion": "48.5"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "ENT [16]",
          "Table 3.\nPerformance": "Vesper-12 [23]",
          "comparison on MELD‚Äôs 7-class": "2024",
          "emotion": "48.0"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "MFSN [17]",
          "Table 3.\nPerformance": "DropFormer [26]",
          "comparison on MELD‚Äôs 7-class": "2024",
          "emotion": "49.3"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "Vesper-12 [23]",
          "Table 3.\nPerformance": "",
          "comparison on MELD‚Äôs 7-class": "",
          "emotion": ""
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "",
          "Table 3.\nPerformance": "EmoQ (Ours)",
          "comparison on MELD‚Äôs 7-class": "2025",
          "emotion": "66.5"
        },
        {
          "Table 1. Performance comparison on IEMOCAP‚Äôs 4-class emotion": "EmoQ (Ours)",
          "Table 3.\nPerformance": "",
          "comparison on MELD‚Äôs 7-class": "",
          "emotion": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance comparison on MELD‚Äôs 7-class emotion",
      "data": [
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "Text"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "‚úó"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "‚úì"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "‚úì"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "‚úì"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "‚úì indicates the component is used; ‚úó indicates the component is"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": "Table 5. Impact of MAL (Eq. 6) on MELD‚Äôs 7-Class SER"
        },
        {
          "Table 4. Effect of input modality and alignment module on MELD‚Äôs": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance comparison on MELD‚Äôs 7-class emotion",
      "data": [
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "Table 5. Impact of MAL (Eq. 6) on MELD‚Äôs 7-Class SER"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "Focal Loss"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "‚úó"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "‚úì"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "‚úó"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": "‚úì"
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        },
        {
          "‚úì indicates the component is used; ‚úó indicates the component is": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "ral\ntransducer\nfor fine-grained speech emotion recognition,‚Äù"
        },
        {
          "5. REFERENCES": "[1] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "in Proc. IEEE Int. Conf. Acoust., Speech and Signal Process."
        },
        {
          "5. REFERENCES": "dinov, and A. Mohamed, ‚ÄúHuBERT: Self-supervised speech",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "(ICASSP), 2024, pp. 10111‚Äì10115."
        },
        {
          "5. REFERENCES": "representation learning by masked prediction of hidden units,‚Äù",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[17] H. Sun, F. Zhang, Y. Gao, S. Zhang, Z. Lian, and J. Feng,"
        },
        {
          "5. REFERENCES": "IEEE/ACM Trans. Audio, Speech, and Language Processing,",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "‚ÄúMFSN: Multi-perspective\nfusion\nsearch\nnetwork\nfor\npre-"
        },
        {
          "5. REFERENCES": "vol. 29, pp. 3451‚Äì3460, 2021.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "training knowledge in speech emotion recognition,‚Äù in Proc."
        },
        {
          "5. REFERENCES": "[2]\nJ. Li, D. Li, S. Savarese,\nand S. Hoi,\n‚ÄúBLIP-2: Bootstrap-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "Interspeech, 2024, pp. 4703‚Äì4707."
        },
        {
          "5. REFERENCES": "ping language-image pre-training with frozen image encoders",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[18] Y.-T. Wu and C.-C. Lee, ‚ÄúMetricAug: A distortion metric-lead"
        },
        {
          "5. REFERENCES": "and large language models,‚Äù in Proc. Int. Conf. Mach. Learn.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "augmentation strategy for training noise-robust speech emotion"
        },
        {
          "5. REFERENCES": "(ICML), 2023.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "recognizer,‚Äù in Proc. Interspeech, 2023, pp. 3587‚Äì3591."
        },
        {
          "5. REFERENCES": "[3] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[19] X. Shi, X. Li,\nand T. Toda,\n‚ÄúEmotion awareness\nin multi-"
        },
        {
          "5. REFERENCES": "Wang, and W. Chen, ‚ÄúLoRA: Low-rank adaptation of large lan-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "utterance\nturn\nfor\nimproving\nemotion\nprediction\nin multi-"
        },
        {
          "5. REFERENCES": "guage models,‚Äù in Proc. Int. Conf. Learn. Represent. (ICLR),",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "speaker conversation,‚Äù in Proc.\nInterspeech, 2023, pp. 765‚Äì"
        },
        {
          "5. REFERENCES": "2022.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "769."
        },
        {
          "5. REFERENCES": "[4] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll¬¥ar, ‚ÄúFo-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[20] T. Feng and S. Narayanan, ‚ÄúFoundation model assisted auto-"
        },
        {
          "5. REFERENCES": "cal loss for dense object detection,‚Äù IEEE Trans. Pattern Anal.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "matic speech emotion recognition: Transcribing, annotating,"
        },
        {
          "5. REFERENCES": "Mach. Intell. (TPAMI), vol. 42, no. 2, pp. 318‚Äì327, 2020.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "and augmenting,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech and"
        },
        {
          "5. REFERENCES": "[5] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "Signal Process. (ICASSP), 2024, pp. 12116‚Äì12120."
        },
        {
          "5. REFERENCES": "M. Lewis, L. Zettlemoyer, and V. Stoyanov, ‚ÄúRoBERTa: A ro-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[21] A.\nIbrahim, S. Shehata, A. Kulkarni, M. Mohamed, and M."
        },
        {
          "5. REFERENCES": "bustly optimized BERT pretraining approach,‚Äù arXiv preprint",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "Abdul-Mageed, ‚ÄúWhat does it\ntake to generalize SER model"
        },
        {
          "5. REFERENCES": "arXiv:1907.11692, 2019.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "across datasets? A comprehensive benchmark,‚Äù in Proc. Inter-"
        },
        {
          "5. REFERENCES": "arXiv\npreprint\n[6] Qwen\net\nal.,\n‚ÄúQwen2.5\ntechnical\nreport,‚Äù",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "speech, 2024, pp. 1590‚Äì1594."
        },
        {
          "5. REFERENCES": "arXiv:2412.15115, 2025.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[22] H.\nZou,\nF.\nLv, D.\nZheng,\nE.\nS. Chng,\nand D. Rajan,"
        },
        {
          "5. REFERENCES": "[7] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "‚ÄúLarge\nlanguage models meet\ncontrastive\nlearning:\nZero-"
        },
        {
          "5. REFERENCES": "Kim, J. N. Chang, S. Lee, and S. S. Narayanan, ‚ÄúIEMOCAP:",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "arXiv preprint"
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "shot\nemotion recognition across\nlanguages,‚Äù"
        },
        {
          "5. REFERENCES": "Interactive emotional dyadic motion capture database,‚Äù Lan-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "arXiv:2503.21806, 2025."
        },
        {
          "5. REFERENCES": "guage Resources and Evaluation, vol. 42, no. 4, pp. 335‚Äì359,",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[23] W. Chen, X. Xing, P. Chen, and X. Xu, ‚ÄúVesper: A compact"
        },
        {
          "5. REFERENCES": "2008.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "and effective pretrained model\nfor\nspeech emotion recogni-"
        },
        {
          "5. REFERENCES": "[8]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "tion,‚Äù IEEE Transactions on Affective Computing, vol. 15, no."
        },
        {
          "5. REFERENCES": "and R. Mihalcea, ‚ÄúMELD: A multimodal multi-party dataset",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "3, pp. 1711‚Äì1724, 2024."
        },
        {
          "5. REFERENCES": "for emotion recognition in conversations,‚Äù in Proc. 57th An-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[24] W. Chen, X. Xing, X. Xu,\nJ. Pang,\nand L. Du,\n‚ÄúDST: De-"
        },
        {
          "5. REFERENCES": "nual Meeting of the Association for Computational Linguistics",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "formable speech transformer for emotion recognition,‚Äù in Proc."
        },
        {
          "5. REFERENCES": "(ACL), 2019, pp. 527‚Äì536.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "IEEE Int. Conf. Acoust., Speech and Signal Process. (ICASSP),"
        },
        {
          "5. REFERENCES": "[9]\nP. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "2023, pp. 1‚Äì5."
        },
        {
          "5. REFERENCES": "Maschinot, C. Liu, and D. Krishnan, ‚ÄúSupervised contrastive",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[25]\nS. Chen, X. Xing, W. Zhang, W. Chen,\nand X. Xu,\n‚ÄúDW-"
        },
        {
          "5. REFERENCES": "learning,‚Äù in Advances in Neural Information Processing Sys-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "Former:\nDynamic window transformer\nfor\nspeech emotion"
        },
        {
          "5. REFERENCES": "tems (NeurIPS), 2020, pp. 18661‚Äì18673.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "recognition,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech and Sig-"
        },
        {
          "5. REFERENCES": "[10]\nI. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regular-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "nal Process. (ICASSP), 2023, pp. 1‚Äì5."
        },
        {
          "5. REFERENCES": "ization,‚Äù in Proc. Int. Conf. Learn. Represent. (ICLR), 2019.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[26]\nJ. Mai, X. Xing, W. Chen, and X. Xu, ‚ÄúDropFormer: A dy-"
        },
        {
          "5. REFERENCES": "[11] Z. Huang, M.-W. Mak,\nand K. A. Lee,\n‚ÄúMM-NodeFormer:",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "namic noise-dropping transformer\nfor speech emotion recog-"
        },
        {
          "5. REFERENCES": "Node Transformer multimodal fusion for emotion recognition",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "nition,‚Äù in Proc. Interspeech, 2024, pp. 2645‚Äì2649."
        },
        {
          "5. REFERENCES": "in conversation,‚Äù in Proc. Interspeech, 2024, pp. 4069-4073.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[27] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, Z. Ma,"
        },
        {
          "5. REFERENCES": "[12] Z. Ma, M. Chen, H. Zhang, Z. Zheng, W. Chen, X. Li, J. Ye,",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "and C. Zhang, ‚ÄúSALMONN: Towards generic hearing abilities"
        },
        {
          "5. REFERENCES": "X. Chen, and T. Hain, ‚ÄúEmoBox: Multilingual multi-corpus",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "for large language models,‚Äù in Proc. Int. Conf. Learn. Repre-"
        },
        {
          "5. REFERENCES": "speech emotion recognition toolkit and benchmark,‚Äù in Proc.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "sent. (ICLR), 2024."
        },
        {
          "5. REFERENCES": "Interspeech, 2024.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[28] Y. Xu, H. Chen, J. Yu, Q. Huang, Z. Wu, S.-X. Zhang, G. Li,"
        },
        {
          "5. REFERENCES": "[13] H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, ‚ÄúSpeech",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "Y\n. Luo, and R. Gu, ‚ÄúSECap: Speech emotion captioning with"
        },
        {
          "5. REFERENCES": "emotion recognition with co-attention based multi-level acous-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "large language model,‚Äù in Proc. AAAI Conf. on Artificial Intel-"
        },
        {
          "5. REFERENCES": "tic information,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech and",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "ligence, vol. 38, no. 17, pp. 19323‚Äì19331, 2024."
        },
        {
          "5. REFERENCES": "Signal Process. (ICASSP), 2022, pp. 7367‚Äì7371.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[29] Q. Zhu, C. Zheng, Z. Zhang, W. Shao, and D. Zhang, ‚ÄúDy-"
        },
        {
          "5. REFERENCES": "[14] W. Chen, X. Xing, X. Xu,\nJ. Pang,\nand L. Du,\n‚ÄúSpeech-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "namic Confidence-Aware Multi-Modal Emotion Recognition,‚Äù"
        },
        {
          "5. REFERENCES": "Former++: A hierarchical efficient framework for paralinguis-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "IEEE Transactions on Affective Computing, vol. 15, no. 3, pp."
        },
        {
          "5. REFERENCES": "tic speech processing,‚Äù IEEE/ACM Trans. Audio, Speech, and",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "1358‚Äì1370, 2024."
        },
        {
          "5. REFERENCES": "Language Processing, vol. 31, pp. 775‚Äì788, 2023.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "[30]\nS. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen,"
        },
        {
          "5. REFERENCES": "[15] Z. Li, X. Xing, Y. Fang, W. Zhang, H. Fan, and X. Xu, ‚ÄúMulti-",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "arXiv\n‚ÄúA Survey on Multimodal Large Language Models,‚Äù"
        },
        {
          "5. REFERENCES": "scale temporal transformer for speech emotion recognition,‚Äù in",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        },
        {
          "5. REFERENCES": "",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": "preprint arXiv:2306.13549, version 4, Nov. 2024."
        },
        {
          "5. REFERENCES": "Proc. Interspeech, 2023, pp. 3652‚Äì3656.",
          "[16]\nS. Shen, Y. Gao, F. Liu, H. Wang, and A. Zhou, ‚ÄúEmotion neu-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "venue": "Proc. Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "4",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "venue": "Proc. Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "5",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Doll√°r"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)"
    },
    {
      "citation_id": "6",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "7",
      "title": "Qwen2.5 technical report",
      "authors": [
        "Qwen"
      ],
      "year": "2025",
      "venue": "Qwen2.5 technical report",
      "arxiv": "arXiv:2412.15115"
    },
    {
      "citation_id": "8",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "10",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "11",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "12",
      "title": "MM-NodeFormer: Node Transformer multimodal fusion for emotion recognition in conversation",
      "authors": [
        "Z Huang",
        "M.-W Mak",
        "K Lee"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "EmoBox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Z Ma",
        "M Chen",
        "H Zhang",
        "Z Zheng",
        "W Chen",
        "X Li",
        "J Ye",
        "X Chen",
        "T Hain"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "15",
      "title": "Speech-Former++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Trans. Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Multiscale temporal transformer for speech emotion recognition",
      "authors": [
        "Z Li",
        "X Xing",
        "Y Fang",
        "W Zhang",
        "H Fan",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Emotion neural transducer for fine-grained speech emotion recognition",
      "authors": [
        "S Shen",
        "Y Gao",
        "F Liu",
        "H Wang",
        "A Zhou"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "18",
      "title": "MFSN: Multi-perspective fusion search network for pretraining knowledge in speech emotion recognition",
      "authors": [
        "H Sun",
        "F Zhang",
        "Y Gao",
        "S Zhang",
        "Z Lian",
        "J Feng"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "MetricAug: A distortion metric-lead augmentation strategy for training noise-robust speech emotion recognizer",
      "authors": [
        "Y.-T Wu",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Emotion awareness in multiutterance turn for improving emotion prediction in multispeaker conversation",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "22",
      "title": "What does it take to generalize SER model across datasets? A comprehensive benchmark",
      "authors": [
        "A Ibrahim",
        "S Shehata",
        "A Kulkarni",
        "M Mohamed",
        "M Abdul-Mageed"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Large language models meet contrastive learning: Zeroshot emotion recognition across languages",
      "authors": [
        "H Zou",
        "F Lv",
        "D Zheng",
        "E Chng",
        "D Rajan"
      ],
      "year": "2025",
      "venue": "Large language models meet contrastive learning: Zeroshot emotion recognition across languages",
      "arxiv": "arXiv:2503.21806"
    },
    {
      "citation_id": "24",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "P Chen",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "DST: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "26",
      "title": "DW-Former: Dynamic window transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "27",
      "title": "DropFormer: A dynamic noise-dropping transformer for speech emotion recognition",
      "authors": [
        "J Mai",
        "X Xing",
        "W Chen",
        "X Xu"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang",
        "W Yu",
        "G Sun",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "C Zhang"
      ],
      "venue": "Proc. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "29",
      "title": "SECap: Speech emotion captioning with large language model",
      "authors": [
        "Y Xu",
        "H Chen",
        "J Yu",
        "Q Huang",
        "Z Wu",
        "S.-X Zhang",
        "G Li",
        "Y Luo",
        "R Gu"
      ],
      "year": "2024",
      "venue": "Proc. AAAI Conf. on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Dynamic Confidence-Aware Multi-Modal Emotion Recognition",
      "authors": [
        "Q Zhu",
        "C Zheng",
        "Z Zhang",
        "W Shao",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "A Survey on Multimodal Large Language Models",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2024",
      "venue": "A Survey on Multimodal Large Language Models",
      "arxiv": "arXiv:2306.13549"
    }
  ]
}