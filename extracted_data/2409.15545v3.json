{
  "paper_id": "2409.15545v3",
  "title": "Addressing Emotion Bias In Music Emotion Recognition And Generation With Frechet Audio Distance",
  "published": "2024-09-23T20:59:15Z",
  "authors": [
    "Yuanchao Li",
    "Azalea Gui",
    "Dimitra Emmanouilidou",
    "Hannes Gamper"
  ],
  "keywords": [
    "Emotion Bias",
    "Music Emotion Recognition",
    "Emotional Music Generation",
    "Frechet Audio Distance",
    "Audio Encoders"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The complex nature of musical emotion introduces inherent bias in both recognition and generation, particularly when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG), employing diverse audio encoders alongside Frechet Audio Distance (FAD), a reference-free evaluation metric. Our study begins with a benchmark evaluation of MER, highlighting the limitations of using a single audio encoder and the disparities observed across different measurements. We then propose assessing MER performance using FAD derived from multiple encoders to provide a more objective measure of musical emotion. Furthermore, we introduce an enhanced EMG approach designed to improve both the variability and prominence of generated musical emotion, thereby enhancing its realism. Additionally, we investigate the differences in realism between the emotions conveyed in real and synthetic music, comparing our EMG model against two baseline models. Experimental results underscore the issue of emotion bias in both MER and EMG and demonstrate the potential of using FAD and diverse audio encoders to evaluate musical emotion more objectively and effectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Emotion Bias In Music Emotion Recognition",
      "text": "Emotion bias in MER arises from various sources, including imbalances in training data, subjective emotion annotations, and cultural differences in emotional expression. Studies have shown that MER models trained on Western music often perform poorly when tested on non-Western music due to differences in emotional interpretation  [2] ,  [5] . Similarly, relying on human-annotated labels introduces subjectivity, as listeners' emotional responses can vary significantly depending on their cultural background, personal experiences, and even their mood during listening sessions  [6] ,  [7] . These biases result in MER models that reflect the emotional norms of a specific population, thereby limiting the universality of the generated predictions. Recent efforts to address these biases include domain adaptation aimed at improving the transferability of MER models across different musical cultures  [8] .\n\nMoreover, while categorical and dimensional emotions are not entirely incompatible, their alignment is imperfect. For instance, Russell's circumplex model (Russell's 4Q)  [9]  effectively captures specific emotions: joy exhibits positive valence and high arousal, while sadness displays negative valence and low arousal. However, it falls short in distinguishing between certain emotions. For example, both fear and anger share negative valence and high arousal. Such misalignment introduces emotion bias in measurement. For instance, an MER method may perform well for categorical emotions but struggle with dimensional emotions, posing challenges to the generalizability of existing MER approaches.\n\nAdditionally, bias can arise from the use of different audio encoders due to their distinct training strategies. For instance, features extracted by Encodec  [10]  may lose subtle emotional nuances because of its audio compression training objectives. Moreover, there is no guarantee that an encoder optimized for categorical emotion classification will perform equally well for dimensional emotion modeling  [11] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Emotion Bias In Emotional Music Generation",
      "text": "EMG is similarly susceptible to emotion bias as MER. Since music generation relies on datasets for training, the emotional diversity of the generated music is inherently tied to the diversity of the training set. Models trained on datasets with emotional imbalances tend to generate music that predominantly expresses the more frequent emotions. Although efforts have been made to create more balanced datasets that cover a broader range of emotional contexts, this requires significant human effort.\n\nMoreover, emotion bias can also arise during model training. As mentioned in Section I, there are two primary approaches to EMG. In the first approach, which relies on using emotion labels as control signals, subjectivity stems from the labels themselves, as they are assigned by human annotators  [4] ,  [12] . In the second approach, which employs an emotion classifier for guidance, bias is inherited from the MER models due to the limitations of the audio encoders. Recent methods have introduced constraints into the generation process to ensure emotional variety and reduce bias in EMG. For instance, rather than directly encoding emotion embeddings from labels,  [10]  mapped categorical labels into four quadrants, each corresponding to specific musical attributes (i.e., symbolic features) used for conditioning. However, this method relies on only four sets of attributes, with each quadrant representing a set of emotions. As a result, all emotions within the same quadrant share the same musical attributes as conditions, limiting the variation and naturalness of the generated emotions.\n\nTo address these issues, we aim to address emotion bias in both recognition and generation by 1) measuring and recognizing music emotion in a more objective and efficient way and 2) generating music emotions that more closely resemble those in real music.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Datasets And Audio Encoders",
      "text": "We adopt multiple datasets to ensure generalizability and for different purposes: two for recognition and one for generation.\n\nEmomusic  [13]  consists of 744 music excerpts, each 45 seconds long, annotated with two-dimensional Valence-Arousal (VA) labels. The dataset includes human singing across various genres, such as pop, rock, classical, and electronic.\n\nMMirex  [14]  contains 903 audio samples, each labeled with emotion tags from the MIREX Mood Classification Task  [15] . The dataset uses five mood clusters as defined by MIREX and features audio from various instruments.\n\nEMOPIA  [1]  comprises 387 piano solo performances of popular music, manually segmented into 1,087 clips for emotion annotation. The emotion classes follow Russell's 4Q model, which defines emotions within the VA space.\n\nFor audio encoders, we include models trained with various objectives, such as VGGish (convolutional embeddings)  [16] , CLAP  [17]  and CLAP-LAION  [18]  (contrastive embeddings), MERT  [19]  (self-supervised embeddings), CDPAM  [20]  (audio similarity), as well as EnCodec  [21]  and DAC  [22]  (low-rate audio codecs). For models with two versions trained on different inputs or objectives (e.g., audio and music), we include both. For MERT, we compare all 12 hidden layers for layer-wise analysis (see Fig.  1 ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Music Emotion Recognition",
      "text": "In this experiment, we use the Emomusic and MMirex datasets. Based on the VA labels from Emomusic for dimensional MER, we further categorize them into Russell's four quadrants (4Q) for categorical MER: Q1 (-V+A), Q2 (-V-A), Q3 (+V+A), and Q4 (+V-A). Both dimensional and categorical MER are performed on this dataset. Additionally, MMirex is used for categorical MER based on five mood clusters. Thus, both dimensional (VA) and categorical (Russell's 4Q and five mood clusters) emotions are investigated to ensure generalizability.\n\nFor dimensional MER, we use the coefficient of determination (R 2 ) as the evaluation metric for VA. For categorical MER, we assess both weighted and unweighted accuracy on Emomusic, given its imbalanced distribution, and use the F1score on MIREX, following the literature. We employ both SVM and Multi-Layer Perceptrons (MLP), while maintaining consistent experimental settings with previous research  [23] . The experiments are conducted using an NVIDIA A100 GPU, with models trained using the Adam optimizer at a learning rate of 0.0001 and a batch size of 32. All models are trained under the same conditions to ensure consistency. It can be observed from Fig.  1 that:  1) Valence is more challenging to predict than arousal, which is consistent with previous research  [24] . While arousal is typically conveyed through prosodic features such as pitch, energy, and tempo, valence relies more heavily on the intricate aspects of musical composition, as noted by  [25] . Moreover, since speech valence is significantly influenced by language content  [26] , this relationship suggests the existence of a distinct \"language\" within music, supporting the conclusions of  [27] .\n\n2) For dimensional emotion, a significant performance gap is observed among audio encoders. However, this gap becomes less pronounced for categorical emotion, resulting in more comparable performance across different encoders. This phenomenon is evident in both categorical quadrants (Emo-Music) and categorical mood clusters (MIREX), illustrating the challenges posed by bias in MER, as discussed in Sec. I. Consequently, an encoder that performs well with categorical emotions does not necessarily excel at encoding dimensional emotions.\n\n3) Different encoders can exhibit significant performance differences. Those trained using contrastive audio-language learning methods, such as CLAP and CLAP-LAION, achieve the best performance on all metrics. However, CDPAM, which is also trained using contrastive learning but on the contrast between different audio samples rather than between audio and language, underperforms compared to most of the other encoders. This suggests that MER benefits from language-like information, indicating that music segments contain contextual meaning that can somewhat serve as a \"language\", which is consistent with the finding in 1). On the contrary, encoders designed for audio compression, such as DAC and Encodec, generally do not perform well in MER. This is likely due to audio compression leading to information loss related to emotion or the emotional information implicitly residing in the embeddings after compression.\n\n4) In the layer-wise results of MERT, we observe an upward-downward trend, consistent with previous studies on speech emotion recognition  [28] ,  [29] , indicating the similarity between speech and music emotions. This trend becomes more pronounced when predicting valence, highlighting the need for deeper encoding to effectively capture valence. This observation further supports the finding that valence in music depends on higher-level musical structures, such as contextual elements.\n\nFurthermore, the performance using R 2 and accuracy differs significantly, with unweighted accuracy notably outperforming weighted accuracy, indicating the impact of data balance. These findings reveal that emotion bias in MER can stem from various factors, including the audio encoder, evaluation metric, and emotion dimension.\n\nV. MEASURING MUSIC EMOTION VIA FAD FAD was initially proposed for evaluating music enhancement quality and has shown a close correlation with human perception as a reference-free metric, without the need for model training  [30] . Recently, it has replaced humans in assessing acoustic and musical quality and similarity, helping to mitigate subjective bias. For instance, it has been shown to effectively distinguish between real and synthetic audio  [31] , as well as audio with different emotions  [32] . In light of this, we adopt FAD as an objective metric to resolve potential emotion bias arising from various sources (implemented using Microsoft FADTK  1  ).\n\nFAD compares acoustic features generated from two audio sets, enabling it to assess two distinct audio sets without ground truth. The FAD score is calculated using multivariate Gaussians from two embedding sets N a (µ a , Σ a ) and N b (µ b , Σ b ) as follows:\n\nwhere tr is the trace of a matrix. To calculate FAD scores, we extract features from the same audio encoders as used in Fig.  1    From Table  I , we observe the following: 1) The scores are high between diagonal quadrants (i.e., Q1 Q3 and Q2 Q4). This is reasonable since the polarities of both valence and arousal are opposite in diagonal quadrants. 2) The score for Q1 Q2 is lower than that for Q1 Q4, and the score for Q3 Q4 is lower than that for Q2 Q3. This indicates that the difference between two music sets is smaller when they share the same arousal polarity but differ in valence polarity, compared to when they share the same valence polarity but differ in arousal polarity. These observations suggest that arousal is more explicitly expressed in music than valence. From Table  II , we observe variations among mood clusters. The score for C1 C2 is the smallest, while the score for C3 C5 is the largest. Although mood clusters do not explicitly represent valence and arousal, we can infer that both C1 (e.g., Passionate) and C2 (e.g., Cheerful) are associated with high arousal, which explains the small FAD score between them. In contrast, C3 (e.g., Wistful) appears to be linked with low arousal, while C5 (e.g., Aggressive) is associated with high arousal, which justifies the relatively large FAD score. These findings are consistent with prior research on the relationship between categorical and dimensional music emotion  [33] . Due to space constraints, we omit detailed descriptions of the mood clusters and refer readers to  [3]  for more information.\n\nThus, averaging FAD scores from various encoders offers a more objective manner for measuring emotions, eliminating the need to train MER models and select metrics, and thereby reducing bias. Furthermore, this approach addresses issues of imbalanced data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vi. Emotional Music Generation",
      "text": "After benchmarking emotion recognition and introducing the use of FAD for objective measurement, we extend this approach to the domain of music generation for emotion assessment. We propose an enhanced generation model that integrates both categorical and dimensional emotions to address their respective limitations, and then compare the music generated by our model with that produced by two baselines.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Baseline Generation Models",
      "text": "Since using emotion classifiers for guidance introduces additional emotion bias  [10] , we use emotion labels solely for conditioning. Two baseline models are selected for generating symbolic music: MIDIEmo  [4]  for dimensional emotion and EmoGen  [10]  for categorical emotion.\n\nMIDIEmo encodes continuous VA labels (e.g., [0.8, 0.8]) as latent embeddings using linear layers, which are then concatenated with music embeddings. We argue that this approach can make music samples with boundary labels more susceptible to subjective bias (as the boundary emotions are usually difficult to distinguish). The challenge of assigning boundary labels with high confidence often leads to ambiguity in the generated emotion. Consequently, while there is considerable variation, ambiguity is prevalent in the generated music, making it difficult to perceive precise emotions.\n\nEmoGen uses four pre-extracted embeddings to represent emotions within the four quadrants. It encodes discrete values of these quadrants (e.g., 1 or 2) into embeddings as the emotion input, thus samples within a particular quadrant share the same emotional conditions (i.e., the emotion embeddings for all samples in each quadrant are identical, regardless of their original labels). As a result, while the generated music exhibits precise emotions, we argue that it lacks variation, leading to a situation where all samples with the same emotion tend to sound emotionally similar.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Proposed Generation Model",
      "text": "Instead of using either continuous VA values or discrete emotion quadrants alone, we combine them in the conditioning process. First, we determine the quadrant based on the continuous VA values (e.g., +V+A corresponds to Q1). Next, we encode both the quadrant and the VA values into hidden embeddings using separate linear layers. Subsequently, we merge these embeddings via a weighted sum to create the final emotion embedding:\n\nwhere embd q , embd va , and wgt q denote the quadrant embedding, VA embedding, and the weight assigned to the quadrant, respectively. We set weight q to 0.5 and use cross-attention to fuse the emotion and music embeddings:\n\nwhere Q e , K m , and V m represent the respective matrix for query (emotion embedding), key (music embedding), and value (music embedding), d k is the size of a key vector, and EM is the emotion-aligned music embedding. Fig.  2  illustrates our proposed model and compares it to baseline models in terms of emotion conditioning principles. It is evident that our approach enhances emotion conditioning by combining the strengths of EmoGen and MIDIEmo, enabling a balance and trade-off between prominence and variability in the generated emotion. The emotion is anchored at the quadrant center by wgt q • embd q and allows for variation within a certain range by (1wgt q ) • embd va . Samples outside this range are adjusted to fall within, ensuring more pronounced emotion. The value of wgt q influences the size of this range: a larger wgt q results in a smaller range and emotion closer to the quadrant, while a smaller wgt q leads to a larger range, potentially reaching the quadrant boundary. By adjusting wgt q , we provide flexibility for emotion to span between precision and variation.\n\nFor the remaining of our model, we follow  [4] , leveraging Music Transformer  [34]  as the backbone and using its Lakh-Spotify dataset based on Lakh Pianoroll 5 full dataset  [35]  for model training. For brevity, we omit the details and refer readers to Sec. III of  [4] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Comparative Evaluation -Real Vs. Synthetic Music",
      "text": "To assess the quality of generated music, prior research has employed either subjective or objective evaluations. In subjective evaluations, human raters assign scores to the generated music based on the accuracy of the conveyed emotions. In objective evaluations, an emotion classifier is used to classify the generated music  [10] ,  [36] . However, both approaches are subject to inherent biases, either from human perception or emotion classifiers. Therefore, we propose an objective measurement based on FAD.\n\nWe generate music using MIDIEmo, EmoGen, and our proposed model, producing a total of 1,000 music samples from each model 2 . For EmoGen, we generate 250 samples for each of the four emotion quadrants. For MIDIEmo and our model, which encode continuous VA values, we generate 250 samples for VA pairs in each quadrant with a balanced distribution. Since all three models produce MIDI music, we use EMOPIA as the reference dataset. EMOPIA, also in MIDI format, comprises approximately 1,000 samples with a balanced distribution across the four quadrants. Consistent with Section V, we utilize all audio encoders and calculate their average FAD scores to mitigate the emotion bias. The comparison of FAD scores across different quadrants is presented in Table  III . The observations are as follows:\n\n2 Samples available: https://yc-li20.github.io/Music-Emotion 1) The FAD scores for Q1 Q2 and Q3 Q4 are notably lower compared to other quadrant pairs. As discussed in Sec. V, this is likely because the emotions in these quadrant pairs share the same arousal polarity, making it more challenging to distinguish between them.\n\n2) EmoGen achieves the highest FAD scores, indicating that it excels at differentiating between various emotions based on their musical characteristics. This is consistent with its training principle, where all music pieces within a given quadrant are conditioned to express the same emotion. However, this practice results in less emotional variation, making the generated emotions appear less natural. An advantage of EmoGen is its effective differentiation for Q1 Q2 and Q3 Q4, as it achieves higher scores than those of real music.\n\n3) MIDIEmo exhibits greater variability than EmoGen. Although its overall FAD scores are lower than those of EmoGen, they are still higher than those for real music, particularly for Q1 Q3 and Q2 Q4. Since emotions in diagonal quadrants are inherently more distinct, a high score in these cases might cause the generated emotional music to sound overly pronounced or exaggerated.\n\n4) Our overall FAD scores are closer to those of real music compared to EmoGen and MIDIEmo, indicating that our model generates more realistic emotions. Furthermore, the score for Q3 Q4 in our model is higher than that of MIDIEmo and real music, demonstrating that our approach effectively leverages the strengths of EmoGen. As discussed in Sec. V, relatively high scores for Q1 Q2 and Q3 Q4 are beneficial for distinguishing between these quadrants. Additionally, the emotion embedding in our model is naturally aligned with the music embedding through cross-attention, resulting in overall lower scores compared to MIDIEmo, which simply concatenates emotion and music embeddings.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this study, we address the issue of emotion bias in music, focusing on both its recognition and generation. We benchmark MER across diverse music datasets, highlighting discrepancies between categorical and dimensional emotions, as well as the limitations of relying on a single encoder or metric description model. To address these issues, we propose employing FAD with various audio encoders as an objective measurement. Additionally, we introduce an enhanced model for emotion generation that integrates the advantages of both categorical and dimensional emotions, resulting in more realistic emotional expression in the generated music, with control over both prominence and variability. The comparative evaluation of real and synthetic music using FAD demonstrates the effectiveness of our approach. This study underscores the issue of emotion bias in music and provides novel insights into addressing this challenge.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison results of MER using different audio encoders on Emomusic and MMirex. ↑: the higher the better. Bold: best performance.",
      "page": 2
    },
    {
      "caption": "Figure 1: by computing the average of the FAD scores to mitigate",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates our proposed model and compares it to",
      "page": 4
    },
    {
      "caption": "Figure 2: Our proposed EMG model (above) and its comparison to baseline",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—The complex nature of musical emotion introduces": "inherent bias\nin both recognition and generation, particularly",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "emotion. The first approach uses emotion labels as condition-"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "when relying\non a\nsingle\naudio\nencoder,\nemotion classifier,\nor",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "ing, where the bias stems directly from the labels assigned by"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "evaluation metric.\nIn this work, we\nconduct a study on Music",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "human annotators [4]. In the second approach, which replaces"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Emotion Recognition (MER) and Emotional Music Generation",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "(EMG),\nemploying\ndiverse\naudio\nencoders\nalongside\nFrechet",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "the human annotator with an MER classifier, emotion bias is"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Audio Distance\n(FAD), a reference-free\nevaluation metric. Our",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "inductive and inherited from the classifier,\nlargely influenced"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "study begins with a benchmark evaluation of MER, highlighting",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "by the\nchoice of\naudio encoders. Therefore,\nit\nis\ncrucial\nto"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "the limitations of using a single audio encoder and the dispar-",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "explore more\nobjective\napproaches\nto measure,\nrecognize,"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "ities observed across different measurements. We then propose",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "and generate music\nemotion, mitigating the\naforementioned"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "assessing MER performance using FAD derived from multiple",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "encoders to provide a more objective measure of musical emotion.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "emotion bias.\nIn this paper:"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Furthermore, we introduce an enhanced EMG approach designed",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "• We\nconduct\na benchmark evaluation of MER for both"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "to\nimprove both the\nvariability\nand prominence\nof\ngenerated",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "categorical and dimensional emotions, utilizing various audio"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "musical\nemotion,\nthereby\nenhancing\nits\nrealism. Additionally,",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "encoders and metrics to highlight the issue of emotion bias. We"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "we\ninvestigate\nthe differences\nin realism between the\nemotions",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "then introduce Frechet Audio Distance (FAD) as an objective"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "conveyed in real and synthetic music, comparing our EMG model",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "against two baseline models. Experimental results underscore the",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "measure to address this challenge."
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "issue of emotion bias\nin both MER and EMG and demonstrate",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "• We\npropose\nan\nenhanced\nEMG approach\nto\nreduce"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "the potential of using FAD and diverse audio encoders to evaluate",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "emotion bias\nin emotion labels\nand improve\nthe\nrealism of"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "musical emotion more objectively and effectively.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "generated\nemotional music. Using\nour\napproach,\nalongside"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Index\nTerms—Emotion\nBias, Music\nEmotion Recognition,",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "two baseline methods, we generate\nthree\ntypes of\nsynthetic"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Emotional Music Generation, Frechet Audio Distance, Audio",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "emotional music for comparison with real music. The effec-"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Encoders",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "tiveness of our approach is demonstrated through evaluations"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "I.\nINTRODUCTION",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "using FAD scores calculated by multiple audio encoders."
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Music Emotion Recognition (MER) and Emotional Music",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "II. RELATED WORK"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Generation (EMG) play crucial\nroles in various applications,",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "A. Emotion Bias in Music Emotion Recognition"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "such\nas music\nrecommendation\nsystems, film scoring,\nand",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "Emotion bias in MER arises from various sources, including"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "emotion\ntherapy.\nFor\ninstance,\nin music\nrecommendation",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "imbalances\nin training data,\nsubjective\nemotion annotations,"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "systems, accurate MER matches user preferences,\nincreasing",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "and cultural differences in emotional expression. Studies have"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "satisfaction. Similarly,\nin emotion therapy, generating music",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "shown\nthat MER models\ntrained\non Western music\noften"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "with emotion can enhance therapeutic effects.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "perform poorly when tested on non-Western music due to dif-"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "However, unlike speech emotion, which can be expressed",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "ferences in emotional\ninterpretation [2], [5]. Similarly, relying"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "through language\n(i.e.,\nspoken content),\nthe perception and",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "on human-annotated labels introduces subjectivity, as listeners’"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "expression of music emotion are more subjective and challeng-",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "emotional responses can vary significantly depending on their"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "ing,\ninevitably leading to biases in its measurement,\nrecogni-",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "cultural\nbackground,\npersonal\nexperiences,\nand\neven\ntheir"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "tion, and generation.\nIn MER,\nthe lack of alignment between",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "mood during listening sessions\n[6],\n[7]. These biases\nresult"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "categorical\nand dimensional models\nchallenges\nthe general-",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "in MER models that reflect\nthe emotional norms of a specific"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "izability\nof\ncurrent MER approaches\n[1],\n[2]. Furthermore,",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "population,\nthereby limiting the universality of\nthe generated"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "MER is\nsometimes\nreferred to as music mood classification",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "predictions. Recent\nefforts\nto\naddress\nthese\nbiases\ninclude"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "when\nit\nemploys\nfive mood\nclusters\nas\nlabels\n[3], which",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "domain adaptation aimed at\nimproving the transferability of"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "also introduces bias due\nto variations\nin human perception.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "MER models across different musical cultures [8]."
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Moreover, audio encoders are designed with distinct\ntraining",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "Moreover, while categorical and dimensional emotions are"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "objectives defined by humans, and relying on a single encoder",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "not\nentirely incompatible,\ntheir\nalignment\nis\nimperfect. For"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "for MER inevitably introduces inductive bias.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "instance, Russell’s circumplex model (Russell’s 4Q) [9] effec-"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "tively captures specific emotions:\njoy exhibits positive valence"
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Work\ndone while Yuanchao\nand Azalea were\ninterning\nat Microsoft",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": ""
        },
        {
          "Abstract—The complex nature of musical emotion introduces": "Research. Correspondence to yuanchao.li@ed.ac.uk.",
          "In EMG,\nthere\nare\ntwo\nprimary\napproaches\nto\nencoding": "and\nhigh\narousal, while\nsadness\ndisplays\nnegative\nvalence"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "and\nlow arousal. However,\nit\nfalls\nshort\nin\ndistinguishing",
          "the higher\nthe better. Bold: best performance.": "[4],\n[12].\nIn the second approach, which employs an emotion"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "between certain emotions. For example, both fear and anger",
          "the higher\nthe better. Bold: best performance.": "classifier for guidance, bias is inherited from the MER models"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "share negative valence and high arousal. Such misalignment",
          "the higher\nthe better. Bold: best performance.": "due to the limitations of\nthe audio encoders. Recent methods"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "introduces\nemotion\nbias\nin measurement.\nFor\ninstance,\nan",
          "the higher\nthe better. Bold: best performance.": "have introduced constraints into the generation process to en-"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "MER method may perform well\nfor categorical emotions but",
          "the higher\nthe better. Bold: best performance.": "sure emotional variety and reduce bias in EMG. For instance,"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "struggle with dimensional emotions, posing challenges to the",
          "the higher\nthe better. Bold: best performance.": "rather than directly encoding emotion embeddings from labels,"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "generalizability of existing MER approaches.",
          "the higher\nthe better. Bold: best performance.": "[10] mapped categorical labels into four quadrants, each corre-"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "Additionally, bias can arise from the use of different audio",
          "the higher\nthe better. Bold: best performance.": "sponding to specific musical attributes (i.e., symbolic features)"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "encoders due to their distinct\ntraining strategies. For instance,",
          "the higher\nthe better. Bold: best performance.": "used for\nconditioning. However,\nthis method relies on only"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "features extracted by Encodec [10] may lose subtle emotional",
          "the higher\nthe better. Bold: best performance.": "four\nsets of attributes, with each quadrant\nrepresenting a set"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "nuances because of\nits audio compression training objectives.",
          "the higher\nthe better. Bold: best performance.": "of emotions. As a result, all emotions within the same quadrant"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "Moreover,\nthere is no guarantee that an encoder optimized for",
          "the higher\nthe better. Bold: best performance.": "share the same musical attributes as conditions,\nlimiting the"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "categorical emotion classification will perform equally well for",
          "the higher\nthe better. Bold: best performance.": "variation and naturalness of\nthe generated emotions."
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "dimensional emotion modeling [11].",
          "the higher\nthe better. Bold: best performance.": "To address\nthese\nissues, we\naim to address\nemotion bias"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "",
          "the higher\nthe better. Bold: best performance.": "1) measuring\nand\nin\nboth\nrecognition\nand\ngeneration\nby"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "B. Emotion Bias in Emotional Music Generation",
          "the higher\nthe better. Bold: best performance.": ""
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "",
          "the higher\nthe better. Bold: best performance.": "recognizing music emotion in a more objective and efficient"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "EMG is similarly susceptible to emotion bias as MER. Since",
          "the higher\nthe better. Bold: best performance.": "way\n2)\ngenerating music\nemotions\nthat more\nclosely\nand"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "music generation relies on datasets for training,\nthe emotional",
          "the higher\nthe better. Bold: best performance.": "resemble those in real music."
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "diversity\nof\nthe\ngenerated music\nis\ninherently\ntied\nto\nthe",
          "the higher\nthe better. Bold: best performance.": ""
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "",
          "the higher\nthe better. Bold: best performance.": "III. DATASETS AND AUDIO ENCODERS"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "diversity of\nthe training set. Models trained on datasets with",
          "the higher\nthe better. Bold: best performance.": ""
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "emotional\nimbalances\ntend to generate music\nthat predomi-",
          "the higher\nthe better. Bold: best performance.": "We adopt multiple datasets to ensure generalizability and for"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "nantly expresses the more frequent emotions. Although efforts",
          "the higher\nthe better. Bold: best performance.": "different purposes: two for recognition and one for generation."
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "have been made to create more balanced datasets that cover a",
          "the higher\nthe better. Bold: best performance.": "Emomusic [13] consists of 744 music excerpts, each 45 sec-"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "broader\nrange of emotional contexts,\nthis requires significant",
          "the higher\nthe better. Bold: best performance.": "onds\nlong,\nannotated with two-dimensional Valence-Arousal"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "human effort.",
          "the higher\nthe better. Bold: best performance.": "(VA) labels. The dataset includes human singing across various"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "Moreover, emotion bias can also arise during model\ntrain-",
          "the higher\nthe better. Bold: best performance.": "genres, such as pop,\nrock, classical, and electronic."
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "ing. As mentioned in Section I,\nthere\nare\ntwo primary ap-",
          "the higher\nthe better. Bold: best performance.": "MMirex [14] contains 903 audio samples, each labeled with"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "proaches to EMG. In the first approach, which relies on using",
          "the higher\nthe better. Bold: best performance.": "emotion tags from the MIREX Mood Classification Task [15]."
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "emotion labels as control signals, subjectivity stems from the",
          "the higher\nthe better. Bold: best performance.": "The dataset uses five mood clusters as defined by MIREX and"
        },
        {
          "Fig. 1.\nComparison results of MER using different audio encoders on Emomusic and MMirex. ↑:": "labels themselves, as they are assigned by human annotators",
          "the higher\nthe better. Bold: best performance.": "features audio from various instruments."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "popular music, manually segmented into 1,087 clips for emo-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "differences. Those\ntrained\nusing\ncontrastive\naudio-language"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "tion\nannotation. The\nemotion\nclasses\nfollow Russell’s\n4Q",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "learning methods, such as CLAP and CLAP-LAION, achieve"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "model, which defines emotions within the VA space.",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "the best performance on all metrics. However, CDPAM, which"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "For audio encoders, we include models trained with various",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "is also trained using contrastive learning but on the contrast"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "objectives, such as VGGish (convolutional embeddings) [16],",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "between different\naudio samples\nrather\nthan between audio"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "CLAP\n[17]\nand CLAP-LAION [18]\n(contrastive\nembed-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "and language, underperforms compared to most of\nthe other"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "dings), MERT [19]\n(self-supervised embeddings), CDPAM",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "encoders. This suggests that MER benefits from language-like"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "[20] (audio similarity), as well as EnCodec [21] and DAC [22]",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "information, indicating that music segments contain contextual"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "(low-rate audio codecs). For models with two versions trained",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "meaning that can somewhat\nserve as a “language”, which is"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "on different\ninputs or objectives\n(e.g., audio and music), we",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "consistent with the finding in 1). On the\ncontrary, encoders"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "include both. For MERT, we compare all 12 hidden layers for",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "designed for audio compression,\nsuch as DAC and Encodec,"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "layer-wise analysis (see Fig. 1).",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "generally do not perform well\nin MER. This\nis\nlikely due"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "to audio compression leading to information loss\nrelated to"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "IV. MUSIC EMOTION RECOGNITION",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "emotion or\nthe\nemotional\ninformation implicitly residing in"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "In\nthis\nexperiment, we\nuse\nthe Emomusic\nand MMirex",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "the embeddings after compression."
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "datasets. Based on the VA labels from Emomusic for dimen-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "4)\nIn\nthe\nlayer-wise\nresults\nof MERT, we\nobserve\nan"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "sional MER, we\nfurther\ncategorize\nthem into Russell’s\nfour",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "upward-downward trend, consistent with previous\nstudies on"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "quadrants (4Q)\nfor categorical MER: Q1 (-V+A), Q2 (-V-A),",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "speech emotion recognition [28], [29], indicating the similarity"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "Q3 (+V+A), and Q4 (+V-A). Both dimensional and categorical",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "between\nspeech\nand music\nemotions. This\ntrend\nbecomes"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "MER are\nperformed\non\nthis\ndataset. Additionally, MMirex",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "more pronounced when predicting valence, highlighting the"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "is\nused\nfor\ncategorical MER based\non five mood\nclusters.",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "need for deeper encoding to effectively capture valence. This"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "Thus, both dimensional\n(VA)\nand categorical\n(Russell’s 4Q",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "observation further supports the finding that valence in music"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "and five mood clusters)\nemotions\nare\ninvestigated to ensure",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "depends on higher-level musical structures, such as contextual"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "generalizability.",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "elements."
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "For dimensional MER, we use the coefficient of determi-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "Furthermore, the performance using R2 and accuracy differs"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "nation (R2) as\nthe evaluation metric for VA. For categorical",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "significantly, with unweighted accuracy notably outperforming"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "MER, we assess both weighted and unweighted accuracy on",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "weighted\naccuracy,\nindicating\nthe\nimpact\nof\ndata\nbalance."
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "Emomusic, given its imbalanced distribution, and use the F1-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "These findings reveal that emotion bias in MER can stem from"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "score on MIREX,\nfollowing the\nliterature. We\nemploy both",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "various factors, including the audio encoder, evaluation metric,"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "SVM and Multi-Layer Perceptrons (MLP), while maintaining",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "and emotion dimension."
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "consistent experimental\nsettings with previous\nresearch [23].",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "V. MEASURING MUSIC EMOTION VIA FAD"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "The experiments are conducted using an NVIDIA A100 GPU,",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "with models\ntrained using the Adam optimizer at a learning",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "FAD was initially proposed for evaluating music enhance-"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "rate of 0.0001 and a batch size of 32. All models are trained",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "ment quality and has\nshown a close correlation with human"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "under\nthe\nsame\nconditions\nto ensure\nconsistency.\nIt\ncan be",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "perception as\na\nreference-free metric, without\nthe need for"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "observed from Fig. 1 that:",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "model\ntraining\n[30]. Recently,\nit\nhas\nreplaced\nhumans\nin"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "1) Valence\nis more\nchallenging\nto\npredict\nthan\narousal,",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "assessing acoustic and musical quality and similarity, helping"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "which is consistent with previous research [24]. While arousal",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "to mitigate\nsubjective bias. For\ninstance,\nit has been shown"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "is typically conveyed through prosodic features such as pitch,",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "to\neffectively\ndistinguish\nbetween\nreal\nand\nsynthetic\naudio"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "energy, and tempo, valence relies more heavily on the intricate",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "[31], as well as audio with different emotions [32]. In light of"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "aspects of musical composition, as noted by [25]. Moreover,",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "this, we adopt FAD as an objective metric to resolve potential"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "since speech valence is\nsignificantly influenced by language",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "emotion bias arising from various sources (implemented using"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "content\n[26],\nthis\nrelationship\nsuggests\nthe\nexistence\nof\na",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "Microsoft FADTK1)."
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "distinct “language” within music,\nsupporting the conclusions",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "FAD compares acoustic features generated from two audio"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "of\n[27].",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "sets,\nenabling\nit\nto\nassess\ntwo\ndistinct\naudio\nsets without"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "2) For dimensional emotion, a significant performance gap",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "ground truth. The FAD score\nis\ncalculated using multivari-"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "is\nobserved\namong\naudio\nencoders. However,\nthis\ngap\nbe-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "ate Gaussians\nfrom two\nembedding\nand\nsets Na(µa, Σa)"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "comes\nless pronounced for categorical emotion,\nresulting in",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "Nb(µb, Σb) as follows:"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "more comparable performance across different encoders. This",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "(cid:112)"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "(1)\nΣaΣb)\nF (Na, Nb) = ||µa − µb||2 + tr(Σa + Σb − 2"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "phenomenon is evident\nin both categorical quadrants\n(Emo-",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "Music)\nand categorical mood clusters\n(MIREX),\nillustrating",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "where tr\nis\nthe trace of a matrix. To calculate FAD scores,"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "the challenges posed by bias in MER, as discussed in Sec.\nI.",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "we extract\nfeatures from the same audio encoders as used in"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "Consequently, an encoder\nthat performs well with categorical",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "Fig. 1 by computing the average of the FAD scores to mitigate"
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "emotions does not necessarily excel at encoding dimensional",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": ""
        },
        {
          "EMOPIA [1]\ncomprises 387 piano solo performances of": "emotions.",
          "3) Different\nencoders\ncan exhibit\nsignificant performance": "1https://github.com/microsoft/fadtk"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "music\nsets\nbased\non\ncontinuous\nvalues\nis\nchallenging, we",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "Since\nusing\nemotion\nclassifiers\nfor\nguidance\nintroduces"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "only consider discrete labels. The results\nfor Emomusic and",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "additional emotion bias [10], we use emotion labels solely for"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "MMirex are presented in Table I and Table II,\nrespectively.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "conditioning. Two baseline models are selected for generating"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "symbolic music: MIDIEmo [4]\nfor dimensional emotion and"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "TABLE I",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "EmoGen [10]\nfor categorical emotion."
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "FAD SCORES ON EMOMUSIC (CATEGORICAL QUADRANT).",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "MIDIEmo encodes continuous VA labels (e.g., [0.8, 0.8]) as"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "Q1 Q2\nQ1 Q3\nQ1 Q4\nQ2 Q3\nQ2 Q4\nQ3 Q4",
          "A. Baseline Generation Models": "latent embeddings using linear layers, which are then concate-"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "8.69\n33.46\n16.64\n23.07\n33.88\n10.47\nFAD",
          "A. Baseline Generation Models": "nated with music embeddings. We argue that this approach can"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "make music samples with boundary labels more susceptible to"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "subjective bias (as the boundary emotions are usually difficult"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "From Table I, we observe the following: 1) The scores are",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "to distinguish). The\nchallenge of\nassigning boundary labels"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "high between diagonal quadrants\n(i.e., Q1 Q3 and Q2 Q4).",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "with high confidence often leads to ambiguity in the generated"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "This\nis\nreasonable\nsince\nthe polarities of both valence\nand",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "emotion. Consequently, while there is considerable variation,"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal are opposite in diagonal quadrants. 2) The score for",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "ambiguity\nis\nprevalent\nin\nthe\ngenerated music, making\nit"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "Q1 Q2 is lower than that for Q1 Q4, and the score for Q3 Q4",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "difficult\nto perceive precise emotions."
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "is lower than that for Q2 Q3. This indicates that the difference",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "EmoGen uses\nfour pre-extracted embeddings\nto represent"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "between two music sets is smaller when they share the same",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "emotions within the four quadrants. It encodes discrete values"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal polarity but differ\nin valence polarity,\ncompared to",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "of\nthese\nquadrants\n(e.g.,\n1\nor\n2)\ninto\nembeddings\nas\nthe"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "when\nthey\nshare\nthe\nsame\nvalence\npolarity\nbut\ndiffer\nin",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "emotion input,\nthus samples within a particular quadrant share"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal\npolarity. These\nobservations\nsuggest\nthat\narousal\nis",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "the same emotional conditions (i.e.,\nthe emotion embeddings"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "more explicitly expressed in music than valence.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "for all\nsamples\nin each quadrant are identical,\nregardless of"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "their original\nlabels). As a result, while the generated music"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "TABLE II",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "FAD SCORES ON MMIREX (CATEGORICAL CLUSTER).",
          "A. Baseline Generation Models": "exhibits\nprecise\nemotions, we\nargue\nthat\nit\nlacks\nvariation,"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "leading to a situation where all samples with the same emotion"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "C1 C2\nC1 C3\nC1 C4\nC1 C5\nC2 C3",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "tend to sound emotionally similar."
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "FAD\n3.27\n9.08\n3.67\n4.34\n6.05",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "C2 C4\nC2 C5\nC3 C4\nC3 C5\nC4 C5",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "B. Proposed Generation Model"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "FAD\n3.28\n9.27\n5.24\n15.00\n7.61",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "Instead of using either\ncontinuous VA values or discrete"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "From Table II, we observe variations among mood clusters.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "emotion quadrants alone, we combine them in the condition-"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "The\nscore\nfor C1 C2\nis\nthe\nsmallest, while\nthe\nscore\nfor",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "ing process. First, we determine\nthe quadrant based on the"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "C3 C5 is the largest. Although mood clusters do not explicitly",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "continuous VA values (e.g., +V+A corresponds to Q1). Next,"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "represent valence and arousal, we can infer that both C1 (e.g.,",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "we encode both the quadrant and the VA values\ninto hidden"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "Passionate) and C2 (e.g., Cheerful) are associated with high",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "embeddings\nusing\nseparate\nlinear\nlayers. Subsequently, we"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal, which explains\nthe small FAD score between them.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "merge\nthese\nembeddings via\na weighted sum to create\nthe"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "In contrast, C3 (e.g., Wistful) appears\nto be linked with low",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "final emotion embedding:"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal, while C5 (e.g., Aggressive)\nis\nassociated with high",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "arousal, which justifies the relatively large FAD score. These",
          "A. Baseline Generation Models": "(2)\nEmotion = wgtq · embdq + (1 − wgtq) · embdva"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "findings are consistent with prior\nresearch on the relationship",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "where embdq, embdva, and wgtq denote the quadrant embed-"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "between categorical and dimensional music emotion [33]. Due",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "ding, VA embedding, and the weight assigned to the quadrant,"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "to space constraints, we omit detailed descriptions of the mood",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "to 0.5 and use cross-attention to\nrespectively. We set weightq"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "clusters and refer\nreaders to [3]\nfor more information.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "fuse the emotion and music embeddings:"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "Thus, averaging FAD scores\nfrom various encoders offers",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "a more objective manner for measuring emotions, eliminating",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "QeK T"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "m√\n(3)\n)Vm\nEM = Attn(Qe, Km, Vm) = sof tmax("
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "the need to train MER models and select metrics, and thereby",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "dk"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "reducing bias. Furthermore,\nthis approach addresses issues of",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "the respective matrix for\nwhere Qe, Km, and Vm represent"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "imbalanced data.",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "query\n(emotion\nembedding),\nkey\n(music\nembedding),\nand"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "VI. EMOTIONAL MUSIC GENERATION",
          "A. Baseline Generation Models": ""
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "",
          "A. Baseline Generation Models": "is the size of a key vector, and\nvalue (music embedding), dk"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "After benchmarking emotion recognition and introducing",
          "A. Baseline Generation Models": "EM is the emotion-aligned music embedding."
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "the use of FAD for objective measurement, we\nextend this",
          "A. Baseline Generation Models": "Fig. 2 illustrates our proposed model\nand compares\nit\nto"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "approach\nto\nthe\ndomain\nof music\ngeneration\nfor\nemotion",
          "A. Baseline Generation Models": "baseline models in terms of emotion conditioning principles. It"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "assessment. We propose an enhanced generation model\nthat",
          "A. Baseline Generation Models": "is evident that our approach enhances emotion conditioning by"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "integrates both categorical\nand dimensional\nemotions\nto ad-",
          "A. Baseline Generation Models": "combining the strengths of EmoGen and MIDIEmo, enabling"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "dress their respective limitations, and then compare the music",
          "A. Baseline Generation Models": "a balance and trade-off between prominence and variability"
        },
        {
          "the bias\nintroduced by any single\nencoder. Since\nseparating": "generated by our model with that produced by two baselines.",
          "A. Baseline Generation Models": "in the generated emotion."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "1) The FAD scores for Q1 Q2 and Q3 Q4 are notably lower"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "compared to other quadrant pairs. As discussed in Sec. V,"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "this\nis\nlikely because\nthe\nemotions\nin these quadrant pairs"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "share the same arousal polarity, making it more challenging"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "to distinguish between them."
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "2) EmoGen achieves the highest FAD scores, indicating that"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "it excels at differentiating between various emotions based on"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "their musical characteristics. This is consistent with its training"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "principle, where all music pieces within a given quadrant are"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "conditioned to express the same emotion. However,\nthis prac-"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "tice results in less emotional variation, making the generated"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "emotions appear\nless natural. An advantage of EmoGen is its"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "effective differentiation for Q1 Q2 and Q3 Q4, as it achieves"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "higher scores than those of\nreal music."
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "3) MIDIEmo exhibits greater variability than EmoGen. Al-"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "though its overall FAD scores are lower than those of EmoGen,"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "they are\nstill higher\nthan those\nfor\nreal music, particularly"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "for Q1 Q3 and Q2 Q4. Since emotions in diagonal quadrants"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "are\ninherently more\ndistinct,\na\nhigh\nscore\nin\nthese\ncases"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "might cause the generated emotional music to sound overly"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "pronounced or exaggerated."
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "4) Our\noverall\nFAD scores\nare\ncloser\nto\nthose\nof\nreal"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "music\ncompared to EmoGen and MIDIEmo,\nindicating that"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "our model generates more realistic emotions. Furthermore,\nthe"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "score for Q3 Q4 in our model is higher than that of MIDIEmo"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "and real music, demonstrating that our\napproach effectively"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "leverages\nthe strengths of EmoGen. As discussed in Sec. V,"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "relatively high scores\nfor Q1 Q2 and Q3 Q4 are beneficial"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "for distinguishing between these quadrants. Additionally,\nthe"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "emotion embedding in our model\nis naturally aligned with"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "the music\nembedding\nthrough\ncross-attention,\nresulting\nin"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "overall\nlower\nscores\ncompared\nto MIDIEmo, which\nsimply"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "concatenates emotion and music embeddings."
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "VII. CONCLUSION"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "In\nthis\nstudy, we\naddress\nthe\nissue\nof\nemotion\nbias\nin"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "music,\nfocusing on both its\nrecognition and generation. We"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "benchmark MER across diverse music datasets, highlighting"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "discrepancies between categorical and dimensional emotions,"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "as well as\nthe limitations of\nrelying on a single encoder or"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "metric description model. To address these issues, we propose"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "employing FAD with\nvarious\naudio\nencoders\nas\nan\nobjec-"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "tive measurement. Additionally, we\nintroduce\nan\nenhanced"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "model for emotion generation that integrates the advantages of"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "both categorical and dimensional emotions,\nresulting in more"
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": ""
        },
        {
          "Ours (syn.)\n1.61\n33.13\n26.10\n17.93\n15.96\n5.99": "realistic\nemotional\nexpression in the generated music, with"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "language\nsupervision\nfor\ngeneral-purpose\naudio\nrepresentations,”\nin"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "the effectiveness of our approach. This study underscores the",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP.\nIEEE, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "issue of\nemotion bias\nin music\nand provides novel\ninsights",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[18] Yusong Wu, Ke Chen,\nTianyu\nZhang, Yuchen Hui,\nTaylor Berg-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "into addressing this challenge.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Kirkpatrick,\nand Shlomo Dubnov,\n“Large-scale\ncontrastive\nlanguage-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "audio pretraining with feature fusion and keyword-to-caption augmen-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "ACKNOWLEDGMENT",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "tation,”\nin ICASSP.\nIEEE, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[19] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "We\nthank Xu Tan\n(former Principal Research Manager),",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Peiling Lu (former Research Software Development Engineer),",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "et al.,\n“MERT: Acoustic music understanding model with large-scale"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Chenfei Kang (former\nintern)\nfrom Microsoft Research",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "self-supervised training,”\nin ICLR, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[20]\nPranay Manocha, Zeyu\nJin, Richard Zhang,\nand Adam Finkelstein,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Asia for their assistance in implementing EmoGen, and Serkan",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“CDPAM: Contrastive\nlearning\nfor\nperceptual\naudio\nsimilarity,”\nin"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Sulun from INESC TEC for his help in implementing MI-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP.\nIEEE, 2021."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "DIEmo.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[21] Alexandre D´efossez,\nJade Copet, Gabriel Synnaeve,\nand Yossi Adi,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Transactions on Machine\n“High fidelity neural\naudio compression,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Yuanchao appreciates the feedback from all members of the",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Learning Research, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Audio and Acoustics Research Group at Microsoft Research,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[22] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs,\nIshaan Kumar,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "as well\nas\nthe great\nsummer\nspent with fellow interns Ard",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "and Kundan Kumar,\n“High-fidelity audio compression with improved"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "rvqgan,” Advances in Neural\nInformation Processing Systems, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Kastrati, Azalea Gui, Eloi Moliner\nJuanpere, Michele Man-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[23] Rodrigo Castellon, Chris Donahue, and Percy Liang,\n“Codified audio"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "cusi, Ruihan Yang, and Tanmay Srivastava.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "language modeling learns useful\nrepresentations for music information"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "retrieval,”\nin ISMIR, 2021."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "REFERENCES",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[24] Youngmoo E Kim, Erik M Schmidt, Raymond Migneco, Brandon G"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[1] Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Morton,\nPatrick Richardson,\nJeffrey\nScott,\nJacquelin A Speck,\nand"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Yi-Hsuan Yang,\n“EMOPIA: A multi-modal pop piano dataset\nfor",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Douglas Turnbull,\n“Music\nemotion\nrecognition: A state\nof\nthe\nart"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "emotion recognition and emotion-based music generation,”\nin ISMIR,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "review,”\nin ISMIR, 2010."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2021.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[25]\nSylvie Droit-Volet, Danilo Ramos, Jos´e LO Bueno, and Emmanuel Bi-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[2] Yi-Hsuan Yang and Homer H Chen, Music emotion recognition, CRC",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "gand, “Music, emotion, and time perception:\nthe influence of subjective"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Press, 2011.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "emotional valence and arousal?,” Frontiers in Psychology, 2013."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[3] Xiao Hu and J Stephen Downie,\n“Exploring mood metadata: Relation-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[26] Yuanchao Li, Carlos Toshinori Ishi, Koji Inoue, Shizuka Nakamura, and"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "ships with genre, artist and usage metadata.,”\nin ISMIR, 2007.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Tatsuya Kawahara,\n“Expressing reactive emotion based on multimodal"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[4]\nSerkan Sulun, Matthew EP Davies, and Paula Viana,\n“Symbolic music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "emotion recognition for natural\nconversation in human–robot\ninterac-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "generation conditioned on continuous-valued emotions,”\nIEEE Access,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "tion,” Advanced Robotics, vol. 33, no. 20, pp. 1030–1041, 2019."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "vol. 10, pp. 44617–44626, 2022.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[27]\nLaura McPherson,\n“The\nrole of music\nin documenting phonological"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[5] Carol L Krumhansl,\n“Music: A link between cognition and emotion,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "the\ngrammar: Two case studies\nfrom west africa,”\nin Proceedings of"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Current directions in psychological science, vol. 11, no. 2, pp. 45–50,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Annual Meetings on Phonology, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2002.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[28] Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai, “Explo-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[6]\nSusan Hallam, Ian Cross, and Michael Thaut, Oxford handbook of music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ration of a self-supervised speech model: A study on emotional corpora,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "psychology, Oxford University Press, 2009.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "in SLT.\nIEEE, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[7]\nPatrick G Hunter\nand E Glenn Schellenberg,\n“Music\nand emotion,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[29] Alexandra Saliba, Yuanchao Li, Ramon Sanabria,\nand Catherine Lai,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Music perception, pp. 129–164, 2010.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“Layer-wise analysis of\nself-supervised acoustic word embeddings: A"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[8] Yi-Wei Chen, Yi-Hsuan Yang,\nand Homer H Chen,\n“Cross-cultural",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "study on speech emotion recognition,”\nin ICASSP SASB Workshop."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "music emotion recognition by adversarial discriminative domain adapta-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "IEEE, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "tion,” in 2018 17th IEEE International Conference on Machine Learning",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[30] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Shar-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Applications (ICMLA).\nIEEE, 2018, pp. 467–472.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ifi, “Fr´echet audio distance: A reference-free metric for evaluating music"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[9]\nJames A Russell, “A circumplex model of affect.,” Journal of personality",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "enhancement algorithms.,”\nin INTERSPEECH, 2019, pp. 2350–2354."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and social psychology, 1980.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[31] Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouili-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[10] Chenfei Kang, Peiling Lu, Botao Yu, Xu Tan, Wei Ye, Shikun Zhang,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "dou, “Adapting frechet audio distance for generative music evaluation,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Jiang Bian,\n“EmoGen: Eliminating subjective bias\nin emotional",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "music generation,” arXiv preprint arXiv:2307.01229, 2023.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[32] Yujia Sun, Zeyu Zhao, Korin Richmond, and Yuanchao Li,\n“Exploring"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[11] Yading Song, Simon Dixon, Marcus T Pearce, and Andrea R Halpern,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "acoustic similarity in emotional\nspeech and music via self-supervised"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "“Perceived and induced emotion responses to popular music: Categorical",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "representations,”\nin ICASSP 2025-2025 IEEE International Conference"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Music\nPerception:\nAn\nInterdisciplinary\nand\ndimensional models,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Journal, vol. 33, no. 4, pp. 472–492, 2016.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "1–5."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[12] Rishi Madhok, Shivali Goel, and Shweta Garg,\n“SentiMozart: Music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[33] Yu Hong, Chuck-Jee Chau, and Andrew Horner,\n“An analysis of\nlow-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "generation based on emotions.,”\nin ICAART (2), 2018, pp. 501–506.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "arousal piano music ratings to uncover what makes calm and sad music"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[13] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "the\nso difficult\nto distinguish in music emotion recognition,” Journal of"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Sha, and Yi-Hsuan Yang, “1000 songs for emotional analysis of music,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Audio Engineering Society, 2017."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "in 2nd ACM international workshop on Crowdsourcing for multimedia,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[34] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\nIan Simon,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2013.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Curtis Hawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[14] Renato\nEduardo\nSilva\nPanda,\nRicardo Malheiro,\nBruno\nRocha,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Monica Dinculescu, and Douglas Eck,\n“Music transformer: Generating"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Ant´onio Pedro Oliveira,\nand Rui Pedro Paiva,\n“Multi-modal music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "music with long-term structure,” in International Conference on Learn-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "emotion\nrecognition: A new dataset, methodology\nand\ncomparative",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ing Representations, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "analysis,”\nin CMMR, 2013.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[35] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang,\nand Yi-Hsuan Yang,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[15]\nJohn Stephen Downie, Cyril Laurier, and Andreas Ehmann, “The 2007",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“Musegan: Multi-track sequential generative\nadversarial networks\nfor"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "mirex audio mood classification task: Lessons learned,” in Proc. 9th Int.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "symbolic music generation and accompaniment,”\nin AAAI, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Conf. Music Inf. Retrieval, 2008, pp. 462–467.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[36]\nLucas N Ferreira, Lili Mou, Jim Whitehead, and Levi HS Lelis,\n“Con-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[16]\nShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "trolling perceived emotion in symbolic music generation with monte"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "carlo tree search,”\nin AAAI, 2022."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Saurous, Bryan Seybold, et al., “Cnn architectures for large-scale audio",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "language\nsupervision\nfor\ngeneral-purpose\naudio\nrepresentations,”\nin"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "the effectiveness of our approach. This study underscores the",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP.\nIEEE, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "issue of\nemotion bias\nin music\nand provides novel\ninsights",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[18] Yusong Wu, Ke Chen,\nTianyu\nZhang, Yuchen Hui,\nTaylor Berg-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "into addressing this challenge.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Kirkpatrick,\nand Shlomo Dubnov,\n“Large-scale\ncontrastive\nlanguage-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "audio pretraining with feature fusion and keyword-to-caption augmen-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "ACKNOWLEDGMENT",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "tation,”\nin ICASSP.\nIEEE, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[19] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "We\nthank Xu Tan\n(former Principal Research Manager),",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Peiling Lu (former Research Software Development Engineer),",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "et al.,\n“MERT: Acoustic music understanding model with large-scale"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Chenfei Kang (former\nintern)\nfrom Microsoft Research",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "self-supervised training,”\nin ICLR, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[20]\nPranay Manocha, Zeyu\nJin, Richard Zhang,\nand Adam Finkelstein,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Asia for their assistance in implementing EmoGen, and Serkan",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“CDPAM: Contrastive\nlearning\nfor\nperceptual\naudio\nsimilarity,”\nin"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Sulun from INESC TEC for his help in implementing MI-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP.\nIEEE, 2021."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "DIEmo.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[21] Alexandre D´efossez,\nJade Copet, Gabriel Synnaeve,\nand Yossi Adi,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Transactions on Machine\n“High fidelity neural\naudio compression,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Yuanchao appreciates the feedback from all members of the",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Learning Research, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Audio and Acoustics Research Group at Microsoft Research,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[22] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs,\nIshaan Kumar,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "as well\nas\nthe great\nsummer\nspent with fellow interns Ard",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "and Kundan Kumar,\n“High-fidelity audio compression with improved"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "rvqgan,” Advances in Neural\nInformation Processing Systems, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Kastrati, Azalea Gui, Eloi Moliner\nJuanpere, Michele Man-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[23] Rodrigo Castellon, Chris Donahue, and Percy Liang,\n“Codified audio"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "cusi, Ruihan Yang, and Tanmay Srivastava.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "language modeling learns useful\nrepresentations for music information"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "retrieval,”\nin ISMIR, 2021."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "REFERENCES",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[24] Youngmoo E Kim, Erik M Schmidt, Raymond Migneco, Brandon G"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[1] Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Morton,\nPatrick Richardson,\nJeffrey\nScott,\nJacquelin A Speck,\nand"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Yi-Hsuan Yang,\n“EMOPIA: A multi-modal pop piano dataset\nfor",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Douglas Turnbull,\n“Music\nemotion\nrecognition: A state\nof\nthe\nart"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "emotion recognition and emotion-based music generation,”\nin ISMIR,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "review,”\nin ISMIR, 2010."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2021.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[25]\nSylvie Droit-Volet, Danilo Ramos, Jos´e LO Bueno, and Emmanuel Bi-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[2] Yi-Hsuan Yang and Homer H Chen, Music emotion recognition, CRC",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "gand, “Music, emotion, and time perception:\nthe influence of subjective"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Press, 2011.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "emotional valence and arousal?,” Frontiers in Psychology, 2013."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[3] Xiao Hu and J Stephen Downie,\n“Exploring mood metadata: Relation-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[26] Yuanchao Li, Carlos Toshinori Ishi, Koji Inoue, Shizuka Nakamura, and"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "ships with genre, artist and usage metadata.,”\nin ISMIR, 2007.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Tatsuya Kawahara,\n“Expressing reactive emotion based on multimodal"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[4]\nSerkan Sulun, Matthew EP Davies, and Paula Viana,\n“Symbolic music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "emotion recognition for natural\nconversation in human–robot\ninterac-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "generation conditioned on continuous-valued emotions,”\nIEEE Access,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "tion,” Advanced Robotics, vol. 33, no. 20, pp. 1030–1041, 2019."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "vol. 10, pp. 44617–44626, 2022.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[27]\nLaura McPherson,\n“The\nrole of music\nin documenting phonological"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[5] Carol L Krumhansl,\n“Music: A link between cognition and emotion,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "the\ngrammar: Two case studies\nfrom west africa,”\nin Proceedings of"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Current directions in psychological science, vol. 11, no. 2, pp. 45–50,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Annual Meetings on Phonology, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2002.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[28] Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai, “Explo-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[6]\nSusan Hallam, Ian Cross, and Michael Thaut, Oxford handbook of music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ration of a self-supervised speech model: A study on emotional corpora,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "psychology, Oxford University Press, 2009.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "in SLT.\nIEEE, 2023."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[7]\nPatrick G Hunter\nand E Glenn Schellenberg,\n“Music\nand emotion,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[29] Alexandra Saliba, Yuanchao Li, Ramon Sanabria,\nand Catherine Lai,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Music perception, pp. 129–164, 2010.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“Layer-wise analysis of\nself-supervised acoustic word embeddings: A"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[8] Yi-Wei Chen, Yi-Hsuan Yang,\nand Homer H Chen,\n“Cross-cultural",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "study on speech emotion recognition,”\nin ICASSP SASB Workshop."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "music emotion recognition by adversarial discriminative domain adapta-",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "IEEE, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "tion,” in 2018 17th IEEE International Conference on Machine Learning",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[30] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Shar-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Applications (ICMLA).\nIEEE, 2018, pp. 467–472.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ifi, “Fr´echet audio distance: A reference-free metric for evaluating music"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[9]\nJames A Russell, “A circumplex model of affect.,” Journal of personality",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "enhancement algorithms.,”\nin INTERSPEECH, 2019, pp. 2350–2354."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and social psychology, 1980.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[31] Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouili-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[10] Chenfei Kang, Peiling Lu, Botao Yu, Xu Tan, Wei Ye, Shikun Zhang,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "dou, “Adapting frechet audio distance for generative music evaluation,”"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "and Jiang Bian,\n“EmoGen: Eliminating subjective bias\nin emotional",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ICASSP, 2024."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "music generation,” arXiv preprint arXiv:2307.01229, 2023.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[32] Yujia Sun, Zeyu Zhao, Korin Richmond, and Yuanchao Li,\n“Exploring"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[11] Yading Song, Simon Dixon, Marcus T Pearce, and Andrea R Halpern,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "acoustic similarity in emotional\nspeech and music via self-supervised"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "“Perceived and induced emotion responses to popular music: Categorical",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "representations,”\nin ICASSP 2025-2025 IEEE International Conference"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Music\nPerception:\nAn\nInterdisciplinary\nand\ndimensional models,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Journal, vol. 33, no. 4, pp. 472–492, 2016.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "1–5."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[12] Rishi Madhok, Shivali Goel, and Shweta Garg,\n“SentiMozart: Music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[33] Yu Hong, Chuck-Jee Chau, and Andrew Horner,\n“An analysis of\nlow-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "generation based on emotions.,”\nin ICAART (2), 2018, pp. 501–506.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "arousal piano music ratings to uncover what makes calm and sad music"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[13] Mohammad Soleymani, Micheal N Caro, Erik M Schmidt, Cheng-Ya",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "the\nso difficult\nto distinguish in music emotion recognition,” Journal of"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Sha, and Yi-Hsuan Yang, “1000 songs for emotional analysis of music,”",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Audio Engineering Society, 2017."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "in 2nd ACM international workshop on Crowdsourcing for multimedia,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[34] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\nIan Simon,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2013.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Curtis Hawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[14] Renato\nEduardo\nSilva\nPanda,\nRicardo Malheiro,\nBruno\nRocha,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "Monica Dinculescu, and Douglas Eck,\n“Music transformer: Generating"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Ant´onio Pedro Oliveira,\nand Rui Pedro Paiva,\n“Multi-modal music",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "music with long-term structure,” in International Conference on Learn-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "emotion\nrecognition: A new dataset, methodology\nand\ncomparative",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "ing Representations, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "analysis,”\nin CMMR, 2013.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[35] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang,\nand Yi-Hsuan Yang,"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[15]\nJohn Stephen Downie, Cyril Laurier, and Andreas Ehmann, “The 2007",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "“Musegan: Multi-track sequential generative\nadversarial networks\nfor"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "mirex audio mood classification task: Lessons learned,” in Proc. 9th Int.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "symbolic music generation and accompaniment,”\nin AAAI, 2018."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Conf. Music Inf. Retrieval, 2008, pp. 462–467.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "[36]\nLucas N Ferreira, Lili Mou, Jim Whitehead, and Levi HS Lelis,\n“Con-"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "[16]\nShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke,",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "trolling perceived emotion in symbolic music generation with monte"
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": "carlo tree search,”\nin AAAI, 2022."
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "Saurous, Bryan Seybold, et al., “Cnn architectures for large-scale audio",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "2017\nieee\ninternational\nconference\non\nacoustics,\nclassification,”\nin",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        },
        {
          "evaluation of real and synthetic music using FAD demonstrates": "speech and signal processing (icassp).\nIEEE, 2017, pp. 131–135.",
          "[17] Benjamin Elizalde, Soham Deshmukh, and Huaming Wang,\n“Natural": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Hsiao-Tzu Hung",
        "Joann Ching",
        "Seungheon Doh",
        "Nabin Kim",
        "Juhan Nam",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation"
    },
    {
      "citation_id": "2",
      "title": "Music emotion recognition",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2011",
      "venue": "Music emotion recognition"
    },
    {
      "citation_id": "3",
      "title": "Exploring mood metadata: Relationships with genre, artist and usage metadata",
      "authors": [
        "Xiao Hu",
        "J Stephen Downie"
      ],
      "year": "2007",
      "venue": "ISMIR"
    },
    {
      "citation_id": "4",
      "title": "Symbolic music generation conditioned on continuous-valued emotions",
      "authors": [
        "Serkan Sulun",
        "Matthew Davies",
        "Paula Viana"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Music: A link between cognition and emotion",
      "authors": [
        "L Carol",
        "Krumhansl"
      ],
      "year": "2002",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "6",
      "title": "Oxford handbook of music psychology",
      "authors": [
        "Susan Hallam",
        "Ian Cross",
        "Michael Thaut"
      ],
      "year": "2009",
      "venue": "Oxford handbook of music psychology"
    },
    {
      "citation_id": "7",
      "title": "Music and emotion",
      "authors": [
        "G Patrick",
        "E Hunter",
        "Schellenberg Glenn"
      ],
      "year": "2010",
      "venue": "Music and emotion"
    },
    {
      "citation_id": "8",
      "title": "Cross-cultural music emotion recognition by adversarial discriminative domain adaptation",
      "authors": [
        "Yi-Wei Chen",
        "Yi-Hsuan Yang",
        "Homer H Chen"
      ],
      "year": "2018",
      "venue": "2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "EmoGen: Eliminating subjective bias in emotional music generation",
      "authors": [
        "Chenfei Kang",
        "Peiling Lu",
        "Botao Yu",
        "Xu Tan",
        "Wei Ye",
        "Shikun Zhang",
        "Jiang Bian"
      ],
      "year": "2023",
      "venue": "EmoGen: Eliminating subjective bias in emotional music generation",
      "arxiv": "arXiv:2307.01229"
    },
    {
      "citation_id": "11",
      "title": "Perceived and induced emotion responses to popular music: Categorical and dimensional models",
      "authors": [
        "Yading Song",
        "Simon Dixon",
        "Marcus Pearce",
        "Andrea Halpern"
      ],
      "year": "2016",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "12",
      "title": "SentiMozart: Music generation based on emotions",
      "authors": [
        "Rishi Madhok",
        "Shivali Goel",
        "Shweta Garg"
      ],
      "year": "2018",
      "venue": "SentiMozart: Music generation based on emotions"
    },
    {
      "citation_id": "13",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "2nd ACM international workshop on Crowdsourcing for multimedia"
    },
    {
      "citation_id": "14",
      "title": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "Renato Eduardo",
        "Silva Panda",
        "Ricardo Malheiro",
        "Bruno Rocha",
        "Pedro Oliveira",
        "Rui Pedro"
      ],
      "year": "2013",
      "venue": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis"
    },
    {
      "citation_id": "15",
      "title": "The 2007 mirex audio mood classification task: Lessons learned",
      "authors": [
        "John Stephen Downie",
        "Cyril Laurier",
        "Andreas Ehmann"
      ],
      "year": "2008",
      "venue": "Proc. 9th Int. Conf. Music Inf"
    },
    {
      "citation_id": "16",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "17",
      "title": "Natural language supervision for general-purpose audio representations",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Huaming Wang"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Large-scale contrastive languageaudio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "MERT: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos"
      ],
      "year": "2023",
      "venue": "ICLR"
    },
    {
      "citation_id": "20",
      "title": "CDPAM: Contrastive learning for perceptual audio similarity",
      "authors": [
        "Pranay Manocha",
        "Zeyu Jin",
        "Richard Zhang",
        "Adam Finkelstein"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "21",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "year": "2023",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "22",
      "title": "High-fidelity audio compression with improved rvqgan",
      "authors": [
        "Rithesh Kumar",
        "Prem Seetharaman",
        "Alejandro Luebs",
        "Ishaan Kumar",
        "Kundan Kumar"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Codified audio language modeling learns useful representations for music information retrieval",
      "authors": [
        "Rodrigo Castellon",
        "Chris Donahue",
        "Percy Liang"
      ],
      "year": "2021",
      "venue": "ISMIR"
    },
    {
      "citation_id": "24",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Erik Youngmoo E Kim",
        "Raymond Schmidt",
        "Migneco",
        "Patrick Brandon G Morton",
        "Jeffrey Richardson",
        "Jacquelin Scott",
        "Douglas Speck",
        "Turnbull"
      ],
      "year": "2010",
      "venue": "Music emotion recognition: A state of the art review"
    },
    {
      "citation_id": "25",
      "title": "Music, emotion, and time perception: the influence of subjective emotional valence and arousal?",
      "authors": [
        "Sylvie Droit-Volet",
        "Danilo Ramos",
        "José Lo Bueno",
        "Emmanuel Bigand"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "26",
      "title": "Expressing reactive emotion based on multimodal emotion recognition for natural conversation in human-robot interaction",
      "authors": [
        "Yuanchao Li",
        "Carlos Toshinori Ishi",
        "Koji Inoue",
        "Shizuka Nakamura",
        "Tatsuya Kawahara"
      ],
      "year": "2019",
      "venue": "Advanced Robotics"
    },
    {
      "citation_id": "27",
      "title": "The role of music in documenting phonological grammar: Two case studies from west africa",
      "authors": [
        "Laura Mcpherson"
      ],
      "year": "2018",
      "venue": "Proceedings of the Annual Meetings on Phonology"
    },
    {
      "citation_id": "28",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "SLT"
    },
    {
      "citation_id": "29",
      "title": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "authors": [
        "Alexandra Saliba",
        "Yuanchao Li",
        "Ramon Sanabria",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "ICASSP SASB Workshop"
    },
    {
      "citation_id": "30",
      "title": "Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms",
      "authors": [
        "Kevin Kilgour",
        "Mauricio Zuluaga",
        "Dominik Roblek",
        "Matthew Sharifi"
      ],
      "year": "2019",
      "venue": "Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms"
    },
    {
      "citation_id": "31",
      "title": "Adapting frechet audio distance for generative music evaluation",
      "authors": [
        "Azalea Gui",
        "Hannes Gamper",
        "Sebastian Braun",
        "Dimitra Emmanouilidou"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "32",
      "title": "Exploring acoustic similarity in emotional speech and music via self-supervised representations",
      "authors": [
        "Yujia Sun",
        "Zeyu Zhao",
        "Korin Richmond",
        "Yuanchao Li"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "An analysis of lowarousal piano music ratings to uncover what makes calm and sad music so difficult to distinguish in music emotion recognition",
      "authors": [
        "Yu Hong",
        "Chuck-Jee Chau",
        "Andrew Horner"
      ],
      "year": "2017",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "34",
      "title": "Music transformer: Generating music with long-term structure",
      "authors": [
        "Cheng-Zhi Anna Huang",
        "Ashish Vaswani",
        "Jakob Uszkoreit",
        "Ian Simon",
        "Curtis Hawthorne",
        "Noam Shazeer",
        "Andrew Dai",
        "Matthew Hoffman",
        "Monica Dinculescu",
        "Douglas Eck"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "35",
      "title": "Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment",
      "authors": [
        "Hao-Wen",
        "Wen-Yi Dong",
        "Li-Chia Hsiao",
        "Yi-Hsuan Yang",
        "Yang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "36",
      "title": "Controlling perceived emotion in symbolic music generation with monte carlo tree search",
      "authors": [
        "Lili Lucas N Ferreira",
        "Jim Mou",
        "Levi Hs Whitehead",
        "Lelis"
      ],
      "year": "2022",
      "venue": "AAAI"
    }
  ]
}