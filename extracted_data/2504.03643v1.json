{
  "paper_id": "2504.03643v1",
  "title": "Potential Indicator For Continuous Emotion Arousal By Dynamic Neural Synchrony",
  "published": "2025-01-24T02:54:28Z",
  "authors": [
    "Guandong Pan",
    "Zhaobang Wu",
    "Yaqian Yang",
    "Xin Wang",
    "Longzhao Liu",
    "Zhiming Zheng",
    "Shaoting Tang"
  ],
  "keywords": [
    "Emotion Annotation",
    "Inter-subject Correlation",
    "Electroencephalography (EEG)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The need for automatic and high-quality emotion annotation is paramount in applications such as continuous emotion recognition and video highlight detection, yet achieving this through manual human annotations is challenging. Inspired by inter-subject correlation (ISC) utilized in neuroscience, this study introduces a novel Electroencephalography (EEG) based ISC methodology that leverages a single-electrode and feature-based dynamic approach. Our contributions are three folds: Firstly, we reidentify two potent emotion features suitable for classifying emotions-first-order difference (FD) an differential entropy (DE). Secondly, through the use of overall correlation analysis, we demonstrate the heterogeneous synchronized performance of electrodes. This performance aligns with neural emotion patterns established in prior studies, thus validating the effectiveness of our approach. Thirdly, by employing a sliding window correlation technique, we showcase the significant consistency of dynamic ISCs across various features or key electrodes in each analyzed film clip. Our findings indicate the method's reliability in capturing consistent, dynamic shared neural synchrony among individuals, triggered by evocative film stimuli. This underscores the potential of our approach to serve as an indicator of continuous human emotion arousal. The implications of this research are significant for advancements in affective computing and the broader neuroscience field, suggesting a streamlined and effective tool for emotion analysis in real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Capturing the ground truth of dynamic human emotions is pivotal for advancements in continuous emotion recognition and video highlight detection  [3] ,  [6] ,  [8] . Traditionally, the collection of two-dimensional emotional responses, namely arousal and valence, has relied on subjective human annotations  [8] ,  [38] . However, the dual demands of video observation and annotation tasks impose a cognitive burden on annotators, which can compromise the quality of the data collected. Additionally, the requirement for conscious rating in such tasks may affect long-term efficiency. These limitations pose a significant scientific challenge: Is there an alternative method to annotate human emotions both unconsciously and efficiently?\n\nThe technique of inter-subject correlation (ISC), derived from neuroscience, offers a promising approach to capturing group-level shared neural responses without conscious effort from participants  [16] ,  [17] ,  [18] ,  [29] . This neural synchrony, reflecting the processing of emotional information in response to evocative video content  [36] , serves as an indicator of variations in human emotional arousal. Traditional ISC research, primarily utilizing functional Magnetic Resonance Imaging (fMRI), has established the reliability of this method across different cinematic experiences  [16] . In contrast, electroencephalography (EEG) offers advantages over fMRI in terms of affordability, portability, and high temporal resolution, making it more applicable to real-world tasks  [40] . Recent studies have applied EEG to extract ISC, but have predominantly focused on multichannel analyses  [9] ,  [10] ,  [11] , which do not meet the practical needs of applications requiring fewer channels. To address this limitation, we propose a novel feature-based dynamic EEG-based ISC method computed on each individual channel. This approach can autonomously capture the continuous synchronization of brain regions among a population, thereby providing a potential neuralbased indicators for representing the ground truth of human dynamic emotion arousal.\n\nIn brief, the contributions of our work are as follows:\n\n-We reidentify two robust emotion features that are pivotal in distinguishing emotional states. The first feature, derived from the time domain, is the first-order difference (FD), and the second, from the frequency domain, is differential entropy (DE). -Utilizing an overall correlation analysis, this study demonstrates the heterogeneous synchronized performance across electrodes. This finding is in alignment with established neural emotion patterns from previous research, thus serving as a validation of our methodology's effectiveness. -By employing a sliding window correlation technique, we illustrate the consistency of dynamic ISCs across various features or key electrodes within each analyzed film clip. This consistency underscores the reliability of our method in capturing dynamic shared neural synchrony among individuals, triggered by emotionally evocative film stimuli. Our approach suggests significant potential to serve as an indicator of continuous human emotion arousal.\n\n2 Related Works",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Inter-Subject Correlation",
      "text": "Inter-subject correlation (ISC) quantifies the similarity in brain responses among individuals exposed to the same naturalistic stimuli, such as films or stories. In the seminal work  [17] , Hasson et al. pioneered the exploration of ISC by demonstrating synchronous neural activity across participants during film viewing, not only in primary sensory areas but also in higher associative cortices involved in complex cognitive functions such as face recognition. Over the past decades, the intuitive underlying principles of ISC have facilitated its adoption across a wide range of fields, yielding significant insights into mechanisms of attention  [23] ,  [33] ,  [35] , friendship prediction  [31] , emotion  [8] ,  [20] ,  [30] , marketing  [2] ,  [9] , memory processes  [5] ,  [15] , video highlight detection  [6] , healthcare applications  [14] , and even interspecies comparisons  [27] .\n\nHistorically, the majority of ISC research utilized functional Magnetic Resonance Imaging (fMRI). However, there has been a gradual shift towards incorporating alternative modalities such as Electroencephalography (EEG)  [8] ,  [9] ,  [25] , Electrocorticography (ECoG)  [19] , Magnetoencephalography (MEG)  [24] , functional Near-Infrared Spectroscopy (fNIRS)  [26] , Electrocardiography (ECG)  [34] , analysis of eye movements  [18] , and other physiological signals  [6] . Notably, Haufe et al.  [19]  confirmed that fMRI, ECoG, and EEG exhibit comparable repeat-reliability across viewings, highlighting EEG's potential for capturing shared neural responses with its high temporal resolution and suitability for naturalistic settings due to portable equipment.\n\nNevertheless, a significant gap in the literature exists regarding the practical implementation of ISC using EEG in real-world environments, where the use of a minimal number of electrodes is preferable. While past studies often employed multiple electrodes to enhance analysis performance  [9] ,  [11] ,  [23] ,  [35] , this approach is less feasible in the real world, potentially limiting the broader application of EEG-based ISC techniques. Addressing this gap by developing methodologies for ISC analysis with fewer electrodes could significantly expand the utility and accessibility of EEG for real-world applications. By isolating the ISC analysis to each individual electrode, we leverage both overall and sliding window correlation techniques to meticulously assess the potential of singlechannel data to capture neural synchrony. Results demonstrate the reliability of our methodology in capturing dynamic shared neural synchrony, suggesting the potential to serve as a reliable indicator of continuous emotional arousal. This approach not only simplifies the hardware requirements for real-world applications but also maintains analytical precision, thus significantly advancing the practical deployment of EEG-based ISC techniques.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg Feature Extraction",
      "text": "Traditionally, inter-subject correlation (ISC) techniques have been applied directly to raw brain signals. However, in the realm of EEG-based emotion recognition, a variety of robust feature extraction methods have been developed. These methods have proven effective in distinguishing emotional valences through model training and testing. This presents a compelling case for implementing featurerelated ISC analysis.\n\nIn 2013, the Differential Entropy (DE) feature was introduced  [12] , with subsequent studies affirming its efficacy in representing emotional states  [40] ,  [41] ,  [45] . A comprehensive review in 2019  [42]  evaluated EEG features across four domains-time, frequency, time-frequency, and spatial-using the sparse linear discriminant analysis (SLDA) method across three distinct datasets (SEED, DREAMER, CAS-THU). This review highlighted that time-domain features, particularly the first-order difference (FD), exhibited superior performance in discriminating emotional valence.\n\nMotivated by these findings, our study begins with a user-dependent comparative analysis of features to ascertain their efficacy in extracting emotional information. Following this preliminary evaluation, DE and FD features are identified as particularly potent and are subsequently selected for further analysis in extracting neural synchrony. This approach aims to enhance the applicability of EEG-based single-channel ISC techniques by leveraging sophisticated featurebased analyses.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Materials And Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset And Preprocessing",
      "text": "The SEED EEG dataset, a publicly available affective dataset, was introduced by Zheng et al.  [40] . It comprises EEG recordings from fifteen Chinese participants (seven males and eight females; mean age: 23.27, SD: 2.37) who viewed various Chinese film clips designed to evoke distinct emotional valences: positive, negative, and neutral. Each participant was exposed to the same 15 clips, approximately four minutes each, across three separate sessions on different days. EEG data were captured using a 62-channel setup conforming to the International 10-20 system via the ESI Neuroscan system. For our study, 45 sessions (15 participants, three sessions each) of EEG records are analyzed to extract statistical shared neural responses at the population level for each film clip.\n\nOur analysis utilize the \"Preprocessed Data\" set, sampled at 200 Hz and bandpass filtered from 0 to 75 Hz. Using MATLAB's EEGLAB toolbox  [7] , we apply a notch filter between 48 and 52 Hz to eliminate electrical noise. Visual inspections of channel data for each session are used for the interpolation of faulty channels. Additionally, artifacts from eye movements and muscle contractions are isolated and removed through independent component analysis (ICA) implemented in EEGLAB.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg-Based Isc",
      "text": "Feature extraction Original features in EEG analysis are derived from the absolute raw values of EEG signals following preprocessing, whereas first-order difference features represent the differences between consecutive sample values. First-order difference features are particularly valuable as they capture the nonlinear dynamics of EEG signals, namely the rate of change in voltage, which is closely linked to emotional states  [42] . In our ISC methodology, we introduce a variable, scale, which aggregates s sample points into a single feature point. This scaling allows for the analysis of shared cognitive information across varying temporal resolutions and reduces the length of the feature vectors, thereby decreasing storage requirements and enhancing computational efficiency.\n\nThe equations for the original and first-order difference feature vectors for a single EEG record are defined as  [32] :\n\nwhere i represents the index of the feature vector, x(j) deontes the j-th sample point in the EEG record, N is the total number of samples, and s is the scale factor converting s sample points into one feature point. Additionally, we incorporate the Differential Entropy (DE) feature  [12] , which provides a quantification of the complexity or uncertainty associated with EEG signal frequency distributions. It has been found that the subbands of EEG signals are nearly subject to Gaussian distribution  [12] . The DE feature is calculated over a fixed scale of 200 sample points, corresponding to a 1-second time scale at a 200 Hz sampling rate. We compute DE features using Fast Fourier Transform with a 1-s-long window and no overlapping window. Given that a series of a certain band X follows a Gaussian distribution N (µ, σ 2 ), where µ is the mean and σ 2 is the variance of the distribution, the DE is defined by the formula:\n\nwhere i represents the index of the feature vector, p(x) represents the probability distribution of the EEG frequency amplitude values within the defined window, namely Gaussian distribution. This fixed-scale analysis provides a consistent measure of entropy across all EEG channels, offering insights into the informational content of the signals related to emotions. In our work, we compute DE features for four frequency bands respectively, i.e., δ and θ band (1-7Hz), α band (8-13Hz), β band (14-29Hz), γ band (30-47Hz).\n\nSimilarity measurement Pearson correlation coefficient (PCC) is used in ISC similarity measurement that was proposed by Karl Pearson. PCC can measure a linear association or dependence, between two continuous variables  [29] . Given two random variables X, Y and their paired N samples x(i), y(i), i = 1, 2, 3, ...N , the formula of PCC is as follows:\n\nwhere Cov(X, Y ) means the covariance between X, Y , x and σ x are the mean and standard deviation of x(i), ȳ and σ y are the mean and standard deviation of y(i) and N means the number of samples.\n\nOverall and sliding window correlation Inter-subject correlation (ISC) can be computed using two distinct methodologies: overall correlation and sliding window correlation. The overall correlation method computes a single correlation coefficient, reflecting the global similarity across the entire lengths of two feature vectors. In contrast, the sliding window correlation method segments the feature vectors into smaller, overlapping windows. For each window, a separate correlation is computed, resulting in a sequence that illustrates temporal fluctuations in correlation, providing insights into dynamic synchronization patterns. The length of this sequence is influenced by both the width of the window and the overlap between successive windows, which is set to 1 second in this study. The choice of window width is crucial and remains a subject of ongoing research  [28] . In our analysis, window widths of 10 seconds and 70 seconds are utilized to adequately capture brief scenes and longer narrative arcs within the films, respectively. These correlation calculations are performed for each pairwise combination of subjects, across all channels and all films, resulting in a substantial dataset comprising 990 pairs, 62 channels, and 15 films within the SEED dataset. For each film and channel, sliding window correlations are averaged across all pairs of subjects to determine population-level synchronous responses. These correlations are denoted as SW-O-ISC, SW-FD-ISC, and SW-DE-ISC for the original, first-order difference (FD), and differential entropy (DE) feature vectors, respectively. To enhance computational efficiency, the Python library 'taichi'  [21]  is employed for parallel computation of sliding window ISC.\n\nThe mathematical representations for the overall (ζ) and sliding window (ξ) correlations are given by:\n\nwhere i, j represent participant indices (1 ≤ i ≤ M, i < j ≤ M , with M being the total number of participants), f, c denote film and channel indices respectively,.\n\nv f,c i (k) represents the segmented feature vector in the kth window. M * signifies the total number of pairwise comparisons. In the field of EEG-based emotion recognition, features in the frequency domain have been widely used, such as power spectral density (PSD) and differential entropy (DE)  [1] ,  [22] ,  [40] ,  [45] ,. However, a recent study  [42]  have showed that features in the time domain, such as the first difference, exhibit superior discriminative power in differentiating emotional valence compared to their frequency domain counterparts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "To further explore the discriminative capabilities of these features, we conducted an experiment using the SEED dataset, employing the linear kernel Support Vector Machine (SVM) classifier. We select four time-domain features for evaluation: the original feature, the first-order difference (FD) feature, the Hjorth parameter, and the non-stable index (NSI). Additionally, we include PSD and DE from the frequency domain for comparative analysis. A detailed description of these feature extraction methods is available in  [42] .\n\nThe study employs a user-dependent training strategy, configuring one classifier per subject. Each EEG record, approximately four minutes in duration, is segmented into 1-second samples from which features are extracted. A five-fold cross-validation approach is utilized for training and testing the classifiers, with data shuffling prior to model fitting to enhance performance. Hyperparameter optimization is conducted through nested grid search, with the SVM's gamma and C parameters ranging logarithmically from 2 -4 to 2 4 , each fold independently selecting optimal parameters.\n\nResults, summarized in Table  1 , reveal that the FD feature achieved the highest classification accuracy at 87.96%, surpassing other features significantly, with a notable 5% improvement over the frequency domain feature, DE. It is important to note, however, that certain features may exhibit enhanced performance with alternative classifiers instead of the linear kernel SVM, such as those employing non-linear kernels. In brief, these results underscore the potential of FD and DE features in effectively capturing nuanced emotional information from EEG data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Heterogeneous Synchronized Percentage Of Electrodes On Overall Isc",
      "text": "We apply the overall correlation to EEG features from identical electrodes, assessing their synchrony to preliminarily evaluate the viability of single-electrode EEG-based Inter-Subject Correlation (ISC). First-order Difference (FD) and Differential Entropy (DE) features are employed to compute ISC. Our findings reveal that electrodes near the temporal lobe exhibit the stronger ability to capture global synchrony, with consistent performance across various scales of the FD feature and different frequency bands of the DE feature.\n\nFor each film clip and each pair of subjects, we correlate two complete feature vectors from the same channel to obtain a p value. This process results in a multi-dimensional p-value tensor (F × P × C), where F denotes films, P indicates pairwise subjects, and C represents channels. Each scale of FD features and each band of DE features correspond to a specific p-value tensor. We apply a Family-Wise Error Rate (FWER) Bonferroni correction to the p-value tensor to address multiple comparisons. Following Hasson et al.  [17] , we use the 'synchronized percentage' as a metric for synchronization performance, calculated as the proportion of significant correlations (p < 0.05) across channels, pairwise subjects, or films. This metric reflects the intensity of shared cognitive activity.\n\nOur analysis across all scales of FD features and the β, γ bands of DE features shows a heterogeneous yet consistent spatial pattern among the electrodes (Figure  1-A B ). Electrodes near the bilateral temporal lobes are notably effective in capturing shared information processing related to emotions, a finding corroborated by other studies in EEG-based emotion recognition  [39] ,  [43] ,  [45] . Similar patterns are observed for films with different emotional valences (Figure  2-A B ), underscoring the consistency of these spatial patterns.\n\nMoreover, films with a 'happy' emotional valence tend to evoke stronger shared neural synchrony, suggesting a greater potential of such films to engage audiences similarly (Figure  2-A B ). Additionally, both FD and DE features show synchronization across many pairwise subjects, confirming that the observed overall synchronization is not limited to a few pairs but is rather widespread (Figure  2-C D ). The consistent induction of neural synchrony by each film, albeit with varying potentials linked to different valences, further supports our findings (Figure  2-E F ). Besides, the montage view plots are improved based on mne.channels.DigMontage.plot  [13] .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Validating The Reliability Of Dynamic Isc",
      "text": "In this section, we employ the sliding window correlation technique (Figure  3-A ) to analyze the dynamic shared neural responses across participants at critical electrodes (FT7, FT8, TP7, TP8, T7, T8). We calculate the mean correlation coefficients derived from the dynamic inter-subject correlation (ISC) of emotion-related features or key electrodes to assess the reliability of our methods. Our findings reveal that the similarity of different dynamic ISCs exhibits significant performance across all film categories and both window sizes, particularly for 'happy' films. This suggests that consistent dynamic synchronous neural responses to stimuli can be effectively captured through single-electrode sliding window correlations, underscoring the potential of this method to represent group-level emotional arousal triggered by the films. We begin with a comparative analysis of the dynamic ISC at electrodes T7 and T8 using sliding window correlations with original inter-subject correlation (SW-O-ISC) and first-difference inter-subject correlation (SW-FD-ISC) at a scale of 20, focusing on the first film from the SEED dataset (Figure  3-B D ). The grey dotted lines in the plots indicate the highest correlation coefficient for SW-O-ISC. Notably, many instances in SW-FD-ISC exceed this benchmark. This comparison demonstrates that SW-FD-ISC provides smoother, stronger, and more consistent responses than SW-O-ISC, highlighting its efficacy in capturing emotion-related dynamic ISC at pivotal electrodes. Additionally, we illustrate a 10-second scene corresponding to the first peak in SW-FD-ISC, which coincides with a humorous moment eliciting significant emotional arousal in the audience (Figure  3-C ).\n\nFurther comprehensive comparisons of dynamic ISCs across different emotionrelated features or several key electrodes are conducted. In each film clip, various scales of first-difference (FD) features and the β or γ bands of differential entropy (DE) features are employed to extract population-level dynamic synchronous responses at each key electrode. The significance of coefficients in each dynamic ISC is assessed using the Wilcoxon signed-rank test with adjustments for multiple comparisons via the Benjamini-Hochberg False Discovery Rate (FDR)  [4] . Only Results with other configurations have similar performance and are not shown for brevity. The significance of each category is tested using one-sample t-tests against a threshold of 0.2.\n\nsignificant coefficients are color-coded, and Z-score normalization is employed prior to comparing the performance of dynamic ISCs. In the first film, significant peaks are consistently observed in dynamic ISCs at window 10, indicating the reliable capture of fine-grained and emotion-related information (Figure  4 -A B). Moreover, dynamic ISCs at window 70 consistently capture all significant variations in shared responses, suggesting the presence of similar cognitive activities among the audience on a larger scale (Figure  4 -C D).\n\nTo assess the performance across all film clips, we calculate the mean correlation coefficients for each individual film clip, categorizing them by movie emotion labels, each category containing five clips. These coefficients are derived by averaging the pairwise correlations of dynamic ISCs across various features or key channels. We evaluate the significance of these means using one-sample t-tests against a threshold of 0.2. The resulting box plots for each category demonstrate significant performance, affirming that the similarity among dynamic ISCs for film clips within each category is substantial (Figure  4-E F ). Particularly noteworthy are films in the 'happy' category, which display mean coefficients approaching 0.8. This suggests that these films are exceptionally effective at engaging the audience and eliciting synchronized neural responses, indicative of strong emotional engagement.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "In this study, we explore the viability of using EEG-based inter-subject correlation (ISC) for automatic and unconscious emotion annotation. With a particular focus on reducing the number of electrodes required, we have developed a novel single-electrode and feature-based dynamic ISC method. The contributions of our work are threefold:  (1)  We reidentify two effective emotion features, one from the time domain and one from the frequency domain: first-order difference (FD) and differential entropy (DE).  (2)  We utilize the overall correlation to demonstrate the heterogeneous synchronized performance of electrodes. This performance is consistent with neural emotion patterns identified in previous research, providing a validation of our method's effectiveness. (3) Applying a sliding window correlation, we illustrate the similarity of dynamic ISCs across various features or key electrodes for each film clip. This indicates the reliability of our method in capturing consistent dynamic shared neural synchrony among individuals induced by evocative film clips, highlighting its potential as an indicator of human continuous emotion arousal.\n\nOur findings on the temporal lobe's dominance in neural synchrony are consistent with a breadth of multidisciplinary literature, encompassing neuroscience and affective computing. Neuroscientists, such as Vytal and Hamann  [39] , using functional Magnetic Resonance Imaging (fMRI), identified distinct activation patterns associated with basic emotions in specific brain regions: happiness primarily activates the right superior temporal gyrus (STG) and left anterior cingulate cortex (ACC), while sadness is primarily associated with the left medial frontal gyrus (medFG). Furthermore, several studies have employed EEG and computational models to delineate spatial distribution patterns of emotional responses. Critical emotion patterns located at the temporal lobe have been observed through various methods, including DE feature frequency bands, weight distributions of deep belief networks  [40] , average energy distributions  [44] ,  [45] , spatio-temporal feature characteristics by contrastive learning intracranial seizure analysis (CLISA)  [37] , and gradient visualization of a teacher-student model  [43] . These findings collectively underscore the integral role of the temporal lobes in brain emotion functions. The heterogeneous electrode performance observed in our results aligns with these established spatial patterns of emotions.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Heterogeneous synchronized performance of electrodes. A and B illus-",
      "page": 8
    },
    {
      "caption": "Figure 2: Additional views of synchronized percentage for overall correlation.",
      "page": 8
    },
    {
      "caption": "Figure 1: -A B). Electrodes near the bilateral temporal lobes are notably eﬀec-",
      "page": 9
    },
    {
      "caption": "Figure 2: -A B), underscoring the consistency of these spatial patterns.",
      "page": 9
    },
    {
      "caption": "Figure 2: -A B). Additionally, both FD and DE features show",
      "page": 9
    },
    {
      "caption": "Figure 2: -C D). The consistent induction of neural synchrony by each ﬁlm, al-",
      "page": 9
    },
    {
      "caption": "Figure 2: -E F). Besides, the montage view plots are improved based on",
      "page": 9
    },
    {
      "caption": "Figure 3: Case comparison of the dynamic ISCs at electrodes T7 and T8. A.",
      "page": 10
    },
    {
      "caption": "Figure 4: The consistent dynamic ISCs on both 10 and 70 window sizes. A and",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FT7": "FT8\nTP8\nTP7"
        },
        {
          "FT7": "T8\nT7"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FD, scale = 5": "FD, scale = 20\n3\nFD, scale = 100\nFD, scale = 200"
        },
        {
          "FD, scale = 5": "2\nDE 14-29Hz, scale = 200\nDE 30-47Hz, scale = 200\n1\n0"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "venue": "Emotions recognition using EEG signals: A survey",
      "doi": "10.1109/TAFFC.2017.2714671"
    },
    {
      "citation_id": "2",
      "title": "A ticket for your thoughts: Method for predicting content recall and sales using neural similarity of moviegoers",
      "authors": [
        "S Barnett",
        "M Cerf"
      ],
      "venue": "A ticket for your thoughts: Method for predicting content recall and sales using neural similarity of moviegoers",
      "doi": "10.1093/jcr/ucw083"
    },
    {
      "citation_id": "3",
      "title": "LIRIS-ACCEDE: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "Chen Liming"
      ],
      "venue": "LIRIS-ACCEDE: A video database for affective content analysis",
      "doi": "10.1109/TAFFC.2015.2396531"
    },
    {
      "citation_id": "4",
      "title": "Discovering the false discovery rate",
      "authors": [
        "Y Benjamini"
      ],
      "venue": "Discovering the false discovery rate",
      "doi": "10.1111/j.1467-9868.2010.00746.x"
    },
    {
      "citation_id": "5",
      "title": "Shared memories reveal shared structure in neural activity across individuals",
      "authors": [
        "J Chen",
        "Y Leong",
        "C Honey",
        "C Yong",
        "K Norman",
        "U Hasson"
      ],
      "venue": "Shared memories reveal shared structure in neural activity across individuals",
      "doi": "10.1038/nn.4450"
    },
    {
      "citation_id": "6",
      "title": "Highlight detection in movie scenes through inter-users, physiological linkage",
      "authors": [
        "C Chênes",
        "G Chanel",
        "M Soleymani",
        "T Pun"
      ],
      "venue": "Social Media Retrieval"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Springer London"
      ],
      "venue": "",
      "doi": "10.1007/978-1-4471-4555-4_10"
    },
    {
      "citation_id": "8",
      "title": "EEGLAB: an open source toolbox for analysis of singletrial EEG dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "venue": "EEGLAB: an open source toolbox for analysis of singletrial EEG dynamics including independent component analysis"
    },
    {
      "citation_id": "9",
      "title": "Inter-brain EEG feature extraction and analysis for continuous implicit emotion tagging during video watching",
      "authors": [
        "Y Ding",
        "X Hu",
        "Z Xia",
        "Y Liu",
        "D Zhang"
      ],
      "venue": "Inter-brain EEG feature extraction and analysis for continuous implicit emotion tagging during video watching",
      "doi": "10.1109/TAFFC.2018.2849758"
    },
    {
      "citation_id": "10",
      "title": "Audience preferences are predicted by temporal reliability of neural processing",
      "authors": [
        "J Dmochowski",
        "M Bezdek",
        "B Abelson",
        "J Johnson",
        "E Schumacher",
        "L Parra"
      ],
      "venue": "Audience preferences are predicted by temporal reliability of neural processing",
      "doi": "10.1038/ncomms5567"
    },
    {
      "citation_id": "11",
      "title": "Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity",
      "authors": [
        "J Dmochowski",
        "J Ki",
        "P Deguzman",
        "P Sajda",
        "L Parra"
      ],
      "venue": "Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity",
      "doi": "10.1016/j.neuroimage.2017.05.037"
    },
    {
      "citation_id": "12",
      "title": "Correlated components of ongoing EEG point to emotionally laden attention -a possible marker of engagement?",
      "authors": [
        "J Dmochowski",
        "P Sajda",
        "J Dias",
        "L Parra"
      ],
      "venue": "Correlated components of ongoing EEG point to emotionally laden attention -a possible marker of engagement?",
      "doi": "10.3389/fnhum.2012.00112/abstract"
    },
    {
      "citation_id": "13",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R Duan",
        "J Zhu",
        "B Lu"
      ],
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER.2013.6695876"
    },
    {
      "citation_id": "14",
      "title": "MEG and EEG data analysis with MNE-python 7",
      "authors": [
        "A Gramfort",
        "M Luessi",
        "E Larson",
        "D Engemann",
        "D Strohmeier",
        "C Brodbeck",
        "R Goj",
        "M Jas",
        "T Brooks",
        "L Parkkonen",
        "M Hämäläinen"
      ],
      "venue": "MEG and EEG data analysis with MNE-python 7",
      "doi": "10.3389/fnins.2013.00267"
    },
    {
      "citation_id": "15",
      "title": "Shared and idiosyncratic cortical activation patterns in autism revealed under continuous real-life viewing conditions",
      "authors": [
        "U Hasson",
        "G Avidan",
        "H Gelbard",
        "I Vallines",
        "M Harel",
        "N Minshew",
        "M Behrmann"
      ],
      "venue": "Shared and idiosyncratic cortical activation patterns in autism revealed under continuous real-life viewing conditions",
      "doi": "10.1002/aur.89"
    },
    {
      "citation_id": "16",
      "title": "Enhanced intersubject correlations during movie viewing correlate with successful episodic encoding",
      "authors": [
        "U Hasson",
        "O Furman",
        "D Clark",
        "Y Dudai",
        "L Davachi"
      ],
      "venue": "Enhanced intersubject correlations during movie viewing correlate with successful episodic encoding",
      "doi": "10.1016/j.neuron.2007.12.009"
    },
    {
      "citation_id": "17",
      "title": "Reliability of cortical activity during natural stimulation",
      "authors": [
        "U Hasson",
        "R Malach",
        "D Heeger"
      ],
      "venue": "Reliability of cortical activity during natural stimulation",
      "doi": "10.1016/j.tics.2009.10.011"
    },
    {
      "citation_id": "18",
      "title": "Intersubject synchronization of cortical activity during natural vision",
      "authors": [
        "U Hasson",
        "Y Nir",
        "I Levy",
        "G Fuhrmann",
        "R Malach"
      ],
      "venue": "Intersubject synchronization of cortical activity during natural vision",
      "doi": "10.1126/science.1089506"
    },
    {
      "citation_id": "19",
      "title": "A hierarchy of temporal receptive windows in human cortex",
      "authors": [
        "U Hasson",
        "E Yang",
        "I Vallines",
        "D Heeger",
        "N Rubin"
      ],
      "venue": "A hierarchy of temporal receptive windows in human cortex",
      "doi": "10.1523/JNEUROSCI.5487-07.2008"
    },
    {
      "citation_id": "20",
      "title": "Elucidating relations between fMRI, ECoG, and EEG through a common natural stimulus",
      "authors": [
        "S Haufe",
        "P Deguzman",
        "S Henin",
        "M Arcaro",
        "C Honey",
        "U Hasson",
        "L Parra"
      ],
      "venue": "Elucidating relations between fMRI, ECoG, and EEG through a common natural stimulus",
      "doi": "10.1016/j.neuroimage.2018.06.016"
    },
    {
      "citation_id": "21",
      "title": "Similar brains blend emotion in similar ways: Neural representations of individual difference in emotion profiles 247",
      "authors": [
        "X Hu",
        "F Wang",
        "D Zhang"
      ],
      "venue": "Similar brains blend emotion in similar ways: Neural representations of individual difference in emotion profiles 247",
      "doi": "10.1016/j.neuroimage.2021.118819"
    },
    {
      "citation_id": "22",
      "title": "Taichi: a language for high-performance computation on spatially sparse data structures",
      "authors": [
        "Y Hu",
        "T Li",
        "L Anderson",
        "J Ragan-Kelley",
        "F Durand"
      ],
      "venue": "Taichi: a language for high-performance computation on spatially sparse data structures"
    },
    {
      "citation_id": "23",
      "title": "Feature extraction and selection for emotion recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "venue": "Feature extraction and selection for emotion recognition from EEG",
      "doi": "10.1109/TAFFC.2014.2339834"
    },
    {
      "citation_id": "24",
      "title": "Attention strongly modulates reliability of neural responses to naturalistic narrative stimuli",
      "authors": [
        "J Ki",
        "S Kelly",
        "L Parra"
      ],
      "venue": "Attention strongly modulates reliability of neural responses to naturalistic narrative stimuli",
      "doi": "10.1523/JNEUROSCI.2942-15.2016"
    },
    {
      "citation_id": "25",
      "title": "Intersubject consistency of cortical MEG signals during movie viewing 92",
      "authors": [
        "K Lankinen",
        "J Saari",
        "R Hari",
        "M Koskinen"
      ],
      "venue": "Intersubject consistency of cortical MEG signals during movie viewing 92",
      "doi": "10.1016/j.neuroimage.2014.02.004"
    },
    {
      "citation_id": "26",
      "title": "What makes a good movie trailer?: Interpretation from simultaneous EEG and eyetracker recording",
      "authors": [
        "S Liu",
        "J Lv",
        "Y Hou",
        "T Shoemaker",
        "Q Dong",
        "K Li",
        "T Liu"
      ],
      "venue": "Proceedings of the 24th ACM international conference on Multimedia",
      "doi": "10.1145/2964284.2967187"
    },
    {
      "citation_id": "27",
      "title": "Measuring speaker-listener neural coupling with functional near infrared spectroscopy",
      "authors": [
        "Y Liu",
        "E Piazza",
        "E Simony",
        "P Shewokis",
        "B Onaral",
        "U Hasson",
        "H Ayaz"
      ],
      "venue": "Measuring speaker-listener neural coupling with functional near infrared spectroscopy",
      "doi": "10.1038/srep43293"
    },
    {
      "citation_id": "28",
      "title": "Sliding window correlation analysis: Modulating window shape for dynamic brain connectivity in resting state",
      "authors": [
        "D Mantini",
        "U Hasson",
        "V Betti",
        "M Perrucci",
        "G Romani",
        "M Corbetta",
        "G Orban",
        "W Vanduffel",
        "F Mokhtari",
        "M Akhlaghi",
        "S Simpson",
        "G Wu",
        "P Laurienti"
      ],
      "venue": "Sliding window correlation analysis: Modulating window shape for dynamic brain connectivity in resting state",
      "doi": "10.1016/j.neuroimage.2019.02.001"
    },
    {
      "citation_id": "29",
      "title": "Measuring shared responses across subjects using intersubject correlation",
      "authors": [
        "S Nastase",
        "V Gazzola",
        "U Hasson",
        "C Keysers"
      ],
      "venue": "Measuring shared responses across subjects using intersubject correlation",
      "doi": "10.1093/scan/nsz037"
    },
    {
      "citation_id": "30",
      "title": "Emotions promote social interaction by synchronizing brain activity across individuals",
      "authors": [
        "L Nummenmaa",
        "E Glerean",
        "M Viinikainen",
        "I Jääskeläinen",
        "R Hari",
        "M Sams"
      ],
      "venue": "Emotions promote social interaction by synchronizing brain activity across individuals",
      "doi": "10.1073/pnas.1206095109"
    },
    {
      "citation_id": "31",
      "title": "Similar neural responses predict friendship",
      "authors": [
        "C Parkinson",
        "A Kleinbaum",
        "T Wheatley"
      ],
      "venue": "Similar neural responses predict friendship",
      "doi": "10.1038/s41467-017-02722-7"
    },
    {
      "citation_id": "32",
      "title": "Toward machine emotional intelligence: analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "venue": "Toward machine emotional intelligence: analysis of affective physiological state",
      "doi": "10.1109/34.954607"
    },
    {
      "citation_id": "33",
      "title": "EEG in the classroom: Synchronised neural recordings during video presentation",
      "authors": [
        "A Poulsen",
        "S Kamronn",
        "J Dmochowski",
        "L Parra",
        "L Hansen"
      ],
      "venue": "EEG in the classroom: Synchronised neural recordings during video presentation",
      "doi": "10.1038/srep43916"
    },
    {
      "citation_id": "34",
      "title": "Conscious processing of narrative stimuli synchronizes heart rate between individuals",
      "authors": [
        "P Pérez",
        "J Madsen",
        "L Banellis",
        "B Türker",
        "F Raimondo",
        "V Perlbarg",
        "M Valente",
        "M Niérat",
        "L Puybasset",
        "L Naccache",
        "T Similowski",
        "D Cruse",
        "L Parra",
        "J Sitt"
      ],
      "venue": "Conscious processing of narrative stimuli synchronizes heart rate between individuals",
      "doi": "10.1016/j.celrep.2021.109692"
    },
    {
      "citation_id": "35",
      "title": "EEG-based intersubject correlations reflect selective attention in a competing speaker scenario 15",
      "authors": [
        "M Rosenkranz",
        "B Holtze",
        "M Jaeger",
        "S Debener"
      ],
      "venue": "EEG-based intersubject correlations reflect selective attention in a competing speaker scenario 15",
      "doi": "10.3389/fnins.2021.685774/full"
    },
    {
      "citation_id": "36",
      "title": "Dynamic intersubject neural synchronization reflects affective responses to sad music 218",
      "authors": [
        "M Sachs",
        "A Habibi",
        "A Damasio",
        "J Kaplan"
      ],
      "venue": "Dynamic intersubject neural synchronization reflects affective responses to sad music 218",
      "doi": "10.1016/j.neuroimage.2019.116512"
    },
    {
      "citation_id": "37",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "venue": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "doi": "10.1109/TAFFC.2022.3164516"
    },
    {
      "citation_id": "38",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "venue": "conference Name: IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2436926"
    },
    {
      "citation_id": "39",
      "title": "Neuroimaging support for discrete neural correlates of basic emotions: A voxel-based meta-analysis",
      "authors": [
        "K Vytal",
        "S Hamann"
      ],
      "venue": "Neuroimaging support for discrete neural correlates of basic emotions: A voxel-based meta-analysis",
      "doi": "10.1162/jocn.2009.21366"
    },
    {
      "citation_id": "40",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "venue": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "doi": "10.1109/TAMD.2015.2431497"
    },
    {
      "citation_id": "41",
      "title": "Learning topology-agnostic EEG representations with geometry-aware modeling 36",
      "authors": [
        "K Yi",
        "Y Wang",
        "K Ren",
        "D Li"
      ],
      "venue": "Learning topology-agnostic EEG representations with geometry-aware modeling 36"
    },
    {
      "citation_id": "42",
      "title": "A review of EEG features for emotion recognition",
      "authors": [
        "M Yu",
        "D Zhang",
        "G Zhang",
        "G Zhao",
        "Y Liu",
        "Y Han",
        "G Chen"
      ],
      "venue": "A review of EEG features for emotion recognition",
      "doi": "10.1360/N112018-00337"
    },
    {
      "citation_id": "43",
      "title": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition 130",
      "authors": [
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "venue": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition 130",
      "doi": "10.1016/j.patcog.2022.108833"
    },
    {
      "citation_id": "44",
      "title": "Classification of five emotions from EEG and eye movement signals: Complementary representation properties",
      "authors": [
        "L Zhao",
        "R Li",
        "W Zheng",
        "B Lu"
      ],
      "venue": "2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER.2019.8717055"
    },
    {
      "citation_id": "45",
      "title": "Identifying stable patterns over time for emotion recognition from",
      "authors": [
        "W Zheng",
        "J Zhu",
        "B Lu"
      ],
      "venue": "EEG",
      "doi": "10.1109/TAFFC.2017.2712143"
    }
  ]
}