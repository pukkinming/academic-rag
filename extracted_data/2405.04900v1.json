{
  "paper_id": "2405.04900v1",
  "title": "Self-Supervised Gait-Based Emotion Representation Learning From Selective Strongly Augmented Skeleton Sequences",
  "published": "2024-05-08T09:13:10Z",
  "authors": [
    "Cheng Song",
    "Lu Lu",
    "Zhen Ke",
    "Long Gao",
    "Shuai Ding"
  ],
  "keywords": [
    "Emotion Recognition",
    "Gait Analysis",
    "Contrastive Learning",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is an important part of affective computing. Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection. Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gaitbased emotion recognition. However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions. In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data. First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask. The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations. Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features. Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries. Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTIONS are everywhere in the daily lives of humans and exert a significant impact on our judgment, decisionmaking, and behavior. Consequently, the capability for automatic emotion detection is significant in the domain of humancomputer interaction  [1]  and has found extensive application in fields such as healthcare  [2] , surveillance  [3] , and robotics  [4] . The existing emotion recognition research predominantly focuses on facial expressions  [5] ,  [6] ,  [7] , speech  [8] ,  [9] , text  [10] ,  [11]  and physiological signals such as electroencephalograms (EEGs)  [12] ,  [13]  and electrocardiography (ECG) signals  [14] . In cases involving the abovementioned emotion cues, facial expression-based emotion recognition methods can be unreliable when people make \"mock expressions\"  [15]  or when self-reported emotional results are deceptive  [16] . Furthermore, it is difficult to capture frontal facial expressions C.Song, L.Lu, Z.Ke, L.Gao and S.Ding are with the School of Management, Hefei University of Technology. (e-mail: songcheng@hfut.edu.cn; luluccc0317@gmail.com; 2020110762@mail.hfut.edu.cn; 2023111033@mail.hfut.edu.cn; dingshuai@hfut.edu.cn). at a close range without being noticed  [17] . Speech-and text-based methods may be less suitable for public scenes and large-scale crowds. Regarding physiological signal-based approaches, wearing specific instruments to access data is not very pragmatic.\n\nRecently, with advancements in the fields of gait analysis and human pose estimation techniques  [18] ,  [19] , gaitbased emotion recognition has attracted increasing attention. Previous studies have indicated that people experiencing different emotional states can act distinct gait behaviors  [20] . Specifically, we can determine that critical gait characteristics affected by emotions involve walking velocity, step frequency, head positioning, and the extent of motion in the shoulders and elbows  [21] . Compared to other emotion recognition approaches, gaits provide several unique advantages. First, considering the large range of motion during walking, gaitbased methods can satisfy long-distance application scenarios and achieve nonintrusive, noncontact emotion detection. At the same time, we can acquire gait data with a webcam without overly restricting the environmental settings or requiring active cooperation from the subject. In addition, gait patterns are inherently difficult to imitate or intentionally deceive  [22] , making them reliable indicators for emotion recognition. Moreover, a gait-based approach involves no facial cues and requires only the positional data of the body's pivotal joints to classify emotions, which ensures people's privacy.\n\nIn gait-based emotion recognition research, earlier works fo-cused on extracting handcrafted features. For instance, Li et al.  [23]  utilized the Fourier transform and statistical techniques to obtain time and frequency features for emotion classification. Bhattacharya et al.  [24]  combined deep features extracted from a long short-term memory (LSTM) network with manually crafted affective features such as stride lengths, joint angles, and walking speeds to train a random forest classifier. As deep learning advances, an increasing number of researchers are directing their attention towards the utilization of neural network models for feature extraction and pattern recognition rather than conventional machine learning algorithms  [25] . After Yan et al.  [26]  incorporated spatial-temporal graph convolutional networks (ST-GCN) into skeleton-based action recognition tasks, the effect of gait-based emotion recognition was effectively improved  [4] ,  [27] ,  [28] . Notably, the above approaches are supervised learning methods that rely on a substantial number of labeled data to learn emotional representations (see Fig.  1 ). However, the process of data annotation is notably labor-intensive, time-consuming, and costly, which consequently restricts the availability of labeled data. Furthermore, emotion labeling approaches are inevitably influenced by subject bias, which may lead to mislabeling. Contrastive learning methods that emphasize instance discrimination provide a powerful technical framework for conducting self-supervised skeleton-based representation learning. The main approach first generates positive samples through different data augmentation methods and then learns data representations by enhancing the similarity between positive samples while concurrently reducing the similarity between negative samples. Many researchers have integrated the contrastive learning paradigm into the realm of skeleton-based action recognition  [29] ,  [30] ,  [31] . To identify the emotions from unlabeled gait data, Lu et al.  [32]  first proposed a crosscoordinate contrastive learning framework named CAGE. Given an input gait sequence, they augmented it into three varying views, learned gait attributes with cross-coordinate supervision, and built a contrastive loss between the Cartesian and spherical coordinate systems. However, CAGE only applies two normal data augmentation strategies that were originally designed for action recognition tasks and does not provide any adaptive improvements for emotion recognition tasks. Designing suitable data augmentation methods is a crucial part of contrastive learning, so we must consider the characteristics of the specific task, that is, the difference between skeleton-based action recognition and emotion recognition. Moreover, the existing skeleton-based contrastive learning methods mostly adopt the ST-GCN  [26]  as their encoder to process skeleton sequences  [33] ,  [34] . Although the ST-GCN provides effective improvements for skeletonbased representation learning tasks, it still has some drawbacks that have not been considered by the existing research. The skeleton graph of the ST-GCN is predefined by referring to the physical structure of the human body, while the latent relationships among spatially distant joints are neglected, which can limit the representation capacities of the model. Therefore, devising an effective method that is suitable for gait-based emotion recognition and can learn representative features from unlabeled data is a significant task.\n\nIn this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for selfsupervised gait-based emotion representation (SSAL), which learns to optimize the encoder from multiple augmented skeleton sequences. First, we propose an SSA method that is designed specifically for the gait emotion recognition task to generate more diverse and targeted positive samples. Next, we design a complementary feature fusion network (CFFN) that integrates graph-domain and image-domain information. Finally, we implement the distributional divergence minimization loss to reduce the distributional divergence between the generally augmented samples and strongly augmented samples.\n\nIn summary, our new self-supervised learning framework for gait emotion recognition provides three key contributions.\n\n1) A selective strong augmentation method is proposed for the gait emotion recognition task, which incorporates upper body jitter and random spatiotemporal mask. This particular augmentation method aims to produce a more varied and focused set of positive samples, motivating the model to learn more representative and robust features. 2) A complementary feature fusion network is designed, which facilitates the integration of cross-domain information derived from the graph domain and image domain. This integration approach is intended to extract topological structural and global adaptive gait features, enhancing the generalization ability of the developed model. 3) We conduct a series of experiments on the Emotion-Gait (E-Gait)  [27]  and Emilya datasets  [35] . The results show that our approach outperforms state-of-the-art selfsupervised techniques across various evaluation protocols. The rest of this paper is organized as follows. Section II reviews the previous works concerning supervised gait-based emotion recognition, self-supervised contrastive learning, and self-supervised skeleton representation. Section III describes the proposed method in detail. Section IV presents the experimental details and results. Section V provides the conclusions of this paper.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Supervised Gait-Based Emotion Recognition",
      "text": "According to previous research, three general types of gaitbased emotion recognition methods are available. The first type of approach utilizes sequence-based models such as recurrent neural networks (RNNs) and LSTM to learn temporal features  [36] ,  [37] . The second category includes image-based methods that encode skeleton sequences and extract features by applying convolutional neural networks (CNNs)  [25] . In the third group, a skeleton graph is constructed in accordance with the physical structure of the human body, and a graph convolutional network (GCN) is used to explore the gait patterns of different emotions  [27] ,  [28] ,  [38] .\n\nAmong these methods, GCN-based approaches have recently received much attention because of their capacity to represent non-Euclidean data. The ST-GCN  [26] , which effectively aggregates spatiotemporal features from data, was the first model in which graph-based neural networks were applied in the domain of skeleton-based action recognition. Bhattacharya et al.  [27]  introduced ST-GCNs to extract deep features, which were combined with manual affective features such as joint angles and velocities to perceive emotions from gaits. Sheng et al.  [37]  presented a multitask learning architecture by constructing a novel attention-enhanced temporal GCN that can concurrently acquire representations for multiple objectives, such as emotion recognition, identity recognition, and auxiliary prediction. Yin et al.  [38]  designed skeleton data with different coarse and fine granularities and then proposed a multiscale adaptive GCN to recognize emotions. Lu et al.  [28]  proposed a joint reconstruction method that effectively improves the resulting classification accuracy by calculating the joint connectivity matrix based on spatiotemporal context, which exploits the latent links between body joints. The approaches mentioned above rely on supervised learning paradigms to extract affective gait features via GCNs. Considering the scarcity of available labeled emotional gait data and the possibility of mislabeling, which affects the performance and generalizability of the utilized model, we employ the self-supervised contrastive learning paradigm to learn emotional representations from unlabeled gait sequences.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Self-Supervised Contrastive Learning",
      "text": "The goal of the self-supervised learning model is to learn an effective feature embedding function from unlabeled data. Previous works  [39] ,  [40]  concentrated on designing diverse pretext tasks to train encoders, such as rotation prediction and jigsaw puzzles. Recently, contrastive learning techniques including MoCo  [41]  and SimCLR  [42]  have shown remarkable performance compared to that of supervised learning. This type of approach applies various data augmentation strategies to generate positive samples while considering other samples as negatives relative to the input. The primary objective is to map the positive and negative sample features into a highdimensional space and reduce the feature distances between positive pairs while increasing the feature distances between negative pairs.\n\nIn the domain of emotion recognition, contrastive learning has been accepted and utilized by many researchers. For example, Shen et al.  [43]  proposed a data-driven approach that performs contrastive learning for intersubject alignment (CLISA). The approach minimized variability across subjects by maximizing the similarity in EEG signal representations among different subjects when they received the same emotional stimuli. Mai et al.  [44]  proposed the HyCon framework for conducting hybrid contrastive learning on trimodal representations to explore interclass and intersample relationships and obtain more discriminative joint embeddings. Shuvendu Roy et al.  [45]  introduced a contrastive learning method for multiview facial expressions (CL-MEx) to exploit facial images captured concurrently from various perspectives. Wang et al.  [46]  presented a self-fusion contrastive learning framework, which aimed at recognizing group emotions through exploiting information acquired from faces, scenes, and objects in images. The abovementioned methods have established a strong theoretical basis for SSAL.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Self-Supervised Skeleton Representation",
      "text": "Self-supervised learning based on 3D human skeleton sequences was first applied in action recognition tasks. Rao et al.  [47]  proposed a contrastive learning framework based on a momentum encoder and designed a series of novel skeleton data augmentation strategies, which laid the groundwork for subsequent research. Li et al.  [29]  explored the application of cross-view consistent knowledge as complementary supervision information to enhance the accuracy of action classification. Guo et al.  [30]  acquired abundant information from extremely augmented positive samples and forced the encoder to learn more robust action representations. Zhang et al.  [31]  introduced a growing data augmentation strategy along with asymmetric hierarchical learning to enhance the model performance.\n\nFor gait-based emotion recognition, Lu et al.  [32]  first explored self-supervised learning and proposed a crosscoordinate contrastive learning framework called CAGE by constructing ambiguity samples. However, CAGE only selected two normal data augmentation methods that originated from the action recognition task. Undoubtedly, there is a discrepancy between gait-based emotion representations and skeleton-based action representations. Therefore, we must design selective augmentations that are suitable and reasonable for gait patterns. Furthermore, most skeleton-based contrastive learning methods use the ST-GCN as their encoder and focus on deep features in the graph domain while ignoring the possibility of cross-domain information fusion. Overall, we propose a contrastive learning framework utilizing selective strong augmented samples and applying a complementary feature fusion network, which can effectively learn affective representations from gait sequences.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overview",
      "text": "As shown in Fig.  2 , we propose a contrastive learning framework utilizing SSA for self-supervised gait-based emotion representation. The architecture is based on the recent advanced practice SkeletonCLR  [29] . It applies SSA to generate positive pairs, working together with a CFFN to capture cross-domain information. Given a 3D skeleton sequence s ∈ R T ×J×C that contains T consecutive frames, J different body joints, and C dimensions for each node, we use a general augmentation and a strong augmentation to generate positive samples s 1 , s 2 and s 3 , respectively. We feed s 1 into the key encoder f θ k and obtain a representation f 1 . Then, we apply a multilayer perceptron (MLP) projector q to project the representation into a lower dimension and obtain the representation z 1 . Similarly, we feed s 2 into the query encoder branch to obtain the representation z 2 . Notably, we adopt the parameter-free Simam attention module  [48]  to force the model to drop several important features and learn more robust representations. Specifically, we feed s 3 into the query encoder, apply the drop module to the fusion features f Given an input sequence s, through a general augmentation T and a strong augmentation T ′ , we obtain general augmentations s 1 and s 2 and a strong augmentation s 3 . A momentum-updated key encoder and an MLP extract z 1 , which is stored in the memory bank and serves as one of the negative samples for the subsequent training steps. The query encoder and an MLP are used to obtain z 2 and z 3 , and the Simam drop is adopted to obtain z ′ 3 .\n\nwhich provides negative samples for the subsequent training steps. Gradient backpropagation is employed to update the query encoder, and a moving average of the query encoder is used to update the key encoder:\n\nis the momentum coefficient. The loss function is described in detail later.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Selective Strong Augmentation For Skeleton",
      "text": "Data augmentation is a critical approach for obtaining more positive samples, which enables the encoder to acquire abundant representations. To explore the \"pattern invariance\" property of skeleton sequences, we first introduce a general augmentation strategy following previous work  [47] . It includes 3 spatial augmentations, shearing, spatial flipping, and rotation, and 2 temporal augmentations, cropping and temporal flipping.\n\n(1) Shearing. To obtain positive samples with different viewpoints while retaining the original pose, we apply 3D shearing to the given skeleton sequence. The transformation is defined as:\n\nwhere X is the original skeleton sequence, and {r 12 , • • • , r 32 } are the shear factors randomly sampled from [-1, 1].\n\n(2) Spatial Flipping. Given that human gait is generally a symmetrical motion, we interchange the left and right sides of the skeleton with a probability of 0.5 to capture behavioral details.\n\n(3) Rotation. We apply random rotation perturbations to make the model more robust to various spatial perspectives. Specifically, we randomly choose an axis A ∈ {X, Y, Z} as the principal axis and randomly rotate it by 0-30 degrees. For the remaining two axes, the rotation angles are randomly set between 0-10 degrees.\n\n(4) Cropping. Cropping is a temporal augmentation method that pads T /γ frames to the original sequence and then randomly selects continuous T frames to form a new sequence. γ is the padding ratio (we set γ=2).\n\n(5) Temporal Flipping. Gaits are periodic, so even if we disrupt the sequence of gait, it will not affect the perception of emotions. Accordingly, we reverse the original sequence with a probability of 0.5.\n\nIn addition to the general augmentations available for skeleton-based pattern recognition, we propose the following strong augmentations to introduce innovative and targeted patterns for emotional representation learning. Fig.  3  shows the visualization process.\n\n(1) Upper Body Jitter. Previous research concluded that the movement of the upper body, especially the arms and the head, was a significant indicator of gait-based emotion recognition  [49] . Therefore, we consider applying an upper body jitter to transform the joint positions to motivate the model to learn representative features. Specifically, we select the upper body joints (shoulders, elbows, and hands) and move these joints to irregular positions while keeping the other joints unchanged. The transformation is defined as follows:\n\nwhere j is the upper body joint set, and {r 11 , • • • , r 33 } are the jitter parameters randomly sampled from [-1, 1].\n\n(2) Random Spatiotemporal Mask. Inspired by the observation that we can also recognize emotion from incomplete gait sequences that lack some time frames and body parts, we propose a random spatiotemporal mask to make the model learn more robust feature representations.\n\nTo generate a spatial mask, we first divide the human skeleton into five body components, the limbs and the torso, which can efficiently reflect body movements. Then, we randomly select one or two of these parts and replace the coordinates of the joints with zeros. The spatial mask formula is as follows:\n\nwhere S spatial (X) is the skeleton joint matrix after applying spatial mask augmentation. X is the input skeleton joint matrix. ⊙ is the dot product operation. RanSamp(•) is the random sampling function that randomly selects one or two parts from the predefined sets. M ask s (•) is the spatial mask function that transforms the joint coordinates of the selected part set to zero. The temporal mask is the same. We randomly select several frames and mask all the joints with zeros. Therefore, the spatiotemporal mask formula is:\n\nwhere S st (X) is the skeleton joint matrix obtained after applying the spatiotemporal mask augmentation. RanSamp(•) is a random sampling function that randomly selects several frames from the original frames. M ask t (•) is the temporal mask function that transforms the joint coordinates of the selected frames to zeros. r is the temporal mask parameter (we set r=0.25), and T is the number of frames.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Complementary Feature Fusion Network",
      "text": "Most of the previously developed skeleton-based contrastive learning frameworks adopt the ST-GCN as their encoder to learn representations. We can see that GCNs have great advantages in terms of processing non-Euclidean data such as human skeleton sequences, but some problems remain. First, the degree of freedom of the human body is so complicated that applying the same adjacency matrices to the channels would limit the ability of the model to address the dependency correlations of joints. Second, the spatial-temporal graph connects only the same joints in different frames, thus the latent links among distant joints in successive frames are neglected. To focus on the global features in the spatial and temporal dimensions, we propose a CFFN, which integrates the cross-domain information derived from the graph domain and image domain to learn complementary feature representations. Specifically, we adopt the ST-GCN as the graph-domain feature extractor to obtain topological structural information. Moreover, we introduce an adaptive frequency filter (AFF)based token mixer  [50]  as the image-domain feature extractor to obtain global adaptive representations. The AFF token mixer utilizes a Fourier transform to transfer a latent representation to the frequency domain and employs elementwise multiplication to realize semantic-adaptive frequency filtering. The architecture of the CFFN is shown in Fig.  4 . For the graph-domain branch, we first apply batch normalization to ensure that the scale of the input augmented sequences is consistent across different joints. The backbone is composed of 9 layers of spatial-temporal graph convolution operators (ST-GCN units). The initial 3 layers, the subsequent 3 layers, and the last 3 layers have 16, 32, and 64 output channels, respectively. The temporal kernel size is 9, and the spatial kernel size is 3. The strides of the 4th and 7th temporal convolution layers are 2, and the strides of the other layers are 1. The final dimensionality of the topological structural features is 64.\n\nFor the image-domain branch, we first employ a convolution stem for tokenization purposes. At each stage, we apply layer normalization (LN) to the input for channel mixing and then feed the result to the AFF token mixer for global token mixing to obtain the output of the AFF block. Then, we use plain fusion to connect the local and global features. The backbone network of AFFNet is composed of multiple AFF blocks. The final dimensionality of the global adaptive features is 64.\n\nWe concatenate these two features directly and obtain a 128-dimensional feature vector. Then, we employ a two-layer nonlinear MLP to project the integrated features to a lowerdimensional space.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Loss Function",
      "text": "The purpose of SSAL is to learn effective emotional representations by contrasting multiple gait sequences. The model is expected to amplify the similarity between the original sequence and the augmented sequences while reducing the similarity between the positive and negative samples in the memory bank. In prior works, the contrastive InfoNCE loss was defined as:\n\nwhere M is the length of the memory queue, m i is the i-th negative sample and τ is the temperature hyperparameter.\n\nConsidering the dramatic discrepancy between the movement patterns of the generally and strongly augmented sequences,  [51]  indicated that for a randomly initialized network, the generally augmented sequence and the strongly augmented sequence possess similar distributions. Thus, we can obtain the following conditional distributions:\n\nwhere p(z 1 |z 2 ) and p(m i |z 2 ) represent the likelihood of the query representation z 2 being assigned to its positive counterpart z 1 and to the embedding m i in the memory bank M, respectively. To minimize the distributional divergence between a generally augmented sequence and a strongly augmented sequence, the loss can be written as follows:\n\nAs mentioned earlier, we adopted the parameter-free Simam attention module. The distributional divergence between a generally augmented sample and a dropped strongly augmented sample is the same:\n\nTherefore, the distributional divergence loss can be given by\n\nThe overall loss for our SSAL method can be formulated as L = αL Inf o + βL d , where α and β are the coefficients used to balance the loss. Here, we set α=β=1 to obtain a more general model.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In Section A, two public datasets used in the experiments are described. In Section B, the experimental settings of SSAL are presented. In Section C, the evaluation criteria are declared. In Section D, the comparison results with state-of-the-art methods are displayed. In Section E, the ablation results on each part are discussed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "1) E-Gait  [27]  includes 2,177 real gaits, and each gait is labeled with one of the four emotion classes (angry, neutral, happy, or sad) by the same 10 annotators. Specifically, the dataset is composed of two parts. Part 1 contains 342 gaits collected from diverse sources, including BML  [52] , Human3.6M  [53] , ICT  [54] , and CMU-MOCAP  [55] . Part 2 is derived from ELMD  [56]  and consists of 1,835 real gait sequences.\n\n2) Emilya  [35]  is a dataset of emotional body expressions concerning different daily actions. It contains 7 daily actions, including simple walking (SW), walking with an object in hands (WH), sitting down (SD), knocking at the door (KD), moving books on a table with two hands (MB), lifting an object (Lf) and throwing an object (Th). Twelve actors were asked to perform the actions with 8 emotions, including anxiety (AX), pride (Pr), joy (Jy), sadness (Sd), panic/fear (PF), shame (Sh), anger (Ag) and neutral (Nt). We select the motion capture data of simple walking with 4 emotions (anger, neutral, joy, and sadness).\n\nWe uniformly convert the skeleton data into 16 body joints and 120 frames. For the E-Gait dataset, we randomly split the training and testing sets at a ratio of 4:1. As for the Emilya dataset, the data of 9 actors are allocated for training, and the remaining is used for testing. To determine the distribution of the data, we calculate the percentage of each emotional class contained in the dataset. As shown in Table  I , the E-Gait dataset contains a few gait data with sad labels, and angry gaits account for more than half of the dataset. The Emilya dataset is relatively balanced across all emotion labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Experimental Settings",
      "text": "We adopt the PyTorch framework to implement the proposed method and conduct all the experiments on an Ubuntu server equipped with an Intel Xeon@2.16 GHz CPU and 4 NVIDIA GTX Titan X graphics cards.\n\nData Augmentation. We compare different general augmentation strategy compositions and select the two most effective general augmentations. By applying general augmentation and SSA to the input skeleton data, we explore the effect of SSA.\n\nSelf-supervised Pretext Training. For the contrastive learning parameter settings, we follow those used in AimCLR  [30] . In particular, the feature dimensionality is 128, the size of the memory bank M is 2560, the momentum coefficient m is 0.999, and the temperature hyperparameter τ is 0.07. For optimization, we employ stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 0.0001. We adopt the CFFN as the encoder. The model is trained for 500 epochs with an initial learning rate of 0.001 (which is multiplied by 0.1 at epoch 400).\n\nLinear Evaluation Protocol. To verify the effectiveness of the representations learned from the pretext training for the gait-based emotion recognition task, we train a linear classifier on labeled datasets. Specifically, we freeze the encoder parameters and train a linear classifier, which consists of a fully connected layer and a softmax layer. The classifier is trained for 200 epochs with an initial learning rate of 0.001 (which is multiplied by 0.1 at epoch 100).\n\nFinetuned Evaluation Protocol. We append a linear classifier to the trained encoder and train the entire model in a supervised training mode to optimize the performance of the model. The model is trained for 100 epochs with an initial learning rate of 0.0001 (which is multiplied by 0.1 at epoch  50) .\n\nSemi-supervised Evaluation Protocol. We fine-tune the pre-trained encoder with only 5%, 10%, 20%, and 50% of the labeled data, and the employed data are randomly selected. The model is trained for 20 epochs with an initial learning rate of 0.001 (which is multiplied by 0.1 at epoch 10).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Evaluation Criteria",
      "text": "To evaluate the performance of the proposed SSAL algorithm in the gait-based emotion classification tasks, we calculate the classification accuracy, precision, recall, and F1 score via the following formulas:\n\nRecall = (\n\nwhere TP, FP, TN, and FN represent the numbers of true positives, false positives, true negatives, and false negatives for the four emotions, respectively. TD represents the total number of data. w i represents the number of samples in each class as a proportion of the total number of samples in all classes, and i = 0, 1, 2, 3.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Comparison With State-Of-The-Art",
      "text": "Since few self-supervised methods are available for gaitbased emotion recognition, we compare the proposed SSAL with related skeleton-based contrastive learning methods that operate similarly in gait-based emotion recognition tasks.\n\nLinear Evaluation Results on the E-Gait Dataset. We conduct an extensive comparison with previously developed supervised methods and recent methods for skeleton-based self-supervised action recognition. As shown in Table  II , the accuracy of SSAL is improved by 0.80%-2.17% over those of the existing contrastive learning methods. Even compared to some supervised methods, our approach achieves superior performance. Moreover, the SSAL achieves the best results in terms of precision, recall, and F1 score, indicating that our method has a high classification learning capacity.\n\nLinear Evaluation Results on the Emilya Dataset. Table III shows that our proposed SSAL outperforms all other self-supervised methods and supervised methods in terms of accuracy, precision, recall, and F1 score. Specifically, compared to the advanced contrastive learning methods, AimCLR  [30]  and HiCLR  [31] , our approach provides accuracy improvements of 15.68% and 8.27%, respectively. Notably, the Emilya dataset has approximately half the data size of the E-Gait dataset. The results show that for the gait-based emotion recognition task, SSAL has great advantages in small sample datasets over the existing skeleton-based methods.\n\nFinetuned Evaluation Results. We compare the finetuned evaluation results on the E-Gait and Emilya datasets. The experimental setup is consistent with CAGE, and the model   trains 20 epochs. Table  V  shows that our proposed SSAL achieves the best performance. Specifically, on the E-gait dataset, SSAL surpasses the self-supervised gait emotion recognition method CAGE by 0.05%. Compared with the latest contrastive learning method HiCLR, the accuracy of SSAL on the E-Gait and Emilya datasets is improved by 0.30% and 6.75%, respectively. Semi-supervised Evaluation Results. We use a small amount of labeled data for semi-supervised evaluation and compare our approach with other outstanding methods on the E-Gait and Emilya datasets. Table  IV  shows that in all cases, our proposed SSAL approach outperforms the other advanced methods. In particular, with only 5% annotated data, SSAL achieves accuracies of 76.75% and 64.58% on the E-Gait and Emilya datasets, respectively, indicating that our approach has a significant advantage in terms of learning from only a small quantity of labeled data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Ablation Study",
      "text": "We conduct ablation experiments to validate the efficiency of the different components of our method. All the experiments follow the self-supervised pretext training and linear evaluation protocol.\n\nThe effectiveness of SSA. We first take the Emilya dataset as an example to select the two most effective general augmentations among the five methods described. As shown in Fig.  5 , we combine each of the three spatial augmentations and the two temporal augmentations individually. Of the six compositional strategies, the combination of \"Shearing\" and \"Cropping\" performs best, and this is consistent with the data augmentation methods used in previous experiments  [29] ,  [30] .\n\nOn this basis, we compare the effects of introducing SSA and other augmentations. As shown in Table VI, after applying upper body jitter (UBJ), the accuracies are improved by 2.75% and 0.52% on the E-Gait and Emilya datasets, respectively, demonstrating that the arms and the head are significant emotional clues. Notably, the random spatiotemporal mask (RSM) performs better, which shows that the models learn high-level semantic information in the spatial and temporal dimensions. When the UBJ and the RSM are used, our proposed SSAL approach achieves the best results.\n\nThe effectiveness of the CFFN. We explore the effectiveness of the graph domain, image domain, and CFFN. As shown  in Table VII, our proposed CFFN integrates cross-domain information and reaches the highest accuracy levels on the E-Gait and Emilya datasets. Especially on the Emilya dataset, the CFFN improves the final accuracies by 1.30% and 8.33%, respectively. This shows that the CFFN has a great capacity to aggregate representative features in the spatial and temporal dimensions and provide global adaptive information about the target skeleton, helping the encoder learn more robust and representative features for downstream tasks.\n\nThe effectiveness of different memory bank sizes. As shown in Fig.  6 , we compare the model performances attained with different memory bank sizes. A large memory bank yields better performance, and our proposed SSAL method obtains the best result when M = 2560. However, when the size of the memory bank reaches a certain level, the number of negatives becomes much larger than that of positives, which may lead to a shortcut during representation learning.\n\nQualitative Results. We apply latent Dirichlet allocation (LDA)  [57]  to show the embedding distributions of SSAL. The results are fair comparisons conducted over 500 epochs of pretraining on the E-Gait and Emilya datasets. In Fig.  7 , the embeddings of SSAL exhibit tight clustering across both datasets, which verifies that SSAL can generate discriminative features to recognize different emotions accurately.\n\nV. CONCLUSION In this paper, we propose a contrastive learning framework SSAL, which utilizes SSA to predict emotion classes from unlabeled gait data. Specifically, upper body jitter and random spatiotemporal mask are used as SSAs together with the general shearing and cropping augmentations to generate positive samples. The CFFN is proposed to extract complementary fusion features, which aggregate cross-domain topological structural and global adaptive representations. Experimental results obtained on the E-Gait and Emilya datasets demonstrate the promising performance of SSAL under a variety of evaluation protocols.\n\nThis study has several limitations. First, the amount of available labeled emotional gait data is limited, and these data are relatively unbalanced. If more data were available, the performance of the proposed model could be further improved. Second, the proposed SSAL approach considers only unimodal gait data, and we can use more information to support the process of classifying emotions in a given application scenario. In the future, it is anticipated that the aforementioned limitations will be addressed to develop a more precise and robust approach for emotion recognition tasks.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The existing approaches train deep neural networks to estimate",
      "page": 1
    },
    {
      "caption": "Figure 1: ). However, the process of data",
      "page": 2
    },
    {
      "caption": "Figure 2: , we propose a contrastive learning",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall framework of the proposed SSAL. Given an input sequence s, through a general augmentation T and a strong augmentation T ′, we",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualization of the strong augmentation. (a) We move the joints of the",
      "page": 4
    },
    {
      "caption": "Figure 4: The architecture of the proposed CFFN. The graph-domain branch is",
      "page": 5
    },
    {
      "caption": "Figure 5: Top-1 accuracy achieved with different general augmentation strategy",
      "page": 8
    },
    {
      "caption": "Figure 5: , we combine each of the three spatial augmentations",
      "page": 8
    },
    {
      "caption": "Figure 6: Comparison among the top-1 accuracy achieved with different memory",
      "page": 9
    },
    {
      "caption": "Figure 7: (a) LDA visualization of the embeddings produced on the E-Gait",
      "page": 9
    },
    {
      "caption": "Figure 6: , we compare the model performances attained",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0": "1",
          "Angry": "Neutral"
        },
        {
          "0": "2",
          "Angry": "Happy"
        },
        {
          "0": "3",
          "Angry": "Sad"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "z": "3"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "z′": "3"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Au",
          "…": "gmented Sequence s"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "2",
      "title": "Emotion-aware and intelligent internet of medical things toward emotion recognition during covid-19 pandemic",
      "authors": [
        "T Zhang",
        "M Liu",
        "T Yuan",
        "N Al-Nabhan"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "3",
      "title": "Real-time video emotion recognition based on reinforcement learning and domain knowledge",
      "authors": [
        "K Zhang",
        "Y Li",
        "J Wang",
        "E Cambria",
        "X Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "4",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "V Narayanan",
        "B Manoghar",
        "V Dorbala",
        "D Manocha",
        "A Bera"
      ],
      "year": "2020",
      "venue": "International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition through facial expression analysis based on a neurofuzzy network",
      "authors": [
        "S Ioannou",
        "A Raouzaiou",
        "V Tzouvaras",
        "T Mailis",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "Facial expression recognition with twobranch disentangled generative adversarial network",
      "authors": [
        "S Xie",
        "H Hu",
        "Y Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition with trade-offs between data augmentation and deep learning features",
      "authors": [
        "S Umer",
        "R Rout",
        "C Pero",
        "M Nappi"
      ],
      "year": "2022",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "8",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "A novel dual attention-based blstm with hybrid features in speech emotion recognition",
      "authors": [
        "Q Chen",
        "G Huang"
      ],
      "year": "2021",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "11",
      "title": "Transformer models for text-based emotion detection: a review of bert-based approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "12",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "16",
      "title": "Telling more than we can know: Verbal reports on mental processes",
      "authors": [
        "R Nisbett",
        "T Wilson"
      ],
      "year": "1977",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "17",
      "title": "Automatic detection of pain from facial expressions: a survey",
      "authors": [
        "T Hassan",
        "D Seuß",
        "J Wollenberg",
        "K Weitz",
        "M Kunz",
        "S Lautenbacher",
        "J.-U Garbas",
        "U Schmid"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "A simple yet effective baseline for 3d human pose estimation",
      "authors": [
        "J Martinez",
        "R Hossain",
        "J Romero",
        "J Little"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "19",
      "title": "Deeppose: Human pose estimation via deep neural networks",
      "authors": [
        "A Toshev",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Critical features for the perception of emotion from gait",
      "authors": [
        "C Roether",
        "L Omlor",
        "A Christensen",
        "M Giese"
      ],
      "year": "2009",
      "venue": "Journal of Vision"
    },
    {
      "citation_id": "22",
      "title": "Recognizing friends by their walk: Gait perception without familiarity cues",
      "authors": [
        "J Cutting",
        "L Kozlowski"
      ],
      "year": "1977",
      "venue": "Bulletin of the Psychonomic Society"
    },
    {
      "citation_id": "23",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "25",
      "title": "Vfl-a deep learning-based framework for classifying walking gaits into emotions",
      "authors": [
        "X Sun",
        "K Su",
        "C Fan"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "26",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "27",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "28",
      "title": "Epic: Emotion perception by spatio-temporal interaction context of gait",
      "authors": [
        "H Lu",
        "S Xu",
        "S Zhao",
        "X Hu",
        "R Ma",
        "B Hu"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "29",
      "title": "3d human action representation learning via cross-view consistency pursuit",
      "authors": [
        "L Li",
        "M Wang",
        "B Ni",
        "H Wang",
        "J Yang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Contrastive learning from extremely augmented skeleton sequences for selfsupervised action recognition",
      "authors": [
        "T Guo",
        "H Liu",
        "Z Chen",
        "M Liu",
        "T Wang",
        "R Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "31",
      "title": "Hierarchical consistent contrastive learning for skeleton-based action recognition with growing augmentations",
      "authors": [
        "J Zhang",
        "L Lin",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "32",
      "title": "See your emotion from gait using unlabeled skeleton data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "33",
      "title": "Multi-granularity anchorcontrastive representation learning for semi-supervised skeleton-based action recognition",
      "authors": [
        "X Shu",
        "B Xu",
        "L Zhang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Contrastive 3d human skeleton action representation learning via crossmoco with spatiotemporal occlusion mask data augmentation",
      "authors": [
        "Q Zeng",
        "C Liu",
        "M Liu",
        "Q Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Perception of emotions and body movement in the emilya database",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Skeleton-based gait index estimation with lstms",
      "authors": [
        "T.-N Nguyen",
        "H.-H Huynh",
        "J Meunier"
      ],
      "year": "2018",
      "venue": "International Conference on Computer and Information Science (ICIS)"
    },
    {
      "citation_id": "37",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Msa-gcn: Multiscale adaptive graph convolution network for gait emotion recognition",
      "authors": [
        "Y Yin",
        "L Jing",
        "F Huang",
        "G Yang",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Colorful image colorization",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference, Amsterdam"
    },
    {
      "citation_id": "40",
      "title": "Context encoders: Feature learning by inpainting",
      "authors": [
        "D Pathak",
        "P Krahenbuhl",
        "J Donahue",
        "T Darrell",
        "A Efros"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)"
    },
    {
      "citation_id": "42",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "43",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "S Zheng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Self-supervised contrastive learning of multiview facial expressions",
      "authors": [
        "S Roy",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "46",
      "title": "A self-fusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "X Wang",
        "D Zhang",
        "H.-Z Tan",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "47",
      "title": "Augmented skeleton based contrastive action learning with momentum lstm for unsupervised action recognition",
      "authors": [
        "H Rao",
        "S Xu",
        "X Hu",
        "J Cheng",
        "B Hu"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "48",
      "title": "Exploring simple siamese representation learning",
      "authors": [
        "X Chen",
        "K He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)"
    },
    {
      "citation_id": "49",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "50",
      "title": "Adaptive frequency filters as efficient global token mixers",
      "authors": [
        "Z Huang",
        "Z Zhang",
        "C Lan",
        "Z.-J Zha",
        "Y Lu",
        "B Guo"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (ICCV)"
    },
    {
      "citation_id": "51",
      "title": "Contrastive learning with stronger augmentations",
      "authors": [
        "X Wang",
        "G.-J Qi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "52",
      "title": "A motion capture library for the study of identity, gender, and emotion perception from biological motion",
      "authors": [
        "Y Ma",
        "H Paterson",
        "F Pollick"
      ],
      "year": "2006",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "53",
      "title": "Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
      "authors": [
        "C Ionescu",
        "D Papava",
        "V Olaru",
        "C Sminchisescu"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Motion recognition of self and others on realistic 3d avatars",
      "authors": [
        "S Narang",
        "A Best",
        "A Feng",
        "S -H. Kang",
        "D Manocha",
        "A Shapiro"
      ],
      "year": "1762",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "55",
      "title": "Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion",
      "authors": [
        "L Sigal",
        "A Balan",
        "M Black"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "A recurrent variational autoencoder for human motion synthesis",
      "authors": [
        "T Komura",
        "I Habibie",
        "D Holden",
        "J Schwarz",
        "J Yearsley"
      ],
      "year": "2017",
      "venue": "The 28th British Machine Vision Conference(BMVC)"
    },
    {
      "citation_id": "57",
      "title": "Latent dirichlet allocation",
      "authors": [
        "D Blei",
        "A Ng",
        "M Jordan"
      ],
      "year": "2003",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}