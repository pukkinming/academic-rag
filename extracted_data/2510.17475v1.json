{
  "paper_id": "2510.17475v1",
  "title": "Damsdan: Distribution-Aware Multi-Source Domain Adaptation Network For Cross-Domain Eeg-Based Emotion Recognition",
  "published": "2025-10-20T12:18:46Z",
  "authors": [
    "Fo Hu",
    "Can Wang",
    "Qinxu Zheng",
    "Xusheng Yang",
    "Bin Zhou",
    "Gang Li",
    "Yu Sun",
    "Wen-an Zhang"
  ],
  "keywords": [
    "EEG emotion recognition",
    "domain adaptation",
    "conditional distribution alignment",
    "marginal distribution alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Significant inter-individual physiological variability poses a critical challenge to the generalization capability of EEGbased emotion recognition models in cross-domain scenarios. Despite advances in domain adaptation, two major challenges remain in multi-source settings: (1) How to dynamically model the distributional heterogeneity across source domains and quantify their relevance to the target domain, thereby mitigating negative transfer; and (2) how to achieve fine-grained semantic consistency between source and target domains to strengthen the model's class-discriminative capacity. To this end, we introduce a novel deep learning framework named distribution-aware multi-source domain adaptation network (DAMSDAN) to address the above challenges. Specifically, DAMSDAN incorporates prototype-based constraints and adversarial training to guide feature encoder in learning discriminative and domain-invariant emotion representations. Furthermore, a domain-aware source weighting strategy based on maximum mean discrepancy is proposed to dynamically evaluate inter-domain shifts and reweight source contributions, effectively mitigating negative transfer effects. Besides, we propose a prototype-guided conditional alignment strategy, driven by dual pseudo-label interaction, which enhances pseudo-label reliability and leverages category-level prototype information to facilitate stable and fine-grained semantic alignment, thereby effectively alleviating pseudo-label noise propagation and semantic drift. The experiments conducted on the SEED and SEED-IV datasets reveal that DAMSDAN achieves average accuracies of 94.86% and 79.78% under cross-subject settings, and 95.12% and 83.15% under cross-session settings, respectively. On the largescale FACED dataset, DAMSDAN further achieves a leading accuracy of 82.88% under cross-subject settings. Extensive ablation",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A FFECTIVE computing, which lies at the intersection of artificial intelligence and cognitive science, has demonstrated substantial potential in advancing the naturalness and emotional responsiveness of human-computer interaction. Its methodologies have been successfully applied in diverse domains, such as brain-computer interfaces  [1] , virtual reality systems  [2] , and neurorehabilitation  [3] . Among the various modalities employed in affective computing, electroencephalogram (EEG) signals have garnered significant attention due to their noninvasive nature, millisecond-level temporal resolution, and robustness against intentional deception. Therefore, EEG-based emotion recognition is increasingly regarded as a promising approach for decoding nuanced emotional states. However, EEG signals inherently exhibit substantial intersubject variability and nonstationarity, which present major challenges for conventional machine learning and deep learning methods to generalize effectively in cross-domain scenarios, such as cross-subject or cross-session emotion recognition. These challenges often lead to noticeable performance degradation when models are deployed on unseen subjects or sessions. To address these limitations, recent studies have increasingly employed transfer learning strategies to exploit labeled data from source domains (i.e., known subjects) and enhance model's adaptability to unlabeled target domains (i.e., unknown subjects). These approaches aim to minimize reliance on target annotations while mitigating performance degradation when recognizing emotions from EEG signals across domains. As a result, transfer learning has become a prominent and active research direction in affective computing  [4] .\n\nCurrent strategies for emotion recognition using EEG through transfer learning are commonly categorized into two paradigms: Domain generalization (DG) and domain adaptation (DA). Domain-invariant representations are learned in DG methods by extracting shared features across multiple source domains, thereby enabling generalization to new target domains without the need for training data from them. While theoretically promising, DG approaches often face difficulties in achieving stable and robust performance in real-world scenarios, especially when the source domain distribution does not sufficiently cover the diversity of target domains or when substantial inter-subject variability exists. In contrast, DA methods utilize unlabeled target domain data to align the feature distributions of source and target domains. By explicitly mitigating the domain shift caused by inter-subject variability, DA methods have shown greater practical effectiveness in improving generalization performance for EEGbased emotion recognition across domains, and are therefore regarded as a more viable solution for real-world applications. Most existing DA approaches adopt two principal alignment strategies:  (1)  Marginal distribution alignment, which seeks to learning domain-invariant features by aligning the overall feature distributions between the source and target domains; and (2) conditional distribution alignment, which focuses on reducing semantic discrepancies by aligning class-level feature distributions across domains. In practical applications, marginal distribution alignment emphasizes the learning of transferable features, whereas conditional distribution alignment focuses on enhancing class-level discriminability. However, consistency in marginal distributions does not necessarily guarantee semantic alignment across classes. Therefore, one of the key issues in cross-domain EEG-based emotion recognition lies in effectively balance domain invariance and class discriminability under semantic consistency constraints, while achieving joint alignment of both marginal and conditional distributions in a robust and reliable manner.\n\nTo mitigate the marginal distribution discrepancy across source as well as target domains, existing studies generally follow two technical paradigms: (1) Statistical criterion-based alignment and (2) adversarial training-based alignment. The former typically employs metrics such as maximum mean discrepancy (MMD) and correlation alignment to reduce distribution discrepancy in the feature space. Li et al.  [5]  employed MMD to align the marginal distributions between the source and target domains, and subsequently trained a classifier on the aligned shared feature space to enhance emotion recognition performance in the target domain. Ma et al.  [6]  proposed a multi-dimensional alignment strategy based on MMD, which jointly models frequency bands, spatial channels, and temporal dynamics, thereby enhancing the consistency and cross-subject transferability of emotional representations. While statistical approaches offer structural simplicity and computational efficiency, their performance tends to degrade under the inherently nonlinear and low signal-to-noise characteristics of EEG data, thereby resulting in unstable alignment in real-world scenarios. To overcome this limitation, adversarial trainingbased methods have been introduced. These methods leverage a domain discriminator equipped with a gradient reversal layer (GRL) to encourage the feature encoder to learn domaininvariant representations that are indistinguishable across different domains. Li et al.  [7]  introduced a cross-subject domain adaptation approach projecting EEG signals through hemisphere-referenced transformation and employing DANN for feature alignment to achieve robust emotion recognition. Zhou et al.  [8]  proposed a prototype-pair learning framework that combines convolutional neural network (CNN)-based feature extraction and adversarial alignment to suppress pseudolabel noise and improve emotion recognition performance. Despite the effectiveness of adversarial training in aligning marginal distributions, most existing approaches remain limited to the single-source domain adaptation (SSDA) paradigm, which assumes a fixed one-to-one correspondence between a single source domain and the target domain. In practice, target domain data frequently exhibit limited sample sizes and complex distributions. As a result, SSDA methods are unable to exploit complementary information from multiple source domains, which constrains their generalization performance. To address this limitation, researchers have proposed the multi-source domain adaptation (MSDA) framework  [9] -  [11] , which aims to enhance target domain modeling by leveraging diverse and complementary information from multiple source domains. However, many existing MSDA methods adopt an oversimplified assumption of distributional homogeneity among source domains, thereby overlooking the distributional heterogeneity and potential redundancy. Recent studies  [12]  have demonstrated that indiscriminately aggregating data from all source domains may introduce feature conflicts and information interference, thereby inducing negative transfer and degrading performance on the target domain. Therefore, a critical challenge in marginal distribution alignment under MSDA settings lies in dynamically modeling the relevance of each source to the target domain, with the goal of effectively exploiting informative sources while suppressing the adverse impact of irrelevant or noisy ones.\n\nMoreover, aligning marginal distributions alone is insufficient to ensure semantic consistency across source and target domains in terms of categories. Consequently, achieving finegrained semantic alignment regarding conditional distributions has emerged as a critical challenge in EEG-based emotion recognition. Existing conditional distribution alignment methods can generally be classified into two categories: (1) Conditional adversarial alignment and (2) pseudo-label guided alignment. Conditional adversarial methods utilize both feature representations and their associated class predictions to form conditional inputs for the domain discriminator, thereby encouraging the feature encoder to learn representations that are simultaneously domain-invariant and class-discriminative. Long et al.  [13]  put forward a conditional adversarial approach that refines adversarial training using class prediction uncertainty, achieving precise semantic alignment and strong crosssubject generalization for emotion recognition using EEG signals. Ye et al.  [14]  proposed ADALAM, which incorporates hierarchical attention and class-conditioned adversarial learning to enhance feature discriminability and improve crossdomain generalization. Despite their promising performance, conditional adversarial methods often over-rely on sourcedomain labels, making them vulnerable to overfitting and semantic drift in unsupervised target domains. To overcome this limitation, pseudo-label-guided conditional alignment strategies have been introduced  [15] . Li et al.  [16]  proposed a classcentroid contrastive alignment strategy that constructs classconditional centroids using source labels and target pseudolabels, and reinforces class separability through inter-class re-pulsion. Hong et al.  [17]  constructed class-specific prototypes and introduced contrastive and alignment losses to facilitate joint distribution alignment, demonstrating strong potential in improving conditional consistency. However, existing pseudolabeling strategies suffer from two key limitations. First, they typically rely on single-perspective label generation without reliable validation, limiting the accuracy of class assignments. Second, conventional pseudo-label-based alignment methods lack stable semantic anchors, hindering the enforcement of consistent class-level semantics across domains. Therefore, it is critical to develop a robust pseudo-labeling strategy and incorporate structural information to enable fine-grained conditional alignment, thereby improving both the generalization and discriminative power of cross-domain EEG-based emotion recognition models.\n\nA novel distribution-aware multi-source domain adaptation network (DAMSDAN) is proposed to address these challenges, which integrates a marginal distribution alignment (MDA) module and a conditional distribution alignment (CDA) module to achieve joint alignment between the source and target domains at both the global and class-specific semantic levels. Specifically, within the MDA module, a prototype consistency constraint (PCC) and an adversarial domain alignment (ADA) mechanism collaboratively guide the feature encoding (FE) module to learn emotion representations that are both domain-invariant and discriminative. In addition, a domainaware source weighting (DASW) strategy is introduced to dynamically quantify the distribution divergence across each source-target domain pair, thereby adaptively reweighting their transfer contributions and mitigating negative transfer. In the CDA module, a dual pseudo-label collaboration (DPLC) strategy is introduced to enhance pseudo-label reliability, while a prototype-guided conditional alignment (PGCA) mechanism is employed to enforce class-level semantic consistency across domains. Collectively, these components enhance the model's generalization capability under both global and class-specific distribution shifts. Extensive ablation studies and comparative experiments are conducted on three benchmark EEG datasets for emotion recognition, namely SEED, SEED-IV, and FACED, which comprehensively demonstrate the effectiveness and robustness of the proposed DAMSDAN framework. This paper's key contributions can be summarized as follows:\n\n1) A novel MDA module is proposed, which integrates PCC, ADA, and MMD-based DASW to enhance domain-invariant feature learning and suppress negative transfer.\n\n2) We develop a CDA module that integrates DPLC and PGCA strategy, which jointly improve pseudo-label reliability and facilitate class-level semantic consistency between source and target domains.\n\n3) The proposed DAMSDAN model consistently outperforms state-of-the-art methods on SEED, SEED-IV, and FACED datasets, demonstrating superior generalization and robustness in cross-domain EEG emotion recognition. , drawn from the joint distribution D Sm = {X Sm , Y Sm } over X × Y . The target domain T contains N T unlabeled samples {X T } = x T j N T j=1 , drawn from the marginal distribution D T = {X T } over X. The proposed MSDA approach aims to learn a mapping G : X T → Y T that enables accurate emotion recognition on the target domain. This is achieved by minimizing the discrepancy between the joint distributions of S m and T in a shared latent feature space Z. The function G represents a deep neural network, parameterized by the trainable parameters Θ. For clarity and consistency in the subsequent derivations, all notations used throughout this paper can be found in Table  I .\n\nThe architecture of DAMSDAN, depicted in Fig.  1 , is composed of three main components: The FE module, the MDA module, and the CDA module. The FE module integrates a common feature encoder (CFE) and a domain-specific feature encoder (DSFE) to extract transferable and discriminative emotion-related representations. The MDA module incorporates three components: The PCC mechanism, the ADA mechanism, and the DASW strategy. This module aligns marginal feature distributions by minimizing the adversarial loss ℓ adv and enhances class-wise structure through the prototype loss ℓ proto , which enhances intra-class similarity and increases inter-class distinction. The DASW strategy dynamically assigning source-specific weights according to distributional divergence, thereby emphasizing informative sources and suppressing negative transfer effects. The CDA module includes the DPLC strategy and the PGCA strategy, both of which are designed to align conditional distributions. Specifically, DPLC enhances pseudo-label reliability in the target domain, while PGCA promotes semantic consistency across domains. Minimizing the conditional alignment loss ℓ cond strengthens class-level alignment and contributes to more stable decision boundaries. All components are jointly optimized in an end-toend manner by minimizing the multi-source classification loss ℓ cls along with ℓ adv , ℓ proto , and ℓ cond , thereby simultaneously enhancing domain transferability and emotional discriminability. The overall optimization objective is defined as follows:\n\nwhere λ 1 , λ 2 , and λ 3 are trade-off hyperparameters controling the relative contribution among the loss components.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "A. Feature Encoding And Classifier Modules",
      "text": "To learn features with both discriminative power and transferability, we propose the FE module, which consists of two components: The CFE and DSFE. The CFE projects samples from both source and target domains into a unified latent space to capture cross-domain invariant representations. Meanwhile, the DSFE incorporates a multi-head self-attention mechanism to capture global feature dependencies and extract informative inter-domain structural patterns, thereby facilitating more effective modeling of domain shifts. By combining shared and domain-specific representations, the FE module preserves transferable information while retaining domainsensitive characteristics, thereby establishing a robust foundation for subsequent classification and distribution alignment.\n\nGiven the proven affective sensitivity and class discriminability of differential entropy (DE) features in EEG-based emotion recognition  [18] , this paper employs DE features as model input, denoted as x DE ∈ R N ×D , where N represents the number of samples and D denotes the feature dimension. The CFE consists of a fully connected network (FCN) with three layers containing 256, 128, and 64 neurons, respectively, performing progressive dimensionality reduction to extract domain-invariant latent representations. The DSFE comprises a single FCN layer with 32 neurons and incorporates a selfattention mechanism inspired by the Transformer architecture to capture long-range dependencies among input features, thereby facilitating the modeling of domain-specific structural variations. The overall network structure is illustrated in Fig.  2 . All FCN layers adopt the LeakyReLU activation function with a negative slope coefficient of α = 0.01, which improves the model's sensitivity to negative-valued inputs. Batch normalization (BN) is subsequently applied to the FE module outputs to standardize feature distributions, thereby accelerating convergence and improving generalization during training. Given an input x DE , the feature extraction process is formally defined as: where FCF E (•) and FDSF E (•) denote the operations of the CFE and DSFE, respectively, and f com ∈ R N ×Dc and f spe ∈ R N ×Ds represent their corresponding output features.\n\nNext, a multi-head self-attention mechanism is employed to capture latent global dependencies within the feature representation f spe . Specifically, f spe is evenly partitioned into H subspaces, producing a set of sub-feature matrices\n\nare applied to generate the corresponding query Q h , key K h , and value V h matrices, respectively. The attention computation is defined as follows:\n\nSubsequently, the scaled dot-product between the query and key matrices is calculated and passed through the Softmax function to yield attention weights, which are then used to weight the corresponding value matrix, producing the output of the h-th attention head, denoted as SA h ∈ R N ×D h . This process can be formally expressed as:\n\nwhere K h T denotes the transpose of the key matrix K h , and d K represents the dimensionality of K h . Finally, the outputs of all attention heads are concatenated and linearly projected to recover the original dimensionality, resulting in the multi-head context-enhanced feature representation F * ∈ R N ×Ds :\n\nwhere Concat (•) denotes the concatenation function, and W * represents the learnable weight matrix used for linear projection.\n\nIn each source domain branch, the proposed classifier module comprises two FCNs responsible for predicting emotion categories of source domain samples. The second FCN employs the LeakyReLU activation function to enhance the model's capability for capturing nonlinear emotional representations. For the m-th source domain, the predicted labels Ŷ Sm are obtained by passing the extracted features F * Sm through the classifier. During training, the classification performance on the source domains is optimized using the cross-entropy loss, formally defined as:\n\nwhere ŷSm i and y Sm i denote the predicted and ground-truth labels of the i-th sample from the m-th source domain, respectively, and J (•) represents the standard cross-entropy loss function. The classification loss ℓ cls encourages the model to learn discriminative EEG representations, thereby providing a reliable foundation for the subsequent distribution alignment modules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Marginal Distribution Alignment Module",
      "text": "To achieve marginal distribution alignment of source and target domains within the feature space, the proposed MDA module comprises three key components: The PCC mechanism, the ADA mechanism, and the DASW strategy. The following outlines the details of each component:\n\nPCC: To improve the structural expressiveness of the extracted features in the semantic space, a prototype-based constraint is introduced. It is assumed that the feature representations corresponding to each emotional category follow a Gaussian distribution N µ c , σ 2 c in the latent embedding space. Accordingly, the prototype vector for category c, denote as µ c , is estimated as the mean of the feature representations assigned to that category. Given a labeled sample\n\nfrom the m-th source domain and a feature encoder F (•), the class-wise prototype vector µ c is computed as:\n\nwhere N c Sm denotes the sample count of category c from the m-th source domain, and µ Sm c denotes the class prototype of category c in source domain S m , which is dynamically updated during training based on the evolving feature distribution. To promote class separability and enhance the structural expressiveness of the feature space, we introduce a prototype loss that minimizes the Euclidean distance of every sample from its respective class prototype. The loss function is defined as: ADA: To match the global feature distributions of different domains, we utilize an ADA mechanism based on gradient reversal. Specifically, a domain discriminator is introduced to differentiate between source and target features, while the feature extractor F (•) is adversarially trained to produce domain-invariant representations. This is achieved through a GRL, which enables end-to-end adversarial learning. The domain adversarial loss is defined as:\n\nDASW: To mitigate negative transfer arising from equal treatment of all source domains in multi-source adaptation, we propose the DASW that dynamically models the varying transfer contributions of each source domain to the target domain. As illustrated in Fig.  3 , DASW strategy quantifies the feature distribution discrepancies between each source domain and the target domain, and assigns adaptive fusion weights accordingly. This approach amplifies the influence of semantically relevant sources while suppressing the interference of irrelevant or noisy ones, thereby improving the model's robustness and effectiveness.\n\nSpecifically, for the m-th source-target domain pair, the source domain features F * Sm and the target domain features F * T , obtained after adversarial training, are formulated as follows:\n\nwhere F * Sm and F * T denote the source and target domain features prior to training, respectively. ADA (•) represents the adversarial domain alignment process. In order to assess transferability between source and target domains, the squared maximum mean discrepancy MMD 2 values are normalized across all source domains to derive the initial weight distribution:\n\nwhere K(•, •) denotes a Gaussian kernel function.\n\nTo emphasize the contribution of source domains with lower distributional discrepancy, a Gaussian function is employed to further amplify their influence. The fusion weight for the m-th source domain is then computed via normalization as follows:\n\nwhere γ controls the decay rate of the weighting function, thereby balancing the contribution between source domains with varying levels of correlation to the target domain.\n\nDuring inference, the final prediction for a target domain sample X T is computed via a weighted aggregation of the output probabilities from all source domain classifiers. where C m F * T denotes the class probability distribution predicted by the m-th source domain model for the target sample X T . In summary, the proposed DASW strategy adaptively adjusts the transfer contribution of each source domain model according to its actual distributional proximity to the target domain, thereby minimizing the risk of negative transfer in multi-source domain adaptation.\n\nIt is noteworthy that during the adversarial domain alignment process, the extracted features not only facilitate marginal distribution alignment via the domain discriminator, but also dynamically update the fusion weights in the DASW strategy (as defined in Eqs. (  12 ) and (  13 )). Specifically, at each training stage, the MMD distance between each sourcetarget domain pair is computed based on the current feature representations, and the corresponding fusion weights are adaptively updated. This joint optimization fosters the synergy between marginal distribution alignment and source model fusion, thereby improving the overall effectiveness and stability of knowledge transfer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Conditional Distribution Alignment Module",
      "text": "In unsupervised domain adaptation tasks, aligning only the marginal distributions between source and target domains and reducing the classification error on the source domain is often insufficient to achieve satisfactory transfer performance  [19] . To enhance the model's generalization capability, it is essential to align conditional distributions by promoting feature consistency among samples of the same emotional class across different domains. To this end, we propose a CDA module that integrates two key strategies. First, the DPLC strategy is introduced to enhance the reliability of pseudo-labels within the target domain. Second, the PGCA strategy is used to construct and align class-level prototypes across domains, thereby mitigating semantic shift and improving cross-domain discriminability.\n\nDPLC: To reduce the negative effects of pseudo-label noise propagation on training stability, we propose a dual pseudolabel generation mechanism that exploits both discriminative and structural information. Specifically, the first type of pseudo-label is derived from classifier-predicted probabilities, reflecting the model's discriminative ability. The second type is obtained via the K-Means++ clustering algorithm, which captures the intrinsic structural distribution of target domain features within the latent space. To improve the quality of pseudo labels, a collaboration strategy is introduced, which selects high-confidence samples based on the consistency between two independently generated pseudo-label sources. These samples are subsequently incorporated into training to effectively suppress error accumulation and improve the robustness of pseudo-label learning.\n\nThe first type of pseudo-label is generated by assessing how similar a target sample is to the class prototypes in the source domains. Let µ Sm c denote the prototype of class c in the m-th source domain. Then, for a target sample x T , its probability of belonging to class c is defined as:\n\nwhere d(•, •) denotes the Euclidean distance function, F(•) represents the feature encoder, and C indicates the overall quantity of emotion classes. This method leverages semantic prototypes from the source domain to guide the class assignment for target samples, thereby generating pseudo-labels. However, owing to notable differences in data distributions across the source and target domains, directly relying on source-domain prototypes for label inference may lead to low pseudo-label confidence, increasing the risk of overfitting during target domain adaptation.\n\nTherefore, a structure-aware pseudo-label generation strategy is proposed by applying the K-Means++ clustering algorithm to perform unsupervised clustering on target domain features. This method aims to uncover the latent structural organization of samples in the embedding space and produce pseudolabels with improved structural consistency. Specifically, the source domain prototype µ Sm c for class c used to initialize the corresponding cluster center µ T c in the target domain, thereby establishing a one-to-one semantic correspondence across categories. During training, µ T c is iteratively updated based on the clustering results, enabling progressive structural adaptation across training rounds. Based on the updated cluster centers, the conditional probability of assigning target sample x T to class c is computed by evaluating its distances to all cluster centers. The probability is defined as follows:\n\nGiven that the two types of pseudo-labels are derived from the discriminative and structural perspectives, each capturing complementary aspects of the data, their consistency provides a critical criterion for evaluating pseudo-label reliability. Based on this insight, we introduce a pseudo-label collaboration learning mechanism to identify high-confidence target samples whose labels are consistent across both perspectives. These samples are then used to lead the training phase, facilitating the model to progressively focus on aligning distributions and enhancing class discrimination using the most reliable supervision. For a target sample x T , let P Sm y T = c|x T and P T y T = c|x T denote the pseudo-label prediction probabilities from the discriminative and structural perspectives, respectively. The predicted class labels ŷT Sm and ŷT T are then obtained via the maximum probability criterion as follows:\n\nBased on the prediction outputs, if the two kinds of pseudolabels are consistent, i.e., ŷT Sm = ŷT T , the corresponding target sample is regarded as label-consistent and included in the consistency-based pseudo-labeled set D eq = x T |ŷ T Sm = ŷT T . To further enhance the reliability of pseudo-labels, we select the top tN T T samples from D eq with the highest prediction confidence at each training epoch t, where N T represents the overall quantity of target samples and T represents the total quantity of training epochs. These selected samples are treated as high-confidence pseudo-supervised instances for the current training round. For each of these samples, the final pseudo-label is generated by fusing the predicted probabilities from both the discriminative and structural perspectives, with fusion weights dynamically adjusted according to the training progress. The fused pseudo-label for a target sample x T is computed as follows:\n\nwhere χ = 1 1 + e -k(t-T /2) . The final fused class label is assigned according to the maximum probability criterion, as defined below:\n\nThis fusion strategy effectively facilitates the model's gradual shift from source-domain supervision to target-domain pseudo-supervision. As training progresses, the model increasingly focuses on the intrinsic feature distribution of the target domain, thereby enhancing its ability to capture underlying structural patterns.\n\nPGCA: To effectively mitigate conditional distribution shifts between source and target domains while preserving class-discriminative structures, we propose a prototype-guided conditional alignment strategy. As illustrated in Fig.  4 , this method enhances class-level feature consistency by minimizing the conditional distribution discrepancy between target samples and source-domain class prototypes.\n\nTo further mitigate the conditional distribution shift between the source and target domains, this paper introduces a prototype-based conditional alignment loss. This loss quantitatively evaluates semantic divergence by computing the distance between source-domain class prototypes and corresponding target-domain features. The objective function is formally defined as follows: where y T denotes the pseudo-label of the target sample (as defined in Eq. (  20 )), and 1[•] corresponds to an indicator function that yields 1 if the condition is met, and 0 if not. This loss function aims to pull target features closer to their corresponding class prototypes when labels match, and push them apart otherwise. By promoting intra-class compactness and inter-class separability, it facilitates class-level conditional distribution alignment and effectively mitigates semantic shift across domains. Finally, the proposed cross-domain emotion recognition model jointly optimizes four core loss components. The overall objective function is formulated as follows:\n\nwhere ℓ cls denotes the source classification loss, ℓ adv represents the adversarial domain alignment loss, ℓ proto corresponds to the prototype consistency loss, and ℓ cond refers to the conditional distribution alignment loss. The coefficients λ 1 , λ 2 , and λ 3 are trade-off parameters that balance the contributions of each loss component. By jointly incorporating discriminative enhancement, marginal distribution alignment, and conditional distribution alignment, the proposed objective function enables the learning of emotion representations that are both domain-generalizable and category-discriminative, thereby improving the robustness of cross-domain emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Preprocessing",
      "text": "To thoroughly assess the capability of the proposed DAMS-DAN model in emotion recognition across domains based on EEG data, we perform comprehensive experiments on three publicly accessible benchmark EEG emotion datasets: SEED  [20] , SEED-IV  [21] , and FACED  [22] . The core characteristics of each dataset, along with their respective preprocessing protocols, are summarized below.\n\nSEED Dataset: This dataset, constructed by Zheng et al.  [20] , is one of the most widely used benchmarks in EEGbased emotion recognition research. A 62-channel ESI Neuroscan system recorded EEG signals at a 1000 Hz sampling rate. The dataset comprises three emotion categories: Positive, negative, and neutral. Data were collected from 15 participants (8 females and 7 males, with a mean age of 23), each of whom watched 15 emotion-eliciting film clips, five for each category, across three recording sessions conducted on different days. SEED-IV Dataset: This dataset serves as an expanded version of the SEED dataset, developed to support more fine-grained multi-class emotion recognition tasks. It adopts the same EEG acquisition protocol as SEED, utilizing a 62channel ESI Neuroscan system. In contrast to SEED, the number of emotional categories is expanded from three to four: Happiness, neutral, sadness, and fear. During data collection, each participant viewed 24 emotion-inducing video clips, including six clips under each emotional label to induce the appropriate affective states.\n\nFACED Dataset: This dataset, constructed by Zhang et al.  [22] , is among the largest publicly available resources for EEG-based emotion recognition. It comprises 32-channel EEG recordings from 123 participants. During the experiment, each annotated with one of nine discrete emotional states: Pleasure, tenderness, inspiration, happiness, sadness, disgust, fear, anger, and neutrality. Following the protocol established in  [23] , this study categorizes pleasure, inspiration, happiness, and tenderness as positive affective states, and anger, disgust, fear, and sadness as negative affective states, thereby formulating a binary emotion classification task.\n\nData Preprocessing: Downsampling to 200 Hz was first applied to the raw EEG signals. Then, a 0.3-50 Hz band-pass filter was then used to filter out low-frequency drifts and highfrequency interference, while preserving neural oscillatory components associated with emotional processing. To further attenuate residual artifacts, we applied independent component analysis. Subsequently, the continuous EEG signals were segmented into non-overlapping time windows of 1 second duration. DE features were then extracted from each window across five standard frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz), and Gamma (31-50 Hz). In the SEED and SEED-IV datasets, each EEG segment comprises signals from 62 channels, resulting in a 310-element feature vector derived from 5 frequency bands across 62 channels. For the FACED dataset, which includes 32 EEG channels, each segment yields a 160-dimensional feature vector accordingly. To capture the temporal dynamics of emotional states and suppress high-frequency fluctuations unrelated to affective content, a linear dynamical system was applied to smooth the DE feature sequences. This process enhanced the temporal coherence of emotion-relevant patterns in the EEG data  [8] .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In this study, a multi-layer perceptron (MLP) architecture is adopted as the backbone to implement the feature encoder, classifier, and domain discriminator. All components utilize the LeakyReLU activation function, and network weights are initialized using a uniform distribution. As described previously, the FE module is composed of two submodules: The CFE and DSFE. The CFE architecture consists of: 310 (input layer)-256 (hidden layer #1)-LeakyReLU-128 (hidden layer #2)-LeakyReLU-64 (output layer). The DSFE is designed as: 64 (input layer)-LeakyReLU-32 (hidden layer)-LeakyReLU-4 head self-attention mechanism-32 (output layer). The classifier module comprises: 32 (input layer)-16 (hidden layer #1)-LeakyReLU-C (output layer). The domain discriminator follows: 32 (input layer)-16 (hidden layer)-LeakyReLU-1 (output layer)-Sigmoid activation.\n\nDuring training, all parameters are optimized using the Adam optimizer with an initial learning rate of 2e-4 and a batch size of 256. All experiments were performed on a workstation featuring an NVIDIA GeForce RTX 4060 GPU, and the implementation is developed using the PyTorch framework with CUDA 10.0 support. Following training, we evaluate the computational complexity of DAMSDAN by measuring the average inference latency and the number of trainable parameters. The model yields an average inference time of 0.26 ms per EEG sample and consists of approximately 7.1 MB of trainable parameters. The complete code is hosted at: https://github.com/ZJUTofBrainIntelligence/DAMSDAN.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Experimental Protocols",
      "text": "To comprehensively evaluate the performance of the proposed DAMSDAN model in cross-domain emotion recognition tasks, we adopt two widely used validation protocols: (1) Cross-subject leave-one-subject-out cross-validation and (2) within-subject cross-session cross-validation. Specifically, for the SEED and SEED-IV datasets, both validation strategies are applied to evaluate the model's generalization across individuals and sessions. For the FACED dataset, which contains only single-session recordings per subject, only cross-subject leaveone-subject-out cross-validation is performed. The detailed experimental settings are as follows:\n\nCross-subject leave-one-subject-out cross-validation. This strategy is among the most widely adopted evaluation protocols in EEG-based emotion recognition research  [5] ,  [24] -  [27] . In each iteration, data from one subject's session are designated as the unlabeled target domain, while data from the corresponding sessions of the remaining subjects serve as labeled source domains. Following prior works  [8] , only the first session of each subject is used in this cross-subject evaluation. For the SEED and SEED-IV datasets, each subject provides a single session containing 3,394 EEG segments. Accordingly, the source domain comprises 14 subjects × session × 3,394 = 47,516 labeled samples, and the target domain includes 1 subject × 1 session × 3,394 = 3,394 unlabeled samples. Each subject is sequentially selected as the target domain, and the final performance is reported as the average recognition accuracy over all testing rounds. For the FACED dataset, which consists of 123 subjects with single-session recordings, we follow the evaluation protocol described in  [23] : Where in 10 subjects are randomly selected as source domains, and the remaining 113 subjects are used individually as target domains. The final performance is presented as the mean recognition accuracy over all target subjects.\n\nWithin-subject cross-session cross-validation. This evaluation protocol is employed to assess the model's temporal stability and its generalization capability across different recording sessions. Specifically, in both the SEED and SEED-IV datasets, each subject participates in three separate recording sessions conducted on different days. In each round of evaluation, the initial two sessions of a subject are considered the labeled source domain, with the final session acting as the unlabeled target domain. For each subject, the source domain contains 2 sessions × 3,394 samples = 6,788 samples, and the target domain contains 1 session × 3,394 samples = 3,394 samples. The session partitioning strategy for SEED-IV follows that of SEED. This procedure is repeated for all subjects, and the final performance is reported as the mean and standard deviation of classification accuracy across all target sessions.\n\nWe comprehensively compared the proposed DAMSDAN model with a wide spectrum of baseline methods, encompassing both traditional machine learning and deep learning-based domain adaptation methods. The traditional machine learning-based domain adaptation methods include: JDA  [27] , MIMD  [27] , MS-STM  [27] , GFHF  [27] , SA  [27] , KNN  [27] , PF  [27] , CORAL  [27] , GFK  [28] , TKL  [28] , TCA  [29] , SVM  [30] , TPT  [30]  and MEDA  [31] . The deep learning-based domain adaptation methods comprise: PR-PL  [8] , UDDA  [16] , ADANN  [17] , EPNNE  [23] , DDC  [27] , DAN  [30] , MS-MDA  [32] , MS-ADA  [33] , DANN  [34] , LRS  [34] , MGFKD  [35] , S2A2-MSDA  [36] , ASJDA  [37] , DGCNN  [38] , DCCA  [39] , MEERNET  [40] , MSHCL  [41] , CLISA  [42]  and MSMRA  [43] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Experimental Results On Cross-Subject Leave-One-Subject-Out Cross-Validation",
      "text": "We conducted cross-subject leave-one-subject-out crossvalidation experiments on three publicly available EEG-based emotion recognition datasets: SEED, SEED-IV, and FACED. The performance comparisons between the proposed DAMS-DAN model and a broad spectrum of state-of-the-art methods are presented in Tables II, III, and IV, respectively. On the SEED dataset, DAMSDAN achieved an average accuracy of 94.86%±04.87%, substantially outperforming all traditional machine learning and deep learning-based methods. Compared to the best-performing method PR-PL (93.06%±05.12%), DAMSDAN achieved a 1.8% absolute improvement, underscoring its superior performance. For the more challenging SEED-IV dataset involving a four-class emotion classification task, DAMSDAN maintained strong performance with an average accuracy of 82.48%±08.02%, outperforming representative domain adaptation methods such as PR-PL, S2A2-MSDA, UDDA, and MS-CNN. On the large-scale FACED dataset, DAMSDAN achieved an accuracy of 82.88%±07.77%, once again ranking first among all compared methods. Except for the second-best method EPNNE, DAMSDAN consistently outperformed the remaining approaches by at least 5%, demonstrating its robustness and scalability in large-sample scenarios. In summary, DAMSDAN consistently attains leading performance on all three benchmark EEG emotion datasets, confirming its effectiveness and generalization capability in cross-subject emotion recognition.\n\nTo further assess the subject-wise adaptability of the proposed DAMSDAN model, we analyze the classification performance for each individual under the cross-subject leave-onesubject-out cross-validation scheme on the SEED and SEED-IV datasets. As illustrated in Fig.  5 , DAMSDAN achieves",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "Pacc(%) Method Pacc(%) Traditional Machine Learning Methods GFK  [28]  71.31±14.09 TCA  [29]  63.64±14.88 TKL  [28]  63.54±15.47 SVM  [30]  58.18±13.85 TPT  [30]  75.17±12.83 JDA  [27]  73.50±06.30 Deep Learning Methods MS-MDA  [32]  81.43±10.17 JDA-Net  [34]  86.70±11.39 UDDA  [16]  88.10±06.54 MGFKD  [35]  90.21±07.57 EPNNE  [23]  89.10±03.60 S2A2-MSDA  [36]  90.11±07.32 PR-PL*  [8]  93.06±05.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Method",
      "text": "Pacc(%) Method Pacc(%) Traditional Machine Learning Methods GFK  [28]  44.04±09.31 TCA  [29]  37.01±10.47 MIMD  [27]  60.22±08.69 MS-STM  [27]  61.41±09.72 GFHF  [27]  49.29±07.60 JDA  [27]  54.60±05.30 Deep Learning Methods MS-MDA  [32]  59.34±05.48 MS-ADA  [33]  59.29±13.65 UDDA  [16]  73.14±09.43 DAN  [30]  58.89±08.13 S2A2-MSDA  [36]  76.23±09.02 DANN  [34]  54.63±08.03 PR-PL*  [8]  81.32±08.53 ASJDA  [37]  79.58±17. over 85% accuracy for all 15 subjects in the SEED dataset, with more than half exceeding 95%. Notably, the DAMSDAN model achieves near-perfect performance for Subjects 2, 3, and 6, with accuracies of 99.71%, 100%, and 100%, respectively. In the SEED-IV dataset, while subject-wise variability is more pronounced, most subjects attain accuracies between 70% and 90%, with Subjects 5, 6, and 13 achieving the highest accuracies of 91.23%, 92.18%, and 92.20%, respectively. These results demonstrate that DAMSDAN consistently yields strong emotion recognition performance across subjects.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. Experimental Results Of Within-Subject Cross-Session Cross-Validation",
      "text": "To further assess the recognition performance and generalization capability of the proposed DAMSDAN model under temporal distribution shift, we conducted within-subject cross-session cross-validation experiments on the SEED and SEED-IV datasets. The results are presented in Table V and Table  VI , respectively. On the SEED dataset, DAMSDAN achieved an average accuracy of 95.12%±5.92%, substantially outperforming all baseline methods. Notably, compared to the second-best deep domain adaptation method PR-PL (93.18%±06.55%), DAMSDAN achieved an absolute improvement of 1.94%. Furthermore, on the SEED-IV dataset,",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "Pacc(%) Method Pacc(%) Traditional Machine Learning Methods GFK  [28]  60.40±08.08 TCA  [29]  56.35±09.68 JDA  [27]  61.54±07.45 MEDA  [31]  62.46±08.71 Deep Learning Methods LRS  [34]  65.83±07.93 DGCNN  [38]  68.25±07.92 DCCA  [39]  74.64±06.07 EPNNE*  [23]  82.81±03.28 MSHCL  [41]  77.00±03.60 CLISA  [42]  75.10±03.50 DAMSDAN 82.88±07.71\n\n* indicates the second-best model.  DAMSDAN attained the highest accuracy of 83.15%±07.43%, significantly surpassing both the representative deep learning method S2A2-MSDA (80.07%±12.13%) and the traditional machine learning method CORAL (66.06%±15.13%). These results demonstrate that DAMSDAN not only maintains high classification accuracy across sessions but also exhibits strong robustness and adaptability to temporal distribution variations.\n\nTo further evaluate the cross-session generalization capability of the proposed DAMSDAN model at the individual level, we report the session-independent classification accuracy for each subject, as illustrated in Fig.  6 . Overall, DAMSDAN exhibits strong generalization performance on the SEED dataset, achieving accuracies above 90% for nearly all subjects. Remarkably, the DAMSDAN model attains nearperfect accuracies of 99.99%, 99.98%, 100%, and 99.79% for",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Method",
      "text": "Pacc(%) Method Pacc(%) Traditional Machine Learning Methods SA  [27]  69.84±09.46 KNN  [27]  75.68±13.82 GFK  [28]  78.79±09.39 TCA  [29]  74.27±12.88 PF  [27]  76.42±11.15 CORAL  [27]  84.18±09.81 Deep Learning Methods MS-MDA  [32]  89.63±06.79 DAN  [30]  80.00±08.88 UDDA  [16]  90.19±10.07 DANN  [34]  84.45±07.92 DDC  [27]  81.53±06.83 MSMRA  [43]  88.31±06.23 PR-PL*  [8]  93.18±06.55 ADANN  [17]",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Method",
      "text": "Pacc(%) Method Pacc(%) Traditional Machine Learning Methods SA  [27]  52.81±16.28 KNN  [27]  54.18±16.28 GFK  [28]  56.14±15.13 TCA  [29]  51.88±15.84 PF  [27]  60.27±16.36 CORAL  [27]  66.06±15.13 Deep Learning Methods MS-MDA  [32]  61.43±15.71 DAN  [30]  48.39±06.97 MEERNet  [40]  72.10±14.10 DANN  [34]  47.66±08.38 PR-PL  [8]  74.62±14.15 MSMRA  [43]  72.38±10.12 ADANN  [17]  77.79±13.97 S2A2-MSDA*  [36]  80.07±12.13 DAMSDAN 83.15±07.43\n\n* indicates the second-best model. subiects 4, 6, 12 and 14, respectively. In contrast, although the overall performance slightly decreases on the more challenging SEED-IV dataset, DAMSDAN still maintains consistent and robust results across most subjects. In particular, subiects 5, 8, and 10 achieveaccuracies notably above the average. These results further substantiate the subject-level stability and effectiveness of DAMSDAN in cross-session EEG-based emotion recognition.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Iv. Discussion",
      "text": "In this section, we perform parameter sensitivity analyses on the SEED dataset using the cross-subject leave-one-subject-out cross-validation protocol. The objective is to systematically investigate the influence of key modules and hyperparameter settings on model performance, thereby elucidating the contribution of each component to the overall effectiveness in EEG-based emotion recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Ablation Study",
      "text": "To systematically evaluate the individual contribution of each core module within the DAMSDAN model, a series of ablation experiments were conducted on the SEED dataset. All experiments were performed under identical training configurations, with only the specific component under investigation CDA module: To assess the performance of the CDA module, we conducted experiments under two settings: (1) Removing the entire CDA module; and (2) replacing the dual pseudo-label collaboration strategy with a conventional single pseudo-label mechanism  [16] . The former led to a performance decline to 92.16%, while the latter yielded an accuracy of 93.10%.\n\nIn summary, the ablation study results clearly demonstrate that each core component of the proposed DAMSDAN model, including ADA mechanism, PCC mechanism, DASW strategy, CDA module, and DPLC strategy, contributes substantially to the overall performance improvement. Among these, the ADA mechanism proves to be the most critical, as its removal leads to the most pronounced performance degradation. In contrast, the PCC mechanism has the least impact, though it still contributes meaningfully to improving feature discriminability. These findings validate the architectural design and confirm the effectiveness of the proposed framework.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. T-Sne",
      "text": "To further assess the effectiveness of DAMSDAN in achieving both domain alignment and discriminative feature learning, we employ the t-SNE method  [44]  to visualize the highdimensional feature representations on the SEED dataset, as depicted in Fig.  7 . In Fig.  7 (a), prior to training, source (red) and target (blue) samples are extensively intermingled in the",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Confusion Matrix",
      "text": "To further evaluate the discriminative capability of DAMS-DAN in EEG-based emotion classification tasks, we present the confusion matrix on the SEED dataset and perform a comparative analysis with several representative baseline models, as illustrated in Fig.  8 . As shown in Fig.  8(d) , DAMSDAN achieves classification accuracies exceeding 94% across all three emotional categories, demonstrating both strong and consistent performance. Specifically, the DAMSDAN model attains accuracies of 95.66%, 94.71%, and 94.58% for the positive, neutral, and negative classes, respectively, with corresponding misclassification rates constrained within the range of 1.16% to 4.26%. In comparison, the compared methods in Fig.  8 (a)-(c) exhibit notably inferior performance across all emotional categories. Compared with the second-best model, PR-PL, DAMSDAN yields absolute accuracy gains of 1.13%, 1.76%, and 3.59% for the positive, neutral, and negative categories, respectively. The performance gap widens further when compared with the weakest model, MS-MDA, showing improvements of 9.6%, 11.86%, and 10.71% for the positive, neutral, and negative categories, respectively. These results underscore the effectiveness of DAMSDAN in reducing inter-class confusion and maintaining balanced performance across emotional states, thereby validating its robustness and generalization capability in cross-domain EEG-based emotion recognition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D. Topographical Analysis Of Salient Eeg Patterns",
      "text": "During the topological feature analysis stage, mutual information is computed between EEG input features and model prediction outputs on the SEED dataset to identify the most informative frequency bands and brain regions contributing to emotion recognition. Specifically, in the j-th validation fold, the target domain input samples are denoted as x T j ∈ R N T ×D T , where N T represents the number of target samples and D T denotes the total dimensionality of DE features extracted from five frequency bands across 62 EEG channels. The corresponding model output is denoted as ŷT j ∈ R N T ×3 , representing the predicted probability distributions over the three emotional classes: negative, neutral, and positive. To quantify the contribution of different EEG patterns to emotion classification, a non-parametric mutual information estimation approach, as proposed in  [45] -  [47] , is employed to compute the mutual information matrix I x T j , ŷT j ∈ R 3×D T between\n\nx T j and ŷT j . This matrix characterizes the statistical dependence between individual frequency-channel feature pairs and the predicted emotional outputs. To ensure consistency across validation folds, the computed mutual information values are normalized to the range [0, 1], where higher values indicate greater discriminative contributions from the corresponding feature modality. Fig.  9  illustrates the average normalized mutual information values across all validation folds. The results reveal that among all frequency bands, the β and γ bands carry the most salient information. Furthermore, the corresponding discriminative features are primarily concentrated in the prefrontal and temporal regions, which are critically involved in affective processing and contribute substantially to robust cross-domain emotion recognition.\n\nV. CONCLUSION In this paper, we introduce a novel deep learning-driven domain adaptation approach, termed DAMSDAN, specifically designed to address the challenges of individual variability and distributional shifts in cross domain EEG-based emotion recognition. The proposed framework is organized into three core modules: FE module, MDA module, and CDA module. These components collectively aim to improve cross-domain generalization and emotional class discriminability. In the feature encoding stage, DAMSDAN integrates both common and domain-specific encoders to effectively extract emotion relevant and transferable EEG representations. For marginal distribution alignment, DAMSDAN integrates the PCC mechanism with the ADA mechanism to simultaneously improve intra-class compactness and inter-class separability. In addition, a DASW strategy is introduced to dynamically adjust the contribution of each source domain based on its distributional relevance to the target domain, effectively suppressing negative transfer. For conditional distribution alignment, DAMSDAN integrates a DPLC strategy together with a PGCA strategy to enhance the stability of pseudo-label supervision and facilitate fine-grained class-level alignment. Extensive experiments conducted on three publicly available EEG emotion datasets, namely SEED, SEED IV, and FACED, demonstrate that DAMSDAN outperforms state-of-the-art methods in both cross-subject and cross-session scenarios. In addition, ablation studies further validate the effectiveness and indispensability of each proposed module. In conclusion, DAMSDAN offers a robust and scalable solution for cross-domain EEG-based emotion recognition and establishes a solid foundation for future research in domain-adaptive affective brain-computer interface systems.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall framework of the proposed DAMSDAN model.",
      "page": 4
    },
    {
      "caption": "Figure 2: All FCN layers adopt the LeakyReLU activation",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of the FE module.",
      "page": 5
    },
    {
      "caption": "Figure 3: , DASW strategy quantifies",
      "page": 6
    },
    {
      "caption": "Figure 3: Illustration of the DASW strategy.",
      "page": 6
    },
    {
      "caption": "Figure 4: Illustration of the PGCA strategy.",
      "page": 8
    },
    {
      "caption": "Figure 5: , DAMSDAN achieves",
      "page": 9
    },
    {
      "caption": "Figure 5: Subject-wise classification accuracy under cross-subject leave-one-",
      "page": 10
    },
    {
      "caption": "Figure 6: Subject-wise classification accuracy under within-subject cross-session",
      "page": 10
    },
    {
      "caption": "Figure 6: . Overall,",
      "page": 10
    },
    {
      "caption": "Figure 7: In Fig. 7(a), prior to training, source (red)",
      "page": 11
    },
    {
      "caption": "Figure 7: t-SNE visualization of feature distributions: (a) before training; (b)",
      "page": 12
    },
    {
      "caption": "Figure 8: Confusion matrices of MS-MDA, S2A2-MADA, PR-PL, and DAMS-",
      "page": 12
    },
    {
      "caption": "Figure 7: (b) displays the feature distribution after training with",
      "page": 12
    },
    {
      "caption": "Figure 9: Topographical analysis of mutual information between EEG patterns",
      "page": 12
    },
    {
      "caption": "Figure 8: As shown in Fig. 8(d), DAMSDAN",
      "page": 12
    },
    {
      "caption": "Figure 8: (a)-(c) exhibit notably inferior performance across all",
      "page": 12
    },
    {
      "caption": "Figure 9: illustrates the average normalized",
      "page": 13
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "An EEG-based brain computer interface for emotion recognition and its application in patients with disorder of consciousness",
      "authors": [
        "H Huang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Focus on Cooperation: A Face-to-Face VR Serious Game for Relationship Enhancement",
      "authors": [
        "Y Bian",
        "C Zhou",
        "Y Zhang",
        "J Liu",
        "J Sheng",
        "Y.-J Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "A Review of Tools and Methods for Detection, Analysis, and Prediction of Allostatic Load Due to Workplace Stress",
      "authors": [
        "K Magtibay",
        "K Umapathy"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Transfer learning for motor imagery based brain-computer interfaces: A tutorial",
      "authors": [
        "D Wu",
        "X Jiang",
        "R Peng"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "5",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Proc. 25th Int. Conf. Neural Inf. Process. Systems (NeurIPS)"
    },
    {
      "citation_id": "6",
      "title": "Cross-Subject Emotion Recognition Based on Domain Similarity of EEG Signal Transfer Learning",
      "authors": [
        "Y Ma",
        "W Zhao",
        "M Meng",
        "Q Zhang",
        "Q She",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "7",
      "title": "A bihemisphere domain adversarial neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "PR-PL: A Novel Prototypical Representation Based Pairwise Learning Framework for Emotion Recognition Using EEG Signals",
      "authors": [
        "R Zhou"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Spectral-Spatial Attention Alignment for Multi-Source Domain Adaptation in EEG-Based Emotion Recognition",
      "authors": [
        "Y Yang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Multi-source and multi-representation adaptation for cross-domain electroencephalography emotion recognition",
      "authors": [
        "J Cao",
        "X He",
        "C Yang",
        "S Chen",
        "Z Li",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "12",
      "title": "Multisource transfer learning for cross-subject EEG emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "M Long"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Cross-subject EEG-based Emotion Recognition Using Adversarial Domain Adaption with Attention Mechanism",
      "authors": [
        "Y Ye",
        "X Zhu",
        "Y Li",
        "T Pan",
        "W He"
      ],
      "year": "2021",
      "venue": "Proc. 43rd Annu. Int. Conf"
    },
    {
      "citation_id": "15",
      "title": "Guide subspace learning for unsupervised domain adaptation",
      "authors": [
        "L Zhang",
        "J Fu",
        "S Wang",
        "D Zhang",
        "Z Dong",
        "C Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "16",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Z Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "17",
      "title": "Adaptive Domain Alignment Neural Networks for Cross-Domain EEG Emotion Recognition",
      "authors": [
        "X Hong",
        "C Du",
        "H He"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "On learning invariant representation for domain adaptation",
      "authors": [
        "H Zhao",
        "R Des Combes",
        "K Zhang",
        "G Gordon"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Machine Learning (ICML)"
    },
    {
      "citation_id": "20",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "21",
      "title": "EmotionMeter: A Multimodal Framework for Recognizing Human Emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "22",
      "title": "A large finer-grained affective computing EEG dataset",
      "authors": [
        "J Chen",
        "X Wang",
        "C Huang",
        "X Hu",
        "X Shen",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "23",
      "title": "Evolutionary Ensemble Learning for EEG-Based Cross-Subject Emotion Recognition",
      "authors": [
        "H Zhang",
        "T Zuo",
        "Z Chen",
        "X Wang",
        "P Sun"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "24",
      "title": "Cross-Subject Cognitive Workload Recognition Based on EEG and Deep Domain Adaptation",
      "authors": [
        "Y Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "25",
      "title": "TFTL: A Task-Free Transfer Learning Strategy for EEG-Based Cross-Subject and Cross-Dataset Motor Imagery BCI",
      "authors": [
        "Y Wang",
        "J Wang",
        "W Wang",
        "J Su",
        "C Bunterngchit",
        "Z.-G Hou"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "26",
      "title": "Band-Level Adaptive Fusion Network for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "Y Wang",
        "L Zhang",
        "Y Zhang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "27",
      "title": "Transfer feature learning with joint distribution adaptation",
      "authors": [
        "M Long",
        "J Wang",
        "G Ding",
        "J Sun",
        "P Yu"
      ],
      "year": "2013",
      "venue": "Proc. IEEE Int. Conf. Computer Vision (ICCV)"
    },
    {
      "citation_id": "28",
      "title": "Domain adaptation techniques for EEG-based emotion recognition: A comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "29",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "30",
      "title": "Reducing the subject variability of EEG signals with adversarial domain generalization",
      "authors": [
        "-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Neural Information Processing (ICONIP)"
    },
    {
      "citation_id": "31",
      "title": "Visual domain adaptation with manifold embedded distribution alignment",
      "authors": [
        "J Wang",
        "W Feng",
        "Y Chen",
        "H Yu",
        "M Huang",
        "P Yu"
      ],
      "year": "2018",
      "venue": "Proc. 26th ACM Multimedia Conf. (ACM MM)"
    },
    {
      "citation_id": "32",
      "title": "Multisource Wasserstein adaptation coding network for EEG emotion recognition",
      "authors": [
        "L Zhu"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "33",
      "title": "Multisource Associate Domain Adaptation for Cross-Subject and Cross-Session EEG Emotion Recognition",
      "authors": [
        "N She",
        "C Zhang",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "34",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "35",
      "title": "MGFKD: A semi-supervised multi-source domain adaptation algorithm for crosssubject EEG emotion recognition",
      "authors": [
        "R Zhang",
        "H Guo",
        "Z Xu",
        "Y Hu",
        "M Chen",
        "L Zhang"
      ],
      "year": "2024",
      "venue": "Brain Research Bulletin"
    },
    {
      "citation_id": "36",
      "title": "Spectral-Spatial Attention Alignment for Multi-Source Domain Adaptation in EEG-Based Emotion Recognition",
      "authors": [
        "Y Yang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Enhancing EEG-Based Cross-Subject Emotion Recognition via Adaptive Source Joint Domain Adaptation",
      "authors": [
        "K Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "40",
      "title": "MEERNet: Multi-source EEGbased emotion recognition network for generalization across subjects and sessions",
      "authors": [
        "H Chen",
        "Z Li",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proc. IEEE 43rd Annu"
    },
    {
      "citation_id": "41",
      "title": "Multi-Scale Hyperbolic Contrastive Learning for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "J Chang",
        "Z Zhang",
        "Y Qian",
        "P Lin"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Contrastive learning of subject invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "Z Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Multisource and multi-representation adaptation for cross-domain electroencephalography emotion recognition",
      "authors": [
        "J Cao",
        "X He",
        "C Yang",
        "S Chen",
        "Z Li",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "44",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "45",
      "title": "Sample estimate of the entropy of a random vector",
      "authors": [
        "L Kozachenko",
        "N Leonenko"
      ],
      "year": "1987",
      "venue": "Problemy Peredachi Informatsii"
    },
    {
      "citation_id": "46",
      "title": "Estimating mutual information",
      "authors": [
        "A Kraskov",
        "H Stögbauer",
        "P Grassberger"
      ],
      "year": "2004",
      "venue": "Physical review E"
    },
    {
      "citation_id": "47",
      "title": "Mutual information between discrete and continuous data sets",
      "authors": [
        "B Ross"
      ],
      "year": "2014",
      "venue": "Plos One"
    }
  ]
}