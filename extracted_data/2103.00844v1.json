{
  "paper_id": "2103.00844v1",
  "title": "Emotion Pattern Detection On Facial Videos Using Functional Statistics",
  "published": "2021-03-01T08:31:08Z",
  "authors": [
    "Rongjiao Ji",
    "Alessandra Micheletti",
    "Natasa Krklec Jerinkic",
    "Zoranka Desnica"
  ],
  "keywords": [
    "functional ANOVA",
    "emotion",
    "expression evolution",
    "action units"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There is an increasing scientific interest in automatically analysing and understanding human behavior, with particular reference to the evolution of facial expressions and the recognition of the corresponding emotions. In this paper we propose a technique based on Functional ANOVA to extract significant patterns of face muscles movements, in order to identify the emotions expressed by actors in recorded videos. We determine if there are time-related differences on expressions among emotional groups by using a functional F-test. Such results are the first step towards the construction of a reliable automatic emotion recognition system 1 Abstract C'è un crescente interesse scientifico nell'analizzare e intepretare automaticamente il comportamento umano, soprattutto rispetto all'evoluzione delle espressioni del volto e al riconoscimento delle corrispondenti emozioni espresse. In questo lavoro proponiamo una tecnica, basata sull'ANOVA Funzionale per estrarre pattern significativi dei movimenti dei muscoli facciali, al fine di identificare le emozioni espresse da alcuni attori in video registrati. In particolare determiniamo se, in istanti specifici, ci siano differenze nell'evoluzione delle espressioni fra diversi gruppi di emozioni, applicando un F-test funzionale. Questi risultati sono il primo passo verso la costruzione di un sistema affidabile per il riconoscimento automatico delle emozioni.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The study of human facial expressions and emotions never stops in our daily life while we communicate with others. Following the increased interest in automatic facial behavior analysis and understanding, the need of a semantic interpretation of the evolution of facial expressions and of human emotions has become of interest in recent years  [4] . In this paper, based on a work cooperated with the Serbian company 3Lateral, which has special expertise on building visual styles and designs in animation movies, we want to explore functional statistical instruments to identify the emotions while analyzing the expressions through recorded videos of human faces. The final aim of this research is to use this information to better and more realistically establish virtual digital characters, able to interact autonomously with real humans.\n\nThe data that we consider are multivariate longitudinal data, showing the evolution in time of different face muscles contraction. Functional Data Analysis (FDA) offers the possibility to analyze the entire expression evolution process over time and to gain detailed and in-depth insight into the analysis of emotion patterns. The basic idea in functional data analysis is that the measured data are noisy observations coming from a smooth function. Ramsay and Silverman  [6]  describe the main features of FDA, that can be used to perform exploratory, confirmatory or predictive data analysis. Ullah and Finch  [7]  published a systematic review on the applications of functional data analysis, where they included all areas where FDA was applied.\n\nIn our application, Functional ANOVA can be used to determine if there are time-related differences between emotion groups by using a functional F-test  [2] . Functional ANOVA yields the possibility to determine if a functional response can be described by scalar or functional variables.\n\nThe structure of this paper goes as follows. In Section 2 we briefly describe the RAVDESS dataset from where the expression data of interest is extracted. Section 3 includes some methods of functional data analysis that we implemented in our application, and in Section 4 our results are presented.\n\n2 The RAVDESS Dataset RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)  [5]  fits our needs for studying the human expression evolution and emotion identification, as it contains 24 professional actors (12 female, 12 male) to offer the performance with good quality and natural behavior under the emotions: calm, happy, sad, angry, fearful, disgusted and surprised. Also a neutral performance is available for each actor. The actors are vocalizing one lexically-matched statement in a neutral North American accent (\"Kids are talking by the door\").\n\nTo avoid being lost in the difference of individual facial appearances, when analyzing the expressions and emotions, researchers mostly focus on the movements of individual facial muscles which are encoded by the Facial Action Coding System (FACS)  [3] . FACS is a common standard to systematically categorize the physical expression of emotions, extracting the geometrical features of the faces and then producing temporal profiles of each facial movement. Such movements, corresponding to contraction of specific muscles of the face, are called action unit (AU). As action units are independent of any interpretation, they can be used for any higherorder decision-making process including recognition of basic emotions. Following the FACS rules, OpenFace  [1] , an open-source software, is capable of recognizing and extracting facial action unit from facial images or videos. We applied Open-Face to extract the engagement degrees of action units for the videos in RAVDESS. The extracted action units include 17 functions for each video, taking values in [0, 5], sampled in about 110 time points (which is also the number of frames in each video) varying around 110.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Functional Statistical Methods",
      "text": "We will represent the action units evolution recorded on each video as a multivariate time series\n\ncontaining a set of D univariate longitudinal functions (D = 17 in our case), each defined on the finite interval [0, T ], 0 < T < +∞. The observation of Y on our sample of videos provides the set Y 1 , . . . , Y n of multivariate curves, that we represent as multivariate functional data.\n\nIt is essential to align the action units functions into a common registered internal timeline that follows the same pronunciation speed, to control the influence of the specific pronounced sentence and to detangle it from the influence of the emotions. Therefore, we need to isolate the phase variability of the action units curves, but keeping, at the same time, the amplitude-phase unchanged to maintain the information of the intensity level of the action units.\n\nThe phase variation is normally represented by a random change of time scale, which is mostly a non-linear transformation. We use the warping functions T i : [0, T ] → [0, T ], i = 1, . . . , n, assuming that they are increasing functions independent of amplitude variation. They map unregistered chronological time t * i to registered internal time t so that T -1 i (t * i ) = t, with E[T i (t)] = t. The observed time-warped curves, represented through a Karhunen-Loeve expansion based on a functional basis\n\nWe used a spline basis and followed the principal components based registration method with a generative process  [9] , whose codes are available in the R package \"registr\"  [8] .\n\nUsing the registered curves representing the AUs evolution in each video, we then investigated if there exist patterns which could discriminate the different emotions, using a Functional ANOVA model. Let y k,g (t) be the evolution of one specific action unit in the video k ∈ {1, . . . , K} (in our case K = 48) for emotion g ∈ {1, . . . , 7}. We can assume that\n\nwhere µ 0 (t) is the grand mean function due to the pronounced sentence and to the actor, independent from all emotions. The term α g (t) is the specific effect on the considered action unit of emotion g, while ε k,g (t) represents the unexplained zero mean variation, specific of the k-th video within emotion group g. To be able to identify them uniquely, we require that they satisfy the constraint\n\nBy grouping the videos representing the same emotion, we can define a 8K × 8 design matrix Z for this model, with suitable 0 and 1 entries, as described in [6, Section 9.2], and rewrite Equation 1 in matrix form: y = Zβ + ε, where β = [µ 0 (t), α 1 (t), . . . , α 7 (t)] T .\n\nTo estimate the parameters we use the functional least squares fitting criterion\n\nsubject to the constraint 0\n\nIn order to investigate which emotions are significantly influencing the change of the action units patterns, for each emotion g and for each action unit we test the null hypothesis H 0 : α g(t) = 0.\n\nSimilarly to the classical univariate ANOVA model, the statistics used to test H 0 is\n\nthat under H 0 has an F distribution with suitable degrees of freedom.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "As mentioned before, we first aligned the curves by separating the amplitude and phase variability. We choose to align the curves by AU25, which represents the lip movement, and then we adjusted the time frames of the other AUs according to this rescaling.\n\nWe then applied the F-test described in the previous section to detect, for each emotion, which AUs have a mean behaviour significantly different from the neutral performance and in which time period during the videos. In Figure  1  we illustrate the results for emotion angry, as an example.  The first row of Figure  1  illustrates the estimated mean µ 0 (t) (neutral emotion) and the angry emotion effects for three action units. The second row displays the observed F-statistics curves together with the pointwise and maximum 95% significance level for the F-distribution in the dashed and horizontal dotted lines respectively. Thus when the observed F-statistics is higher than the critical level lines, the emotion has a significant effect on the AU's pattern. We found in general three main situations of influence of one emotion on expression evolution: 1. locally strengthening (Figure  1d : AU07 in frame range 45 to 55) 2. locally inhibiting (Figure  1f : AU26 in frame range 70 to 90) 3. globally strengthening (Figure  1e : AU10 in almost the whole time). Further, we pointed out the time zones of significant effects of the angry emotion on the action units in Figure  2 , which is beneficial to understand and detect dynamically when and how the facial muscles contractions differ from the baseline.\n\nTable  1  summarizes for each emotion of interest the related action units that show significant changes from the neutral case for our videos dataset. Similarly to the example of angry, we found that for happy and disgust emotions more action units have the globally strengthening effect on a large time range. Sad emotion sometimes affects the action units to be more constant than in neutral case. Emotion Fearful has more influence on upper half face (brows, eye lids and nose), while emotion calm is more related with the center of the face (Cheek Raiser, Lid Tightener and Lip Corner Puller). Surprised emotion is the only emotion where AU45 is significantly influenced.\n\nAs a conclusion, our results can be joined in a multivariate setting and exploited to build a classifier able to automatically recognize the emotions. This task is left to subsequent works.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: we illustrate",
      "page": 4
    },
    {
      "caption": "Figure 1: The functional coefﬁcients of action units 07 (Lid Tightener), 10 (Upper Lip",
      "page": 5
    },
    {
      "caption": "Figure 2: Which and where AU values are affected signiﬁcantly by angry emotion",
      "page": 5
    },
    {
      "caption": "Figure 1: illustrates the estimated mean µ0(t) (neutral emotion)",
      "page": 5
    },
    {
      "caption": "Figure 1: d: AU07 in frame range 45 to 55) 2. locally inhibiting (Figure 1f:",
      "page": 5
    },
    {
      "caption": "Figure 1: e: AU10 in al-",
      "page": 5
    },
    {
      "caption": "Figure 2: , which is beneﬁcial to under-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "understanding human behavior, with particular reference to the evolution of facial"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "expressions and the recognition of\nthe corresponding emotions.\nIn this paper we"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "propose a technique based on Functional ANOVA to extract signiﬁcant patterns of"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "face muscles movements,\nin order to identify the emotions expressed by actors in"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "recorded videos. We determine if there are time-related differences on expressions"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "among emotional groups by using a functional F-test. Such results are the ﬁrst step"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "towards the construction of a reliable automatic emotion recognition system 1"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "Abstract C’`e un crescente interesse scientiﬁco nell’analizzare e intepretare au-"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "tomaticamente il comportamento umano, soprattutto rispetto all’evoluzione delle"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "espressioni del volto e al riconoscimento delle corrispondenti emozioni espresse."
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "In questo lavoro proponiamo una tecnica, basata sull’ANOVA Funzionale per es-"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "trarre pattern signiﬁcativi dei movimenti dei muscoli facciali, al ﬁne di identiﬁcare"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "le emozioni espresse da alcuni attori\nin video registrati. In particolare determini-"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "amo se,\nin istanti speciﬁci, ci siano differenze nell’evoluzione delle espressioni\nfra"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "diversi gruppi di emozioni, applicando un F-test\nfunzionale. Questi risultati sono"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "il primo passo verso la costruzione di un sistema afﬁdabile per il riconoscimento"
        },
        {
          "Abstract There is an increasing scientiﬁc interest\nin automatically analysing and": "automatico delle emozioni."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "1 Introduction"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "The study of human facial expressions and emotions never stops in our daily life"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "while we communicate with others. Following the increased interest\nin automatic"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "facial behavior analysis and understanding, the need of a semantic interpretation of"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "the evolution of facial expressions and of human emotions has become of interest"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "in recent years [4]. In this paper, based on a work cooperated with the Serbian com-"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "pany 3Lateral, which has special expertise on building visual styles and designs in"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "animation movies, we want\nto explore functional statistical\ninstruments to identify"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "the emotions while analyzing the expressions through recorded videos of human"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "faces. The ﬁnal aim of\nthis research is to use this information to better and more"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "realistically establish virtual digital characters, able to interact autonomously with"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "real humans."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "The data that we consider are multivariate longitudinal data, showing the evolu-"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "tion in time of different face muscles contraction. Functional Data Analysis (FDA)"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "offers the possibility to analyze the entire expression evolution process over\ntime"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "and to gain detailed and in-depth insight into the analysis of emotion patterns. The"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "basic idea in functional data analysis is that\nthe measured data are noisy observa-"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "tions coming from a smooth function. Ramsay and Silverman [6] describe the main"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "features of FDA, that can be used to perform exploratory, conﬁrmatory or predictive"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "data analysis. Ullah and Finch [7] published a systematic review on the applications"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "of functional data analysis, where they included all areas where FDA was applied."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "In our application, Functional ANOVA can be used to determine if\nthere are"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "time-related differences between emotion groups by using a functional F-test\n[2]."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "Functional ANOVA yields the possibility to determine if a functional response can"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "be described by scalar or functional variables."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "The structure of this paper goes as follows. In Section 2 we brieﬂy describe the"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "RAVDESS dataset from where the expression data of interest\nis extracted. Section"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "3 includes some methods of\nfunctional data analysis that we implemented in our"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "application, and in Section 4 our results are presented."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "2 The RAVDESS Dataset"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) [5] ﬁts"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "our needs for studying the human expression evolution and emotion identiﬁcation,"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "as it contains 24 professional actors (12 female, 12 male) to offer the performance"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "with good quality and natural behavior under the emotions: calm, happy, sad, angry,"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "fearful, disgusted and surprised. Also a neutral performance is available for each"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "actor. The actors are vocalizing one lexically-matched statement in a neutral North"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "American accent (“Kids are talking by the door”)."
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "To avoid being lost\nin the difference of individual facial appearances, when an-"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "alyzing the expressions and emotions, researchers mostly focus on the movements"
        },
        {
          "2\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "of individual facial muscles which are encoded by the Facial Action Coding System"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "(FACS)\n[3]. FACS is a common standard to systematically categorize the physi-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "cal expression of emotions, extracting the geometrical features of the faces and then"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "producing temporal proﬁles of each facial movement. Such movements, correspond-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "ing to contraction of speciﬁc muscles of\nthe face, are called action unit\n(AU). As"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "action units are independent of any interpretation, they can be used for any higher-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "order decision-making process including recognition of basic emotions. Following"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "the FACS rules, OpenFace [1], an open-source software,\nis capable of recognizing"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "and extracting facial action unit\nfrom facial\nimages or videos. We applied Open-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "Face to extract the engagement degrees of action units for the videos in RAVDESS."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "The extracted action units include 17 functions for each video, taking values in [0, 5],"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "sampled in about 110 time points (which is also the number of frames in each video)"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "varying around 110."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "3 Functional Statistical Methods"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "We will represent the action units evolution recorded on each video as a multivariate"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "time series Y(t) = (Y1(t), . . . ,Yd(t), . . . ,YD(t)),t ∈ [0, T ] containing a set of D uni-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "variate longitudinal functions (D = 17 in our case), each deﬁned on the ﬁnite interval"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "[0, T ], 0 < T < +∞. The observation of Y on our sample of videos provides the set"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "Y1, . . . , Yn of multivariate curves, that we represent as multivariate functional data."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "It is essential to align the action units functions into a common registered internal"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "timeline that follows the same pronunciation speed,\nto control\nthe inﬂuence of the"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "speciﬁc pronounced sentence and to detangle it from the inﬂuence of the emotions."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "Therefore, we need to isolate the phase variability of\nthe action units curves, but"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "keeping, at the same time, the amplitude-phase unchanged to maintain the informa-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "tion of the intensity level of the action units."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "The phase variation is normally represented by a random change of time scale,"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "which is mostly a non-linear\n:\ntransformation. We use the warping functions Ti"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "[0, T ] → [0, T ], i = 1, . . . , n, assuming that they are increasing functions independent"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "of amplitude variation. They map unregistered chronological\ntime t∗\nto registered"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "i"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "(t∗\nso that T −1"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "internal\ni ) = t, with E[Ti(t)] = t. The observed time-warped\ni"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "curves, represented through a Karhunen-Loeve expansion based on a functional ba-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n3": "sis f j,d, are"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 0: and 1 entries, as described in",
      "data": [
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "Let yk,g(t) be the evolution of one speciﬁc action unit in the video k ∈ {1, . . . , K}"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "(in our case K = 48) for emotion g ∈ {1, . . . , 7}. We can assume that"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "(1)\nyk,g(t) = µ0(t) + αg(t) + εk,g(t),"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "where µ0(t) is the grand mean function due to the pronounced sentence and to the"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "actor,\nindependent\nfrom all emotions. The term αg(t) is the speciﬁc effect on the"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "considered action unit of emotion g, while εk,g(t) represents the unexplained zero"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "mean variation, speciﬁc of\nthe k-th video within emotion group g. To be able to"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "7∑\nidentify them uniquely, we require that they satisfy the constraint\nαg(t) = 0, ∀t."
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "g=1"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "By grouping the videos representing the same emotion, we can deﬁne a 8K ×"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "8 design matrix Z for\nthis model, with suitable 0 and 1 entries, as described in"
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "[6, Section 9.2], and rewrite Equation 1 in matrix form: y = Zβ + ε, where β ="
        },
        {
          "4\nRongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka Desnica": "[µ0(t), α1(t), . . . , α7(t)]T ."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "(a) AU07.\n(b) AU10.\n(c) AU26."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "(d) AU07.\n(e) AU10.\n(f) AU26."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "Fig. 1: The functional coefﬁcients of action units 07 (Lid Tightener), 10 (Upper Lip"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "Raiser) and 26 (Jaw Drop) under neutral and angry emotion and the corresponding"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "F-test results"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "Fig. 2: Which and where AU values are affected signiﬁcantly by angry emotion"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "The ﬁrst row of Figure 1 illustrates the estimated mean µ0(t) (neutral emotion)"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "and the angry emotion effects for three action units. The second row displays the"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "observed F-statistics curves together with the pointwise and maximum 95% signif-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "icance level for the F-distribution in the dashed and horizontal dotted lines respec-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "tively. Thus when the observed F-statistics is higher than the critical level lines, the"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "emotion has a signiﬁcant effect on the AU’s pattern. We found in general three main"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "situations of inﬂuence of one emotion on expression evolution: 1.\nlocally strength-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "ening (Figure 1d: AU07 in frame range 45 to 55) 2.\nlocally inhibiting (Figure 1f:"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "AU26 in frame range 70 to 90) 3. globally strengthening (Figure 1e: AU10 in al-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "most\nthe whole time). Further, we pointed out\nthe time zones of signiﬁcant effects"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "of the angry emotion on the action units in Figure 2, which is beneﬁcial\nto under-"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "stand and detect dynamically when and how the facial muscles contractions differ"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "from the baseline."
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "Table 1 summarizes for each emotion of interest the related action units that show"
        },
        {
          "Emotion pattern detection on facial videos using functional statistics\n5": "signiﬁcant changes from the neutral case for our videos dataset. Similarly to the"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "Surprised 06, 09, 10, 12, 14, 15, 17, 23, 25, 26, 45"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "example of angry, we found that for happy and disgust emotions more action units"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "have the globally strengthening effect on a large time range. Sad emotion sometimes"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "affects the action units to be more constant than in neutral case. Emotion Fearful has"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "more inﬂuence on upper half face (brows, eye lids and nose), while emotion calm"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "is more related with the center of\nthe face (Cheek Raiser, Lid Tightener and Lip"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "Corner Puller). Surprised emotion is the only emotion where AU45 is signiﬁcantly"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "inﬂuenced."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "As a conclusion, our results can be joined in a multivariate setting and exploited"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "to build a classiﬁer able to automatically recognize the emotions. This task is left to"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "subsequent works."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "References"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "1. B. Amos, L. Bartosz, and M. Satyanarayanan. Openface: A general-purpose face recognition li-"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "brary with mobile applications. Technical report, CMU-CS-16-118, CMU School of Computer"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "Science, 2016. https://cmusatyalab.github.io/openface/."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "2.\nJ. Dannenmaier, C. Kaltenbach, T K¨olle, and G. Krischak.\nApplication of\nfunctional data"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "analysis to explore movements: walking,\nrunning and jumping-a systematic review. Gait &"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "postureh, pages 182–189, 2020."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "the face reveals: Basic and applied studies of spontaneous expression using\n3. R. Ekman. What"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "the Facial Action Coding System (FACS). Oxford University Press, USA, 1997."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "4. A.J. Fridlund. Human facial expression: An evolutionary view. Academic Press, 2014."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "5.\nS.R. Livingstone and F.A. Russo. The ryerson audio-visual database of emotional speech and"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "english. PloS one, 13(5):e0196391, 2018. https://smartlaboratory.org/ravdess/."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "6.\nJ. Ramsay and B.W. Silverman. Functional data analysis. Springer, 1997."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "7.\nS. Ullah and F. F. Caroline. Applications of functional data analysis: A systematic review. BMC"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "medical research methodology, 2013."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "Journal of Open\n8.\nJ. Wrobel.\nRegister: Registration for exponential\nfamily functional data."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "Source Software, 3(22):557, 2018."
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "9.\nJ. Wrobel, V. Zipunnikov, J. Schrack, and J. Goldsmith. Registration for exponential\nfamily"
        },
        {
          "Disgust\n04, 06, 07, 09, 10, 12, 14, 17, 23, 25, 26": "functional data. Biometrics, 75(1):48–57, 2019."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Openface: A general-purpose face recognition library with mobile applications",
      "authors": [
        "B Amos",
        "L Bartosz",
        "M Satyanarayanan"
      ],
      "year": "2016",
      "venue": "Openface: A general-purpose face recognition library with mobile applications"
    },
    {
      "citation_id": "2",
      "title": "Application of functional data analysis to explore movements: walking, running and jumping-a systematic review",
      "authors": [
        "J Dannenmaier",
        "C Kaltenbach",
        "G Kölle",
        "Krischak"
      ],
      "year": "2020",
      "venue": "Gait & postureh"
    },
    {
      "citation_id": "3",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "4",
      "title": "Human facial expression: An evolutionary view",
      "authors": [
        "A Fridlund"
      ],
      "year": "2014",
      "venue": "Human facial expression: An evolutionary view"
    },
    {
      "citation_id": "5",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "6",
      "title": "Functional data analysis",
      "authors": [
        "J Ramsay",
        "B Silverman"
      ],
      "year": "1997",
      "venue": "Functional data analysis"
    },
    {
      "citation_id": "7",
      "title": "Applications of functional data analysis: A systematic review",
      "authors": [
        "S Ullah",
        "F Caroline"
      ],
      "year": "2013",
      "venue": "BMC medical research methodology"
    },
    {
      "citation_id": "8",
      "title": "Register: Registration for exponential family functional data",
      "authors": [
        "J Wrobel"
      ],
      "year": "2018",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "9",
      "title": "Registration for exponential family functional data",
      "authors": [
        "J Wrobel",
        "V Zipunnikov",
        "J Schrack",
        "J Goldsmith"
      ],
      "year": "2019",
      "venue": "Biometrics"
    }
  ]
}