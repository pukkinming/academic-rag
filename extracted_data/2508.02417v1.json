{
  "paper_id": "2508.02417v1",
  "title": "The Role Of Review Process Failures In Affective State Estimation: An Empirical Investigation Of Deap Dataset",
  "published": "2025-08-04T13:40:25Z",
  "authors": [
    "Nazmun N Khan",
    "Taylor Sweet",
    "Chase A Harvey",
    "Calder Knapp",
    "Dean J. Krusienski",
    "David E Thompson"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The reliability of affective state estimation using EEG data is in question, given the variability in reported performance and the lack of standardized evaluation protocols. To investigate this, we reviewed 101 studies, focusing on the widely used DEAP dataset for emotion recognition. Our analysis revealed widespread methodological issues-including data leakage from improper segmentation, biased feature selection, flawed hyperparameter optimization, neglect of class imbalance, and insufficient methodological reporting. Notably, we found that nearly 87% of the reviewed papers contained one or more of these errors. Moreover, through experimental analysis, we observed that such methodological flaws can inflate the classification accuracy by up to 46%. These findings reveal fundamental gaps in standardized evaluation practices and highlight critical deficiencies in the peer review process for machine learning applications in neuroscience, emphasizing the urgent need for stricter methodological standards and evaluation protocols.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional human-computer interaction could be advanced by the development of affective brain-computer interfaces (aBCIs). These technologies enable the development of adaptive and automated user interfaces, as well as integration into larger systems as specialized subcomponents, enhancing user experience and interaction  (1) . To enhance the generalizability of emotion recognition algorithms across diverse subjects, researchers are increasingly focusing on developing various algorithmic approaches. These efforts include exploring different preprocessing techniques, feature extraction methods, feature selection strategies, and classifier combinations, alongside the integration of machine learning algorithms to optimize performance and accuracy. Machine learning (ML) and artificial intelligence (AI) have become integral components of modern healthcare, transitioning from experimental research to practical deployment. This shift is evident in the increasing number of AI/ML-powered medical devices receiving regulatory approval from the U.S. Food and Drug Administration (FDA)  (2) .\n\nAlthough there have been significant advancements in the field of affective state estimation with the application of machine learning  (3, 4) , we contend that the validity of these approaches is compromised by common errors and pitfalls frequently observed in aBCI experiments. The reliability of aBCI systems depends on the systematic execution of each stage within the machine learning pipeline, which includes data acquisition, noise removal, signal processing, feature extraction and selection, model selection, and performance evaluation. Common factors observed in affective state estimation using EEG that can affect performance include the mixing of training and test datasets, data leakage due to inappropriate feature selection methods, incorrect hyperparameter optimization, classifier selection, and flawed validation process. These methodological errors vary significantly in their impact, with some causing minimal degradation while others severely compromise estimation accuracy.\n\nSegmentation, or windowing, is a widely used practice in EEG analysis, typically employed to increase the effective sample size by dividing long trials into multiple shorter segments  (5) (6) (7) (8) . Segmenting a single trial into multiple segments is not inherently problematic. However, it is crucial to ensure that all segments from the same trial are assigned to the same data partition (training, testing, or validation). Distributing segments from a single trial across multiple partitions introduces data leakage, which can lead to significantly inflated performance estimates and compromise the generalizability of the model  (9, 10) . Another common pitfall is applying data preprocessing using global statistical properties computed from the whole dataset  (11, 12)  . For example, baseline correction, a common preprocessing step in EEG-based machine learning, can inadvertently cause data leakage if not applied correctly. Baseline correction often involves computing the mean or median EEG activity before a stimulus and subtracting it from the entire trial. If this correction is performed before splitting the data into train and test sets, it may introduce dependencies between training and test sets. Commonly utilized techniques in machine learning model development include feature/channel selection, hyperparameter optimization, and the selection of both classifiers and feature sets. However, if these processes are performed using information from the test set or the entire dataset, it can result in data leakage  (11) . Besides that, bias in emotion recognition datasets can lead to performance overestimation in machine learning models  (13) . When certain emotions are overrepresented in the data, the model may favor these emotions. Class imbalance can mislead performance metrics by favoring the majority class  (13) (14) (15) . All these issues, which are present in published studies, lead to overfitting, reduced generalizability, and misleading evaluation metrics that do not accurately reflect the real scenario of aBCI model.\n\nNotably, the methodological challenges discussed in this context are not unique and centered only on aBCI applications  (16) . They are also commonly observed in other machine learning/ artificial intelligence applications, including healthcare, medical diagnostics, neuroimaging, data analysis, computer Security, Internet of Things (IoT) and data mining  (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) . Schroeder et al. systematically compared four crossvalidation strategies to assess their impact on classifier performance estimation in passive BCI experiments  (27) . Kapoor et al. identified issues of data leakage and the reproducibility crisis in machine learning-based research across 17 scientific disciplines  (26) . Similarly, Balendran et al.  (17)  explored the impact and prevalence of various perturbations in machine learning for healthcare, including data imbalance, feature extraction and selection techniques, training strategies, and hyperparameter tuning. These factors significantly influence model robustness, generalization, and performance in medical applications. A recent review provides a comprehensive overview of how machine learning can be intentionally or unintentionally misused in brain imaging  (12) . Other previous studies have explored the impact of data bias on medical diagnostics and healthcare outcomes  (28, 16, 29, 30) . The presence of temporal bias, such as data collected at specific times or specific conditions, in current AI and robotic systems is addressed in  (31) .\n\nDespite the promising advancements in aBCI systems and their high classification performance in controlled research environments, their transition to real-world applications remains challenging. A comprehensive analysis of aBCI research published between 2019 and 2023 revealed that only 4.58% of studies explicitly focused on online emotion classification  (13) . Another survey based on aBCI claimed that 90% of the reviewed research utilized offline classification methods, while only 8% implemented online classification, which are more applicable to real-time scenarios  (32) . This limitation is not exclusive to aBCI but reflects a broader trend in health research. The research and development community in BCI has not sufficiently prioritized the evaluation methods necessary for transitioning these systems into practical applications  (33) . Alarmingly, these pitfalls persist despite being widely recognized, raising concerns about the rigor of the peer-review process  (34) (35) (36) . The persistent approval and publication of studies that overlook these critical issues indicate a lack of diligence among reviewers.\n\nIn this study, we hypothesized that these common pitfalls occur frequently and consistently in the field of aBCI-based classification algorithms and are often inadequately addressed by reviewers during the evaluation process. Moreover, this limitation not only hinders the practical implementation of aBCI technologies but also results in inefficient use of research efforts and funding. Research inefficiency in biomedical fields represents a significant concern, with a study indicating that approximately 85% of biomedical research is largely inefficient and wasted  (37) (38) (39) . Additionally, multiple studies highlighted that these methodological flaws, lack of reproducibility, and limited applicability in real-world settings lead to the inefficient use and misallocation of healthcare research funding, ultimately restricting the contribution to scientific progress  (37, 40, 41) .\n\nTherefore, we aimed to investigate the methodological flaws in affective state estimation as presented in the literature and to provide recommendations for avoiding these issues to enhance their reliability in realworld scenarios. To achieve this, we performed a comprehensive review of 101 emotion recognition papers using EEG, specifically focusing on the DEAP dataset  (42) , which is widely cited publicly available dataset in the field of aBCI. Subsequently, we expanded our investigation to validate the occurrence of observed pitfalls identified in the reviewed papers by conducting experimental analyses. For this purpose, we utilized both the DEAP dataset and data from EEG sensors applied to a watermelon collected in our own laboratory, inspired by the approach in  (43) . As part of this analysis, we also examined the extent of performance inflation caused by these methodological errors. Finally, we compare this performance inflation to a standardized fair analysis. This could be used as a reference for best practices to avoid these pitfalls.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Study Selection",
      "text": "Our systematic literature search, using keywords related to emotion recognition and EEG (e.g.,\" DEAP\", \"EEG emotion classification\"), yielded 686 papers published between 2017 and 2023. Figure  1  illustrates a summarized overview of the selection process. After applying filters, the dataset was refined to 385 publications. At this stage, we identified 146 papers from 2023, 98 from 2022, 51 from 2021, 47 from 2020, 22 from 2019, 10 from 2018, and 11 from 2017. We implemented a two-stage selection process. In the initial stage, we selected 163 candidate papers, including the 30 papers with the highest citations from each year. For years with fewer available papers, we included all publications: 22 from 2019, 10 from 2018, and 11 from 2017. Subsequently, we prioritized full-text availability and citation metrics to select 114 papers for comprehensive review (20 papers from each year between 2023 and 2020, 18 from 2019, 8 from 2018, and 8 from 2017). During the detailed analysis phase, 13 papers were excluded due to their multimodal methodological approaches. Consequently, the final review included 101 papers for our systematic analysis and synthesis. A comprehensive list of all 101 papers is provided in Supplementary Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prevalence Of Methodological Pitfalls From Reviewed Papers",
      "text": "In our systematic review, we identified several prevalent methodological issues across the reviewed papers. Here, our analysis specifically focused on four critical methodological pitfalls that represent the most common sources of data leakage and performance inflation: segmentation strategy, feature/dimension reduction method, hyperparameter selection, and feature/model selection using grid search (Table  1 ). For each category, papers were categorized into one of three groups-valid, invalid, or undetermined-based on the methodological descriptions provided in the paper.\n\nFor both hyperparameter optimization and feature/dimension reduction, papers were marked as invalid when there was reason to believe-from the language in the text-that hyperparameter selection was performed by evaluating model performance on the entire dataset and dimension reduction algorithm was applied on whole dataset prior to train/test splitting. We cannot be certain whether this error actually occurred in all cases due to the lack of detailed reporting. However, the language used often led us, as secondary reviewers, to reasonably claim that the error occurred. Then, some papers mentioned performing hyperparameter search or feature reduction method, but the descriptions were so vague that it was unclear whether the process was conducted in a valid manner. These were marked as undetermined. Papers were marked as valid when there was no indication of methodological error in the text-for example, when reduction techniques were applied only to the training set, or when a dedicated validation set was used for optimization.\n\nIn the case of segmentation, papers were marked as valid when the text, figures, or reporting-such as the order of steps described in the methods section or the structure of performance matrices-indicated that segmentation was performed after train/test splitting. This also included cases where appropriate validation strategies, such as leave-one-trial-out or leave-one-subject-out, were explicitly mentioned. Papers were marked as invalid when segmentation appeared to have been conducted prior to data splitting, based on their descriptions of using k-fold or percentage-based cross-validation on already segmented data. Studies were marked as undetermined when segmentation was mentioned but the available information was insufficient to determine whether proper evaluation procedures were followed. For grid search evaluation, papers were marked as invalid if they reported a table or figure where test set performance biased the feature or model selection process.\n\nFigure  2A  presents the frequency of four common methodological pitfalls identified across 101 reviewed papers. The most frequent issue was observed in segmentation, with 58 papers marked as invalid and an additional 4 as undetermined. Hyperparameter optimization was the second prominent observed error, with 33 papers marked as invalid and 13 as undetermined-indicating that nearly half of the reviewed studies either applied questionable optimization procedures or failed to report them clearly. Feature reduction methods were also a significant concern, with 27 papers marked as invalid and 8 as undetermined. Although feature reduction methods showed 66 studies marked as valid in Figure  2A , it is important to clarify that this high number is largely due to the fact that 64 out of 101 papers did not apply any feature or dimension reduction method at all. Among the 37 papers that did apply such algorithms, only 2 papers implemented them correctly by applying the procedure solely to the training set, and were therefore marked as valid. In contrast, 27 papers applied feature/dimension reduction across the entire dataset, and 8 papers provided insufficient detail to assess their procedure. This highlights a critical issue: among studies that applied feature reduction methods, the vast majority (35 out of 37) performed so improperly or unclearly, a concern masked by the aggregated valid count. Grid search-based feature/model selection showed fewer identifiable issues, though 31 papers still exhibited methodological faults, highlighting that this area is not free from concern. To further examine the potential impact of this error, we analyzed the magnitude of inflation it could introduce. For papers that reported test set performance under multiple configurations, we collected the highest and lowest reported test accuracies-reflecting the selection of models or features based on test set results. The difference in reported test accuracy ranged from 0.24% to 49.7%. While we do not claim that all papers experienced large inflation due to these issues, the fact that accuracy could vary by up to 49% demonstrates that the impact of such methodological flaws cannot be ignored.\n\nFigure  2B  illustrates the distribution of papers based on the number of methodological faults simultaneously present, focusing on four key areas. Our analysis revealed a concerning pattern: 42 papers contained one of these methodological errors, while 33 exhibited two errors. Also, 11 studies demonstrated three concurrent methodological flaws and 2 studies presented with all four, potentially compromising their reported outcomes. Our review identified 13 studies that did not exhibit any of these four methodological errors based on their reported procedures and documentation.\n\nA detailed evaluation of all reviewed papers across the four major methodological pitfalls is provided in Supplementary Table  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Additional Reporting And Reproducibility Concerns",
      "text": "In addition to evaluating methodological procedures, we identified several additional qualities and reporting concerns. We assessed how class imbalance was addressed across the reviewed studies. Among the reviewed studies, only 38 papers reported F1 scores or complementary metrics such as precision, recall, sensitivity, and specificity, thus acknowledging potential class distribution concerns. Details are reported in Supplementary Table  2 . Furthermore, we identified seven studies that reported results with confidence intervals or standard deviation bounds overlapping the theoretical chance level in at least one experimental task. Notably, these results were not acknowledged as statistically indistinguishable from chance in their respective discussions or limitations sections. This raises concerns about the rigor of peer review and highlights that studies with no statistically meaningful outcomes may still be presented-and accepted-as valid contributions.\n\nAnother concern is reporting bias, where studies selectively omit prior models with superior performance in their comparison tables. Of the 31 eligible binary classification studies (based on threshold 5 and participant-dependent analysis), 21 included comparison tables with results from previous publications (See Supplementary Table  3 ). Figure  2C  shows the relationship between the reported performance of each paper's model and the highest accuracy listed in its own comparison table. Valence and arousal accuracies are shown separately, yielding correlations of 0.78 and 0.79 for valence and arousal tasks, respectively. Notably, only 4 out of 21 papers acknowledged literature results that exceeded their own accuracy, suggesting a tendency to underreport stronger prior models. To further quantify this bias, we identified how many higher-performing studies from our review had been published prior to each paper's submission date. Our analysis revealed that thirteen of these papers (62%) had at least three or more previously published models that achieved superior performance but were not mentioned in their comparison tables.\n\nAs part of our review, we also assessed the reproducibility of the reported studies by checking the availability of code or pretrained models. Out of the 101 papers, only eight provided accessible links to their code repositories or shared models. However, most of these links lacked sufficient content to fully replicate the reported experiments. An additional three papers stated that their code would be made available but did not include a link or any accessible resource at the time of our review.\n\nA detailed record of all reviewed papers, including their evaluations based on each methodological criterion, is provided in the Supplementary File.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Analysis: Segmentation",
      "text": "In this experiment, we performed binary classification on the valence axis of the DEAP dataset, after segmenting each trial into multiple segments. Additionally, binary classification was conducted on the watermelon dataset with randomly generated binary labels. Figure  3  shows how different validation strategies impact classification performance when segmentation or windowing is applied, using the DEAP dataset and watermelon dataset. The graph highlights a notable difference between two validation methods: k-fold cross-validation applied to all segmented trials, and leave-one-trial-out, which is applied to each original trial with all segmented trials in one-fold, as the number of segments per trial increases. In the Figure  3A , the k-fold cross-validation method shows a significant improvement in performance, increasing from about 53% accuracy without segmentation to over 90% with just 6 segments per trial, and nearly 100% with 60 segments. On the other hand, the leave-one-trial-out validation method remains relatively consistent at around 52% accuracy, regardless of the segmentation level. A comparable phenomenon was also observed in the binary classification experiments using the watermelon dataset, where similar patterns of performance inflation occurred despite the absence of meaningful neurophysiological signals (Fig.  3B ). These results conclusively demonstrate that conventional k-fold cross-validation after segmentation can cause inflated performance estimates.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Analysis: Feature/Dimension Reduction Method",
      "text": "To assess the performance inflation caused by improperly implemented feature reduction procedures, we conducted an experimental comparison of valid and invalid reduction strategies using the DEAP and Watermelon datasets (Table  2 ). To perform binary classification, we tested two validation approaches. The Global Selection approach-where feature selection is applied on the whole dataset-consistently yielded inflated accuracy compared to the Local Selection approach which exclusively used training data for feature selection. Two performance metrics are reported: regular accuracy, which does not account for class imbalance, and balanced accuracy, which accounts for class bias. For the DEAP dataset in the valence axis, Global Selection inflated conventional accuracy by 15.2 percentage points (77.4% versus 62.2%) and balanced accuracy by 15.9 percentage points (75.3% versus 59.4%). Similar patterns were observed for arousal (inflation of 14.2 and 16.2 percentage points) and dominance (inflation of 15.9 and 18.1 percentage points) dimensions. Notably, the watermelon dataset-containing no neurophysiological signalsexhibited a 17-percentage point inflation in regular accuracy when using the Global Selection methodology. Since this dataset contains balanced class labels, regular accuracy and balanced accuracy are almost equivalent by definition; thus, only regular accuracy is reported. These findings quantitatively demonstrate that improper implementation of feature reduction algorithms-particularly the use of test-set data during feature ranking-can substantially overestimate classification performance, with inflation magnitudes ranging from approximately 14-18 percentage points across all conditions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Analysis: Hyperparameter Optimization",
      "text": "In our hyperparameter optimization experiments, we observed consistent performance disparities between compromised and methodologically correct approaches (Table  3 ). Two experimental approaches were employed: (i) hyperparameter optimization applied to both the training and test sets, and (ii) optimization restricted to the training set only. In the valence axis, the regular accuracy increased by 21.9 percentage points when hyperparameter tuning was improperly performed using test data (84.4% vs. 62.5%). Similar inflations were observed for arousal (21.6%) and dominance (21.4%). The balanced accuracy metric, which compensates for class imbalance, also showed substantial inflation: 24.2, 25.2, and 24.9 percentage points for valence, arousal, and dominance, respectively. Most strikingly, our watermelon dataset showed an increase in accuracy from 50.5% to 77.6%-a 27.1 percentage point inflation-when hyperparameters were improperly optimized using test data. These results highlight that improper inclusion of the test set in hyperparameter optimization leads to an overestimation of performance, with inflation magnitudes ranging from approximately 21 to 27 percentage points across all evaluated conditions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "In this study, we investigated the prevalent methodological errors commonly encountered in aBCI experiments by analyzing the most highly cited publications utilizing the DEAP dataset from 2017 to 2023. Our findings indicate that while many studies report high-performance results in EEG-based emotion estimation, a significant portion of these results may be attributed to the methodological pitfalls identified in DEAP-related research. The high prevalence of data leakage during segmentation (58 publications) highlights a fundamental misunderstanding of proper validation protocols. Similarly, the misapplication of feature reduction algorithm (27 publications) and hyperparameter optimization (33 publications) across entire datasets rather than restricting them to the training phase indicates a critical methodological gap. Notably, almost 87% of the reviewed papers exhibited at least one or more of these four pitfalls. In addition to these explicit errors, we identified a substantial number of papers where the methodological description was insufficient to assess validity -specifically, 4 in segmentation, 8 in feature reduction, and 13 in hyperparameter optimization. Many of these studies failed to document critical methodological details, such as thresholding criteria, data partitioning strategies, or whether analyses were subject-dependent. Although these cases were not labeled as definitively invalid, these undetermined categories also reflect a broader concern with inadequate transparency. Vague or incomplete methodological descriptions obscure potential errors that should be identified during the peer review process. This concern is further underscored by additional reporting issues we observed, including limited reporting of class-imbalance metrics, failure to acknowledge results statistically indistinguishable from chance, and a lack of acknowledgment of prior models that achieved higher accuracy. We found that even within the limited scope of this review, authors seem to be selectively choosing papers with lower performance than their own. Given that the primary contribution of these papers is often the model performance itself, failing to cite models with higher performance is a major issue. Together, these findings highlight not only pervasive implementation errors, but also a lack of reporting rigor and review scrutiny.\n\nOur experimental analysis demonstrated that high classification accuracy on DEAP dataset can be achieved when single-trial segmentation, especially when train-test data leakage occurs due to improper validation strategies. The gap between the two validation methods-about 45 percentage points at the highest segmentation-shows how improper validation can result in misleading high-performance metrics in affective BCI research. Moreover, hyperparameter optimization or feature reduction errors in individual can inflate performance estimates by approximately 14-25%. More concerning is that multiple methodological errors can occur simultaneously, which could significantly inflate the classification accuracy. The inflated results from the watermelon dataset confirm that improper application of segmentation, dimensionality reduction, or hyperparameter optimization can substantially inflate reported model performance-even in the absence of meaningful neural signals. In this experimental study, we conducted an in-depth analysis of the segmentation, feature/dimension reduction method, and hyperparameter search; however, the effects of bias and data imbalance were not extensively examined. For in-depth observation, we refer to our previous analysis which demonstrated that neglecting class imbalance can lead to misleading results  (15) .\n\nOur findings also highlight that a lack of expertise in machine learning can lead to overestimation, raising concerns about the reliability of current aBCI research. Notably, our review was conducted solely based on the documented methodologies and reported procedures in the publications. While only 8 out of 101 papers provided accessible code repositories, we examined available code when provided to verify methodological descriptions. However, the majority of our assessment relied on the information reported in the publications themselves, without access to training models or implementation details. The identification of these pitfalls through careful examination of published materials indicates that these issues should be detectable during peer review. Our ability to identify these methodological flaws using only the information available in publications suggests the potential gaps in the rigor of the review process, where critical methodological details may not receive sufficient scrutiny. This finding underscores the need for more thorough methodological assessment during peer review to ensure research validity and reproducibility.\n\nTherefore, we propose some critical recommendations to mitigate the pitfalls observed in our review. A primary concern is preventing data leakage between training and testing datasets, as such contamination invalidates the reported performance. To ensure a clear separation, all methodological procedures must remain independent of test data  (26) . All methodological decisions-including preprocessing techniques, feature reduction approaches, and hyperparameter optimization-should be executed exclusively on training data, without incorporating information from the test set  (11, 14) . Decisions regarding classifier selection and feature type, and frequency bands, should derive solely from training data performance rather than prioritizing combinations that maximize test set outcomes. Choosing a suitable validation strategy is essential, especially for windowing/segmentation. Leave-one-trial-out or leave-one-subject-out are more appropriate. Our experimental analysis also demonstrates the correct application of these approaches, serving as a guideline to avoid common pitfalls. The code is available upon request. Additionally, appropriate performance metric selection is essential for addressing class imbalance issues; conventional metrics such as F1 score, precision, and sensitivity may prove inadequate when calculated predominantly on majority class samples. Balanced accuracy represents a better alternative metric due to its inherent insensitivity to class imbalance  (15) . To ensure the model robustness, it should be evaluated on multiple datasets and through online testing. Researchers must ensure that the test set is unbiased and representative of the target distribution. Furthermore, insufficient methodological details hinder model reproducibility. All model parameters, random seeds, and relevant information should be clearly reported. Additionally, all code, training models, and datasets should be accessible to all. Finally, ensuring that reviewers have comprehensive knowledge across relevant domains is crucial. Insufficient knowledge in any single aspect of BCIs or machine learning can lead to deficiencies in experimental design, statistical analyses, or result interpretation. Adding reviewers from multidisciplinary field might be helpful to identify methodological shortcomings in both fields. Additionally, reviewers should critically evaluate model reproducibility and availability during the review process.\n\nOur review methodology has certain limitations. We utilized only the Scopus database for our literature search, potentially limiting the scope of included studies from multi-database approaches. The keyword search specifically targeted studies on emotion recognition using the DEAP dataset, selected due to its high citation frequency in the field. These limitations may have led to the exclusion of broader developments in the field. However, we believe that using a single dataset allows for a more consistent and comparable evaluation of methodological errors. Future reviews would benefit from incorporating multiple databases, expanding the study period, and including studies utilizing diverse emotional datasets to provide more comprehensive insights. Determining whether a method within a paper committed a methodological error based solely on the text is a difficult task, and inherently asks the reviewer to make many judgement calls.\n\nTo reduce bias, multiple reviewers assessed each paper and resolved discrepancies through discussion. While individual assessments may not be flawless, we believe this approach provides a valid evaluation of the literature and supports the reliability of our aggregate findings.\n\nIn conclusion, our comprehensive analysis of the affective BCI literature reveals widespread methodological shortcomings that significantly impact the field's scientific rigor. By identifying and replicating these issues, we emphasize the necessity for stricter validation strategies, transparent reporting, and unbiased evaluation metrics. Our findings are not merely a critique but a call to the scientific community to uphold higher standards in experimental design, peer review, and result interpretation. Failure to do so may compromise the integrity of the findings and continue to perpetuate biased and misleading studies. Addressing these concerns will enhance the reliability of aBCI research, fostering reproducible and impactful advancements in the field.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Review Paper Strategy",
      "text": "Search strategy and selection criteria: In this study, we used the Scopus database to identify relevant studies for our review. Our search criteria were designed to be selective, focusing on core research papers central to our topic as of February 29, 2024. We specifically targeted affective state estimation studies that used the DEAP dataset, as DEAP is one of the most widely cited databases in emotion recognition  (44) . Therefore, we focused on \"DEAP\" as a keyword in our search. Using keyword combinations such as \"EEG,\" \"emotion,\" \"recognition,\" \"using,\" and \"DEAP,\" we conducted an initial search (Fig.  1 ). We limited the publication years to 2017 through 2023. Afterward, we applied filters to restrict the language to \"English,\" document type to \"article,\" and source type to \"journal\". These filtering criteria were implemented to focus solely on journal publications, which typically provide more comprehensive methodological details and novel applications compared to other publication formats. The collected publications were then organized for each year, by ranking them from highest to lowest citations. To examine the methodological developments over time, we initially aimed to select the 30 most highly cited papers from each year between 2023 and 2020, while including all papers from 2019, 2018, and 2017 due to their limited numbers. Then, we attempted to select an equal number of 20 papers from each year by considering the full article availability and citation metrics. Finally, we manually excluded the studies that applied multimodal emotion recognition to restrict the focus to EEG-based affective state estimation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Review Process And Data Extraction:",
      "text": "The paper review process was conducted in four stages to ensure rigor and transparency. In the first stage, the initial paper search and selection were performed by one author based on the criteria mentioned above. In the second stage, each selected paper was independently assessed by two authors to evaluate its methodological quality and pitfalls. In the third stage, any discrepancies or conflicts in the assessments were resolved through group discussions involving all authors. Finally, in the fourth stage, the reported outcomes were systematically re-evaluated by all authors to ensure clarity and consistency.\n\nMethodological quality assessment criteria: Based on the extracted information from the revision, the papers were then categorized based on four common methodological pitfalls: data leakage due to segmentation, data leakage in hyperparameter optimization, data leakage in feature/dimension reduction methods, and feature/model selection using grid search. The defined concepts for each of these issues are detailed in Table  1 . For each identified methodological pitfall, papers were systematically classified into three categories based on the clarity and appropriateness of their reported procedures:\n\n▪ Valid: Papers that clearly described proper methodological procedures, with explicit indications of appropriate data partitioning and no indication of data leakage or improper practices. ▪ Invalid: Papers where the described methodology indicated violations of best practices, data leakage or procedures that would compromise model generalizability. ▪ Undetermined: Papers that mentioned relevant procedures but provided insufficient detail to determine whether the approach followed a valid evaluation process, often due to vague or incomplete reporting.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Additional Reporting Quality Indicators:",
      "text": "We evaluated additional quality indicators including: (1) class imbalance acknowledgment, (2) statistical significance relative to chance performance, and (3) code availability for reproducibility.\n\nFor class imbalance assessment, papers were classified as \"reported\" if they included any of the following metrics that account for class distribution: F1 scores, precision, recall, sensitivity, or specificity. Papers reporting only accuracy or similar aggregate metrics were marked as \"not reported\". For statistical significance evaluation, we identified studies reporting confidence intervals or standard deviation bounds and determined whether these overlapped with theoretical chance levels. Theoretical chance levels were calculated based on the number of classes in each classification task (50% for binary classification, 25% for four-class classification). Studies were flagged when the chance classification level was within two of the reported standard deviations, indicating results potentially indistinguishable from random performance. For code availability assessment, papers were marked as \"available\" only if they provided accessible links to code repositories or pretrained models. Studies that mentioned code availability but lacked any accessible links or provided only pseudocode or implementation summaries within the paper were not counted.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Literature Comparison Bias Analysis:",
      "text": "To assess selective reporting bias, we investigated the extent to which published studies accurately report prior models with superior performance. To ensure fair comparison, we restricted our analysis to papers that performed participant-dependent binary classification using a threshold of 5 on the valence or arousal dimension-yielding a subset of 31 papers from our original collection. Of these, 21 papers included explicit comparison tables listing prior models. For each of these 21 papers, we extracted: (1) the reported accuracy of each paper's proposed model, and (2) the highest accuracy reported in their comparison table from prior studies. Pearson correlation coefficients were computed to assess the relationship between proposed and literature-reported accuracies for valence and arousal tasks separately.\n\nTo further quantify underreporting, we cross-referenced our review database to identify how many highperforming models had been published prior to each paper's submission date.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Dataset Deap",
      "text": "The DEAP dataset(  42 ) is a widely utilized benchmark for emotion recognition using EEG signals. It consists of EEG recordings from 32 healthy participants (50% female, aged 19-37; mean age: 26.9 ± 4.45) while they watched 40 music videos designed to elicit emotional responses. The database provides two datasets: raw data and preprocessed data. The raw data includes 32 BioSemi .bdf files, each containing 48 recorded channels: 32 EEG channels, 12 peripheral channels (including electrooculography (EOG), electromyography (EMG), galvanic skin response (GSR), and respiration), three unused channels, and one status channel. The EEG signals were originally recorded at a sampling rate of 512 Hz. To facilitate analysis, a preprocessed version of the dataset was provided, incorporating several modifications. The data was downsampled to 128 Hz, EOG artifacts were removed, and a 4-45 Hz band-pass filter was applied to retain relevant EEG frequency components. Additionally, the signals were re-referenced using a common average reference, and the data was segmented into 60-second trials, with a 3-second pre-trial baseline.\n\nEach participant has 40 trials. Following each video stimulus, participants provided self-assessment ratings on a nine-point scale for four emotional dimensions: valence, arousal, dominance, and liking. Emotional states in the DEAP are often interpreted using Russell's circumplex model, where valence and arousal define a two-dimensional emotional space.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Watermelon Dataset",
      "text": "The EEG collection method using watermelon was previously referred to as phantom EEG in earlier studies  (43, 45, 46) . The watermelon EEG data focuses solely on temporal correlation features without capturing any stimulus-driven neural responses  (43) . Here, watermelon EEG data acquisition was performed using the Cognionics Mobile-72 EEG system, a high-density mobile EEG device with 64 channels (Fig.  4A ). We collected EEG data from the watermelon for more than one hour to ensure sufficient data for the classification task. The recordings were conducted at a sampling frequency of 500 Hz. The EEG cap, equipped with standard active Ag/AgCl electrodes, was positioned according to the international 10-20 system, a widely recognized standard in EEG studies. Reference and ground electrodes were placed on either side of the watermelon to mimic standard mastoid placements and ensure signal quality. The watermelon EEG data was then reorganized to follow the same structure as the DEAP dataset. From the first one-hour recording, 40 trials were extracted, each lasting 60 seconds. A 30-second interval was maintained between consecutive trials to avoid unwanted influences between trials. These 40 trials were then randomly assigned binary labels, either 1 or 0, for the final classification task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Procedure Of Experimental Analysis",
      "text": "In a typical EEG-based emotion recognition classification framework, the process generally follows a standardized pipeline that includes data preprocessing, feature extraction, feature selection, train-test partitioning, classifier selection, and performance evaluation using metrics such as accuracy and F1-score.\n\nIn this study, we implemented all these steps; however, to systematically assess the significance and impact of specific methodological pitfalls, we prioritized one pitfall per experiment. Accordingly, we conducted separate experiments focusing on segmentation, feature selection, and hyperparameter optimization. In all cases, the same experimental protocol was applied consistently to both the DEAP and watermelon datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Segmentation Analysis",
      "text": "To investigate the improper use of EEG trial segmentation, we performed subject-dependent binary classification using the raw DEAP dataset. Each trial in the DEAP dataset originally lasted 60 seconds (Fig.  4B ). To increase the sample size, we segmented each trial into t-second intervals, resulting in a total of N= 60/t segments per trial. For instance, segmenting into 1-second intervals resulted in a total of 2400 trials (40 trials × 60 seconds). By varying the segment length (t), we assessed the effect of segmentation on classification performance. A threshold of 5 was applied, where ratings less than or equal to 5 (≤5) were considered low states, and ratings greater than 5 (>5) were considered high states. For each emotional axis, there are two classes: high valence vs. low valence, high arousal vs. low arousal, and high dominance vs. low dominance. In emotion recognition studies, EEG data are often analyzed across five frequency bandsdelta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-30 Hz), and gamma  (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) . The power spectral density (PSD), one of the most used features in aBCI classification, was computed using MATLAB builtin 'pwelch' function for those frequency bands as the feature for classification.\n\nVarious classification algorithms, including decision trees, logistic regression, Naive Bayes, k-nearest neighbors (kNN), support vector machine (SVM), CNN, and neural networks (NN) have been employed for binary classification in prior studies. In our study, we selected kNN as the primary classifier for categorizing binary emotional states across all three dimensions. The kNN classifier was implemented using MATLAB's built-in 'fitcknn' function with default parameters, with the number of nearest neighbors (k) specifically set to 5. To rigorously evaluate the classifier's performance, we performed two distinct validation methodologies: k-fold cross-validation and leave-one-trial-out validation. For the former approach, 5-fold cross-validation was applied to all 2400 samples from each individual subject. In the latter approach, we implemented a leave-one-trial-out validation method across the original 40 trials, ensuring that all segmented samples derived from a single trial remained within the same fold. This methodological choice was crucial to prevent potential data leakage between training and testing sets that could occur due to segmentation. Specifically, the training data comprised segmented samples from 39 trials, while the testing data was exclusively derived from the remaining trial. This process was iteratively repeated until each trial had served as a test set. Here, a well-known performance metric, balanced accuracy, was also applied to evaluate the performance of both validation methods. Balanced accuracy, which accounts for the potential class imbalance in the emotional state classification task, is calculated by averaging the individual class accuracy for all classes  (15) . The same preprocessing protocols, feature extraction techniques, and classification methodology were subsequently applied to the watermelon dataset for comparative analysis.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Feature Selection Analysis",
      "text": "Here, we employed the DEAP preprocessed dataset which is also widely used for affective state estimation research. We analyzed all trials of 60-second experimental signals after removing the 3-second baseline, or and applied the same thresholding with a value of 5 to divide the 40 trials into binary classes. As the DEAP preprocessed data was already filtered through a 4-45 Hz band-pass filter, we calculated the PSD across four frequency bands: theta (4-8 Hz), alpha (8-14 Hz), beta (14-30 Hz), and gamma  (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) . For feature selection, we implemented a statistical t-test using MATLAB's built-in \"rankfeatures\" function to identify the most relevant features while reducing redundancy. Classification was performed using the kNN classifier with default parameters. However, we conducted subject-dependent experiments using 70% of the trials as the train set and 30% of the trials as the test set. We systematically performed two feature selection approaches. Two feature selection strategies were compared: (1) Global Selection, where t-test feature selection was applied to the entire dataset before splitting into train and test sets-a common but flawed practice that can introduce data leakage by allowing test data to influence feature selection, potentially inflating performance metrics; and (2) Local Selection, where the training set underwent 5-fold cross-validation. In each cross-validation fold, t-test feature selection was applied using only the training data (i.e., 4 out of 5 folds), ensuring that the validation fold remained completely unseen during feature selection. The selected top-ranked features were then used to train a kNN classifier on the training data, and the model was evaluated on the remaining 1-fold validation set. This was repeated for all folds and across a range of feature counts. The average classification accuracy across the five folds was computed for each feature count, and the number of features yielding the best performance was selected as optimal. After determining the optimal count, the final feature set was re-ranked using the full training set and applied to the independent test set for final evaluation. The latter approach maintains strict separation between training and test data, preventing information leakage and yielding more realistic performance estimates despite potentially introducing variability in selected features across folds. The effectiveness of both approaches was assessed using both accuracy and balanced accuracy as mentioned above.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Hyperparameter Optimization",
      "text": "In this analysis, we also employed the preprocessed DEAP dataset, maintained identical pre-processing and feature extraction methodologies as outlined in the preceding section. We also applied the same validation approach, a subject-dependent experiments using a 70/30 train/test split. To investigate potential data leakage during hyperparameter optimization, we implemented two distinct approaches with a kNN classifier. In the first approach, we repeatedly adjusted the classifier's hyperparameters (number of neighbors, standardization setting, and distance metric), selecting the combination that yielded the highest classification accuracy on the test data. This method, while yielding optimal performance metrics, risks overfitting to the test set. In contrast, the second strategy followed proper machine learning practice by isolating the test set entirely during the tuning process. We employed a 5-fold cross-validation procedure within the training set to optimize the combination of hyperparameters, including the number of neighbors, standardization (enabled or disabled), and type of distance metric. For each hyperparameter setting, the training data was partitioned into five folds. In each fold, the model was trained on 4 folds and validated on the remaining one. The balanced accuracy was computed on the validation fold and averaged across all five folds. The combination of hyperparameters that achieved the highest average balanced accuracy was selected as optimal. The final model was then retrained on the full training set using these optimal settings and subsequently evaluated on the untouched test set. This approach ensured that no test data was used during model tuning, thus avoiding data leakage and providing a more reliable estimate of generalization performance. Both accuracy and balanced accuracy were chosen as the performance metrics to compare both approaches, providing insights into how optimization methodology impacts classification results.  A 1-second segmentation was applied to each 60-second trial, resulting in 2400 segments from 40 trials. A k-fold cross-validation was then performed on these 2400 segments, revealing the presence of samples from the same trial in both the training and testing sets  (47) .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Figures And Tables",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Bias In Hyperparameter Tuning",
      "text": "Hyperparameter optimization based on accuracy of both train and test sets.\n\nSelected configuration and settings of the classifier based on highest classifier performance  (48) .\n\nData leakage due to feature/dimension reduction method Dimensionality or feature reduction algorithm was performed using the entire dataset before splitting into training and test sets, which can lead to data leakage and inflated performance estimates.\n\nDimension reduction applied on whole dataset before train/test set partition  (49) .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Feature/Model Selection Using Grid Search",
      "text": "Feature or model selection was conducted using grid search procedures where the selection criteria were based on performance metrics evaluated on the test set.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates",
      "page": 3
    },
    {
      "caption": "Figure 2: A presents the frequency of four common methodological pitfalls identified across 101 reviewed",
      "page": 4
    },
    {
      "caption": "Figure 2: A, it is important to clarify that",
      "page": 4
    },
    {
      "caption": "Figure 2: B illustrates the distribution of papers based on the number of methodological faults simultaneously",
      "page": 5
    },
    {
      "caption": "Figure 2: C shows the relationship between the reported performance of each",
      "page": 5
    },
    {
      "caption": "Figure 3: shows how different validation",
      "page": 6
    },
    {
      "caption": "Figure 3: A, the k-fold cross-validation method shows a significant improvement in performance, increasing",
      "page": 6
    },
    {
      "caption": "Figure 1: ). We limited the",
      "page": 9
    },
    {
      "caption": "Figure 4: A). We collected EEG data from the watermelon for more than one hour to ensure sufficient data for the",
      "page": 11
    },
    {
      "caption": "Figure 4: B). To increase the sample size, we segmented each trial into t-second intervals, resulting in a total of N=",
      "page": 11
    },
    {
      "caption": "Figure 1: Systematic review selection process for EEG-based emotion recognition studies using the DEAP",
      "page": 14
    },
    {
      "caption": "Figure 2: Prevalence and distribution of methodological errors in EEG-based emotion recognition studies. (A)",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B \nA": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A \nB \nOriginal EEG \ntrial (60s) \nSegmenting \nusing 1-second \nwindow \nFig.4. EEG data collection, and segmentation. (A) Demonstration of EEG data collection using a watermelon \nphantom. (B) EEG trial segmentation process, illustrating a 60-second EEG trial divided into 1-second non-\noverlapping windows, resulting in 60 segments per trial. This approach is commonly used to augment sample \nsize but may introduce data leakage if segments from the same trial are distributed across training and test sets. \nSegments are illustrative and not shown to scale.": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 1: Definitions and examples of common methodological pitfalls identified in EEG-based",
      "data": [
        {
          "Pitfalls": "Data leakage due to \nsegmentation",
          "Definitions": "Presence of samples which are \nderived from the same trial after \nsegmentation or windowing, in both \nthe training and testing sets.",
          "Examples": "A 1-second segmentation was applied \nto each 60-second trial, resulting in \n2400 segments from 40 trials. A k-fold \ncross-validation was then performed on \nthese 2400 segments, revealing the \npresence of samples from the same \ntrial in both the training and testing \nsets(47)."
        },
        {
          "Pitfalls": "Bias in \nhyperparameter \ntuning",
          "Definitions": "Hyperparameter optimization based \non accuracy of both train and test sets.",
          "Examples": "Selected configuration and settings of \nthe classifier based on highest \nclassifier performance(48)."
        },
        {
          "Pitfalls": "Data leakage due to \nfeature/dimension \nreduction method",
          "Definitions": "Dimensionality or feature reduction \nalgorithm was performed using the \nentire dataset before splitting into \ntraining and test sets, which can lead \nto data leakage and inflated \nperformance estimates.",
          "Examples": "Dimension reduction applied on whole \ndataset before train/test set \npartition(49)."
        },
        {
          "Pitfalls": "Feature/model \nselection using grid \nsearch",
          "Definitions": "Feature or model selection was \nconducted using grid search \nprocedures where the selection \ncriteria were based on performance \nmetrics evaluated on the test set.",
          "Examples": "1.  Selected the best electrodes based \non highest performance accuracy(48). \n2. Selected classifier based on highest \ntest set accuracy(49)."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 1: Definitions and examples of common methodological pitfalls identified in EEG-based",
      "data": [
        {
          "Regular accuracy": "Global",
          "Balanced accuracy": "Global"
        },
        {
          "Regular accuracy": "77.41",
          "Balanced accuracy": "75.31"
        },
        {
          "Regular accuracy": "76.77",
          "Balanced accuracy": "70.39"
        },
        {
          "Regular accuracy": "77.17",
          "Balanced accuracy": "71.63"
        },
        {
          "Regular accuracy": "64.00",
          "Balanced accuracy": "--"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 3: Comparison of Hyperparameter Optimization Strategies in Binary Classification.",
      "data": [
        {
          "Regular \naccuracy": "Wrong",
          "Balanced \naccuracy": "Correct  Wrong"
        },
        {
          "Regular \naccuracy": "84.42",
          "Balanced \naccuracy": "83.16"
        },
        {
          "Regular \naccuracy": "82.26",
          "Balanced \naccuracy": "78.95"
        },
        {
          "Regular \naccuracy": "82.96",
          "Balanced \naccuracy": "78.01"
        },
        {
          "Regular \naccuracy": "77.66",
          "Balanced \naccuracy": "--"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "N Ahmed",
        "Z Aghbari",
        "S Girija"
      ],
      "year": "2023",
      "venue": "Intell. Syst. Appl"
    },
    {
      "citation_id": "2",
      "title": "Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices",
      "authors": [
        "R Health"
      ],
      "year": "2024",
      "venue": "Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices"
    },
    {
      "citation_id": "3",
      "title": "Review on Emotion Recognition Based on Electroencephalography. Front. Comput. Neurosci",
      "authors": [
        "H Liu",
        "Y Zhang",
        "Y Li",
        "X Kong"
      ],
      "year": "2021",
      "venue": "Review on Emotion Recognition Based on Electroencephalography. Front. Comput. Neurosci"
    },
    {
      "citation_id": "4",
      "title": "Deep learning for electroencephalogram (EEG) classification tasks: a review",
      "authors": [
        "A Craik",
        "Y He",
        "J Contreras-Vidal"
      ],
      "year": "2019",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "5",
      "title": "Two-dimensional CNN-based distinction of human emotions from EEG channels selected by multi-objective evolutionary algorithm",
      "authors": [
        "L Moctezuma",
        "T Abe",
        "M Molinas"
      ],
      "year": "2022",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "6",
      "title": "Affective EEG-Based Person Identification Using the Deep Learning Approach",
      "authors": [
        "T Wilaiprasitporn",
        "A Ditthapron",
        "K Matchaparn",
        "T Tongbuasirilai",
        "N Banluesombatkul",
        "E Chuangsuwanich"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "7",
      "title": "Interpretable Emotion Recognition Using EEG Signals",
      "authors": [
        "C Qing",
        "R Qiao",
        "X Xu",
        "Y Cheng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Investigating Patterns for Self-Induced Emotion Recognition from EEG Signals",
      "authors": [
        "N Zhuang",
        "Y Zeng",
        "K Yang",
        "C Zhang",
        "L Tong",
        "B Yan"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "9",
      "title": "Mini review: Challenges in EEG emotion recognition",
      "authors": [
        "Z Zhang",
        "J Fort",
        "L Mateu"
      ],
      "year": "2024",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "10",
      "title": "The Impact of Cross-Validation Schemes for EEG-Based Auditory Attention Detection with Deep Neural Networks",
      "authors": [
        "G Ivucic",
        "S Pahuja",
        "F Putze",
        "S Cai",
        "H Li",
        "T Schultz"
      ],
      "year": "2024",
      "venue": "2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "11",
      "title": "A Comparative Study on the Impacts of Data Leakage During Feature Selection using the CIC-IoT 2023 Intrusion Detection Dataset",
      "authors": [
        "S Mallampati",
        "S Hari"
      ],
      "year": "2024",
      "venue": "2024 10th International Conference on Electrical Energy Systems (ICEES)"
    },
    {
      "citation_id": "12",
      "title": "Introduction to machine learning for brain imaging",
      "authors": [
        "S Lemm",
        "B Blankertz",
        "T Dickhaus",
        "K.-R Müller"
      ],
      "year": "2011",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "13",
      "title": "Toward the Construction of Affective Brain-Computer Interface: A Systematic Review",
      "authors": [
        "H Chen",
        "J Li",
        "H He",
        "J Zhu",
        "S Sun",
        "X Li",
        "B Hu"
      ],
      "year": "2025",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "14",
      "title": "Machine Learning Methods for BCI: challenges, pitfalls and promises",
      "authors": [
        "J Riascos",
        "M Molinas",
        "F Lotte"
      ],
      "year": "2024",
      "venue": "ESANN 2024 Proceesdings (Ciaco -i6doc.com, Bruges (Belgium) and online"
    },
    {
      "citation_id": "15",
      "title": "Affective brain-computer interfaces: Choosing a meaningful performance measuring metric",
      "authors": [
        "M Mowla",
        "R Cano",
        "K Dhuyvetter",
        "D Thompson"
      ],
      "year": "2020",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "16",
      "title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans",
      "authors": [
        "M Roberts",
        "D Driggs",
        "M Thorpe",
        "J Gilbey",
        "M Yeung",
        "S Ursprung",
        "A Aviles-Rivero",
        "C Etmann",
        "C Mccague",
        "L Beer",
        "J Weir-Mccall",
        "Z Teng",
        "E Gkrania-Klotsas",
        "A Ruggiero",
        "A Korhonen",
        "E Jefferson",
        "E Ako",
        "G Langs",
        "G Gozaliasl",
        "G Yang",
        "H Prosch",
        "J Preller",
        "J Stanczuk",
        "J Tang",
        "J Hofmanninger",
        "J Babar",
        "L Sánchez",
        "M Thillai",
        "P Gonzalez",
        "P Teare",
        "X Zhu",
        "M Patel",
        "C Cafolla",
        "H Azadbakht",
        "J Jacob",
        "J Lowe",
        "K Zhang",
        "K Bradley",
        "M Wassin",
        "M Holzer",
        "K Ji",
        "M Ortet",
        "T Ai",
        "N Walton",
        "P Lio",
        "S Stranks",
        "T Shadbahr",
        "W Lin",
        "Y Zha",
        "Z Niu",
        "J Rudd",
        "E Sala",
        "C.-B Schönlieb"
      ],
      "year": "2021",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "17",
      "title": "A scoping review of robustness concepts for machine learning in healthcare",
      "authors": [
        "A Balendran",
        "C Beji",
        "F Bouvier",
        "O Khalifa",
        "T Evgeniou",
        "P Ravaud",
        "R Porcher"
      ],
      "year": "2025",
      "venue": "Npj Digit. Med"
    },
    {
      "citation_id": "18",
      "title": "A Comparative Study on the Impacts of Data Leakage During Feature Selection using the CIC-IoT 2023 Intrusion Detection Dataset",
      "authors": [
        "S Mallampati",
        "S Hari"
      ],
      "year": "2024",
      "venue": "2024 10th International Conference on Electrical Energy Systems (ICEES)"
    },
    {
      "citation_id": "19",
      "title": "Inflated prediction accuracy of neuropsychiatric biomarkers caused by data leakage in feature selection",
      "authors": [
        "M Shim",
        "S.-H Lee",
        "H.-J Hwang"
      ],
      "year": "2021",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "20",
      "title": "Risk of data leakage in estimating the diagnostic performance of a deep-learning-based computer-aided system for psychiatric disorders",
      "authors": [
        "H.-T Lee",
        "H -R. Cheon",
        "S.-H Lee",
        "M Shim",
        "H.-J Hwang"
      ],
      "year": "2023",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "21",
      "title": "Bias in error estimation when using cross-validation for model selection",
      "authors": [
        "S Varma",
        "R Simon"
      ],
      "year": "2006",
      "venue": "BMC Bioinformatics"
    },
    {
      "citation_id": "22",
      "title": "Selection bias in gene extraction on the basis of microarray gene-expression data",
      "authors": [
        "C Ambroise",
        "G Mclachlan"
      ],
      "year": "2002",
      "venue": "Proc. Natl. Acad. Sci"
    },
    {
      "citation_id": "23",
      "title": "Data leakage jeopardizes ecological applications of machine learning",
      "authors": [
        "A Stock",
        "E Gregr",
        "K Chan"
      ],
      "year": "2023",
      "venue": "Nat. Ecol. Evol"
    },
    {
      "citation_id": "24",
      "title": "leakage inflates prediction performance in connectome-based machine learning models",
      "authors": [
        "M Rosenblatt",
        "L Tejavibulya",
        "R Jiang",
        "S Noble",
        "D Scheinost"
      ],
      "year": "2024",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "25",
      "title": "Dos and Don'ts of Machine Learning in Computer Security",
      "authors": [
        "D Arp",
        "E Quiring",
        "F Pendlebury",
        "A Warnecke",
        "F Pierazzi",
        "C Wressnegger",
        "L Cavallaro",
        "K Rieck"
      ],
      "year": "2022",
      "venue": "Dos and Don'ts of Machine Learning in Computer Security"
    },
    {
      "citation_id": "26",
      "title": "Leakage and the reproducibility crisis in machine-learning-based science",
      "authors": [
        "S Kapoor",
        "A Narayanan"
      ],
      "year": "2023",
      "venue": "Patterns"
    },
    {
      "citation_id": "27",
      "title": "The impact of cross-validation choices on pBCI classification metrics: lessons for transparent reporting",
      "authors": [
        "F Schroeder",
        "S Fairclough",
        "F Dehais",
        "M Richins"
      ],
      "year": "2025",
      "venue": "Front. Neuroergonomics"
    },
    {
      "citation_id": "28",
      "title": "A Survey on Bias and Fairness in Machine Learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2022",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "29",
      "title": "Genetic Misdiagnoses and the Potential for Health Disparities",
      "authors": [
        "A Manrai",
        "B Funke",
        "H Rehm",
        "M Olesen",
        "B Maron",
        "P Szolovits",
        "D Margulies",
        "J Loscalzo",
        "I Kohane"
      ],
      "year": "2016",
      "venue": "N. Engl. J. Med"
    },
    {
      "citation_id": "30",
      "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?",
      "year": "2019",
      "venue": "AMA J. Ethics"
    },
    {
      "citation_id": "31",
      "title": "The Ugly Truth About Ourselves and Our Robot Creations: The Problem of Bias and Social Inequity",
      "authors": [
        "A Howard",
        "J Borenstein"
      ],
      "year": "2018",
      "venue": "Sci. Eng. Ethics"
    },
    {
      "citation_id": "32",
      "title": "Emotions Recognition Using EEG Signals: A Survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "33",
      "title": "Comprehensive evaluation methods for translating BCI into practical applications: usability, user satisfaction and usage of online BCI systems",
      "authors": [
        "H Pan",
        "P Ding",
        "F Wang",
        "T Li",
        "L Zhao",
        "W Nan",
        "Y Fu",
        "A Gong"
      ],
      "year": "2024",
      "venue": "Front. Hum. Neurosci"
    },
    {
      "citation_id": "34",
      "title": "The role of scoping reviews in reducing research waste",
      "authors": [
        "H Khalil",
        "M Peters",
        "P Mcinerney",
        "C Godfrey",
        "L Alexander",
        "C Evans",
        "D Pieper",
        "E Moraes",
        "A Tricco",
        "Z Munn",
        "D Pollock"
      ],
      "year": "2022",
      "venue": "J. Clin. Epidemiol"
    },
    {
      "citation_id": "35",
      "title": "The scandal of poor medical research",
      "authors": [
        "D Altman"
      ],
      "year": "1994",
      "venue": "BMJ"
    },
    {
      "citation_id": "36",
      "title": "Reproducibility in Science: Improving the Standard for Basic and Preclinical Research",
      "authors": [
        "C Begley",
        "J Ioannidis"
      ],
      "year": "2015",
      "venue": "Reproducibility in Science: Improving the Standard for Basic and Preclinical Research"
    },
    {
      "citation_id": "37",
      "title": "Avoidable waste in the production and reporting of research evidence",
      "authors": [
        "I Chalmers",
        "P Glasziou"
      ],
      "year": "2009",
      "venue": "The Lancet"
    },
    {
      "citation_id": "38",
      "title": "How to increase value and reduce waste when research priorities are set",
      "authors": [
        "I Chalmers",
        "M Bracken",
        "B Djulbegovic",
        "S Garattini",
        "J Grant",
        "A Gülmezoglu",
        "D Howells",
        "J Ioannidis",
        "S Oliver"
      ],
      "year": "2014",
      "venue": "The Lancet"
    },
    {
      "citation_id": "39",
      "title": "Increasing value and reducing waste in biomedical research regulation and management",
      "authors": [
        "R -S. Salman",
        "E Beller",
        "J Kagan",
        "E Hemminki",
        "R Phillips",
        "J Savulescu",
        "M Macleod",
        "J Wisely",
        "I Chalmers"
      ],
      "year": "2014",
      "venue": "The Lancet"
    },
    {
      "citation_id": "40",
      "title": "Biomedical research: increasing value, reducing waste. The Lancet",
      "authors": [
        "M Macleod",
        "S Michie",
        "I Roberts",
        "U Dirnagl",
        "I Chalmers",
        "J Ioannidis",
        "R .-S. Salman",
        "A.-W Chan",
        "P Glasziou"
      ],
      "year": "2014",
      "venue": "Biomedical research: increasing value, reducing waste. The Lancet"
    },
    {
      "citation_id": "41",
      "title": "The Economics of Reproducibility in Preclinical Research",
      "authors": [
        "L Freedman",
        "I Cockburn",
        "T Simcoe"
      ],
      "year": "2015",
      "venue": "PLOS Biol"
    },
    {
      "citation_id": "42",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "Jong-Seok Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "43",
      "title": "Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals",
      "authors": [
        "X Xu",
        "B Wang",
        "B Xiao",
        "Y Niu",
        "Y Wang",
        "X Wu",
        "J Chen"
      ],
      "year": "2024",
      "venue": "arXiv",
      "doi": "10.48550/ARXIV.2405.17024"
    },
    {
      "citation_id": "44",
      "title": "EEG-Based Emotion Recognition Datasets for Virtual Environments: A Survey",
      "authors": [
        "H Hamzah",
        "K Abdalla"
      ],
      "year": "2024",
      "venue": "Appl. Comput. Intell. Soft Comput"
    },
    {
      "citation_id": "45",
      "title": "Effects of mobile phone electromagnetic fields on brain waves in healthy volunteers",
      "authors": [
        "J Van Der Meer",
        "Y Eisma",
        "R Meester",
        "M Jacobs",
        "A Nederveen"
      ],
      "year": "2023",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "46",
      "title": "A quantitative physical model of the TMSinduced discharge in EEG",
      "authors": [
        "D Freche",
        "J Naim-Feil",
        "A Peled",
        "N Levit-Binnun",
        "E Moses"
      ],
      "year": "2018",
      "venue": "PLOS Comput. Biol"
    },
    {
      "citation_id": "47",
      "title": "Adaptive neural decision tree for EEG based emotion recognition",
      "authors": [
        "Y Zheng",
        "J Ding",
        "F Liu",
        "D Wang"
      ],
      "year": "2023",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "48",
      "title": "Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification",
      "authors": [
        "S Gannouni",
        "A Aledaily",
        "K Belwafi",
        "H Aboalsamh"
      ],
      "year": "2021",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "49",
      "title": "EEG-based cross-subject emotion recognition using Fourier-Bessel series expansion based empirical wavelet transform and NCA feature selection method",
      "authors": [
        "A Anuragi",
        "D Singh Sisodia",
        "R Pachori"
      ],
      "year": "2022",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "50",
      "title": "Temporal relative transformer encoding cooperating with channel attention for EEG emotion analysis",
      "authors": [
        "G Peng",
        "K Zhao",
        "H Zhang",
        "D Xu",
        "X Kong"
      ],
      "year": "2023",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "51",
      "title": "EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "52",
      "title": "Recognizing Emotion From Multichannel EEG Signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng",
        "Sparsedgcnn"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "53",
      "title": "CNN and LSTM based ensemble learning for human emotion recognition using EEG recordings",
      "authors": [
        "A Iyer",
        "S Das",
        "R Teotia",
        "S Maheshwari",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "54",
      "title": "EEG emotion recognition based on the attention mechanism and pre-trained convolution capsule network",
      "authors": [
        "S Liu",
        "Z Wang",
        "Y An",
        "J Zhao",
        "Y Zhao",
        "Y.-D Zhang"
      ],
      "year": "2023",
      "venue": "Knowl.-Based Syst"
    },
    {
      "citation_id": "55",
      "title": "TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "56",
      "title": "Adaptive neural decision tree for EEG based emotion recognition",
      "authors": [
        "Y Zheng",
        "J Ding",
        "F Liu",
        "D Wang"
      ],
      "year": "2023",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "57",
      "title": "EEG-based Emotion Recognition via Transformer Neural Architecture Search",
      "authors": [
        "C Li",
        "Z Zhang",
        "X Zhang",
        "G Huang",
        "Y Liu",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Ind. Inform"
    },
    {
      "citation_id": "58",
      "title": "TC-Net: A Transformer Capsule Network for EEG-based emotion recognition",
      "authors": [
        "Y Wei",
        "Y Liu",
        "C Li",
        "J Cheng",
        "R Song",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "59",
      "title": "EEG-Based Emotion Recognition via Neural Architecture Search",
      "authors": [
        "C Li",
        "Z Zhang",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "60",
      "title": "Subject-independent EEG emotion recognition with hybrid spatio-temporal GRU-Conv architecture",
      "authors": [
        "G Xu",
        "W Guo",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Med. Biol. Eng. Comput"
    },
    {
      "citation_id": "61",
      "title": "GLFANet: A global to local feature aggregation network for EEG emotion recognition",
      "authors": [
        "S Liu",
        "Y Zhao",
        "Y An",
        "J Zhao",
        "S.-H Wang",
        "J Yan"
      ],
      "year": "2023",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "62",
      "title": "Functional Integration and Separation of Brain Network Based on Phase Locking Value During Emotion Processing",
      "authors": [
        "Z.-M Wang",
        "R Zhou",
        "Y He",
        "X.-M Guo"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "63",
      "title": "EEG emotion recognition using improved graph neural network with channel selection",
      "authors": [
        "X Lin",
        "J Chen",
        "W Ma",
        "W Tang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Comput. Methods Programs Biomed"
    },
    {
      "citation_id": "64",
      "title": "Effective Emotion Recognition by Learning Discriminative Graph Topologies in EEG Brain Networks",
      "authors": [
        "C Li",
        "P Li",
        "Y Zhang",
        "N Li",
        "Y Si",
        "F Li",
        "Z Cao",
        "H Chen",
        "B Chen",
        "D Yao",
        "P Xu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst"
    },
    {
      "citation_id": "65",
      "title": "Deep time-frequency features and semisupervised dimension reduction for subject-independent emotion recognition from multi-channel EEG signals",
      "authors": [
        "B Zali-Vargahan",
        "A Charmin",
        "H Kalbkhani",
        "S Barghandan"
      ],
      "year": "2023",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "66",
      "title": "Comprehensive Analysis of Feature Extraction Methods for Emotion Recognition from Multichannel EEG Recordings",
      "authors": [
        "R Yuvaraj",
        "P Thagavel",
        "J Thomas",
        "J Fogarty",
        "F Ali"
      ],
      "year": "2023",
      "venue": "Comprehensive Analysis of Feature Extraction Methods for Emotion Recognition from Multichannel EEG Recordings"
    },
    {
      "citation_id": "67",
      "title": "Emotion recognition using spatial-temporal EEG features through convolutional graph attention network",
      "authors": [
        "Z Li",
        "G Zhang",
        "L Wang",
        "J Wei",
        "J Dang"
      ],
      "year": "2023",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "68",
      "title": "Brain Emotion Perception Inspired EEG Emotion Recognition With Deep Reinforcement Learning",
      "authors": [
        "D Li",
        "L Xie",
        "Z Wang",
        "H Yang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst"
    },
    {
      "citation_id": "69",
      "title": "An Efficient LSTM Network for Emotion Recognition From Multichannel EEG Signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "70",
      "title": "Subject independent emotion recognition from EEG using VMD and deep learning",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2022",
      "venue": "J. King Saud Univ. -Comput. Inf. Sci"
    },
    {
      "citation_id": "71",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sens. J"
    },
    {
      "citation_id": "72",
      "title": "Multi-Source Domain Transfer Discriminative Dictionary Learning Modeling for Electroencephalogram-Based Emotion Recognition",
      "authors": [
        "X Gu",
        "W Cai",
        "M Gao",
        "Y Jiang",
        "X Ning",
        "P Qian"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Comput. Soc. Syst"
    },
    {
      "citation_id": "73",
      "title": "Emotion recognition from EEG based on multi-task learning with capsule network and attention mechanism",
      "authors": [
        "C Li",
        "B Wang",
        "S Zhang",
        "Y Liu",
        "R Song",
        "J Cheng",
        "X Chen"
      ],
      "year": "2022",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "74",
      "title": "Subject independent emotion recognition using EEG signals employing attention driven neural networks",
      "authors": [
        "A Arjun",
        "M Rajpoot",
        "Panicker"
      ],
      "year": "2022",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "75",
      "title": "Deep Learning-Based Approach for Emotion Recognition Using Electroencephalography (EEG) Signals Using Bi-Directional Long Short-Term Memory (Bi-LSTM)",
      "authors": [
        "M Algarni",
        "F Saeed",
        "T Al-Hadhrami",
        "F Ghabban",
        "M Al-Sarem"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "76",
      "title": "Automated Feature Extraction on AsMap for Emotion Classification Using EEG",
      "authors": [
        "Z I Md",
        "N Ahmed",
        "S Sinha",
        "E Phadikar",
        "Ghaderpour"
      ],
      "year": "2022",
      "venue": "Automated Feature Extraction on AsMap for Emotion Classification Using EEG"
    },
    {
      "citation_id": "77",
      "title": "EEG-based Emotion Recognition with Feature Fusion Networks",
      "authors": [
        "Q Gao",
        "Y Yang",
        "Q Kang",
        "Z Tian",
        "Y Song"
      ],
      "year": "2022",
      "venue": "Int. J. Mach. Learn. Cybern"
    },
    {
      "citation_id": "78",
      "title": "A novel ensemble learning method using multiple objective particle swarm optimization for subject-independent EEG-based emotion recognition",
      "authors": [
        "R Li",
        "C Ren",
        "X Zhang",
        "B Hu"
      ],
      "year": "2022",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "79",
      "title": "EEG-Based Emotion Recognition Using Spatial-Temporal Graph Convolutional LSTM With Attention Mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "80",
      "title": "Ren, 4D attention-based neural network for EEG emotion recognition",
      "authors": [
        "G Xiao",
        "M Shi",
        "M Ye",
        "B Xu",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "Cogn. Neurodyn"
    },
    {
      "citation_id": "81",
      "title": "Spatial-Temporal Feature Fusion Neural Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "82",
      "title": "Minimum spanning tree based graph neural network for emotion classification using EEG",
      "authors": [
        "H Liu",
        "J Zhang",
        "Q Liu",
        "J Cao"
      ],
      "year": "2022",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "83",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based crossdomain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "84",
      "title": "IDEA: Intellect database for emotion analysis using EEG signal",
      "authors": [
        "V Joshi",
        "R Ghongade"
      ],
      "year": "2022",
      "venue": "J. King Saud Univ. -Comput. Inf. Sci"
    },
    {
      "citation_id": "85",
      "title": "A Novel DE-CNN-BiLSTM Multi-Fusion Model for EEG Emotion Recognition",
      "authors": [
        "F Cui",
        "R Wang",
        "W Ding",
        "Y Chen",
        "L Huang"
      ],
      "year": "2022",
      "venue": "A Novel DE-CNN-BiLSTM Multi-Fusion Model for EEG Emotion Recognition"
    },
    {
      "citation_id": "86",
      "title": "EEG emotion recognition using fusion model of graph convolutional neural networks and LSTM",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Appl. Soft Comput"
    },
    {
      "citation_id": "87",
      "title": "Emotion Recognition From Multi-Channel EEG via Deep Forest",
      "authors": [
        "J Cheng",
        "M Chen",
        "C Li",
        "Y Liu",
        "R Song",
        "A Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "88",
      "title": "Emotion recognition based on EEG feature maps through deep learning network",
      "authors": [
        "A Topic",
        "M Russo"
      ],
      "year": "2021",
      "venue": "Eng. Sci. Technol. Int. J"
    },
    {
      "citation_id": "89",
      "title": "EEG Channel Correlation Based Model for Emotion Recognition",
      "authors": [
        "R Md",
        "Md Islam",
        "Md Islam",
        "C Rahman",
        "S Mondal",
        "M Singha",
        "A Ahmad",
        "Md Awal",
        "M Islam",
        "Moni"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "90",
      "title": "Differences first in asymmetric brain: A bihemisphere discrepancy convolutional neural network for EEG emotion recognition",
      "authors": [
        "D Huang",
        "S Chen",
        "C Liu",
        "L Zheng",
        "Z Tian",
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "91",
      "title": "A Channel-Fused Dense Convolutional Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "92",
      "title": "Automated accurate emotion recognition system using rhythm-specific deep convolutional neural network technique with multi-channel EEG signals",
      "authors": [
        "D Maheshwari",
        "S Ghosh",
        "R Tripathy",
        "M Sharma",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "93",
      "title": "Emotion recognition from EEG signals using empirical mode decomposition and second-order difference plot",
      "authors": [
        "N Salankar",
        "P Mishra",
        "L Garg"
      ],
      "year": "2021",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "94",
      "title": "A prototype-based SPD matrix network for domain adaptation EEG emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "95",
      "title": "Prime pattern and tunable q-factor wavelet transform techniques for automated accurate EEG emotion recognition",
      "authors": [
        "A Dogan",
        "M Akay",
        "P Barua",
        "M Baygin",
        "S Dogan",
        "T Tuncer",
        "A Dogru",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "96",
      "title": "Multi-Domain Feature Fusion for Emotion Classification Using DEAP Dataset",
      "authors": [
        "M Khateeb",
        "S Anwar",
        "M Alnowami"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "97",
      "title": "Exploring Deep Learning Features for Automatic Classification of Human Emotion Using EEG Rhythms",
      "authors": [
        "F Demir",
        "N Sobahi",
        "S Siuly",
        "A Sengur"
      ],
      "year": "2021",
      "venue": "IEEE Sens. J"
    },
    {
      "citation_id": "98",
      "title": "Predicting Exact Valence and Arousal Values from EEG",
      "authors": [
        "F Galvão",
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "99",
      "title": "Subject independent emotion recognition system for people with facial deformity: an EEG based approach",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2021",
      "venue": "J. Ambient Intell. Humaniz. Comput"
    },
    {
      "citation_id": "100",
      "title": "Multi-Feature Input Deep Forest for EEG-Based Emotion Recognition",
      "authors": [
        "Y Fang",
        "H Yang",
        "X Zhang",
        "H Liu",
        "B Tao"
      ],
      "year": "2021",
      "venue": "Front. Neurorobotics"
    },
    {
      "citation_id": "101",
      "title": "ScalingNet: Extracting features from raw EEG data for emotion recognition",
      "authors": [
        "J Hu",
        "C Wang",
        "Q Jia",
        "Q Bu",
        "R Sutcliffe",
        "J Feng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "102",
      "title": "Investigating the Use of Pretrained Convolutional Neural Network on Cross-Subject and Cross-Dataset EEG Emotion Recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "103",
      "title": "EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowl.-Based Syst"
    },
    {
      "citation_id": "104",
      "title": "EEG-Based Emotion Classification Using a Deep Neural Network and Sparse Autoencoder",
      "authors": [
        "J Liu",
        "G Wu",
        "Y Luo",
        "S Qiu",
        "S Yang",
        "W Li",
        "Y Bi"
      ],
      "year": "2020",
      "venue": "Front. Syst. Neurosci"
    },
    {
      "citation_id": "105",
      "title": "Multi-channel EEG-based emotion recognition via a multi-level features guided capsule network",
      "authors": [
        "Y Liu",
        "Y Ding",
        "C Li",
        "J Cheng",
        "R Song",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "106",
      "title": "Domain Adaptation for EEG Emotion Recognition Based on Latent Representation Similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "107",
      "title": "Automated emotion recognition based on higher order statistics and deep learning algorithm",
      "authors": [
        "R Sharma",
        "R Pachori",
        "P Sircar"
      ],
      "year": "2020",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "108",
      "title": "Emotion recognition with convolutional neural network and EEG-based EFDMs",
      "authors": [
        "F Wang",
        "S Wu",
        "W Zhang",
        "Z Xu",
        "Y Zhang",
        "C Wu",
        "S Coleman"
      ],
      "year": "2020",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "109",
      "title": "EEG-based emotion recognition using 4D convolutional recurrent neural network",
      "authors": [
        "F Shen",
        "G Dai",
        "G Lin",
        "J Zhang",
        "W Kong",
        "H Zeng"
      ],
      "year": "2020",
      "venue": "Cogn. Neurodyn"
    },
    {
      "citation_id": "110",
      "title": "A comparative analysis of machine learning methods for emotion recognition using EEG and peripheral physiological signals",
      "authors": [
        "V Doma",
        "M Pirouz"
      ],
      "year": "2020",
      "venue": "J. Big Data"
    },
    {
      "citation_id": "111",
      "title": "Data augmentation for enhancing EEG-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "112",
      "title": "A Mutual Information Based Adaptive Windowing of Informative EEG for Emotion Recognition",
      "authors": [
        "L Piho",
        "T Tjahjadi"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "113",
      "title": "Locally robust EEG feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "114",
      "title": "An Investigation of Deep Learning Models for EEG-Based Emotion Recognition",
      "authors": [
        "Y Zhang",
        "J Chen",
        "J Tan",
        "Y Chen",
        "Y Chen",
        "D Li",
        "L Yang",
        "J Su",
        "X Huang",
        "W Che"
      ],
      "year": "2020",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "115",
      "title": "Machine-Learning-Based Emotion Recognition System Using EEG Signals",
      "authors": [
        "R Alhalaseh",
        "S Alasasfeh"
      ],
      "year": "2020",
      "venue": "Computers"
    },
    {
      "citation_id": "116",
      "title": "Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm",
      "authors": [
        "H He",
        "Y Tan",
        "J Ying",
        "W Zhang"
      ],
      "year": "2020",
      "venue": "Appl. Soft Comput"
    },
    {
      "citation_id": "117",
      "title": "Latent Factor Decoding of Multi-Channel EEG for Emotion Recognition Through Autoencoder-Like Neural Networks",
      "authors": [
        "X Li",
        "Z Zhao",
        "D Song",
        "Y Zhang",
        "J Pan",
        "L Wu",
        "J Huo",
        "C Niu",
        "D Wang"
      ],
      "year": "2020",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "118",
      "title": "EEG-Based Emotion Recognition Using Logistic Regression with Gaussian Kernel and Laplacian Prior and Investigation of Critical Frequency Bands",
      "authors": [
        "C Pan",
        "C Shi",
        "H Mu",
        "J Li",
        "X Gao"
      ],
      "year": "2020",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "119",
      "title": "Emotion Recognition From Multi-Channel EEG Signals by Exploiting the Deep Belief-Conditional Random Field Framework",
      "authors": [
        "H Chao",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "120",
      "title": "Spatio-Temporal Representation of an Electoencephalogram for Emotion Recognition Using a Three-Dimensional Convolutional Neural Network",
      "authors": [
        "J Cho",
        "H Hwang"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "121",
      "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "122",
      "title": "Domain Adaptation Techniques for EEG-Based Emotion Recognition: A Comparative Study on Two Public Datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Muller-Putz"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "123",
      "title": "Emotion Recognition from Multiband EEG Signals Using CapsNet",
      "authors": [
        "H Chao",
        "L Dong",
        "Y Liu",
        "B Lu"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "124",
      "title": "SAE+LSTM: A New Framework for Emotion Recognition From Multi-Channel EEG",
      "authors": [
        "X Xing",
        "Z Li",
        "T Xu",
        "L Shu",
        "B Hu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "SAE+LSTM: A New Framework for Emotion Recognition From Multi-Channel EEG"
    },
    {
      "citation_id": "125",
      "title": "Accurate EEG-Based Emotion Recognition on Combined Features Using Deep Convolutional Neural Networks",
      "authors": [
        "J Chen",
        "P Zhang",
        "Z Mao",
        "Y Huang",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "126",
      "title": "Column CNN Model for Emotion Recognition from EEG Signals",
      "authors": [
        "H Yang",
        "J Han",
        "K Min",
        "Multi"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "127",
      "title": "Phase-Locking Value Based Graph Convolutional Neural Networks for Emotion Recognition",
      "authors": [
        "Z Wang",
        "Y Tong",
        "X Heng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "128",
      "title": "Channel Selection Method for EEG Emotion Recognition Using Normalized Mutual Information",
      "authors": [
        "Z.-M Wang",
        "S.-Y Hu",
        "H Song"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "129",
      "title": "EEG-Based Multi-Modal Emotion Recognition using Bag of Deep Features: An Optimal Feature Selection Approach",
      "authors": [
        "M Asghar",
        "M Khan",
        "Y Fawad",
        "M Amin",
        "M Rizwan",
        "S Rahman",
        "S Badnava",
        "Mirjavadi"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "130",
      "title": "Multi-method Fusion of Cross-Subject Emotion Recognition Based on High-Dimensional EEG Features",
      "authors": [
        "F Yang",
        "X Zhao",
        "W Jiang",
        "P Gao",
        "G Liu"
      ],
      "year": "2019",
      "venue": "Front. Comput. Neurosci"
    },
    {
      "citation_id": "131",
      "title": "Synchrosqueezing transform based feature extraction from EEG signals for emotional state prediction",
      "authors": [
        "P Ozel",
        "A Akan",
        "B Yilmaz"
      ],
      "year": "2019",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "132",
      "title": "Electroencephalogram Emotion Recognition Based on Empirical Mode Decomposition and Optimal Feature Selection",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "D.-Y Li",
        "S.-H Li"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cogn. Dev. Syst"
    },
    {
      "citation_id": "133",
      "title": "Recognition of Emotional States Using Multiscale Information Analysis of High Frequency EEG Oscillations",
      "authors": [
        "Z Gao",
        "X Cui",
        "W Wan",
        "Z Gu"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "134",
      "title": "Emotion Recognition Based on Framework of BADEBA-SVM",
      "authors": [
        "Z Wang",
        "Z Zhang",
        "W Wang"
      ],
      "year": "2019",
      "venue": "Math. Probl. Eng"
    },
    {
      "citation_id": "135",
      "title": "Providing a Four-layer Method Based on Deep Belief Network to Improve Emotion Recognition in Electroencephalography in Brain Signals",
      "authors": [
        "S Mousavinasr",
        "A Pourmohammad",
        "M Saffari"
      ],
      "year": "2019",
      "venue": "J. Med. Signals Sens"
    },
    {
      "citation_id": "136",
      "title": "Exploring EEG Features in Cross-Subject Emotion Recognition",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "Y Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2018",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "137",
      "title": "EEG-Based Emotion Recognition using 3D Convolutional Neural Networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Wahby"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "138",
      "title": "Emotion recognition based on time-frequency distribution of EEG signals using multivariate synchrosqueezing transform",
      "authors": [
        "A Mert",
        "A Akan"
      ],
      "year": "2018",
      "venue": "Digit. Signal Process"
    },
    {
      "citation_id": "139",
      "title": "EEG-based classification of emotions using empirical mode decomposition and autoregressive model",
      "authors": [
        "Y Zhang",
        "S Zhang",
        "X Ji"
      ],
      "year": "2018",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "140",
      "title": "Recognition of Emotions Using Multichannel EEG Data and DBN-GC-Based Ensemble Deep Learning Framework",
      "authors": [
        "H Chao",
        "H Zhi",
        "L Dong",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "141",
      "title": "Emotion Recognition from EEG Signals Using Multidimensional Information in EMD Domain",
      "authors": [
        "N Zhuang",
        "Y Zeng",
        "L Tong",
        "C Zhang",
        "H Zhang",
        "B Yan"
      ],
      "year": "2017",
      "venue": "BioMed Res. Int"
    },
    {
      "citation_id": "142",
      "title": "Human Emotion Recognition with Electroencephalographic Multidimensional Features by Hybrid Deep Neural Networks",
      "authors": [
        "Y Li",
        "J Huang",
        "H Zhou",
        "N Zhong"
      ],
      "year": "2017",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "143",
      "title": "Cross-Subject EEG Feature Selection for Emotion Recognition Using Transfer Recursive Feature Elimination",
      "authors": [
        "Z Yin",
        "Y Wang",
        "L Liu",
        "W Zhang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Front. Neurorobotics"
    },
    {
      "citation_id": "144",
      "title": "Familiarity effects in EEG-based emotion recognition",
      "authors": [
        "N Thammasan",
        "K Moriyama",
        "K Fukui",
        "M Numao"
      ],
      "year": "2017",
      "venue": "Brain Inform"
    },
    {
      "citation_id": "145",
      "title": "Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset",
      "authors": [
        "M Menezes",
        "A Samara",
        "L Galway",
        "A Sant'anna",
        "A Verikas",
        "F Alonso-Fernandez",
        "H Wang",
        "R Bond"
      ],
      "year": "2017",
      "venue": "Pers. Ubiquitous Comput"
    },
    {
      "citation_id": "146",
      "title": "Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals",
      "authors": [
        "P Arnau-González",
        "M Arevalillo-Herráez",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "147",
      "title": "Deep fusion of multi-channel neurophysiological signal for emotion recognition and monitoring",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2017",
      "venue": "Int. J. Data Min. Bioinforma"
    }
  ]
}