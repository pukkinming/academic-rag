{
  "paper_id": "2312.15848v1",
  "title": "Modality-Collaborative Transformer With Hybrid Feature Reconstruction For Robust Emotion Recognition",
  "published": "2023-12-26T01:59:23Z",
  "authors": [
    "Chengxin Chen",
    "Pengyuan Zhang"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "Transformer",
    "Sequential data missing",
    "Hybrid feature reconstruction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: 1) improving the efficiency of constructing joint representations from unaligned multimodal features, and 2) relieving the performance decline caused by random modality feature missing. In this paper, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder which concurrently extracts and dynamically balances the intra-and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise complete and incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios. CCS Concepts: ‚Ä¢ Human-centered computing ‚Üí HCI theory, concepts and models; ‚Ä¢ Computing methodologies ‚Üí Neural networks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition (MER) is aimed at recognizing and comprehending human emotional states from video clips by leveraging data primarily from three channels: audio (vocal expressions/prosody), vision (facial expressions/gestures), and text (spoken words). With the unprecedented advances of social media in recent years, MER has gained increasing attention owing to various promising applications, such as user preference analysis  [47] , mental health care  [2] , and more empathetic chatting machine  [75] . Existing works have explored numerous approaches to fully utilize the complementary information across available modalities in a relatively simple and static multimedia environment  [1] . However, the real-world scenarios are far more complicated, posing great challenges to MER.\n\nOn the one hand, the multimodal features are naturally unaligned due to the varying sampling rates of involved modalities. One paradigm for handling unaligned multimodal sequences is to first summarize each modality into a high-level representation by collapsing the time dimension. Afterwards, the cross-modal interactions are modeled using different fusion mechanisms, such as tensor-based operation  [65]   [36] , factorized subspace learning  [57]   [27] , or deep canonical correlation analysis  [54]   [73] . Albeit, these methods fail to capture the temporal multimodal dynamics. For example, a frown and some upset words may appear at the same time, while a sigh may appear at the end of the utterance. To this end, several works  [12] [66]  [67]   [35]  propose to integrate the temporal and cross-modal interactions at each time step of Recurrent Neural Networks (RNNs). The limitation of these methods is the requirement to explicitly align multimodal signals to the equivalent sampling rate in advance, adding extra manual labeling costs. Inspired by the power of Transformer  [60] , researchers have looked into ways to implicitly align multimodal sequences during model training  [56] [64]  [38]   [25] . Most of these models are built upon the combination of independent self-attention and cross-attention reinforcement units introduced in  [56] . However, fusing multimodal sequences in this directional pairwise manner is inefficient due to limited modality interactions and duplicated model parameters.\n\nOn the other hand, the multimodal signals are frequently contaminated in real-world applications. For instance, the voice may be lost due to the speaker's silence or background noise, the text may be unavailable due to automatic speech recognition errors, and the faces may be invisible due to visual blur or occlusion. These phenomenons are collectively referred to as Random Modality Feature Missing (RMFM) in this paper. When the RMFM problem arises, existing MER models that are trained on ideal samples typically experience a severe performance decline. To address this issue, researchers have explored different strategies to recover the semantics of missing modalities, including missing modality imputation  [39]   [26] , generative methods  [55]   [19]   [71] , and translationbased methods  [46] [74]  [70] . The main idea of these approaches is to discover the correlations across modalities, then try to impute the information of missing modalities on the basis of available modalities. Despite the advancements, these techniques mainly focus on situations where one or two modalities are entirely absent. In practice, modality missing problem usually occurs at part of the time steps in multimodal sequential signals. In recent years, several approaches  [34]   [33] [64]  [53]  have been proposed to handle the fine-grained modality feature missing problem. However, these works are centered around the scenario where the probabilities of feature missing in training and testing data are comparable. As a result, different models need to be built for varying feature missing rates.\n\nMotivated by the above observations, we propose Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), a unified framework for efficient and robust MER in realworld scenarios with uncertain feature missing rates. We first design the Modality-Collaborative Transformer (MCT) for efficient representation learning from unaligned multimodal input. The key innovation of MCT is the Multimodal Re-scaled Attention Unit (MRAU), which takes multiple feature sequences as input and directly returns the reinforced counterparts. All modalities in MRAU are dynamically re-scaled with respect to sequence lengths and arranged into a hyper-modality to acquire modality-collaborative information. The hyper-modality is utilized to reinforce each modality via cross-modal attention. Moreover, MRAU can be stacked multiple times to deepen the interactions across modalities. Compared with the predominant Transformer-based models, MCT avoids the inefficient directional pairwise attention modeling and the repeated parameters of feature projection for the same modality. On the basis of MCT, we introduce the Hybrid Feature Reconstruction (HFR) to further strengthen the model robustness towards incomplete data scenarios. During model training, we apply a dynamic temporal masking on the complete sequences to simulate the RMFM cases, and utilize HFR to reconstruct the missing features. Specifically, HFR consists of two modules: 1) the Local Feature Imagination (LFI) is designed to reproduce the local missing elements of the input sequences leveraging the reinforced modality features, while 2) the Global Feature Alignment (GFA) is designed to align the the global semantics summarized from the complete and incomplete views of multimodal input. Consequently, HFR tries to capture the feature missing patterns from two diverse perspectives, which can further complement each other. The superiority of our proposed framework is verified by extensive experiments on different multimodal emotion recognition benchmarks, IEMOCAP  [8]  and MSP-IMPROV  [9] , with both complete and incomplete testing data.\n\nThe main contributions of this paper can be summarized as follows:\n\n‚Ä¢ We propose MCT to promote the efficiency of representation learning from unaligned multimodal sequences. With lower time and space complexity, MCT can achieve better performance over existing Transformer-based models. ‚Ä¢ We propose HFR to strengthen the robustness of MCT in real-world scenarios with uncertain feature missing rates. HFR is intended to reconstruct the missing semantics from both the local and global perspectives to stabilize the representation learning from incomplete multimodal sequences. ‚Ä¢ Experiments on two benchmark datasets demonstrate that our proposed MCT-HFR outperforms several state-of-the-art models in both complete and incomplete data scenarios, highlighting the efficacy of MCT-HFR for practical applications.\n\nThe remainder of this paper is organized as follows: Section 2 reviews the previous related work. Section 3 formalizes the problem statement and introduces the vanilla attention units. Section 4 describes the overall workflow and detailed components of our proposed method. Section 5 introduces the experimental datasets and setup in detail. Section 6 presents the experimental results and analysis. Finally, Section 7 concludes this work and discusses the future research directions.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Since humans can express their emotions in a variety of ways, including through tone of voice, facial expressions, and verbal language, it is expected that combining data from several modalities will be more effective than unimodal approaches  [48] . The multimodal fusion approaches can be broadly categorized into three levels: 1) feature-level fusion, 2) model-level fusion, and 3) decision-level fusion. Generally, feature-level fusion concatenates the time-aligned features at the front end to build up the mutlimodal input  [51] [41]  [21] , while decision-level fusion performs inference based on each modality and subsequently combines the predictions from all modalities  [68]  [24]  [44] . Nevertheless, these two types of fusion methods fail to explicitly model cross-modal interactions. Therefore, various modern fusion strategies have been centered around model-level fusion in the past few years. One line of research utilized independent encoders to extract high-level representations of different modalities prior to feature fusion  [65] [36]  [27]  [57]  [54]  [73] . For instance, Hazarika et al.  [27]  factorize modalities into complementary subspaces to construct a comprehensive multimodal representation. To further capture fine-grained interactions across modalities, a variety of approaches have been proposed, such as multi-view RNN networks  [12] [66][67]  [35]  and multimodal word embeddings  [61]  [50]  [58] . Despite the advancements, the requirement for timealigned features is a bottleneck for these methods in practical use. More recently, Transformer-based methods  [56]    [32] [64]  [38] [25]  [15]  have gained increasing popularity because of their capacity to fuse unaligned multimodal features directly. For example, Tsai et al.  [56]  propose the modality reinforcement unit to repeatedly reinforce a target modality with information from a source modality by learning the directional pairwise attention. Lv et al.  [38]  introduce a message hub to exchange information with each modality, then iteratively update the common message and the involved modality features. Based on the message hub, Sun et al.  [53]  further reduced the time complexity via local-global partial attention mechanism. Compared with existing approaches that build upon the combination of vanilla attention units, our proposed MCT optimizes the attention mechanism specifically for multimodal input, reinforcing the associated modalities collaboratively in a single layer with less time and space complexity.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methods For Random Modality Feature Missing Problem",
      "text": "While enormous approaches have been proposed to improve MER in the context of ideal data, the generalization of MER in real-world scenarios has yet to be investigated. Based on the assumption for data imperfection, previous works can be generally categorized into two groups: 1) modality missing, and 2) modality feature missing. The former one considers the situations where one or two modalities are entirely absent in the testing phase. In recent years, several approaches have been explored to handle this problem  [22] [39]  [26] [71]  [46] [74]  [70] . For instance, Pham et al.  [46]  propose a robust representation learning strategy leveraging cyclic translations from source to target modalities. Moreover, Zeng et al.  [70]  incorporate a tag encoding module with common space projection to address the issue of uncertain missing modalities. In contrast, modality feature missing considers a more general situation where partial time steps of sequential features are missing. Under this assumption, Liang et al.  [34]  develop a low-rank regularization based model for time-aligned multimodal sequences, and Li et al.  [33]  further improve the model by exploiting the temporal-wise dynamics of data with less space complexity. More recently, Yuan et al.  [64]  propose a Transformerbased model with sequential feature reconstruction for unaligned multimodal sequences, and Sun et al.  [53]  integrate the former low-level feature reconstruction with high-level feature attraction to enhance the model robustness. Although intriguing, these techniques require distinct models to be trained for various feature missing rates, which limits the models' generalization ability. In contrast, this work attempts to build one unified framework with superior robustness against uncertain feature missing patterns. In the training phase, the incomplete multimodal features are dynamically generated, and the complete view of multimodal input is referenced for the reconstruction of missing features.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Self-Supervised Representation Learning",
      "text": "Self-supervised learning (SSL), a promising paradigm for representation learning without requiring manually labeled data, has advanced natural language processing  [45]   [18] [17]  [29] , speech processing  [42] [52][4]  [13] , and computer vision  [14]   [11] [49]  [6] . By setting up the appropriate pretext tasks, SSL methods enable models to learn the inherent structure or patterns present in unlabeled data. For example, masked prediction is designed to learn meaningful contextualized representations by predicting the masked elements of data, e.g., word embeddings  [18] [17], quantized speech units  [4]   [31] , and discretized visual tokens  [49][6] . Contrastive learning is another approach to learn semantically rich yet discriminative representations by bringing similar samples closer and separating diverse samples apart  [14] [72]  [62] . Generally, the pre-trained models can be used as seed models and then optimized for downstream tasks. Moreover, Hendrycks et al.  [30]  reveal that substantial regularization may be achieved by integrating self-supervision and full supervision, which enhances robustness and uncertainty estimation. Inspired by the power of SSL, our proposed HFR sets up a pretext task as an auxiliary supervision, which reconstructs the missing semantics of incomplete multimodal sequences from both the local and global perspectives. The pretext task is trained jointly with the major classification task.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Preliminary",
      "text": "In this section, we first formalize the problem and its corresponding notations. Subsequently, we briefly introduce the vanilla attention units.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Problem Definition",
      "text": "Our task is to recognize the emotion expressed in videos leveraging three major modalities, including audio (ùëé), vision (ùë£), and language (ùëô). The complete feature sequences are denoted by X ùëö ‚àà R ùëá ùëö √óùëë ùëö , where ùëá ùëö and ùëë ùëö represent the sequence length and feature dimension of the corresponding modality ùëö ‚àà {ùëé, ùë£, ùëô }, respectively. The feature masking indicators are denoted by M ùëö ‚àà R ùëá ùëö , where 1 denotes ablating the feature vector at this time step, while 0 denotes keeping it unaltered. The derived incomplete feature sequences are denoted by X ùëö = X ùëö ‚äô M ùëö ‚àà R ùëá ùëö √óùëë ùëö . Hence, the problem can be formulated as how to map (X ùëé , X ùë£ , X ùëô ) into the emotion label y ‚àà R ùëê for each video segment. Additionally, X ùëé , X ùë£ , X ùëô and (M ùëé , M ùë£ , M ùëô ) are utilized as auxiliary references for the feature reconstruction during model training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vanilla Attention Units",
      "text": "Transformer  [60]  is proposed to directly models the dependence between any pairwise frames of sequential features via the scaled dot-product attention. In the Self-attention Unit (SAU), the input sequences X ùë° ‚àà R ùëá ùë° √óùëë ùë° are first transformed into queries, keys, and values, which are denoted by\n\nAfterwards, the output of SAU is calculated as:\n\nwhere Y ùë° ‚àà R ùëá ùë° √óùëë ùë£ . Further, multi-head attention is introduced in  [60]  to jointly learn diverse relationships between queries and keys from different representation sub-spaces. The Cross-modal Attention Unit (CAU) is introduced to reinforce the features of target modality from another feature sequences of source modality  [56] , denoted as X ùë° ‚àà R ùëá ùë° √óùëë ùë° and X ùë† ‚àà R ùëá ùë† √óùëë ùë† , respectively. Different from SAU, the queries in CAU are mapped from the target modality, denoted as Q ùë° = X ùë° W ùëÑ , while the keys and values are mapped from the source modality, denoted as K ùë† = X ùë† W ùêæ , and V ùë† = X ùë† W ùëâ , respectively. Hence, the output of CAU can be formulated as:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methodology",
      "text": "As depicted in Fig.  1 , the overall workflow of MCT-HFR consists of two major branches: 1) Emotion recognition task related branch: The MCT is utilized to transform the incomplete sequences into reinforced modality representations, which are then summarized into global vectors for the emotion category classification. This branch is activated throughout the training and testing phases; 2) Feature reconstruction related branch: In the LFI module, the imagined modality representations are generated by corresponding Transformer-based decoders, then we calculate the reconstruction loss of the local missing features. In the GFA module, we also extract the global vectors of complete sequences to compute the similarity loss with the counterparts of incomplete sequences. Together, these two modules make up the HFR. This branch is activated only in the training phase. In the following sub-sections, we present the details of each module.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Modality-Collaborative Transformer",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Unimodal Encoding Module.",
      "text": "To capture the neighborhood information of the input sequence ùëã ùëö , a 1D convolution (Conv1D) layer is first employed on the temporal dimension:\n\nwhere ùëò ùëö is the convolution kernel size, ùëö ‚àà {ùëé, ùë£, ùëô }. In this way, the input sequences are projected into corresponding semantic spaces with a shared feature dimension ùëë. We further integrate the element-wise position information into the projected features using the sinusoidal position embedding (PE)  [60] :\n\nwhere PE (ùëá ùëö , ùëë) ‚àà R ùëá ùëö √óùëë computes the embeddings for each time step.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal",
      "text": "Re-scaled Attention Unit. The architecture of the MRAU is shown in Fig.  2 . In order to efficiently capture the underlying correlations between elements across modalities, we first construct a hyper-modality that integrates the information from all the modalities. Later, each modality is reinforced by the hyper-modality using cross-modal attention. Formally, MRAU takes multiple feature sequences as input and returns the collaboratively reinforced features:\n\nwhere E ùëö ‚àà R ùëá ùëö √óùëë , ùëö ‚àà {ùëé, ùë£, ùëô }. Note that the input and output sequences in each MRAU layer share the same feature shapes, thus multiple layers could be stacked to deeply reinforces the involved modalities. For each MRAU layer, the input features are first projected into the queries, keys, and values, respectively:\n\nIntuitively, the hyper-modality can be regarded as the source modality of cross-modal attention, and the associated keys and values can be obtained via temporal-wise concatenation. However, the simple concatenation of unaligned sequences may lead to inferior performance. For each modality ùëö, the feature length ùëá ùëö is variable with a maximum length of ùëá max ùëö during training. On the one hand, the attention scores are calculated collaboratively across all modalities. Due to the varied sampling rate of different modalities, the longer sequence of a certain modality may be assigned with larger accumulated attention weights, degrading the importance of other modalities. On the other hand, if the testing samples greatly exceed the maximum length, the increased length of concatenated sequence may distract the attention from emotionally salient parts.\n\nTo this end, we propose a dynamic re-scaling strategy on the keys of each modality: 1) Attention balance factor: We first introduce ùõæ ùëè = 1",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "‚àö",
      "text": "ùëá ùëö as a penalty to balance the relative importance of different modalities in computing the attention scores; 2) Attention extrapolation factor: In  [16] , Chiang et al. proposed a log-length scaled attention to improve the extrapolation ability of selfattention. Inspired by this work, we extend the log-length factor into a multimodal form, denoted as ùõæ ùëí = log ùëö ùëá max ùëö ùëö ùëá ùëö , to overcome the attention dispersion problem. Hence, the construction of K ùëê and V ùëê can be formulated as:\n\nAfterwards, the computation in MRAU can be formulated as:\n\nwhere LN denotes layer normalization  [3] , and FFN is the position-wise feed-forward network in Transformer.\n\nNote that the projections of Q ùëö , K ùëö , V ùëö occur at the beginning of a single MRAU layer. They are calculated only once for each modality and shared for later modality reinforcements in the same MRAU layer. Compared with the predominant Transformer-based approaches that independently model the interactions between pairs of modalities, MRAU can reduce the modality reinforcement units from \"2ùëÄ CAU + 2ùëÄ SAU\"  [38]   [53]  to \"ùëÄ CAU\" for ùëÄ modalities. As a result, MRAU benefits from less space and time complexity, especially when more modalities are involved.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pooling And Classification",
      "text": "Finally, the attention-based pooling  [40]  is applied to summarize the reinforced modality features into global representations. The derived vectors, denoted as h ùëé , h ùë£ and h ùëô , are concatenated as the overall multimodal representation h for the downstream emotion classifier:\n\nwhere y ‚Ä≤ ‚àà R ùê∂ is the estimated probabilities, ùê∂ is the number of classes, and FC denotes fully connected layers. Afterwards, the standard Cross-Entropy (CE) loss is employed for the classification task:\n\nwhere ùë¶ ùëñ and ùë¶ ‚Ä≤ ùëñ are the ground-truth and predicted scores for each class ùëñ.\n\nwhere D ùëö denotes the regenerated sequences leveraging original input and reinforced output of MCT. With the help of collaboratively reinforced features, it's promising to reproduce the almost \"identical\" local features. In  [23] , Girshick et al. introduced a smooth L1 loss, which is less sensitive to outliers than L1 loss and prevents exploding gradients in some cases. Mathematically, we have:\n\nFollowing prior works  [64] [53], LFI also adopts smooth L1 loss to measure the difference between the imagined missing features and the associated ground-truth features. Hence, the overall loss function of LFI can be formulated as:\n\nwhere the feature masking indicators M ùëö are utilized to constraint the loss function at the missing positions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Global",
      "text": "Feature Alignment. The final goal of MCT is to enrich the emotional semantics of the global multimodal representations, which is not directly captured by recovering the local semantics of each modality. Inspired by the philosophy of contrastive learning, the GFA module is intended to reduce the discrepancy between the latent representations summarized from the complete and incomplete views of data. Specifically, both the complete and incomplete multimodal features are utilized to compute the global vectors, h and h, respectively. Afterwards, we adopt the Central Moment Discrepancy (CMD)  [69]  metric, which is an advanced distance metric that compares the order-wise moment differences of two representations to assess the discrepancy in their distributions. Mathematically, we have:\n\nwhere E (x) is the empirical expectation vector of the input sample x, and C ùëò (x) = E (x -E (x)) ùëò denotes the vector of ùëò th order sample central moments with respect to the coordinates. By optimizing the distance metric, the global semantics of complete and incomplete views are implicitly aligned. For comparison, we implement another two popular distance metrics in contrastive learning, including cosine similarity and Jensen-Shannon Divergence (JSD). However, they lead to inferior performance (see in Section 6.2).\n\nTo improve the quality of representations summarized from the complete view, we also calculate the associate CE loss as an additional supervision. Hence, the overall loss function of GFA can be formulated as follows:\n\nwhere FC is utilized for the semantic space mapping.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model Training",
      "text": "4.4.1 Overall Loss Function. In the proposed MCT-HFR framework, the above representation learning process is guided by three different supervisions. The classification loss of incomplete data L CE is the primary supervision for emotion recognition. While the feature reconstruction losses (including L GFA and L LFI ) on the complete-incomplete data pairs are regarded as the auxiliary supervisions for robust representation learning. Combining all the objectives together, the overall loss function can be formulated as follows:\n\nwhere ùõº and ùõΩ ‚àà R are the hyper-parameters that determine the contribution of hybrid feature reconstruction.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Dynamic Incomplete",
      "text": "Training. Different from prior works  [64] [53], the incomplete training data are dynamically generated on the fly in this work. The probability of sequential feature masking is denoted as ùëù miss , which follows the Bernoulli distribution. Besides, the proportion of incomplete data in a training batch increases linearly from 0.0 to 1.0 in the first 5 epochs. We empirically find that ùëù miss = 0.2 will suffice in our evaluation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experimental Datasets And Setup",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset Description",
      "text": "IEMOCAP and MSP-IMPROV are two popular benchmark datasets for multimodal emotion recognition. In Table  1 , we present the emotion distribution in each session of the datasets. IEMOCAP: The IEMOCAP dataset  [8]  contains approximately 12 hours of audio-visual data from 10 actors. The dataset contains 5 sessions and each session is performed by one female and male actor in scripted and improvised scenarios. To be consistent with the previous works, we merge the instances labeled \"excited\" into the \"happy\" class. Moreover, we remove the special notes in the manual transcriptions, such as \"[LAUGHTER]\", \"[BREATHING]\", and \"[GARBAGE]\".\n\nMSP-IMPROV: The MSP-IMPROV dataset  [9]  contains 6 sessions of dyadic interactions between pairs of male-female actors. 15 target sentences are used to collect the recordings. For each target sentence, 4 emotional scenarios were created to elicit happy, angry, sad, and neutral responses. We remove the videos shorter than 1 second, and select the videos in the \"Other-improvised\" group which were recorded in improvised scenarios. Compared with IEMOCAP, the emotion distribution of MSP-IMPROV is more unbalanced. Besides, the emotion behaviors presented in MSP-IMPROV are more natural and spontaneous, making it a more challenging dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Feature Extraction",
      "text": "We first pre-process the raw video segments by extracting the features of each modality for both datasets. The detailed feature extraction process is shown as follows:\n\n5.2.1 Text Modality. Recently, Transformer-based pre-trained language models has gained great success in the field of natural language processing  [63] . In this paper, a pre-trained BERT  [18]  model 1  is utilized to encode the sentences into deep linguistic features. As suggested by previous works  [45] , the average results of hidden states from the last four layers are calculated as the final word-level features X ùëô with ùëë ùëô equal to 768. In order to simulate the transcription errors, the missing tokens are replaced by the unknown token \"[UNK]\" in BERT.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Audio",
      "text": "Modality. Similar to the feature extraction process of text modality, a pre-trained wav2vec 2.0 [4] model 2  is exploited as the acoustic feature extractor. The derived acoustic features X ùëé are sampled at a frame rate of 50 Hz with ùëë ùëé equal to 512. Finally, the missing frames are replaced by zero vectors.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vision Modality.",
      "text": "To start with, OpenFace toolkit  [5]  is used to extract aligned faces from the raw videos at a frame rate of 5 Hz. Afterwards, a ResNet50 [28] model 3  fine-tuned on VGGFace2  [10]  is utilized to further extract deep visual features. Global average pooling is applied on the output of the last convolution layer to derive the feature vector of a certain facial expression. The time steps with no detected faces are padded with zero vectors to obtain the frame-level features X ùë£ with ùëë ùë£ equal to 512. Finally, the missing frames are replaced by zero vectors.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Implementation Details",
      "text": "Our proposed model was implemented using the PyTorch  [43]  framework on a single Tesla P100 GPU (12 GB). For parallel computing, the multimodal features of the same mini-batch were truncated or padded to the equal-length, and the batch size was set to 32. Concretely, the maximum lengths of each modality ùëá max ùëé ,ùëá max ùë£ ,ùëá max ùëô were set to (400, 40, 50), respectively. The output channels ùëë of Conv1D were set to 128, and the kernel sizes (ùëò ùëé , ùëò ùë£ , ùëò ùëô ) were set to (3, 3, 1). The stacked MRAU layers (ùëÅ ) were set to 4, and the cross-modal attention comprised of 4 heads with 32 nodes (ùëë ùëò ) in each head. An AdamW optimizer  [37]  with a learning rate of 10 -4 was applied to optimize the model parameters. To determine the weight of different losses, we adjusted the hyper-parameters ùõº and ùõΩ from 0 to 1 with a step length of 0.2. The models were trained for a maximum of 40 epochs, and the training was stopped if the validation loss did not decrease for 8 consecutive epochs.\n\nFollowing prior works, leave-one-session-out cross-validation (LOSO CV) was employed on both datasets. In each fold, the data from a certain session was split by speakers into the validation and testing sets, while the rest sessions were used for training. All models were trained using five different random seeds for a fair comparison, and the average results are reported in this paper.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Baseline Models",
      "text": "To validate the effectiveness of our proposed method, we reproduce the following state-of-the-art models as the baselines. All the approaches are applicable on unaligned multimodal sequences.\n\nMulT. Multimodal Transformer (MulT)  [56]  employs directional pairwise cross-modal attention to repeatedly reinforce a target modality with information from the other modalities.\n\nMISA. By projecting each sample's modality into two subspaces, this method learns both Modality-Invariant and -Specific Representations (MISA)  [27]  which benefit the multimodal fusion.\n\nPMR. Progressive Modality Reinforcement (PMR)  [38]  introduces a message hub to exchange information with each modality by reinforcing the common messages and modality-specific semantics in an iterative way.\n\nCHFN. Cross Hyper-modality Fusion Network (CHFN)  [25]  generates the multimodal-shifted word representations to dynamically capture the variations of nonverbal contexts.\n\nTFR-Net. Transformer-based Feature Reconstruction Network (TFR-Net)  [64]  integrates selfattention reinforcement on the basis of MulT as the feature encoder, and introduces an MLP-based temporal feature reconstruction module to tackle the modality feature missing problem.\n\nEMT-DLFR. Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR)  [53]  introduces a mutual promotion unit with local-global partial attention mechanism to reduce computational complexity, and integrates the former MLP-based temporal feature reconstruction  [64]  with high-level feature attraction to enhance the model robustness.\n\nThe training methods described in the original papers for these baselines can be divided into two categories: 1) \"Conventional complete training\" is employed in MulT, MISA, PMR and CHFN. This strategy trains the models only on the complete data; 2) \"One-to-one training\" is employed in TFR-Net and EMT-DLFR. This strategy generates the incomplete training data statically before model training, and the feature missing rate is aligned with the testing data. Hence, different models need to be trained for evaluation as the missing rate of testing data changes. By contrast, our proposed \"dynamic incomplete training\" only trains one model, and this model is evaluated on the testing data with arbitrary missing rate.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In this research, the performance of emotion recognition is evaluated using four metrics: unweighted average accuracy (UA), weighted average accuracy (WA), unweighted average F1 (UF1) and weighted average F1 (WF1). Concisely, UA/UF1 is referred to as a mean of accuracies/F1-scores for various emotion classes, and WA/WF1 is a weighted mean accuracies/F1-scores over various emotion classes with weights proportionate to the number of instances in a class  [7] . Given the inherent class imbalance present in both the IEMOCAP and MSP-IMPROV datasets, UA, UF1 and WF1 are adopted as the major evaluation metrics, while WA is considered as a supplementary assessment metric. For all the metrics, higher values indicate better model performance.\n\nTo evaluate the overall performance of model in incomplete data scenarios, we compute the Area Under Indicators Line Chart (AUILC) value for each metric following  [64] . Given the model evaluation metric scores {ùë† 0 , ùë† 1 , . . . , ùë† ùë° } with increasing missing rates {ùëü 0 , ùëü 1 , . . . , ùëü ùë° }, AUILC value is defined as:\n\nOn both datasets, we evaluate the model under the feature missing rates of {0.0, 0.1, . . . , 0.9}.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "6.1 Comparison With Existing Works 6.1.1 Classification Performance Analysis. First of all, we perform the quantitative experiments and evaluate different approaches in both complete and incomplete data scenarios. Table  2  and Table  3  present the experimental results on the IEMOCAP and MSP-IMPROV datasets, respectively. For fair comparison, these models can be divided into: traditional MER models using \"conventional complete training\" strategy (G-I), MER models designed for modality feature missing using \"oneto-one training\" strategy (G-II), and our proposed MCT-HFR with all the three training strategies (G-III). Note that MCT-HFR is equivalent to MCT when using the \"conventional complete training\" strategy. From the experimental results, we conclude the following observations: 1) Compared with the models in G-I, our proposed MCT outperforms currently advanced approaches on both datasets. For the complete testing data, MCT exceeds the best performing baselines by 1.32%/1.39% UA and 0.45%/1.17% UF1 on the IEMOCAP/MSP-IMPROV datasets, respectively. For the incomplete testing data, a similar conclusion can be drawn. We attribute these promising results to the collaborative modality reinforcement in MCT, which promotes the efficiency of modeling interactions across modalities and fastens the convergence of representation learning.\n\n2) Compared with the models in G-II, our proposed MCT-HFR outperforms the state-of-the-art models using the same training strategy on both datasets. For the incomplete testing data, MCT-HFR surpasses the suboptimal model, EMT-DLFR, by the AUILC values of 0.42%/0.66% UA, 0.55%/0.79% UF1 and 0.52%/0.72% WF1 on the IEMOCAP/MSP-IMPROV datasets, respectively. Furthermore, we plot the curves of detailed performance with respect to various missing rates in Fig.  3  and Fig.  4 . As the missing rate increases, the performance decline of our method is smaller than the others. Taking the results on IEMOCAP for example, as the missing rate increases from 0.0 to 0.9, the performance of baselines decreases by 11.07%~29.54% UA and 11.57%~30.04% UF1, while Table  2 . Performance of different approaches on the IEMOCAP dataset using 5-fold LOSO CV. For the incomplete testing data, the AUILC values of each metric with missing rate intervals {0.0, 0.1, . . . , 0.9} are reported. Models with ‚òÖ were trained using \"dynamic incomplete training\" strategy, while models with ‚Ä† were trained using \"one-to-one training\" strategy. The best results are highlighted in bold.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Models",
      "text": "Scores(%) on Complete Testing Data Scores(%) on Incomplete Testing Data  MCT-HFR only decreases by 9.86% UA and 9.44% UF1, highlighting the robustness of MCT-HFR.\n\nWe attribute these encouraging results to both the collaborative modality reinforcement in MCT and the complementary feature reconstruction in HFR.\n\n3) By comparing the models in G-III, we can observe that the introduction of \"dynamic incomplete training\" strategy can further enhances AUILC values for MCT-HFR on the incomplete testing data. It is interesting that MCT-HFR ‚òÖ can achieve even better performance than MCT-HFR ‚Ä† for most metrics on the complete testing data. This phenomenon is highly related to the \"dynamic incomplete training\" strategy. In some aspects, this strategy is similar to data augmentation, which may alleviate the overfitting problem when the dataset (IEMOCAP and MSP-IMPROV) is relatively  To further investigate the efficiency of our proposed MCT-HFR, we measure the computational complexity via the metrics of trainable parameters and multiply-accumulate operations (MACs)  4  . We perform the calculations on a random generated multimodal instance with (ùëá ùëé ,ùëá ùë£ ,ùëá ùëô ) equal to (400, 40, 50) and report the results in Table  4 . For fair comparison, the hyper-parameters of attention units are kept in line (4 layers and 4 attention heads with 32 nodes in each head) across the Transformer-based models. For TFR-Net, EMT-DLFR, and MCT-HFR, the feature reconstruction networks are omitted during inference. From the experimental results, we conclude the following observations: 1) We first compare the computational complexity of different fusion networks consisted of one layer. From Table  4 , we find that each layer of MCT-HFR (i.e., an MRAU in Section 4.1) enjoys the least trainable parameters (3√ó less than TFR-Net and 2√ó less than EMT-DLFR) and MACs (3√ó less than TFR-Net and slightly less than EMT-DLFR). Specifically, MulT and TFR-Net are based on directional pairwise attention modeling, requiring ùëÄ (ùëÄ -1) and ùëÄ 2 attention-based reinforcement units for ùëÄ modalities, respectively. Although PMR reduces the reinforcement units to 2ùëÄ, this model constructs a new hyper-modality as the message hub and iteratively updates it along with the original modalities, resulting in the highest MACs. On the basis of PMR, EMT-DLFR reduced the time complexity via local-global partial attention mechanism. In contrast, our proposed MCT-HFR further narrows the reinforcement units to ùëÄ, and the projected keys, queries, and values of each modality are shared for later modality reinforcements in the same layer. Additionally, MCT-HFR dynamically builds a hyper-modality via a parameter-free re-scaling mechanism and regards it as the source modality. As a result, the trainable parameters and MACs of MCT-HFR can be consistently lowered.\n\n2) Subsequently, we look into the results of overall models (basically including the unimodal encoders, multimodal fusion networks, pooling layers, and classifiers). Note that we exclude the pre-trained feature extractors. In MulT and TFR-Net, an extra computational complexity of selfattention is introduced for aggregating the repeatedly reinforced representations of each modality, which is not required in MCT-HFR. During model training, the feature reconstruction networks can increase the computational cost of TFR-Net, EMT-DLFR, and MCT-HFR. Compared with TFR-Net, the trainable parameters and MACs of MCT-HFR drop by 61.5% and 55.8%. Compared with EMT-DLFR, the higher MACs of MCT-HFR are due to the more sophisticated Transformer-based decoders in LFI. During model inference, MCT-HFR also achieves the lowest computational cost at the model level, with 2√ó less trainable parameters and slightly less MACs than the sub-optimal model.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Study",
      "text": "To better understand the influence of different modules in our proposed MCT-HFR framework, we perform extensive ablation studies on the IEMOCAP dataset and report the AUILC results for the incomplete testing data by default. In the following subsections, we employ the \"dynamic incomplete training\" strategy for all models.\n\n6.2.1 Effects of the Re-scaling Factors in MCT. As illustrated in Section 4.1, the re-scaling factors ùõæ ùëè and ùõæ ùëí are introduced in constructing the keys of hyper-modality. The motivation is to address the issue of modality imbalance and attention dispersion caused by excessively long concatenated sequences. To fully investigate the effects of the re-scaling factors, we ablate ùõæ ùëè and ùõæ ùëí respectively, then evaluate the models on the testing data split by different duration intervals. As shown in Table  5 , the models' performances generally increase as the duration grows due to more comprehensive contextual information. However, we can observe a performance decline from \"6~8 s\" to \">8 s\" for   The experimental results of different models are shown in Fig.  5 . We observe that both MCT-GFA and MCT-LFI exhibit performance gain over MCT, proving that the feature reconstruction from both local and global perspectives can improve the basic recognition ability and model robustness against feature missing. Specifically, MCT-GFA outperforms MCT-LFI on the complete testing data, while MCT-LFI surpasses MCT-GFA on the incomplete testing data. We believe that GFA can better capture the global semantics at relatively lower feature missing rates, while LFI can effectively relieve the performance decline as the missing rate grows. Consequently, MCT-HFR achieves the best performing by combining these two complementary modules together. For better comparison, we also present the results of TFR-Net and EMT-DLFR in Fig.  5 . We observe that even MCT can achieve competitive performance with TFR-Net, indicating that the collaborative modality reinforcements of MCT are beneficial to the model robustness.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Distance Metrics For Gfa.",
      "text": "As illustrated in Section 4.3, we employ CMD as the distance metric in GFA to measure the discrepancy between the global semantics of complete and incomplete views of data. In this subsection, we compare CMD with another three distance metrics, i.e., smooth L1 loss, cosine similarity, and JSD. The experimental results are depicted in Fig.  6 . Compared with the other three distance metrics, CMD exhibits the great advantage for enhancing the model robustness. Among the four metrics, smooth L1 loss turns out to be the worst performing metric, mainly due to the excessive constraint on the global semantic spaces. Besides, we conjecture that the relatively poor performance of cosine similarity can be due to the potential risk of shortcut learning  [20] . While JSD is based on mean (first raw moment) matching of statistical moments, CMD can explicitly match the higher order moments of the view-specific distributions. In this way, CMD helps to align the global semantics more comprehensively. Fig.  6 . Ablation results for the distance metrics of GFA on the IEMOCAP dataset. The scores (%) of UA, UF1 and WF1 are reported for the incomplete testing data.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Hyper-Parameter Analysis.",
      "text": "In this part, we investigate the effects of different stacked layers (ùëÅ ) in MCT, along with the controlling factors (ùõº and ùõΩ) in the objective loss function, respectively. From the results presented in Fig.  7 , we conclude the following observations:\n\n1) The performance of MCT-HFR significantly improves as ùëÅ grows from 2 to 4, indicating that the stacked layers can deepen the interactions across modalities. Nevertheless, MCT-HFR suffers from a performance decline as ùëÅ continues to increase. Considering the limited scale of the IEMOCAP dataset, we believe that this may be resulted from the overfitting issue.\n\n2) As illustrated in Section 5.3, grid searching is conducted to determine the optimal controlling factors, with ùõº = 0.4 and ùõΩ = 0.6. By fixing one factor and adjusting the other one, we observe that ùõº is more sensitive to the hyper-parameter tuning, with a considerable performance decline when ùõº equals 1. Considering the relatively larger values of L GFA , we speculate that the feature reconstruction loss may overwhelm the primary task loss with relatively higher ùõº. Consequently, the overall robustness of MCT-HFR may be improved by carefully balancing the importance of different losses in the objective loss function.  MCT-HFR at missing rate=0.5\n\nFig.  8 . Visualization for confusion matrices of MCT and MCT-HFR on the IEMOCAP dataset with feature missing rates of 0.0 and 0.5, respectively.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Visualization Analysis",
      "text": "In this subsection, we perform the following visualization experiments to qualitatively analyze the effectiveness of our proposed method.\n\n6.3.1 Visualization of Confusion Matrices. Fig.  8  visualizes the confusion matrices of MCT and MCT-HFR on both complete and incomplete testing set. We find that the \"happy\", \"angry\" and \"sad\" emotions are most likely to be confused with the \"neutral\" emotion, mainly due to the predominant proportion of \"neutral\" samples on the IEMOCAP dataset. By integrating HFR into MCT, the recognition accuracy of the \"neutral\" emotion improves from 57.16% to 65.35% on the complete testing data. Moreover, HFR considerably relieve the accuracy decline of all the emotion classes as the feature missing rate increases from 0.0 to 0.5. As a result, MCT-HFR outperforms MCT in both complete and incomplete data scenarios.\n\n6.3.2 Visualization of Embedding Spaces. Fig.  9  visualizes the distribution of the multimodal embeddings from the complete and incomplete views of testing data using t-SNE  [59] . We observe that the distributions of the \"complete\" and \"incomplete\" embeddings are very close at a relatively lower feature missing rate. As the missing rate grows, the deviations of specific distribution positions become more apparent. Nonetheless, the boundaries of overall distributions in the same modality are still very similar, indicating the superior feature reconstruction ability of MCT-HFR even in severely feature missing cases. MCT-HFR at missing rate=0.9 hv ha hl hv ha hl Fig.  9 . T-SNE visualization for the modality-wise latent representations of MCT-HFR from complete and incomplete views. The results are derived from the first-fold testing set of the IEMOCAP dataset with feature missing rates of 0.3, 0.6 and 0.9, respectively.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we present a unified framework, MCT-HFR, to promote the efficiency and robustness of multimodal emotion recognition in real-world scenarios. Instead of stacking vanilla attention units, MCT is specifically optimized for unaligned multimodal input, with the key innovation of modality-wise parameter sharing and parameter-free re-scaling mechanism. By collaborative modality reinforcements, MCT can surpass the most advanced Transformer-based models with noticeably less space and time complexity. During model training, HFR is introduced as an auxiliary supervision to further enhance the robustness of MCT from both the local and global perspectives. Experimental results on two benchmark datasets demonstrate the effectiveness of our method. Through quantitative and qualitative analysis, we first confirm that MCT-HFR consistently outperforms currently advanced approaches with various feature missing rates, achieving the best performance in both complete and incomplete data scenarios. Later, we perform extensive ablation studies to verify the roles of different modules in this framework. The visualization analysis further reveals the excellent feature reconstruction ability of MCT-HFR, indicating its superior robustness against random feature missing.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the overall workflow of MCT-HFR consists of two major branches: 1) Emotion",
      "page": 5
    },
    {
      "caption": "Figure 1: The overview framework of our proposed Modality-Collaborative Transformer with Hybried Feature",
      "page": 6
    },
    {
      "caption": "Figure 2: Detailed diagram of the Multimodal Re-scaled Attention Unit.",
      "page": 7
    },
    {
      "caption": "Figure 4: As the missing rate increases, the performance decline of our method is smaller than the",
      "page": 12
    },
    {
      "caption": "Figure 3: Performance variation curves of different approaches on the IEMOCAP dataset under different feature",
      "page": 14
    },
    {
      "caption": "Figure 4: Performance variation curves of different approaches on the MSP-IMPROV dataset under different",
      "page": 14
    },
    {
      "caption": "Figure 5: Ablation results for the components of HFR on the IEMOCAP dataset. The scores (%) of UA and UF1",
      "page": 16
    },
    {
      "caption": "Figure 5: We observe that both MCT-GFA",
      "page": 16
    },
    {
      "caption": "Figure 5: We observe that",
      "page": 17
    },
    {
      "caption": "Figure 6: Compared with the",
      "page": 17
    },
    {
      "caption": "Figure 6: Ablation results for the distance metrics of GFA on the IEMOCAP dataset. The scores (%) of UA, UF1",
      "page": 17
    },
    {
      "caption": "Figure 7: , we conclude the following observations:",
      "page": 17
    },
    {
      "caption": "Figure 7: Ablation results for different hyper-parameters ùëÅ, ùõºand ùõΩon the IEMOCAP dataset. The scores (%)",
      "page": 18
    },
    {
      "caption": "Figure 8: Visualization for confusion matrices of MCT and MCT-HFR on the IEMOCAP dataset with feature",
      "page": 18
    },
    {
      "caption": "Figure 8: visualizes the confusion matrices of MCT and",
      "page": 19
    },
    {
      "caption": "Figure 9: visualizes the distribution of the multimodal",
      "page": 19
    },
    {
      "caption": "Figure 9: T-SNE visualization for the modality-wise latent representations of MCT-HFR from complete and",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UA": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UA": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UA": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal video sentiment analysis using deep learning approaches, a survey",
      "authors": [
        "Sarah Abdu",
        "Ahmed Yousef",
        "Ashraf Salem"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "Deƒüer Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "3",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "OpenFace 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "6",
      "title": "BEiT: Bert pre-training of image Transformers",
      "authors": [
        "Hangbo Bao",
        "Li Dong",
        "Songhao Piao",
        "Furu Wei"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "Pattern recognition and machine learning",
      "authors": [
        "M Christopher",
        "Bishop",
        "M Nasser",
        "Nasrabadi"
      ],
      "year": "2006",
      "venue": "Pattern recognition and machine learning"
    },
    {
      "citation_id": "8",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Emerging properties in self-supervised vision Transformers",
      "authors": [
        "Mathilde Caron",
        "Hugo Touvron",
        "Ishan Misra",
        "Herv√© J√©gou",
        "Julien Mairal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "Minghai Chen",
        "Sen Wang",
        "Paul Liang",
        "Tadas Baltru≈°aitis",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "13",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Key-sparse Transformer for multimodal speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofeng Xing",
        "Xiangmin Xu",
        "Jichen Yang",
        "Jianxin Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Overcoming a Theoretical Limitation of Self-Attention",
      "authors": [
        "David Chiang",
        "Peter Cholak"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc V Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "19",
      "title": "Semisupervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Towards interpreting and mitigating shortcut learning behavior of NLU models",
      "authors": [
        "Mengnan Du",
        "Varun Manjunatha",
        "Rajiv Jain",
        "Ruchi Deshpande",
        "Franck Dernoncourt",
        "Jiuxiang Gu",
        "Tong Sun",
        "Xia Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "21",
      "title": "Deep hierarchical fusion with application in Sentiment Analysis",
      "authors": [
        "Efthymios Georgiou",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Georgios Paraskevopoulos, and Alexandros Potamianos. 2021. M 3 : MultiModal masking applied to sentiment analysis",
      "authors": [
        "Efthymios Georgiou"
      ],
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "23",
      "title": "Fast R-CNN",
      "authors": [
        "Ross Girshick"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "24",
      "title": "Online early-late fusion based on adaptive hmm for sign language recognition",
      "authors": [
        "Dan Guo",
        "Wengang Zhou",
        "Houqiang Li",
        "Meng Wang"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "25",
      "title": "Dynamically adjust word representations using unaligned multimodal information",
      "authors": [
        "Jiwei Guo",
        "Jiajia Tang",
        "Weichen Dai",
        "Yu Ding",
        "Wanzeng Kong"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "MM-Align: Learning optimal transport-based alignment dynamics for fast and accurate inference on missing modality sequences",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Min-Yen Kan",
        "Soujanya Poria"
      ],
      "year": "2022",
      "venue": "MM-Align: Learning optimal transport-based alignment dynamics for fast and accurate inference on missing modality sequences",
      "arxiv": "arXiv:2210.12798"
    },
    {
      "citation_id": "27",
      "title": "MISA: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "DeBERTa: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "30",
      "title": "Using self-supervised learning can improve model robustness and uncertainty",
      "authors": [
        "Dan Hendrycks",
        "Mantas Mazeika",
        "Saurav Kadavath",
        "Dawn Song"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "Multimodal Transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "TPFN: Applying outer product along time to multimodal sentiment analysis fusion on incomplete data",
      "authors": [
        "Binghua Li",
        "Chao Li",
        "Feng Duan",
        "Ning Zheng",
        "Qibin Zhao"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020"
    },
    {
      "citation_id": "34",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "Paul Pu Liang",
        "Zhun Liu",
        "Yao-Hung Hubert Tsai",
        "Qibin Zhao",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "NY Chen et al. Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "38",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "SMIL: Multimodal learning with severely missing modality",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Sergey Tulyakov",
        "Cathy Wu",
        "Xi Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP 2017-2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltru≈°aitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "42",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "43",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "MFAS: Multimodal fusion architecture search",
      "authors": [
        "Juan-Manuel P√©rez-R√∫a",
        "Valentin Vielzeuf",
        "St√©phane Pateux",
        "Moez Baccouche",
        "Fr√©d√©ric Jurie"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Deep contextualized word representations",
      "authors": [
        "Mark Matthew E Peters",
        "Mohit Neumann",
        "Matt Iyyer",
        "Christopher Gardner",
        "Kenton Clark",
        "Luke Lee",
        "Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "46",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnab√°s P√≥czos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Towards emotion-aware recommender systems: an affective coherence model based on emotion-driven behaviors",
      "authors": [
        "Marco Polignano",
        "Fedelucio Narducci",
        "Marco De Gemmis",
        "Giovanni Semeraro"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "48",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "49",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "50",
      "title": "Integrating multimodal information in large pretrained Transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amirali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "Ensemble of svm trees for multimodal emotion recognition",
      "authors": [
        "Sankaranarayanan Viktor Rozgiƒá",
        "Shirin Ananthakrishnan",
        "Rohit Saleem",
        "Rohit Kumar",
        "Prasad"
      ],
      "year": "2012",
      "venue": "Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "52",
      "title": "wav2vec: Unsupervised pre-Training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "53",
      "title": "Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "55",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "Luan Tran",
        "Xiaoming Liu",
        "Jiayu Zhou",
        "Rong Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Multimodal Transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "57",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "58",
      "title": "Multimodal embeddings from language models for emotion recognition in the wild",
      "authors": [
        "Shao-Yen",
        "Shrikanth Tseng",
        "Panayiotis Narayanan",
        "Georgiou"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "59",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "60",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "62",
      "title": "Unified contrastive learning in image-text-label space",
      "authors": [
        "Jianwei Yang",
        "Chunyuan Li",
        "Pengchuan Zhang",
        "Bin Xiao",
        "Ce Liu",
        "Lu Yuan",
        "Jianfeng Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "Recent trends in deep learning based natural language processing",
      "authors": [
        "Tom Young",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "64",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "65",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "67",
      "title": "Multiattention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "69",
      "title": "Central moment discrepancy (CMD) for domain-invariant representation learning",
      "authors": [
        "Werner Zellinger",
        "Thomas Grubinger",
        "Edwin Lughofer",
        "Thomas Natschl√§ger",
        "Susanne Saminger-Platz"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "70",
      "title": "Tag-assisted multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Tianyi Liu",
        "Jiantao Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "71",
      "title": "Deep partial multi-view learning",
      "authors": [
        "Changqing Zhang",
        "Yajie Cui",
        "Zongbo Han",
        "Joey Zhou",
        "Huazhu Fu",
        "Qinghua Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "72",
      "title": "Cross-modal contrastive learning for text-to-image generation",
      "authors": [
        "Han Zhang",
        "Jing Yu Koh",
        "Jason Baldridge",
        "Honglak Lee",
        "Yinfei Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "73",
      "title": "Feature fusion for multimodal emotion recognition based on deep canonical correlation analysis",
      "authors": [
        "Ke Zhang",
        "Yuanqing Li",
        "Jingyu Wang",
        "Zhen Wang",
        "Xuelong Li"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "74",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "75",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "Hao Zhou",
        "Minlie Huang",
        "Tianyang Zhang",
        "Xiaoyan Zhu",
        "Bing Liu"
      ],
      "year": "2007",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}