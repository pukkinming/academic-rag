{
  "paper_id": "2510.04251v1",
  "title": "Machine Unlearning In Speech Emotion Recognition Via Forget Set Alone",
  "published": "2025-10-05T15:44:15Z",
  "authors": [
    "Zhao Ren",
    "Rathi Adarshi Rammohan",
    "Kevin Scheck",
    "Tanja Schultz"
  ],
  "keywords": [
    "Speech emotion recognition",
    "machine unlearning",
    "privacy",
    "adversarial attacks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition aims to identify emotional states from speech signals and has been widely applied in humancomputer interaction, education, healthcare, and many other fields. However, since speech data contain rich sensitive information, partial data can be required to be deleted by speakers due to privacy concerns. Current machine unlearning approaches largely depend on data beyond the samples to be forgotten. However, this reliance poses challenges when data redistribution is restricted and demands substantial computational resources in the context of big data. We propose a novel adversarial-attack-based approach that fine-tunes a pre-trained speech emotion recognition model using only the data to be forgotten. The experimental results demonstrate that the proposed approach can effectively remove the knowledge of the data to be forgotten from the model, while preserving high model performance on the test set for emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human speech is an information-rich resource that can provide valuable insights into paralinguistic cues such as emotional states. The growing trend towards more naturalistic humanmachine interactions has made the ability to automatically understand and interpret emotions from speech more relevant than ever  [1] . Speech Emotion Recognition (SER) has been proposed to automatically identify emotional states from human speech using machine learning methods  [2] . SER is promising for manifold applications, such as car-driving  [3] , education  [4] , healthcare  [5] , etc. More recently, end-to-end models, e. g., Wav2Vec  [6]  and HuBERT  [7] , trained with selfsupervised learning on large-scale datasets have demonstrated a strong capability in extracting abstract emotion-relevant representations. Therefore, they can yield superior performance for SER when fine-tuned on emotional speech datasets. Such good performance promotes the applications of SER based on streaming speech data from various devices, including online platforms, wearable devices, and many others.\n\nNevertheless, the storage of speech data across multiple platforms and its use in various SER applications can elevate the risk of privacy leakage  [8] . Particularly, speech contains a variety of sensitive information usable for identifying the speakers, and inferring their emotions and mental health  [9, 10] . Leakage of such sensitive information can cause malicious usages and attacks. For instance, leakage of personal information, e. g., gender and demographic information, can cause the attacks to reduce the model performance in depression detection  [11] . In this context, users can request to delete partial speech data to protect their privacy. Meanwhile, even after the data is deleted, SER models still retain information derived from it. Therefore, it is crucial to effectively eliminate the knowledge that these models have learnt from the data.\n\nMachine unlearning has been proposed to train machine learning models for forgetting sensitive data samples, classes, and attributes from a pre-trained model with knowledge of a full dataset  [12] . Most machine unlearning approaches are model-agnostic to increase their generability for different SER models. Typical machine unlearning requires both the data to be erased (i. e., forget set) and the remaining data (i. e., remain set) in unlearning. Such a way can maintain the model performance on the original test data for SER. However, leveraging the remain set becomes challenging when other users restrict data redistribution or when the data has already been removed from storage. Additionally, using the remain set is expensive in storage and computing resources in the context of the large volume of speech streams nowadays.\n\nWe propose applying a machine unlearning approach using only the data to be forgotten for SER. The proposed approach can (i) train a model to forget the data to be erased, and (ii) maintain model performance using generated adversarial samples that capture the emotional class characteristics of the remain set. The experimental results demonstrate that the proposed approach can train an SER model to forget the erased data, resulting in a reduction of SER accuracy to approx. 0.0% for erased data, while still achieving adequate performance on the original test set.\n\nRelated Work. While machine unlearning concepts have been primarily developed for image-related tasks, only a few studies have explored their application in the speech processing domain. Machine unlearning in speech-related tasks mainly focuses on speaker attribute unlearning and instance unlearning. For speaker attribute unlearning, the study in  [13]   For instance unlearning, the negative gradient method, where the gradient direction is reversed to make the model forget selected samples, was found to be very efficient in a speech recognition task  [14] . The study in  [15]  employed the remaining data during unlearning and proposed a forgetting strategy based on curriculum learning to dynamically learn sample weights  [15] . However, as aforementioned, including the remain set can improve the model performance on the original test set, while it may increase the privacy risk of the remain set and require a large storage resource.\n\nFor SER, the study in  [12]  proposed a weight averaging method to combine multiple models, each of which is trained on a data shard. Differently, we focus on instance unlearning of one SER model with the data to be forgotten only, avoiding potential privacy risk of using the remain set. Inspired by the study for image classification  [16] , we maintain the model performance for SER using generated adversarial samples and elastic model weights.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "The training data are represented as D : {x, y}, where x is the speech samples and y denotes the emotion labels. The speech samples to be erased in machine unlearning are denoted as D e : {x e , y e }, and the remaining samples are represented as D r : {x r , y r }, i. e., D = D e ∪ D r , D e ∩ D r = ϕ. Given a pre-trained SER model f , the proposed machine unlearning method aims to fine-tune f not only to misclassify D e , but also to preserve the knowledge from D r . As shown in Fig.  1 , the misclassification of D e is achieved by random labelling (see Section 2.1). Adversarial attacks are applied to generate ad-versarial samples for preserving the model knowledge on D r . We further employ elastic model weights during unlearning to preserve more knowledge of D r (see Section 2.2).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Misclassification Of Forget Set",
      "text": "To misclassify D e , an effective way is to randomly relabel the data with wrong labels that are different from their original true ones  [15] . In this regard, we randomly generate wrong labels for data samples in D e . This random relabelling procedure leads to De : {x e , ŷe }, ŷi e ̸ = y i e , where i means the sample index. The model is then trained on De for misclassification using the loss function:\n\nwhere L CE denotes the cross-entropy loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preservation Of Knowledge From Remain Set",
      "text": "Random labelling in Section 2.1 can make the model focus on misclassifying the forget set. However, the model can also forget its knowledge learnt from the remain set before unlearning.\n\nUsing the remain set for unlearning has hidden risks of high computing resources and leakage of other speakers' privacy. In this regard, the following two methods are applied to preserve the model knowledge on the remain set without using it.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Adversarial Attacks",
      "text": "Adversarial attacks have been shown to have a strong attacking capability to make a model misclassify adversarial data with very poor performance  [17] . The adversarial data are usually well-designed and human-indistinguishable from the original real data. In addition to attacking a model, adversarial attacks have also been used for data augmentation, which outperformed typical augmentation methods like random noise  [17] .\n\nAdversarial data was also demonstrated to contain the feature information of the targeted labels in adversarial attacks  [18] .\n\nThe SER model is trained on generated adversarial data rather than the remain set in this work. Herein, we generate adversarial samples using targeted Projected Gradient Descent (PGD) attacks. Specifically, we randomly assign multiple targeted labels {ỹ\n\ne , ..., ỹi(M) e } for each sample x i e in D e , where M is the number of adversarial samples for each sample to be erased and ỹi(j) e ̸ = y i e . The PGD attack is a strong attack method with an iteration of Fast Gradient Signed Method (FGSM) in P steps  [17] . Given a targeted label ỹi(j) e , x ′i(j) e is firstly computed by adding x i e and a small random noise with values smaller than τ , where τ is a constant hyperparameter. The adversarial sample xi(j) is then calculated by PGD based on x ′i(j) e\n\n. In such a way, the adversarial samples will be different from each other, especially when ỹi(j)\n\nwhere ∇ is stands for the gradient, xi(j) e(1) = x ′i(j) e\n\n, and t = {1, ..., P }. Finally, we limit the data difference between adversarial data and real data to be small and invisible with xi(j) e(P ) = clip(x i(j) e(P ) , x i(j) e(P ) -τ, x i(j) e(P ) + τ ). Given the adversarial data and targeted labels, the model is trained by\n\n(3)",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Elastic Model Weights",
      "text": "Lifelong learning has been demonstrated its effectiveness in preserving the prior knowledge of a model in transfer learning  [19] . To further preserve model knowledge learnt from D r , the Elastic Weight Consolidation (EWC)  [20]  is employed to assign high constraints for important model parameters and low constraints for unimportant parameters. The importance of model parameters is calculated with Fisher matrix  [21] , which is approaching the second derivative of the loss function. Herein, as the model is expected to forget the data to be erased, we calculate the Fisher matrix via the cross-entropy loss between f (x e ) and ŷe . Therefore, high constraints are given to parameters important for misclassifying the forget set.\n\nThe Fisher matrix F is used in the loss function of EWC:\n\nwhere θ denotes the parameters of f , and θ * is the model parameters before machine unlearning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Training",
      "text": "To train a model which can forget the data to be erased and also remember the knowledge of the remain set, the loss function is combined by\n\nwhere λ 1 , λ 2 , and λ 3 are the coefficients for the three loss functions, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "The Database of Elicited Mood in Speech (DEMoS)  [22]    samples), validation (3, 024 samples), and test sets (3, 317 samples) in a speaker-independent setting. The detail of the data distribution can be found in  [17, 19] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "The pre-trained Wav2Vec 2.0 model  [6]  on the Librispeech corpus  [23]  is fine-tuned on the speech samples resampled in 16 kHz for 20 epochs with an \"Adam\" optimiser. The learning rate is experimentally set as 3E -5, and the batch size is Baselines. In machine unlearning, the training epoch is 15 and the optimiser setting is the same as that in fine-tuning. The adversarial-attack-based machine unlearning approach is compared to two baselines of machine unlearning. (i) Remainingdata-involved unlearning. The model is trained not only with random labelling, but also on the remain set D r , thereby the loss function is L mis + L CE (f (x r , y r ))  [15] . (ii) Random labelling. The model is trained with the forget set and randomly generated wrong labels using (1)  [15] .\n\nTable  1 : Model performance (UAR) on the forget set (D e ) and the validation/test set when training models on the train(ing) / train(ing)+val(idation) sets. The N denotes the number of speech samples to be forgotten. Apart from fine-tuning a Wav2Vec 2.0 model, the baselines of remaining-data-involved unlearning (Remain. Unl.) and random labelling (Ran. Lab.) are compared with the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best performance of the proposed approach and Ran. Lab. are compared with significant tests (*: p < 0.001 in a one-tailed z-test).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We compare the model performance when forgetting different numbers of speech samples that vary from 10 to 100 and when using different clip values τ varying from 0.1 to 0.7 in Fig.  2 . The proposed unlearning approaches are compared, including the approach using adversarial attacks, and the one with adversarial attacks and elastic model weights. The model performance of UAR is below 0.4 when τ = 0.1. This might be caused by the adversarial samples that are too close to the real samples in D e and cannot preserve the information of the remain set, when τ is too small. Correspondingly, the model performance also decreases when τ is too large as 0.7, since a large τ can result in the shift in class distribution or outliers.\n\nWhen comparing the model performance on N , the models perform mainly better when N is smaller. This is reasonable as it is more challenging to forget more data. When N = 30, 50, 100, the model performance across different τ values is not stable as those when N = 10. The reason might be that the generated adversarial samples sometimes have a shift in class distribution compared to the remaining data's distribution when N is large. When comparing the two proposed unlearning methods, the performance of models with adversarial attacks and elastic model weights is mostly better than the performance of models using adversarial attacks only. This indicates the effectiveness of the lifelong learning method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results Comparison",
      "text": "In Table  1 , we select the best results in the multiple settings of τ from the model performance on the validation set. The remaining-data-involved unlearning can perform comparably with the fine-tuned model when the number of forgotten samples (N ) varies from 10 to 100. This can be expected as the remain set is involved in unlearning. In comparison, the models trained only on the forget set perform worse than remainingdata-involved unlearning, since the amount of the training data decreases in unlearning and the model can forget the knowl-edge learnt from the remain set. Both remaining-data-involved unlearning and random labelling methods cause model performance on D e higher than the chance level (i. e., 0.143 for seven-class classification), which means the models cannot completely forget the knowledge learnt from D e .\n\nCompared to the baselines, the proposed two approaches using adversarial attacks can make the model forget the knowledge learnt from D e . Both approaches perform on D e with UARs not higher than 0.110. Both approaches also outperform the random labelling method, indicating the effectiveness of adversarial attacks in augmenting the data and simulating the data distribution of the remain set. When comparing the two proposed approaches, using elastic model weights can further improve the model performance. The reason can be that the EWC method regulates the models to only update unimportant model parameters. Finally, the models in the proposed two approaches perform worse when N increases, which is reasonable. We can still see significant improvement of the model performance compared to random labelling when N = 10, 30, 50 (p < 0.001 in a one-tailed z-test).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This work proposed a machine unlearning approach using adversarial attacks to protect data privacy hidden in emotional speech. The proposed approach utilises the forget set only, and generates adversarial samples to help the model preserve the knowledge learnt from the remain set. The weights of the speech emotion recognition model are updated by considering parameter importance during unlearning, thereby preserving more knowledge of the remain set. The experimental results indicate that the proposed approach can effectively train the model to forget the date to be erased and still perform well on unseen speakers' data for emotion recognition. In future efforts, we will validate the effectiveness of the approach in various speech emotion recognition models. We will also investigate the approach for forgetting specific emotional classes and sensitive information, such as gender.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed approach of machine unlearning using",
      "page": 2
    },
    {
      "caption": "Figure 2: Performance of machine unlearning models on the",
      "page": 3
    },
    {
      "caption": "Figure 2: The proposed unlearning approaches are compared,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "ABSTRACT"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "Speech emotion recognition aims to identify emotional states"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "from speech signals and has been widely applied in human-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "computer interaction, education, healthcare, and many other"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "fields. However, since speech data contain rich sensitive in-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "formation, partial data can be required to be deleted by speak-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "ers due to privacy concerns. Current machine unlearning ap-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "proaches largely depend on data beyond the samples to be"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "forgotten. However, this reliance poses challenges when data"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "redistribution is restricted and demands substantial computa-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "tional resources in the context of big data. We propose a novel"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "adversarial-attack-based approach that fine-tunes a pre-trained"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "speech emotion recognition model using only the data to be"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "forgotten. The experimental results demonstrate that the pro-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "posed approach can effectively remove the knowledge of the"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "data to be forgotten from the model, while preserving high"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "model performance on the test set for emotion recognition."
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "Index Terms— Speech emotion recognition, machine un-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "learning, privacy, adversarial attacks."
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "1.\nINTRODUCTION"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "Human speech is an information-rich resource that can provide"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "valuable insights into paralinguistic cues such as emotional"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "states. The growing trend towards more naturalistic human-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "machine interactions has made the ability to automatically"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "understand and interpret emotions from speech more relevant"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "than ever [1]. Speech Emotion Recognition (SER) has been"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "proposed to automatically identify emotional states from hu-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "man speech using machine learning methods [2].\nSER is"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "promising for manifold applications, such as car-driving [3],"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "education [4], healthcare [5], etc. More recently, end-to-end"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "models, e. g., Wav2Vec [6] and HuBERT [7], trained with self-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "supervised learning on large-scale datasets have demonstrated"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "a strong capability in extracting abstract emotion-relevant rep-"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "resentations. Therefore, they can yield superior performance"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "for SER when fine-tuned on emotional speech datasets. Such"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "good performance promotes the applications of SER based on"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "streaming speech data from various devices, including online"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "platforms, wearable devices, and many others."
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "This study is supported by the Deutsche Forschungsgemeinschaft (DFG,"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": ""
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "German Research Foundation) through the project “Silent Paralinguistics”"
        },
        {
          "Cognitive Systems Lab, University of Bremen, Germany": "with grant number 40301193."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speech samples": "to be forgotten",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "We further employ elastic model weights during unlearning to"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "preserve more knowledge of Dr (see Section 2.2)."
        },
        {
          "Speech samples": "Random \nAdversarial",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "labelling\nattacks",
          "versarial samples for preserving the model knowledge on Dr.": "2.1. Misclassification of Forget Set"
        },
        {
          "Speech samples": "Samples with",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "Adversarial",
          "versarial samples for preserving the model knowledge on Dr.": "To misclassify De, an effective way is to randomly relabel the"
        },
        {
          "Speech samples": "wrong labels\nsamples",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "data with wrong labels that are different from their original true"
        },
        {
          "Speech samples": "Elastic model",
          "versarial samples for preserving the model knowledge on Dr.": "ones [15]. In this regard, we randomly generate wrong labels"
        },
        {
          "Speech samples": "weight updating",
          "versarial samples for preserving the model knowledge on Dr.": "for data samples in De. This random relabelling procedure"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "ˆ"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "̸= yi\nleads to\nDe\n: {xe, ˆye}, ˆyi\ne\ne, where i means the sample"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "index. The model is then trained on ˆDe for misclassification"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "using the loss function:"
        },
        {
          "Speech samples": "Wrong",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "emotion",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "(1)\nLmis = LCE(f (xe), ˆye),"
        },
        {
          "Speech samples": "Fig. 1: The proposed approach of machine unlearning using",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "where LCE denotes the cross-entropy loss."
        },
        {
          "Speech samples": "randomly labelled and adversarial samples. The model weights",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "are selectively updated during training. Finally,\nthe speech",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "samples are misclassified as the generated random labels.",
          "versarial samples for preserving the model knowledge on Dr.": "2.2. Preservation of Knowledge from Remain Set"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "Random labelling in Section 2.1 can make the model focus on"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "misclassifying the forget set. However, the model can also for-"
        },
        {
          "Speech samples": "domain-adversarial training to identify gender-based violence",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "get its knowledge learnt from the remain set before unlearning."
        },
        {
          "Speech samples": "victim condition and forget speaker identification information.",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "Using the remain set for unlearning has hidden risks of high"
        },
        {
          "Speech samples": "For instance unlearning, the negative gradient method, where",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "computing resources and leakage of other speakers’ privacy. In"
        },
        {
          "Speech samples": "the gradient direction is reversed to make the model forget",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "this regard, the following two methods are applied to preserve"
        },
        {
          "Speech samples": "selected samples, was found to be very efficient in a speech",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "the model knowledge on the remain set without using it."
        },
        {
          "Speech samples": "recognition task [14]. The study in [15] employed the remain-",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "ing data during unlearning and proposed a forgetting strategy",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "based on curriculum learning to dynamically learn sample",
          "versarial samples for preserving the model knowledge on Dr.": "2.2.1. Adversarial Attacks"
        },
        {
          "Speech samples": "weights [15]. However, as aforementioned, including the re-",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "Adversarial attacks have been shown to have a strong attacking"
        },
        {
          "Speech samples": "main set can improve the model performance on the original",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "capability to make a model misclassify adversarial data with"
        },
        {
          "Speech samples": "test set, while it may increase the privacy risk of the remain",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "very poor performance [17]. The adversarial data are usually"
        },
        {
          "Speech samples": "set and require a large storage resource.",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "well-designed and human-indistinguishable from the original"
        },
        {
          "Speech samples": "For SER, the study in [12] proposed a weight averaging",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "real data. In addition to attacking a model, adversarial attacks"
        },
        {
          "Speech samples": "method to combine multiple models, each of which is trained",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "have also been used for data augmentation, which outper-"
        },
        {
          "Speech samples": "on a data shard. Differently, we focus on instance unlearning",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "formed typical augmentation methods like random noise [17]."
        },
        {
          "Speech samples": "of one SER model with the data to be forgotten only, avoiding",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "Adversarial data was also demonstrated to contain the feature"
        },
        {
          "Speech samples": "potential privacy risk of using the remain set. Inspired by the",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "information of the targeted labels in adversarial attacks [18]."
        },
        {
          "Speech samples": "study for\nimage classification [16], we maintain the model",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "The SER model\nis trained on generated adversarial data"
        },
        {
          "Speech samples": "performance for SER using generated adversarial samples and",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "rather than the remain set in this work. Herein, we generate"
        },
        {
          "Speech samples": "elastic model weights.",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "adversarial samples using targeted Projected Gradient Descent"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "(PGD) attacks. Specifically, we randomly assign multiple tar-"
        },
        {
          "Speech samples": "2. METHODOLOGY",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": ", ˜yi(2)\n, ..., ˜yi(M )\ngeted labels {˜yi(1)\n} for each sample xi\ne in De,"
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "where M is the number of adversarial samples for each sample"
        },
        {
          "Speech samples": "The training data are represented as D : {x, y}, where x is the",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "̸= yi\nto be erased and ˜yi(j)\ne. The PGD attack is a strong at-"
        },
        {
          "Speech samples": "speech samples and y denotes the emotion labels. The speech",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "tack method with an iteration of Fast Gradient Signed Method"
        },
        {
          "Speech samples": "samples to be erased in machine unlearning are denoted as",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "(FGSM) in P steps [17]. Given a targeted label ˜yi(j)\n, x′i(j)\nis"
        },
        {
          "Speech samples": "De : {xe, ye}, and the remaining samples are represented as",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "firstly computed by adding xi\ne and a small random noise with"
        },
        {
          "Speech samples": "Dr\n: {xr, yr},\ni. e., D = De ∪ Dr, De ∩ Dr = ϕ. Given a",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "values smaller than τ , where τ is a constant hyperparameter."
        },
        {
          "Speech samples": "pre-trained SER model f , the proposed machine unlearning",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "The adversarial sample ˜xi(j) is then calculated by PGD based"
        },
        {
          "Speech samples": "method aims to fine-tune f not only to misclassify De, but also",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "",
          "versarial samples for preserving the model knowledge on Dr.": "on x′i(j)\n. In such a way, the adversarial samples will be differ-"
        },
        {
          "Speech samples": "to preserve the knowledge from Dr. As shown in Fig. 1, the",
          "versarial samples for preserving the model knowledge on Dr.": ""
        },
        {
          "Speech samples": "misclassification of De is achieved by random labelling (see",
          "versarial samples for preserving the model knowledge on Dr.": "= ˜yi(k)\nent from each other, especially when ˜yi(j)\n, j ̸= k. In"
        },
        {
          "Speech samples": "Section 2.1). Adversarial attacks are applied to generate ad-",
          "versarial samples for preserving the model knowledge on Dr.": "the t-th iteration step of PGD, FGSM generates the adversarial"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sample through the model gradient:": "xi(j)"
        },
        {
          "sample through the model gradient:": ")),\n(2)\ne(t+1) = ˜xi(j)\ne(t) + σ ∗ sign(∇L(x′i(j)\ne(t) , ˜yi(j)"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "xi(j)"
        },
        {
          "sample through the model gradient:": "where ∇ is\nstands\nfor\nthe gradient,\n,\nand\ne(1) = x′i(j)"
        },
        {
          "sample through the model gradient:": "t = {1, ..., P }. Finally, we limit the data difference between"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "adversarial data and real data to be small and invisible with"
        },
        {
          "sample through the model gradient:": "e(P ) = clip(˜xi(j)\ne(P ), xi(j)\ne(P ) − τ, xi(j)\ne(P ) + τ ). Given the adver-"
        },
        {
          "sample through the model gradient:": "sarial data and targeted labels, the model is trained by"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "(3)\nLadv = LCE(f (˜xe), ˜ye)."
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "2.2.2. Elastic Model Weights"
        },
        {
          "sample through the model gradient:": "Lifelong learning has been demonstrated its effectiveness in"
        },
        {
          "sample through the model gradient:": "preserving the prior knowledge of a model in transfer learn-"
        },
        {
          "sample through the model gradient:": "ing [19]. To further preserve model knowledge learnt from Dr,"
        },
        {
          "sample through the model gradient:": "the Elastic Weight Consolidation (EWC) [20] is employed to"
        },
        {
          "sample through the model gradient:": "assign high constraints for important model parameters and"
        },
        {
          "sample through the model gradient:": "low constraints for unimportant parameters. The importance"
        },
        {
          "sample through the model gradient:": "of model parameters is calculated with Fisher matrix [21],"
        },
        {
          "sample through the model gradient:": "which is approaching the second derivative of the loss func-"
        },
        {
          "sample through the model gradient:": "tion. Herein, as the model is expected to forget the data to be"
        },
        {
          "sample through the model gradient:": "erased, we calculate the Fisher matrix via the cross-entropy"
        },
        {
          "sample through the model gradient:": "loss between f (xe) and ˆye. Therefore, high constraints are"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "given to parameters important for misclassifying the forget set."
        },
        {
          "sample through the model gradient:": "The Fisher matrix F is used in the loss function of EWC:"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "(cid:88) k\n(4)\nFk(θk − θ∗\nLewc =\nk)2,"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "where θ denotes the parameters of f , and θ∗\nis the model"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "parameters before machine unlearning."
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "2.2.3. Model Training"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "To train a model which can forget the data to be erased and also"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "remember the knowledge of the remain set, the loss function"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "is combined by"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "(5)\nL = λ1Lmis + λ2Ladv + λ3Lewc,"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "where λ1, λ2, and λ3 are the coefficients for the three loss"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "functions, respectively."
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "3. EXPERIMENTAL RESULTS"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "3.1. Database"
        },
        {
          "sample through the model gradient:": ""
        },
        {
          "sample through the model gradient:": "The Database of Elicited Mood in Speech (DEMoS) [22] is"
        },
        {
          "sample through the model gradient:": "used to validate the proposed approach. The DEMoS corpus"
        },
        {
          "sample through the model gradient:": "is an Italian speech dataset\nrecorded from 68 speakers (23"
        },
        {
          "sample through the model gradient:": "females and 45 males) with 9, 697 speech samples in total."
        },
        {
          "sample through the model gradient:": "The 332 neutral samples are not used in this study for class"
        },
        {
          "sample through the model gradient:": "balance.\nThe other 9, 365 samples are annotated in seven"
        },
        {
          "sample through the model gradient:": "classes,\nincluding Anger, Disgust, Fear, Guilt, Happiness,"
        },
        {
          "sample through the model gradient:": "Sadness, and Surprise. The data is split into training (3, 024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Model performance (UAR) on the forget set (De) and the validation/test set when training models on the train(ing) /": "train(ing)+val(idation) sets. The N denotes the number of speech samples to be forgotten. Apart from fine-tuning a Wav2Vec 2.0"
        },
        {
          "Table 1: Model performance (UAR) on the forget set (De) and the validation/test set when training models on the train(ing) /": "model, the baselines of remaining-data-involved unlearning (Remain. Unl.) and random labelling (Ran. Lab.) are compared with"
        },
        {
          "Table 1: Model performance (UAR) on the forget set (De) and the validation/test set when training models on the train(ing) /": "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best"
        },
        {
          "Table 1: Model performance (UAR) on the forget set (De) and the validation/test set when training models on the train(ing) /": "performance of the proposed approach and Ran. Lab. are compared with significant tests (*: p < 0.001 in a one-tailed z-test)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "performance of the proposed approach and Ran. Lab. are compared with significant tests (*: p < 0.001 in a one-tailed z-test)."
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": ""
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": ""
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Method"
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Fine-tune"
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": ""
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Remain. Unl. [15]"
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": ""
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Ran. Lab. [15]"
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Adv."
        },
        {
          "the proposed approaches using adversarial attacks (Adv.) and adversarial attacks + elastic model weights (Adv. + Ela.). The best": "Adv. Ela."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "Adv. Ela.\n0.000\n0.855*\n0.000\n0.814\n0.024\n0.796*\n0.000",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "0.731\n0.029\n0.739*\n0.020\n0.727*\n0.071\n0.429*\n0.110\n0.331"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "3.3. Ablation Study",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "edge learnt from the remain set. Both remaining-data-involved"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "unlearning and random labelling methods cause model per-"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "We compare the model performance when forgetting different",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "formance on De higher than the chance level (i. e., 0.143 for"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "numbers of\nspeech samples\nthat vary from 10 to 100 and",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "seven-class classification), which means the models cannot"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "when using different clip values τ varying from 0.1 to 0.7 in",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "completely forget the knowledge learnt from De."
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "Fig. 2. The proposed unlearning approaches are compared,",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "Compared to the baselines, the proposed two approaches"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "including the approach using adversarial attacks, and the one",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "using adversarial attacks can make the model forget the knowl-"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "with adversarial attacks and elastic model weights. The model",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "edge learnt from De. Both approaches perform on De with"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "performance of UAR is below 0.4 when τ = 0.1. This might",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "UARs not higher than 0.110. Both approaches also outper-"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "be caused by the adversarial samples that are too close to the",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "form the random labelling method, indicating the effectiveness"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "real samples in De and cannot preserve the information of the",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "of adversarial attacks in augmenting the data and simulating"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "remain set, when τ is too small. Correspondingly, the model",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "the data distribution of the remain set. When comparing the"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "performance also decreases when τ is too large as 0.7, since a",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "two proposed approaches, using elastic model weights can"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "large τ can result in the shift in class distribution or outliers.",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "further improve the model performance. The reason can be"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "When comparing the model performance on N , the mod-",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "that\nthe EWC method regulates the models to only update"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "els perform mainly better when N is smaller.\nThis is rea-",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "unimportant model parameters.\nFinally,\nthe models in the"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "sonable as it is more challenging to forget more data. When",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "proposed two approaches perform worse when N increases,"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "N = 30, 50, 100,\nthe model performance across different τ",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "which is reasonable. We can still see significant improvement"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "values is not stable as those when N = 10. The reason might",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "of the model performance compared to random labelling when"
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "be that\nthe generated adversarial samples sometimes have a",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": "N = 10, 30, 50 (p < 0.001 in a one-tailed z-test)."
        },
        {
          "Adv.\n0.000\n0.840\n0.000\n0.856*\n0.071\n0.773\n0.000": "shift in class distribution compared to the remaining data’s dis-",
          "0.747*\n0.024\n0.738\n0.030\n0.556\n0.011\n0.398\n0.051\n0.384": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "SP, 2021, pp. 141–159."
        },
        {
          "5. REFERENCES": "[1] Soumya Dutta and Sriram Ganapathy, “LLM supervised",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[13] Emma Reyner-Fuentes, Esther Rituerto-Gonzalez, and"
        },
        {
          "5. REFERENCES": "pre-training for multimodal emotion recognition in con-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Carmen Pelaez-Moreno,\n“Machine unlearning reveals"
        },
        {
          "5. REFERENCES": "versations,” in Proc. ICASSP, 2025, pp. 1–5.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "that the gender-based violence victim condition can be de-"
        },
        {
          "5. REFERENCES": "[2] Taiba Majid Wani, Teddy Surya Gunawan, Syed Asif Ah-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "tected from speech in a speaker-agnostic setting,” arXiv"
        },
        {
          "5. REFERENCES": "mad Qadri, Mira Kartiwi, and Eliathamby Ambikairajah,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "preprint arXiv:2411.18177, 2024, 65 pages."
        },
        {
          "5. REFERENCES": "“A comprehensive review of speech emotion recognition",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[14] Alkis Koudounas, Claudio Savelli, Flavio Giobergia, and"
        },
        {
          "5. REFERENCES": "systems,” IEEE access, vol. 9, pp. 47795–47814, 2021.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Elena Baralis, ““Alexa, can you forget me?” Machine un-"
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "learning benchmark in spoken language understanding,”"
        },
        {
          "5. REFERENCES": "[3] Yuhan Yang, Yan Zhang, Zhinan Zhong, Wan Dai, Yun-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "in Proc. INTERSPEECH, 2025, pp. 1768–1772."
        },
        {
          "5. REFERENCES": "fei Chen, and Mo Chen,\n“Intelligent\nin-car emotion",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "regulation interaction system based on speech emotion",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[15]\nJiali Cheng and Hadi Amiri,\n“Speech unlearning,”\nin"
        },
        {
          "5. REFERENCES": "recognition,” in Proc. ICCCR, 2024, pp. 142–150.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Proc. INTERSPEECH, 2025, pp. 3209–3213."
        },
        {
          "5. REFERENCES": "[4] Kiavash Bahreini, Rob Nadolski, and Wim Westera, “To-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[16] Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee,"
        },
        {
          "5. REFERENCES": "wards real-time speech emotion recognition for affective",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Taesup Moon, and Moontae Lee, “Learning to unlearn:"
        },
        {
          "5. REFERENCES": "e-learning,” Education and Information Technologies,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Instance-wise unlearning for pre-trained classifiers,” in"
        },
        {
          "5. REFERENCES": "vol. 21, no. 5, pp. 1367–1386, 2016.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Proc. AAAI, 2024, vol. 38, pp. 11186–11194."
        },
        {
          "5. REFERENCES": "[5] Siddique Latif, Junaid Qadir, Adnan Qayyum, Muham-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[17] Zhao Ren, Alice Baird, Jing Han, Zixing Zhang, and"
        },
        {
          "5. REFERENCES": "mad Usama, and Shahzad Younis, “Speech technology",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Bj¨orn W Schuller,\n“Generating and protecting against"
        },
        {
          "5. REFERENCES": "for healthcare: Opportunities, challenges, and state of",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "adversarial attacks for deep speech-based emotion recog-"
        },
        {
          "5. REFERENCES": "the art,” IEEE Reviews in Biomedical Engineering, vol.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "nition models,” in Proc. ICASSP, 2020, pp. 7184–7188."
        },
        {
          "5. REFERENCES": "14, pp. 342–356, 2020.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[18] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-"
        },
        {
          "5. REFERENCES": "[6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "gan Engstrom, Brandon Tran, and Aleksander Madry,"
        },
        {
          "5. REFERENCES": "and Michael Auli, “wav2vec 2.0: A framework for self-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "“Adversarial examples are not bugs, they are features,” in"
        },
        {
          "5. REFERENCES": "supervised learning of speech representations,”\n2020,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Proc. NeurIPS, 2019, vol. 32, pp. 1–12."
        },
        {
          "5. REFERENCES": "vol. 33, pp. 12449–12460.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[19] Zhao Ren, Jing Han, Nicholas Cummins, and Bj¨orn W"
        },
        {
          "5. REFERENCES": "[7] Wei-Ning Hsu et al., “HuBERT: Self-supervised speech",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Schuller, “Enhancing transferability of black-box adver-"
        },
        {
          "5. REFERENCES": "representation learning by masked prediction of hidden",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "sarial attacks via lifelong learning for speech emotion"
        },
        {
          "5. REFERENCES": "units,” IEEE/ACM Transactions on Audio, Speech, and",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "recognition models,” in Proc. INTERSPEECH, 2020, pp."
        },
        {
          "5. REFERENCES": "Language Processing, vol. 29, pp. 3451–3460, 2021.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "496–500."
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[20]\nJames Kirkpatrick et al., “Overcoming catastrophic for-"
        },
        {
          "5. REFERENCES": "[8] Thanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "getting in neural networks,” Proceedings of the National"
        },
        {
          "5. REFERENCES": "Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Academy of Sciences, vol. 114, no. 13, pp. 3521–3526,"
        },
        {
          "5. REFERENCES": "and Quoc Viet Hung Nguyen,\n“A survey of machine",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "2017."
        },
        {
          "5. REFERENCES": "unlearning,” arXiv preprint arXiv:2209.02299, 2022, 24",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "pages.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[21] Guillaume Desjardins, Karen Simonyan, Razvan Pas-"
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "canu, and Koray Kavukcuoglu,\n“Natural neural net-"
        },
        {
          "5. REFERENCES": "[9] Tiantian Feng and Shrikanth Narayanan, “Privacy and",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "works,” in Proc. NeurIPS, 2015, pp. 2071–2079."
        },
        {
          "5. REFERENCES": "utility preserving data transformation for speech emotion",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "recognition,” in Proc. ACII, 2021, pp. 1–7.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[22] Emilia Parada-Cabaleiro, Giovanni Costantini, Anton"
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Batliner, Maximilian Schmitt, and Bj¨orn W Schuller,"
        },
        {
          "5. REFERENCES": "[10] Zhao Ren, Jing Han, Nicholas Cummins, Qiuqiang Kong,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "“DEMoS: An Italian emotional speech corpus: Elicitation"
        },
        {
          "5. REFERENCES": "Mark D Plumbley,\nand Bj¨orn W. Schuller,\n“Multi-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "methods, machine learning, and perception,” Language"
        },
        {
          "5. REFERENCES": "instance learning for bipolar disorder diagnosis using",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "Resources and Evaluation, vol. 54, no. 2, pp. 341–383,"
        },
        {
          "5. REFERENCES": "weakly labelled speech data,” in Proc. DPH, 2019, pp.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "2020."
        },
        {
          "5. REFERENCES": "79–83.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "[23] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-"
        },
        {
          "5. REFERENCES": "[11] Basmah Alsenani, Anna Esposito, Alessandro Vinciarelli,",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "jeev Khudanpur, “LibriSpeech: An asr corpus based on"
        },
        {
          "5. REFERENCES": "and Tanaya Guha, “Assessing privacy risks of attribute",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "public domain audio books,” in Proc. ICASSP, 2015, pp."
        },
        {
          "5. REFERENCES": "inference attacks against speech-based depression detec-",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": "5206–5210."
        },
        {
          "5. REFERENCES": "tion system,” in Proc. ECAI, pp. 3797–3804. 2024.",
          "[12] Lucas Bourtoule et al., “Machine unlearning,” in Proc.": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "LLM supervised pre-training for multimodal emotion recognition in conversations",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2025",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "3",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "Taiba Majid Wani",
        "Teddy Surya Gunawan",
        "Syed Asif",
        "Ahmad Qadri",
        "Mira Kartiwi",
        "Eliathamby Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "Intelligent in-car emotion regulation interaction system based on speech emotion recognition",
      "authors": [
        "Yuhan Yang",
        "Yan Zhang",
        "Zhinan Zhong",
        "Wan Dai",
        "Yunfei Chen",
        "Mo Chen"
      ],
      "year": "2024",
      "venue": "Proc. ICCCR"
    },
    {
      "citation_id": "5",
      "title": "Towards real-time speech emotion recognition for affective e-learning",
      "authors": [
        "Kiavash Bahreini",
        "Rob Nadolski",
        "Wim Westera"
      ],
      "year": "2016",
      "venue": "Education and Information Technologies"
    },
    {
      "citation_id": "6",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Adnan Qayyum"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for selfsupervised learning of speech representations"
    },
    {
      "citation_id": "8",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "A survey of machine unlearning",
      "authors": [
        "Thanh Tam Nguyen",
        "Thanh Huynh",
        "Zhao Ren",
        "Le Phi",
        "Alan Wee-Chung Nguyen",
        "Hongzhi Liew",
        "Quoc Yin",
        "Hung Viet",
        "Nguyen"
      ],
      "year": "2022",
      "venue": "A survey of machine unlearning",
      "arxiv": "arXiv:2209.02299"
    },
    {
      "citation_id": "10",
      "title": "Privacy and utility preserving data transformation for speech emotion recognition",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "venue": "Proc. ACII, 2021"
    },
    {
      "citation_id": "11",
      "title": "Multiinstance learning for bipolar disorder diagnosis using weakly labelled speech data",
      "authors": [
        "Jing Zhao Ren",
        "Nicholas Han",
        "Qiuqiang Cummins",
        "Mark Kong",
        "Björn Plumbley",
        "Schuller"
      ],
      "year": "2019",
      "venue": "Proc. DPH"
    },
    {
      "citation_id": "12",
      "title": "Assessing privacy risks of attribute inference attacks against speech-based depression detection system",
      "authors": [
        "Basmah Alsenani",
        "Anna Esposito",
        "Alessandro Vinciarelli",
        "Tanaya Guha"
      ],
      "year": "2024",
      "venue": "Proc. ECAI"
    },
    {
      "citation_id": "13",
      "title": "Machine unlearning",
      "authors": [
        "Lucas Bourtoule"
      ],
      "venue": "Proc. SP, 2021"
    },
    {
      "citation_id": "14",
      "title": "Machine unlearning reveals that the gender-based violence victim condition can be detected from speech in a speaker-agnostic setting",
      "authors": [
        "Emma Reyner-Fuentes",
        "Esther Rituerto-Gonzalez",
        "Carmen Pelaez-Moreno"
      ],
      "year": "2024",
      "venue": "Machine unlearning reveals that the gender-based violence victim condition can be detected from speech in a speaker-agnostic setting",
      "arxiv": "arXiv:2411.18177"
    },
    {
      "citation_id": "15",
      "title": "Machine unlearning benchmark in spoken language understanding",
      "authors": [
        "Alkis Koudounas",
        "Claudio Savelli",
        "Flavio Giobergia",
        "Elena Baralis"
      ],
      "year": "2025",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "16",
      "title": "Speech unlearning",
      "authors": [
        "Jiali Cheng",
        "Hadi Amiri"
      ],
      "venue": "Proc. INTERSPEECH, 2025"
    },
    {
      "citation_id": "17",
      "title": "Learning to unlearn: Instance-wise unlearning for pre-trained classifiers",
      "authors": [
        "Sungmin Cha",
        "Sungjun Cho",
        "Dasol Hwang",
        "Honglak Lee",
        "Taesup Moon",
        "Moontae Lee"
      ],
      "year": "2024",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "18",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Alice Zhao Ren",
        "Jing Baird",
        "Zixing Han",
        "Björn Zhang",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Adversarial examples are not bugs, they are features",
      "authors": [
        "Andrew Ilyas",
        "Shibani Santurkar",
        "Dimitris Tsipras",
        "Logan Engstrom",
        "Brandon Tran",
        "Aleksander Madry"
      ],
      "year": "2019",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "20",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Jing Zhao Ren",
        "Nicholas Han",
        "Björn Cummins",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "21",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "James Kirkpatrick"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "22",
      "title": "Natural neural networks",
      "authors": [
        "Guillaume Desjardins",
        "Karen Simonyan",
        "Razvan Pascanu",
        "Koray Kavukcuoglu"
      ],
      "year": "2015",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "23",
      "title": "DEMoS: An Italian emotional speech corpus: Elicitation methods, machine learning, and perception",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Giovanni Costantini",
        "Anton Batliner",
        "Maximilian Schmitt",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "24",
      "title": "LibriSpeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    }
  ]
}