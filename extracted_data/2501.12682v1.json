{
  "paper_id": "2501.12682v1",
  "title": "Emoformer: A Text-Independent Speech Emotion Recognition Using Hybrid Transformer-Cnn Model",
  "published": "2025-01-22T07:00:15Z",
  "authors": [
    "Rashedul Hasan",
    "Meher Nigar",
    "Nursadul Mamun",
    "Sayan Paul"
  ],
  "keywords": [
    "Audio features",
    "Emotion recognition",
    "Textindependent",
    "MFCC",
    "X-vector",
    "Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition is a crucial area of research in human-computer interaction. While significant work has been done in this field, many state-of-the-art networks struggle to accurately recognize emotions in speech when the data is both speech and speaker-independent. To address this limitation, this study proposes, EmoFormer, a hybrid model combining CNNs (CNNs) with Transformer encoders to capture emotion patterns in speech data for such independent datasets. The EmoFormer network was trained and tested using the Expressive Anechoic Recordings of Speech (EARS) dataset, recently released by META. We experimented with two feature extraction techniques-MFCCs and x-vectors. The model was evaluated on different emotion sets comprising 5, 7, 10, and 23 distinct categories. The results demonstrate that the model achieved its best performance with five emotions, attaining an accuracy of 90%, a precision of 0.92, a recall, and an F1score of 0.91. However, performance decreased as the number of emotions increased, with an accuracy of 83% for seven emotions compared to 70% for the baseline network. This study highlights the effectiveness of combining CNNs and Transformer-based architectures for emotion recognition from speech, particularly when using MFCC features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) has become a critical area of research in human-computer interaction  [1] . The applications of SER range from healthcare to customer service and virtual assistants. Emotions play a fundamental role in the communication sector and the ability to automatically detect them from speech enhances the effectiveness of interactions between humans and machines.  [2]  However, SER presents unique challenges due to the variability in vocal tone, pitch, speed, and context. This can significantly affect the emotional content of speech  [3] .\n\nOver the years, various methods have been employed for SER. Early research relied on traditional machine learning approaches like Support Vector Machines (SVMs) and Hidden Markov Models (HMMs). In  [4] , the proposed system enhances SER by extracting features using discrete wavelet transform (DWT), pitch, energy, and zero crossing rate. Here, the decision tree classifier outperforms other classifiers like SVM and linear discriminant analysis (LDA). In  [5] , the study compared three HMM-based architectures for SER where SGMM-HMMs performed best on multiple datasets. However, deep learning has transformed the field, allowing for more automated and sophisticated feature extraction and classification techniques. In  [6] , a comparative study was carried out utilizing MFCC and modulation in spectral features for SER systems. In this study, RNN performed better than multiple linear regression (MLR) and SVM. Despite this, the RNN approach struggled with computational efficiency, particularly for large datasets. The work in  [7]  presents a deep recurrent neural network with a novel attention-based feature pooling strategy for improved automatic emotion recognition from speech. In  [8] , the study developed 1D and 2D convolutional neural networks (CNN) with long short-term memory (CNN-LSTM) networks to learn local and global emotion features from speech. The model outperformed traditional methods like deep belief network (DBN) and CNN. The authors in  [9] ,  [10]  proposed a lightweight CNN-based SER model with low computational complexity and high accuracy. More recently, transformer-based architectures have emerged, providing a powerful means to capture contextual relationships across entire speech sequences. In  [11] , the authors proposed a novel SER system using a cross-attention transformer to fuse raw waveform data, spectrogram, and MFCC features. However, the reliance on multiple input modalities increased the model's complexity and make it challenging to apply in real-time applications. The paper  [12]  introduced a modular End-to-End SER system leveraging self-supervised features and demonstrated its ability to achieve SOTA results using only the speech modality. However, it is limited by the lack of multitask learning and multimodal integration. The paper in  [13]  proposes a lightweight FCNN for speech emotion recognition, designed to be efficient for systems with limited hardware resources. Although it achieved competitive performance compared to state-of-the-art models, the model's smaller size makes it more suitable for embedded applications.\n\nDespite advancements in SER, most studies remain focused on either traditional machine learning methods or deep learning models that utilize single feature sets. While these approaches show effectiveness, they often struggle with speech and speaker variability and fail to fully capture the temporal and contextual dynamics of emotional speech. Variability in speech arises from differences in accents, speaking styles, and environmental factors. These limitations highlight the need for a more comprehensive approach that integrates diverse feature representations and advanced deep learning architectures.\n\nTo address these challenges, this study proposes a hybrid approach that leverages both MFCC and x-vector features, combined with CNN and transformer architectures.MFCC features effectively capture the spectral properties of speech and x-vector features provide robust speaker-independent representations. The integration of CNNs and transformer-based encoders enables the system to extract spatial features and model long-range temporal and contextual dependencies and addresses both speech variability and the dynamic nature of emotions.\n\nThe contributions of this work are twofold: first, the effective use of MFCC features to capture the spectral properties of speech. Second, the introduction of transformerbased encoders to model long-range contextual relationships and speaker variability in speaker-independent datasets. By fusing these diverse feature sets with advanced deep learning architectures, our system aims to significantly enhance the accuracy of emotion recognition in speech.\n\nThe organization of this paper is as follows: Section II offers a comprehensive overview of the data preprocessing and feature extraction methods employed. Section III details the development of the hybrid models implemented in this study. Part IV presents the results and analyzes the outcomes of emotion recognition. Lastly, Section V wraps up the paper by addressing the limitations encountered and suggesting possible directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology A. Dataset",
      "text": "In speech emotion recognition, state-of-the-art models often face challenges with text-independent datasets due to variability in spoken content. To tackle this issue, our study introduces a hybrid model using the Expressive Anechoic Recordings of Speech (EARS) dataset, designed for both subject-and textindependent emotion recognition.\n\nThe recently released EARS dataset, provided by META, includes 749 audio files from 107 speakers, amounting to 100 hours of clean, anechoic speech. It covers 23 distinct emotions, with 107 samples per emotion, all recorded at a sampling rate of 48 kHz. The dataset features high speaker diversity, spanning various ethnic backgrounds and age groups ranging from 18 to 75 years. Audio durations within the dataset vary significantly, with the shortest file lasting 1.92 seconds and the longest extending to 30.59 seconds, while the average file length is 14.51 seconds. This wide range of durations not only introduces natural variability but also enhances the diversity of speech samples, improving model robustness. Though the original dataset included 23 distinct emotions, we experimented with different sets of emotions in the context of this work. This even distribution helps prevent bias toward any particular emotion during the development of the recognition models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Data Augmentation",
      "text": "Due to the limited dataset size, data augmentation techniques were applied to the training set to enhance its diversity. To ensure consistency during augmentation, all audio files were resampled at 16,000 Hz. The speech signals were then time-stretched by factors of 0.9 and 1.1, simulating variations in speech delivery rate without altering the pitch. Additionally, the pitch was adjusted by ±2 semitones, introducing vocal pitch variations that naturally occur due to different user characteristics or emotional states. Lastly, all audio files were either padded or truncated to a fixed length of 15 seconds for uniformity.\n\nC. Feature Extraction 1) Mel-frequency cepstral coefficients: Mel-frequency cepstral coefficients (MFCC) are features used in speech processing to capture the most important aspects of the speech signal. MFCCs are computed by transforming a raw audio signal into a more compact representation based on how humans perceive sound.\n\nAt first, the signal x[n] is pre-emphasized through a filter to boost high frequencies:\n\nwhere 0.9 < α < 1.\n\nThe signal is then divided into overlapping frames, and each frame is multiplied by a window function. The Fourier Transform converts each frame from the time domain to the frequency domain, giving the magnitude spectrum:\n\nwhere 0 < m < M . Then, the spectrum is passed through filters spaced on the Mel scale:\n\nThe logarithm of the filter bank output is computed to mimic the ear's sensitivity to loudness. Finally, the Discrete Cosine Transform (DCT) is applied to the logarithm of the Melfiltered energies to decorrelate them and obtain the MFCCs. Here, the MFCC features for various emotions extracted in this work are illustrated in Fig.  1 . The features were derived from audio using 13 coefficients, which represent the spectral characteristics of the speech signal. These coefficients capture speech features over time, with each time frame associated with a set of MFCC values.\n\nTo enhance temporal pattern recognition, the extracted MFCC features were divided into overlapping segments, each consisting of 469-time frames with an overlap of 128 frames. This segmentation enables the model to effectively capture the temporal patterns critical for emotion recognition.\n\n2) X-vector Feature Extraction: X-vector extraction is a deep learning-based method commonly used for speaker and speech feature extraction. X-vectors provide a fixed-length representation of the audio segment by capturing essential features through a DNN and pooling statistical summaries across frames. The process involves training a deep neural network to capture speaker or speech-related information from audio. Given an input audio signal x[n], it is processed into overlapping frames represented by feature vectors:\n\nwhere T is the total number of frames, and each x T is a ddimensional feature vector.\n\nFeatures of Each frame are passed through a DNN, transforming them into higher-level embeddings as follows:\n\nwhere W (l) and b (l) are the weights and biases of the l-th layer, and f is a nonlinear activation function. The X-vector is then formed by concatenating the mean µ and standard deviation σ:\n\nFinally, the X-vector is further refined through fully connected layers:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Model Architecture",
      "text": "The proposed EmoFormer network features a hybrid architecture that combines CNNs and transformer encoders to capture both local and global patterns in audio data for emotion recognition.\n\nAs shown in Fig.  2 , the model was tested with two types of input features: MFCC and X-vector. For MFCC, the input shape was (50,469), while for X-vectors it was (512,1). The architecture begins with four convolutional layers. The first CNN layer applies 16 filters with a 5x5 kernel and ReLU activation, followed by batch normalization and max-pooling to reduce spatial dimensions. The second and third CNN layers apply 32 and 64 filters, respectively, each using a 3x3 kernel, followed by batch normalization and max-pooling. The final CNN layer retains 64 filters and is followed by global average pooling. A dense layer with 64 units is then used to reduce dimensionality before passing the features to the transformer block.  Following the CNN layers, a transformer encoder is applied. The encoder begins with layer normalization, followed by Multi-Head Attention with 8 heads to focus on different parts of the input sequence. Each attention head and the feedforward layer have a dimensionality of 128. A dropout layer with a rate of 0.2 is used to reduce overfitting, followed by a residual connection and another layer normalization. Both normalization layers use an epsilon value of 1 -6 . Afterward, a dense layer with ReLU activation is followed by a dropout layer and another dense layer. The output is flattened into a 1D vector to be fed into the final dense layer, which applies a softmax activation function to produce predictions for the 5 to 23 emotion classes. Table  I  outlines the layer-wise architecture of the CNN-Transformer model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Experimental Setup",
      "text": "In this work, various pre-processing techniques were applied to optimize data preparation for emotion recognition and improve model performance. Standardization was used to scale numerical features to a mean of 0 and a standard deviation of 1, ensuring that all features were on a similar scale and preventing features with larger magnitudes from dominating the learning process. The dataset was split into training and testing sets, with 70% (2,621 samples) used for training and 30% (1,124 samples) reserved for testing. Label encoding was applied to convert categorical variables into numerical values, assigning integer labels to each category. However, this approach may introduce an unintended ordinal relationship between categories.\n\nThe model was trained using the Adam optimizer and categorical cross-entropy as the loss function. For the MFCCbased model, early stopping was applied with patience of 10 epochs, and training was performed over 50 epochs with a batch size of 64. Similarly, the X-vector-based model used early stopping with patience of 5 epochs and was trained for 20 epochs with the same batch size. Both models monitored validation accuracy to control training and prevent overfitting, ensuring the models were effectively tuned to the data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Results",
      "text": "This section presents the proposed network's performance across different sets of emotions, evaluated using precision, recall, F1-score, and accuracy. Additionally, the model's performance is compared to a baseline network in terms of accuracy percentage.\n\nFigure  3  shows the confusion matrices for the proposed model when classifying seven emotions using MFCC and Xvector features. These matrices visually represent the model's classification performance, where diagonal values represent correct classifications and off-diagonal values indicate misclassifications. The MFCC-based model demonstrates strong performance in identifying emotions such as anger, fear, disappointment, and pain. The X-vector-based model also performs well in classifying anger and pain. However, the MFCC-based model consistently outperforms the X-vector model overall.\n\nTable  II  provides a detailed comparison of the performance of the SER model in classifying seven emotions using MFCC, X-vector, and combined MFCC+X-vector features. The results are presented for both the models trained without data augmentation and those trained with augmentation.\n\nFor models trained without augmentation, the MFCC-based model significantly outperforms the X-vector-based model. The MFCC model achieves a precision of 0.45, recall of 0.41, and F1-score of 0.41. In contrast, the X-vector model shows lower performance with a precision of 0.22, recall of 0.22, and F1-score of 0.21.\n\nWhen features are combined (MFCC+X-vector) without augmentation, the performance improves over X-vector alone, yielding a precision of 0.56, recall of 0.56, and F1-score of 0.55. However, it still lags behind the standalone MFCC model.\n\nFor models trained with augmentation, the MFCC model maintains its superior performance, with precision, recall, and F1-score values all reaching 0.83. The augmented X-vector model also improves the value of precision, recall, and F1scores with 0.74 each. But it still falls behind the augmented MFCC model. The combination of MFCC+X-vector features with augmentation improves performance to a precision of 0.80, recall of 0.80, and an F1-score of 0.80. Overall, as the MFCC features consistently outperformed the X-vector features, both with and without augmentation, MFCC features were used for the rest of the analysis in this study. The results obtained using MFCC features for different sets of emotions are presented in Table  III . As seen in the table, when evaluating five emotions-adoration, anger, fear, neutral, and sadness-the model achieved an accuracy of 90%, with a precision of 0.92, recall of 0.91, and an F1-score of 0.91, demonstrating strong performance. Next, two additional emotions-disappointment and pain-were introduced to assess the model's performance. In this case, the accuracy dropped to 83%, with an F1-score of 0.826, indicating that classification becomes more challenging as the number of emotions increases.\n\nWhen the set was expanded to include guilt, disgust, and distress, the model's performance decreased further, with accuracy falling to 72% and the F1-score to 0.719. Finally, when tasked with classifying 23 emotions, including a broader range such as amazement, confusion, serenity, and others, accuracy declined to 65%, and the F1-score dropped to 0.64. This reduction in performance could be attributed to the overlapping nature of acoustic features among the emotions. Additionally, the computational time for 7 emotions is around 1.34s, and we also calculated the model's Multiply Accumulate(MAC) value, which is 35041444. In summary, the results indicate that the model performs significantly better with a smaller set of emotions. As the number of emotions increases, the classification task becomes more complex, leading to lower accuracy and F1 scores.\n\nThis study compares the proposed network's SER performance against two baseline networks: one based on CNN  [14]  and another using a hybrid Transformer-LSTM architecture  [15] . The performance evaluation, conducted on seven emotions, is presented in Table  IV . The CNN model  [14]  achieved an accuracy of 70%, while the hybrid Transformer-LSTM model  [15]  performed significantly worse, with only 39% accuracy. In contrast, the proposed network showed promising results. When using Stacked GRU and Stacked LSTM layers, the model achieved 63% and 60% accuracy, respectively. Performance improved with a Transformer network incorporating Bi-LSTM layers, reaching 68% accuracy. The most significant improvement occurred when Bi-LSTM layers were replaced with CNN in the Transformer-BiLSTM model, resulting in an accuracy of 83%. Additionally, the EmoFormer network attained strong precision (0.8293), recall (0.8265), and F1score (0.8255). These results not only highlight the superior performance of the proposed model compared to the baseline architectures tested in this study but also show that it substantially outperforms models from previous research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "This study introduces a hybrid model, known as Emo-Former, for SER, combining CNN and Transformer encoders. While prior studies have relied on datasets that were not entirely text-independent, our approach uniquely utilizes a dataset that is fully text-independent. The model was evaluated using MFCC and x-vector features on an augmented speech dataset. The results demonstrate that the MFCC feature-based model consistently outperformed the x-vector model across various emotion sets. Notably, the model achieved a peak accuracy of 90% when classifying five emotions, although performance gradually declined as the number of emotions increased, reaching 65% for 23 emotions due to the complexity and overlap of acoustic features. Compared to baseline networks, including CNN and Transformer-LSTM architectures, the proposed model showed superior performance. It achieved 83% accuracy for seven emotions, along with high precision, recall, and F1-scores, demonstrating its effectiveness in emotion classification. Future work could focus on improving the classification of subtle emotions by incorporating multimodal data, such as integrating speech with visual cues like facial expressions or body language. Additionally, exploring more advanced Transformer architectures or ensemble models may help address performance declines as the number of emotions increases.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: MFCC features for different emotions such as Adortion, Sadness,",
      "page": 2
    },
    {
      "caption": "Figure 1: The features were derived",
      "page": 2
    },
    {
      "caption": "Figure 2: , the model was tested with two types",
      "page": 3
    },
    {
      "caption": "Figure 2: Architecture of the proposed EmoFormer network",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the confusion matrices for the proposed",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrix generated using a) MFCC and b) X-vector",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer": "Conv2D",
          "Kernel Size": "(5, 5)",
          "Input Shape": "(13, 469, 1)",
          "Output Shape": "(13, 469, 16)"
        },
        {
          "Layer": "Conv2D",
          "Kernel Size": "(3, 3)",
          "Input Shape": "(13, 469, 16)",
          "Output Shape": "(13, 469, 32)"
        },
        {
          "Layer": "Conv2D",
          "Kernel Size": "(3, 3)",
          "Input Shape": "(13, 469, 32)",
          "Output Shape": "(6, 234, 32)"
        },
        {
          "Layer": "Conv2D",
          "Kernel Size": "(3, 3)",
          "Input Shape": "(6, 234, 32)",
          "Output Shape": "(3, 117, 64)"
        },
        {
          "Layer": "Conv2D",
          "Kernel Size": "(3, 3)",
          "Input Shape": "(3, 117, 64)",
          "Output Shape": "(1, 58, 64)"
        },
        {
          "Layer": "Conv2D",
          "Kernel Size": "(3, 3)",
          "Input Shape": "(1, 58, 64)",
          "Output Shape": "(1, 58, 64)"
        },
        {
          "Layer": "Dense",
          "Kernel Size": "-",
          "Input Shape": "(1, 58, 64)",
          "Output Shape": "(64,)"
        },
        {
          "Layer": "Transformer Encoder",
          "Kernel Size": "-",
          "Input Shape": "(64,)",
          "Output Shape": "(64,)"
        },
        {
          "Layer": "Flatten",
          "Kernel Size": "-",
          "Input Shape": "(64,)",
          "Output Shape": "(64,)"
        },
        {
          "Layer": "Dense (Output)",
          "Kernel Size": "-",
          "Input Shape": "(64,)",
          "Output Shape": "(7,)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "2",
      "title": "Emotech: A multi-modal speech emotion recognition using multi-source low-level information with hybrid recurrent network",
      "authors": [
        "S Habib Avro",
        "T Taher",
        "N Mamun"
      ],
      "year": "2024",
      "venue": "International Conference on Signal Processing, Information, Communication and Systems"
    },
    {
      "citation_id": "3",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "5",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Automatic speech emotion recognition using machine learning",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "M Mbarki",
        "K Raoof",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Social Media and Machine Learning"
    },
    {
      "citation_id": "7",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "9",
      "title": "Deep-net: A lightweight cnn-based speech emotion recognition system using deep frequency features",
      "authors": [
        "T Anvarjon",
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "A text independent speech emotion recognition based on convolutional neural network",
      "authors": [
        "S Sarker",
        "K Akter",
        "N Mamun"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Electrical, Computer and Communication Engineering (ECCE)"
    },
    {
      "citation_id": "11",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Y He",
        "N Minematsu",
        "D Saito"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Lightsernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami",
        "B Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using convolution neural networks",
      "authors": [
        "K Chauhan",
        "K Sharma",
        "T Varma"
      ],
      "year": "2021",
      "venue": "2021 international conference on artificial intelligence and smart systems (ICAIS)"
    },
    {
      "citation_id": "15",
      "title": "Hybrid lstmtransformer model for emotion recognition from speech audio files",
      "authors": [
        "F Andayani",
        "L Theng",
        "M Tsun",
        "C Chua"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    }
  ]
}