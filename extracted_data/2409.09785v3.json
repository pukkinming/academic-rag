{
  "paper_id": "2409.09785v3",
  "title": "Large Language Model Based Generative Error Correction: A Challenge And Baselines For Speech Recognition, Speaker Tagging, And Emotion Recognition",
  "published": "2024-09-15T16:32:49Z",
  "authors": [
    "Chao-Han Huck Yang",
    "Taejin Park",
    "Yuan Gong",
    "Yuanchao Li",
    "Zhehuai Chen",
    "Yen-Ting Lin",
    "Chen Chen",
    "Yuchen Hu",
    "Kunal Dhawan",
    "Piotr Żelasko",
    "Chao Zhang",
    "Yun-Nung Chen",
    "Yu Tsao",
    "Jagadeesh Balam",
    "Boris Ginsburg",
    "Sabato Marco Siniscalchi",
    "Eng Siong Chng",
    "Peter Bell",
    "Catherine Lai",
    "Shinji Watanabe",
    "Andreas Stolcke"
  ],
  "keywords": [
    "Language modeling",
    "speech recognition postprocessing",
    "speaker tagging",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recognition (ASR) model. To explore new capabilities in language modeling for speech processing, we introduce the generative speech transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion recognition. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Early statistical ASR systems based on the noisy channel model were conceived as utilizing two model components: acoustic model and language model (LM)  [1] . First-pass decoding results could be subjected to postprocessing, or rescoring, to apply more powerful LMs or additional knowledge sources  [2] . With the introduction of end-to-end (E2E) ASR systems in the early 2020s, language modeling for post-ASR has become more complex, e.g., by also modeling the implicit internal LM of E2E ASR systems  [3] [4] [5] [6] ). With the advent of LLMs, however, post-ASR processing has become very attractive again, given the capacity of LLMs to model linguistic patterns, contextual influences, and even world knowledge as reflected in language. † Equal contribution.\n\nMore recently, LLMs and speech/language-model alignment methods have sparked considerable interest in new methods, such as cascaded LLM correction  [7] , for ASR and speech translation. LLM-based text-to-text generative error correction  [7, 8]  has shown accuracy improvements over baseline rescoring methods, even surpassing n-best oracle performance, by bringing external knowledge to bear in ASR  [9] , speech translation  [10] , and image captioning  [11, 12] .\n\nWhile a text-based ASR-LLM interface limits the richness of information utilized in post-processing, such as acousticprosodic expressions of speaker identity and paralinguistic properties, ASR outputs will still be sensitive to such information, especially when multiple hypotheses are output, effectively providing a text-based feature map that weakly reflects acoustic information  [7] .\n\nInspired by this observation, as well as by the early LLMbased studies cited, our challenge task aims to push research in two directions:  (1)  how large performance gains could be achieved by applying cascaded ASR-LLMs, and (2) how well cascaded LLMs could perform tasks beyond word-level transcription, such as recovering speaker and paralinguistic information. In other words, by leveraging LLMs, even text-only output from first-pass ASR system (such as available from a black-box API) might be enriched with paralinguistic and meta-information that is commonly thought to be encoded principally in acoustic-prosodic features. Fig.  1  illustrates the three challenge tasks, highlighting the assumption that ASR hypotheses in textual form contain sufficient implicit acoustic information to perform these tasks.\n\nThe short-term goal of the challenge is to introduce new ASR-LM tasks to the speech community that leverage the latest developments in LLM-based post-ASR text modeling, potentially benefiting the design of voice-interface LLM agents that use only text-based encodings. Through this initiative, we aim to advance the understanding of LLM capabilities in implicit acoustic modeling within the spoken language processing community. In the longer term, our goal is to highlight the importance of audio and other modalities for speech processing and understanding, setting the stage for future challenges that go beyond the initial text-only framework.\n\nBy promoting ongoing cooperation and knowledge sharing, we seek to catalyze significant progress in the field of LLM-based voice interfaces, driving innovation and opening new avenues for research, development, and practical applications of ASR-LLM. This includes error type analysis and cross-lingual variants of these tasks. In the following sections we give more background and introduce the specifics of the challenge tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Post-Asr Text Modeling",
      "text": "This challenge considers an agent-based LLM application scenario with a fixed ASR interface. The focus is on characterizing how LLMs can enhance speech processing by leveraging the textual N-best hypotheses without explicit encoding of acoustic information. Additionally, we encourage participants to \"push the limits of language modeling for ASR\" given a setup based on a black-box ASR interface, which would be readily accessible through APIs, at modest cost.\n\nThe success of probabilistic language modeling systems in ASR can be traced back to several influential tools and frameworks, including SRILM  [13] , CNTK  [14] , and the LM components within Kaldi  [15] . These tools have significantly contributed to the development and enhancement of speech recognition technology by providing robust methods for handling the complexities of speech processing. LLMs, on the other hand, have also benefited from democratized model in-ference (e.g., Claude) and open-source models (e.g., LLaMA) to establish end-to-end agent learning-based interfaces, such as AudioGPT  [16] . For instance, work on task-activating prompting (TAP)  [7]  illustrates that instruction-prompted LLMs for ASR can correct recognition errors by inferring phonetic confusions or grammatical variants from the ASRdecoded text.\n\nTo investigate this form of ASR-LLM pipeline, we introduce three tasks based on ASR-decoded text, which have been studied previously and were shown to benefit from combined acoustic and language modeling. In this challenge, participants can explore a training-free setup by optimizing instruction prompts for the speech tasks, or by hosting LLMs in their own compute environment. To examine the limits of the text-only modality for speech processing, we limit the first SLT challenge to text-to-text modeling without access to acoustic embeddings. The acoustic information will be accessed through ASR hypotheses ranked by acoustic confidence scores and error word-level or utterance-level error attributions. We hope this simple setup will entice researchers without a speech background to become active in speech processing through language modeling.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Open Topics In Llm-Based Speech Modeling",
      "text": "To avoid test set data that may have leaked into the pretrained LLMs, we prepare a non-public test set for each challenge subtask. While LLMs hold promise for post-ASR correction, they are not without problems. One concern is the potential for introducing biases reflected in the training data, which could affect the accuracy and fairness of the corrected transcripts. Additionally, LLMs could produce enriched text that diverges from the intended meaning or introduces new types of errors, necessitating novel error analysis methodologies. These potential issues highlight the need for ongoing research and future challenges that assess the reliability and effectiveness of LLMs in ASR postprocessing.\n\nCross-modal setups will be incorporated into future versions of the challenge by providing acoustic embeddings  [8]  or raw waveforms. By connecting the latest research and developments in speech language modeling with practical applications, the challenge promotes implementation, adoption, and understanding of cutting-edge language technologies, including in scarce-training or low-compute scenarios. Participants can expect to foster the development of innovative solutions at the intersection of speech and language technology.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Challenge Description",
      "text": "The GenSEC challenge at IEEE SLT 2024 consists of three tasks for post-ASR language modeling:\n\nThe goal of this task is to map from N-best Hypotheses to ground Truth transcriptions (H2T), similar to the setup in Yang et al.  [7] . The training set includes recognition scores from various pretrained end-to-end ASR models and Nbest hypotheses. Participants are allowed to use N-best hypotheses and their scores for re-ranking or generative correction to produce final transcriptions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "• Task 2: Post-Asr Speaker Tagging Correction",
      "text": "This task aims at correcting the speaker tags in the output of a speaker-attributed (multi-speaker) ASR system. Speaker tagging in Task 2 refers to the speaker indices or anonymized speaker names (e.g., \"speaker-A\", \"speaker-2\") used to identify who spoke which words. We will provide errorful speaker-attributed transcripts produced by a multi-speaker ASR system. Participants in Task 2 are asked to submit corrected versions of the transcripts with accurate speaker tagging. A metric that gauges both speaker tagging and ASR accuracy will be used for evaluation. Similar to the other tasks, the current version of the Track-2 challenge allows use of the text modality only.  improve ASR results, usually achieving good performance gains  [2, [17] [18] [19] . In this approach, an external LM is trained separately and used to re-score the N-best hypotheses generated by the ASR system. While text error correction (TEC) has been explored  [7, 20] , ASR error correction is distinct due to the variability and distinct patterns of spoken language  [21] . Neural models have been used widely with E2E models for text error correction or normalization  [22] [23] [24] . These models often use beam search to generate new estimates, and can usually handle text normalization and denormalization of spelling errors.\n\nMotivation: As shown in Fig.  2 , with Task 1 we aim to explore the limits of ASR-LLM error correction, as well as how best to utilize the ambiguity conveyed by N-best output.\n\nN-best dataset: The N-best open source corpus HyPoradise  [9]  will be made open-source under the MIT license. This includes HyPoradise training sets (316.8k pairs), development sets such as Librispeech-test-clean (2.6k pairs) and WSJ-dev93 (503 pairs), and evaluation sets including Librispeech-test-other (2.9k pairs) and WSJ-dev93 (333 pairs).\n\nBaseline: We provide pretrained 1st-pass and 2nd-pass models. The details of existing engineering pipelines are listed below. Training code has been released 1  and the pretrained LLaMA2-7B model 2  has been released.\n\nEvaluation: The challenge participants are allowed to apply their own 2nd-pass model to the provided ASR hypotheses decoded by beam search using Whisper.\n\nThe WER of the corrected hypotheses is used for evaluation. This WER is compared to two \"oracle\" WERs calculated from the N-best inputs, namely, 1) the lowest WER achievable by picking the best hypothesis from each N-best list, and 2) the compositional oracle method ocp: the achievable WER using \"all tokens\" in the N-best hypothesis list. The former can be viewed as a lower bound on re-ranking methods, while the latter denotes the lower bound using elements already occurring in the list. To understand the effect of text normalization (punctuation and capitalization, P&C) on ASR performance, both normalized and unnormalized (P&C) WERs are reported. Background: While the use of lexical cues in speaker diarization, speaker turn detection, and speaker segmentation has been explored previously, it is still less commonly used than acoustic-only speaker diarization. Early studies in this area, such as those presented in  [25, 26] , utilized linguistic patterns to identify speakers during the diarization process. Several studies have enhanced speaker segmentation and clustering accuracy by integrating ASR output to leverage lexical cues  [27] [28] [29] . Furthermore, lexical cues can be incorporated into speaker diarization by combining speaker turn probabilities based on both audio and text during the clustering phase  [30] . Alternatively, spoken words and speaker channels can be decoded jointly, thus utilizing lexical cues implicitly for diarization  [31, 32] .\n\nMore recently, the study presented in  [33]  introduced semantic information through neural embeddings generated by a spoken language processing (SLP) unit. Subsequently, a multimodal (audio-text) speaker change detector was proposed  [34] , along with a speaker error correction (SEC) system  [35]  based on a pretrained language model. Due to the recent popularity of LLMs, the multi-speaker ASR and speaker diarization community has also begun em-Table  3 . Task-2 cpWER (%) of the source files and the baseline system for text-only speaker recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "System Dev Eval",
      "text": "Source Transcript 24.65 28.45\n\nTask-2 Baseline 24.  54 28.37  ploying LLMs to enhance performance. One framework established for this purpose fine-tunes PaLM 2-S  [36]  to correct speaker diarization errors from GCP's Universal Speech Model  [37] , which uses Turn-to-Diarize  [28]  for speaker diarization  [38] . More recently, an ensemble of LLMs has been proposed to correct speaker diarization outputs  [39] . Motivation: As discussed for Task 1, LM rescoring for ASR has been widely studied and adopted, as external language models can be trained on relatively larger text-only datasets. Numerous studies have demonstrated the benefits of using LLMs for speaker diarization correction. Despite an abundance of research, there has been no standardized evaluation of multi-speaker error correction systems. We believe that our GenSEC Challenge Task 2 is timely in filling this gap. To lower the bar for entry, we focus on the text modality, excluding acoustic or visual modalities in this first round. Therefore, in Task 2, we employ the system proposed in  [40]  without utilizing acoustic information from the (acoustic-only) speaker diarization system.\n\nDatasets: The DiPCo  [41] , Mixer6  [42] , AMI  [43] , and CallHome American English Speech (CHAES)  [44]  corpora have been divided into training, development, and evaluation sets. The session names have been anonymized to prevent participants from exploiting the publicly available groundtruth data. Additionally, the evaluation scripts display the total word count and the count of erroneous words, to verify whether the output transcripts have altered the total number of words. In total, there are 222 training samples, 13 development samples, and 11 evaluation samples. The dataset is accessible through Huggingface.  3 Baseline: We generated the speaker-annotated transcripts from the system proposed in  [45]  as a baseline system,  4  based on NeMo  [46]  speaker diarization and NeMo ASR models. We provide the postprocessing method proposed in  [40] , where we replace the LLM with n-gram language models. Since the n-gram-based system shows similar results and has low computational demands, we use this n-gram baseline to gauge the performance of beam-search-based speaker tag correction. Fig.  3  shows how beam search decoding can correct the speaker tagging. To mask out acoustic information, the speaker probability values in Fig.  4  are all fixed at 0.96. Table  3  shows the accuracy of the baseline for development Evaluation: We employ concatenated minimum permutation word error rate (cpWER), as presented in  [47] . cp-WER is calculated by concatenating the speaker-wise transcripts for every label permutation and selecting the permutation that results in the lowest WER, using the open-source and publicly available MeetEval  [48]  multi-speaker ASR evaluation toolkit. Additionally, we provide a Hugging Face-style leaderboard 5 for challenge participants to upload and evaluate their submissions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task 3: Post-Asr Speech Emotion Recognition",
      "text": "Background: Text-based SER has advanced significantly over the past decade. However, its use in real-world applica-5 https://huggingface.co/spaces/GenSEC-LLM/task2_ speaker_tagging_leaderboard tions remains rare. One reason is that the majority of SER research relies on human annotation, i.e., manual transcripts. In contrast, even for elicited emotion corpora, transcripts from a state-of-the-art ASR system can result in high WERs  [49] , meaning that few findings obtained in the lab can be replicated in the wild. Moreover, SER on ASR transcripts is an understudied topic. Traditionally, researchers have considered confidence scores of recognized words  [50] , ASR error correction  [51, 52] , as well as fusion with audio information  [53, 54]  to mitigate the side effects of ASR errors. Still, there is a lack of comprehensive studies covering diverse situations (i.e., corpora, metrics, WERs, fusion techniques). With the rise of LLMs, it has become feasible to perform SER on ASR transcripts with simple prompting  [55] . This emerging approach, however, has not been established as a reliable solution given the uneven performance with different prompting templates and the general lack of explainability of LLM outputs.\n\nMotivation: Using text input only, without access to acoustic information, is a good starting point for exploring LLMs for SER, especially given that most LLMs are text-based. Insights gained about text-based SER, including handling of ASR errors, can be a foundation for future evaluations incorporating acoustic features. Potential future tasks could include ASR-error-robust multimodal fusion, enhancing word embeddings from ASR transcripts with discrete speech units, and developing ASR-integrated multimodal LLMs based on spoken language.\n\nDataset: We use the public IEMOCAP dataset  [56] . Speech transcripts from eleven ASR models (Wav2vec2, HuBERT, Whisper, etc.) are provided for each audio segment  [57] . We ask participants to predict four emotion classes: angry, happy (combined with excited), neutral, and sad, for each segment. All segments are presented in the order An exemplary data entry is shown in Fig.  5 , where need prediction indicates whether this utterance should be included in the prediction procedure. \"yes\" denotes the utterances labeled with the four emotion classes and \"no\" denotes all other utterances. Note that we have removed the utterances that have no human annotations. The key emotion indicates the emotion label of the utterance. The key id indicates the utterance ID, which is also the name of the audio file in IEMOCAP dataset. The ID is exactly the same as the raw ID in IEMOCAP. The key speaker indicates the speaker of the utterance. The key groundtruth indicates the original human transcription provided by IEMOCAP while the remaining ten keys indicate the ASR transcription generated by the various ASR models.\n\nBaseline: We provide two performance baselines with ASR transcripts from Whisper-tiny used as the text input: one with an LLM-based approach using GPT-3.5-turbo,  6  the other with a traditional approach based on a deep learning model. For the GPT-3.5-turbo approach, we performed zeroshot prediction with a context window of three (only previous utterances allowed), with code available 7  to participants as a reference. For the deep learning-based model, a two-layer feed-forward network was trained following the standard fivefold cross-validation of IEMOCAP. The first layer encodes RoBERTa output of dimension 768 into hidden states of dimension 128, and the second further encodes it into a dimension of 16. ReLU is used as the activation function between the layers. The dataset statistics and our baseline results are given in Table  4 .\n\nFor the method based on GPT3.5-turbo, the accuracy is 44.70% on the training set and 55.18% on the test set. This Evaluation: We use unweighted four-class accuracy (number of correctly predicted samples / total number of samples). We will release a training set and a test set. Participants can use the training set to develop their methods and tune hyperparameters. The test set does not come with emotion labels or ground-truth transcription and is strictly disallowed for use in model development. We will rank the models based on their accuracy on the test set, but will also further evaluate the models with a separate unpublished test set to assess generalization. Participants are free to use any LLM (such as GPT or LLaMA) or non-LLM methods (traditional text-based emotion classifiers) based on the provided ASR transcriptions. For fairness considerations, participants should not use any audio data (including audio waveforms or acoustic features) or transcribe speech using their own ASR model. Participants are allowed to use additional training datasets, as long as they are specified and publicly available, but they must not include IEMOCAP. To encourage innovation, we do not place any other restrictions on the methods used, as long as they are automated.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We have created the GenSEC challenge to probe the capabilities of large language models for post-processing of ASR outputs. By standardizing the tasks, datasets and metrics, we hope to create a community of researchers that will advance the state of the art in speech processing systems by loose coupling of off-the-shelf ASR systems and techniques based on generative LMs, such as instruction prompting, text generation and in-context learning. We propose three tasks that go beyond speech transcription correction and include text-based speaker diarization correction and emotion recognition. Unlike traditional models, LLMs provide the potential for improving these tasks by leveraging linguistic and world knowledge learned during pretraining, and by taking advantage of long conversational context.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the",
      "page": 1
    },
    {
      "caption": "Figure 1: The framework for LLM postprocessing of “text representation of speech” via ASR-decoded information, for three",
      "page": 2
    },
    {
      "caption": "Figure 2: Example Task 1 approach: post-speech recognition",
      "page": 3
    },
    {
      "caption": "Figure 2: , with Task 1 we aim to",
      "page": 3
    },
    {
      "caption": "Figure 3: shows how beam search decoding can cor-",
      "page": 4
    },
    {
      "caption": "Figure 4: are all fixed at 0.96.",
      "page": 4
    },
    {
      "caption": "Figure 3: Example Task 2 approach based on beam-search decoding for speaker tagging [40]",
      "page": 5
    },
    {
      "caption": "Figure 4: Dataflow for the Task 2 baseline.",
      "page": 5
    },
    {
      "caption": "Figure 5: Example of a data entry of Task 3.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6Tsinghua University\n7Academia Sinica": "yuangong@mit.edu\n{hucky, taejinp}@nvidia.com",
          "8University of Palermo\n9CMU\n10Uniphore": "yuanchao.li@ed.ac.uk\nandreas.stolcke@uniphore.com"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "ABSTRACT",
          "8University of Palermo\n9CMU\n10Uniphore": "More recently, LLMs and speech/language-model align-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "Given recent advances\nin generative AI\ntechnology,\na key",
          "8University of Palermo\n9CMU\n10Uniphore": "ment methods\nhave\nsparked\nconsiderable\ninterest\nin\nnew"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "question is how large language models (LLMs) can enhance",
          "8University of Palermo\n9CMU\n10Uniphore": "methods, such as cascaded LLM correction [7], for ASR and"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "acoustic modeling\ntasks\nusing\ntext\ndecoding\nresults\nfrom",
          "8University of Palermo\n9CMU\n10Uniphore": "speech translation.\nLLM-based text-to-text generative error"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "a\nfrozen,\npretrained\nautomatic\nspeech\nrecognition\n(ASR)",
          "8University of Palermo\n9CMU\n10Uniphore": "correction [7,8] has shown accuracy improvements over base-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "model.\nTo explore new capabilities\nin language modeling",
          "8University of Palermo\n9CMU\n10Uniphore": "line rescoring methods, even surpassing n-best oracle perfor-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "for\nspeech processing, we introduce the generative speech",
          "8University of Palermo\n9CMU\n10Uniphore": "mance, by bringing external knowledge to bear in ASR [9],"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "transcription error correction (GenSEC) challenge. This chal-",
          "8University of Palermo\n9CMU\n10Uniphore": "speech translation [10], and image captioning [11, 12]."
        },
        {
          "6Tsinghua University\n7Academia Sinica": "lenge comprises three post-ASR language modeling tasks: (i)",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "While a text-based ASR-LLM interface limits the richness"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "post-ASR transcription correction,\n(ii) speaker\ntagging, and",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "of information utilized in post-processing, such as acoustic-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "(iii) emotion recognition. These tasks aim to emulate future",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "prosodic expressions of\nspeaker\nidentity and paralinguistic"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "LLM-based\nagents\nhandling\nvoice-based\ninterfaces while",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "properties, ASR outputs will still be sensitive to such infor-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "remaining accessible to a broad audience by utilizing open",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "mation, especially when multiple hypotheses are output, ef-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "pretrained language models or agent-based APIs. We also",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "fectively providing a text-based feature map that weakly re-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "discuss insights from baseline evaluations, as well as lessons",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "flects acoustic information [7]."
        },
        {
          "6Tsinghua University\n7Academia Sinica": "learned for designing future evaluations.",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "Inspired by this observation, as well as by the early LLM-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "Index Terms— Language modeling, speech recognition",
          "8University of Palermo\n9CMU\n10Uniphore": "based studies cited, our challenge task aims to push research"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "postprocessing, speaker tagging, speech emotion recognition.",
          "8University of Palermo\n9CMU\n10Uniphore": "in two directions:\n(1) how large performance gains could be"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "achieved by applying cascaded ASR-LLMs, and (2) how well"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "1.\nINTRODUCTION",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "cascaded LLMs could perform tasks beyond word-level tran-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "Early statistical ASR systems based on the noisy channel",
          "8University of Palermo\n9CMU\n10Uniphore": "scription, such as recovering speaker and paralinguistic infor-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "model were conceived as utilizing two model components:",
          "8University of Palermo\n9CMU\n10Uniphore": "mation.\nIn other words, by leveraging LLMs, even text-only"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "acoustic model\nand language model\n(LM)\n[1].\nFirst-pass",
          "8University of Palermo\n9CMU\n10Uniphore": "output\nfrom first-pass ASR system (such as available from"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "decoding results\ncould be\nsubjected to postprocessing,\nor",
          "8University of Palermo\n9CMU\n10Uniphore": "a black-box API) might be enriched with paralinguistic and"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "rescoring,\nto apply more powerful LMs or additional knowl-",
          "8University of Palermo\n9CMU\n10Uniphore": "meta-information that\nis commonly thought\nto be encoded"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "edge sources [2]. With the introduction of end-to-end (E2E)",
          "8University of Palermo\n9CMU\n10Uniphore": "principally in acoustic-prosodic features. Fig. 1 illustrates the"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "ASR systems in the early 2020s, language modeling for post-",
          "8University of Palermo\n9CMU\n10Uniphore": "three challenge tasks, highlighting the assumption that ASR"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "ASR has become more complex, e.g., by also modeling the",
          "8University of Palermo\n9CMU\n10Uniphore": "hypotheses in textual form contain sufficient implicit acoustic"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "implicit\ninternal LM of E2E ASR systems [3–6]). With the",
          "8University of Palermo\n9CMU\n10Uniphore": "information to perform these tasks."
        },
        {
          "6Tsinghua University\n7Academia Sinica": "advent of LLMs, however, post-ASR processing has become",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "The short-term goal of the challenge is to introduce new"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "very attractive again, given the capacity of LLMs to model",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "ASR-LM tasks to the speech community that leverage the lat-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "linguistic\npatterns,\ncontextual\ninfluences,\nand\neven world",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "est developments in LLM-based post-ASR text modeling, po-"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "knowledge as reflected in language.",
          "8University of Palermo\n9CMU\n10Uniphore": ""
        },
        {
          "6Tsinghua University\n7Academia Sinica": "",
          "8University of Palermo\n9CMU\n10Uniphore": "tentially benefiting the design of voice-interface LLM agents"
        },
        {
          "6Tsinghua University\n7Academia Sinica": "†Equal contribution.",
          "8University of Palermo\n9CMU\n10Uniphore": "that use only text-based encodings. Through this initiative, we"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "representation of speech” via ASR-decoded information,\nfor\nthree"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "ference (e.g., Claude) and open-source models (e.g., LLaMA)"
        },
        {
          "task-instruction": "to establish end-to-end agent\nlearning-based interfaces, such"
        },
        {
          "task-instruction": "as AudioGPT [16].\nFor\ninstance, work on task-activating"
        },
        {
          "task-instruction": "prompting\n(TAP)\n[7]\nillustrates\nthat\ninstruction-prompted"
        },
        {
          "task-instruction": "LLMs\nfor ASR can correct\nrecognition errors by inferring"
        },
        {
          "task-instruction": "phonetic confusions or grammatical variants from the ASR-"
        },
        {
          "task-instruction": "decoded text."
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "To investigate this\nform of ASR-LLM pipeline, we in-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "troduce three tasks based on ASR-decoded text, which have"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "been studied previously and were shown to benefit from com-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "bined acoustic and language modeling. In this challenge, par-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "ticipants can explore a training-free setup by optimizing in-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "struction prompts for\nthe speech tasks, or by hosting LLMs"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "in their own compute environment.\nTo examine the limits"
        },
        {
          "task-instruction": "of the text-only modality for speech processing, we limit\nthe"
        },
        {
          "task-instruction": "first SLT challenge to text-to-text modeling without access"
        },
        {
          "task-instruction": "to acoustic embeddings.\nThe acoustic information will be"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "accessed through ASR hypotheses ranked by acoustic confi-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "dence scores and error word-level or utterance-level error at-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "tributions. We hope this simple setup will entice researchers"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "without a speech background to become active in speech pro-"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "cessing through language modeling."
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "2.2. Open Topics in LLM-based Speech Modeling"
        },
        {
          "task-instruction": ""
        },
        {
          "task-instruction": "To avoid test set data that may have leaked into the pretrained"
        },
        {
          "task-instruction": "LLMs, we prepare a non-public test set\nfor each challenge"
        },
        {
          "task-instruction": "subtask. While LLMs hold promise for post-ASR correction,"
        },
        {
          "task-instruction": "they are not without problems. One concern is the potential"
        },
        {
          "task-instruction": "for\nintroducing biases\nreflected in the training data, which"
        },
        {
          "task-instruction": "could affect\nthe accuracy and fairness of\nthe corrected tran-"
        },
        {
          "task-instruction": "scripts. Additionally, LLMs could produce enriched text that"
        },
        {
          "task-instruction": "diverges from the intended meaning or introduces new types"
        },
        {
          "task-instruction": "of errors, necessitating novel error analysis methodologies."
        },
        {
          "task-instruction": "These potential issues highlight the need for ongoing research"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "tasks for post-ASR language modeling:",
          "separately and used to re-score the N-best hypotheses gener-": "ated by the ASR system. While text error correction (TEC)"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "has been explored [7, 20], ASR error correction is distinct"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "• Task 1: Post-ASR Output Correction by LLM",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "due\nto the variability and distinct patterns of\nspoken lan-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "The goal of this task is to map from N-best Hypotheses to",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "guage [21]. Neural models have been used widely with E2E"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "ground Truth transcriptions (H2T), similar to the setup in",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "models\nfor\ntext\nerror\ncorrection or normalization [22–24]."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "Yang et al. [7]. The training set includes recognition scores",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "These models often use beam search to generate new esti-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "from various pretrained end-to-end ASR models and N-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "mates, and can usually handle text normalization and denor-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "best hypotheses. Participants are allowed to use N-best hy-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "malization of spelling errors."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "potheses and their scores for re-ranking or generative cor-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "Motivation: As shown in Fig. 2, with Task 1 we aim to"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "rection to produce final transcriptions.",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "explore the limits of ASR-LLM error correction, as well as"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "how best to utilize the ambiguity conveyed by N-best output."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "• Task 2: Post-ASR Speaker Tagging Correction",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "N-best dataset: The N-best open source corpus HyPo-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "This task aims at correcting the speaker\ntags in the out-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "radise [9] will be made open-source under\nthe MIT license."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "put of a speaker-attributed (multi-speaker) ASR system.",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "This\nincludes HyPoradise\ntraining sets\n(316.8k pairs), de-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "Speaker tagging in Task 2 refers to the speaker indices or",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "velopment\nsets\nsuch as Librispeech-test-clean (2.6k pairs)"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "anonymized speaker names (e.g., “speaker-A”, “speaker-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "and WSJ-dev93 (503 pairs),\nand evaluation sets\nincluding"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "2”) used to identify who spoke which words. We will",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "Librispeech-test-other\n(2.9k\npairs)\nand WSJ-dev93\n(333"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "provide\nerrorful\nspeaker-attributed\ntranscripts\nproduced",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "pairs)."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "by a multi-speaker ASR system.\nParticipants\nin Task 2",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "Baseline: We provide pretrained 1st-pass and 2nd-pass"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "are asked to submit corrected versions of\nthe transcripts",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "models.\nThe details of\nexisting engineering pipelines\nare"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "with accurate speaker tagging. A metric that gauges both",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "listed below. Training code has been released1 and the pre-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "speaker tagging and ASR accuracy will be used for evalu-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "trained LLaMA2-7B model2 has been released."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "ation. Similar to the other tasks, the current version of the",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "Evaluation: The challenge participants are allowed to ap-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "Track-2 challenge allows use of the text modality only.",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "ply their own 2nd-pass model\nto the provided ASR hypothe-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "ses decoded by beam search using Whisper."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "• Task 3: Post-ASR Speech Emotion Recognition",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "The WER of\nthe corrected hypotheses is used for eval-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "This task aims to achieve utterance-level speech emotion",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "uation. This WER is compared to two “oracle” WERs cal-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "recognition (SER) based on errorful ASR transcripts. Par-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "culated from the N-best\ninputs, namely, 1)\nthe lowest WER"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "ticipants will develop ASR error correction methods com-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "achievable by picking the best hypothesis from each N-best"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "bined with traditional deep-learning-based SER models,",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "list, and 2) the compositional oracle method ocp:\nthe achiev-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "design novel prompt\ntemplates utilizing LLMs for SER,",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "able WER using “all\ntokens” in the N-best hypothesis\nlist."
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "or utilize any other methods based on text\ninput. Partici-",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "The former can be viewed as a lower bound on re-ranking"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "pants are encouraged to use the conversation as context to",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "methods, while the latter denotes the lower bound using ele-"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "predict the emotion of a target utterance.",
          "separately and used to re-score the N-best hypotheses gener-": ""
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "ments already occurring in the list. To understand the effect of"
        },
        {
          "The GenSEC challenge at\nIEEE SLT 2024 consists of\nthree": "",
          "separately and used to re-score the N-best hypotheses gener-": "text normalization (punctuation and capitalization, P&C) on"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "hyp 1: I flies to"
        },
        {
          "GenSEC Task-1 ASR-LLM": "Pittsburgh from York\nNew LLM Loss"
        },
        {
          "GenSEC Task-1 ASR-LLM": "hyp 2: I fly to Pot Burger \nText Augmentation \nfrom New York"
        },
        {
          "GenSEC Task-1 ASR-LLM": "New Adapters\nPre-trained"
        },
        {
          "GenSEC Task-1 ASR-LLM": "LM"
        },
        {
          "GenSEC Task-1 ASR-LLM": "hyp 3: I fly to Pittsburgh \nAM"
        },
        {
          "GenSEC Task-1 ASR-LLM": "hyp 1: I fly to Pittsburgh \nfrom New Yorker"
        },
        {
          "GenSEC Task-1 ASR-LLM": "from New York"
        },
        {
          "GenSEC Task-1 ASR-LLM": "Fusion Techniques"
        },
        {
          "GenSEC Task-1 ASR-LLM": "Tokenizer"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "Fig. 2.\nExample Task 1 approach:\npost-speech recognition"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "error correction with different techniques on LLMs."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "improve ASR results, usually achieving good performance"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "gains [2, 17–19].\nIn this approach, an external LM is trained"
        },
        {
          "GenSEC Task-1 ASR-LLM": "separately and used to re-score the N-best hypotheses gener-"
        },
        {
          "GenSEC Task-1 ASR-LLM": "ated by the ASR system. While text error correction (TEC)"
        },
        {
          "GenSEC Task-1 ASR-LLM": "has been explored [7, 20], ASR error correction is distinct"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "due\nto the variability and distinct patterns of\nspoken lan-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "guage [21]. Neural models have been used widely with E2E"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "models\nfor\ntext\nerror\ncorrection or normalization [22–24]."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "These models often use beam search to generate new esti-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "mates, and can usually handle text normalization and denor-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "malization of spelling errors."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "Motivation: As shown in Fig. 2, with Task 1 we aim to"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "explore the limits of ASR-LLM error correction, as well as"
        },
        {
          "GenSEC Task-1 ASR-LLM": "how best to utilize the ambiguity conveyed by N-best output."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "N-best dataset: The N-best open source corpus HyPo-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "radise [9] will be made open-source under\nthe MIT license."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "This\nincludes HyPoradise\ntraining sets\n(316.8k pairs), de-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "velopment\nsets\nsuch as Librispeech-test-clean (2.6k pairs)"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "and WSJ-dev93 (503 pairs),\nand evaluation sets\nincluding"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "Librispeech-test-other\n(2.9k\npairs)\nand WSJ-dev93\n(333"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "pairs)."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "Baseline: We provide pretrained 1st-pass and 2nd-pass"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "models.\nThe details of\nexisting engineering pipelines\nare"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "listed below. Training code has been released1 and the pre-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "trained LLaMA2-7B model2 has been released."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "Evaluation: The challenge participants are allowed to ap-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "ply their own 2nd-pass model\nto the provided ASR hypothe-"
        },
        {
          "GenSEC Task-1 ASR-LLM": "ses decoded by beam search using Whisper."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "The WER of\nthe corrected hypotheses is used for eval-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "uation. This WER is compared to two “oracle” WERs cal-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "culated from the N-best\ninputs, namely, 1)\nthe lowest WER"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "achievable by picking the best hypothesis from each N-best"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "list, and 2) the compositional oracle method ocp:\nthe achiev-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "able WER using “all\ntokens” in the N-best hypothesis\nlist."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "The former can be viewed as a lower bound on re-ranking"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "methods, while the latter denotes the lower bound using ele-"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "ments already occurring in the list. To understand the effect of"
        },
        {
          "GenSEC Task-1 ASR-LLM": "text normalization (punctuation and capitalization, P&C) on"
        },
        {
          "GenSEC Task-1 ASR-LLM": "ASR performance, both normalized and unnormalized (P&C)"
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "WERs are reported."
        },
        {
          "GenSEC Task-1 ASR-LLM": ""
        },
        {
          "GenSEC Task-1 ASR-LLM": "1https://github.com/Hypotheses-Paradise/"
        },
        {
          "GenSEC Task-1 ASR-LLM": "Hypo2Trans"
        },
        {
          "GenSEC Task-1 ASR-LLM": "2https://huggingface.co/GenSEC-LLM."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Task-1 dataset statistics: number of hypothesis-",
      "data": [
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "line system for text-only speaker recognition."
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "train",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "System"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "10.43",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "Source Transcript"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "9.61",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "Task-2 Baseline"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "9.90",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "9.21",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "ploying LLMs to enhance performance. One framework es-"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "8.62",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "tablished for"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "rect speaker diarization errors from GCP’s Universal Speech"
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "8.71",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": ""
        },
        {
          "Table 1. Task-1 WER (%) of post-ASR LM correction on the": "",
          "Table 3. Task-2 cpWER (%) of the source files and the base-": "Model [37], which uses Turn-to-Diarize [28] for speaker di-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Task-1 dataset statistics: number of hypothesis-",
      "data": [
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "ASR has been widely studied and adopted, as external"
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "guage models can be trained on relatively larger"
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "of using LLMs\nfor\nspeaker diarization correction."
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "an abundance of\nresearch,"
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "this gap.\nTo lower\nthe bar"
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "round.\nTherefore,"
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        },
        {
          "proposed to correct speaker diarization outputs [39].": "(acoustic-only) speaker diarization system."
        },
        {
          "proposed to correct speaker diarization outputs [39].": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "Fig. 4.\nDataflow for\nthe Task 2 baseline.\nNote\nthat\nthe"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "acoustic-only diarization probability values are set\nto fixed"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "values."
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "and evaluation sets."
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "Evaluation: We employ concatenated minimum permu-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "tation word error\nrate (cpWER), as presented in [47].\ncp-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "WER is calculated by concatenating the speaker-wise tran-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "scripts for every label permutation and selecting the permuta-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "tion that results in the lowest WER, using the open-source and"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "publicly available MeetEval [48] multi-speaker ASR evalua-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "tion toolkit. Additionally, we provide a Hugging Face-style"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "leaderboard5\nfor challenge participants to upload and evalu-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "ate their submissions."
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "4.3. Task 3: Post-ASR Speech Emotion Recognition"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "Background:\nText-based SER has\nadvanced significantly"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "over the past decade. However,\nits use in real-world applica-"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": ""
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "5https://huggingface.co/spaces/GenSEC-LLM/task2_"
        },
        {
          "Fig. 3. Example Task 2 approach based on beam-search decoding for speaker tagging [40]": "speaker_tagging_leaderboard"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Task-3 dataset statistics and baseline unweighted",
      "data": [
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "accuracies (%)."
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "Training set\nTest set"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "Number of samples (all)\n5,525\n4,730"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "Number of samples (four emotions)\n2,577\n2,923"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "Baseline accuracy (GPT3.5-turbo)\n44.70\n55.18"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "Baseline accuracy (traditional)\n62.34\n51.08"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "significant discrepancy is\nreasonable since the training set"
        },
        {
          "Table 4.\nTask-3 dataset\nstatistics and baseline unweighted": "contains scripted dialogs (which may not align with emotion"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Task-3 dataset statistics and baseline unweighted",
      "data": [
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "significant discrepancy is\nreasonable since the training set"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "contains scripted dialogs (which may not align with emotion"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "labels), while the test set\nis more spontaneous.\nFor\nthe tra-"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "ditional deep learning approach,\nthe accuracy is 62.34% on"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "the training set and 51.08% on the test set. This discrepancy"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "is also plausible, considering duplicate textual scripts in the"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "training set, which results in overlap between the training and"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "development subsets of the training set."
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "Evaluation: We\nuse\nunweighted\nfour-class\naccuracy"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "(number of\ncorrectly predicted samples\n/\ntotal number of"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "samples). We will release a training set and a test set. Par-"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "ticipants can use the training set\nto develop their methods"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "and tune hyperparameters. The test set does not come with"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "emotion labels or ground-truth transcription and is\nstrictly"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "disallowed for use in model development. We will rank the"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "models based on their accuracy on the test set, but will also"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "further evaluate the models with a separate unpublished test"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "set\nto assess generalization. Participants are free to use any"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "LLM (such as GPT or LLaMA) or non-LLM methods (tradi-"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "tional\ntext-based emotion classifiers) based on the provided"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "ASR transcriptions. For fairness considerations, participants"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "should not use any audio data (including audio waveforms or"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "acoustic features) or transcribe speech using their own ASR"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "model.\nParticipants are allowed to use additional\ntraining"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "datasets, as long as they are specified and publicly available,"
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": ""
        },
        {
          "Baseline accuracy (traditional)\n62.34\n51.08": "but\nthey must not\ninclude IEMOCAP. To encourage innova-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "ankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei"
        },
        {
          "6. REFERENCES": "[1]\nFrederick Jelinek, “Continuous speech recognition by statisti-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Huang,\nJinglin Liu,\net al.,\n“AudioGPT: Understanding and"
        },
        {
          "6. REFERENCES": "cal methods,” Proceedings of the IEEE, 1976.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "generating speech, music, sound, and talking head,”\nin AAAI,"
        },
        {
          "6. REFERENCES": "[2] M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "2024."
        },
        {
          "6. REFERENCES": "and J. R. Rohlicek, “Integration of diverse recognition method-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[17] Andreas Stolcke, Barry Chen, Horacio Franco, Venkata Ra-"
        },
        {
          "6. REFERENCES": "ologies through reevaluation of N-best sentence hypotheses,”",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "mana Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Ka-"
        },
        {
          "6. REFERENCES": "in Workshop on Speech and Natural Language, 1991.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "trin Kirchhoff, Arindam Mandal, Nelson Morgan, Xin Lei,"
        },
        {
          "6. REFERENCES": "[3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "et al.,\n“Recent\ninnovations in speech-to-text\ntranscription at"
        },
        {
          "6. REFERENCES": "Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "IEEE Transactions on Audio, Speech, and\nSRI-ICSI-UW,”"
        },
        {
          "6. REFERENCES": "Li,\nand Yifan Gong,\n“Internal\nlanguage model\nestimation",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Language Processing, 2006."
        },
        {
          "6. REFERENCES": "for domain-adaptive end-to-end speech recognition,”\nin SLT.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "IEEE, 2021.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[18] Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran,\nand"
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Stanley Chen,\n“Bidirectional\nrecurrent neural network lan-"
        },
        {
          "6. REFERENCES": "[4] Mohammadreza Ghodsi, Xiaofeng Liu, James Apfel, Rodrigo",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "guage models for automatic speech recognition,”\nin ICASSP."
        },
        {
          "6. REFERENCES": "Cabrera, and Eugene Weinstein,\n“Rnn-transducer with state-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "IEEE, 2015."
        },
        {
          "6. REFERENCES": "less prediction network,” in ICASSP. IEEE, 2020.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[19] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide,"
        },
        {
          "6. REFERENCES": "[5] Yukun Ma, Chong Zhang, Qian Chen, Wen Wang, and Bin",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Michael L Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey"
        },
        {
          "6. REFERENCES": "Ma, “Tuning large language model for speech recognition with",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Zweig, “Toward human parity in conversational speech recog-"
        },
        {
          "6. REFERENCES": "mixed-scale re-tokenization,” IEEE Signal Processing Letters,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "IEEE/ACM Transactions on Audio, Speech, and Lan-\nnition,”"
        },
        {
          "6. REFERENCES": "2024.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "guage Processing, 2017."
        },
        {
          "6. REFERENCES": "[6] Yu Yu, Chao-Han Huck Yang,\nJari Kolehmainen,\net\nal.,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "“Low-rank adaptation of\nlarge language model\nrescoring for",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[20] Daniel Dahlmeier and Hwee Tou Ng,\n“Better evaluation for"
        },
        {
          "6. REFERENCES": "parameter-efficient speech recognition,” in ASRU. IEEE, 2023,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "grammatical error correction,” in NAACL-HLT, 2012."
        },
        {
          "6. REFERENCES": "pp. 1–8.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[21] Al¨ena Aks¨enova, Daan van Esch, James Flynn, and Pavel Go-"
        },
        {
          "6. REFERENCES": "[7] Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "lik, “How might we create better benchmarks for speech recog-"
        },
        {
          "6. REFERENCES": "Ivan Bulyko, and Andreas Stolcke, “Generative speech recog-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "nition?,” in 1st Workshop on Benchmarking: Past, Present and"
        },
        {
          "6. REFERENCES": "nition error correction with large language models and task-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Future, 2021."
        },
        {
          "6. REFERENCES": "activating prompting,” in ASRU. IEEE, 2023.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[22] Hao Zhang, Richard Sproat, Axel H Ng, Felix Stahlberg, Xi-"
        },
        {
          "6. REFERENCES": "[8]\nSrijith Radhakrishnan, Chao-Han Yang, Sumeer Khan, Ro-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "aochang Peng, Kyle Gorman, and Brian Roark, “Neural mod-"
        },
        {
          "6. REFERENCES": "hit Kumar, Narsis Kiani, David Gomez-Cabrero, and Jesper",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "els of\ntext normalization for speech applications,” Computa-"
        },
        {
          "6. REFERENCES": "Tegn´er,\n“Whispering LLaMA: A cross-modal generative er-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "tional Linguistics, 2019."
        },
        {
          "6. REFERENCES": "ror correction framework for speech recognition,” in EMNLP,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[23]\nJinxi Guo, Tara N Sainath, and Ron J Weiss,\n“A spelling cor-"
        },
        {
          "6. REFERENCES": "2023.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "rection model for end-to-end speech recognition,” in ICASSP."
        },
        {
          "6. REFERENCES": "[9] Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "IEEE, 2019."
        },
        {
          "6. REFERENCES": "Siniscalchi, Pin-Yu Chen, and Eng-Siong Chng, “HyPoradise:",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[24] Rao Ma, Mark JF Gales, Kate M Knill, and Mengjie Qian,"
        },
        {
          "6. REFERENCES": "An open baseline for generative speech recognition with large",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "“N-best\nt5: Robust asr error correction using multiple input"
        },
        {
          "6. REFERENCES": "language models,” NeurIPS, 2024.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "arXiv preprint\nhypotheses and constrained decoding space,”"
        },
        {
          "6. REFERENCES": "[10] Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "arXiv:2303.00456, 2023."
        },
        {
          "6. REFERENCES": "Dong Zhang, Zhehuai Chen, and Eng Siong Chng, “GenTrans-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[25] Leonardo Canseco-Rodriguez, Lori Lamel, and Jean-Luc Gau-"
        },
        {
          "6. REFERENCES": "late: Large language models are generative multilingual speech",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "vain, “Speaker diarization from speech transcripts,” in ICSLP,"
        },
        {
          "6. REFERENCES": "and machine translators,” in ACL, 2024.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "2004."
        },
        {
          "6. REFERENCES": "[11] David Chan, Austin Myers, Sudheendra Vijayanarasimhan,",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "David Ross, and John Canny, “IC3: Image captioning by com-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[26] Leonardo Canseco, Lori Lamel, and J-L Gauvain,\n“A com-"
        },
        {
          "6. REFERENCES": "mittee consensus,” in EMNLP, 2023.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "parative study using manual and automatic transcriptions for"
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "diarization,” in ASRU. IEEE, 2005."
        },
        {
          "6. REFERENCES": "[12] Yusuke Hirota, Ryo Hachiuma, Chao-Han Huck Yang,\nand",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "Yuta Nakashima, “From descriptive richness to bias: Unveiling",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[27] Tae Jin Park and Panayiotis Georgiou,\n“Multimodal speaker"
        },
        {
          "6. REFERENCES": "arXiv\nthe dark side of generative image caption enrichment,”",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "segmentation and diarization using lexical and acoustic cues"
        },
        {
          "6. REFERENCES": "preprint arXiv:2406.13912, 2024.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "INTER-\nvia\nsequence\nto\nsequence\nneural\nnetworks,”\nin"
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "SPEECH, 2018."
        },
        {
          "6. REFERENCES": "[13] Andreas Stolcke et al.,\n“SRILM-an extensible language mod-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "eling toolkit.,” in INTERSPEECH, 2002.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[28] Wei Xia, Han Lu, Quan Wang, Anshuman Tripathi, Yiling"
        },
        {
          "6. REFERENCES": "[14]\nFrank Seide and Amit Agarwal,\n“CNTK: Microsoft’s open-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Huang,\nIgnacio Lopez Moreno,\nand Hasim Sak,\n“Turn-to-"
        },
        {
          "6. REFERENCES": "source deep-learning toolkit,” in KDD, 2016.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "diarize: Online speaker diarization constrained by transformer"
        },
        {
          "6. REFERENCES": "",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "transducer speaker turn detection,” in ICASSP. IEEE, 2022."
        },
        {
          "6. REFERENCES": "[15] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": ""
        },
        {
          "6. REFERENCES": "get, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "[29] Aparna Khare, Eunjung Han, Yuguang Yang,\nand Andreas"
        },
        {
          "6. REFERENCES": "Motlicek, Yanmin Qian, Petr Schwarz,\net\nal.,\n“The Kaldi",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "Stolcke,\n“ASR-aware\nend-to-end\nneural\ndiarization,”\nin"
        },
        {
          "6. REFERENCES": "speech recognition toolkit,” in ASRU. IEEE, 2011.",
          "[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xu-": "ICASSP. IEEE, 2022, pp. 8092–8096."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Bowen Zhou, Panayiotis Georgiou, and Shrikanth Narayanan,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "perlen,\n“CALLHOME\nAmerican\nEnglish\nspeech,”"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "“Speaker diarization with lexical\ninformation,”\nin INTER-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "https://catalog.ldc.upenn.edu/LDC97S42, 1997."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "SPEECH, 2019.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[45] Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[31] Laurent El Shafey, Hagen Soltau, and Izhak Shafran,\n“Joint",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev,"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "speech recognition and speaker diarization via sequence trans-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Jagadeesh Balam, and Boris Ginsburg,\n“The CHiME-7 chal-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "duction,” in INTERSPEECH, 2019.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "lenge:\nSystem description and performance of nemo team’s"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[32] Naoyuki Kanda, Xiong Xiao, Yashesh Gaur, Xiaofei Wang,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "dasr system,” arXiv preprint arXiv:2310.12378, 2023."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Zhong Meng, Zhuo Chen, and Takuya Yoshioka, “Transcribe-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[46] Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk,"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "to-diarize: Neural\nspeaker diarization for unlimited number",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beli-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "of\nspeakers\nusing\nend-to-end\nspeaker-attributed ASR,”\nin",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "aev, Vitaly Lavrukhin, Jack Cook, et al., “NeMo: a toolkit for"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "ICASSP. IEEE, 2022, pp. 8082–8086.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "building AI applications using neural modules,” arXiv preprint"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[33] Luyao Cheng, Siqi Zheng, Zhang Qinglin, Hui Wang, Yafeng",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "arXiv:1909.09577, 2019."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Chen, and Qian Chen, “Exploring speaker-related information",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[47]\nShinji Watanabe, Michael Mandel, Jon Barker, et al., “CHiME-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "in spoken language understanding for better speaker diariza-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "6 challenge: Tackling multispeaker speech recognition for un-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "tion,” in Findings of ACL 2023, 2023.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "segmented recordings,” in CHiME Workshop, 2020."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[34]\nJee-weon Jung et al.,\n“Encoder-decoder multimodal speaker",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[48] Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, and"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "change detection,” in INTERSPEECH, 2023.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Reinhold Haeb-Umbach,\n“MeetEval: A meeting transcrip-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[35] Rohit Paturi et al., “Lexical speaker error correction: Leverag-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "tion\nevaluation\ntoolkit,” https://github.com/fgnt/"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "ing language models for speaker diarization error correction,”",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "meeteval, 2024."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "in INTERSPEECH, 2021.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[49] Yuanchao Li, Zeyu Zhao, Ondrej Klejch,\nPeter Bell,\nand"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[36] Aakanksha\nChowdhery,\nSharan\nNarang,\nJacob\nDevlin,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Catherine Lai,\n“ASR and emotional speech: A word-level\nin-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "vestigation of the mutual impact of speech and emotion recog-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "nition,” in INTERSPEECH, 2023."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "et al.,\n“Palm:\nScaling language modeling with pathways,”",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[50]\nJennifer Santoso, Takeshi Yamada, Shoji Makino, Kenkichi"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Journal of Machine Learning Research, vol. 24, no. 240, pp.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Ishizuka, and Takekatsu Hiramura,\n“Speech emotion recog-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "1–113, 2023.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "nition based on attention weight correction using word-level"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[37] Yu Zhang, Wei Han,\nJames Qin, Yongqiang Wang, Ankur",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "confidence measure.,” in INTERSPEECH, 2021."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Bapna,\nZhehuai Chen, Nanxin Chen, Bo Li, Vera Axel-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[51] Yuanchao Li, Pinzhen Chen, Peter Bell,\nand Catherine Lai,"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "rod, Gary Wang,\net al.,\n“Google USM: Scaling automatic",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "“Crossmodal ASR error correction with discrete speech units,”"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "arXiv preprint\nspeech recognition beyond 100 languages,”",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "in SLT. IEEE, 2024."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "arXiv:2303.01037, 2023.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[38] Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[52]\nJiajun He, Xiaohan Shi, Xingfeng Li, and Tomoki Toda, “MF-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Xia,\nand Hank Liao,\n“DiarizationLM: Speaker diarization",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "AED-AEC: Speech emotion recognition by leveraging multi-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "arXiv preprint\npost-processing with large language models,”",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "modal fusion, ASR error detection, and ASR error correction,”"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "arXiv:2401.03506, 2024.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "arXiv preprint arXiv:2401.13260, 2024."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[39] Georgios Efstathiadis, Vijay Yadav, and Anzar Abbas, “LLM-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[53]\nJeremy Ang,\nRajdip Dhillon,\nAshley Krupski,\nElizabeth"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "based\nspeaker\ndiarization\ncorrection:\nA generalizable\nap-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Shriberg, and Andreas Stolcke, “Prosody-based automatic de-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "proach,” arXiv preprint arXiv:2406.04927, 2024.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "tection of annoyance and frustration in human-computer dia-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "log,” in INTERSPEECH, 2002."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[40] Tae Jin Park, Kunal Dhawan, Nithin Koluguri, and Jagadeesh",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Balam,\n“Enhancing speaker diarization with large language",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[54] Yuanchao Li, Peter Bell,\nand Catherine Lai,\n“Fusing ASR"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "models: A contextual beam search approach,” ICASSP, 2023.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "outputs in joint\ntraining for speech emotion recognition,”\nin"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[41] Maarten Van Segbroeck, Ahmed Zaid, Ksenia Kutsenko, Cire-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "ICASSP. IEEE, 2022."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "nia Huerta, Tinh Nguyen, Xuewen Luo, Bj¨orn Hoffmeister, Jan",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[55] Tiantian Feng and Shrikanth Narayanan,\n“Foundation model"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Trmal, Maurizio Omologo, and Roland Maas, “DiPCo–Dinner",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "assisted automatic speech emotion recognition: Transcribing,"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Party corpus,” arXiv preprint arXiv:1909.13447, 2019.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "annotating, and augmenting,” in ICASSP. IEEE, 2024."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[42] Linda Brandschain, David Graff, Christopher Cieri, Kevin",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[56] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Walker, Chris Caruso, and A Neely, “The Mixer 6 corpus: Re-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "sources for cross-channel and text independent speaker recog-",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“IEMO-"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "nition,” in Proc. of LREC, 2010.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "CAP:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "[43]\nJean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn,",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "LREC, 2008."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Mael Guillemot,\nThomas Hain,\nJaroslav Kadlec,\nVasilis",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "[57] Yuanchao Li, Peter Bell, and Catherine Lai,\n“Speech emotion"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al.,\n“The",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "recognition with ASR transcripts: A comprehensive study on"
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "AMI meeting corpus: A pre-announcement,”\nin International",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": "word error rate and fusion techniques,” in SLT. IEEE, 2024."
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "workshop on machine\nlearning for multimodal\ninteraction.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        },
        {
          "[30] Tae\nJin\nPark,\nKyu\nJ Han,\nJing Huang,\nXiaodong He,": "Springer, 2005.",
          "[44] Alexandra\nCanavan,\nDavid\nGraff,\nand\nGeorge\nZip-": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Continuous speech recognition by statistical methods",
      "authors": [
        "Frederick Jelinek"
      ],
      "year": "1976",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Integration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses",
      "authors": [
        "M Ostendorf",
        "A Kannan",
        "S Austin",
        "O Kimball",
        "R Schwartz",
        "J Rohlicek"
      ],
      "year": "1991",
      "venue": "Workshop on Speech and Natural Language"
    },
    {
      "citation_id": "4",
      "title": "Internal language model estimation for domain-adaptive end-to-end speech recognition",
      "authors": [
        "Zhong Meng",
        "Sarangarajan Parthasarathy",
        "Eric Sun",
        "Yashesh Gaur",
        "Naoyuki Kanda",
        "Liang Lu",
        "Xie Chen",
        "Rui Zhao",
        "Jinyu Li",
        "Yifan Gong"
      ],
      "year": "2021",
      "venue": "SLT"
    },
    {
      "citation_id": "5",
      "title": "Rnn-transducer with stateless prediction network",
      "authors": [
        "Mohammadreza Ghodsi",
        "Xiaofeng Liu",
        "James Apfel",
        "Rodrigo Cabrera",
        "Eugene Weinstein"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Tuning large language model for speech recognition with mixed-scale re-tokenization",
      "authors": [
        "Yukun Ma",
        "Chong Zhang",
        "Qian Chen",
        "Wen Wang",
        "Bin Ma"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "7",
      "title": "Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition",
      "authors": [
        "Yu Yu",
        "Chao-Han Huck Yang",
        "Jari Kolehmainen"
      ],
      "year": "2023",
      "venue": "Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition"
    },
    {
      "citation_id": "8",
      "title": "Generative speech recognition error correction with large language models and taskactivating prompting",
      "authors": [
        "Chao-Han Huck",
        "Yile Yang",
        "Yi-Chieh Gu",
        "Shalini Liu",
        "Ivan Ghosh",
        "Andreas Bulyko",
        "Stolcke"
      ],
      "year": "2023",
      "venue": "ASRU"
    },
    {
      "citation_id": "9",
      "title": "Whispering LLaMA: A cross-modal generative error correction framework for speech recognition",
      "authors": [
        "Srijith Radhakrishnan",
        "Chao-Han",
        "Sumeer Yang",
        "Rohit Khan",
        "Narsis Kumar",
        "David Kiani",
        "Jesper Gomez-Cabrero",
        "Tegnér"
      ],
      "year": "2023",
      "venue": "EMNLP"
    },
    {
      "citation_id": "10",
      "title": "HyPoradise: An open baseline for generative speech recognition with large language models",
      "authors": [
        "Chen Chen",
        "Yuchen Hu",
        "Chao-Han Huck Yang",
        "Sabato Marco Siniscalchi",
        "Pin-Yu Chen",
        "Eng-Siong Chng"
      ],
      "year": "2024",
      "venue": "HyPoradise: An open baseline for generative speech recognition with large language models"
    },
    {
      "citation_id": "11",
      "title": "GenTranslate: Large language models are generative multilingual speech and machine translators",
      "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Chao-Han Huck Yang",
        "Ruizhe Li",
        "Dong Zhang",
        "Zhehuai Chen",
        "Eng Siong"
      ],
      "year": "2024",
      "venue": "ACL"
    },
    {
      "citation_id": "12",
      "title": "Image captioning by committee consensus",
      "authors": [
        "David Chan",
        "Austin Myers",
        "Sudheendra Vijayanarasimhan",
        "David Ross",
        "John Canny"
      ],
      "year": "2023",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "From descriptive richness to bias: Unveiling the dark side of generative image caption enrichment",
      "authors": [
        "Yusuke Hirota",
        "Ryo Hachiuma",
        "Yang Chao-Han Huck",
        "Yuta Nakashima"
      ],
      "year": "2024",
      "venue": "From descriptive richness to bias: Unveiling the dark side of generative image caption enrichment",
      "arxiv": "arXiv:2406.13912"
    },
    {
      "citation_id": "14",
      "title": "SRILM-an extensible language modeling toolkit",
      "authors": [
        "Andreas Stolcke"
      ],
      "year": "2002",
      "venue": "SRILM-an extensible language modeling toolkit"
    },
    {
      "citation_id": "15",
      "title": "CNTK: Microsoft's opensource deep-learning toolkit",
      "authors": [
        "Frank Seide",
        "Amit Agarwal"
      ],
      "year": "2016",
      "venue": "KDD"
    },
    {
      "citation_id": "16",
      "title": "The Kaldi speech recognition toolkit",
      "authors": [
        "Daniel Povey",
        "Arnab Ghoshal",
        "Gilles Boulianne",
        "Lukas Burget",
        "Ondrej Glembek",
        "Nagendra Goel",
        "Mirko Hannemann",
        "Petr Motlicek",
        "Yanmin Qian",
        "Petr Schwarz"
      ],
      "year": "2011",
      "venue": "The Kaldi speech recognition toolkit"
    },
    {
      "citation_id": "17",
      "title": "AudioGPT: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "Rongjie Huang",
        "Mingze Li",
        "Dongchao Yang",
        "Jiatong Shi",
        "Xuankai Chang",
        "Zhenhui Ye",
        "Yuning Wu",
        "Zhiqing Hong",
        "Jiawei Huang",
        "Jinglin Liu"
      ],
      "year": "2024",
      "venue": "AAAI"
    },
    {
      "citation_id": "18",
      "title": "Recent innovations in speech-to-text transcription at SRI-ICSI-UW",
      "authors": [
        "Andreas Stolcke",
        "Barry Chen",
        "Horacio Franco",
        "Venkata Ramana Rao",
        "Martin Gadde",
        "Mei-Yuh Graciarena",
        "Katrin Hwang",
        "Arindam Kirchhoff",
        "Nelson Mandal",
        "Xin Morgan",
        "Lei"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Bidirectional recurrent neural network language models for automatic speech recognition",
      "authors": [
        "Ebru Arisoy",
        "Abhinav Sethy",
        "Bhuvana Ramabhadran",
        "Stanley Chen"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Toward human parity in conversational speech recognition",
      "authors": [
        "Wayne Xiong",
        "Jasha Droppo",
        "Xuedong Huang",
        "Frank Seide",
        "L Michael",
        "Andreas Seltzer",
        "Dong Stolcke",
        "Geoffrey Yu",
        "Zweig"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Better evaluation for grammatical error correction",
      "authors": [
        "Daniel Dahlmeier",
        "Hwee Tou Ng"
      ],
      "year": "2012",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "22",
      "title": "How might we create better benchmarks for speech recognition?",
      "authors": [
        "Alëna Aksënova",
        "James Daan Van Esch",
        "Pavel Flynn",
        "Golik"
      ],
      "year": "2021",
      "venue": "1st Workshop on Benchmarking: Past, Present and Future"
    },
    {
      "citation_id": "23",
      "title": "Neural models of text normalization for speech applications",
      "authors": [
        "Hao Zhang",
        "Richard Sproat",
        "Axel Ng",
        "Felix Stahlberg",
        "Xiaochang Peng",
        "Kyle Gorman",
        "Brian Roark"
      ],
      "year": "2019",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "A spelling correction model for end-to-end speech recognition",
      "authors": [
        "Jinxi Guo",
        "Tara Sainath",
        "Ron Weiss"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "25",
      "title": "N-best t5: Robust asr error correction using multiple input hypotheses and constrained decoding space",
      "authors": [
        "Rao Ma",
        "J Mark",
        "Kate Gales",
        "Mengjie Knill",
        "Qian"
      ],
      "year": "2023",
      "venue": "N-best t5: Robust asr error correction using multiple input hypotheses and constrained decoding space",
      "arxiv": "arXiv:2303.00456"
    },
    {
      "citation_id": "26",
      "title": "Speaker diarization from speech transcripts",
      "authors": [
        "Leonardo Canseco-Rodriguez",
        "Lori Lamel",
        "Jean-Luc Gauvain"
      ],
      "year": "2004",
      "venue": "ICSLP"
    },
    {
      "citation_id": "27",
      "title": "A comparative study using manual and automatic transcriptions for diarization",
      "authors": [
        "Leonardo Canseco",
        "Lori Lamel",
        "J-L Gauvain"
      ],
      "year": "2005",
      "venue": "ASRU"
    },
    {
      "citation_id": "28",
      "title": "Multimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks",
      "authors": [
        "Jin Tae",
        "Panayiotis Park",
        "Georgiou"
      ],
      "year": "2018",
      "venue": "Multimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks"
    },
    {
      "citation_id": "29",
      "title": "Turn-todiarize: Online speaker diarization constrained by transformer transducer speaker turn detection",
      "authors": [
        "Wei Xia",
        "Han Lu",
        "Quan Wang",
        "Anshuman Tripathi",
        "Yiling Huang",
        "Ignacio Moreno",
        "Hasim Sak"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "30",
      "title": "ASR-aware end-to-end neural diarization,\" in ICASSP",
      "authors": [
        "Aparna Khare",
        "Eunjung Han",
        "Yuguang Yang",
        "Andreas Stolcke"
      ],
      "year": "2022",
      "venue": "ASR-aware end-to-end neural diarization,\" in ICASSP"
    },
    {
      "citation_id": "31",
      "title": "Speaker diarization with lexical information",
      "authors": [
        "Jin Tae",
        "Park",
        "J Kyu",
        "Jing Han",
        "Xiaodong Huang",
        "Bowen He",
        "Panayiotis Zhou",
        "Shrikanth Georgiou",
        "Narayanan"
      ],
      "year": "2019",
      "venue": "Speaker diarization with lexical information"
    },
    {
      "citation_id": "32",
      "title": "Joint speech recognition and speaker diarization via sequence transduction",
      "authors": [
        "Laurent Shafey",
        "Hagen Soltau",
        "Izhak Shafran"
      ],
      "year": "2019",
      "venue": "Joint speech recognition and speaker diarization via sequence transduction"
    },
    {
      "citation_id": "33",
      "title": "Transcribeto-diarize: Neural speaker diarization for unlimited number of speakers using end-to-end speaker-attributed ASR",
      "authors": [
        "Naoyuki Kanda",
        "Xiong Xiao",
        "Yashesh Gaur",
        "Xiaofei Wang",
        "Zhong Meng",
        "Zhuo Chen",
        "Takuya Yoshioka"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "34",
      "title": "Exploring speaker-related information in spoken language understanding for better speaker diarization",
      "authors": [
        "Luyao Cheng",
        "Siqi Zheng",
        "Zhang Qinglin",
        "Hui Wang",
        "Yafeng Chen",
        "Qian Chen"
      ],
      "year": "2023",
      "venue": "Findings of ACL 2023"
    },
    {
      "citation_id": "35",
      "title": "Encoder-decoder multimodal speaker change detection",
      "authors": [
        "Jee-Weon Jung"
      ],
      "year": "2023",
      "venue": "Encoder-decoder multimodal speaker change detection"
    },
    {
      "citation_id": "36",
      "title": "Lexical speaker error correction: Leveraging language models for speaker diarization error correction",
      "authors": [
        "Rohit Paturi"
      ],
      "year": "2021",
      "venue": "Lexical speaker error correction: Leveraging language models for speaker diarization error correction"
    },
    {
      "citation_id": "37",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "38",
      "title": "Google USM: Scaling automatic speech recognition beyond 100 languages",
      "authors": [
        "Yu Zhang",
        "Wei Han",
        "James Qin",
        "Yongqiang Wang",
        "Ankur Bapna",
        "Zhehuai Chen",
        "Nanxin Chen",
        "Bo Li",
        "Vera Axelrod",
        "Gary Wang"
      ],
      "year": "2023",
      "venue": "Google USM: Scaling automatic speech recognition beyond 100 languages",
      "arxiv": "arXiv:2303.01037"
    },
    {
      "citation_id": "39",
      "title": "DiarizationLM: Speaker diarization post-processing with large language models",
      "authors": [
        "Quan Wang",
        "Yiling Huang",
        "Guanlong Zhao",
        "Evan Clark",
        "Wei Xia",
        "Hank Liao"
      ],
      "year": "2024",
      "venue": "DiarizationLM: Speaker diarization post-processing with large language models",
      "arxiv": "arXiv:2401.03506"
    },
    {
      "citation_id": "40",
      "title": "LLMbased speaker diarization correction: A generalizable approach",
      "authors": [
        "Georgios Efstathiadis",
        "Vijay Yadav",
        "Anzar Abbas"
      ],
      "year": "2024",
      "venue": "LLMbased speaker diarization correction: A generalizable approach",
      "arxiv": "arXiv:2406.04927"
    },
    {
      "citation_id": "41",
      "title": "Enhancing speaker diarization with large language models: A contextual beam search approach",
      "authors": [
        "Jin Tae",
        "Kunal Park",
        "Nithin Dhawan",
        "Jagadeesh Koluguri",
        "Balam"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "42",
      "title": "DiPCo-Dinner Party corpus",
      "authors": [
        "Maarten Van Segbroeck",
        "Ahmed Zaid",
        "Ksenia Kutsenko",
        "Cirenia Huerta",
        "Tinh Nguyen",
        "Xuewen Luo",
        "Björn Hoffmeister",
        "Jan Trmal",
        "Maurizio Omologo",
        "Roland Maas"
      ],
      "year": "2019",
      "venue": "DiPCo-Dinner Party corpus",
      "arxiv": "arXiv:1909.13447"
    },
    {
      "citation_id": "43",
      "title": "The Mixer 6 corpus: Resources for cross-channel and text independent speaker recognition",
      "authors": [
        "Linda Brandschain",
        "David Graff",
        "Christopher Cieri",
        "Kevin Walker",
        "Chris Caruso",
        "Neely"
      ],
      "year": "2010",
      "venue": "Proc. of LREC"
    },
    {
      "citation_id": "44",
      "title": "The AMI meeting corpus: A pre-announcement",
      "authors": [
        "Jean Carletta",
        "Simone Ashby",
        "Sebastien Bourban",
        "Mike Flynn",
        "Mael Guillemot",
        "Thomas Hain",
        "Jaroslav Kadlec",
        "Vasilis Karaiskos",
        "Wessel Kraaij",
        "Melissa Kronenthal"
      ],
      "year": "2005",
      "venue": "International workshop on machine learning for multimodal interaction"
    },
    {
      "citation_id": "45",
      "title": "CALLHOME American English speech",
      "authors": [
        "Alexandra Canavan",
        "David Graff",
        "George Zipperlen"
      ],
      "year": "1997",
      "venue": "CALLHOME American English speech"
    },
    {
      "citation_id": "46",
      "title": "The CHiME-7 challenge: System description and performance of nemo team's dasr system",
      "authors": [
        "Jin Tae",
        "He Park",
        "Ante Huang",
        "Kunal Jukic",
        "Krishna Dhawan",
        "Nithin Puvvada",
        "Nikolay Koluguri",
        "Aleksandr Karpov",
        "Jagadeesh Laptev",
        "Boris Balam",
        "Ginsburg"
      ],
      "year": "2023",
      "venue": "The CHiME-7 challenge: System description and performance of nemo team's dasr system",
      "arxiv": "arXiv:2310.12378"
    },
    {
      "citation_id": "47",
      "title": "NeMo: a toolkit for building AI applications using neural modules",
      "authors": [
        "Oleksii Kuchaiev",
        "Jason Li",
        "Huyen Nguyen",
        "Oleksii Hrinchuk",
        "Ryan Leary",
        "Boris Ginsburg",
        "Samuel Kriman",
        "Stanislav Beliaev",
        "Vitaly Lavrukhin",
        "Jack Cook"
      ],
      "year": "2019",
      "venue": "NeMo: a toolkit for building AI applications using neural modules",
      "arxiv": "arXiv:1909.09577"
    },
    {
      "citation_id": "48",
      "title": "CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings",
      "authors": [
        "Shinji Watanabe",
        "Michael Mandel",
        "Jon Barker"
      ],
      "year": "2020",
      "venue": "CHiME Workshop"
    },
    {
      "citation_id": "49",
      "title": "MeetEval: A meeting transcription evaluation toolkit",
      "authors": [
        "Christoph Thilo Von Neumann",
        "Marc Boeddeker",
        "Reinhold Delcroix",
        "Haeb-Umbach"
      ],
      "year": "2024",
      "venue": "MeetEval: A meeting transcription evaluation toolkit"
    },
    {
      "citation_id": "50",
      "title": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Zeyu Zhao",
        "Ondrej Klejch",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "ASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition"
    },
    {
      "citation_id": "51",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "Jennifer Santoso",
        "Takeshi Yamada",
        "Shoji Makino",
        "Kenkichi Ishizuka",
        "Takekatsu Hiramura"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition based on attention weight correction using word-level confidence measure"
    },
    {
      "citation_id": "52",
      "title": "Crossmodal ASR error correction with discrete speech units",
      "authors": [
        "Yuanchao Li",
        "Pinzhen Chen",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "SLT"
    },
    {
      "citation_id": "53",
      "title": "MF-AED-AEC: Speech emotion recognition by leveraging multimodal fusion, ASR error detection, and ASR error correction",
      "authors": [
        "Jiajun He",
        "Xiaohan Shi",
        "Xingfeng Li",
        "Tomoki Toda"
      ],
      "year": "2024",
      "venue": "MF-AED-AEC: Speech emotion recognition by leveraging multimodal fusion, ASR error detection, and ASR error correction",
      "arxiv": "arXiv:2401.13260"
    },
    {
      "citation_id": "54",
      "title": "Prosody-based automatic detection of annoyance and frustration in human-computer dialog",
      "authors": [
        "Jeremy Ang",
        "Rajdip Dhillon",
        "Ashley Krupski",
        "Elizabeth Shriberg",
        "Andreas Stolcke"
      ],
      "year": "2002",
      "venue": "Prosody-based automatic detection of annoyance and frustration in human-computer dialog"
    },
    {
      "citation_id": "55",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "56",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP"
    },
    {
      "citation_id": "57",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "LREC"
    },
    {
      "citation_id": "58",
      "title": "Speech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "SLT"
    }
  ]
}