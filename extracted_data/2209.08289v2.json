{
  "paper_id": "2209.08289v2",
  "title": "Continuously Controllable Facial Expression Editing In Talking Face Videos",
  "published": "2022-09-17T09:05:47Z",
  "authors": [
    "Zhiyao Sun",
    "Yu-Hui Wen",
    "Tian Lv",
    "Yanan Sun",
    "Ziyang Zhang",
    "Yaoyuan Wang",
    "Yong-Jin Liu"
  ],
  "keywords": [
    "Facial expression editing",
    "continuously controllable expressions",
    "talking face video generation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently audio-driven talking face video generation has attracted considerable attention. However, very few researches address the issue of emotional editing of these talking face videos with continuously controllable expressions, which is a strong demand in the industry. The challenge is that speech-related expressions and emotion-related expressions are often highly coupled. Meanwhile, traditional image-to-image translation methods cannot work well in our application due to the coupling of expressions with other attributes such as poses, i.e., translating the expression of the character in each frame may simultaneously change the head pose due to the bias of the training data distribution. In this paper, we propose a high-quality facial expression editing method for talking face videos, allowing the user to control the target emotion in the edited video continuously. We present a new perspective for this task as a special case of motion information editing, where we use a 3DMM to capture major facial movements and an associated texture map modeled by a StyleGAN to capture appearance details. Both representations (3DMM and texture map) contain emotional information and can be continuously modified by neural networks and easily smoothed by averaging in coefficient/latent spaces, making our method simple yet effective. We also introduce a mouth shape preservation loss to control the trade-off between lip synchronization and the degree of exaggeration of the edited expression. Extensive experiments and a user study show that our method achieves state-of-the-art performance across various evaluation criteria.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "I N recent years, numerous methods have been proposed to generate realistic-looking talking videos driven by audio input, e.g.,  [1] ,  [2] ,  [3] ,  [4] ,  [5] ,  [6] . Much effort has been made to improve the quality of the generated result, mainly on identity preservation and lip synchronization. However, most existing studies have not explicitly taken emotions into consideration and cannot explicitly control the generated expression. A few recent works  [7] ,  [8] ,  [9]  have attempted to generate talking videos with pre-specified emotions; however, the accuracy and naturalness of the generated expression and the lip synchronization are not satisfactory due to the coupled nature of the speech-related facial expression and the emotion-related facial expression, which makes the continuously controllable facial expression generation problem highly challenging.\n\nOur study is based on the following key observation: Since directly generating talking videos with realistic and controllable expressions is very challenging, we decompose Corresponding authors: Yong-Jin Liu and Yu-Hui Wen. This work was partially supported by National Key R&D Program of China (2022ZD0117900), the Natural Science Foundation of China (62332019, 62202257, U2336214) and Beijing Jiaotong University Youth Elite Project (2023XKRC045).\n\nthis task into two sub-tasks, each of which can find a reasonable solution:  (1)  generating an emotionless talking face video and (2) editing the expression in each frame using pre-specified emotion labels. Many existing works can already do the first sub-task well. In this paper, we propose a simple yet effective method to accomplish the second subtask (Figure  1 ).\n\nIntuitively, existing image-to-image translation methods (e.g.,  [10] ,  [11] ,  [12] ) or facial attribute editing methods (e.g.,  [13] ,  [14] ) could potentially be used for the second sub-task. However, our preliminary study shows that these methods do not work well for three reasons. First, these methods may change the shape of the mouth, making the edited video out of sync with the audio. Second, imageto-image translation methods may simultaneously change the pose, lighting, or other attributes if they are entangled with the facial expressions in the training data, leading to problems such as misalignment. We also pay attention to the StyleGAN model  [15] ,  [16] , which can generate highquality images and learn semantically disentangled latent space. Many state-of-the-art studies  [13] ,  [17] ,  [18] ,  [19]  have explored its powerful editing capabilities. However, finding distinguishable and decoupled editing directions for different expressions is still challenging. Third, applying these image translation/editing methods to a video frame-by-frame may cause a serious inter-frame discontinuity problem.\n\nIn this paper, we tackle the second sub-task from a new perspective, that is, we regard expression editing in a talking video as a specific type of motion information editing. This perspective is based on an observation that when a person speaks, his/her facial muscles control both expressions and mouth movements. Accordingly, our method uses a twolevel motion information approach for expression editing: (1) a 3DMM  [20]  to model the primary facial movements and (2) a deliberate texture map generated by StyleGAN  [16]  to capture high-quality appearance details and their subtle variations. Our experiments find that the 3DMM is able to capture large-scale deformations such as opening the mouth wide in anger or raising eyebrows in joy, which can impact people's perception of whether an expression is positive or negative. However, we observe that due to limited expressiveness, 3DMM cannot capture color changes and some fine facial details such as wrinkles. These details affect people's perception of subtle facial expressions such as sadness, anger, fear, disgust, and contempt in negative emotions. Therefore, we incorporate a high-quality texture map to capture these details. Given this two-level motion information representation, our expression editing method first reconstructs the 3D face and extracts the texture from the input frame. Next, our method transforms the 3DMM shape coefficient and the texture of the 3D face to the target emotion, respectively. Finally, our method renders the face image using the reconstructed pose, the generated 3D face shape and texture.\n\nOur two-level motion information editing strategy is effective and simple to use and has the following advantages. First, since we decompose the facial expression into a 3DMM shape and an associate texture map, we can focus on the pose-independent shape and texture respectively during expression editing. This enables our method to be unaffected by the coupling of emotions and poses from the bias of the training data distribution. Second, with the aid of the key points in the 3DMM, we can use a simple loss function to impose an explicit constraint on the mouth shape, giving us control over the tradeoff between mouth shape preservation and the degree of exaggeration in the edited expression. Third, our method does not require complex network structures or loss functions to ensure interframe continuity. By decoupling the motion information into major facial movement (via 3DMM) and subtle changes in fine appearance details (via texture map), we can smooth major shape information and fine appearance details separately in adjacent frames and then combine them to achieve inter-frame continuity. As the 3DMM we used is a linear model, we can smooth the face shape by simply taking the average of 3DMM coefficients inferred from adjacent frames. Appearance details can also be smoothed easily by averaging the codes in the latent space of textures modeled by StyleGAN. Fourth, our use of textures generated by StyleGAN enables high-quality editing at resolutions up to 1024 × 1024.\n\nWe evaluate our proposed method comprehensively and compare it with some existing baseline methods. Some commonly used metrics for lip synchronization, identity preservation and image quality of the generated results are adopted in our evaluation, including LSE-D and LSE-C, the average ArcFace feature, FID and CPBD, etc.; see Section 4.2 for details. In addition, we adopt the FED metric to assess the reality degree of the generated expressions and propose a novel metric called the LIE metric. This metric is specifically designed to evaluate the change rate in the intensity of facial expression editing, which can indicate the smoothness of the method's control over facial expressions.\n\nIn summary, we make three contributions in this paper:",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We characterize the facial expression editing problem as a special motion information editing problem and propose a two-level expression representation to decompose the motion information into major facial movement (by 3DMM) and subtle changes in fine appearance details (by texture map).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "Based on the two-level expression representation, we propose a simple yet effective method for continuously controllable facial expression editing in talking face videos, with delicate choices of architectures and losses to enable accurate and high-resolutional expression editing while preserving identity, lip synchronization and temporal consistency.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We propose a new metric -LIE, to evaluate the linearity of intensity editing in the scenario of expression editing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3D Face Model And Reconstruction",
      "text": "Parametric models are popular for 3D face representation. The 3D Morphable Face Model (3DMM)  [20]  is probably the most popular 3D parametric face model, which uses shape and texture vectors to characterize the statistical distribution of face shape and textures. Many variants of 3DMMs have been proposed (e.g.,  [21] ,  [22] ,  [23] ), most of which are linear models built by performing principal component analysis on a set of scanned 3D face data. The Basel Face Model (BFM)  [21]  is the first widely used 3DMM, which models the shape and texture with the neutral expression. Some later 3DMMs  [22] ,  [23]  introduce expression bases to model different expression types. For a detailed overview, readers can refer to the survey paper  [24] . 3DMMs are extensively used for 3D face reconstruction. Many reconstruction methods regress 3DMM coefficients (such as the coefficients of shape and texture) and camera parameters from 2D images. They follow the analysis-bysynthesis paradigm, which minimizes the difference between a rendered image of the 3D reconstructed face and the original image through direct optimization  [25] ,  [26]  or deep neural network prediction  [27] ,  [28] ,  [29] .\n\nIn this paper, we use the method proposed in  [27]  to reconstruct the 3D face. This method employs a modified 3DMM based on  [21]  and the expression bases from  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Facial Expression Editing",
      "text": "Facial expression editing refers to the manipulation of facial expressions in images or videos. There are various image-toimage translation methods available, such as  [10] ,  [11] ,  [12] , as well as methods specifically designed for facial expression editing, such as  [14] ,  [30] ,  [31] ,  [32] ,  [33] ). ExprGAN  [30]  is a method based on conditional GAN, which can transform faces into specified expressions with continuous intensities.\n\nICface  [14]  uses head angles and action units to control facial expressions and poses through a two-stage neural network. GANmut  [33]  proposes a GAN-based framework that learns an expressive and interpretable conditional space of emotions. However, these methods do not consider 3D information and face difficulty in disentangling expressions from pose, lip shape, background and other attributes. Geng et al.  [31]  incorporate a 3DMM in their approach to separate the face into shape and texture, and modify them respectively based on the input 3DMM coefficients, but the quality of the generated image is not satisfactory.\n\nRecently, StyleGAN-based attribute editing has gained attention due to the high editability of StyleGAN and the high quality of the generated results  [15] ,  [16] . This type of attribute editing first projects the input image to StyleGAN's latent space by either optimization  [16] ,  [34] ,  [35] ,  [36] , or regression  [17] ,  [18] ,  [19] ,  [37] . Then, the corresponding latent code is moved towards target attribute's location. Finally, StyleGAN generates the edited image from the modified latent code. This approach has advantages over the variational autoencoder (VAE), because VAE inherently tends to generate blurry images  [38] ,  [39] . Nevertheless, finding decoupled editing directions for different facial expressions is laborious and challenging, because these expressions are found to couple with other attributes such as poses in the latent space of StyleGAN, due to the bias of the training data  [13] ,  [40] ,  [41] . StyleRig  [32]  incorporates 3DMM into StyleGANbased editing, where 3DMM coefficients are mapped to StyleGAN's latent code, achieving explicit control over the expression and pose. However, this approach cannot control fine-grained details in textures and may fail if the target expression or pose is absent in the training data.\n\nIt is worth noting that all the methods mentioned above are designed for images; thus, temporal consistency or interframe continuity cannot be guaranteed when applying them to video frames. Additionally, most of those methods do not consider lip shape preservation. There are a few methods designed to edit facial expressions in a video. Ma et al.  [42]  propose a video editing method that reconstructs a 3DMM and transforms the expression coefficients according to the emotion label. In this method, temporal consistency is achieved by window-based smoothing, and lip synchronization is achieved by imposing a lip shape constraint. The Wav2Lip-Emotion method  [43]  extends the lip synchronization architecture  [5]  by modifying facial expressions with the aid of L 1 reconstruction and pre-trained emotion objectives; however, the face identity in test images is not preserved, and the visual quality of generated images is very low. NED  [9]  allows speech-preserving facial expression editing for videos based on 3DMM and neural rendering. However, this method suffers from jitter in the rendered face. Furthermore, these methods cannot generate facial expressions with different emotional intensities, nor can they seamlessly transition between different emotions. Solanki et al.  [44]  propose a method that allows the user to continuously control the intensity of the target emotion when editing a video. However, it cannot preserve the lip shape, making it unsuitable for talking face videos. More recently, Liu et al.  [45]  propose a sketch-based facial video editing framework built on StyleGAN3  [46] , where they map video frames to the latent space of StyleGAN3 and guide the editing through user-drawn sketches and masks. Their method can generate smooth transitions with good visual quality and temporal consistency. However, when editing talking videos, it is difficult to draw the sketches around the mouth area, as editing may cause the lip out of sync with the audio.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Audio-Driven Talking Face Video Generation",
      "text": "Audio-driven talking face video generation aims to synthesize realistic-looking talking videos that are synchronized with input audio and have the same facial identity as input face images or video clips  [47] . To achieve this goal, motion features need to be predicted from input audio and applied to input visual data. Some methods predict modified pixels directly in the image space  [3] ,  [5] , and other methods use the 3DMM  [2] ,  [4] ,  [48]  or facial landmarks  [1]  to predict expression coefficients, vertex coordinates, or landmark locations from extracted audio features, which are then used for video generation. Recently, Lahiri et al.  [6]  propose explicitly decoupling 3D pose, geometry, texture and lighting in their pipeline, enabling data-efficient learning and high-quality synthesis. However, these methods do not distinguish between speech-related expressions and emotion-related expressions, because it is difficult to infer them simultaneously from the input audio. Therefore, they cannot generate videos with explicit control of facial expressions; in most cases, they can only generate videos that resemble neutral expressions.\n\nA few studies have explored video generation with specified emotions. For example, Karras et al.  [48]  include a learned latent representation of emotional states into the input of their system, enabling video generation with different emotions. However, their method requires manual mining for meaningful emotion vectors, which is a demanding task. Moreover, their method only considers shape deformations without modeling textures. Recently, Ji et al.  [7]  learn two disentangled latent spaces (a duration-independent emotion space and a duration-dependent content space) using paired audio signals to generate videos with assigned emotions or emotions inferred from the input audio. However, the generated expressions are not accurate enough, and the texture of the face is not consistent in a video, resulting in degraded expressiveness and noticeable artifacts. The method also does not perform well on lip synchronization, probably due to the coupling of expressions and audio signals. Ye et al.  [8]  use Wav2Lip  [5]  to generate the face geometry from the input audio and a dynamic neural texture module to sample the face texture based on the input emotion label. However, their method cannot produce results with high expression intensity properly and sometimes has a color shift problem.\n\nTo summarize, generating natural talking face videos with different expressions from the input audio is very challenging. Given that many existing methods can generate good talking face videos with a neutral expression, our work offers a simple yet effective solution to edit facial expressions in talking videos.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overview",
      "text": "In this paper, we address the following problem: Problem 1. Editing the expressions in a talking face video according to prespecified emotion labels (including both emotion type and intensity)) so that the edited video preserves the face identity and maintains audio-lip synchronization.\n\nAs aforementioned, existing image-to-image translation methods and image attribute editing methods cannot solve this problem, because they cannot deal with inter-frame continuity and the coupling of facial expressions and head poses. In this paper, we propose a simple yet effective solution to this problem from a new perspective: noting that when speaking, a person's facial expression and mouth movement are controlled by facial muscles, we address Problem 1 from the perspective of two-level motion information editing. That is, we use a 3DMM to capture major facial movements (level 1) and an associated texture map modeled by StyleGAN to capture appearance fine details and subtle variations (level 2). The overall pipeline of our method is illustrated in Figure  2 .\n\nThe input of our method includes a talking face video and an emotion vector for each frame 1 . Our method outputs an edited video with emotions as specified by the emotion vectors. The emotion vector indicates both expression type and intensity, which is defined as an n e -dimensional nonnegative vector e = (e 1 , . . . , e ne ) ∈ R ne + , where n e is the number of expression types (except for the neutral expression). There is at most one non-zero element in e. Each dimension in R ne + stands for an expression type (such as happiness, sadness, etc.) except for the neutral expression, and the value e i indicates the intensity of that expression. The zero vector represents the neutral expression. Throughout this paper, the intensity is linearly normalized to [0, 1].\n\nThe two-level motion information editing is performed frame by frame, and for each frame, a windowed smoothing operation is applied for three adjacent frames to ensure inter-frame continuity. In more detail, at a discrete time t, a single frame I t and a corresponding emotion vector e t are input into the system. First, we reconstruct a 3D face (using 3DMM) and extract the texture from I t (Section 3.2). Then we use a GAN model to transform the 3DMM coefficients (Section 3.3) , which is the level-1 modification for capturing major facial movements. We also edit the texture in the latent space of StyleGAN according to the emotion vector e t (Section 3.4), which is the level-2 modification to capture appearance fine details and subtle variations. By combining level-1 and level-2 modifications, we achieve continuous control of the emotion types and intensities. At the end of level-1 and level-2 modifications, we apply a windowed smoothing operation (Section 3.5.1) to ensure inter-frame continuity. Next, we render the 3D face with the transformed shape and texture onto the original background. Because the 3DMM does not model the teeth area, we use a teeth filling module (Sec. 3.5.2) to fill the cavity. Finally, we combine the generated frames together to obtain the edited video.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "3D Face Reconstruction And Texture Extraction",
      "text": "Given a video frame I (Fig.  3a ), we reconstruct a 3D face using a state-of-the-art method  [27] . This method regresses 1. If only emotion vectors of some keyframes are provided, we use linear interpolation to assign an emotion vector for each frame. Fig.  2 . The pipeline of our method. (Top row) Facial expression editing is done frame by frame, and for each frame, a windowed smoothing operation is applied before rendering the final output. (Bottom row) Given a video frame, we first reconstruct a 3D face to obtain the 3DMM coefficients and texture map. Then we transform the shape and the texture respectively. For the texture, we use an encoder to map it into a latent code and add an editing direction vector inferred from the emotion vector to generate the transformed texture from the modified latent code by the StyleGAN. For the shape, we use a shape transformation network to transform the 3DMM coefficients according to the emotion vector. Windowed smoothing is applied to ensure inter-frame continuity. Next, we render the face with the new shape and texture. Finally, we use a teeth filling module to fill the cavity in the teeth area. coefficients of a 3DMM face model from I. The coefficients are in the form of (α, β, δ, γ, p) ∈ R 257 , where α ∈ R 80 is the shape coefficient, β ∈ R 64 is the expression coefficient, δ ∈ R 80 is the texture coefficient, γ ∈ R 27 is the lighting coefficient, and p ∈ R 6 is the pose coefficient including rotation and translation. The reconstructed pose-independent 3D face shape S is represented by S = S+B shape α+B exp β, where the mean shape S and shape bases B shape are from the 3DMM called BFM09  [21]  and the expression bases B exp are from FaceWarehouse  [22] . We use predefined UV coordinates (Fig.  3c ) for each vertex of the 3D face mesh S, and build a mapping f between the UV coordinates and image pixels by reprojecting S onto the image plane (Fig.  3b ) using the standard rasterization pipeline. Finally, we extract the texture (Fig.  3d ) of S from I using the mapping f .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Level-1 Shape Transformation",
      "text": "One of the key challenges in our system is how to transform emotion-related expressions while retaining lip synchronization (which preserves speech-related expressions to some extent). In other words, when editing facial expressions, we must ensure that the transformed mouth shape still matches the pronunciation. One possible way is to collect paired training data of 3DMM coefficients that are aligned to the same phoneme under different facial expressions and then learn the mappings. However, it is laborious and difficult to collect such precise paired data. In our system, we design a shape transformation network trained by unpaired training data with a cycle-consistent training scheme similar to  [11] ,  [12]  (Section 3.3.1), which supports pose-independent expression type editing and intensity control (Section 3.3.2). In addition, we design elaborate loss terms to preserve the mouth shape and personal identity (Section 3.3.3).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Network Architecture",
      "text": "The shape transformation network is a GAN model composed of a generator G and a discriminator D. The generator is a fully connected network including an input layer, four hidden layers with LeakyReLU activation and an output layer. The input to G consists of the emotion vector e, the shape and expression coefficients α, β of the 3DMM, so the input layer has n c + n e neurons, where n c is the dimension of the concatenated coefficients c = (α, β) and n e is the number of the emotion types except for the neutral expression. Each hidden layer has n h neurons whose value is determined empirically in Section 4.1.2, and the output layer has n c neurons.\n\nThe discriminator has a structure similar to G, except that the input layer has n c neurons and the output layer consists of two branches: one branch outputs the estimated probability D rf (•) of whether the input signal to D is real, and the other branch estimates the emotion vector e ′ = D reg (•) in the input signal to D.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expression Type Editing And Intensity Control",
      "text": "Given the 3DMM coefficients c = (α, β) ∈ R nc and the target emotion vector e ∈ R ne , the pose-independent editing of expression types with intensity control at the level-1 shape transformation is achieved by using the generator G to predict new coefficients G(c, e) ∈ R nc . Then the edited 3D shape with the original pose in the input frame I can be reconstructed by the 3DMM with the coefficients c ′ = (α ′ , β ′ ) = G(c, e) and the pose coefficient p.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss Function",
      "text": "We adopt the Wasserstein GAN objective with gradient penalty  [49] ,  [50]  to calculate the adversarial loss L adv and the gradient penalty loss L gp , using the output of the probability branch D rf of the discriminator. To ensure that the generated coefficients match the given emotion vector, we propose a regression loss term. For the discriminator,\n\nwhere e is the true emotion vector (inferred from the emotion labels in the training data) associated with the shape coefficients c. And for the generator,\n\nDenote the 3D mesh shape reconstructed from 3DMM coefficients c = (α, β) using the bases (B shape , B exp ) by S(c) and its vertex-wise vector form by V S(c) , i.e., each entity v i ∈ V S(c) is the (x, y, z) coordinate of the ith vertex in the mesh S(c). In our experiments, we find that measuring the difference in the 3D shape space rather than in the coefficient space leads to better results. Thus, we propose the cycle-consistency loss L rec as\n\nwhere c is a shape coefficient with a neutral expression, c * is a shape coefficient with a non-neutral expression e * , both sampled from the dataset. The target emotion vectors are set to a random non-neutral expression e and a neutral expression 0, respectively. Note that G(G(c, e), 0) transforms the edited shape G(c, e) with expression e back into a shape with the neutral expression and similar explanation for G(G(c * , 0), e * ) holds.\n\nWe propose a mouth shape preservation loss to force the transformed mouth shape to be similar to the original one. Different from  [42]  that calculates the absolute 3D distance of selected key points between the input and transformed lip vertices, we first measure the relative distance from the upper lip to the lower lip and then calculate the difference of the relative distance between the input and transformed one. We believe the relative distance can better model the opening and closing of the mouth and is less affected by lateral movements. This is especially important for conveying emotions such as contempt, where the mouth is often slightly puckered to one side. The loss L mouth is proposed as\n\nwhere V Su(c) is the vector of selected key points of the upper lip in the mesh corresponding to the 3DMM coefficient c, i.e., each element v ui ∈ V Su(c) is the (x, y, z) coordinate of the u i th key points of the upper lip. V S d (c) is defined similarly for the lower lip. Finally, to avoid producing large and abnormal distortion, we add a regularizing term to constrain the deformation caused by the shape coefficient α. We measure the difference of the mesh reconstructed from only the original and generated shape coefficient, i.e., (α, 0) and (α ′ , 0) where the α and α ′ are taken from c and G(c, e) and the expression coefficients are both set to 0:\n\n(\n\nTo summarize, the objective functions for G and D are\n\nwhere the weights λ gp , λ reg , λ rec , λ mouth , λ r are specified in Section 4.1.2.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Level-2 Texture Transformation",
      "text": "The texture contains a good amount of emotion-related information, such as fine appearance details and subtle variations between frames, which are expressed through the facial colors and fine texture details that the coarse geometric shape cannot represent (e.g., wrinkles). In our method, we train a StyleGAN (Section 3.4.1) to generate texture maps from edited latent codes, where the edited latent codes are obtained by projecting the input texture maps to the StyleGAN's latent space using an encoder (Section 3.4.2), and then modified based on the emotion vectors (Section 3.4.3).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Stylegan",
      "text": "The StyleGAN  [15] ,  [16]  is a state-of-the-art image generation model introduced by Karras et al., which uses a stylebased approach to generate highly realistic images. Given a latent code z in the input latent space Z, a mapping network f : Z → W first map it to an intermediate latent code w in the intermediate latent space W. Then, w is replicated and converted by n learned affine transformations {A i |i = 1, 2, . . . , n} into n styles, which are used to modulate the n convolution layers of StyleGAN's synthesis network. The synthesis network generates the output image by upsampling from a learned constant input, guided by the styles. We denotes this image generation process using the following equations, where t is the generated image:\n\nSpecifically, for each actor, we train a StyleGAN-based model  [16]  using the texture maps extracted from the training videos. More training details can be found in Section 4.1.2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Encoder",
      "text": "Given a real image t, StyleGAN itself cannot determine the corresponding latent code z or the intermediate latent code w for the most accurate image reconstruction. Moreover, the Z or W space has been found to have limited capacity to express real images. Therefore, it is common practice to invert images into an extended latent space W+, defined by the stacking of n different w codes, by either optimization  [16] ,  [34] ,  [35] ,  [36] , or regression  [17] ,  [18] ,  [19] . The process of inversion and reconstruction can be expressed as follows:\n\nwhere the reconstructed image t is expected to be similar to the real image t. For simplicity, we also express Eq.(  11 ) as t = stylegan(W) with a slight abuse of notation.\n\nAs optimization is time-consuming, we adopt an encoder  [17]  which maps the real image t into the W+ space by regression. The encoder first extracts multi-resolutional feature maps from t using a standard feature pyramid over a ResNet backbone  [51] . It then applies n fully convolutional networks to transform the feature maps into n w codes. We represent this process as W = enc(t), which is analogous to Eq.  (10) .\n\nWe use the trained StyleGAN in the previous section as the decoder with fixed weights and train the encoder by minimizing the pixel difference and perceptual distance between the input training texture map t and reconstructed texture map t = stylegan(enc(t)). Refer to Section 4.1.2 for further details.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Texture Editing In The Latent Space",
      "text": "After training the StyleGAN and encoder for the texture maps, we calculate the editing direction vector in the Style-GAN's latent space for each non-neutral emotion. Denote the set of all the training texture maps with the highest intensity of emotion x as T x (for the neutral emotion x ′ , all the texture maps of x ′ are selected into T x ′ ), we compute the editing direction vectors in the following two steps: (1) for each emotion x, map all the texture maps in T x to the latent space and calculate the average latent code, and (2) for each non-neutral emotion y, calculate the difference between the average latent code of y and the neutral emotion, which yields the editing direction vector d y . These two steps can be formulated as:\n\nDuring editing, given an extracted texture t 0 with the neutral emotion and an emotion vector e with only one nonzero element e y (which indicates the target emotion type is y and the intensity is e y ), the texture transformation is performed by:\n\nwhere we first get the latent code of t 0 , add the intensityweighted editing direction vector, and then let the Style-GAN generate the transformed texture from the modified latent code. Note that in order to make the intensity change of transformed texture as linearly as possible, we only use the textures with the highest emotion intensity to calculate the editing direction vectors, rather than considering all intensities and calculating the editing direction vectors for each intensity range. The reason is that the former makes the modified latent code a linear function of the intensity, while the latter may lead to different rates of change of the latent code in different intensity ranges. Furthermore, since the texture captures the appearance details, a texture that varies linearly with the intensity will greatly contribute to the final rendered face expression that varies linearly with intensity.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Refinement",
      "text": "After transforming the shape (level-1) and texture (level-2), we render the new face with modified expressions and paste it back into the original image. Several special treatments need to be considered to achieve high-quality expression editing, lip synchronization and inter-frame continuity, which are presented in the following subsections.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Windowed Smoothing",
      "text": "If smoothing is not applied, the generated shapes and textures may be temporally discontinuous (e.g., containing jitters) because they are obtained in a frame-by-frame manner. Thanks to our decoupled representations of the shape and texture, we can apply a simple operation that smooths the 3DMM coefficients in the shape space and the latent codes in the texture space. I.e., for the frame I t , we average the edited 3DMM coefficients and latent codes in I t-1 , I t and I t+1 using the weight of a Hanning window.\n\nBenefiting from our elaborately designed two-level expression representation, the simple smoothing operation works, because (1) the 3DMM we adopt is a linear model, and (2) the latent codes in the texture space generated from consecutive frames are close to each other (i.e., they can be thought of as lying in a hyperplane locally approximating the tangent plane of an expression manifold) and then can be smoothed by a simple average.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Teeth Completion",
      "text": "Note that the 3DMM we adopt does not model the teeth and the interior of the mouth, which leaves a blank area within the mouth in the rendered image, as shown in Figure  2 . Thus, we propose a teeth filling module (see bottom right in Figure  2 ) to fill this cavity, which is a typical inpainting problem. Since humans are particularly sensitive to the artifacts around the mouth, we utilize a StyleGAN and an encoder which are the same kinds of networks (with smaller sizes) in Section 3.4) in this module to ensure high visual quality and inter-frame continuity.\n\nWe use the following steps to collect the training data for the teeth filling module. First, for all the training frames of an actor, we only run the reconstruction step and the rendering step in our pipeline, i.e., skipping the editing step and teeth filling step, to get paired face images with and without teeth (see the first row in Figure  4  for an example). Second, to handle faces with various poses, we estimate a projective transformation to align the mouth area. Specifically, after reconstructing the original image, we first get the 2D mouth landmarks l. Then we set the pose parameter of the face model to 0 and project it to the image plane to obtain the frontalized mouth landmarks l ′ . Finally, we estimate the prospective transformation from l and l ′ , apply it to the face images and crop the mouth area (see the second row of Figure  4    During editing, given a rendered frame without teeth, we align and crop the mouth area in the same way, map it to a toothed latent code with the encoder, and use the mouth StyleGAN to generate the mouth image with teeth from the latent code. Then, we paste it back to the rendered frame to obtain the final output frame.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Masked Blending",
      "text": "To seamlessly blend the rendered face with the original background, we use a soft mask where the value gradually increases from 0 to 1 near the edge of the face. We get this mask by first eroding the original hard mask and then applying a Gaussian blur step.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiment Setup",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiments on two datasets: MEAD  [52]  and RAVDESS  [53] . The MEAD dataset contains high-quality talking-face videos of 60 actors with eight categories of emotions (i.e., neutral, happy, sad, angry, fearful, surprised, disgusted and contemptuous) at three intensity levels, recorded from seven different view angles. For every actor, there are about 30 videos for each intensity of each emotion at each view angle. We select four actors and for each of them, we sample one out of every five frames from the videos with the frontal view to obtain experimental data, resulting in about 15,000 frames per actor. To show the generalization of our model, we also conduct experiments on RAVDESS  [53]  (Section 4.5), another emotional audio-visual dataset with eight categories of emotions (i.e., neutral, happy, sad, angry, fearful, surprised, disgusted and calm) at two intensity levels. However, each intensity has only four frontal-view videos, which are far fewer than MEAD. Therefore, we extract all the video frames (about 7,000 frames per actor) for the experiment.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation And Training Details",
      "text": "Our method is implemented using PyTorch. The parameters for the shape transformation network are set as follows: n c = 144, n e = 7, n h = 512 and λ gp = 10, λ reg = 20, λ rec = 5e 3 , λ mouth = 1, λ r = 1e 3 . For each actor, we train the shape transformation network using Adam optimizer with lr = 1e -3 , β 1 = 0.5 and β 2 = 0.999 for 80k iterations with a batch size of 64.\n\nFor each actor, we train a 512 × 512 StyleGAN for the texture with a batch size of 16 for 100k iterations, and train its encoder with a batch size of 4 for 100k iterations. For the teeth filling module, we train a 128 × 128 StyleGAN with a batch size of 32 for 30k iterations, and train its encoder with a batch size of 4 for 60k iterations. The other settings of the StyleGAN and the encoder are the same as in  [16]  and  [17] .\n\nBenefiting from the powerful capabilities of StyleGAN, our method is easily generalized to produce results of higher resolution. To show this capacity, we also train a model with a 1024 × 1024 texture StyleGAN and a 256 × 256 teeth StyleGAN. This high-resolution model can produce much finer grained details. The corresponding results can be found in the supplemental demo video.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We use the following metrics to evaluate the effect of expression editing, lip synchronization and image quality of the generated results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Linearity Of Intensity Editing (Lie).",
      "text": "To easily control the editing of expression intensity using a scale value e y in [0, 1], it is desired that the change in perceived expression intensity is linear with the change in e y . To evaluate this linear property, we design a new perception-based scaleinvariant metric called LIE. Given an image x with the neutral expression and a target emotion type y, we first generate n + 1 images from x using emotion vectors of type y whose intensities increase linearly from 0 to 1. Then we use the LPIPS  [54]  model to calculate the pairwise perceptual difference between neighboring images (i.e., having neighboring intensity values), denoted as d i (i = 1, 2, . . . , n). Finally, we compute the LIE, which is defined as the coefficient of variation of d i :\n\nwhere σ(d i ) is the standard deviation of d i and µ(d i ) is the mean of d i . Note that LIE is non-negative, and when the map from e y to the perceived expression intensity is linear, LIE is exactly zero.\n\nFréchet emotion/inception distance (FED/FID). Apart from the Fréchet inception distance (FID)  [55]  that has been widely used to assess the quality of images created by a generative model, we use the Fréchet emotion distance (FED), where the Inception model in FID is replaced with an emotion recognition network  [56]  trained on the AffectNet  [57]  dataset which has the same emotion categories as in MEAD, to assess the effect (e.g., correctness and similarity) of expression editing.\n\nOther metrics. We use the cumulative probability of blur detection (CPBD)  [58]  to assess the sharpness of generated faces, and cosine similarity of the average ArcFace  [51]  feature for identity preservation (ID). We also use LSE-D and LSE-C from SyncNet  [5] ,  [59]  to assess the lip synchronization of generated videos.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "Image Space Latent Space Fig.  5 . Comparison of smoothing in image space and latent space.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "3Dmm-Based Face Representation",
      "text": "As noted in previous sections, while StyleGAN is capable of generating high-quality facial images and has been used for expression editing, preserving attributes like pose while only modifying the expression during editing is challenging, especially when strong biases exist in the relationship between expressions and poses. Therefore, we employ a 3DMM to first convert the face into a pose-independent shape and texture before editing each separately. To validate the necessity of such decomposition, we remove all 3DMMrelated modules (the face reconstruction module and the shape transformation network) and the teeth filling module. The ablation method, \"ours w/o 3DMM\", trains a Style-GAN and an encoder using face images directly and edits expressions in the latent space of the StyleGAN, which resembles typical StyleGAN-based latent code editing methods  [17] ,  [18] ,  [37] . Experiments show that the StyleGAN struggles to preserve the actor's pose during editing, as illustrated in the last column of Fig.  6  and the demo video.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Level-1 And Level-2 Transformation",
      "text": "Our method uses a two-level facial expression editing strategy, i.e., level-1 shape transformation and level-2 texture transformation. Here we present a study to verify that both levels of transformation are necessary. First, we generate 28 videos of seven non-neutral expression types for four actors using level-1 shape transformation only.\n\nHappiness and surprise are classified as positive expressions, while the remaining are classified as negative expressions. We then invite 11 users (seven males and four females, average age 25 years and SD = 2.75 years) and ask them to judge whether the edited expression is positive or negative. The results show that the accuracy is 74%, indicating that level-1 shape transformation can model the major facial movement and thus is necessary.\n\nNext, we show that both level-1 and level-2 transformations are necessary. For each actor, we generate 105 videos with seven non-neutral expression types and three intensities from five neutral videos with or without level-1 or level-2 transformation, respectively. Based on the generated videos, we compute the values of FED, FID and ID metrics, and the results are summarized in Table  1 . The results show that compared with only using level-1 transformation, using both levels of transformations can significantly improve the performance on three metrics, indicating that level-2 texture transformation can capture appearance details and their subtle changes between adjacent frames and thus is also necessary.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Smoothing Operation",
      "text": "To verify the effectiveness of the smoothing operation, we generate 14 pairs of videos with or without this operation (other conditions are the same). We invite eight users (four males and four females, average age 26 years and SD = 2.52 years), and asked them to choose in each pair the video that is better in terms of inter-frame continuity, audio-visual synchronization and visual clarity. The results (Table  2 ) show that our proposed smoothing operation significantly improves the inter-frame continuity and slightly improves visual clarity (by mitigating jitters). Interestingly, we find that the smoothing operation also helps to improve audiovisual synchronization, probably because they can alleviate some abrupt changes caused by noise during generation. Our smoothing operation smooths texture maps using the latent codes in the texture space. To verify this special design, we compare the results of image-space smoothing (i.e., average by pixel information) and latent-space smoothing. Some qualitative results are shown in Figure  5 , from which we observe that the image-space smoothing may lead to blurry or even unrealistic results. In contrast, the latentspace smoothing can consistently produce clear results.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Mouth Shape Preservation Loss",
      "text": "We propose a novel mouth shape preservation loss (ref.\n\nEq.(  4 )) in the loss function (Eq.(  7 )). The weight λ mouth of L mouth in Eq.(  7 ) controls the trade-off between the preservation of the original mouth shape and the degree of exaggeration of the edited expression. Some qualitative results For brevity, we refer to Dynamic Neural Texture  [8]  as DNT, Emotional Video Portraits  [7]  as EVP, StarGAN  [12]  as Star, ExprGAN  [30]  as Expr, and Wav2Lip-Emotion  [43]  as W2L-E. The input of all methods contains a video with a neutral emotion and a target emotion vector, and we show the frames from the generated videos. The red boxes indicate some artifacts (e.g., misaligned and eroded areas). are shown in Figure  7 , showing that zero or small weights may lead to degraded lip synchronization and abnormal expressions, while large weights may lead to degraded expressiveness of expression editing. We also include quantitative results in Table  3 , where we compare the mouth shape preservation loss based on relative distance, absolute distance, and no mouth shape preservation loss. The results clearly demonstrate that using relative distance yields the best outcome.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With State Of The Arts",
      "text": "We compare our method with three state-of-the-art methods, Dynamic Neural Texture  [8] , Emotional Video Portraits  [7]  and Wav2Lip-Emotion  [43] , which are designed for generating or editing talking videos with controllable expressions. Since our expression editing task can be addressed by image-to-image translation (I2I) methods, we also compare two state-of-the-art I2I methods, including StarGAN  [12]  and ExprGAN  [30] . Both methods are conditional-GANs, where StarGAN is a multi-domain I2I method and Ex-prGAN is dedicated to expression editing for face images. We adapt StarGAN's input and objectives to accept emotion vectors as input. All the methods are individually trained using cropped face images from the MEAD dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Qualitative Evaluation",
      "text": "We present a qualitative analysis in this section. Some editing results of different identities (male vs. female, white vs. yellow) and expression types (happy, sad, angry and contempt) are illustrated in Figure  6  and the supplementary demo video. The input to each method includes a video clip with a neutral expression and a target emotion vector (we also input the original audio clip for DNT and EVP as they require audio input), and the output is a video with the expression type and intensity as specified in the emotion vector. The visual results in Figure  6  and demo video show that I2I methods fail to decouple the expression with the pose, background and other unrelated attributes, resulting in face misalignment and posture changes. The inter-frame continuity of the videos generated by I2I methods is also low. The results of Wav2Lip-Emotion suffer severely from facial distortion and inter-frame discontinuity. DNT and EVP can generate talking videos with desirable facial expressions, but there are still the following problems. The intensity of the generated expressions is not strong enough and the video quality of DNT is not good enough. EVP produces better video quality, but throughout the video, the texture of the face area is inconsistent (e.g., with teeth and wrinkles moving) and the background gradually erodes the face. This is probably because EVP generates images based on edges, causing the distortion of texture information in edge areas. Moreover, compared to the ground truth in the dataset, some expressions generated by EVP are not accurate. By contrast, our method has good visual quality, high accuracy of expression editing, and good temporal consistency.\n\nOur method is specially designed so that the edited facial expressions can vary smoothly with different intensity values. To demonstrate this property, results with different expression types and intensities are generated by our method, which are shown in Figure  8 . In particular, our method takes an image with neutral emotion as input, and the intensity values of each expression type increase linearly from 0 to 1. The results show that our method can smoothly change facial expressions (for different types or intensity values) while preserving other attributes. Note that in the training dataset from MEAD, there are only three discrete intensity levels, whereas our method can generate results with intensity values continuously varying in [0, 1] by interpolation in the latent space characterizing facial expressions.\n\nThe reader is also referred to the supplementary demo video for more results and comparisons with other methods. Next, we evaluate all the methods using metrics FED, FID, ID, LSE-D, LSE-C and CPBD based on the generated videos. Given videos with a neutral expression, we edit them into videos with non-neutral expressions in three intensities (except for W2L-E which can only generate videos with happy or sad expressions in one intensity). We select five videos for each actor and compute the FED and FID between the frames of the generated video and the corresponding video with the target expression type in the MEAD dataset. The ID is computed with the input. As shown in Table  5 , our method achieves the lowest FED and FID, and the highest ID, indicating that (1) our edited expressions are the closest to ground truth videos with the target expression type and (2) the identities of the characters are best preserved. Moreover, our method has the secondbest LSE-D and LSE-C scores. We note that there is a tradeoff between the ability to express emotions and the preservation of the original mouth shape, and it is worthwhile to have a reasonable deformation of the mouth, which is essential to express some strong emotions such as happiness, anger and surprise. Although the lip synchronization of our method is slightly worse than StarGAN, it is still very close to the input, and the trade-off gives our method a better expressiveness ability for emotions with strong intensity. In addition, this trade-off is controllable by adjusting the weight of L mouth . Our method also has the highest CPBD among all the methods, and even higher CPBD can be easily achieved by increasing the resolution of the StyleGAN.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "User Study",
      "text": "To further examine the effectiveness of our method, we use different methods to generate 14 videos covering all seven facial expression types and four actors, and perform a user study in which 20 participants (12 males and 8 females, average age 26 years and SD = 3.94 years) are invited and asked to rate the results in terms of various metrics, including the accuracy of the generated expression, identity preservation, audio-visual synchronization, interframe continuity and overall quality. Table  6  summarizes the average ratings, where our method outperforms other methods in all aspects.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Additional Results",
      "text": "We provide more animated editing results and comparisons in the supplemental demo videos. In addition to the MEAD dataset, we further conduct experiments on another dataset named RAVDESS  [53] . Even though the dataset is about onetenth the size of MEAD, we still achieve relatively good editing results, as shown in Figure  9  and the demo video. This suggests that our method has potential in adapting to new datasets. We also note that our method uses 3DMM to decouple face shape from pose and scale, thus making our method robust to changes in pose and scale. This even enables our  method to edit faces with those poses unseen in the training data, as shown in Figure  10  and the demo video.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Right 30°Down Top",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a simple yet effective method for high-quality facial expression editing in talking videos. Our method can output videos with specified target emotions of continuous intensities. Our key idea is regarding facial expression editing in talking videos as a special motion information editing, and we propose a two-level transformation strategy. This strategy uses a 3DMM to capture major facial movements (level-1) and an associated texture map modeled by StyleGAN to capture appearance details (level-2). Extensive experiments, including qualitative and quantitative evaluations, an ablation study and a user study show the effectiveness of this strategy and demonstrate that our method outperforms existing methods and achieves a good trade-off in terms of commonly used quality measures such as FID, FED, CPBD, ID, LSE-D and LSE-C, as well as a new proposed metric LIE. In the future work, we plan to explore the feasibility of training a universal model capable of generating high-quality results for multiple people.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Appendix Training And Inference Speed",
      "text": "All experiments are conducted on a server with the Intel Xeon Gold 6126 CPU and NVIDIA Titan RTX GPUs. Training two StyleGANs takes 28 hours on four GPUs and training two corresponding encoders and a shape transformation network takes 20 hours on one GPU. During inference, we generate 1080p videos at a rate of about 6 fps. However, we believe this rate can be further improved by utilizing batch processing instead of processing each frame individually.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Multi-Label Emotion Editing",
      "text": "Our method is specifically designed for generating a single emotion. However, we find it has a certain ability to generate multi-label emotions. We present some results in",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Editing Of Audio-Driven Talking Faces",
      "text": "Our method can also be used to edit the facial expressions and improve the visual quality of audio-driven talking face videos. We show the editing result of a video generated by Wav2Lip  [1]",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Failure Cases Of Unseen Large Poses",
      "text": "Although our model is robust to changes in pose and scale and can handle small poses that were not seen during training, it may still fail when the poses differs greatly from those in the dataset. In such situations, the trained texture may fail to adequately cover the face or reflect the specular light from the new angle. Additionally, the projective transformation used to align the generated frontal mouth area may produce unnatural results.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Using Other 3Dmm",
      "text": "We substitute our 3DMM with FLAME  [2]  and employ DECA  [3]  for 3D face reconstruction. We select only the facial area of the FLAME mesh, and use the \"shape\", \"exp\" and jaw pose parameters in the shape transformation network. The other settings are kept the same as in previous experiments. We show qualitative in Fig.  A .4 and quantitative results in Table A.1. There are no significant differences between the two settings except that our original setting has better lip shape preservation, indicating our method can be adapted to other face models and reconstruction methods.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our method takes a video with neutral facial expression (top row) and target emotion labels (including emotion type and intensity)",
      "page": 2
    },
    {
      "caption": "Figure 2: The input of our method includes a talking face video",
      "page": 4
    },
    {
      "caption": "Figure 3: a), we reconstruct a 3D face",
      "page": 4
    },
    {
      "caption": "Figure 2: The pipeline of our method. (Top row) Facial expression editing is done frame by frame, and for each frame, a windowed smoothing operation",
      "page": 5
    },
    {
      "caption": "Figure 3: We extract the texture from the input image based on the",
      "page": 5
    },
    {
      "caption": "Figure 3: c) for each ver-",
      "page": 5
    },
    {
      "caption": "Figure 3: b) using the standard rasterization",
      "page": 5
    },
    {
      "caption": "Figure 3: d) of S from I",
      "page": 5
    },
    {
      "caption": "Figure 2: Thus, we propose a teeth filling module (see bottom right",
      "page": 7
    },
    {
      "caption": "Figure 2: ) to fill this cavity, which is a typical inpainting",
      "page": 7
    },
    {
      "caption": "Figure 4: for an example).",
      "page": 8
    },
    {
      "caption": "Figure 4: We collect training data for the teeth filling module from the",
      "page": 8
    },
    {
      "caption": "Figure 5: Comparison of smoothing in image space and latent space.",
      "page": 9
    },
    {
      "caption": "Figure 6: and the demo video.",
      "page": 9
    },
    {
      "caption": "Figure 6: Comparison of edited results by different methods. For brevity, we refer to Dynamic Neural Texture [8] as DNT, Emotional Video Portraits [7]",
      "page": 10
    },
    {
      "caption": "Figure 7: The generated expression using different values of λmouth.",
      "page": 10
    },
    {
      "caption": "Figure 7: , showing that zero or small weights",
      "page": 10
    },
    {
      "caption": "Figure 6: and the supplementary",
      "page": 10
    },
    {
      "caption": "Figure 8: Editing results of different facial expression types and intensities generated by our method. The input includes an image with a neutral",
      "page": 11
    },
    {
      "caption": "Figure 6: and demo video show that",
      "page": 11
    },
    {
      "caption": "Figure 8: In particular, our",
      "page": 11
    },
    {
      "caption": "Figure 9: and the demo video.",
      "page": 12
    },
    {
      "caption": "Figure 9: Editing results (right) on the RAVDESS [53] dataset. The input",
      "page": 12
    },
    {
      "caption": "Figure 10: Our method can edit faces with poses unseen in the training",
      "page": 12
    },
    {
      "caption": "Figure 10: and the demo video.",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reconstruction": "& rendering"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: , our method achieves the lowest FED",
      "data": [
        {
          "Metric": "FED↓\nFID↓\nID↑",
          "Ours\nDNT\nEVP\nW2L-E": "9.40\n11.06\n16.71\n26.75\n30.36\n56.42\n53.42\n111.65\n0.931\n0.921\n0.791\n0.686",
          "Star\nExpr": "14.33\n19.44\n47.47\n90.19\n0.921\n0.917",
          "Input": "N/A\nN/A\nN/A"
        },
        {
          "Metric": "LSE-D↓\nLSE-C↑",
          "Ours\nDNT\nEVP\nW2L-E": "8.03\n10.72\n10.61\n8.43\n7.11\n3.68\n4.14\n6.40",
          "Star\nExpr": "7.64\n8.17\n7.59\n7.14",
          "Input": "7.45\n7.84"
        },
        {
          "Metric": "CPBD↑",
          "Ours\nDNT\nEVP\nW2L-E": "0.166\n0.121\n0.152\n0.124",
          "Star\nExpr": "0.096\n0.024",
          "Input": "0.213"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Hierarchical crossmodal talking face generation with dynamic pixel-wise loss",
      "authors": [
        "L Chen",
        "R Maddox",
        "Z Duan",
        "C Xu"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "Neural voice puppetry: Audio-driven facial reenactment",
      "authors": [
        "J Thies",
        "M Elgharib",
        "A Tewari",
        "C Theobalt",
        "M Nießner"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "3",
      "title": "Realistic speech-driven facial animation with gans",
      "authors": [
        "K Vougioukas",
        "S Petridis",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "4",
      "title": "Predicting personalized head movement from short video and speech signal",
      "authors": [
        "R Yi",
        "Z Ye",
        "Z Sun",
        "J Zhang",
        "G Zhang",
        "P Wan",
        "H Bao",
        "Y.-J Liu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "5",
      "title": "A lip sync expert is all you need for speech to lip generation in the wild",
      "authors": [
        "K Prajwal",
        "R Mukhopadhyay",
        "V Namboodiri",
        "C Jawahar"
      ],
      "year": "2020",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Lipsync3d: Data-efficient learning of personalized 3d talking faces from video using pose and lighting normalization",
      "authors": [
        "A Lahiri",
        "V Kwatra",
        "C Fr Üh",
        "J Lewis",
        "C Bregler"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "7",
      "title": "Audio-driven emotional video portraits",
      "authors": [
        "X Ji",
        "H Zhou",
        "K Wang",
        "W Wu",
        "C Loy",
        "X Cao",
        "F Xu"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Dynamic neural textures: Generating talking-face videos with continuously controllable expressions",
      "authors": [
        "Z Ye",
        "Z Sun",
        "Y Wen",
        "Y Sun",
        "T Lv",
        "R Yi",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Neural emotion director: Speech-preserving semantic control of facial expressions in \"in-the-wild\" videos",
      "authors": [
        "F Papantoniou",
        "P Filntisis",
        "P Maragos",
        "A Roussos"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "12",
      "title": "Stargan: Unified generative adversarial networks for multi-domain imageto-image translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Interpreting the latent space of gans for semantic face editing",
      "authors": [
        "Y Shen",
        "J Gu",
        "X Tang",
        "B Zhou"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "14",
      "title": "Icface: Interpretable and controllable face reenactment using gans",
      "authors": [
        "S Tripathy",
        "J Kannala",
        "E Rahtu"
      ],
      "year": "2020",
      "venue": "WACV"
    },
    {
      "citation_id": "15",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "16",
      "title": "Analyzing and improving the image quality of stylegan",
      "authors": [
        "T Karras",
        "S Laine",
        "M Aittala",
        "J Hellsten",
        "J Lehtinen",
        "T Aila"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "17",
      "title": "Encoding in style: A stylegan encoder for image-to-image translation",
      "authors": [
        "E Richardson",
        "Y Alaluf",
        "O Patashnik",
        "Y Nitzan",
        "Y Azar",
        "S Shapiro",
        "D Cohen-Or"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Designing an encoder for stylegan image manipulation",
      "authors": [
        "O Tov",
        "Y Alaluf",
        "Y Nitzan",
        "O Patashnik",
        "D Cohen-Or"
      ],
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "19",
      "title": "Restyle: A residualbased stylegan encoder via iterative refinement",
      "authors": [
        "Y Alaluf",
        "O Patashnik",
        "D Cohen-Or"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "20",
      "title": "A morphable model for the synthesis of 3d faces",
      "authors": [
        "V Blanz",
        "T Vetter"
      ],
      "year": "1999",
      "venue": "SIGGRAPH"
    },
    {
      "citation_id": "21",
      "title": "A 3d face model for pose and illumination invariant face recognition",
      "authors": [
        "P Paysan",
        "R Knothe",
        "B Amberg",
        "S Romdhani",
        "T Vetter"
      ],
      "year": "2009",
      "venue": "AVSS"
    },
    {
      "citation_id": "22",
      "title": "Facewarehouse: A 3d facial expression database for visual computing",
      "authors": [
        "C Cao",
        "Y Weng",
        "S Zhou",
        "Y Tong",
        "K Zhou"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Vis. Comput. Graph"
    },
    {
      "citation_id": "23",
      "title": "Learning a model of facial shape and expression from 4d scans",
      "authors": [
        "T Li",
        "T Bolkart",
        "M Black",
        "H Li",
        "J Romero"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "24",
      "title": "3d morphable face models -past, present, and future",
      "authors": [
        "B Egger",
        "W Smith",
        "A Tewari",
        "S Wuhrer",
        "M Zollh Öfer",
        "T Beeler",
        "F Bernard",
        "T Bolkart",
        "A Kortylewski",
        "S Romdhani",
        "C Theobalt",
        "V Blanz",
        "T Vetter"
      ],
      "year": "2020",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "25",
      "title": "Reconstruction of personalized 3d face rigs from monocular video",
      "authors": [
        "P Garrido",
        "M Zollh Öfer",
        "D Casas",
        "L Valgaerts",
        "K Varanasi",
        "P Pérez",
        "C Theobalt"
      ],
      "year": "2016",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "26",
      "title": "3d face reconstruction with geometry details from a single image",
      "authors": [
        "L Jiang",
        "J Zhang",
        "B Deng",
        "H Li",
        "L Liu"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "27",
      "title": "Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set",
      "authors": [
        "Y Deng",
        "J Yang",
        "S Xu",
        "D Chen",
        "Y Jia",
        "X Tong"
      ],
      "year": "2019",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "28",
      "title": "Weakly-supervised multi-face 3d reconstruction",
      "authors": [
        "J Zhang",
        "L Lin",
        "J Zhu",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "29",
      "title": "Learning an animatable detailed 3d face model from in-the-wild images",
      "authors": [
        "Y Feng",
        "H Feng",
        "M Black",
        "T Bolkart"
      ],
      "year": "2021",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "30",
      "title": "Exprgan: Facial expression editing with controllable expression intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "31",
      "title": "3d guided fine-grained face manipulation",
      "authors": [
        "Z Geng",
        "C Cao",
        "S Tulyakov"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Stylerig: Rigging stylegan for 3d control over portrait images",
      "authors": [
        "A Tewari",
        "M Elgharib",
        "G Bharaj",
        "F Bernard",
        "H Seidel",
        "P Pérez",
        "M Zollh",
        "C Theobalt"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "33",
      "title": "Ganmut: Learning interpretable conditional space for gamut of emotions",
      "authors": [
        "S Apolito",
        "D Paudel",
        "Z Huang",
        "A Romero",
        "L Gool"
      ],
      "year": "2021",
      "venue": "CVPR. Computer Vision Foundation / IEEE"
    },
    {
      "citation_id": "34",
      "title": "Precise recovery of latent vectors from generative adversarial networks",
      "authors": [
        "Z Lipton",
        "S Tripathi"
      ],
      "year": "2017",
      "venue": "ICLR (Workshop)"
    },
    {
      "citation_id": "35",
      "title": "Image2stylegan: How to embed images into the stylegan latent space?",
      "authors": [
        "R Abdal",
        "Y Qin",
        "P Wonka"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "36",
      "title": "Image2stylegan++: How to edit the embedded images?",
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "37",
      "title": "High-fidelity GAN inversion for image attribute editing",
      "authors": [
        "T Wang",
        "Y Zhang",
        "Y Fan",
        "J Wang",
        "Q Chen"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "38",
      "title": "A note on the evaluation of generative models",
      "authors": [
        "L Theis",
        "A Van Den Oord",
        "M Bethge"
      ],
      "year": "2016",
      "venue": "Proc. Int. Conf. Learn. Representations"
    },
    {
      "citation_id": "39",
      "title": "Adversarially learned inference",
      "authors": [
        "V Dumoulin",
        "I Belghazi",
        "B Poole",
        "A Lamb",
        "M Arjovsky",
        "O Mastropietro",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Learn. Representations"
    },
    {
      "citation_id": "40",
      "title": "Ganspace: Discovering interpretable GAN controls",
      "authors": [
        "E Härk Önen",
        "A Hertzmann",
        "J Lehtinen",
        "S Paris"
      ],
      "year": "2020",
      "venue": "Ganspace: Discovering interpretable GAN controls"
    },
    {
      "citation_id": "41",
      "title": "Closed-form factorization of latent semantics in gans",
      "authors": [
        "Y Shen",
        "B Zhou"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "Real-time facial expression transformation for monocular RGB video",
      "authors": [
        "L Ma",
        "Z Deng"
      ],
      "year": "2019",
      "venue": "Comput. Graph. Forum"
    },
    {
      "citation_id": "43",
      "title": "Invertable frowns: Video-to-video facial emotion translation",
      "authors": [
        "I Magnusson",
        "A Sankaranarayanan",
        "A Lippman"
      ],
      "year": "2021",
      "venue": "ADGD @ ACM Multimedia"
    },
    {
      "citation_id": "44",
      "title": "Deep semantic manipulation of facial videos",
      "authors": [
        "G Solanki",
        "A Roussos"
      ],
      "year": "2022",
      "venue": "ECCV Workshops (6), ser"
    },
    {
      "citation_id": "45",
      "title": "DeepFaceVideoEditing: Sketch-based deep editing of face videos",
      "authors": [
        "F.-L Liu",
        "S.-Y Chen",
        "Y.-K Lai",
        "C Li",
        "Y.-R Jiang",
        "H Fu",
        "L Gao"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "46",
      "title": "Alias-free generative adversarial networks",
      "authors": [
        "T Karras",
        "M Aittala",
        "S Laine",
        "E Härk Önen",
        "J Hellsten",
        "J Lehtinen",
        "T Aila"
      ],
      "venue": "Proc. NeurIPS, 2021"
    },
    {
      "citation_id": "47",
      "title": "What comprises a good talking-head video generation?: A survey and benchmark",
      "authors": [
        "L Chen",
        "G Cui",
        "Z Kou",
        "H Zheng",
        "C Xu"
      ],
      "year": "2020",
      "venue": "What comprises a good talking-head video generation?: A survey and benchmark"
    },
    {
      "citation_id": "48",
      "title": "Audiodriven facial animation by joint end-to-end learning of pose and emotion",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "A Herva",
        "J Lehtinen"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "49",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "ICML"
    },
    {
      "citation_id": "50",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Improved training of wasserstein gans"
    },
    {
      "citation_id": "51",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "52",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "ECCV (21)"
    },
    {
      "citation_id": "53",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "54",
      "title": "The unreasonable effectiveness of deep features as a perceptual metric",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros",
        "E Shechtman",
        "O Wang"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "55",
      "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
      "authors": [
        "M Heusel",
        "H Ramsauer",
        "T Unterthiner",
        "B Nessler",
        "S Hochreiter"
      ],
      "year": "2017",
      "venue": "Gans trained by a two time-scale update rule converge to a local nash equilibrium"
    },
    {
      "citation_id": "56",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "SISY"
    },
    {
      "citation_id": "57",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hassani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "58",
      "title": "A no-reference image blur metric based on the cumulative probability of blur detection (CPBD)",
      "authors": [
        "N Narvekar",
        "L Karam"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "59",
      "title": "Out of time: Automated lip sync in the wild",
      "authors": [
        "J Chung",
        "A Zisserman"
      ],
      "year": "2016",
      "venue": "ACCV Workshops (2)"
    },
    {
      "citation_id": "60",
      "title": "A lip sync expert is all you need for speech to lip generation in the wild",
      "authors": [
        "K Prajwal",
        "R Mukhopadhyay",
        "V Namboodiri",
        "C Jawahar"
      ],
      "year": "2020",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "61",
      "title": "Learning a model of facial shape and expression from 4d scans",
      "authors": [
        "T Li",
        "T Bolkart",
        "M Black",
        "H Li",
        "J Romero"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "62",
      "title": "Learning an animatable detailed 3d face model from in-the-wild images",
      "authors": [
        "Y Feng",
        "H Feng",
        "M Black",
        "T Bolkart"
      ],
      "year": "2021",
      "venue": "ACM Trans. Graph"
    }
  ]
}