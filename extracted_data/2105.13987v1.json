{
  "paper_id": "2105.13987v1",
  "title": "Scalingnet: Extracting Features From Raw Eeg Data For Emotion Recognition",
  "published": "2021-02-07T08:54:27Z",
  "authors": [
    "Jingzhao Hu",
    "Chen Wang",
    "Qiaomei Jia",
    "Qirong Bu",
    "Jun Feng"
  ],
  "keywords": [
    "Deep Learning",
    "Convolutional Neural Networks",
    "EEG",
    "emotion recognition",
    "ScalingNet"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in a variety of tasks. Recently, CNNs based methods that are fed with hand-extracted EEG features gradually produce a powerful performance on the EEG data based emotion recognition task. In this paper, we propose a novel convolutional layer allowing to adaptively extract effective data-driven spectrogram-like features from raw EEG signals, which we reference as scaling layer. Further, it leverages convolutional kernels scaled from one data-driven pattern to exposed a frequency-like dimension to address the shortcomings of prior methods requiring hand-extracted features or their approximations. The proposed neural network architecture based on the scaling layer, references as ScalingNet, has achieved the state-of-the-art result across the established DEAP benchmark dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a very important role in human-computer interaction  [1] . Through recognizing human emotions more accurately and quickly, we can promote a smarter life  [2] . Generally, expressive modalities are used to judge human being's emotions, such as facial expressions, audio-visual expressions, and body language, etc.  [3] . In recent years, more and more studies that recognize human emotions have used physiological electrical signals  [4]   [5] , such as electrocardiogram (ECG), electromyography (EMG) and electroencephalography (EEG). Among them, EEG signals can better reflect real human emotions because it is not affected by subjective factor  [6] . In this work, we use EEG signals to recognize human emotions.\n\nIt has been proved that there are intimate correlations between human emotions and their different brain states  [7]   [8] . With the progress in EEG hardware equipment, it is more convenient to collect EEG signals with a high sampling rate nowadays  [9] . Meanwhile, the processing and analysis methods of EEG signals are being explored and researched constantly  [10] . In EEG based emotion recognization, researchers mainly focus on three technical sects. The first and most widespread methods are based on feature engineering and machine learning algorithms to recognize human arXiv:2105.13987v1 [eess.SP] 7 Feb 2021 emotions  [11] , which requires hand-extracted emotion-related features from EEG signals, such as Power Spectral Density (PSD), Differential Entropy (DE), etc. With the proposal of deep learning, some methods tend to combine feature engineering and deep neural networks, which replace classifiers from machine learning algorithms to deep neural networks, such as CNNs  [12] . Furthermore, some researchers consider extracting data-driven features from EEG signals, which employ parameterizable data representation methods or neural networks  [13]  as a feature extractor. While the feature extraction methods mentioned above achieved remarkable performance of EEG based emotion recognition, there is still potential for improvement. Hand-extracted features are mostly tasks related, and mostly require strong hypotheses and mathematically driven theoretical supports. Considering the reality, we may say that the hand-extracting of features is not easy works and potentially not robust.\n\nInspired by the shortcomings of hand-extracted feature based methods, we introduce an end-to-end artificial neural network method mainly constructed by our well-designed data-driven signal feature extracting layer, which we reference as ScalingNet, allowing to robustly performs raw EEG data based emotion recognition without requiring any handextracted features. The idea of the layer, which we reference as scaling layer, is to dynamically generate a series of convolution kernels scaled from one data-driven pattern to produce a robust data-driven spectrogram-like feature map from raw EEG signals for downstream tasks. The introduced architecture has several interesting properties:(1)It automatically extracts robust feature maps from raw EEG signals without any hand-interaction. (2)It handles any length of EEG signal without requiring data alignment. (3)It is fully convolutional. (4)It is compatible with the existing neural networks, providing robust feature extraction for different downstream tasks. We validate the proposed approach on the challenging DEAP benchmark dataset, achieving the state-of-the-art result that highlights the potential of models for data-driven feature extraction from raw EEG signals.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In EEG based emotion recognition, machine learning based methods fed with hand-extracted EEG features are possibly the most widely used framework. With the development of deep learning, researchers gradually tend to replace machine learning methods with deep neural networks, especially CNNs  [14] . The hand-extracted EEG features are mainly time domain, frequency domain, time-frequency domain and spatial signal features. The classification methods mainly include random forest, SVM, CNNs, LSTM, etc. Zheng et al.  [15]  extracted the time domain and frequency domain features of EEG, such as differential entropy(DE), Power Spectral Density(PSD), etc., and used Support Vector Machine (SVM) for emotion classification. Liu et al.  [16]  extracted time domain, frequency domain and time-frequency domain features, such as Hjorth, PSD, Discrete Cosine Transform(DCT), etc., use k-Nearest Neighbor(KNN) and Random Forest(RF) as classifiers for classification. Li et al.  [17]  proposed to perform Continuous Wavelet Transform(CWT) on the EEG signal of each channel, and then convert it to scalograms, then input the construction frame into CNNs and Long Short-Term Memory (LSTM) for emotion recognition. Kim et al.  [18]  proposed to extract brain asymmetry features and heart rate features, respectively, and ConvLSTM(Combination of CNN and LSTM) was used for classification.\n\nInspired by the powerful feature transform ability of deep neural networks, some researchers commit to design an end-to-end framework for EEG based emotion recognition. Wang et al.  [19]  proposed an EmotionNet network for EEG-based emotion classification. It can take EEG as input and uses 3-D convolution to extract spatial and temporal features for emotion recognition. However, for general purpose network layers, it is hard to learn and extract robust features from signals. In the long run, this research field still has great potential for development. Consider, there is a need for a special neural network layer that specially design for robust feature extraction from raw EEG signals, and a neat neural network architecture that can naturally inference on raw EEG signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we will firstly present the building block layer used to adaptively extract effective data-driven spectrogramlike features from raw EEG signals, which we reference as scaling layer. Then we will introduce a fully convolutional neural network constructed through basing the scaling layer, which we reference as ScalingNet since its core feature is the application of scaling layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Scaling Layer",
      "text": "The motivation is to dynamically generate a series of convolutional kernels by scaling one data-driven pattern to different periods in order to expose a frequency-like dimension from signals. This motivation brings the possibility of automatically adaptive extracting effective and robust data-driven spectrogram-like features for downstream tasks from raw EEG signals.\n\nWe consider a multi-kernel convolutional layer that takes a one-dimensional signal shaped like (sampling points, 1) as input and a two-dimensional spectrogram-like feature map shaped like (sampling points, scaling levels) as output with the following defined layer-wise propagation rule:\n\nwhere H input is the input vector shaped like (time steps, 1), i.e. the one-dimensional signal. H output is the matrix of activations shaped like (time steps, scaling levels), i.e. the data-driven spectrogram-like feature map. bias is the biases for multi-kernel generated by scaling a basic kernel. δ(•) denotes an activation function; weight is the basic kernel where others kernel scaled from. l is a hyper-parameter that controls the scaling level.\n\n⊗ is a valid cross-correlation operator, normally defined as:\n\nwhere f is downSample(weight, l), g is H input .\n\ndownSample is a pooling operator that downsamples the weight by average filter with a window of size 2. To ensure that the length of downsampled weight still is odd, the downSample setup a padding of size 1 for the filter when the length of directly downsampled weight potentially is even.\n\nFurther, bias(l) is the bias for the kernel generated at l th scaling level. H output (l) is the activation of l th scaling level. downSample(weight, l) denotes the generated kernel scaled from weight at l th level, which recursively filters the weight l-times.\n\nSteply, assume we would extract features for signal H input at l th scaling level. We first generate the l th scaling level kernel scaled from weight by downSample(weight, l). Then, we perform the cross-correlation operator of the scaled kernel and H input by Equation  2 . Then, we add the previous result and the bias(l), and then feed to activation function δ(•), i.e. Equation  1 .\n\nWe repeat the above process expected total scaling level times with different setup of hyper-parameter l on a range of 0 to maximum scaling level. Finally, we stack all extracted feature vectors into a 2D tensor to obtain the data-driven spectrogram-like feature map. In particular, in order to ensure the alignment of extracted feature vectors, the length of basic kernel weight must be odd and the input signal H input must be padded with (scaledKernelLength -1)/2.\n\nFor the backpropagation, the trainable parameter are the basic kernel weight and biases bias, which will be handled by autograd mechanism. The core principle of scaling layer is illustrated by Figure  1 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Scalingnet",
      "text": "In this subsection, we introduce a neural network architecture mainly constructed by a series of parallel scaling layers to perform raw EEG data based emotion recognition, which we reference as ScalingNet. The ScalingNet architecture is illustrated in Figure  2 . Considering that the scaling layers that mainly used to construct the ScalingNet extract data-driven spectrogram-like feature maps for EEG channels separately, we especially illustrate the EEG channels by carefully stacking the data-driven spectrogram-like feature maps extracted by scaling layer from EEG signal of different channels into a 3D tensor.\n\nThe EEG signals of different channels are first fed to scaling layers separately to extract data-driven spectrogram-like feature maps. Then, the feature maps extracted by scaling layers are stacked into a 3D tensor along the EEG channel dimension. Then the 3D tensor fed into several convolutional layers to perform feature map transform. Finally, the transformed features maps are fed into an average global pooling layer and a linear layer to perform emotion classification. Worthily, the ScalingNet architecture robustly performs raw EEG data based emotion recognition without requiring any hand-extracted features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental & Results",
      "text": "We evaluate the performance of the proposed ScalingNet architecture on EEG data based emotion recognition task using the established challenging DEAP dataset  [20]  and compare it with strong benchmarks or previous state-of-the-art methods. In this section, we first introduce the DEAP dataset, then proceed to a detailed description of the experimental setup, and finally report the experimental results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "The DEAP  [21]  is an established challenging benchmark dataset for EEG based emotion recognition. The dataset contains EEG and physiological signals collected from 32 subjects stimulated by watching music videos. After they watch each video, they self-evaluate their valence, arousal, dominance, and liking according to 1-9 immediately. Each subject is asked to watch 40 videos, and 63 seconds of signals are collected for each video. The signals are default downsampled to 128Hz and filtered with a 4.0Hz to 45.0Hz bandpass filter. In this paper, only EEG signals are used to classify the valence, arousal, and dominance by the rating threshold of 5, which closely follows the setting of  [22] . Specifically, 1280 EEG samples from 32 subjects are used for three binary classification tasks of cross-subject emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "The five-fold cross-validation strategy is employed to objectively evaluate the raw EEG data based emotion recognition performance of the proposed ScalingNet architecture. We manually optimize the hyper-parameters of proposed ScalingNet architecture on the DEAP dataset, and the most related tuned hyper-parameters are reported in Table  1 .\n\nwhere the \"length of weight\" is the size of basic kernel weight of scaling layer in Equation  1 . The \"kernel size\" is the size of convolutional kernels used in feature map transform convolutional layers of proposed ScalingNet architecture illustrated in Figure  2 . The \"number of filter\" is the number of filters used in feature map transform convolutional layers of proposed ScalingNet architecture illustrated in Figure  2 . All experiments in this paper were conducted using a Geforce RTX 2080 Ti. The machine learning framework used in this paper is PyTorch  [23] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "The experimental results of the proposed ScalingNet architecture compared with previous state-of-the-art methods using the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion recognition accuracies of arousal, valance, dominance in closely following previous studies. In Table  2 , Chao et al.  [24]  extract MFM features and use CapsNet as a classifier for emotion recognition. Chen et al.  [25]  use H-AAT-BGRU to classify emotions. Li et al.  [26]  and Yang et al.  [27]  use SVM for classification by extracting DBN features and VAE features respectively. Gupta, R  [28]  use graph-theoretic features and RVM for classification.\n\nThe results 2 show that the accuracy of the proposed method in this paper is 69.99%, 71.13%, and 70.78% for arousal, valence, and dominance, respectively, which are both higher than the previous state-of-the-art studies. It indicates that the proposed ScalingNet architecture is effective and feasible for EEG data based emotion recognition. Noticeably, its performance achieves the state-of-the-are result, but without any hand-interaction.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we have elaborately designed a series of experiments to explore the properties of the scaling layer and ScalingNet, visualize the data-driven spectrogram-like feature maps extracted by scaling layers to explore its interpretability, and verify it's contribution through ablation experiments.\n\nSince the scaling layer handles any length of EEG signals without requiring data alignment, we can arbitrarily adjust the length of the basic kernel weight to explore the relationship between model capacity and its representational capacity. We explore the relationship through observing the emotion recognition performance of ScalingNet with different setups of scaling layers. In the experiments, we deliberately select several representative parameters of the basic kernel weight in scaling layers. The results is shown in Table  3 .\n\nWe can observe that the representational capacity attains the best at the model capacity of setting the length of weight to 33. Obviously, the 33 is related to the DEAP dataset, and here are more interested in the Table  3  itself. In addition, we visualize the data-driven spectrogram-like feature maps extracted by scaling layers under the architecture of ScalingNet and the dataset of DEAP. The visualized data-driven spectrogram-like feature maps are shown in Figure  3 , where the horizontal axis denotes sampling points and the vertical axis denotes the frequency-like dimension, i.e. the time and scaling levels. We can observe that Figure  3  (a) contains more low frequency-like energy and (b) contains more high frequency-like energy, it all starts with that one data-driven pattern that used to generate scaled kernels to extract useful information. These learned useful information contained in the data-driven spectrogram-like feature maps are aggregated by followed layers and used for downstream tasks.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We have presented the scaling layer and ScalingNet, a novel convolutional layer for extracting a spectrogram-like feature map from raw signals and a neural network that operates on raw EEG data for classification, leveraging dynamically generated convolutional kernels by scaling from one data-driven pattern. We demonstrate that it can automatically adaptive extracting robust data-driven spectrogram-like feature maps and successfully applied to raw EEG data based emotion recognition. Thus it addresses many shortcomings of prior methods based on hand-extracted features with strong hypotheses or their approximations. Our ScalingNet models leveraging scaling layers have successfully achieved state-of-the-art performance across the well-established emotion recognition benchmarks.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The core principle of scaling layer. Scaling layer directly extracts data-driven spectrogram-like feature maps",
      "page": 3
    },
    {
      "caption": "Figure 2: The ScalingNet architecture. It’s mainly constructed by a series of parallel scaling layers that are followed by",
      "page": 4
    },
    {
      "caption": "Figure 2: Considering that the scaling layers that mainly used to construct",
      "page": 4
    },
    {
      "caption": "Figure 2: The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers",
      "page": 5
    },
    {
      "caption": "Figure 2: Table 1: The hyper-parameters of proposed ScalingNet architecture tuned on the DEAP dataset.",
      "page": 5
    },
    {
      "caption": "Figure 3: , where the horizontal axis denotes sampling points and the vertical axis denotes the frequency-like dimension, i.e. the",
      "page": 6
    },
    {
      "caption": "Figure 3: (a) contains more low frequency-like energy and (b) contains",
      "page": 6
    },
    {
      "caption": "Figure 3: Data-driven spectrogram-like feature maps extracted by scaling layers under the architecture of ScalingNet",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in a"
        },
        {
          "ABSTRACT": "variety of tasks. Recently, CNNs based methods that are fed with hand-extracted EEG features"
        },
        {
          "ABSTRACT": "gradually produce a powerful performance on the EEG data based emotion recognition task. In this"
        },
        {
          "ABSTRACT": "paper, we propose a novel convolutional layer allowing to adaptively extract effective data-driven"
        },
        {
          "ABSTRACT": "spectrogram-like features from raw EEG signals, which we reference as scaling layer. Further,"
        },
        {
          "ABSTRACT": "leverages convolutional kernels scaled from one data-driven pattern to exposed a frequency-like"
        },
        {
          "ABSTRACT": "dimension to address the shortcomings of prior methods requiring hand-extracted features or their"
        },
        {
          "ABSTRACT": "approximations. The proposed neural network architecture based on the scaling layer, references as"
        },
        {
          "ABSTRACT": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "1\nIntroduction"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "Emotion recognition plays a very important role in human-computer interaction[1]. Through recognizing human"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "emotions more accurately and quickly, we can promote a smarter life[2]. Generally, expressive modalities are used to"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "judge human being’s emotions, such as facial expressions, audio-visual expressions, and body language, etc.[3].\nIn"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "recent years, more and more studies that recognize human emotions have used physiological electrical signals[4][5],"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "such as electrocardiogram (ECG), electromyography (EMG) and electroencephalography (EEG). Among them, EEG"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "signals can better reﬂect real human emotions because it is not affected by subjective factor[6]. In this work, we use"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "EEG signals to recognize human emotions."
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "It has been proved that there are intimate correlations between human emotions and their different brain states[7][8]."
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "With the progress in EEG hardware equipment, it is more convenient to collect EEG signals with a high sampling rate"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "nowadays[9]. Meanwhile,\nthe processing and analysis methods of EEG signals are being explored and researched"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "constantly[10].\nIn EEG based emotion recognization, researchers mainly focus on three technical sects. The ﬁrst"
        },
        {
          "Keywords Deep Learning · Convolutional Neural Networks · EEG · emotion recognition · ScalingNet": "and most widespread methods are based on feature engineering and machine learning algorithms to recognize human"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "is to dynamically generate a series"
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - MAY 31, 2021": "We consider a multi-kernel convolutional layer that takes a one-dimensional signal shaped like (sampling points, 1)"
        },
        {
          "A PREPRINT - MAY 31, 2021": "as input and a two-dimensional spectrogram-like feature map shaped like (sampling points, scaling levels) as output"
        },
        {
          "A PREPRINT - MAY 31, 2021": "with the following deﬁned layer-wise propagation rule:"
        },
        {
          "A PREPRINT - MAY 31, 2021": "H output(l) = δ(bias(l) + downSample(weight, l) ⊗ H input)\n(1)"
        },
        {
          "A PREPRINT - MAY 31, 2021": "where H input\nis the input vector shaped like (time steps, 1), i.e.\nthe one-dimensional signal. H output\nis the matrix"
        },
        {
          "A PREPRINT - MAY 31, 2021": "of activations shaped like (time steps, scaling levels), i.e.\nthe data-driven spectrogram-like feature map. bias is the"
        },
        {
          "A PREPRINT - MAY 31, 2021": "biases for multi-kernel generated by scaling a basic kernel. δ(·) denotes an activation function; weight is the basic"
        },
        {
          "A PREPRINT - MAY 31, 2021": "kernel where others kernel scaled from.\nl is a hyper-parameter that controls the scaling level."
        },
        {
          "A PREPRINT - MAY 31, 2021": "⊗ is a valid cross-correlation operator, normally deﬁned as:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - MAY 31, 2021": "3.2\nScalingNet"
        },
        {
          "A PREPRINT - MAY 31, 2021": "In this subsection, we introduce a neural network architecture mainly constructed by a series of parallel scaling layers"
        },
        {
          "A PREPRINT - MAY 31, 2021": "to perform raw EEG data based emotion recognition, which we reference as ScalingNet."
        },
        {
          "A PREPRINT - MAY 31, 2021": "Figure 2: The ScalingNet architecture. It’s mainly constructed by a series of parallel scaling layers that are followed by"
        },
        {
          "A PREPRINT - MAY 31, 2021": "neat convolutional and linear layers. With the help of data-driven spectrogram-like feature maps extract by scaling"
        },
        {
          "A PREPRINT - MAY 31, 2021": "layers, it performs raw EEG data based emotion recognition without any hand-extracted features."
        },
        {
          "A PREPRINT - MAY 31, 2021": "The ScalingNet architecture is illustrated in Figure 2. Considering that the scaling layers that mainly used to construct"
        },
        {
          "A PREPRINT - MAY 31, 2021": "the ScalingNet extract data-driven spectrogram-like feature maps for EEG channels separately, we especially illustrate"
        },
        {
          "A PREPRINT - MAY 31, 2021": "the EEG channels by carefully stacking the data-driven spectrogram-like feature maps extracted by scaling layer from"
        },
        {
          "A PREPRINT - MAY 31, 2021": "EEG signal of different channels into a 3D tensor."
        },
        {
          "A PREPRINT - MAY 31, 2021": "The EEG signals of different channels are ﬁrst fed to scaling layers separately to extract data-driven spectrogram-like"
        },
        {
          "A PREPRINT - MAY 31, 2021": "feature maps. Then, the feature maps extracted by scaling layers are stacked into a 3D tensor along the EEG channel"
        },
        {
          "A PREPRINT - MAY 31, 2021": "dimension. Then the 3D tensor\nfed into several convolutional\nlayers to perform feature map transform.\nFinally,"
        },
        {
          "A PREPRINT - MAY 31, 2021": "the transformed features maps are fed into an average global pooling layer and a linear layer to perform emotion"
        },
        {
          "A PREPRINT - MAY 31, 2021": "classiﬁcation. Worthily, the ScalingNet architecture robustly performs raw EEG data based emotion recognition without"
        },
        {
          "A PREPRINT - MAY 31, 2021": "requiring any hand-extracted features."
        },
        {
          "A PREPRINT - MAY 31, 2021": "4\nExperimental & Results"
        },
        {
          "A PREPRINT - MAY 31, 2021": "We evaluate the performance of the proposed ScalingNet architecture on EEG data based emotion recognition task"
        },
        {
          "A PREPRINT - MAY 31, 2021": "using the established challenging DEAP dataset[20] and compare it with strong benchmarks or previous state-of-the-art"
        },
        {
          "A PREPRINT - MAY 31, 2021": "methods. In this section, we ﬁrst introduce the DEAP dataset, then proceed to a detailed description of the experimental"
        },
        {
          "A PREPRINT - MAY 31, 2021": "setup, and ﬁnally report the experimental results."
        },
        {
          "A PREPRINT - MAY 31, 2021": "4.1\nDatasets"
        },
        {
          "A PREPRINT - MAY 31, 2021": "The DEAP[21] is an established challenging benchmark dataset for EEG based emotion recognition. The dataset"
        },
        {
          "A PREPRINT - MAY 31, 2021": "contains EEG and physiological signals collected from 32 subjects stimulated by watching music videos. After they"
        },
        {
          "A PREPRINT - MAY 31, 2021": "watch each video, they self-evaluate their valence, arousal, dominance, and liking according to 1-9 immediately. Each"
        },
        {
          "A PREPRINT - MAY 31, 2021": "subject is asked to watch 40 videos, and 63 seconds of signals are collected for each video. The signals are default"
        },
        {
          "A PREPRINT - MAY 31, 2021": "downsampled to 128Hz and ﬁltered with a 4.0Hz to 45.0Hz bandpass ﬁlter. In this paper, only EEG signals are used"
        },
        {
          "A PREPRINT - MAY 31, 2021": "to classify the valence, arousal, and dominance by the rating threshold of 5, which closely follows the setting of [22]."
        },
        {
          "A PREPRINT - MAY 31, 2021": "Speciﬁcally, 1280 EEG samples from 32 subjects are used for three binary classiﬁcation tasks of cross-subject emotion"
        },
        {
          "A PREPRINT - MAY 31, 2021": "recognition."
        },
        {
          "A PREPRINT - MAY 31, 2021": "4.2\nExperimental setup"
        },
        {
          "A PREPRINT - MAY 31, 2021": "The ﬁve-fold cross-validation strategy is employed to objectively evaluate the raw EEG data based emotion recognition"
        },
        {
          "A PREPRINT - MAY 31, 2021": "performance of\nthe proposed ScalingNet architecture. We manually optimize the hyper-parameters of proposed"
        },
        {
          "A PREPRINT - MAY 31, 2021": "ScalingNet architecture on the DEAP dataset, and the most related tuned hyper-parameters are reported in Table 1."
        },
        {
          "A PREPRINT - MAY 31, 2021": "4"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "of proposed ScalingNet architecture illustrated in Figure 2."
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": ""
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "Hyper-parameters"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "batch size"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "length of weight"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "kernel size"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "number of ﬁlter"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "activation function"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "loss"
        },
        {
          "illustrated in Figure 2. The \"number of ﬁlter\" is the number of ﬁlters used in feature map transform convolutional layers": "optimizer"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "recognition accuracies of arousal, valance, dominance in closely following previous studies."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": ""
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": ""
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Studies"
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": ""
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Li et al."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "CHEN et al."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Yang et al."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Gupta, R."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Chao, H. et al."
        },
        {
          "the DEAP dataset and the same evaluation strategy are shown in Table2. Where evaluation criteria are the emotion": "Ours"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the",
      "data": [
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "architecture of ScalingNet and the dataset of DEAP."
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": ""
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "Length of weight"
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": ""
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "129"
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "65"
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "63"
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "33"
        },
        {
          "Table 3: The relationship of scaling layers between its model capacity and its representational capacity under the": "17"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nConclusion": "We have presented the scaling layer and ScalingNet, a novel convolutional layer for extracting a spectrogram-like feature"
        },
        {
          "6\nConclusion": ""
        },
        {
          "6\nConclusion": "generated convolutional kernels by scaling from one data-driven pattern. We demonstrate that"
        },
        {
          "6\nConclusion": "adaptive extracting robust data-driven spectrogram-like feature maps and successfully applied to raw EEG data based"
        },
        {
          "6\nConclusion": "emotion recognition. Thus it addresses many shortcomings of prior methods based on hand-extracted features with"
        },
        {
          "6\nConclusion": "strong hypotheses or their approximations. Our ScalingNet models leveraging scaling layers have successfully achieved"
        },
        {
          "6\nConclusion": "state-of-the-art performance across the well-established emotion recognition benchmarks."
        },
        {
          "6\nConclusion": "Acknowledgment"
        },
        {
          "6\nConclusion": "This work was\nsupported\nby\nthe National Key Research\nand Development Program of China"
        },
        {
          "6\nConclusion": "2017YFB1002504."
        },
        {
          "6\nConclusion": "References"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2017YFB1002504.": "References"
        },
        {
          "2017YFB1002504.": "[1] Youjun Li, Jiajin Huang, Haiyan Zhou, and Ning Zhong. Human emotion recognition with electroencephalographic"
        },
        {
          "2017YFB1002504.": "multidimensional features by hybrid deep neural networks. Applied Sciences, 7(10):1060, 2017."
        },
        {
          "2017YFB1002504.": "[2] Wei Liu, Wei-Long Zheng, and Bao-Liang Lu. Multimodal emotion recognition using multimodal deep learning."
        },
        {
          "2017YFB1002504.": "arXiv preprint arXiv:1602.08225, 2016."
        },
        {
          "2017YFB1002504.": "[3] Ralph Adolphs, Daniel Tranel, Hanna Damasio, and Antonio Damasio.\nImpaired recognition of emotion in facial"
        },
        {
          "2017YFB1002504.": "expressions following bilateral damage to the human amygdala. Nature, 372(6507):669–672, 1994."
        },
        {
          "2017YFB1002504.": "[4] Sali Issa, Qinmu Peng, and Xinge You. Emotion classiﬁcation using eeg brain signals and the broad learning"
        },
        {
          "2017YFB1002504.": "system.\nIEEE Transactions on Systems, Man, and Cybernetics: Systems, 2020."
        },
        {
          "2017YFB1002504.": "[5] Zhongke Gao, Yanli Li, Yuxuan Yang, Xinmin Wang, Na Dong, and Hsiao-Dong Chiang. A gpso-optimized"
        },
        {
          "2017YFB1002504.": "convolutional neural networks for eeg-based emotion recognition. Neurocomputing, 380:225–235, 2020."
        },
        {
          "2017YFB1002504.": "[6] Lubna Shibly Mokatren, Rashid Ansari, Ahmet Enis Çetin, Alex D Leow, Heide Klumpp, Olusola Ajilore, and"
        },
        {
          "2017YFB1002504.": "Fatos T Yarman-Vural.\nImproved eeg classiﬁcation by factoring in sensor topography. CoRR, 2019."
        },
        {
          "2017YFB1002504.": "[7] Zhongke Gao, Xinmin Wang, Yuxuan Yang, Yanli Li, Kai Ma, and Guanrong Chen. A channel-fused dense"
        },
        {
          "2017YFB1002504.": "IEEE Transactions on Cognitive and Developmental\nconvolutional network for eeg-based emotion recognition."
        },
        {
          "2017YFB1002504.": "Systems, 2020."
        },
        {
          "2017YFB1002504.": "[8] Heekyung Yang, Jongdae Han, and Kyungha Min. A multi-column cnn model for emotion recognition from eeg"
        },
        {
          "2017YFB1002504.": "signals. Sensors, 19(21):4736, 2019."
        },
        {
          "2017YFB1002504.": "[9] Diego Fabiano and Shaun Canavan.\nEmotion recognition using fused physiological signals.\nIn 2019 8th"
        },
        {
          "2017YFB1002504.": "International Conference on Affective Computing and Intelligent Interaction (ACII), pages 42–48. IEEE, 2019."
        },
        {
          "2017YFB1002504.": "[10] Peiyang Li, Huan Liu, Yajing Si, Cunbo Li, Fali Li, Xuyang Zhu, Xiaoye Huang, Ying Zeng, Dezhong Yao,"
        },
        {
          "2017YFB1002504.": "Yangsong Zhang, et al. Eeg based emotion recognition by combining functional connectivity network and local"
        },
        {
          "2017YFB1002504.": "activations.\nIEEE Transactions on Biomedical Engineering, 66(10):2869–2881, 2019."
        },
        {
          "2017YFB1002504.": "[11] Weilong Zheng, Wei Liu, Yifei Lu, Baoliang Lu, and Andrzej Cichocki. Emotionmeter: A multimodal framework"
        },
        {
          "2017YFB1002504.": "for recognizing human emotions.\nIEEE Transactions on Systems, Man, and Cybernetics, 49(3):1110–1122, 2019."
        },
        {
          "2017YFB1002504.": "[12] Xiaofen Xing, Zhenqi Li, Tianyuan Xu, Lin Shu, Bin Hu, and Xiangmin Xu. Sae+lstm: A new framework for"
        },
        {
          "2017YFB1002504.": "emotion recognition from multi-channel eeg. Frontiers in Neurorobotics, 13:37, 2019."
        },
        {
          "2017YFB1002504.": "[13]\nJingxia Chen, Dongmei Jiang, and Yizhai Zhang. A hierarchical bidirectional gru model with attention for"
        },
        {
          "2017YFB1002504.": "eeg-based emotion classiﬁcation.\nIEEE Access, 7:118530–118540, 2019."
        },
        {
          "2017YFB1002504.": "[14] Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Jocelyn Faubert."
        },
        {
          "2017YFB1002504.": "Deep learning-based electroencephalography analysis:\na systematic review.\nJournal of Neural Engineering,"
        },
        {
          "2017YFB1002504.": "16(5):051001, 2019."
        },
        {
          "2017YFB1002504.": "[15] Weilong Zheng, Jiayi Zhu, and Baoliang Lu.\nIdentifying stable patterns over time for emotion recognition from"
        },
        {
          "2017YFB1002504.": "eeg. arXiv: Human-Computer Interaction, 2016."
        },
        {
          "2017YFB1002504.": "[16]\nJingxin Liu, Hongying Meng, Asoke K Nandi, and Maozhen Li. Emotion detection from eeg recordings. pages"
        },
        {
          "2017YFB1002504.": "1722–1727, 2016."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "multi-channel eeg data through convolutional recurrent neural network. pages 352–359, 2016."
        },
        {
          "A PREPRINT - MAY 31, 2021": "[18] Byung Hyung Kim and Sungho Jo. Deep physiological affect network for the recognition of human emotions."
        },
        {
          "A PREPRINT - MAY 31, 2021": "IEEE Transactions on Affective Computing, pages 1–1, 2018."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "eeg-based emotion recognition. pages 1–7, 2018."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis using physiological signals."
        },
        {
          "A PREPRINT - MAY 31, 2021": "IEEE Transactions on Affective Computing, 3(1):18–31, 2012."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological"
        },
        {
          "A PREPRINT - MAY 31, 2021": "signals.\nIEEE transactions on affective computing, 3(1):18–31, 2011."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "convolutional recurrent neural network.\nIn 2018 International Joint Conference on Neural Networks (IJCNN),"
        },
        {
          "A PREPRINT - MAY 31, 2021": "pages 1–7, 2018."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "capsnet. Sensors, 19(9):2212, 2019."
        },
        {
          "A PREPRINT - MAY 31, 2021": "J. X. Chen, D. M. Jiang, and Y. N. Zhang. A hierarchical bidirectional gru model with attention for eeg-based"
        },
        {
          "A PREPRINT - MAY 31, 2021": "emotion classiﬁcation.\nIEEE Access, 7:118530–118540, 2019."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "using unsupervised deep feature learning. 2015."
        },
        {
          "A PREPRINT - MAY 31, 2021": "learning for emotion recognition using physiology.\nIn"
        },
        {
          "A PREPRINT - MAY 31, 2021": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages"
        },
        {
          "A PREPRINT - MAY 31, 2021": "1184–1188, 2019."
        },
        {
          "A PREPRINT - MAY 31, 2021": ""
        },
        {
          "A PREPRINT - MAY 31, 2021": "graph-theoretic features for automatic affective state characterization. Neurocomputing, 174(JAN.22PT.B):875–"
        },
        {
          "A PREPRINT - MAY 31, 2021": "884, 2016."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human emotion recognition with electroencephalographic multidimensional features by hybrid deep neural networks",
      "authors": [
        "Youjun Li",
        "Jiajin Huang",
        "Haiyan Zhou",
        "Ning Zhong"
      ],
      "year": "2017",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition using multimodal deep learning",
      "authors": [
        "Wei Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2016",
      "venue": "Multimodal emotion recognition using multimodal deep learning",
      "arxiv": "arXiv:1602.08225"
    },
    {
      "citation_id": "3",
      "title": "Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala",
      "authors": [
        "Ralph Adolphs",
        "Daniel Tranel",
        "Hanna Damasio",
        "Antonio Damasio"
      ],
      "year": "1994",
      "venue": "Nature"
    },
    {
      "citation_id": "4",
      "title": "Emotion classification using eeg brain signals and the broad learning system",
      "authors": [
        "Sali Issa",
        "Qinmu Peng",
        "Xinge You"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "5",
      "title": "A gpso-optimized convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "Zhongke Gao",
        "Yanli Li",
        "Yuxuan Yang",
        "Xinmin Wang",
        "Na Dong",
        "Hsiao-Dong Chiang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Improved eeg classification by factoring in sensor topography",
      "authors": [
        "Lubna Shibly Mokatren",
        "Rashid Ansari",
        "Ahmet Enis Çetin",
        "Alex Leow",
        "Heide Klumpp",
        "Olusola Ajilore",
        "Fatos T Yarman- Vural"
      ],
      "year": "2019",
      "venue": "Improved eeg classification by factoring in sensor topography"
    },
    {
      "citation_id": "7",
      "title": "A channel-fused dense convolutional network for eeg-based emotion recognition",
      "authors": [
        "Zhongke Gao",
        "Xinmin Wang",
        "Yuxuan Yang",
        "Yanli Li",
        "Kai Ma",
        "Guanrong Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "8",
      "title": "A multi-column cnn model for emotion recognition from eeg signals",
      "authors": [
        "Heekyung Yang",
        "Jongdae Han",
        "Kyungha Min"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using fused physiological signals",
      "authors": [
        "Diego Fabiano",
        "Shaun Canavan"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "10",
      "title": "Eeg based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "Peiyang Li",
        "Huan Liu",
        "Yajing Si",
        "Cunbo Li",
        "Fali Li",
        "Xuyang Zhu",
        "Xiaoye Huang",
        "Ying Zeng",
        "Dezhong Yao",
        "Yangsong Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "11",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Weilong Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Baoliang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "12",
      "title": "Sae+lstm: A new framework for emotion recognition from multi-channel eeg",
      "authors": [
        "Xiaofen Xing",
        "Zhenqi Li",
        "Tianyuan Xu",
        "Lin Shu",
        "Bin Hu",
        "Xiangmin Xu"
      ],
      "year": "2019",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "13",
      "title": "A hierarchical bidirectional gru model with attention for eeg-based emotion classification",
      "authors": [
        "Jingxia Chen",
        "Dongmei Jiang",
        "Yizhai Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Yannick Roy",
        "Hubert Banville",
        "Isabela Albuquerque",
        "Alexandre Gramfort",
        "H Tiago",
        "Jocelyn Falk",
        "Faubert"
      ],
      "year": "2019",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "15",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "Weilong Zheng",
        "Jiayi Zhu",
        "Baoliang Lu"
      ],
      "year": "2016",
      "venue": "Identifying stable patterns over time for emotion recognition from eeg"
    },
    {
      "citation_id": "16",
      "title": "Emotion detection from eeg recordings",
      "authors": [
        "Jingxin Liu",
        "Hongying Meng",
        "Asoke Nandi",
        "Maozhen Li"
      ],
      "year": "2016",
      "venue": "Emotion detection from eeg recordings"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from multi-channel eeg data through convolutional recurrent neural network",
      "authors": [
        "Xiang Li",
        "Dawei Song",
        "Peng Zhang",
        "Guangliang Yu",
        "Yuexian Hou",
        "Bin Hu"
      ],
      "year": "2016",
      "venue": "Emotion recognition from multi-channel eeg data through convolutional recurrent neural network"
    },
    {
      "citation_id": "18",
      "title": "Deep physiological affect network for the recognition of human emotions",
      "authors": [
        "Byung Hyung",
        "Sungho Jo"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition",
      "authors": [
        "Yi Wang",
        "Zhiyi Huang",
        "Brendan Mccane",
        "Phoebe Neo"
      ],
      "year": "2018",
      "venue": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition"
    },
    {
      "citation_id": "20",
      "title": "Deap: A database for emotion analysis using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jongseok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from multi-channel eeg through parallel convolutional recurrent neural network",
      "authors": [
        "Y Yang",
        "Q Wu",
        "M Qiu",
        "Y Wang",
        "X Chen"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Pytorch"
      ],
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from multiband eeg signals using capsnet",
      "authors": [
        "Hao Chao",
        "Liang Dong",
        "Yongli Liu",
        "Baoyun Lu"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "A hierarchical bidirectional gru model with attention for eeg-based emotion classification",
      "authors": [
        "J Chen",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Eeg based emotion identification using unsupervised deep feature learning",
      "authors": [
        "Peng Zhang",
        "Xiang Li",
        "Yuexian Hou",
        "Guangliang Yu",
        "Dawei Song",
        "Bin Hu"
      ],
      "year": "2015",
      "venue": "Eeg based emotion identification using unsupervised deep feature learning"
    },
    {
      "citation_id": "27",
      "title": "An attribute-invariant variational learning for emotion recognition using physiology",
      "authors": [
        "H Yang",
        "C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "28",
      "title": "Relevance vector classifier decision fusion and eeg graph-theoretic features for automatic affective state characterization",
      "authors": [
        "Rishabh Gupta",
        "Khalil Ur Rehman",
        "Tiago H Laghari",
        "Falk"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    }
  ]
}