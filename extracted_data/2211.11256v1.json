{
  "paper_id": "2211.11256v1",
  "title": "Unimse: Towards Unified Multimodal Sentiment Analysis And Emotion Recognition",
  "published": "2022-11-21T08:46:01Z",
  "authors": [
    "Guimin Hu",
    "Ting-En Lin",
    "Yi Zhao",
    "Guangming Lu",
    "Yuchuan Wu",
    "Yongbin Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMO-CAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of multimodal machine learning  (Liang et al., 2022; Baltrušaitis et al., 2018)  and dialog system  (He et al., 2022a,b,c) , Multimodal Sentiment Analysis (MSA) and Emotion Recognition in Conversations (ERC) have become the keys for machines to perceive, recognize, and understand human behaviors and intents  (Zhang et al., 2021a,b; Hu et al., 2021a,b) . Multimodal data provides not only verbal information, such as textual (spoken words) features but also non-verbal information, including acoustic (prosody, rhythm, pitch) and visual (facial attributes) features. These different modalities allow the machine to make decisions from different perspectives, thereby achieving more accurate predictions  (Ngiam et al., 2011) . The goal of MSA is to predict sentiment intensity or polarity, and ERC aims to predict predefined emotion categories. There are many research directions in MSA and ERC, such as multimodal fusion  (Yang et al., 2021) , modal alignment  (Tsai et al., 2019a) , context modeling  (Mao et al., 2021)  and external knowledge  (Ghosal et al., 2020) . However, most existing works treat MSA and ERC as separate tasks, ignoring the similarities and complementarities between sentiments and emotions.\n\nOn the one hand, from a psychological perspective, both sentiments and emotions are experiences that result from the combined influences of the biological, cognitive, and social  (Stets, 2006) , and could be expressed similarly. In Figure  1 , we illustrate how sentiments and emotions are relevant in the verbal or non-verbal, and could be projected into a unified embedding space. On the other hand, emotions are reflections of the perceived change in the present within a short period  (Batson et al., 1992) , while sentiments are held and formed in longer periods  (Murray and Morgan, 1945) . In our preliminary study, we found that the video dura-tion of MSA is almost twice of ERC 1  , which is consistent with the above definitions. A variety of psychological literature  (Davidson et al., 2009; Ben-Ze'ev, 2001; Shelly, 2004)  explain the similarities and differences between sentiment and emotion.  Munezero et al. (2014)  also investigates the relevance and complementarity between the two and point out that analyzing sentiment and emotion together could better understand human behaviors.\n\nBased on the above motivation, we propose a multimodal sentiment knowledge-sharing framework that Unified MSA and ERC (UniMSE) tasks. UniMSE reformulates MSA and ERC as a generative task to unify input, output, and task. We extract and unify audio and video features and formalize MSA and ERC labels into Universal Labels (UL) to unify sentiment and emotion.\n\nBesides, previous works on multimodal fusion at multi-level textual features  (Peters et al., 2018; Vaswani et al., 2017) , like syntax and semantics, are lacking. Therefore, we propose a pre-trained modality fusion layer (PMF) and embed it in Transformer  (Vaswani et al., 2017)  layers of T5  (Raffel et al., 2020) , which fuses the acoustic and visual information with different level textual features for probing richer information. Last but not least, we perform inter-modal contrastive learning (CL) to minimize intra-class variance and maximize interclass variance across modalities.\n\nOur contributions are summarized as follows:\n\n1. We propose a multimodal sentimentknowledge sharing framework 2  (UniMSE) that unifies MSA and ERC tasks. The proposed method exploits the similarities and complementaries between sentiments and emotions for better prediction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Sentiment Analysis (MSA) MSA aims to predict sentiment polarity and sentiment intensity under a multimodal setting  (Morency et al., 2011) . MSA research could be divided into four groups. The first is multimodal fusion. Early works of multimodal fusion mainly operate geometric manipulation in the feature spaces  (Zadeh et al., 2017) . The recent works develop the reconstruction loss  (Hazarika et al., 2020) , or hierarchical mutual information maximization  (Han et al., 2021)  to optimize multimodal representation. The second group focuses on modal consistency and difference through multi-task joint learning  (Yu et al., 2021a)  or translating from one modality to another  (Mai et al., 2020) . The third is multimodal alignment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversations (Erc)",
      "text": "With growing research interest in dialogue systems  (Dai et al., 2021 (Dai et al., , 2020a,b;,b; Lin and Xu, 2019a,b; Lin et al., 2020; Zhang et al., 2022b) , how to recognize the predefined emotion in the conversation has become a research hotspot. Meanwhile, with the rise of multimodal machine learning  (Mao et al., 2022; Yuan et al., 2021; Yu et al., 2021b; Zhang et al., 2022a; Lin et al., 2022)   external knowledge, such as transfer learning  (Hazarika et al., 2019; Lee and Lee, 2021) , commonsense knowledge  (Ghosal et al., 2020) , multi-task learning  (Akhtar et al., 2019) , and external information  (Zhu et al., 2021)  to solve ERC task.\n\nUnified Framework In recent years, the unification of related but different tasks into a framework has achieved significant progress  (Chen et al., 2022; Xie et al., 2022; Zhang et al., 2022c) . For example, T5  (Raffel et al., 2020)  unifies various NLP tasks by casting all text-based language problems as a text-to-text format and achieves state-of-the-art results on many benchmarks. More recently, the works  (Wang et al., 2021a; Cheng et al., 2021b; Wang et al., 2021a)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Task Formalization",
      "text": "Given a multimodal signal\n\ni , m ∈ {t, a, v} to represent unimodal raw sequence drawn from the video fragment i, where {t, a, v} denote the three types of modalities-text, acoustic and visual. MSA aims to predict the real number y r i ∈ R that reflects the sentiment strength, and ERC aims to predict the emotion category of each utterance. MSA and ERC are unified in input feature, model architecture, and label space through task formalization. Task formalization contains input formalization and label formalization, where input formalization is used to process the dialogue text and modal feature, and label formalization is used to unify MSA and ERC tasks by transferring their labels into universal labels. Furthermore, we formalize the MSA and ERC as a generative task to unify them in a single architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Formalization",
      "text": "The contextual information in conversation is especially important to understand human emotions and intents  (Lee and Lee, 2021; Hu et al., 2022) . Based on this observation, we concatenate current utterance u i with its former 2-turn utterances {u i-1 , u i-2 }, and its latter 2-turn utterances {u i+1 , u i+2 } as raw text. Additionally, we set segment id S t i to distinguish utterance u i and its contexts in textual modality:\n\nwhere the utterances are processed into the format of I t i , and we take I t i as the textual modality of I i . Furthermore, we process raw acoustic input into numerical sequential vectors by librosa 3 to extract Mel-spectrogram as audio features. It is the short-term power spectrum of sound and is widely used in modern audio processing. For video, we extract fixed T frames from each segment and use effecientNet  (Tan and Le, 2019)  pre-trained (supervised) on VGGface 4 and AFEW dataset to obtain video features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label Formalization",
      "text": "To break the information boundary between MSA and ERC, we design a universal label (UL) scheme and take UL as the target sequence of UniMSE. The universal label aims to fully explore the shared knowledge between MSA and ERC on sentiment and emotion. Given a universal label y i = {y p i , y r i , y c i }, it is composed by sentiment polarity y p i ∈ {positive, negative and neutral} contained in MSA and ERC, sentiment intensity y r i 3 https://github.com/librosa/librosa. 4 https://www.robots.ox.ac.uk/ vgg/software/vgg_face/. (the supervision signal of MSA task, a real number ranged from -3 to +3) and an emotion category y c i (the supervision signal of ERC task, a predefined emotion category). We align the sample with similar semantics (like 1.6 and joy), in which one is annotated with sentiment intensity, and the other is annotated with emotion category. After the alignment of label space, each sample's label is formalized into a universal label format. Next, we introduce in detail how to unify MSA and ERC tasks in the label space, as follows:\n\nFirst, we classify the samples of MSA and ERC into positive, neutral, and negative sample sets according to their sentiment polarity. Then we calculate the similarity of two samples with the same sentiment polarity but belonging to different annotation scheme, thereby completing the missing part in the universal label. We show an example in Figure  3 . Given an MSA sample m 2 , it carries a positive sentiment and an annotation score of 1.6. Benchmarking the format of universal label, m 2 lacks an emotion category label. In this example, e 1 has the maximal semantic similarity to m 2 , and then we assign the emotion category of e 1 as m 2 's emotion category.\n\nPrevious works  (Tsai et al., 2019a; Yang et al., 2021)  have demonstrated that textual modality is more indicative than the other modalities, so we adopt textual similarity as the semantic similarity among samples. Specifically, we utilize a strong sentence embedding framework SimCSE  (Gao et al., 2021)  to calculate the semantic similarity of two texts for completion of the universal label. Similarly, the sample in the ERC dataset is assigned a real number by calculating the MSA sample that is most similar to it. After our formalization, the samples of MSA and ERC are processed into {(I 0 , y 0 ), (I 1 , y 1 )...(I N , y N )}, where I i denotes raw multimodal signal of sample i and y i denotes its universal label. We can obtain the predictions of MSA and ERC by decoding from the predicted UL. Additionally, we evaluate the performance of the generated automatically part in the universal labels. We randomly selected 80 samples with universal labels from MOSI and manually evaluated the generated labels used for universal label completion; the accuracy is about 90%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pre-Trained Modality Fusion (Pmf)",
      "text": "Unlike the previous works just using a pre-trained model (such as T5) as a text encoder, we embed the multimodal fusion layers into the pre-trained model.   (Peters et al., 2018; Vaswani et al., 2017)  are fused with audio and video features into multimodal representation. Besides, the injection of audio and vision to T5 can probe the relevant information in the massive pre-trained text knowledge, thereby incorporating richer pretrained understanding into the multimodal fusion representation. We name this multimodal fusion process as pre-trained modality fusion (PMF).\n\nWe use T5 as the backbone of UniMSE. T5 contains multiple stacked Transformer layers, and each Transformer layer for the encoder and decoder contains a feedforward layer. The multimodal fusion layer is set to follow after the feedforward layer. Essentially, the PMF unit in the first Transformer layer of T5 receives a triplet M i = (X t i , X a i , X v i ) as the input, where X m i , X m i ∈ R lm×dm denotes the modality representation of I m i , m ∈ {t, a, v}, l m and d m are the sequence length and the representation dimension of modality m, respectively. We view the multimodal fusion layer as an adapter  (Houlsby et al., 2019)  and insert it into the T5 model to optimize specific parameters for multimodal fusion. The multimodal fusion layer receives modal representation triplet M i , and maps the multimodal concatenation representation's size back to the layer's input size. Specifically, we concatenate the three modal representations and then feed the concatenation into the down-projection and up-projection layers to fuse representations. For j-th PMF, the multimodal fusion is given by: denotes the fusion representation after (j -1) Transformer layers. denotes the element addition. The output of the fusion layer is then passed directly into the following layer normalization  (Ba et al., 2016) .\n\nAlthough we can embed a multimodal fusion layer in each Transformer of T5's encoder and decoder, it may bring two shortcomings: 1) disturb the encoding of text sequences, and 2) cause overfitting as more parameters are set for the multimodal fusion layer. Considering these issues, we use the former j Transformer layers to encode the text, and the remaining Transformer layers are injected with the non-verbal (i.e., acoustic and visual) signals.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inter-Modality Contrastive Learning",
      "text": "Contrastive learning (CL) has gained major advances in representation learning by viewing sample from multiple views  (Gutmann and Hyvärinen, 2010; Khosla et al., 2020; Gao et al., 2021) . The principle of contrastive learning is that an anchor and its positive sample should be pulled closer, while the anchor and negative samples should be pushed apart in feature space. In our work, we perform inter-modality contrastive learning to enhance the interaction between modalities and magnify the differentiation of fusion representation among samples. To ensure that each element of the input sequence is aware of its context, we process each modal representation to the same sequence length. We pass acoustic representation X a i , visual representations X v i and fusion representation F (j) i through a 1D temporal convolutional layer:\n\nwhere F (j) i is obtained after j Transformer layers containing pre-trained modality fusion. k u is the size of the convolutional kernel for modalities u, u ∈ {a, v}, k f is the size of the convolutional kernel for fusion modality.\n\nWe construct each mini-batch with K samples (each sample consists of acoustic, visual, and text modalities). Previous works  (Han et al., 2021; Tsai et al., 2019a)  have proved that textual modality is more important than the other two modalities, so we take the textual modality as the anchor and the other two modalities as its augmented version.\n\nA batch of randomly sampled pairs for each anchor consists of two positive pairs and 2K negative pairs. Here, the positive sample is the modality pair composed of text and corresponding acoustic in the same sample, and the modality pair composed of text and corresponding visual in the same sample. The negative example is the modality pair composed of text and the other two modalities of the other samples. For each anchor sample, the self-supervised contrastive loss is formulated as follows:\n\nwhere L ta,j and L tv,j represent the contrastive loss of text-acoustic and text-visual performing on the j-th Transformer layer of encoder, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Grounding Ul To Msa And Erc",
      "text": "During the training phase, we use the negative loglikelihood to optimize the model, which takes the universal label as the target sequence. The overall loss function can be formulated as follows:\n\nwhere L task denotes the generative task loss, j is the index of the Transformer layer of the Encoder, and {α, β} are decimals between 0 and 1, indicating the weight values. Moreover, during the inference, we use the decoding algorithm 5  to convert the output sequence into the real number for MSA and the emotion category for ERC.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiments on four publicly available benchmark datasets of MSA and ERC, including Multimodal Opinion-level Sentiment Intensity dataset (MOSI)  (Zadeh et al., 2016) , Multimodal Opinion Sentiment and Emotion Intensity (MOSEI)  (Zadeh et al., 2018) , Multimodal EmotionLines Dataset (MELD)  (Poria et al., 2019)  and Interactive Emotional dyadic Motion CAPture database (IEMOCAP)  (Busso et al., 2008) . The detailed statistic of four datasets are shown in Table  1 . More details can see Appendix A.1.\n\nMOSI contains 2199 utterance video segments, and each segment is manually annotated with a sentiment score ranging from -3 to +3 to indicate the sentiment polarity and relative sentiment strength of the segment. MOSEI is an upgraded version of MOSI, annotated with sentiment and emotion. MOSEI contains 22,856 movie review clips from YouTube. Most existing studies only use MOSEI's sentiment annotation, and MOSEI's emotion annotation is multiple labels, so we do not use its emotion annotation although they are available. Note that there is no overlap between MOSI and MOSEI, and the data collection and labeling processes for the two datasets are independent.\n\nIEMOCAP consists of 7532 samples. Following previous works  (Wang et al., 2019; Hu et al., 2022) , we select six emotions for emotion recognition, including joy, sadness, angry, neutral, excited, and frustrated. MELD contains 13,707 video clips of multi-party conversations, with labels following Ekman's six universal emotions, including joy, sadness, fear, anger, surprise and disgust.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "For MOSI and MOSEI, we follow previous works  (Han et al., 2021)  and adopt mean absolute error (MAE), Pearson correlation (Corr), seven-class classification accuracy (ACC-7), binary classification accuracy (ACC-2) and F1 score computed for positive/negative and non-negative/negative classification as evaluation metrics. For MELD and IEMOCAP, we use accuracy (ACC) and weighted F1 (WF1) for evaluation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "We compare the proposed method with competitive baselines in MSA and ERC tasks. For MSA, the baselines can be grouped into 1) early multimodal fusion methods like Tensor Fusion Network TFN  (Zadeh et al., 2017) , Low-rank Multimodal Fusion LMF  (Liu et al., 2018) , and Multimodal Factorization Model MFM  (Tsai et al., 2019b) , and 2) the methods that fuse multimodality through modeling modality interaction, such as multimodal Transformer MulT  (Tsai et al., 2019a) , interaction canonical correlation network ICCN  (Sun et al., 2020) , sparse phased Transformer SPC  (Cheng et al., 2021a) , and modal-temporal attention graph MTAG  (Yang et al., 2021)  and 3) the methods focusing on the consistency and the difference of modality, in which MISA  (Hazarika et al., 2020)  controls the modal representation space, Self-MM  (Yu et al., 2021a)  learns from unimodal representation using multi-task learning, MAG-BERT (Rahman et al., 2020) designs a fusion gate, and MMIM  (Han et al., 2021)  hierarchically maximizes the mutual information.\n\nWith the rise of multimodal information, MMGCN  (Hu et al., 2021c) , MM-DFN  (Hu et al., 2022)  and COGMEN  (Joshi et al., 2022)  consider the multimodal conversational context to solve ERC task. Some works only use textual modality to recognize emotion, in which ERMC-DisGCN  (Sun et al., 2021) , Psychological  (Li et al., 2021a) , DAG-ERC  (Shen et al., 2021)  and DialogueGCN  (Ghosal et al., 2019)  adapt the GNN-based model to capture contexts. Additionally, CoG-BART  (Li et al., 2021b)  learns the context knowledge from the pre-trained model, COSMIC  (Ghosal et al., 2020)  incorporates different elements of commonsense, and TODKAT  (Zhu et al., 2021)  uses topicdriven knowledge-aware Transformer to model affective states. Similar to MSA and ERC works, UniMSE still attends to improve multimodal fusion representation and modality comparison in feature space. But, UniMSE unifies MSA and ERC tasks into a single architecture to implement knowledgesharing.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Settings",
      "text": "We use pre-trained T5-Base 6  as the backbone of UniMSE. We integrate the training sets of MOSI, MOSEI, MELD, IEMOCAP to train the model and valid sets to select hyperparameters. The batch size is 96, the learning rate for T5 fine-tuning is set at 3e-4, and the learning rate for main and pretrained modality fusion are 0.0001 and 0.0001, respectively. The hidden dim of acoustic and visual representation is 64, the T5 embedding size is 768, and the fusion vector size is 768. We insert a pretrained modality fusion layer into the last 3 Transformer layers of T5's encoder. The contrastive learning performs the last 3 Transformer layers of T5's encoder, and we set α = 0.5 and β = 0.5. More details can see Appendix A.3.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "We compare UniMSE with the baselines on datasets MOSI, MOSEI, IEMOCAP, and MELD, and the comparative results are shown in Table  2 . UniMSE significantly outperforms SOTA in all metrics on MOSI, MOSEI, IEMOCAP, and MELD. Compared to the previous SOTA, UniMSE improves ACC-2 of MOSI, ACC-2 of MOSEI, ACC of MELD, and ACC of IEMOCAP by 1.65%, 1.16%, 2.6%, and 2.35% respectively, and improves F1 of MOSI, F1 of MOSEI, and WF1 of IEMOCAP by 1.73%, 1.29%, and 2.48% respectively. It can be observed that early works like LMF, TFN, and MFM performed on the four datasets. However, the later works, whether MSA or ERC, only evaluate their models on partial datasets or metrics, yet we provide results on all datasets and corresponding metrics. For example, MTAG only conducts experiments on MOSI, and most ERC works only give the WF1, which makes MSA and ERC tasks tend to be isolated in sentiment knowledge. Unlike these works, UniMSE unifies MSA and ERC tasks on these four datasets and evaluates them based on the common metrics of the two tasks. In summary, 1) UniMSE performs on all benchmark datasets of MSA and ECR; 2) UniMSE significantly outperforms SOTA in most cases. These results illustrate the superiority of UniMSE in MSA and ERC tasks and demonstrate the effectiveness of a unified framework in knowledge sharing among tasks and datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted a series of ablation studies on MOSI, and the results are shown in Table  3 . First, we eliminate one or several modalities from multimodal signals to verify the modal effects on model performance. We can find that removing visual and acoustic modalities or one of them all leads to performance degradation, which indicates that the non-verbal signals (i.e., visual and acoustic) are necessary for solving MSA, and demonstrates the complementarity among text, acoustic, and visual. We also find that the acoustic modality is more important than the visual to UniMSE. Then we eliminate module PMF and CL from UniMSE, which leads to an increase in MAE and a decrease in Corr. These results illustrate the effectiveness of PMF and CL in multimodal representation learning. Additionally, we conduct experiments to verify the impact of the dataset on UniMSE. We remove IEMOCAP, MELD, and MOSEI from the training set and evaluate model performance on the MOSI test set. Removing IEMOCAP and MELD hurts the performance, especially in metrics MAE and Corr. This result may be because the removal of MELD/IEMOCAP has reduced the information they provide for MSA task. We also remove MO- SEI, resulting in poor performance in the four metrics. The proposed UniMSE is orthogonal to the existing works, and it is believed that introducing our unified framework to other tasks can also bring improvements.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Visualization",
      "text": "To verify the effects of UniMSE's UL and crosstask learning on multimodal representation, we visualize multimodal fusion representation (i.e., F\n\ni ) of the last Transformer layer. Specifically, we select samples that carry positive/negative sentiment polarity from the test set of MOSI and select samples that have the joy/sadness emotion from the test set of MELD. Their representation visualization is shown in Figure  4 (a). It can be observed that the representations of samples with positive sentiment cover the representation of samples with joy emotion, which demonstrates that although these samples are from different tasks, a common feature space exists between the samples with joy emotion and positive sentiment.\n\nMoreover, we also select the MOSI samples with generated emotion joy/sadness and compare them to MELD samples with the original emotion label joy/sadness in embedding space. Their visualization is shown in Figure  4 (b). The samples with joy emotion, whether annotated with the original label or generated based on UL, share a common feature space. These results verify the superiority of UniMSE on representation learning across samples and demonstrate the complementarity between sentiment and emotion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper provides a psychological perspective to demonstrate that jointly modeling sentiment and emotion is feasible and reasonable. We present a unified multimodal knowledge-sharing framework, UniMSE, to solve MSA and ERC tasks. UniMSE not only captures knowledge of sentiment and emotion, but also aligns the input features and output labels. Moreover, we fuse acoustic and visual modal representation with multi-level textual features and introduce inter-modality contrastive learning. We conduct extensive experiments on four benchmark datasets and achieve SOTA results in all metrics. We also provide the visualization of multimodal representation, proving the relevance of sentiment and emotion in embedding space. We believe this work presents a new experimental setting that can provide a new and different perspective to the MSA and ERC research communities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "In this preliminary study, we only integrate context information on MELD and IEMOCAP, and the context on MOSI and MOSEI will be considered in the future. Furthermore, the generation of universal labels only considers textual modality, without considering acoustic and visual modalities, which will also be solved in our future work.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Appendix",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.1 Datasets",
      "text": "We count the duration of the video segment in MSA and ERC and give the results in Table  4 . We take the length of the video segment as the duration of sentiment or emotion. We can observe that the average time of sentiment in MSA is longer than that of emotion in ERC, demonstrating the difference between sentiment and emotion. The average length of the video segment in MOSEI is 7.6 seconds. This may indicate why MOSEI is usually used to study sentiments rather than emotions. Furthermore, we count emotion categories of MELD and IEMOCAP, and their distributions of the train set, valid set, and test set are shown in Table  5",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Decoding Algorithm For Msa And Erc Tasks",
      "text": "In this part, we introduce the decoding algorithm we used to convert the predicted target sequence of UniMSE into a sentiment intensity for MSA and an emotion category for ERC. The algorithm is shown in Algorithm 1.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.3 Experimental Environment",
      "text": "All experiments are conducted in the NVIDIA RTX A100 and NVIDIA RTX V100",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of sentiment and emotion sharing",
      "page": 1
    },
    {
      "caption": "Figure 1: , we illus-",
      "page": 1
    },
    {
      "caption": "Figure 2: The overview of UniMSE.",
      "page": 3
    },
    {
      "caption": "Figure 2: , UniMSE comprises the",
      "page": 3
    },
    {
      "caption": "Figure 3: The generating process of a universal label",
      "page": 4
    },
    {
      "caption": "Figure 3: Given an MSA sample m2, it carries a",
      "page": 4
    },
    {
      "caption": "Figure 4: (a). It can be observed that",
      "page": 8
    },
    {
      "caption": "Figure 4: T-SNE visualization comparison of the multi-",
      "page": 9
    },
    {
      "caption": "Figure 4: (b). The samples with",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "1\nIntroduction"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "With the rapid development of multimodal ma-"
        },
        {
          "ments compared with state-of-the-art methods.": "chine learning (Liang et al., 2022; Baltrušaitis et al.,"
        },
        {
          "ments compared with state-of-the-art methods.": "2018) and dialog system (He et al., 2022a,b,c), Mul-"
        },
        {
          "ments compared with state-of-the-art methods.": "timodal Sentiment Analysis (MSA) and Emotion"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "Recognition in Conversations (ERC) have become"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "the keys for machines to perceive, recognize, and"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "understand human behaviors and intents (Zhang"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "et al., 2021a,b; Hu et al., 2021a,b). Multimodal"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "data provides not only verbal information, such as"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "textual (spoken words) features but also non-verbal"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "information, including acoustic (prosody, rhythm,"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "pitch) and visual (facial attributes) features. These"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "different modalities allow the machine to make de-"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "cisions from different perspectives, thereby achiev-"
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "ing more accurate predictions (Ngiam et al., 2011)."
        },
        {
          "ments compared with state-of-the-art methods.": ""
        },
        {
          "ments compared with state-of-the-art methods.": "∗Corresponding authors."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "Figure 1: Illustration of sentiment and emotion sharing"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "a uniﬁed embedding space. The bottom is a uniﬁed la-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "bel after formalizing sentiment and emotion according"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "(cid:95)"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "to the similarity\nsim between samples with the same"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "sentiment polarity label."
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "The goal of MSA is to predict sentiment intensity"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "or polarity, and ERC aims to predict predeﬁned"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "emotion categories. There are many research direc-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "tions in MSA and ERC, such as multimodal fusion"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "(Yang et al., 2021), modal alignment (Tsai et al.,"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "2019a), context modeling (Mao et al., 2021) and"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "external knowledge (Ghosal et al., 2020). How-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "ever, most existing works treat MSA and ERC as"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "separate tasks,\nignoring the similarities and com-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "plementarities between sentiments and emotions."
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "On the one hand, from a psychological perspec-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "tive, both sentiments and emotions are experiences"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "that result from the combined inﬂuences of the bi-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "ological, cognitive, and social (Stets, 2006), and"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "could be expressed similarly. In Figure 1, we illus-"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "trate how sentiments and emotions are relevant in"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "the verbal or non-verbal, and could be projected"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "into a uniﬁed embedding space. On the other hand,"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "emotions are reﬂections of the perceived change"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "in the present within a short period (Batson et al.,"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "1992), while sentiments are held and formed in"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": ""
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "longer periods (Murray and Morgan, 1945). In our"
        },
        {
          "{zhao.yi,luguangm}@hit.edu.cn": "preliminary study, we found that\nthe video dura-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "class variance across modalities."
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "Our contributions are summarized as follows:"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "1. We\npropose\na"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "knowledge\nsharing framework2"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "that\nuniﬁes MSA and ERC tasks."
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "complementaries\nbetween"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "emotions for better prediction."
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "2. We\nfuse multimodal"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "multi-level\ntextual"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "Meanwhile, we utilize"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "timodal representations."
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "3. Experimental\nresults"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "MSA and ERC tasks."
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": "4. To the best of our knowledge, we are the ﬁrst"
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        },
        {
          "minimize intra-class variance and maximize inter-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "consistent with the above deﬁnitions. A variety",
          "ion, and the ﬁrst to use uniﬁed audio and video": "features across MSA and ERC tasks."
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "of psychological literature (Davidson et al., 2009;",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "2\nRelated Work"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Ben-Ze’ev, 2001; Shelly, 2004) explain the similar-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "ities and differences between sentiment and emo-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Multimodal Sentiment Analysis (MSA)\nMSA"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "tion. Munezero et al. (2014) also investigates the",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "aims to predict sentiment polarity and sentiment in-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "relevance and complementarity between the two",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "tensity under a multimodal setting (Morency et al.,"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "and point out that analyzing sentiment and emotion",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "2011). MSA research could be divided into four"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "together could better understand human behaviors.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "groups. The ﬁrst is multimodal fusion. Early works"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Based on the above motivation, we propose a",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "of multimodal\nfusion mainly operate geometric"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "multimodal sentiment knowledge-sharing frame-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "manipulation in the feature spaces (Zadeh et al.,"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "work that Uniﬁed MSA and ERC (UniMSE) tasks.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "2017). The recent works develop the reconstruc-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "UniMSE reformulates MSA and ERC as a genera-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "tion loss (Hazarika et al., 2020), or hierarchical mu-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "tive task to unify input, output, and task. We extract",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "tual\ninformation maximization (Han et al., 2021)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "and unify audio and video features and formalize",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "to optimize multimodal representation. The second"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "MSA and ERC labels into Universal Labels (UL)",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "group focuses on modal consistency and difference"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "to unify sentiment and emotion.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "through multi-task joint learning (Yu et al., 2021a)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Besides, previous works on multimodal fusion",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "or translating from one modality to another (Mai"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "at multi-level textual features (Peters et al., 2018;",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "et al., 2020). The third is multimodal alignment."
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Vaswani et al., 2017),\nlike syntax and semantics,",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Tsai et al. (2019a) and Luo et al. (2021) leverage"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "are lacking. Therefore, we propose a pre-trained",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "cross-modality and multi-scale modality represen-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "modality fusion layer (PMF) and embed it in Trans-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "tation to implement modal alignment, respectively."
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "former (Vaswani et al., 2017) layers of T5 (Raffel",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Lastly, studies of multimodal context integrate the"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "et al., 2020), which fuses the acoustic and visual",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "unimodal context, in which Chauhan et al. (2019)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "information with different level textual features for",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "adapts context-aware attention, Ghosal et al. (2018)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "probing richer information. Last but not least, we",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "uses multi-modal attention, and Poria et al. (2017)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "perform inter-modal contrastive learning (CL) to",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "proposes a recurrent model with multi-level mul-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "minimize intra-class variance and maximize inter-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "tiple attentions to capture contextual information"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "class variance across modalities.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "among utterances."
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Our contributions are summarized as follows:",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Emotion Recognition in Conversations\n(ERC)"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "1. We\npropose\na\nmultimodal\nsentiment-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "With growing research interest in dialogue systems"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "knowledge\nsharing framework2\n(UniMSE)",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "(Dai et al., 2021, 2020a,b; Lin and Xu, 2019a,b; Lin"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "that\nuniﬁes MSA and ERC tasks.\nThe",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "et al., 2020; Zhang et al., 2022b), how to recognize"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "proposed method exploits the similarities and",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "the predeﬁned emotion in the conversation has be-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "complementaries\nbetween\nsentiments\nand",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "come a research hotspot. Meanwhile, with the rise"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "emotions for better prediction.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "of multimodal machine learning (Mao et al., 2022;"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "2. We\nfuse multimodal\nrepresentation\nfrom",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Yuan et al., 2021; Yu et al., 2021b; Zhang et al.,"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "multi-level\ntextual\ninformation by injecting",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "2022a; Lin et al., 2022), the studies of ERC have"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "acoustic and visual signals into the T5 model.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "been extended to multimodal paradigm. The multi-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "Meanwhile, we utilize\ninter-modality con-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "modal emotion recognition in conversation gained"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "trastive learning to obtain discriminative mul-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "great progress.\nThe research direction could be"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "timodal representations.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "categorized into multimodal fusion, context-aware"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "3. Experimental\nresults\ndemonstrate\nthat",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "models, and incorporating external knowledge. Hu"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "UniMSE achieves a new state-of-the-art per-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "et al. (2022, 2021c); Joshi et al. (2022) adopt graph"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "formance on four public benchmark datasets,",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "neural networks to model the inter/intra dependen-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "MOSI, MOSEI, MELD and IEMOCAP, for",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "cies of utterances or speakers. For context incorpo-"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "MSA and ERC tasks.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "ration, Sun et al. (2021); Li et al. (2021a); Ghosal"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "4. To the best of our knowledge, we are the ﬁrst",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "et al. (2019) model\nthe contexts by constructing"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "to solve MSA and ERC in a generative fash-",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "graph structure, and Mao et al. (2021) introduces"
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "",
          "ion, and the ﬁrst to use uniﬁed audio and video": "the concept of emotion dynamics to capture context."
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "1Please see Appendix A.1 for details.",
          "ion, and the ﬁrst to use uniﬁed audio and video": ""
        },
        {
          "tion of MSA is almost\ntwice of ERC1, which is": "2https://github.com/LeMei/UniMSE.",
          "ion, and the ﬁrst to use uniﬁed audio and video": "Moreover, some advancing ERC works incorporate"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The overview of UniMSE.": "external knowledge, such as transfer learning (Haz-"
        },
        {
          "Figure 2: The overview of UniMSE.": "arika et al., 2019; Lee and Lee, 2021), common-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "sense knowledge (Ghosal et al., 2020), multi-task"
        },
        {
          "Figure 2: The overview of UniMSE.": "learning (Akhtar et al., 2019), and external\ninfor-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "mation (Zhu et al., 2021) to solve ERC task."
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "Uniﬁed Framework\nIn recent years, the uniﬁca-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "tion of related but different tasks into a framework"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "has achieved signiﬁcant progress (Chen et al., 2022;"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "Xie et al., 2022; Zhang et al., 2022c). For exam-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "ple, T5 (Raffel et al., 2020) uniﬁes various NLP"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "tasks by casting all text-based language problems"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "as a text-to-text format and achieves state-of-the-art"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "results on many benchmarks. More recently,\nthe"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "works (Wang et al., 2021a; Cheng et al., 2021b;"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "Wang et al., 2021a) using uniﬁed frameworks have"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "attracted lots of attention, such as Yan et al. (2021a)"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "solves all ABSA tasks in a uniﬁed index generative"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "way, Chen et al. (2022) investigates a uniﬁed gen-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "erative dialogue understanding framework, Zhang"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "et al.\n(2021c) proposes a uniﬁed framework for"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "multimodal summarization, Wang et al.\n(2021b)"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "uniﬁes entity detection and relation classiﬁcation"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "on their label space to eliminate the different treat-"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "ment, and Yan et al. (2021b) integrates the ﬂat NER,"
        },
        {
          "Figure 2: The overview of UniMSE.": "nested NER, and discontinuous NER subtasks in a"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "Seq2Seq framework. These works demonstrate the"
        },
        {
          "Figure 2: The overview of UniMSE.": "superiority of such a uniﬁed framework in improv-"
        },
        {
          "Figure 2: The overview of UniMSE.": "ing model performance and generalization. In our"
        },
        {
          "Figure 2: The overview of UniMSE.": ""
        },
        {
          "Figure 2: The overview of UniMSE.": "work, we use T5 as the backbone to unify the MSA"
        },
        {
          "Figure 2: The overview of UniMSE.": "and ERC and learn a uniﬁed embedding space in"
        },
        {
          "Figure 2: The overview of UniMSE.": "this framework."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sample that\nis most similar\nto it. After our\nfor-": "malization, the samples of MSA and ERC are pro-",
          "and up-projection layers to fuse representations.": "For j-th PMF, the multimodal fusion is given by:"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "cessed into {(I0, y0), (I1, y1)...(IN , yN )}, where",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Ii denotes raw multimodal signal of sample i and",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "· X a,la\n· X v,lv\n]\nFi = [F (j−1)"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i\ni\ni"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "label. We can obtain the\nyi denotes its universal",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i = σ(W dFi + bd)"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "predictions of MSA and ERC by decoding from",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "(2)"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "F u"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the predicted UL. Additionally, we evaluate the",
          "and up-projection layers to fuse representations.": "i = W uF d\ni + bu"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "performance of the generated automatically part in",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "= W (F u\n)\ni (cid:12) F (j−1)\ni"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the universal labels. We randomly selected 80 sam-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "ples with universal labels from MOSI and manually",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "∈ R1×da\n∈ R1×dv\nwhere X a,la\nand X v,lv\nare"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "evaluated the generated labels used for universal",
          "and up-projection layers to fuse representations.": "i\ni"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "time step of X a\nand X v"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "label completion; the accuracy is about 90%.",
          "and up-projection layers to fuse representations.": "i\ni , re-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "spectively. X a\nand X v\nare acoustic and visual"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i\ni"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "modality representations encoded by two individ-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "3.3\nPre-trained Modality Fusion (PMF)",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "ual LSTMs, respectively.\n[·] is concatenation oper-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Unlike the previous works just using a pre-trained",
          "and up-projection layers to fuse representations.": "ation on feature dim, σ is the Sigmoid function,"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "model (such as T5) as a text encoder, we embed the",
          "and up-projection layers to fuse representations.": "and {W d, W u, W, bd, bu} are learnable parame-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "multimodal fusion layers into the pre-trained model.",
          "and up-projection layers to fuse representations.": "F (0)"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "ters.\nand X t\nis the text\nrepresenta-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i\ni\ni"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Thus the acoustic and visual signals can participate",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "tion encoded by the ﬁrst Transformer layer of T5,"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "in text encoding and are fused with multiple levels",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "and F (j−1)\ndenotes the fusion representation after"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "of textual information. The low-level text syntax",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "(j − 1) Transformer layers. (cid:12) denotes the element"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "feature encoded by the shallow Transformer layers",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "addition. The output of\nthe fusion layer\nis then"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "and high-level\ntext semantic feature encoded by",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "passed directly into the following layer normaliza-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the deep Transformer layers (Peters et al., 2018;",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "tion (Ba et al., 2016)."
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Vaswani et al., 2017) are fused with audio and video",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "Although we can embed a multimodal\nfusion"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "features into multimodal representation. Besides,",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "layer in each Transformer of T5’s encoder and de-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the injection of audio and vision to T5 can probe",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "coder,\nit may bring two shortcomings: 1) disturb"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the relevant information in the massive pre-trained",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "the encoding of text sequences, and 2) cause overﬁt-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "text knowledge, thereby incorporating richer pre-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "ting as more parameters are set for the multimodal"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "trained understanding into the multimodal fusion",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "fusion layer. Considering these issues, we use the"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "representation. We name this multimodal fusion",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "former j Transformer layers to encode the text, and"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "process as pre-trained modality fusion (PMF).",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "the remaining Transformer layers are injected with"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "We use T5 as the backbone of UniMSE. T5 con-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "the non-verbal (i.e., acoustic and visual) signals."
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "tains multiple stacked Transformer layers, and each",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Transformer layer for the encoder and decoder con-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "3.4\nInter-modality Contrastive Learning"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "tains a feedforward layer. The multimodal fusion",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "Contrastive learning (CL) has gained major ad-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "layer is set\nto follow after the feedforward layer.",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "vances in representation learning by viewing sam-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "Essentially, the PMF unit in the ﬁrst Transformer",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "ple from multiple views (Gutmann and Hyvärinen,"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "layer of T5 receives a triplet Mi = (X t\ni )\ni , X v\ni , X a",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "2010; Khosla et al., 2020; Gao et al., 2021). The"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "as the input, where X m\n∈ Rlm×dm denotes",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "i\ni",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "principle of contrastive learning is that an anchor"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the modality representation of I m\n, m ∈ {t, a, v},",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "i",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "and its positive sample should be pulled closer,"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "lm and dm are the sequence length and the repre-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "while the anchor and negative samples should be"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "sentation dimension of modality m, respectively.",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "pushed apart in feature space. In our work, we per-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "We view the multimodal fusion layer as an adapter",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "form inter-modality contrastive learning to enhance"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "(Houlsby et al., 2019) and insert\nit\ninto the T5",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "the interaction between modalities and magnify"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "model\nto optimize speciﬁc parameters for multi-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "the differentiation of fusion representation among"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "modal\nfusion.\nThe multimodal\nfusion layer\nre-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "samples. To ensure that each element of\nthe in-"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "ceives modal representation triplet Mi, and maps",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "put sequence is aware of its context, we process"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "the multimodal concatenation representation’s size",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "each modal representation to the same sequence"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "back to the layer’s input size. Speciﬁcally, we con-",
          "and up-projection layers to fuse representations.": ""
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "length. We pass acoustic representation X a\n, visual"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "catenate the three modal representations and then",
          "and up-projection layers to fuse representations.": "i"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "feed the concatenation into the down-projection",
          "and up-projection layers to fuse representations.": "representations X v\nand fusion representation F (j)"
        },
        {
          "sample that\nis most similar\nto it. After our\nfor-": "",
          "and up-projection layers to fuse representations.": "i\ni"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "vert the output sequence into the real number for"
        },
        {
          "through a 1D temporal convolutional layer:": "i = Conv1D(X u",
          "inference, we use the decoding algorithm 5 to con-": "MSA and the emotion category for ERC."
        },
        {
          "through a 1D temporal convolutional layer:": "(3)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "F (j)\n, kf )\n= Conv1D(F (j)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "i\ni",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "4\nExperiments"
        },
        {
          "through a 1D temporal convolutional layer:": "where F (j)\nis obtained after j Transformer\nlay-",
          "inference, we use the decoding algorithm 5 to con-": "4.1\nDatasets"
        },
        {
          "through a 1D temporal convolutional layer:": "i",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "ers containing pre-trained modality fusion. ku is",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "We conduct experiments on four publicly avail-"
        },
        {
          "through a 1D temporal convolutional layer:": "the size of the convolutional kernel for modalities",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "able benchmark datasets of MSA and ERC, includ-"
        },
        {
          "through a 1D temporal convolutional layer:": "u, u ∈ {a, v}, kf\nis the size of the convolutional",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "ing Multimodal Opinion-level Sentiment Intensity"
        },
        {
          "through a 1D temporal convolutional layer:": "kernel for fusion modality.",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "dataset (MOSI) (Zadeh et al., 2016), Multimodal"
        },
        {
          "through a 1D temporal convolutional layer:": "We construct each mini-batch with K samples",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "Opinion Sentiment and Emotion Intensity (MOSEI)"
        },
        {
          "through a 1D temporal convolutional layer:": "(each sample consists of acoustic, visual, and text",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "(Zadeh et al., 2018), Multimodal EmotionLines"
        },
        {
          "through a 1D temporal convolutional layer:": "modalities). Previous works (Han et al., 2021; Tsai",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "Dataset (MELD) (Poria et al., 2019) and Interac-"
        },
        {
          "through a 1D temporal convolutional layer:": "et al., 2019a) have proved that textual modality is",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "tive Emotional dyadic Motion CAPture database"
        },
        {
          "through a 1D temporal convolutional layer:": "more important than the other two modalities, so",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "(IEMOCAP)\n(Busso et al., 2008).\nThe detailed"
        },
        {
          "through a 1D temporal convolutional layer:": "we take the textual modality as the anchor and the",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "statistic of four datasets are shown in Table 1. More"
        },
        {
          "through a 1D temporal convolutional layer:": "other two modalities as its augmented version.",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "details can see Appendix A.1."
        },
        {
          "through a 1D temporal convolutional layer:": "A batch of randomly sampled pairs for each an-",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "MOSI contains 2199 utterance video segments,"
        },
        {
          "through a 1D temporal convolutional layer:": "chor consists of two positive pairs and 2K negative",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "and each segment is manually annotated with a sen-"
        },
        {
          "through a 1D temporal convolutional layer:": "pairs. Here,\nthe positive sample is the modality",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "timent score ranging from -3 to +3 to indicate the"
        },
        {
          "through a 1D temporal convolutional layer:": "pair composed of text and corresponding acoustic",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "sentiment polarity and relative sentiment strength"
        },
        {
          "through a 1D temporal convolutional layer:": "in the same sample, and the modality pair com-",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "of the segment. MOSEI is an upgraded version"
        },
        {
          "through a 1D temporal convolutional layer:": "posed of text and corresponding visual in the same",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "of MOSI, annotated with sentiment and emotion."
        },
        {
          "through a 1D temporal convolutional layer:": "sample. The negative example is the modality pair",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "MOSEI contains 22,856 movie review clips from"
        },
        {
          "through a 1D temporal convolutional layer:": "composed of text and the other two modalities of",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "YouTube. Most existing studies only use MOSEI’s"
        },
        {
          "through a 1D temporal convolutional layer:": "the other samples.\nFor each anchor sample,\nthe",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "sentiment annotation, and MOSEI’s emotion anno-"
        },
        {
          "through a 1D temporal convolutional layer:": "self-supervised contrastive loss is formulated as",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "tation is multiple labels, so we do not use its emo-"
        },
        {
          "through a 1D temporal convolutional layer:": "follows:",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "tion annotation although they are available. Note"
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "that there is no overlap between MOSI and MOSEI,"
        },
        {
          "through a 1D temporal convolutional layer:": "X a\nexp( ˆF (j)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "i )",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "i\nLta,j = − log",
          "inference, we use the decoding algorithm 5 to con-": "and the data collection and labeling processes for"
        },
        {
          "through a 1D temporal convolutional layer:": "X a\nX a\nexp( ˆF (j)\nk=1 exp( ˆF (j)\ni\ni ) + (cid:80)K\nk )",
          "inference, we use the decoding algorithm 5 to con-": "the two datasets are independent."
        },
        {
          "through a 1D temporal convolutional layer:": "X v\nexp( ˆF (j)",
          "inference, we use the decoding algorithm 5 to con-": "IEMOCAP consists of 7532 samples. Follow-"
        },
        {
          "through a 1D temporal convolutional layer:": "i )",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "i\nLtv,j = − log",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "ing previous works (Wang et al., 2019; Hu et al.,"
        },
        {
          "through a 1D temporal convolutional layer:": "X v\nX v\nexp( ˆF (j)\nk=1 exp( ˆF (j)\ni\ni ) + (cid:80)K\nk )",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "2022), we select six emotions for emotion recogni-"
        },
        {
          "through a 1D temporal convolutional layer:": "(4)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "tion, including joy, sadness, angry, neutral, excited,"
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "and frustrated. MELD contains 13,707 video clips"
        },
        {
          "through a 1D temporal convolutional layer:": "where Lta,j and Ltv,j\nrepresent the contrastive loss",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "of multi-party conversations, with labels follow-"
        },
        {
          "through a 1D temporal convolutional layer:": "of text-acoustic and text-visual performing on the",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "ing Ekman’s six universal emotions, including joy,"
        },
        {
          "through a 1D temporal convolutional layer:": "j-th Transformer layer of encoder, respectively.",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "sadness, fear, anger, surprise and disgust."
        },
        {
          "through a 1D temporal convolutional layer:": "3.5\nGrounding UL to MSA and ERC",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "4.2\nEvaluation metrics"
        },
        {
          "through a 1D temporal convolutional layer:": "During the training phase, we use the negative log-",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "For MOSI and MOSEI, we follow previous works"
        },
        {
          "through a 1D temporal convolutional layer:": "likelihood to optimize the model, which takes the",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "(Han et al., 2021) and adopt mean absolute er-"
        },
        {
          "through a 1D temporal convolutional layer:": "universal label as the target sequence. The overall",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "ror (MAE), Pearson correlation (Corr), seven-class"
        },
        {
          "through a 1D temporal convolutional layer:": "loss function can be formulated as follows:",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "classiﬁcation accuracy (ACC-7), binary classiﬁca-"
        },
        {
          "through a 1D temporal convolutional layer:": "(cid:88)\n(cid:88)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "tion accuracy (ACC-2) and F1 score computed for"
        },
        {
          "through a 1D temporal convolutional layer:": "L = Ltask + α(\nLta,j) + β(\nLtv,j)\n(5)",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "j\nj",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "positive/negative and non-negative/negative clas-"
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "siﬁcation as evaluation metrics. For MELD and"
        },
        {
          "through a 1D temporal convolutional layer:": "where Ltask denotes the generative task loss, j is",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "IEMOCAP, we use accuracy (ACC) and weighted"
        },
        {
          "through a 1D temporal convolutional layer:": "the index of the Transformer layer of the Encoder,",
          "inference, we use the decoding algorithm 5 to con-": ""
        },
        {
          "through a 1D temporal convolutional layer:": "",
          "inference, we use the decoding algorithm 5 to con-": "F1 (WF1) for evaluation."
        },
        {
          "through a 1D temporal convolutional layer:": "and {α, β} are decimals between 0 and 1,\nindi-",
          "inference, we use the decoding algorithm 5 to con-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: The details of MOSI, MOSEI, MELD, and 4.4 ExperimentalSettings",
      "data": [
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(cid:33)\n(cid:37)\nMOSI\n1284\n229\n686\n2199",
          "UniMSE still attends to improve multimodal fusion": "representation and modality comparison in feature"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(cid:33)\n(cid:33)\nMOSEI\n16326\n1871\n4659\n22856",
          "UniMSE still attends to improve multimodal fusion": "space. But, UniMSE uniﬁes MSA and ERC tasks"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(cid:37)\n(cid:33)",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "MELD\n9989\n1108\n2610\n13707",
          "UniMSE still attends to improve multimodal fusion": "into a single architecture to implement knowledge-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(cid:37)\n(cid:33)\nIEMOCAP\n5354\n528\n1650\n7532",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "sharing."
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "Table 1: The details of MOSI, MOSEI, MELD, and",
          "UniMSE still attends to improve multimodal fusion": "4.4\nExperimental Settings"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "IEMOCAP,\nincluding data splitting and the labels\nit",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "We use pre-trained T5-Base 6 as the backbone of"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "contains, where Senti.\nand Emo.\nrepresent\nthe label",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "UniMSE. We integrate the training sets of MOSI,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "sentiment polarity and intensity of MSA and emotion",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "MOSEI, MELD, IEMOCAP to train the model and"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "category of ERC,\nrespectively. (cid:33)and (cid:37)denote\nthe",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "dataset has or does not have the label.",
          "UniMSE still attends to improve multimodal fusion": "valid sets to select hyperparameters.\nThe batch"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "size is 96,\nthe learning rate for T5 ﬁne-tuning is"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "set at 3e-4, and the learning rate for main and pre-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "4.3\nBaselines",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "trained modality fusion are 0.0001 and 0.0001, re-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "We compare the proposed method with competi-",
          "UniMSE still attends to improve multimodal fusion": "spectively. The hidden dim of acoustic and visual"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "tive baselines in MSA and ERC tasks. For MSA,",
          "UniMSE still attends to improve multimodal fusion": "representation is 64, the T5 embedding size is 768,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "the baselines can be grouped into 1) early multi-",
          "UniMSE still attends to improve multimodal fusion": "and the fusion vector size is 768. We insert a pre-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "modal fusion methods like Tensor Fusion Network",
          "UniMSE still attends to improve multimodal fusion": "trained modality fusion layer into the last 3 Trans-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "TFN (Zadeh et al., 2017), Low-rank Multimodal",
          "UniMSE still attends to improve multimodal fusion": "former\nlayers of T5’s encoder.\nThe contrastive"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "Fusion LMF (Liu et al., 2018), and Multimodal",
          "UniMSE still attends to improve multimodal fusion": "learning performs the last 3 Transformer layers of"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "Factorization Model MFM (Tsai et al., 2019b),",
          "UniMSE still attends to improve multimodal fusion": "T5’s encoder, and we set α = 0.5 and β = 0.5."
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "and 2) the methods that fuse multimodality through",
          "UniMSE still attends to improve multimodal fusion": "More details can see Appendix A.3."
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "modeling modality interaction, such as multimodal",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "4.5\nResults"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "Transformer MulT (Tsai et al., 2019a), interaction",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "canonical correlation network ICCN (Sun et al.,",
          "UniMSE still attends to improve multimodal fusion": "We\ncompare UniMSE with\nthe\nbaselines\non"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "2020),\nsparse phased Transformer SPC (Cheng",
          "UniMSE still attends to improve multimodal fusion": "datasets MOSI, MOSEI, IEMOCAP, and MELD,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "et al., 2021a), and modal-temporal attention graph",
          "UniMSE still attends to improve multimodal fusion": "and the comparative results are shown in Table"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "MTAG (Yang et al., 2021) and 3) the methods fo-",
          "UniMSE still attends to improve multimodal fusion": "2. UniMSE signiﬁcantly outperforms SOTA in"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "cusing on the consistency and the difference of",
          "UniMSE still attends to improve multimodal fusion": "all metrics on MOSI, MOSEI,\nIEMOCAP, and"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "modality,\nin which MISA (Hazarika et al., 2020)",
          "UniMSE still attends to improve multimodal fusion": "MELD. Compared to the previous SOTA, UniMSE"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "controls the modal representation space, Self-MM",
          "UniMSE still attends to improve multimodal fusion": "improves ACC-2 of MOSI, ACC-2 of MOSEI,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(Yu et al., 2021a) learns from unimodal representa-",
          "UniMSE still attends to improve multimodal fusion": "ACC of MELD, and ACC of IEMOCAP by 1.65%,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "tion using multi-task learning, MAG-BERT (Rah-",
          "UniMSE still attends to improve multimodal fusion": "1.16%,\n2.6%,\nand 2.35% respectively,\nand im-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "man et al., 2020) designs a fusion gate, and MMIM",
          "UniMSE still attends to improve multimodal fusion": "proves F1 of MOSI, F1 of MOSEI, and WF1 of"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(Han et al., 2021) hierarchically maximizes the mu-",
          "UniMSE still attends to improve multimodal fusion": "IEMOCAP by 1.73%, 1.29%, and 2.48% respec-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "tual information.",
          "UniMSE still attends to improve multimodal fusion": "tively. It can be observed that early works like LMF,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "With\nthe\nrise\nof multimodal\ninformation,",
          "UniMSE still attends to improve multimodal fusion": "TFN, and MFM performed on the four datasets."
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "MMGCN (Hu et al., 2021c), MM-DFN (Hu et al.,",
          "UniMSE still attends to improve multimodal fusion": "However, the later works, whether MSA or ERC,"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "2022) and COGMEN (Joshi et al., 2022) consider",
          "UniMSE still attends to improve multimodal fusion": "only evaluate their models on partial datasets or"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "the multimodal conversational context\nto solve",
          "UniMSE still attends to improve multimodal fusion": "metrics, yet we provide results on all datasets and"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "ERC task. Some works only use textual modality",
          "UniMSE still attends to improve multimodal fusion": "corresponding metrics. For example, MTAG only"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "to recognize emotion, in which ERMC-DisGCN",
          "UniMSE still attends to improve multimodal fusion": "conducts experiments on MOSI, and most ERC"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(Sun et al., 2021), Psychological (Li et al., 2021a),",
          "UniMSE still attends to improve multimodal fusion": "works only give the WF1, which makes MSA and"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "DAG-ERC (Shen et al., 2021) and DialogueGCN",
          "UniMSE still attends to improve multimodal fusion": "ERC tasks tend to be isolated in sentiment knowl-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "(Ghosal et al., 2019) adapt the GNN-based model",
          "UniMSE still attends to improve multimodal fusion": "edge. Unlike these works, UniMSE uniﬁes MSA"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "to capture contexts. Additionally, CoG-BART (Li",
          "UniMSE still attends to improve multimodal fusion": "and ERC tasks on these four datasets and evaluates"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "et al., 2021b) learns the context knowledge from",
          "UniMSE still attends to improve multimodal fusion": "them based on the common metrics of the two tasks."
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "the pre-trained model, COSMIC (Ghosal et al.,",
          "UniMSE still attends to improve multimodal fusion": "In summary, 1) UniMSE performs on all bench-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "2020) incorporates different elements of common-",
          "UniMSE still attends to improve multimodal fusion": "mark datasets of MSA and ECR; 2) UniMSE sig-"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "sense, and TODKAT (Zhu et al., 2021) uses topic-",
          "UniMSE still attends to improve multimodal fusion": "niﬁcantly outperforms SOTA in most cases. These"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "driven knowledge-aware Transformer to model af-",
          "UniMSE still attends to improve multimodal fusion": ""
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "",
          "UniMSE still attends to improve multimodal fusion": "6https://github.com/huggingface/transformers/tree/main"
        },
        {
          "Train\nValid\nTest\nAll\nSenti.\nEmo.": "fective states.\nSimilar\nto MSA and ERC works,",
          "UniMSE still attends to improve multimodal fusion": "/src/transformers/models/t5."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: First, we -w/oMELD 0.722 0.776 84.05/84.96 84.50/84.64",
      "data": [
        {
          "MOSI": "ACC-7↑",
          "MOSEI": "ACC-7↑",
          "MELD": "WF1↑",
          "IEMOCAP": "WF1↑"
        },
        {
          "MOSI": "33.20",
          "MOSEI": "48.00",
          "MELD": "58.30",
          "IEMOCAP": "56.49"
        },
        {
          "MOSI": "34.90",
          "MOSEI": "50.20",
          "MELD": "57.74",
          "IEMOCAP": "55.13"
        },
        {
          "MOSI": "35.40",
          "MOSEI": "51.30",
          "MELD": "57.80",
          "IEMOCAP": "61.60"
        },
        {
          "MOSI": "38.90",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "39.00",
          "MOSEI": "51.60",
          "MELD": "-",
          "IEMOCAP": "63.50"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "43.90",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "67.63"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "46.65",
          "MOSEI": "54.24",
          "MELD": "-",
          "IEMOCAP": "-"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "58.10",
          "IEMOCAP": "64.18"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "58.39",
          "IEMOCAP": "66.20"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "63.65",
          "IEMOCAP": "68.03"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "64.22",
          "IEMOCAP": "64.10"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "64.81",
          "IEMOCAP": "66.18"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "65.18",
          "IEMOCAP": "66.96"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "65.21",
          "IEMOCAP": "65.28"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "65.47",
          "IEMOCAP": "61.33"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "58.65",
          "IEMOCAP": "66.22"
        },
        {
          "MOSI": "-",
          "MOSEI": "-",
          "MELD": "59.46",
          "IEMOCAP": "68.18"
        },
        {
          "MOSI": "48.68",
          "MOSEI": "54.39",
          "MELD": "65.51",
          "IEMOCAP": "70.66"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: First, we -w/oMELD 0.722 0.776 84.05/84.96 84.50/84.64",
      "data": [
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "MM-DFN\n-\n-\n-\n-\n-\n-",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "-\n-\n-\n-\n62.49\n59.46\n68.21\n68.18"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "UniMSE\n0.691\n0.809\n48.68\n85.85/86.9\n85.83/86.42\n0.523",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "0.773\n54.39\n85.86/87.50\n85.79/87.46\n65.09\n65.51\n70.56\n70.66"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "Table 2: Results on MOSI, MOSEI, MELD, and IEMOCAP. *The performances of baselines are updated by their",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "authors in the ofﬁcial code repository, and the baselines with italics indicate it only uses textual modality. The",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "results with underline denote the previous SOTA performance.",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "results illustrate the superiority of UniMSE in MSA",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "MAE\nCorr\nACC-2\nF1"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "and ERC tasks and demonstrate the effectiveness of",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "UniMSE\n0.691\n0.809\n85.85/86.9\n85.83/86.42"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "a uniﬁed framework in knowledge sharing among",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o A\n0.719\n0.794\n83.82/85.20\n83.86/85.69"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "tasks and datasets.",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o V\n0.714\n0.798\n84.37/85.37\n84.71/85.78"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o A, V\n0.721\n0.780\n83.72/85.11\n83.52/85.11"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o PMF\n0.722\n0.785\n85.13/86.59\n85.03/86.37"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "4.6\nAblation Study",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o CL\n0.713\n0.795\n85.28/86.59\n85.27/86.55"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "We conducted a series of ablation studies on MOSI,",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o IEMOCAP\n0.718\n0.784\n84.11/85.88\n84.75/85.47"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o MELD\n0.722\n0.776\n84.05/84.96\n84.50/84.64"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "and the results are shown in Table 3.\nFirst, we",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "- w/o MOSEI\n0.775\n0.727\n80.68/81.22\n81.35/81.83"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "eliminate one or\nseveral modalities\nfrom multi-",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "modal signals to verify the modal effects on model",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "Table 3: Ablation study of UniMSE on MOSI. V and"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "performance. We can ﬁnd that\nremoving visual",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "A represent visual and acoustic modalities, respectively."
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "and acoustic modalities or one of them all\nleads",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "PMF and CL represent pre-trained modality fusion and"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "to performance degradation, which indicates that",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "contrastive learning, respectively."
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "the non-verbal signals (i.e., visual and acoustic)",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "are necessary for solving MSA, and demonstrates",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "SEI, resulting in poor performance in the four met-"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "the complementarity among text, acoustic, and vi-",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "rics. The proposed UniMSE is orthogonal\nto the"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "sual. We also ﬁnd that\nthe acoustic modality is",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "existing works, and it is believed that introducing"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "more important than the visual to UniMSE. Then",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "our uniﬁed framework to other tasks can also bring"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "we eliminate module PMF and CL from UniMSE,",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "improvements."
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "which leads to an increase in MAE and a decrease",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "in Corr. These results illustrate the effectiveness of",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "4.7\nVisualization"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "PMF and CL in multimodal representation learn-",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": ""
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "ing. Additionally, we conduct experiments to verify",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "To verify the effects of UniMSE’s UL and cross-"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "the impact of the dataset on UniMSE. We remove",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "task learning on multimodal\nrepresentation, we"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "IEMOCAP, MELD, and MOSEI from the training",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "visualize multimodal\nfusion representation (i.e.,"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "set and evaluate model performance on the MOSI",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "F (j)\n) of the last Transformer layer. Speciﬁcally,"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "i"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "test set. Removing IEMOCAP and MELD hurts",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "we select samples that carry positive/negative senti-"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "the performance, especially in metrics MAE and",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "ment polarity from the test set of MOSI and select"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "Corr. This result may be because the removal of",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "samples that have the joy/sadness emotion from the"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "MELD/IEMOCAP has\nreduced the information",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "test set of MELD. Their representation visualiza-"
        },
        {
          "MMGCN\n-\n-\n-\n-\n-\n-": "they provide for MSA task. We also remove MO-",
          "-\n-\n-\n-\n-\n58.65\n-\n66.22": "tion is shown in Figure 4(a). It can be observed that"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tion is shown in Figure 4(b). The samples with": "joy emotion, whether annotated with the original"
        },
        {
          "tion is shown in Figure 4(b). The samples with": "label or generated based on UL, share a common"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "feature space. These results verify the superiority"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "of UniMSE on representation learning across sam-"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "ples and demonstrate the complementarity between"
        },
        {
          "tion is shown in Figure 4(b). The samples with": "sentiment and emotion."
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "5\nConclusion"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "This paper provides a psychological perspective to"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "demonstrate that\njointly modeling sentiment and"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "emotion is feasible and reasonable. We present a"
        },
        {
          "tion is shown in Figure 4(b). The samples with": "uniﬁed multimodal knowledge-sharing framework,"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "UniMSE, to solve MSA and ERC tasks. UniMSE"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "not only captures knowledge of sentiment and emo-"
        },
        {
          "tion is shown in Figure 4(b). The samples with": "tion, but also aligns the input features and output la-"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "bels. Moreover, we fuse acoustic and visual modal"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "representation with multi-level textual features and"
        },
        {
          "tion is shown in Figure 4(b). The samples with": "introduce inter-modality contrastive learning. We"
        },
        {
          "tion is shown in Figure 4(b). The samples with": ""
        },
        {
          "tion is shown in Figure 4(b). The samples with": "conduct extensive experiments on four benchmark"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "provide a new and different perspective to the MSA": "and ERC research communities."
        },
        {
          "provide a new and different perspective to the MSA": "Limitations"
        },
        {
          "provide a new and different perspective to the MSA": "In this preliminary study, we only integrate con-"
        },
        {
          "provide a new and different perspective to the MSA": "text information on MELD and IEMOCAP, and the"
        },
        {
          "provide a new and different perspective to the MSA": "context on MOSI and MOSEI will be considered in"
        },
        {
          "provide a new and different perspective to the MSA": "the future. Furthermore, the generation of univer-"
        },
        {
          "provide a new and different perspective to the MSA": "sal labels only considers textual modality, without"
        },
        {
          "provide a new and different perspective to the MSA": "considering acoustic and visual modalities, which"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "will also be solved in our future work."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Ethics Statement"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "The data used in this study are all open-source data"
        },
        {
          "provide a new and different perspective to the MSA": "for\nresearch purposes. While making machines"
        },
        {
          "provide a new and different perspective to the MSA": "understand human emotions sounds appealing,\nit"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "could be applied to emotional companion robots"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "or intelligent customer service. However, even in"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "simple six-class emotion recognition (MELD), the"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "proposed method can achieve only 65% in accuracy,"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "which is far from usable."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Acknowledgement"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "This work was supported by the Natural Science"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Foundation of Guangdong, China under Grant No."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "2020A1515010812 and 2021A1515011594."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "References"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Md. Shad Akhtar, Dushyant Singh Chauhan, Deep-"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "anway Ghosal,\nSoujanya Poria, Asif Ekbal,\nand"
        },
        {
          "provide a new and different perspective to the MSA": "Pushpak Bhattacharyya. 2019. Multi-task learning"
        },
        {
          "provide a new and different perspective to the MSA": "for multi-modal emotion recognition and sentiment"
        },
        {
          "provide a new and different perspective to the MSA": "the 2019 Conference\nanalysis.\nIn Proceedings of"
        },
        {
          "provide a new and different perspective to the MSA": "of\nthe North American Chapter of\nthe Association"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "for Computational Linguistics: Human Language"
        },
        {
          "provide a new and different perspective to the MSA": "Technologies, NAACL-HLT 2019, Minneapolis, MN,"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "USA, June 2-7, 2019, Volume 1 (Long and Short"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Papers), pages 370–379. Association for Computa-"
        },
        {
          "provide a new and different perspective to the MSA": "tional Linguistics."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Lei\nJimmy Ba,\nJamie Ryan Kiros,\nand Geoffrey E."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Hinton.\n2016.\nLayer\nnormalization.\nCoRR,"
        },
        {
          "provide a new and different perspective to the MSA": "abs/1607.06450."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Tadas\nBaltrušaitis,\nChaitanya\nAhuja,\nand\nLouis-"
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "Philippe Morency. 2018. Multimodal machine learn-"
        },
        {
          "provide a new and different perspective to the MSA": "IEEE transac-\ning:\nA survey\nand\ntaxonomy."
        },
        {
          "provide a new and different perspective to the MSA": "tions on pattern analysis and machine intelligence,"
        },
        {
          "provide a new and different perspective to the MSA": "41(2):423–443."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "C Daniel Batson, Laura L Shaw, and Kathryn C Oleson."
        },
        {
          "provide a new and different perspective to the MSA": "1992. Differentiating affect, mood, and emotion: To-"
        },
        {
          "provide a new and different perspective to the MSA": "ward functionally based conceptual distinctions."
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": ""
        },
        {
          "provide a new and different perspective to the MSA": "The\nsubtlety of\nAaron Ben-Ze’ev. 2001.\nemotions."
        },
        {
          "provide a new and different perspective to the MSA": "MIT press."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "smith. 2009. Handbook of affective sciences. Ox-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "nette N. Chang,\nSungbok Lee,\nand Shrikanth S.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "ford University Press."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Narayanan. 2008.\nIEMOCAP: interactive emotional",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "dyadic motion capture database. Lang. Resour. Eval-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "uation, 42(4):335–359.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Simcse: Simple contrastive learning of sentence em-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "beddings.\nIn Proceedings of the 2021 Conference on"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Empirical Methods in Natural Language Processing,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Dushyant Singh Chauhan, Md. Shad Akhtar, Asif Ek-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "EMNLP 2021, Virtual Event\n/ Punta Cana, Domini-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "bal,\nand Pushpak Bhattacharyya. 2019.\nContext-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "can Republic, 7-11 November, 2021, pages 6894–"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "aware\ninteractive\nattention\nfor multi-modal\nsenti-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "6910. Association for Computational Linguistics."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "the\nment and emotion analysis.\nIn Proceedings of",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "2019 Conference on Empirical Methods\nin Natu-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Deepanway Ghosal, Md. Shad Akhtar, Dushyant Singh"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ral Language Processing and the 9th International",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Chauhan, Soujanya Poria, Asif Ekbal, and Pushpak"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Joint Conference on Natural Language Processing,",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Bhattacharyya. 2018. Contextual\ninter-modal atten-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "EMNLP-IJCNLP 2019, Hong Kong, China, Novem-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "tion for multi-modal sentiment analysis.\nIn Proceed-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ber 3-7, 2019, pages 5646–5656. Association for",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "ings of\nthe 2018 Conference on Empirical Methods"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Computational Linguistics.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "in Natural Language Processing, Brussels, Belgium,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "October 31 - November 4, 2018, pages 3454–3466."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Zhi Chen, Lu Chen, Bei Chen, Libo Qin, Yuncong Liu,",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Association for Computational Linguistics."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Su Zhu, Jian-Guang Lou, and Kai Yu. 2022. Unidu:",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Towards A uniﬁed generative dialogue understand-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Deepanway Ghosal, Navonil Majumder, Alexander F."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ing framework. CoRR, abs/2204.04637.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "COSMIC:\ncommonsense\nknowledge\nfor\nemotion"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Junyan\nCheng,\nIordanis\nFostiropoulos,\nBarry W.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "identiﬁcation in conversations.\nIn Findings of"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Boehm, and Mohammad Soleymani. 2021a. Multi-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Association for Computational Linguistics: EMNLP"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "modal phased transformer for sentiment analysis.\nIn",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "2020, Online Event, 16-20 November 2020, pages"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Proceedings of\nthe 2021 Conference on Empirical",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "2470–2481."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Methods in Natural Language Processing, EMNLP",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "2021, Virtual Event\n/ Punta Cana, Dominican Re-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Deepanway Ghosal, Navonil Majumder, Soujanya Po-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "public, 7-11 November, 2021, pages 2447–2458. As-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "ria, Niyati Chhaya, and Alexander F. Gelbukh. 2019."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "sociation for Computational Linguistics.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Dialoguegcn: A graph convolutional neural network"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "for emotion recognition in conversation.\nIn Proceed-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Kewei Cheng, Ziqing Yang, Ming Zhang, and Yizhou",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "ings of\nthe 2019 Conference on Empirical Methods"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Sun. 2021b. Uniker: A uniﬁed framework for com-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "in Natural Language Processing and the 9th Inter-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "bining embedding and deﬁnite horn rule reasoning",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "national Joint Conference on Natural Language Pro-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "for\nknowledge\ngraph\ninference.\nIn Proceedings",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "cessing, EMNLP-IJCNLP 2019, Hong Kong, China,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "of\nthe 2021 Conference on Empirical Methods\nin",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "November 3-7, 2019, pages 154–164. Association"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Natural Language Processing, EMNLP 2021, Vir-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "for Computational Linguistics."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "tual Event / Punta Cana, Dominican Republic, 7-11",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Michael Gutmann and Aapo Hyvärinen. 2010. Noise-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "November, 2021, pages 9753–9771. Association for",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "contrastive estimation: A new estimation principle"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Computational Linguistics.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "for unnormalized statistical models.\nIn Proceedings"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "of the Thirteenth International Conference on Artiﬁ-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Yinpei Dai, Hangyu Li, Yongbin Li,\nJian Sun, Fei",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "cial Intelligence and Statistics, AISTATS 2010, Chia"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Huang, Luo Si, and Xiaodan Zhu. 2021.\nPreview,",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Laguna Resort, Sardinia,\nItaly, May 13-15, 2010,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "attend and review: Schema-aware curriculum learn-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "pages 297–304."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ing for multi-domain dialogue state tracking.\nIn Pro-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ceedings of the 59th Annual Meeting of the Associa-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Wei Han, Hui Chen, and Soujanya Poria. 2021.\nIm-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "tion for Computational Linguistics and the 11th In-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "proving multimodal fusion with hierarchical mutual"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "ternational Joint Conference on Natural Language",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "information maximization for multimodal sentiment"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Processing (Volume 2:\nShort Papers), pages 879–",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "analysis.\nIn Proceedings of the 2021 Conference on"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "885.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Empirical Methods in Natural Language Processing,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "EMNLP 2021, Virtual Event\n/ Punta Cana, Domini-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin Li,",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "can Republic, 7-11 November, 2021, pages 9180–"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Jian Sun, and Xiaodan Zhu. 2020a.\nLearning low-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "9192. Association for Computational Linguistics."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "resource end-to-end goal-oriented dialog for fast and",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "the\nreliable system deployment.\nIn Proceedings of",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Devamanyu Hazarika,\nSoujanya\nPoria, Roger Zim-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "58th Annual Meeting of\nthe Association for Compu-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "mermann,\nand Rada Mihalcea.\n2019.\nEmotion"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "tational Linguistics, pages 609–618.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "recognition in conversations with transfer\nlearning"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "from generative\nconversation modeling.\nCoRR,"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Yinpei Dai, Huihua Yu, Yixuan Jiang, Chengguang",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "abs/1910.04980."
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "Tang, Yongbin Li, and Jian Sun. 2020b. A survey",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": ""
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "on dialog management: Recent advances and chal-",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "Devamanyu Hazarika, Roger Zimmermann, and Sou-"
        },
        {
          "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe": "lenges. arXiv preprint arXiv:2005.02233.",
          "Richard J Davidson, Klaus R Sherer, and H Hill Gold-": "janya Poria. 2020. MISA: modality-invariant and"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "analysis.\nIn MM ’20: The 28th ACM International",
          "and the 11th International Joint Conference on Nat-": "ural Language Processing, ACL/IJCNLP 2021, (Vol-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Conference on Multimedia, Virtual Event\n/ Seattle,",
          "and the 11th International Joint Conference on Nat-": "ume 1:\nLong Papers), Virtual Event, August 1-6,"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "WA, USA, October 12-16, 2020, pages 1122–1131.",
          "and the 11th International Joint Conference on Nat-": "2021, pages 5666–5675. Association for Computa-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "ACM.",
          "and the 11th International Joint Conference on Nat-": "tional Linguistics."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Wanwei He, Yinpei Dai, Binyuan Hui, Min Yang,",
          "and the 11th International Joint Conference on Nat-": "Abhinav\nJoshi,\nAshwani\nBhat,\nAyush\nJain,"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Zheng Cao, Jianbo Dong, Fei Huang, Luo Si, and",
          "and the 11th International Joint Conference on Nat-": "Atin Vikram Singh,\nand Ashutosh Modi.\n2022."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Yongbin Li. 2022a.\nSpace-2: Tree-structured semi-",
          "and the 11th International Joint Conference on Nat-": "COGMEN: contextualized GNN based multimodal"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "supervised contrastive pre-training for task-oriented",
          "and the 11th International Joint Conference on Nat-": "emotion recognition. CoRR, abs/2205.02455."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "the 29th\ndialog understanding.\nIn Proceedings of",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "International Conference on Computational Linguis-",
          "and the 11th International Joint Conference on Nat-": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "tics.",
          "and the 11th International Joint Conference on Nat-": "Sarna,\nYonglong\nTian,\nPhillip\nIsola,\nAaron"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "Maschinot, Ce Liu, and Dilip Krishnan. 2020.\nSu-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Wanwei He, Yinpei Dai, Min Yang,\nJian Sun, Fei",
          "and the 11th International Joint Conference on Nat-": "pervised contrastive learning.\nIn Advances in Neu-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Huang, Luo Si, and Yongbin Li. 2022b.\nSpace-3:",
          "and the 11th International Joint Conference on Nat-": "ral Information Processing Systems 33: Annual Con-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Uniﬁed dialog model pre-training for\ntask-oriented",
          "and the 11th International Joint Conference on Nat-": "ference on Neural\nInformation Processing Systems"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "dialog understanding and generation.\nIn Proceed-",
          "and the 11th International Joint Conference on Nat-": "2020, NeurIPS 2020, December 6-12, 2020, virtual."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "ings of\nthe 45th International ACM SIGIR Confer-",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "ence on Research and Development\nin Information",
          "and the 11th International Joint Conference on Nat-": "Joosung Lee and Wooin Lee. 2021. Compm: Context"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Retrieval, pages 187–200.",
          "and the 11th International Joint Conference on Nat-": "modeling with speaker’s pre-trained memory track-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "ing for emotion recognition in conversation. CoRR,"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu,",
          "and the 11th International Joint Conference on Nat-": "abs/2108.11626."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Huang, Luo Si,\net\nal. 2022c.\nSpace:\nA genera-",
          "and the 11th International Joint Conference on Nat-": "Jiangnan Li, Zheng Lin, Peng Fu, and Weiping Wang."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "tive pre-trained model for task-oriented dialog with",
          "and the 11th International Joint Conference on Nat-": "2021a.\nPast, present,\nand future:\nConversational"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "semi-supervised learning and explicit policy injec-",
          "and the 11th International Joint Conference on Nat-": "emotion recognition through structural modeling of"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Proceedings of\nthe AAAI Conference on Arti-\ntion.",
          "and the 11th International Joint Conference on Nat-": "psychological knowledge.\nIn Findings of the Associ-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "ﬁcial Intelligence.",
          "and the 11th International Joint Conference on Nat-": "ation for Computational Linguistics: EMNLP 2021,"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "Virtual Event\n/ Punta Cana, Dominican Republic,"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,",
          "and the 11th International Joint Conference on Nat-": "16-20 November, 2021, pages 1204–1214. Associa-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Bruna Morrone, Quentin de Laroussilhe, Andrea",
          "and the 11th International Joint Conference on Nat-": "tion for Computational Linguistics."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Gesmundo, Mona Attariyan,\nand\nSylvain Gelly.",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "2019. Parameter-efﬁcient transfer learning for NLP.",
          "and the 11th International Joint Conference on Nat-": "Shimin Li, Hang Yan, and Xipeng Qiu. 2021b.\nCon-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "In Proceedings of the 36th International Conference",
          "and the 11th International Joint Conference on Nat-": "trast and generation make BART a good dialogue"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "on Machine Learning, ICML 2019, 9-15 June 2019,",
          "and the 11th International Joint Conference on Nat-": "emotion recognizer. CoRR, abs/2112.11202."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Long Beach, California, USA, pages 2790–2799.",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "Paul\nPu\nLiang,\nAmir\nZadeh,\nand\nLouis-Philippe"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Dou Hu, Xiaolong Hou, Lingwei Wei, Lian-Xin Jiang,",
          "and the 11th International Joint Conference on Nat-": "Morency.\n2022.\nFoundations\nand\nrecent\ntrends"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "and Yang Mo. 2022.\nMM-DFN: multimodal dy-",
          "and the 11th International Joint Conference on Nat-": "in multimodal machine learning:\nPrinciples,\nchal-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "namic\nfusion network for\nemotion recognition in",
          "and the 11th International Joint Conference on Nat-": "arXiv\npreprint\nlenges,\nand\nopen\nquestions."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "conversations.\nIn IEEE International Conference on",
          "and the 11th International Joint Conference on Nat-": "arXiv:2209.03430."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Acoustics, Speech and Signal Processing,\nICASSP",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "2022, Virtual and Singapore, 23-27 May 2022, pages",
          "and the 11th International Joint Conference on Nat-": "Ting-En Lin, Yuchuan Wu, Fei Huang, Luo Si,\nJian"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "7037–7041.",
          "and the 11th International Joint Conference on Nat-": "Sun, and Yongbin Li. 2022.\nDuplex conversation:"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "Towards human-like interaction in spoken dialogue"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Guimin Hu, Guangming Lu, and Yi Zhao. 2021a. Bidi-",
          "and the 11th International Joint Conference on Nat-": "the 28th ACM SIGKDD\nsystem.\nIn Proceedings of"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "rectional hierarchical attention networks based on",
          "and the 11th International Joint Conference on Nat-": "Conference on Knowledge Discovery & Data Min-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "document-level context for emotion cause extraction.",
          "and the 11th International Joint Conference on Nat-": "ing."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "the Association for Computational\nIn Findings of",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Linguistics:\nEMNLP 2021, Virtual Event\n/ Punta",
          "and the 11th International Joint Conference on Nat-": "Ting-En Lin and Hua Xu. 2019a. Deep unknown intent"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Cana, Dominican Republic, 16-20 November, 2021,",
          "and the 11th International Joint Conference on Nat-": "the\ndetection with margin loss.\nIn Proceedings of"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "pages 558–568.",
          "and the 11th International Joint Conference on Nat-": "57th Annual Meeting of\nthe Association for Compu-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "tational Linguistics, pages 5491–5496."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Guimin Hu, Guangming Lu, and Yi Zhao. 2021b. FSS-",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "GCN: A graph convolutional networks with fusion",
          "and the 11th International Joint Conference on Nat-": "Ting-En Lin and Hua Xu. 2019b. A post-processing"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "of semantic and structure for emotion cause analysis.",
          "and the 11th International Joint Conference on Nat-": "method for detecting unknown intent of dialogue"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Knowl. Based Syst., 212:106584.",
          "and the 11th International Joint Conference on Nat-": "system via pre-trained deep neural network classiﬁer."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "",
          "and the 11th International Joint Conference on Nat-": "Knowledge-Based Systems, 186:104979."
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "Jingwen Hu, Yuchen Liu, Jinming Zhao, and Qin Jin.",
          "and the 11th International Joint Conference on Nat-": ""
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "2021c. MMGCN: multimodal fusion via deep graph",
          "and the 11th International Joint Conference on Nat-": "Ting-En Lin, Hua Xu, and Hanlei Zhang. 2020. Dis-"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "convolution network for emotion recognition in con-",
          "and the 11th International Joint Conference on Nat-": "covering new intents via constrained deep adaptive"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "the 59th Annual Meet-\nversation.\nIn Proceedings of",
          "and the 11th International Joint Conference on Nat-": "clustering with cluster\nreﬁnement.\nIn Proceedings"
        },
        {
          "-speciﬁc\nrepresentations\nfor multimodal\nsentiment": "ing of the Association for Computational Linguistics",
          "and the 11th International Joint Conference on Nat-": "of AAAI, pages 8360–8367."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "narasimhan, Paul Pu Liang, Amir Zadeh, and Louis-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Gardner, Christopher Clark, Kenton Lee, and Luke"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Philippe Morency. 2018.\nEfﬁcient\nlow-rank multi-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Zettlemoyer. 2018. Deep contextualized word rep-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "modal fusion with modality-speciﬁc factors.\nIn Pro-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "the 2018 Confer-\nresentations.\nIn Proceedings of"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ceedings of the 56th Annual Meeting of the Associa-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "ence of\nthe North American Chapter of\nthe Associ-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "tion for Computational Linguistics, ACL 2018, Mel-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "ation for Computational Linguistics: Human Lan-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "bourne, Australia, July 15-20, 2018, Volume 1: Long",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "guage Technologies, NAACL-HLT 2018, New Or-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Papers, pages 2247–2256. Association for Computa-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "leans, Louisiana, USA, June 1-6, 2018, Volume 1"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "tional Linguistics.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "(Long Papers), pages 2227–2237. Association for"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Computational Linguistics."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Huaishao Luo, Lei\nJi, Yanyong Huang, Bin Wang,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Shenggong Ji,\nand Tianrui Li. 2021.\nScalevlad:",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Soujanya Poria, Erik Cambria, Devamanyu Hazarika,"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Improving multimodal sentiment analysis via multi-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Navonil Majumder, Amir Zadeh, and Louis-Philippe"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "scale\nfusion\nof\nlocally\ndescriptors.\nCoRR,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Morency.\n2017.\nMulti-level multiple\nattentions"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "abs/2112.01368.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "for contextual multimodal\nsentiment analysis.\nIn"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "2017 IEEE International Conference on Data Min-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Sijie Mai, Haifeng Hu,\nand Songlong Xing.\n2020.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "ing, ICDM 2017, New Orleans, LA, USA, November"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Modality to modality translation: An adversarial rep-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "18-21, 2017, pages 1033–1038."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "resentation learning and graph fusion network for",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "multimodal fusion.\nIn The Thirty-Fourth AAAI Con-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ference on Artiﬁcial\nIntelligence, AAAI 2020, The",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Thirty-Second Innovative Applications of Artiﬁcial",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "halcea. 2019.\nMELD: A multimodal multi-party"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Intelligence Conference, IAAI 2020, The Tenth AAAI",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "dataset for emotion recognition in conversations.\nIn"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Symposium on Educational Advances in Artiﬁcial In-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Proceedings of\nthe 57th Conference of\nthe Associa-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "telligence, EAAI 2020, New York, NY, USA, Febru-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "tion for Computational Linguistics, ACL 2019, Flo-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ary 7-12, 2020, pages 164–172. AAAI Press.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "rence,\nItaly,\nJuly 28- August 2,\n2019, Volume 1:"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Long Papers, pages 527–536. Association for Com-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Huisheng Mao, Ziqi Yuan, Hua Xu, Wenmeng Yu, Yihe",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "putational Linguistics."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Liu, and Kai Gao. 2022. M-sena: An integrated plat-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "form for multimodal sentiment analysis.\nIn Proceed-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ings of\nthe 60th Annual Meeting of\nthe Association",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "for Computational Linguistics:\nSystem Demonstra-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Wei Li, and Peter J. Liu. 2020. Exploring the limits"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "tions, pages 204–213.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "of transfer learning with a uniﬁed text-to-text\ntrans-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "former. J. Mach. Learn. Res., 21:140:1–140:67."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "and Xuan Li. 2021. Dialoguetrm: Exploring multi-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee,"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "modal emotional dynamics\nin a conversation.\nIn",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "AmirAli Bagher Zadeh, Chengfeng Mao,\nLouis-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Findings of\nthe Association for Computational Lin-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Philippe Morency, and Mohammed E. Hoque. 2020."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "guistics: EMNLP 2021, Virtual Event / Punta Cana,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Integrating multimodal\ninformation\nin\nlarge\npre-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Dominican Republic, 16-20 November, 2021, pages",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "trained transformers.\nIn Proceedings of the 58th An-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "2694–2704.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "nual Meeting of\nthe Association for Computational"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Linguistics, ACL 2020, Online,\nJuly\n5-10,\n2020,"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Louis-Philippe Morency, Rada Mihalcea,\nand Payal",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "pages 2359–2369."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Doshi. 2011.\nTowards multimodal sentiment anal-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ysis:\nharvesting opinions\nfrom the web.\nIn Pro-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Robert K Shelly. 2004. Emotions, sentiments, and per-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ceedings of\nthe 13th International Conference on",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "formance expectations.\nIn Theory and research on"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Multimodal Interfaces, ICMI 2011, Alicante, Spain,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "human emotions. Emerald Group Publishing Lim-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "November 14-18, 2011, pages 169–176.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "ited."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Myriam Munezero, Calkin Suero Montero, Erkki Suti-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "nen, and John Pajunen. 2014. Are they different? af-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Quan. 2021. Directed acyclic graph network for con-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "fect, feeling, emotion, sentiment, and opinion detec-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "versational emotion recognition.\nIn Proceedings of"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "tion in text.\nIEEE Trans. Affect. Comput., 5(2):101–",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "the 59th Annual Meeting of the Association for Com-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "111.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "putational Linguistics\nand\nthe\n11th\nInternational"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Joint Conference on Natural Language Processing,"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Henry Alexander Murray and Christiana D Morgan.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Vir-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "1945. A clinical study of sentiments (i & ii). Ge-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "tual Event, August 1-6, 2021, pages 1551–1560. As-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "netic Psychology Monographs.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "sociation for Computational Linguistics."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Jiquan Ngiam, Aditya Khosla, Mingyu Kim,\nJuhan",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Jan E Stets. 2006. Emotions and sentiments.\nIn Hand-"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "Nam, Honglak Lee, and Andrew Y. Ng. 2011. Multi-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "book of social psychology, pages 309–335. Springer."
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "modal deep learning.\nIn Proceedings of the 28th In-",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": ""
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "ternational Conference on Machine Learning, ICML",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "Yang\nSun, Nan Yu,\nand Guohong\nFu.\n2021.\nA"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "2011, Bellevue, Washington, USA, June 28 - July 2,",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "discourse-aware graph neural network for emotion"
        },
        {
          "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-": "2011, pages 689–696. Omnipress.",
          "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt": "recognition in multi-party conversation.\nIn Findings"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nthe Association for Computational Linguistics:": "EMNLP 2021, Virtual Event\n/ Punta Cana, Domini-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Lei Li, and Junchi Yan. 2021b. Unire: A uniﬁed la-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "can Republic, 16-20 November, 2021, pages 2949–",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "bel space for entity relation extraction.\nIn Proceed-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "2958. Association for Computational Linguistics.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ings of\nthe 59th Annual Meeting of\nthe Association"
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "for Computational Linguistics and the 11th Interna-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Zhongkai\nSun,\nPrathusha\nKameswara\nSarma,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tional Joint Conference on Natural Language Pro-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "William A.\nSethares,\nand Yingyu\nLiang.\n2020.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "cessing, ACL/IJCNLP 2021,\n(Volume 1:\nLong Pa-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Learning\nrelationships\nbetween\ntext,\naudio,\nand",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "pers), Virtual Event, August 1-6, 2021, pages 220–"
        },
        {
          "of\nthe Association for Computational Linguistics:": "video via deep canonical correlation for multimodal",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "231."
        },
        {
          "of\nthe Association for Computational Linguistics:": "The\nThirty-Fourth AAAI\nlanguage\nanalysis.\nIn",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "Conference on Artiﬁcial\nIntelligence, AAAI 2020,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "The\nThirty-Second\nInnovative\nApplications\nof",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Torsten Scholak, Michihiro Yasunaga, Chien-Sheng"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Artiﬁcial\nIntelligence Conference,\nIAAI 2020, The",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Tenth AAAI Symposium on Educational Advances",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "in Artiﬁcial Intelligence, EAAI 2020, New York, NY,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Ansong Ni, Ziyu Yao, Dragomir R. Radev, Caiming"
        },
        {
          "of\nthe Association for Computational Linguistics:": "USA, February 7-12, 2020, pages 8992–8999.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Luke Zettlemoyer, and Tao Yu. 2022.\nUniﬁedskg:"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Mingxing Tan and Quoc V. Le. 2019.\nEfﬁcientnet:",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Unifying\nand multi-tasking\nstructured\nknowledge"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Rethinking model scaling for convolutional neural",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "grounding with text-to-text language models. CoRR,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "the 36th International\nnetworks.\nIn Proceedings of",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "abs/2201.05966."
        },
        {
          "of\nthe Association for Computational Linguistics:": "Conference on Machine Learning,\nICML 2019, 9-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "15 June 2019, Long Beach, California, USA, vol-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng"
        },
        {
          "of\nthe Association for Computational Linguistics:": "ume 97 of Proceedings of Machine Learning Re-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Zhang. 2021a. A uniﬁed generative framework for"
        },
        {
          "of\nthe Association for Computational Linguistics:": "search, pages 6105–6114. PMLR.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "aspect-based sentiment analysis.\nIn Proceedings of"
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "the 59th Annual Meeting of the Association for Com-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "putational Linguistics\nand\nthe\n11th\nInternational"
        },
        {
          "of\nthe Association for Computational Linguistics:": "J. Zico Kolter, Louis-Philippe Morency, and Ruslan",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Joint Conference on Natural Language Processing,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Salakhutdinov. 2019a. Multimodal\ntransformer\nfor",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Vir-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "unaligned multimodal\nlanguage sequences.\nIn Pro-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tual Event, August 1-6, 2021, pages 2416–2429."
        },
        {
          "of\nthe Association for Computational Linguistics:": "ceedings of\nthe 57th Conference of\nthe Association",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "for Computational Linguistics, ACL 2019, Florence,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Hang Yan, Tao Gui,\nJunqi Dai, Qipeng Guo, Zheng"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Italy, July 28- August 2, 2019, Volume 1: Long Pa-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Zhang, and Xipeng Qiu. 2021b. A uniﬁed genera-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "pers, pages 6558–6569. Association for Computa-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tive framework for various NER subtasks.\nIn Pro-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "tional Linguistics.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ceedings of the 59th Annual Meeting of the Associa-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tion for Computational Linguistics and the 11th In-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ternational Joint Conference on Natural Language"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Louis-Philippe Morency, and Ruslan Salakhutdinov.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Processing, ACL/IJCNLP 2021,\n(Volume 1:\nLong"
        },
        {
          "of\nthe Association for Computational Linguistics:": "2019b. Learning factorized multimodal representa-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Papers), Virtual Event, August\n1-6,\n2021,\npages"
        },
        {
          "of\nthe Association for Computational Linguistics:": "tions.\nIn 7th International Conference on Learning",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "5808–5822. Association for Computational Linguis-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Representations, ICLR 2019, New Orleans, LA, USA,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "tics."
        },
        {
          "of\nthe Association for Computational Linguistics:": "May 6-9, 2019. OpenReview.net.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Azaan Rehman, Amir Zadeh, Soujanya Poria, and"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Louis-Philippe Morency.\n2021.\nMTAG: modal-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Kaiser, and Illia Polosukhin. 2017. Attention is all",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "temporal attention graph for unaligned human multi-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "you need.\nIn Advances in Neural Information Pro-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "the\nmodal\nlanguage sequences.\nIn Proceedings of"
        },
        {
          "of\nthe Association for Computational Linguistics:": "cessing Systems 30: Annual Conference on Neural",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "2021 Conference of\nthe North American Chapter"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Information Processing Systems 2017, December 4-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "of\nthe Association for Computational Linguistics:"
        },
        {
          "of\nthe Association for Computational Linguistics:": "9, 2017, Long Beach, CA, USA, pages 5998–6008.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Human Language Technologies, NAACL-HLT 2021,"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Wenhui Wang, Hangbo Bao,\nLi Dong,\nand\nFuru",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Online, June 6-11, 2021, pages 1009–1021."
        },
        {
          "of\nthe Association for Computational Linguistics:": "Wei. 2021a.\nVlmo: Uniﬁed vision-language pre-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "training with mixture-of-modality-experts.\nCoRR,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. 2021a."
        },
        {
          "of\nthe Association for Computational Linguistics:": "abs/2111.02358.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Learning modality-speciﬁc representations with self-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "supervised multi-task learning for multimodal senti-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ment analysis.\nIn Thirty-Fifth AAAI Conference on"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Amir Zadeh,\nand Louis-Philippe Morency.\n2019.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Con-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Words can shift: Dynamically adjusting word rep-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ference on Innovative Applications of Artiﬁcial Intel-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "resentations\nusing\nnonverbal\nbehaviors.\nIn The",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ligence, IAAI 2021, The Eleventh Symposium on Ed-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "Thirty-Third AAAI Conference on Artiﬁcial\nIntelli-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "ucational Advances in Artiﬁcial\nIntelligence, EAAI"
        },
        {
          "of\nthe Association for Computational Linguistics:": "gence, AAAI 2019, The Thirty-First\nInnovative Ap-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "2021, Virtual Event, February\n2-9,\n2021,\npages"
        },
        {
          "of\nthe Association for Computational Linguistics:": "plications of Artiﬁcial Intelligence Conference, IAAI",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "10790–10797. AAAI Press."
        },
        {
          "of\nthe Association for Computational Linguistics:": "2019, The Ninth AAAI Symposium on Educational",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": ""
        },
        {
          "of\nthe Association for Computational Linguistics:": "Advances in Artiﬁcial Intelligence, EAAI 2019, Hon-",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. 2021b."
        },
        {
          "of\nthe Association for Computational Linguistics:": "olulu, Hawaii, USA, January 27 - February 1, 2019,",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "Learning modality-speciﬁc representations with self-"
        },
        {
          "of\nthe Association for Computational Linguistics:": "pages 7216–7223.",
          "Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,": "supervised multi-task learning for multimodal sen-"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "ference on Artiﬁcial Intelligence, volume 35, pages",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "Conference on Artiﬁcial\nIntelligence, AAAI 2022,"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "10790–10797.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "Thirty-Fourth Conference\non\nInnovative Applica-"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "tions of Artiﬁcial Intelligence, IAAI 2022, The Twel-"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Ziqi Yuan, Wei Li, Hua Xu, and Wenmeng Yu. 2021.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "veth Symposium on Educational Advances in Artiﬁ-"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Transformer-based\nfeature\nreconstruction\nnetwork",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "cial Intelligence, EAAI 2022 Virtual Event, February"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "for\nrobust multimodal sentiment analysis.\nIn Pro-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "22 - March 1, 2022, pages 11757–11764."
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "ceedings of the 29th ACM International Conference",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "on Multimedia, pages 4400–4407.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou,"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "and Yulan He. 2021.\nTopic-driven and knowledge-"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Amir Zadeh, Minghai Chen,\nSoujanya Poria,\nErik",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "aware transformer\nfor dialogue emotion detection."
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Cambria, and Louis-Philippe Morency. 2017.\nTen-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "the 59th Annual Meeting of\nthe\nIn Proceedings of"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "sor\nfusion network for multimodal sentiment anal-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "Association for Computational Linguistics and the"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "the 2017 Conference on\nysis.\nIn Proceedings of",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "11th International Joint Conference on Natural Lan-"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Empirical Methods in Natural Language Processing,",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "guage Processing, ACL/IJCNLP 2021,\n(Volume 1:"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "EMNLP 2017, Copenhagen, Denmark, September 9-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "Long Papers),\nVirtual Event,\nAugust\n1-6,\n2021,"
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "11, 2017, pages 1103–1114. Association for Compu-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": "pages 1571–1582."
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "tational Linguistics.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cam-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "bria,\nand Louis-Philippe Morency. 2018.\nMulti-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "modal language analysis in the wild: CMU-MOSEI",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "dataset and interpretable dynamic fusion graph.\nIn",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Proceedings of\nthe 56th Annual Meeting of\nthe As-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "sociation for Computational Linguistics, ACL 2018,",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Melbourne, Australia,\nJuly\n15-20,\n2018, Volume",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "1: Long Papers, pages 2236–2246. Association for",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Computational Linguistics.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Amir Zadeh, Rowan Zellers, Eli Pincus,\nand Louis-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Philippe Morency. 2016. Multimodal sentiment\nin-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "tensity analysis in videos: Facial gestures and verbal",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "messages.\nIEEE Intell. Syst., 31(6):82–88.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Hanlei Zhang, Hua Xu, and Ting-En Lin. 2021a. Deep",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "open\nintent\nclassiﬁcation with\nadaptive\ndecision",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "the AAAI Conference on\nboundary. Proceedings of",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Artiﬁcial Intelligence, 35(16):14374–1438.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Hanlei Zhang, Hua Xu, Ting-En Lin,\nand Rui Lyu.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "2021b. Discovering new intents with deep aligned",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "the AAAI Conference on\nclustering. Proceedings of",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Artiﬁcial Intelligence, 35(16):14365–14373.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Hanlei Zhang, Hua Xu, Xin Wang, Qianrui Zhou, Shao-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "jie Zhao, and Jiayan Teng. 2022a. Mintrec: A new",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "arXiv\ndataset\nfor multimodal\nintent\nrecognition.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "preprint arXiv:2209.04355.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Sai Zhang, Yuwei Hu, Yuchuan Wu, Jiaman Wu, Yong-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "bin Li,\nJian Sun, Caixia Yuan, and Xiaojie Wang.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "2022b.\nA slot\nis not built\nin one utterance:\nSpo-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "ken language dialogs with sub-slots.\nIn Findings of",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "the Association for Computational Linguistics: ACL",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "2022, pages 309–321, Dublin,\nIreland. Association",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "for Computational Linguistics.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Zhengkun\nZhang,\nXiaojun Meng,\nYasheng Wang,",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Xin\nJiang, Qun Liu,\nand Zhenglu Yang.\n2021c.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Unims: A uniﬁed framework for multimodal sum-",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "marization with\nknowledge\ndistillation.\nCoRR,",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "abs/2109.05812.",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Zhengkun Zhang, Xiaojun Meng, Yasheng Wang, Xin",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "Jiang, Qun Liu, and Zhenglu Yang. 2022c. Unims:",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        },
        {
          "the AAAI Con-\ntiment analysis.\nIn Proceedings of": "A uniﬁed framework for multimodal summarization",
          "with knowledge distillation.\nIn Thirty-Sixth AAAI": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 6: The distribution of emotion category on",
      "data": [
        {
          "A\nAppendix": "",
          "neural\nfrustrated\nangry\nsadness\njoy": "train\n1187\n1322\n832\n762\n431",
          "excited": "703"
        },
        {
          "A\nAppendix": "A.1\nDatasets",
          "neural\nfrustrated\nangry\nsadness\njoy": "dev\n137\n146\n101\n77\n21",
          "excited": "39"
        },
        {
          "A\nAppendix": "",
          "neural\nfrustrated\nangry\nsadness\njoy": "test\n384\n381\n170\n245\n299",
          "excited": "143"
        },
        {
          "A\nAppendix": "We count the duration of the video segment in MSA",
          "neural\nfrustrated\nangry\nsadness\njoy": "",
          "excited": ""
        },
        {
          "A\nAppendix": "",
          "neural\nfrustrated\nangry\nsadness\njoy": "all\n1708\n1849\n1103\n1084\n751",
          "excited": "885"
        },
        {
          "A\nAppendix": "and ERC and give the results in Table 4. We take",
          "neural\nfrustrated\nangry\nsadness\njoy": "",
          "excited": ""
        },
        {
          "A\nAppendix": "the length of\nthe video segment as the duration",
          "neural\nfrustrated\nangry\nsadness\njoy": "Table\n6:\nThe\ndistribution\nof\nemotion",
          "excited": "category\non"
        },
        {
          "A\nAppendix": "of sentiment or emotion. We can observe that the",
          "neural\nfrustrated\nangry\nsadness\njoy": "dataset IEMOCAP.",
          "excited": ""
        },
        {
          "A\nAppendix": "average time of sentiment in MSA is longer than",
          "neural\nfrustrated\nangry\nsadness\njoy": "",
          "excited": ""
        },
        {
          "A\nAppendix": "that of emotion in ERC, demonstrating the differ-",
          "neural\nfrustrated\nangry\nsadness\njoy": "Algorithm 1: Decoding Algorithm for",
          "excited": ""
        },
        {
          "A\nAppendix": "ence between sentiment and emotion. The average",
          "neural\nfrustrated\nangry\nsadness\njoy": "MSA and ERC tasks",
          "excited": ""
        },
        {
          "A\nAppendix": "length of the video segment in MOSEI is 7.6 sec-",
          "neural\nfrustrated\nangry\nsadness\njoy": "Input: Target task t ∈ {M SA, ERC},",
          "excited": ""
        },
        {
          "A\nAppendix": "onds. This may indicate why MOSEI is usually",
          "neural\nfrustrated\nangry\nsadness\njoy": "target sequence",
          "excited": ""
        },
        {
          "A\nAppendix": "used to study sentiments rather than emotions. Fur-",
          "neural\nfrustrated\nangry\nsadness\njoy": "Y = {y1, y2, · · ·\n, yN } and",
          "excited": ""
        },
        {
          "A\nAppendix": "",
          "neural\nfrustrated\nangry\nsadness\njoy": "yi = (yp",
          "excited": ""
        },
        {
          "A\nAppendix": "thermore, we count emotion categories of MELD",
          "neural\nfrustrated\nangry\nsadness\njoy": "i , yc\ni )\ni , yr",
          "excited": ""
        },
        {
          "A\nAppendix": "and IEMOCAP, and their distributions of the train",
          "neural\nfrustrated\nangry\nsadness\njoy": "Output: Task prediction",
          "excited": ""
        },
        {
          "A\nAppendix": "",
          "neural\nfrustrated\nangry\nsadness\njoy": "Y t = {yt\n, yt",
          "excited": ""
        },
        {
          "A\nAppendix": "set, valid set, and test set are shown in Table 5 and",
          "neural\nfrustrated\nangry\nsadness\njoy": "1, yt\n2, · · ·\nN } for target",
          "excited": ""
        },
        {
          "A\nAppendix": "Table 6, respectively.",
          "neural\nfrustrated\nangry\nsadness\njoy": "t",
          "excited": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 6: The distribution of emotion category on",
      "data": [
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "Y t = {yt"
        },
        {
          "and IEMOCAP, and their distributions of the train": "set, valid set, and test set are shown in Table 5 and",
          "Output: Task prediction": "1, yt\n2, · · ·"
        },
        {
          "and IEMOCAP, and their distributions of the train": "Table 6, respectively.",
          "Output: Task prediction": "t"
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "Y t = {}"
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "in Y do\nfor each yi"
        },
        {
          "and IEMOCAP, and their distributions of the train": "Task\nDataset\nD-A VL (s)\nT-A VL (s)",
          "Output: Task prediction": ""
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "yr"
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "i =yi[1]"
        },
        {
          "and IEMOCAP, and their distributions of the train": "MOSI\n4.2",
          "Output: Task prediction": ""
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "yc"
        },
        {
          "and IEMOCAP, and their distributions of the train": "MSA\n7.3",
          "Output: Task prediction": "i =yi[2]"
        },
        {
          "and IEMOCAP, and their distributions of the train": "MOSEI\n7.6",
          "Output: Task prediction": ""
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "if t is MSA then"
        },
        {
          "and IEMOCAP, and their distributions of the train": "MELD\n3.2",
          "Output: Task prediction": "Y t.append(yr"
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "i )"
        },
        {
          "and IEMOCAP, and their distributions of the train": "3.7\nERC",
          "Output: Task prediction": ""
        },
        {
          "and IEMOCAP, and their distributions of the train": "IEMOCAP\n4.6",
          "Output: Task prediction": "end"
        },
        {
          "and IEMOCAP, and their distributions of the train": "",
          "Output: Task prediction": "if t is ERC then"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 6: The distribution of emotion category on",
      "data": [
        {
          "and T-A VL(s) denote": "",
          "the": "",
          "average video length of": ""
        },
        {
          "and T-A VL(s) denote": "datasets and tasks, respectively.",
          "the": "",
          "average video length of": ""
        },
        {
          "and T-A VL(s) denote": "",
          "the": "",
          "average video length of": ""
        },
        {
          "and T-A VL(s) denote": "",
          "the": "",
          "average video length of": ""
        },
        {
          "and T-A VL(s) denote": "",
          "the": "sadness",
          "average video length of": "joy"
        },
        {
          "and T-A VL(s) denote": "train",
          "the": "683",
          "average video length of": "1744"
        },
        {
          "and T-A VL(s) denote": "",
          "the": "",
          "average video length of": ""
        },
        {
          "and T-A VL(s) denote": "dev",
          "the": "112",
          "average video length of": "163"
        },
        {
          "and T-A VL(s) denote": "test",
          "the": "208",
          "average video length of": "402"
        },
        {
          "and T-A VL(s) denote": "all",
          "the": "1003",
          "average video length of": "2309"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "Shad Akhtar",
        "Dushyant Singh Chauhan",
        "Deepanway Ghosal",
        "Soujanya Poria",
        "Asif Ekbal",
        "Pushpak Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": "10.18653/v1/n19-1034"
    },
    {
      "citation_id": "2",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei",
        "Jamie Ba",
        "Geoffrey Ryan Kiros",
        "Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Differentiating affect, mood, and emotion: Toward functionally based conceptual distinctions",
      "authors": [
        "Laura Daniel Batson",
        "Kathryn Shaw",
        "Oleson"
      ],
      "year": "1992",
      "venue": "Differentiating affect, mood, and emotion: Toward functionally based conceptual distinctions"
    },
    {
      "citation_id": "5",
      "title": "The subtlety of emotions",
      "authors": [
        "Aaron Ben-Ze'ev"
      ],
      "year": "2001",
      "venue": "The subtlety of emotions"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "7",
      "title": "Contextaware interactive attention for multi-modal sentiment and emotion analysis",
      "authors": [
        "Dushyant Singh Chauhan",
        "Md Akhtar",
        "Asif Ekbal",
        "Pushpak Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-1566"
    },
    {
      "citation_id": "8",
      "title": "Unidu: Towards A unified generative dialogue understanding framework",
      "authors": [
        "Zhi Chen",
        "Lu Chen",
        "Bei Chen",
        "Libo Qin",
        "Yuncong Liu",
        "Su Zhu",
        "Jian-Guang Lou",
        "Kai Yu"
      ],
      "year": "2022",
      "venue": "Unidu: Towards A unified generative dialogue understanding framework",
      "doi": "10.48550/arXiv.2204.04637"
    },
    {
      "citation_id": "9",
      "title": "2021a. Multimodal phased transformer for sentiment analysis",
      "authors": [
        "Junyan Cheng",
        "Iordanis Fostiropoulos",
        "Barry Boehm",
        "Mohammad Soleymani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.189"
    },
    {
      "citation_id": "10",
      "title": "Uniker: A unified framework for combining embedding and definite horn rule reasoning for knowledge graph inference",
      "authors": [
        "Kewei Cheng",
        "Ziqing Yang",
        "Ming Zhang",
        "Yizhou Sun"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.769"
    },
    {
      "citation_id": "11",
      "title": "Preview, attend and review: Schema-aware curriculum learning for multi-domain dialogue state tracking",
      "authors": [
        "Yinpei Dai",
        "Hangyu Li",
        "Yongbin Li",
        "Jian Sun",
        "Fei Huang",
        "Luo Si",
        "Xiaodan Zhu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Learning lowresource end-to-end goal-oriented dialog for fast and reliable system deployment",
      "authors": [
        "Yinpei Dai",
        "Hangyu Li",
        "Chengguang Tang",
        "Yongbin Li",
        "Jian Sun",
        "Xiaodan Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "A survey on dialog management: Recent advances and challenges",
      "authors": [
        "Yinpei Dai",
        "Huihua Yu",
        "Yixuan Jiang",
        "Chengguang Tang",
        "Yongbin Li",
        "Jian Sun"
      ],
      "year": "2020",
      "venue": "A survey on dialog management: Recent advances and challenges",
      "arxiv": "arXiv:2005.02233"
    },
    {
      "citation_id": "14",
      "title": "Handbook of affective sciences",
      "authors": [
        "Klaus Richard J Davidson",
        "H Hill Sherer",
        "Goldsmith"
      ],
      "year": "2009",
      "venue": "Handbook of affective sciences"
    },
    {
      "citation_id": "15",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.552"
    },
    {
      "citation_id": "16",
      "title": "Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Md Deepanway Ghosal",
        "Dushyant Akhtar",
        "Soujanya Singh Chauhan",
        "Poria"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/d18-1382"
    },
    {
      "citation_id": "17",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "18",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "19",
      "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
      "authors": [
        "Michael Gutmann",
        "Aapo Hyvärinen"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort"
    },
    {
      "citation_id": "20",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.723"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling"
    },
    {
      "citation_id": "22",
      "title": "Tree-structured semisupervised contrastive pre-training for task-oriented dialog understanding",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria",
        "; Acm",
        "Yinpei He",
        "Binyuan Dai",
        "Min Hui",
        "Zheng Yang",
        "Jianbo Cao",
        "Fei Dong",
        "Luo Huang",
        "Yongbin Si",
        "Li"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413678"
    },
    {
      "citation_id": "23",
      "title": "Space-3: Unified dialog model pre-training for task-oriented dialog understanding and generation",
      "authors": [
        "Wanwei He",
        "Yinpei Dai",
        "Min Yang",
        "Jian Sun",
        "Fei Huang",
        "Luo Si",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "24",
      "title": "2022c. Space: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection",
      "authors": [
        "Wanwei He",
        "Yinpei Dai",
        "Yinhe Zheng",
        "Yuchuan Wu",
        "Zheng Cao",
        "Dermot Liu",
        "Peng Jiang",
        "Min Yang",
        "Fei Huang",
        "Luo Si"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Parameter-efficient transfer learning for NLP",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "26",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9747397"
    },
    {
      "citation_id": "27",
      "title": "2021a. Bidirectional hierarchical attention networks based on document-level context for emotion cause extraction",
      "authors": [
        "Guimin Hu",
        "Guangming Lu",
        "Yi Zhao"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.51"
    },
    {
      "citation_id": "28",
      "title": "2021b. FSS-GCN: A graph convolutional networks with fusion of semantic and structure for emotion cause analysis",
      "authors": [
        "Guimin Hu",
        "Guangming Lu",
        "Yi Zhao"
      ],
      "venue": "Knowl. Based Syst",
      "doi": "10.1016/j.knosys.2020.106584"
    },
    {
      "citation_id": "29",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "30",
      "title": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain"
      ],
      "year": "2022",
      "venue": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "doi": "10.48550/arXiv.2205.02455"
    },
    {
      "citation_id": "31",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2021",
      "venue": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation"
    },
    {
      "citation_id": "33",
      "title": "2021a. Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.104"
    },
    {
      "citation_id": "34",
      "title": "2021b. Contrast and generation make BART a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "venue": "2021b. Contrast and generation make BART a good dialogue emotion recognizer"
    },
    {
      "citation_id": "35",
      "title": "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions",
      "arxiv": "arXiv:2209.03430"
    },
    {
      "citation_id": "36",
      "title": "Duplex conversation: Towards human-like interaction in spoken dialogue system",
      "authors": [
        "Yuchuan Ting-En Lin",
        "Fei Wu",
        "Luo Huang",
        "Jian Si",
        "Yongbin Sun",
        "Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "37",
      "title": "2019a. Deep unknown intent detection with margin loss",
      "authors": [
        "Ting-En Lin",
        "Hua Xu"
      ],
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "2019b. A post-processing method for detecting unknown intent of dialogue system via pre-trained deep neural network classifier",
      "authors": [
        "Ting-En Lin",
        "Hua Xu"
      ],
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "39",
      "title": "Discovering new intents via constrained deep adaptive clustering with cluster refinement",
      "authors": [
        "Hua Ting-En Lin",
        "Hanlei Xu",
        "Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of AAAI"
    },
    {
      "citation_id": "40",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne",
      "doi": "10.18653/v1/P18-1209"
    },
    {
      "citation_id": "41",
      "title": "Scalevlad: Improving multimodal sentiment analysis via multiscale fusion of locally descriptors",
      "authors": [
        "Huaishao Luo",
        "Lei Ji",
        "Yanyong Huang",
        "Bin Wang",
        "Shenggong Ji",
        "Tianrui Li"
      ],
      "year": "2021",
      "venue": "Scalevlad: Improving multimodal sentiment analysis via multiscale fusion of locally descriptors"
    },
    {
      "citation_id": "42",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "M-sena: An integrated platform for multimodal sentiment analysis",
      "authors": [
        "Huisheng Mao",
        "Ziqi Yuan",
        "Hua Xu",
        "Wenmeng Yu",
        "Yihe Liu",
        "Kai Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "44",
      "title": "Dialoguetrm: Exploring multimodal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.229"
    },
    {
      "citation_id": "45",
      "title": "Towards multimodal sentiment analysis: harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th International Conference on Multimodal Interfaces, ICMI 2011",
      "doi": "10.1145/2070481.2070509"
    },
    {
      "citation_id": "46",
      "title": "Are they different? affect, feeling, emotion, sentiment, and opinion detection in text",
      "authors": [
        "Myriam Munezero",
        "Calkin Suero Montero",
        "Erkki Sutinen",
        "John Pajunen"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2014.2317187"
    },
    {
      "citation_id": "47",
      "title": "A clinical study of sentiments (i & ii)",
      "authors": [
        "Henry Alexander",
        "Christiana Morgan"
      ],
      "year": "1945",
      "venue": "A clinical study of sentiments (i & ii)"
    },
    {
      "citation_id": "48",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML 2011"
    },
    {
      "citation_id": "49",
      "title": "Deep contextualized word representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans",
      "doi": "10.18653/v1/n18-1202"
    },
    {
      "citation_id": "50",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining, ICDM 2017",
      "doi": "10.1109/ICDM.2017.134"
    },
    {
      "citation_id": "51",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "52",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "53",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Md Wasifur Rahman",
        "Sangwu Hasan",
        "Amirali Lee",
        "Chengfeng Bagher Zadeh",
        "Louis-Philippe Mao",
        "Mohammed Morency",
        "Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
      "doi": "10.18653/v1/2020.acl-main.214"
    },
    {
      "citation_id": "54",
      "title": "Emotions, sentiments, and performance expectations",
      "authors": [
        "K Robert",
        "Shelly"
      ],
      "year": "2004",
      "venue": "Theory and research on human emotions"
    },
    {
      "citation_id": "55",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "56",
      "title": "Emotions and sentiments",
      "authors": [
        "Jan Stets"
      ],
      "year": "2006",
      "venue": "Handbook of social psychology"
    },
    {
      "citation_id": "57",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": "10.18653/v1/2021.findings-emnlp.252"
    },
    {
      "citation_id": "58",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "59",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
      "doi": "10.18653/v1/p19-1656"
    },
    {
      "citation_id": "60",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations"
    },
    {
      "citation_id": "61",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "62",
      "title": "2021a. Vlmo: Unified vision-language pretraining with mixture-of-modality-experts",
      "authors": [
        "Wenhui Wang",
        "Hangbo Bao",
        "Li Dong",
        "Furu Wei"
      ],
      "venue": "2021a. Vlmo: Unified vision-language pretraining with mixture-of-modality-experts"
    },
    {
      "citation_id": "63",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33017216"
    },
    {
      "citation_id": "64",
      "title": "2021b. Unire: A unified label space for entity relation extraction",
      "authors": [
        "Yijun Wang",
        "Changzhi Sun",
        "Yuanbin Wu",
        "Hao Zhou",
        "Lei Li",
        "Junchi Yan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.19"
    },
    {
      "citation_id": "65",
      "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "authors": [
        "Tianbao Xie",
        "Chen Wu",
        "Peng Shi",
        "Ruiqi Zhong",
        "Torsten Scholak",
        "Michihiro Yasunaga",
        "Chien-Sheng Wu",
        "Ming Zhong",
        "Pengcheng Yin",
        "I Sida",
        "Victor Wang",
        "Bailin Zhong",
        "Chengzu Wang",
        "Connor Li",
        "Ansong Boyle",
        "Ziyu Ni",
        "Dragomir Yao",
        "Caiming Radev",
        "Lingpeng Xiong",
        "Rui Kong",
        "Noah Zhang",
        "Luke Smith",
        "Tao Zettlemoyer",
        "Yu"
      ],
      "year": "2022",
      "venue": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models"
    },
    {
      "citation_id": "66",
      "title": "2021a. A unified generative framework for aspect-based sentiment analysis",
      "authors": [
        "Hang Yan",
        "Junqi Dai",
        "Tuo Ji",
        "Xipeng Qiu",
        "Zheng Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.188"
    },
    {
      "citation_id": "67",
      "title": "2021b. A unified generative framework for various NER subtasks",
      "authors": [
        "Hang Yan",
        "Tao Gui",
        "Junqi Dai",
        "Qipeng Guo",
        "Zheng Zhang",
        "Xipeng Qiu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.451"
    },
    {
      "citation_id": "68",
      "title": "MTAG: modaltemporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": "10.18653/v1/2021.naacl-main.79"
    },
    {
      "citation_id": "69",
      "title": "2021a. Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "70",
      "title": "2021b. Learning modality-specific representations with selfsupervised multi-task learning for multimodal sen-timent analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "71",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "72",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/d17-1115"
    },
    {
      "citation_id": "73",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "74",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2016.94"
    },
    {
      "citation_id": "75",
      "title": "2021a. Deep open intent classification with adaptive decision boundary",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Ting-En Lin"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "76",
      "title": "2021b. Discovering new intents with deep aligned clustering",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Rui Ting-En Lin",
        "Lyu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "77",
      "title": "Mintrec: A new dataset for multimodal intent recognition",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Xin Wang",
        "Qianrui Zhou",
        "Shaojie Zhao",
        "Jiayan Teng"
      ],
      "year": "2022",
      "venue": "Mintrec: A new dataset for multimodal intent recognition",
      "arxiv": "arXiv:2209.04355"
    },
    {
      "citation_id": "78",
      "title": "2022b. A slot is not built in one utterance: Spoken language dialogs with sub-slots",
      "authors": [
        "Sai Zhang",
        "Yuwei Hu",
        "Yuchuan Wu",
        "Jiaman Wu",
        "Yongbin Li",
        "Jian Sun",
        "Caixia Yuan",
        "Xiaojie Wang"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "79",
      "title": "2021c. Unims: A unified framework for multimodal summarization with knowledge distillation",
      "authors": [
        "Zhengkun Zhang",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Xin Jiang",
        "Qun Liu",
        "Zhenglu Yang"
      ],
      "venue": "2021c. Unims: A unified framework for multimodal summarization with knowledge distillation"
    },
    {
      "citation_id": "80",
      "title": "2022c. Unims: A unified framework for multimodal summarization with knowledge distillation",
      "authors": [
        "Zhengkun Zhang",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Xin Jiang",
        "Qun Liu",
        "Zhenglu Yang"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event"
    },
    {
      "citation_id": "81",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}