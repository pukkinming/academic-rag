{
  "paper_id": "2409.18339v2",
  "title": "Aer-Llm: Ambiguity-Aware Emotion Recognition Leveraging Large Language Models",
  "published": "2024-09-26T23:25:21Z",
  "authors": [
    "Xin Hong",
    "Yuan Gong",
    "Vidhyasaharan Sethu",
    "Ting Dang"
  ],
  "keywords": [
    "emotion recognition",
    "ambiguous emotion",
    "large language models",
    "prompt design",
    "multimodal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advancements in Large Language Models (LLMs) have demonstrated great success in many Natural Language Processing (NLP) tasks. In addition to their cognitive intelligence, exploring their capabilities in emotional intelligence is also crucial, as it enables more natural and empathetic conversational AI. Recent studies have shown LLMs' capability in recognizing emotions, but they often focus on single emotion labels and overlook the complex and ambiguous nature of human emotions. This study is the first to address this gap by exploring the potential of LLMs in recognizing ambiguous emotions, leveraging their strong generalization capabilities and in-context learning. We design zero-shot and few-shot prompting and incorporate past dialogue as context information for ambiguous emotion recognition. Experiments conducted using three datasets indicate significant potential for LLMs in recognizing ambiguous emotions, and highlight the substantial benefits of including context information. Furthermore, our findings indicate that LLMs demonstrate a high degree of effectiveness in recognizing less ambiguous emotions and exhibit potential for identifying more ambiguous emotions, paralleling human perceptual capabilities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recent advancements in large language models (LLMs)  [1]  have shown remarkable abilities in comprehending, interpreting, and generating human-like text. This cognitive intelligence facilitates effective human-AI interactions of conversational AI. Equally important is the emotional and social intelligence, which enables it to understand human emotions and adapt its communication accordingly.\n\nText-based emotion recognition has shown considerable potential, with various feature engineering techniques and advanced deep learning models  [2] -  [4] . With the emerging capabilities of LLMs, particularly their proficiency in in-context learning and robust generalization without extensive training, research on exploring their potential for emotion recognition and further use as annotation tools have attracted increasing attentions  [5] -  [8] . Nonetheless, these investigations predominantly focus on recognizing single emotions, thereby overlooking the complex nature of human emotions. Typically, a single emotion label is obtained from the majority vote of multiple annotators labeling the same stimuli. This approach disregards discrepancies among annotators, which indicates the inherent ambiguity of emotions. High ambiguity, i.e., high disagreement, indicates more complex emotions, and such ambiguity impacts conversations, leading to modified communication strategies and affecting † Yuan Gong completed this work at MIT and is now with xAI Corp. relationship dynamics  [9] . For example, the listener might use more cautious language and tones or avoid sensitive topics to prevent misunderstandings if high emotional ambiguity is perceived. Future LLMs need to understand the complexity of emotions, recognize emotional ambiguity, and adjust their responses dynamically.\n\nLLMs, trained on diverse and large-scale datasets, enable semantic richness and offer significant potential in comprehending the complexity of emotions. Additionally, their long-range contextual understanding allows them to decode emotions through in-context learning by analyzing conversational history, which is particularly noteworthy. This study aims to explore the potential of LLMs in recognizing inherently ambiguous emotions and the contributions are summarized below:\n\n• This is the first study to analyze LLMs in recognizing ambiguityaware emotions, demonstrating their potential for human-like emotional intelligence. • We proposed zero-shot and few-shot prompt designs and incorporated in-context learning capabilities to enhance recognition, demonstrating an average 35% relative improvements in terms of Bhattacharyya coefficient. • We further included speech features as textual prompt to enhance learning, further enhancing the ambiguous emotion recognition. • Further analysis concerning different levels of ambiguity revealed that LLMs are more effective at recognizing less ambiguous emotions and less effective with more ambiguous ones.\n\nThis investigation offers valuable insights into emotional intelligence in LLMs and could potentially advance the development of more natural and empathetic conversational AI systems through dynamic emotional responses.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A range of feature sets and modeling frameworks have been developed to advance text-based emotion recognition. TF-IDF, which highlights frequent keywords to indicate emotional cues, has been effectively used in emotion classification  [10] . Subsequently, word embeddings have demonstrated greater effectiveness in capturing semantic relationships, such as Word2Vec, GloVe, and BERT  [2] -  [4] . Deep learning models have also advanced, with Bi-Gated Recurrent Units (GRUs), Convolutional Neural Networks (CNNs)  [4] , and hierarchical Long Short-Term Memory (LSTM) networks  [2]  all showing effectiveness. However, their effectiveness is highly dependent on a large annotated dataset for training  [4] ,  [11] .\n\nRecent advances in LLMs, such as LLAMA3  [12]  and GPT-4  [13] , have opened up new possibilities for text understanding and analysis. Their strong generalization capabilities enable effective text comprehension to recognize emotions without the need for extensive retraining  [5] ,  [6] ,  [8] ,  [14] ,  [15] . One recent study  [6]  compared the emotion labels recognized by GPT-4 to human annotators and found that labels generated by GPT-4 were preferred by human evaluators. Another work proposed InstructERC  [8] , which treats emotion recognition in conversations as a retrieval-based Seq2Seq paradigm utilizing LLMs to infer emotions from conversation history.\n\nDespite the promises, existing studies all focus on the single emotion label obtained from majority vote and have not studied the inherent ambiguity of emotions. Previous studies on non-LLM models accounting for ambiguity in emotions either use the ambiguity in the loss function as an enhancement for the majority vote recognition  [16]  or treat ambiguous emotions as an out-of-distribution (OOD) separate class  [17] . The recent study based on LLMs  [6]  includes data with multi-label classes, but the focus is still on single label recognition, and majority of the data is also single-labeled. None of these studies carefully represent emotions with ambiguity or explore the potential of LLMs in recognizing ambiguity-aware emotions, especially their strong capabilities in in-context learning.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Ambiguous Emotion Recognition Via Llms",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. System Overview",
      "text": "Fig.  1  illustrates our framework for recognizing ambiguous emotions using LLMs. For each target utterance, we construct detailed prompts for ambiguous emotion recognition. To evaluate the performance of the LLMs, we compare the predicted emotion distribution p(x) with the ground truth distribution p(x), which is inferred from N different human annotators A1 to AN .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Prompt Design",
      "text": "1) Zero-shot and few-shot prompting: Carefully designed zeroshot (ZS) and few-shot (FS) prompting are key for LLMs to generalize across various domains  [18] . Zero-shot prompting evaluates the capability of pre-trained knowledge in LLMs. Few-shot prompting includes a limited set of demonstration examples within the prompts, facilitating the adaptation of pre-trained knowledge to specific new tasks. We design zero-shot prompt as outlined in Table I and Eq. (  1 ):\n\nBackground (BG) provides information about the conversation scenario, while Context (C) incorporates the retrospective dialogue Two speakers are talking.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context (M=3)",
      "text": "The conversation is:\n\n• Ses01 F: \"We could hide away.\"\n\n• Ses01 M: \"Run away?\"\n\n• Ses01 F: \"Mm hmm. We'll build a bunker and never come out.\"",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Target Utterance (Tu)",
      "text": "Now Ses01 M says: \"I really don't want to go, I don't want to go...\"",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task",
      "text": "Predict the probability of the emotion of the sentence from the options [neutral, happy, angry, sad], consider the conversation context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Output Constraints (Oc)",
      "text": "Output satisfies the following rules.\n\n• Rule 1: Generate a dictionary of emotion probabilities in format of {'neutral':0.1, 'happy':0.0, 'angry':0.\n\n2) Prompt with speech features: As humans express emotions through multiple cues, we further included speech in addition to text for ambiguity-emotion recognition. We transformed speech features into text format and incorporated them into the prompt design. Since LLMs have been trained extensively on text data, they are expected to understand speech information in text format, e.g., high pitch values: 4. Specifically, we extracted 88-dimensional eGeMAPS features  [19] , a standard acoustic parameter set. It described the speech features of the target utterance in textual format, as illustrated in",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Features",
      "text": "Here are 88 speech features of the current speaker's sentence. The features are: Average Fundamental Frequency in Semitones from 27.5 Hz: 37.039505 ... the full prompts shown in Eq. (  3 ) and (4).\n\nP rompt a f s = BG + C + T U + Speech + T ask + Exps + OR (4)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Context-Aware Recognition",
      "text": "A major advantage of LLMs lies in their capability for in-context learning, offering the potential to analyze long past conversation histories for emotion recognition. Since emotions generally evolve smoothly within dynamic conversations, considering past conversations provides a more comprehensive understanding of the emotional state over time. A longer context window enables LLMs to effectively decode information over extended ranges, and we formally study this by increasing the number M of context windows in the prompt design, i.e., including the corresponding text information from the past M utterances, and evaluated the performance accordingly.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Setup And Results",
      "text": "A. Experimental setup 1) Dataset: Three datasets are used, MSP-Podcast  [20] , IEMO-CAP  [21]  and GoEmotions  [22] . MSP-Podcast contains recordings cover a wide range of subjects in Podcasts. We selected four emotional labels: neutral, angry, happy, and sad. Any utterances annotated by annotators outside of these four categories will be excluded from the analysis, leading to 4114 utterances. We manually reorganized sentences into the original full podcast to enable dynamic information. For IEMOCAP, we also selected the same four emotional labels, resulting in a total of 4370 utterances. Examples in few-shot prompting are from the same session with the target utterance.\n\nGoEmotions is a text-based dataset sourced from Reddit. Given the long-tail distribution of emotion labels, we selected the 4 most common labels (admiration, gratitude, approval, and amusement) along with Neutral. To ensure adequate representation of ambiguous emotions (more than one label per utterance), we applied log inverse frequency weighting and selected 210 instances, with 33% being multi-labeled. 1  2) Models: Gemini-1.5-Flash was chosen as the LLM backbone due to its capability for processing long-range contexts, with a context window of up to one million tokens. The experiments were conducted using the Gemini API  [23] . For few-shot prompts, we tested 5 and 10 examples in GoEmotions. In MSP-Podcast and IEMOCAP, we matched the number of few-shot examples to the context window, varying the context window within [0,30] 2 .\n\n3) Evaluations: We include both uncertainty-centric and accuracycentric metrics to evaluate our model's performance. For uncertaintycentric metrics, we use Jensen-Shannon Divergence (JS), Bhattacharyya coefficient (BC), and R 2 . JS divergence measures the difference between predicted and ground truth distributions, with lower values indicating better predictions. BC measures the similarity between these distributions, and R 2 assesses the goodness-of-fit, with higher values indicating better performance. Additionally, we 1 GoEmotions contains only single utterances without context information. We randomly sampled specific sentences as examples for few-shot prompting. 2 Code link: https://github.com/mHealthUnimelb/AER-LLM. 4) Baseline descriptions: Baseline  [24]  utilizes pretrained embeddings for single emotion recognition, while  [25]  treats ambiguous emotion sentences as additional out-of-distribution class for ambiguous emotion recognition. The other three studies  [5] ,  [6] ,  [8]  focus on LLMs for single emotion recognition tasks, with  [8]  building a two-step system.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Performance On Ambiguity-Aware Prediction",
      "text": "Table IV demonstrates the performance on three datasets. For the text modality, the zero-shot prompt achieves relatively acceptable performance, e.g., R 2 of 0.41 for MSP-Podcast, 0.49 for IEMOCAP, and 0.43 for GoEmotions. With few-shot prompting, it demonstrates significant improvement, with approximately 25% reduction in JS, 49% increase in BC, and 31% increase in R 2 for MSP-Podcast. Similar increasing trend is also observed in IEMOCAP and GoEmotions. This suggests that LLMs are strong in learning from few examples for ambiguous emotion recognition, and leveraging their in-context learning capabilities by looking at past examples is highly beneficial.\n\nFor the joint modeling of text and speech modality, both zeroshot and few-shot demonstrate significant improvement compared to the corresponding performance using text only, suggesting the LLMs' capabilities in recognizing speech information despite being in textual format. Furthermore, the few-shot prompting consistently outperforms the zero-shot prompting, exhibiting superior performance for all three datasets. The consistent trend in ECE also implies a similar effect on probability calibrations, suggesting an improved interpretation of ambiguity. The disparities are stable because LLM typically provide consistent responses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Impact Of Context Window",
      "text": "Fig.  2  shows the few-shot performance using text and speech with the increasing context windows from 0 to 30 in MSP-Podcast. Including context information proves significantly beneficial compared to that without contextual information. When the window size increases from 0 to 5, we observe a 16%, 28%, 22% and 19% improvements in terms of JS, BC, R 2 and ECE, respectively. As the context window expands beyond 10 to 30, the observed improvements become marginal, indicating that further increasing the context window size beyond 10 may not substantially enhance ambiguous emotion recognition. In the IEMOCAP dataset, the most benefit was obtained when the context window increases to 20. These  findings collectively suggest that incorporating context information is crucial for ambiguous emotion recognition in LLMs, and a context window of 10 to 20 is likely to be adequate. This is reasonable, as humans do not need infinite context to recognize emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Performance With Different Ambiguity Levels",
      "text": "To gain deeper insights into how LLMs recognize varying levels of ambiguity in emotions, we evaluated their performance with respect to different ambiguity levels. We used entropy, inferred from the ground truth distributions, as an indicator of the ambiguity levels. As entropy increases, the utterance exhibits greater ambiguity in emotion. An entropy of 0 indicates unanimous agreement on one emotion class. As shown in Fig.  3 , six majority entropy with each more than 100 utterances are shown.\n\nAs entropy increases, the medians (black lines) of JS rise, while BC and R 2 decrease in general, except when entropy is 0.7219. The trend indicates that LLMs are more effective at recognizing less ambiguous emotions. This is likely due to the inherent complexity of predicting a high entropy emotional distribution. This observation aligns with the human difficulty in recognizing more ambiguous emotions  [26] . Better performance is observed with an entropy of 0.7219, as we found that 92% of the utterances are annotated with at least a neutral class, more than in neighbouring entropy groups, and LLMs recognize neutral with a 75% true positive rate, leading to higher performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Prediction On Majority Vote",
      "text": "We further estimated the single emotion from the distribution by selecting the one with the highest probability and compared it with the majority vote. Noted that the prompt design for LLMs is not optimized for recognizing the majority vote, this analysis is designed to provide insights into LLMs capabilities in understanding the most likely emotion. It is not a direct comparison due to the slightly different data and tasks.\n\nIn Table  V , the best performances on MSP-Podcast, IEMOCAP and GoEmotions achieve 56.15%, 59.06% and 51.05% in terms of W-F1, respectively, demonstrating emotional understanding of LLMs. Note  that the prompt is not designed for single-label emotion recognition and LLMs is not trained, but it still achieves comparable performance to the models specifically trained for single-label emotion recognition. The accuracy with respect to the context window and the entropy in MSP-Podcast is further demonstrated in Fig.  4 . Notably, there is no majority label when entropy is 1.5219 as two classes share the same probabilities. Compared to a context window of 0, a context window of 30 achieved higher accuracy across all entropy groups. Additionally, better performance is observed when entropy is 0, indicating no ambiguity, compared to high entropy of 1.371, which corresponds to high ambiguity. We also observe a similar pattern in IEMOCAP, with a more consistent increasing trend as the context window increases and a decreasing trend as the entropy increases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "We investigated LLMs potential in recognizing ambiguous emotions and discovered that it can comprehend such emotions to a certain extent without additional training. Incorporating previous dialogues by leveraging LLMs in-context learning capabilities significantly enhances its emotional intelligence, with a context window of 10 to 20 being adequate. Moreover, these models demonstrate greater proficiency in recognizing less ambiguous emotions compared to highly ambiguous ones, similar to human perception. These findings highlight the potential of LLMs for application in emotional conversational AI.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: System overview. Ai, i ∈[1, N] represents the ith annotator,",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates our framework for recognizing ambiguous emo-",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the few-shot performance using text and speech",
      "page": 3
    },
    {
      "caption": "Figure 2: Performance comparison with increasing context windows",
      "page": 4
    },
    {
      "caption": "Figure 3: Performance comparison among different levels of ambiguity",
      "page": 4
    },
    {
      "caption": "Figure 3: , six majority entropy with each more than 100",
      "page": 4
    },
    {
      "caption": "Figure 4: Comparison of W-F1 across five entropy groups with context",
      "page": 4
    },
    {
      "caption": "Figure 4: Notably, there is",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality Prompt": "ZS\nT\nFS",
          "JS↓ BC↑ R2↑": "0.56 0.39 0.41\n0.42 0.58 0.54"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ZS\nT\nFS": "ZS\nT+S\nFS",
          "0.51 0.46 0.49\n0.37 0.67 0.58": "0.47 0.51 0.51\n0.35 0.69 0.59"
        },
        {
          "ZS\nT\nFS": "ZS\nT\nFS",
          "0.51 0.46 0.49\n0.37 0.67 0.58": "0.49 0.54 0.43\n0.44 0.60 0.48"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality Methods": "ZS\nT\nFS"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Large language modelbased emotional speech annotation using context and acoustic feature for speech emotion recognition",
      "authors": [
        "J Santoso",
        "K Ishizuka",
        "T Hashimoto"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert",
      "authors": [
        "C Huang",
        "A Trabelsi",
        "O Zaïane"
      ],
      "year": "2019",
      "venue": "Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert",
      "arxiv": "arXiv:1904.00132"
    },
    {
      "citation_id": "3",
      "title": "Nelec at semeval-2019 task 3: think twice before going deep",
      "authors": [
        "P Agrawal",
        "A Suri"
      ],
      "year": "2019",
      "venue": "Nelec at semeval-2019 task 3: think twice before going deep",
      "arxiv": "arXiv:1904.03223"
    },
    {
      "citation_id": "4",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "S Bharti",
        "S Varadhaganapathy",
        "R Gupta",
        "P Shukla",
        "M Bouye",
        "S Hingaa",
        "A Mahmoud"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "M Niu",
        "M Jaiswal",
        "E Provost"
      ],
      "year": "2024",
      "venue": "From text to emotion: Unveiling the emotion annotation capabilities of llms"
    },
    {
      "citation_id": "7",
      "title": "Is gpt a computational of emotion?",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multitask llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multitask llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "9",
      "title": "Relational uncertainty and interpersonal communication",
      "authors": [
        "L Knobloch"
      ],
      "year": "2010",
      "venue": "New directions in interpersonal communication research"
    },
    {
      "citation_id": "10",
      "title": "Performance comparison of tf-idf and word2vec models for emotion text classification",
      "authors": [
        "D Cahyani",
        "I Patasik"
      ],
      "year": "2021",
      "venue": "Bulletin of Electrical Engineering and Informatics"
    },
    {
      "citation_id": "11",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "12",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "13",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "14",
      "title": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "authors": [
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Ckerc: Joint large language models with commonsense knowledge for emotion recognition in conversation",
      "arxiv": "arXiv:2403.07260"
    },
    {
      "citation_id": "15",
      "title": "Exploring large-scale language models to evaluate eeg-based multimodal data for mental health",
      "authors": [
        "Y Hu",
        "S Zhang",
        "T Dang",
        "H Jia",
        "F Salim",
        "W Hu",
        "A Quigley"
      ],
      "year": "2024",
      "venue": "WellComp co-located with UbiComp 2024"
    },
    {
      "citation_id": "16",
      "title": "Multi-classifier interactive learning for ambiguous speech emotion recognition",
      "authors": [
        "Y Zhou",
        "X Liang",
        "Y Gu",
        "Y Yin",
        "L Yao"
      ],
      "year": "2022",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "17",
      "title": "Handling ambiguity in emotion: From out-of-domain detection to distribution estimation",
      "authors": [
        "W Wu",
        "B Li",
        "C Zhang",
        "C.-C Chiu",
        "Q Li",
        "J Bai",
        "T Sainath",
        "P Woodland"
      ],
      "year": "2024",
      "venue": "Handling ambiguity in emotion: From out-of-domain detection to distribution estimation",
      "arxiv": "arXiv:2402.12862"
    },
    {
      "citation_id": "18",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "Y Wu",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai",
        "A Hauth"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "19",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "20",
      "title": "The mspconversation corpus",
      "authors": [
        "L Martinez-Lucas",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2020",
      "venue": "The mspconversation corpus"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Goemotions: A dataset of fine-grained emotions",
      "arxiv": "arXiv:2005.00547"
    },
    {
      "citation_id": "23",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "M Reid",
        "N Savinov",
        "D Teplyashin",
        "D Lepikhin",
        "T Lillicrap",
        "J -B. Alayrac",
        "R Soricut",
        "A Lazaridou",
        "O Firat",
        "J Schrittwieser"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "24",
      "title": "You're not you when you're angry: Robust emotion features emerge by recognizing speakers",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Distribution-based emotion recognition in conversation",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "26",
      "title": "Perceiving facial affective ambiguity: A behavioral and neural comparison of adolescents and adults",
      "authors": [
        "T.-H Lee",
        "M Perino",
        "N Mcelwain",
        "E Telzer"
      ],
      "year": "2020",
      "venue": "Emotion"
    }
  ]
}