{
  "paper_id": "2407.01562v1",
  "title": "Small But Fair! Fairness For Multimodal Human-Human And Robot-Human Mental Wellbeing Coaching",
  "published": "2024-05-15T10:29:05Z",
  "authors": [
    "Jiaee Cheong",
    "Micol Spitale",
    "Hatice Gunes"
  ],
  "keywords": [
    "small dataset",
    "fairness",
    "multimodal",
    "well-being coaching",
    "human-robot interaction * equal contribution",
    "alphabetical order"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, the affective computing (AC) and human-robot interaction (HRI) research communities have put fairness at the centre of their research agenda. However, none of the existing work has addressed the problem of machine learning (ML) bias in HRI settings. In addition, many of the current datasets for AC and HRI are 'small', making ML bias and debias analysis challenging. This paper presents the first work to explore ML bias analysis and mitigation of three small multimodal datasets collected within both a human-human and robot-human wellbeing coaching settings. The contributions of this work includes: i) being the first to explore the problem of ML bias and fairness within HRI settings; and ii) providing a multimodal analysis evaluated via modelling performance and fairness metrics across both high and low-level features and proposing a simple and effective data augmentation strategy (MixFeat) to debias the small datasets presented within this paper; and iii) conducting extensive experimentation and analyses to reveal ML fairness insights unique to AC and HRI research in order to distill a set of recommendations to aid AC and HRI researchers to be more engaged with fairness-aware ML-based research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "In recent years, the advancement in machine learning (ML), the availability of large-scale datasets and the enhancement in computing have led to the widespread use of machinelearning prediction systems in our society  [1] . However, the problem of bias in machine-learning based tools and systems are becoming an increasing source of concern  [2] . Such risks are also present in the field of affective computing as affect recognition tools are increasingly deployed in a wide range of high-stake use-cases such as mental wellbeing prediction  [3]  and robotic mental wellbeing coaching  [4, 5] . The problem of bias is also becoming an increasingly greater source of concern within the human-robot interaction (HRI) research community  [6] . Some of the fairness related concerns highlighted include fairness within a HRI teamwork context  [7] , robot navigation  [8]  as well as HRI ethics and robot design  [9] . However, this relatively nascent field has yet to consider the fairness-related challenges that occur due to the bias present in ML algorithms deployed within a HRI setting.\n\nA wide range of fairness measures and bias mitigation techniques have been proposed to quantify and mitigate the bias present in machine learning models  [10, 11] . As existing approaches chiefly focus on large datasets, they may not be effective for small datasets. However, most of the datasets currently available for affective computing (AC) application scenarios and within human-robot interaction (HRI) contexts are small, i.e., containing just a few hundred instances of data  [12, 7] . Figure  1  considers papers that have been published within the last three editions of the IEEE International Conference on Affective Computing & Intelligent Interaction (ACII) and the ACM/IEEE international conference on Human-Robot Interaction (HRI) respectively. Both figures illustrate that papers focusing on small datasets typically represent 40% to 60% of the total papers accepted for presentation at the main conference track. Based on this, we consider any dataset that has less than 40 (median) subjects or 500 (median) samples 'small'. We excluded papers that used large benchmark datasets such as AffectNet. Given the above, bias mitigation strategies developed within the mainstream ML fairness literature for large datasets may not work well on AC and HRI datasets. Given that sensitive use cases such as wellbeing coaching  [13]  and children's wellbeing assessment  [14]  are increasingly explored within both the AC and HRI research community, there is an urgent need to develop bias mitigation methodologies that will work for small AC and HRI datasets.\n\nWithin our ACII 2023 paper  [15] , we highlighted the ACII community's attempt to be more ethically oriented as exemplified by the mandatory ethics impact statement to guard against the potential risks and harms that could be perpetuated by affect-related technology  1  . We hypothesised and showed that bias exists even for small datasets. Our experiments demonstrated that high-level features were often more informative and more reliable than low-level features and that a multimodal approach is often better than a unimodal approach across both performance and fairness metrics. We also proposed a simple, yet effective method that is able to mitigate against the bias present. Our intention is to contend that every analysis on small datasets should have a bias analysis section. In this work, within wellbeing context, we widen the impact of our previous work by extending the experiments towards small datasets in human-robot interaction scenarios.\n\nWe do so by introducing the first comprehensive work which explores the problem of bias in a small dataset within a highstake and sensitive use of wellbeing coaching both within a dyadic human-human and robot-human mental wellbeing coaching small dataset setup. We investigate different data augmentation approaches to debias three small temporal multimodal mental wellbeing datasets. We further investigate the contribution of each individual modality (i.e., face, audio, verbal) and the importance of high and low-level features for data-driven applications. The main contributions of this paper are as follows. First, we conduct the first ML bias and fairness analysis in a HRI context by extending our humanhuman interaction (HHI) work in ACII'23  [15]  to the HRI datasets in this paper. Second, till now, the two HRI datasets have never been explored for ML-based wellbeing prediction. Third, we provide a thorough multimodal analysis and a feature importance analysis evaluated using both performance and fairness metrics across three different wellbeing coaching datasets. Fourth, we experiment with different data augmentation strategies to reduce the bias in small dataset experimental settings. We compare our proposed method against a baseline data augmentation approach across both single and multiple modalities. And last, we distill insights regarding bias and fairness within a small dataset setup and provide guidelines to assist affective computing and human-robot interaction researchers in future small dataset ML studies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Robotic Mental Wellbeing Coaching",
      "text": "Only a small number of studies have explored the use of robotic coaches to support mental wellbeing  [16, 5, 17] . For example, Jeong et al.  [16]  conducted a longitudinal study where Jibo robots provided positive psychology interventions to students in home settings over 7 days. The participants reported experiencing improved wellbeing, mood, and readiness to change, as well as developing an affinity for the robot. Shi et al.  [18]  investigated how physical embodiment and personalization affect the perceived quality of text-tospeech (TTS) voices used for mindfulness exercises. They found that user-personalized TTS voices performed nearly as well as human voices, suggesting personalization can enhance the user's perception of TTS voice quality. Spitale et al.  [19]  conducted a longitudinal study in which employees of a tech company interact with two different forms of robotic coaches that delivered positive psychology exercises over 4 weeks. Their results indicated that the specific form of the robotic coach may impact the users' perceptions and experiences. Jeong et al.  [20]  explored how the robot's role (assistant, coach, or companion) affected the therapeutic alliance during wellbeing practice. They found that the companion robot was the most effective in building a positive relationship with users. Axelsson et al.  [17]  deployed a robotic mindfulness coach at a public cafe, where participants could join robot-led meditation sessions in a group setting. Their results suggest that participants perceived the robot-led mindfulness practice to be more helpful over time. Spitale et al  [5]  proposed a novel LLM-based multimodal system, namely VITA, that allows robotic coaches to autonomously adapt to the coachee's multi-modal behaviours (facial valence and speech duration) and deliver coaching exercises. Overall, this emerging body of research suggests that robotic coaches have potential to support mental wellbeing, but none of these previous studies have explored the bias and fairness of computational models embedded in robotic wellbeing coaches.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Fairness In Mental Wellbeing",
      "text": "Though recent attempts at applying ML for the investigation and understanding of mental health has been promising  [21] , there is only a handful of studies which have looked into bias in mental wellbeing prediction  [22, 23, 24, 25] . Zanna et al.  [24]  conducted their experiments on data collected in the wild with a specific focus on anxiety prediction. Ryan et al.  [22]  proposed three categories of fairness definitions they deem relevant to mental health. Park et al.  [26]  analysed bias across gender in mobile mental health assessment and proposed an algorithmic impact remover to mitigate unwanted bias. Bailey and Plumbley  [23]  attempted to mitigate the gender bias present in the DAIC-WOZ dataset using data re-distribution.  [25]  examined whether bias exists in existing mental health datasets and algorithms and provided practical suggestions to avoid hampering bias mitigation efforts in ML for mental health. However, all of the existing works consist of relatively large datasets (more than 500 samples or more than 40 subjects) which differ from our small dataset setup. In addition, no investigation has specifically looked into the problem of bias in the context of a human-human and robothuman mental wellbeing coaching.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Ml Bias And Fairness In Hri",
      "text": "There is minimal work examining ML bias in the context of HRI. Most of the existing work on bias and fairness in HRI relate to fairness-related considerations within a HRI setting  [27, 9, 28]  rather than the fairness-related challenges that occur due to the employed ML prediction algorithms.  [27]  presented a survey on fairness in robot learning.  [29]  conducted experiments to investigate the impact that a robot's \"skin colour\" can have on human perceptions of the robot's behaviour.  [9]  explored the idea of fairness via the concept of resource allocation.  [7]  investigated fairness within the context of human-robot teaming where fairness is quantified in terms of each member's contribution.  [30]  investigated the effects of a gender-biased robot and its effect on humans' implicit gender stereotypes.  [31]  evaluated how security mitigation can result in unfairness in human-robot interaction.  [32]  outlined the implications that a robot's design has on the a human's bias to interact socially with the robots.  [33]  investigated the human biases present within human-robot versus humanhuman trust interactions settings.  [34]  introduced an optimisation approach for fairer subtask allocation.  [35]  analysed the human automation bias that arose when participants believe a robot's false judgment. However, none of the existing works have investigated the problem of ML bias and fairness within HRI context. Our work is the first study investigating bias and fairness in ML for HRI.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "D. Data Augmentation For Bias Mitigation",
      "text": "Bias can be mitigated at the pre-processing, in-processing or post-processing stage  [11] . The proposed method falls under the pre-processing data augmentation category which has proven to be effective in mitigating bias  [36] . There is minimal work that focus on mitigating bias for a small dataset setup  [37] . For a small dataset problem,  [37]  leverages on a small annotated dataset to debias a larger dataset. This is distinct from our work as it focuses specifically on an item recommendation system. Existing research has indicated that re-sampling outperforms reweighting for correcting sampling bias  [38] . Given the above, we propose a simple re-sampling or data augmentation method based on the mixup method proposed in  [39] . Mixup has proven to be a simple yet highly effective method to address challenges ranging from robustness  [40] , fairness  [41]  and regularisation  [42] . As a result, Mixup has been frequently used as a benchmark for new data augementation techniques and there are recent works proposing new variations of the original method  [41] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Problem Formulation",
      "text": "We study the problem of model fairness using a machine learning approach, where the goal is to predict a correct outcome y i ∈ Y from input x i ∈ X based on the available dataset D for individual i ∈ I. In our setup, y i ∈ Y is thus the outcome where Y = 1 denotes \"high-PA\" (i.e., high positive affect, indicative of higher levels of mental wellbeing) whereas Y = 0 denotes \"low-PA\" (i.e., low positive affect, indicative of lower levels of mental wellbeing). The fairness measure of a model M is then evaluated according to the sensitive groups of individuals defined by their sensitive attributes A gender and race in this work. In our experiments, both sensitive attributes analysed are binary. They belong to the majority group, e.g.: A race = 1 if they are White or A race = 0 if otherwise. Ŷ denotes the predicted class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Fairness Measures",
      "text": "The fairness measures are similar to that in  [43]  and  [24] .\n\n• Equal Accuracy (EA), a group-based metric, is used to compare the group fairness between the models. This can be understood as the accuracy gap between the majority and the minority group:\n\nwhere M AE represents the Mean Absolute Error (MAE) of the classification task of each sensitive group.\n\nHuman Participant Human Well-being Coach Fig.  2 . AFAR-BSFT Dataset: interaction between the participants (on the left side), and the human wellbeing coach (on the right side).\n\n• Disparate Impact (DI), measures the ratio of positive outcome ( Ŷ = 1) for both the majority and minority group as represented by the following equation:\n\nThe two measures above represent different aspects of bias.\n\nEA evaluates fairness based on the model's predictive performance measured in terms of accuracy. whereas DI evaluates fairness based purely on the predicted outcomes Ŷ .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Proposed Method: Mixfeat",
      "text": "Our proposed methodology (MixFeat) is based on the data augmentation technique proposed by  [39] . Given a dataset of size N where A represents the audio cue, F represents the facial cue and V represents the verbal cue, the new training sample (A k , F k , V k ) is therefore generated as follow:\n\nwhere i, j ∈ {1, ...N }, i ̸ = j and λ A , λ F , λ V ∼ Beta(0,1). We use the above method to generate synthetic samples for the minority group to obtain balanced samples across the sensitive attributes of race and gender. The intuition behind this method is that if we generate new samples by mixing up features from other samples with the same sensitive attribute, the new samples will inherit the sensitive-attribute specific features. Thus, this method preserves the relation between the synthetic samples and supervision signal which gives the algorithm more samples to learn from without imposing strong assumptions  [39] . Figure  5  outlines the experimental setup and how the method is integrated into the overall classification pipeline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Datasets And Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Datasets",
      "text": "1) The AFAR BSFT Dataset: We collected a dataset of human-human dyadic interactions between a human wellbeing coach and 11 participants over four weeks. The human wellbeing coach was instructed to deliver a Brief-Solution Focused Therapy (BSFT) style coaching, asking participants to focus on solutions rather than analysing the problem  [44]  for about 20 minutes. After each session, we asked participants to complete the Positive And Negative Affect Scale (PANAS)  [45]  to evaluate their positive and negative affect. a) Data Collection: 11 participants were recruited via email advertising of the University of Cambridge. We conducted the study in a dedicated room (see Figure  5 ) where a human wellbeing coach and one participant were seated in front of each other. Video recordings were done using two external cameras, one facing the participant and the other facing the human coach that can be used for further analysis (beyond the scope of this paper) on dyadic interactions during the coaching practice. We collected 44 videos (11 participants × 4 weeks, 20 mins per session) of dyadic wellbeing coaching interactions. 3 out of 44 sessions were excluded due to technical issues (e.g., corrupted video or audio recordings).\n\nb) Sensitive Groups: Two human annotators labelled the gender and race of the participants (with a 100% agreement). This resulted in 7 participants being labelled as males and 4 as females, and 8 participants being labelled as Whites, and 3 as non-Whites.\n\n2) The AFAR Robocoaching 2022 Dataset (AFAR-RC22): In 2022, we collected a dataset of human-robot interactions between a robotic mental wellbeing coach and 26 participants over four weeks in a tech company (Cambridge Consultants Inc.)  [19] . The robotic wellbeing coach delivered four positive psychology exercises once a week -savouring, gratitude, accomplishments, and optimism about the future -that lasted around 10 minutes each. As we did for the AFAR BSFT dataset, among other measures, we asked participants to fill out the PANAS questionnaire after each interaction session with the robotic coach. The robotic coach was pre-programmed to conduct the positive psychology practice following predefined steps regardless the employees speech. For example, when the robotic coach asked the employee to share what they have been grateful for during the last week, the robot asked the same follow up question to all employees without adapting the coaching to what has been said by the employees.\n\na) Data Collection: Cambridge Consultants Inc advertised the study via their communication channels and the participation was on voluntary-basis. 26 participants that took part were healthy employees of the company. Please refer to our paper  [19]  for more information about this study and the recruitment and screening process. Employees interacted once a week with a robotic coach that delivered positive psychology exercises over four weeks. This was a betweensubject study in which employees were randomly assigned to interact either with a QT robot (humanoid-like appearance) or a Misty II robot (toy-like appearance), as shown in Figure 3. Video recordings were done using an external camera that captured the employees' behaviours during the robotic wellbeing coaching  [46] . We collected a total of 104 videos (26 participants X 4 weeks, 10 mins each) of robotic wellbeing coaching sessions. 4 of them were excluded due to technical issues (e.g., corrupted audio-visual recordings).\n\nb) Sensitive Groups: Participants self-reported their gender before the study: 6 participants self-reported as female, 1 as non-binary person, and 19 as males. While for race, two human annotators labelled the race of participants (with again 100% agreement), that resulted in 21 participants labeled as Whites, and 5 participants as non-Whites.\n\n3) The AFAR Robcoaching 2023 Dataset (AFAR-RC23): We collected the second dataset on longitudinal robot coaching in 2023 as reported in  [47, 5]  in the same tech company. This dataset collated data recorded into two study. The first study reported in  [47]  involved 12 participants -who had already interacted with a robotic coach in  [19] . The second study involved 17 new participants who had never interacted with a robotic coach before.\n\na) Data Collection: A total of 29 participants were involved in the two studies with the same recruitment process reported in Section IV-A2. Please refer to the following papers for more details about the two studies  [47, 5] . Employees interacted once a week with the QT robot (the robotic platform chosen for these two studies, see Figure  4 ) that delivered four positive psychology exercises over four weeks -savouring, gratitude, accomplishments, and one door closes one door opens. Audio-visual data were collected via an external camera that captured the face and body of the employees interacting the the robotic wellbeing coach. We collected a total of 116 videos (29 participants x 4 week, 10 mins for each session). 1 of them was excluded due to technical issues.\n\nb) Sensitive Groups: Participants self-reported their gender: (study 1) 3 females, 1 non-binary, and 8 males; and (study 2) 7 females, and 10 males. While for the race, two human annotators have labelled the dataset with a full agreement as follows: (study 1) 3 non-Whites and 9 Whites; (study 2) 1 non-White and 16 Whites.\n\n4) Annotations: We assessed the participants' positive affect using the self-report results of the PANAS questionnaire  [45]  for all three datasets, which has been widely used by practitioners to identify strengths and concerns in mental wellbeing. We computed the positive affect (PA) and negative affect (NA) sub-scales according to the manual in  [45] . We set the threshold value to 33.3, corresponding to the mean value for the American population  [45] , and we then classified the",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Creating A Balanced Dataset",
      "text": "The AFAR-BSFT Video Dataset Fig.  5 . The model pipeline with our proposed data augmentation technique: MixFeat. After extracting the high-level features from the dataset, we generate synthetic sample features using Equation  3 . Each modality's feature generation process is chiefly governed by their respective λ ∼ Beta(0,1) parameters. videos collected into \"high-PA\" and \"low-PA\". This resulted in: (1) AFAR-BSFT DB: 17 videos for the \"low-PA\" and 26 videos for the \"high-PA\" class; (2) AFAR-RC22 DB: 45 videos for the \"low-PA\" and 57 videos for the \"high-PA\" class; and (3) AFAR-RC23 DB: 51 videos for the \"low-PA\" and 64 videos for the \"high-PA\" class. Given the small size of all three datasets, we decided to limit our problem to a binary classification problem.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Self-Report Affect Detection",
      "text": "1) Dataset Pre-processing: Before extracting the features, we split the audio and video recordings of the three datasets. We asked a human annotator to transcribe the dyadic interactions between the human coach and the participants manually for the AFAR-BSFT DB. The annotator also took note of the timestamp of the speech so that we were able to diarize the audio files. For the AFAR-RC22 dataset, we did not transcribe coachee interactions during robotic coaching. However, for the AFAR-RC23 dataset, we automatically transcribed the coachees' dialogue during interactions with the robotic coach, enabling adaptive conversation responses.\n\n2) Multi-modal Feature Extraction: We extracted the facial features using OpenFace 2.0  [48]  -which represents one of the state-of-the-art tools for extracting facial features within the ACII community, e.g., in  [12]  -resulting in the following: eye gaze directions, the intensity and presence of 17 facial action units (FAUs), facial landmarks, head pose coordinates, and point-distribution model (PDM) parameters for facial location, scale, rotation and deformation, resulting in 709 facial features. We adopted this facial features extraction strategy for all three datasets. We used librosa 2  to extract the audio features for the AFAR-BSFT dataset, namely pitch, speech duration, 128 Mel spectrograms, 20 MFCC, 20 delta MFCC, spectral centroid, and RMS, which results in 172 audio features, as in previous works, e.g.,  [49] . While for AFAR-RC22 and AFAR-RC23 DBs, we used openSMILE 3  to extract audio features using the GeMaPs method, that includes e.g., loudness, alpha ratio, hammarberg index, slop, spectral flux and MFCC, that resulted in a total of 25 features, because we have already extracted such features for our previous works  [19, 5, 47] . We used ROBERTa  4  to extract the predicted sentiment for all three datasets from the participants' transcriptions resulting in 2 verbal features (label and probability), as in  [50] .\n\n3) Pre-processing: We first removed constant and null features to prepare the multi-modal features for the machine learning models. Then, we decided to condense the temporal information of each video clip into statistical descriptors as in  [12, 50] , computing a fixed-length vector for each multimodal feature of each clip that consists of mean, median, standard deviation, minimum, maximum, and auto-correlation with 1-second lag, resulting in: (  1   4) Feature Selection: We defined the high-level and lowlevel features as interpretable (e.g., facial action unit, pitch) and not-interpretable (e.g., spectral features) to select the most informative ones for the positive affect detection model  [51] . The low-level features were 1) facial: facial landmarks, head pose coordinates, and point-distribution model (PDM) parameters, and 2) audio: 128 Mel spectrograms, 20 MFCC, 20 delta MFCC, spectral centroid, and RMS for AFAR-BSFT",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5) Data Fusion Strategies:",
      "text": "We explored different state-ofthe-art data fusion strategies  [52]  for all three datasets. We experimented with early fusion, which consisted of concatenating features from different modalities that resulted in a single vector of features, and different late fusion strategies, namely majority voting (soft and hard) and stacking (soft and hard). In majority voting, the final decision is made according to the most frequent class label predicted across the different uni-modal models (hard) or the classifier whose predicted class probability is the highest across the different uni-modal models (soft). In stacking, the final decision is made by another classifier (e.g., logistic regression model) fed by either the predicted class label (hard) or the predicted class probabilities (soft) of each uni-modal model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Modeling And Bias Analysis Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Modeling And Feature Selection",
      "text": "We first conducted experiments using various machine learning techniques as in  [12, 50]  -namely logistic regression, linear support vector machine (SVM), random forest tree, bagging, XGBoost, AdaBoost, decision tree, radial basis function support vector machine (RBF-SVM), multi-layer perceptron (MLP), and long-short term memory (LSTM) neural network -and validating them with three different cross-validation approaches (i.e., 5-fold CV and leave-one-subject-out (LOSO)). Our results showed that the outperforming models were RBF-SVM and MLP among the machine learning techniques we experimented with. Due to space constraints, we only report the bestperforming model results and analyses in the following sections.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Low Vs High Level Feature Analysis 1) Afar-Bsft:",
      "text": "We trained different experimental models with either the high or low-level features, and compared their performances. Table  I  reports the results of the uni-modal models, while Table  I  reports the results of the multi-modal (i.e., face and audio) models. We have not reported the trimodal (i.e., face, audio, and verbal) analysis because the verbal feature vector contains only high-level information, making comparison impossible. Our results showed that the models trained with high-level features performed better in terms of accuracy and F1 in all uni-modal and most of the multi-modal setups (see Tables  I  and I ). Therefore in the rest of our work, we only considered high-level features to train the models and conduct the bias analysis.\n\n2) AFAR-RC22: For the AFAR-RC22 dataset, with reference to Table  II , we see a similar trend where the high-level features are better for the face modality. Across the audio modality, though the low features seem to perform better for F1 scores across both the RBM SVM and MLP methods, the gap in results are minor. Across the multimodal setup, with    reference to Table  II , we see a similar trend with the AFAR-BSFT dataset. For the early fusion strategy, low-level features seem to perform better whereas for the late fusion strategies (soft voting and stacking) high-level features performed better.\n\n3) AFAR-RC23: For the AFAR-RC23 dataset, with reference to Table  III , we see a similar trend where the highlevel features are better for the face modality. Across the audio modality, RBF SVM performed better with low level features and MLP performed better for high level features. Across the verbal modality, both models seem to produce similar results across both high and low level features. Across the multimodal setup, with reference to Table  III  , we see a similar trend with the two other datasets. The key difference is that across early fusion, the RBM SVM performed better using high level features whereas the MLP performed better with low level features. For the late fusion strategies (soft voting and stacking), high-level features performed better just as before.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Uni-Modal Vs Multi-Modal Analysis",
      "text": "We conducted several experiments to compare uni-modal and multi-modal (with either early or late fusion) approaches.   1) AFAR-BSFT: As observed in Tables IV and IV, overall the multi-modal approach outperformed the uni-modal models. Interestingly, the audio modality consistently gives the best accuracy and fairness scores across all three sets of experiments. This could be due to the fact that BSFT coaching is dialogue oriented. Across fairness, not all multimodal approaches led to bias reduction. For example, the MLP-based soft major voting approach seems to reduce gender and race biases with respect to audio or face unimodal approaches, however, early fusion techniques for both MLP and RBF SVM-based approaches increase both gender and race biases. We hypothesise that this may be due to fact that early fusion simply combines features without considering their gender-based associations which caused the underlying bias to be amplified by the respective models 2) AFAR-RC22: We see from Tables V and V that overall, a multimodal approach is often better than a unimodal approach across both performance and fairness. For instance, all of the original multimodal fusion methods (early fusion, soft voting and stacking) gave improvements across accuracy and UAR compared to the best performing unimodal modality and model. Across fairness, stacking seems to produce the best improvement across the EA metrics whereas soft voting seems to produce the best improvement across the DI metrics.\n\n3) AFAR-RC23: We see that previous observations are generally consistent. We see from Tables III and III that a multimodal approach is often better than the best performing unimodal approach across most measures. The best performing accuracy results improved from 0.57 to 0.61 and the best performing UAR results improved from 0.62 to 0.63. Across fairness, the best performing original unimodal EA Gender results improved from 0.04 to 0.02.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Debiasing Approach And Results",
      "text": "To provide a comparison to our proposed method, we use a baseline data balancing method to mitigate the bias present. We employ a similar data balancing method as  [43] . We resample the minority group by randomly oversampling datapoints to obtain an augmented dataset with samples balanced across both sensitive attributes.\n\nThe implementation of our proposed method is similar to that of the baseline method. The key difference is that instead of randomly oversampling data points, we generate synthetic samples according to the method outline in Equation  3 .\n\nWe implemented this data balancing and proposed method similar to all three DBs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Afar-Bsft",
      "text": "After data balancing, we retrain the models and capture the results in Table  IV  and IV .\n\nWith reference to Table  IV , we see that across the unimodal experiments, our method consistently produces a more accurate and fairer outcome across most metrics for both sensitive attributes compared to the baseline. Within the multimodal approach depicted in Table  IV , we see that this gap in predictive and fairness performance is diminished.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Afar-Rc22",
      "text": "Within a unimodal setting, we see from Table V that our proposed method, MixFeat consistently produces the best results acorss all performance measures for both modalities. This is also true within a multimodal setting. Across fairness, our results are also better compared to baseline. For instance, across early fusion, our proposed data augmentation method resulted in the fairest EA Gender score of 0.01 with the RBM SVM classifier and the fairest EA Race score of 0.03 with the",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Research Field",
      "text": "Recommendations Why? How?",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ml Training (R1) Train Human-Centric Models",
      "text": "The problem of bias is often Ensure that participants are balanced across on a balanced dataset associated with data imbalances  [53] . sensitive groups (e.g., gender, race) or perform data balancing methods as needed (e.g., data augmentation  [15] ).\n\nML Features Selection (R2) Use higher level features making The high-level features often include Extract and train the model using use of multimodality less noisy information and make multimodal high-level features easier for the model to learn the by experimenting different fusion representations in small datasets  [54]  strategies  [55]  if more information (i.e., multimodal) is used.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ml Modeling",
      "text": "(R3) Employ a variety of ML models Past works  [55, 14]  showed that Conduct experiments using a variety and evaluation and fairness specific models work better on certain of different models and metrics  [12] . metrics modalities across specific metrics.\n\nAC/Robotic System Design (R4) Balance the trade off between Adaptive and more complex AI-based Adaptive models that need to be embedded AC/robotic system complexity and system may be more difficult in a robot should be tested for their fairness to debias  [56] . fairness in advance  [57] . When this is not applicable, on-the-fly model should have embedded bias mitigation strategies.\n\nAC/Robotic System Design (R5) Define field and The ethical research in robotics studies the Adopt human-centric approach, like value-& Ethics context specific ethical principles consequences of deploying robots in social sensitive design  [58] , to distill main ethical when designing / deploying AC contexts and of interacting with humans. recommendations to use robots in a specific systems for wellbeing\n\nThe study and definition of ethical context  [59]  guidelines can help building fair HRI  [27]  MLP classifier. Within the multi-modal approach in Table  V , we see that our proposed method still consistently produces the best results across all performance metrics. Across fairness, though our results perform better across most fusion methods we see that the baseline methods produces better fairness scores across DI Gender across both the early fusion (0.94) and stacking (0.98) approaches.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Afar-Rc23",
      "text": "We see a consistent trend across the unimodal results in Table  VI . For instance, for the face modality, we achieved an overall accuracy, F1 and UAR of 0.63, 0.71 and 0.74 respectively. Across the audio modality, our proposed method also produced the best results across EA Gender , DI Gender and DI Race with a score of 0.01, 1.00 and 0.91 respectively. Across the multimodal results, VI, our proposed method also consistently produces good results compared to the baseline data augmenation method across both performance and fairness measures. With reference to Table VI, we see that both data augmentation methods were effective at improving performance and reducing bias. Within the multi-modal approach depicted in Table VI, we see a similar trend. In general, we see our proposed method performing better than the baseline across most fusion strategies.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Recommendations And Conclusion",
      "text": "This paper highlighted the problem of bias in small datasets in the context of human-human and robot-human wellbeing coaching and illustrated that it is possible to debias a small dataset using a principled and methodological approach. Efforts must be made to ensure fairness, especially in robothuman interactions, given the responsibility of building and designing these systems in sensitive contexts such as wellbeing. In addition to our findings reported in  [15] , we have also noted that as much as it is possible to debias small datasets, the performance of a model, as measured using the standard metrics such as accuracy an F1, is still very correlated and dependent on the size of the dataset. AFAR-RC22 (26 participants with a total of 101 datapoints) and AFAR-RC23 (29 participants with a total of 116 datapoints) are bigger than AFAR-BSFT (11 participants with a total of 41 datapoints). We see from our results that the same models trained on the AFAR-RC22 and AFAR-RC23 datasets perform better across measures such as overall accuracy and overall F1. In addition, The effects of data augmentation and data balancing in these dataset produced greater improvements in performance metrics compared to the BSFT dataset. This is likely due to the fact that we had to generate more synthetic samples in order to balance the samples across the different sensitive attribute groups for the larger datasets (AFAR-RC22 and AFAR-RC23). As a result, the increase in training set is likely to have introduced more data for the model to better learn from. From our findings, we distilled a set of recommendations (R1 -R5) in Table VII that can be used by AC and HRI researchers to integrate and address fairness-related concerns within their ML-based research when working with small datasets. R1: Train human-centric models on a balanced dataset. Our results suggest that employing an imbalanced human dataset may lead to fairness issues when training ML models. Small datasets are commonly utilised in studies within Affective Computing (AC) and Human-Robot Interaction (HRI), particularly in contexts related to wellbeing  [5] . Involving humans in experiments poses challenges and time constraints, making it even more difficult to collect a balanced dataset and in turn negatively impacting fairness. This is also supported by literature that highlighted how the problem of bias is often associated with data imbalances  [53] . Imbalanced datasets can lead to bias against certain groups and that oversampling can improve model performance for underrepresented groups  [60] .\n\n[61] has annotated benchmark affective computing datasets to analyse the bias in detecting facial expressions. Their results show biases in age and ethnicity groups, for which models performed better with people under 34 years old. To overcome the issue of imbalanced datasets, many strategies have been proposed spanning from data augmentation, ensemble techniques, to evaluation metrics for imbalanced data. Curating the dataset by ensuring balanced participation and representation across sensitive groups (e.g., gender, race) during data collection is crucial for fairness. When this is not applicable, data balancing techniques can be employed. Simple methods such as data augmentation via upsampling are already capable of significantly improving the lack of fairness present. Therefore, we recommend to train human-centric ML models on balanced datasets by using simple balancing techniques such as data augmentation.\n\nR2: Make use of multimodal and higher level features. Our results show that fairness has been improved by using multi-modal and high-level features. Small AC and HRI datasets often include audio-visual recordings and sometimes physiological signals (e.g., EEG  [62] , heart rate variability, etc.). As such, they encompass different modalities that can be combined to provide more information for a machine learning algorithm to learn from. Past works  [63, 55]  have shown how multi-modal models performed better than unimodal ML models. For example,  [54]  proposed two novel spectral representations, i.e., spectral heatmaps and spectral vectors, to represent video-level multi-scale temporal dynamics of expressive behaviour to detect depression from multimodal data. Their results suggest that multi-modal models outperformed current state of the art in detecting depression. Conducting experiments with multiple modalities is crucial as they carry more information for the ML models to learn from. This should involve experiments with different fusion strategies (e.g., early or late fusion) to evaluate the best way to combine multi-modal information  [63] . Additionally, we suggest extracting high-level features from small datasets and selecting the most informative ones to train the models on, which can also improve fairness. Therefore, we recommend using high-level and multi-modal features when working with human-centric small datasets.\n\nR3: Employ a variety of ML models and evaluation and fairness metrics. Our results highlight the importance of exploring a variety of machine learning models and evaluating them using diverse evaluation and fairness metrics. This approach is a best practice to adopt when conducting ML experiments, and it is especially crucial when assessing fairness. Researchers can gain a more holistic understanding of the model's performance and potential biases when working with human-centric datasets, where fairness is a critical consideration. Past works  [25]  emphasised the importance of using various measures for fairness to represent different aspects of bias. For instance, Equity-Accuracy (EA) assesses fairness by considering the model's predictive accuracy, whereas Disparate Impact (DI) evaluates fairness by focusing on the predicted outcomes. In addition, a model that demonstrates strong fairness across diverse subgroups may exhibit lower accuracy compared to a model optimized solely for high performance. Therefore, it is crucial for each researcher to determine the appropriate fairness-accuracy trade off that best suits the specific task and context at hand  [64, 65] . Specific ML models may provide more favourable results across certain accuracy and fairness metrics. Therefore, studies should report on experimental results across a variety of metrics (e.g., fairness-accuracy trade off). This will help ensure that the developed AI-based systems, like robotic coaches, are not only accurate, but also fair when deployed for delivering wellbeing practices.\n\nR4: Balance the trade off between AC/robotic system complexity and fairness. Our results show that the adaptive capability of robotic systems may have impacted the debiasing strategies in different ways. The complexity of adaptive capabilities can pose challenges in effectively enhancing fairness, while the adaptability of robotic systems might inherently reduce bias. Recently, the HRI field is rapidly employing autonomous and adaptive robots that leverage AI components (e.g., ChatGPT) and capabilities to create naturalistic and smooth robot-human interactions  [5] . These advancements led to improvement in the interaction outcomes and user perceptions towards the robot  [5] , but also to an increased complexity and reduced transparency of the robotic system, making it more difficult to control for bias. This has been supported by the literature as well  [43, 66] . For instance, Large Language Models pose challenges in debiasing due to their non-transparent and complex nature when embedded in robotic systems. They can exhibit emergent behaviours and unintended consequences that are difficult to predict and control for, causing further challenges when embedded into embodied artificial systems such as robots.\n\nDespite the aforementioned challenges, adaptive models still need to be assessed in terms of bias and fairness before they can be embedded in embodied systems that will be deployed in real world settings with real world consequences  [57] . When this is not feasible, real-time models should incorporate in-processing bias mitigation techniques. Therefore, we recommend researchers to carefully balance embodied / robotic system complexity and fairness to safeguard against biased robot-human interactions.\n\nR5: Define field and context specific ethical principles when designing / deploying AC systems for wellbeing. Our results suggest that the use of robotic coaches may help improve fairness if, as in the case of the AFAR-RC22  [19]  and AFAR-RC23  [5, 47] , the design of the robot-human interactions adhere to certain ethical principles  [59] . For designing robotic mental wellbeing coaches,  [59]  distilled a set of design and ethical recommendations through an iterative process involving stakeholders, such as human wellbeing coaches and coachees. When we collected the AFAR-RC22 and AFAR-RC23 datasets, we aimed to adhere to these guidelines when deploying robotic coaches to deliver positive psychology exercises in the workplace. Other relevant works  [27]  also highlight how the employment of ethical guideline may enhance fairness of AI-based systems  [67, 68] . Recent EU guidelines also emphasise the use of a human-centric approach by involving stakeholders to define and discuss ethical considerations. In other research fields, there exists notable recent efforts in this direction. For example,  [68]  highlights the importance of ethical design in AI-based decision-making systems and  [67]  explores strategies to reduce bias and improve fairness in AI systems.However, in the AC and HRI fields, no work has yet distilled a set of ethical guidelines to reduce bias in AI-based models and embodied systems like robots.\n\nTo ensure fair human-AI/robot interactions it is of utmost importance to adhere to ethical principles when taking the developed systems and robots out into the real world. Developing field and context specific guidelines for ethics and fairness will no doubt enable more purposeful and responsible innovation in AC and HRI research fields while expanding their research scope and significance.",
      "page_start": 9,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: considers papers that have been published",
      "page": 1
    },
    {
      "caption": "Figure 1: Proportion of small dataset papers accepted at ACII’21-’23 and HRI’22",
      "page": 2
    },
    {
      "caption": "Figure 2: AFAR-BSFT Dataset: interaction between the participants (on the left",
      "page": 3
    },
    {
      "caption": "Figure 5: outlines the experimental setup and how the",
      "page": 3
    },
    {
      "caption": "Figure 3: AFAR Robocoaching 2022 Dataset (AFAR-RC22): the setting of the",
      "page": 4
    },
    {
      "caption": "Figure 4: AFAR-Robo Coaching 2023 Dataset (ARC-2023): the setting of the",
      "page": 4
    },
    {
      "caption": "Figure 4: ) that delivered four",
      "page": 4
    },
    {
      "caption": "Figure 5: The model pipeline with our proposed data augmentation technique:",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face\nAudio\nVerbal": "Low\nHigh"
        },
        {
          "Face\nAudio\nVerbal": "0.45\n0.27\n0.44\n0.33\n0.43\n0.35\n0.37\n0.37"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Early\nSoft Voting\nStacking\nLow\nHigh\nLow\nHigh\nLow\nHigh": "0.58\n0.53\n0.54\n0.55\n0.47\n0.52\n0.71\n0.61\n0.62\n0.66\n0.52\n0.59\n0.59\n0.59\n0.52\n0.55\n0.55\n0.55\n0.65\n0.71\n0.71\n0.56\n0.60\n0.60"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Face\nAudio\nVerbal": "Low\nHigh"
        },
        {
          "Face\nAudio\nVerbal": "0.70\n0.63\n0.78\n0.74\n0.73\n0.66\n0.78\n0.75"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Early\nSoft Voting\nStacking": "Low\nHigh\nLow\nHigh\nLow\nHigh"
        },
        {
          "Early\nSoft Voting\nStacking": "0.60\n0.72\n0.72\n0.59\n0.65\n0.69\n0.75\n0.78\n0.78\n0.74\n0.77\n0.77\n0.59\n0.73\n0.73\n0.50\n0.63\n0.62\n0.70\n0.80\n0.79\n0.62\n0.74\n0.73"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "R\nM\nR\nM\nR\nM": "0.59\n0.81\n0.62\n0.43\n0.55\n0.45\n0.72\n0.50\n0.83\n0.68\n0.52\n0.70\n0.59\n0.43\n0.81\n0.55\n0.45\n0.62"
        },
        {
          "R\nM\nR\nM\nR\nM": "0.07\n0.38\n0.31\n0.14\n0.07\n0.21\n0.07\n0.01\n0.10\n0.10\n0.17\n0.14\n1.19\n0.94\n0.82\n0.75\n1.21\n1.37\n0.76\n0.33\n1.33\n1.06\n0.84\n0.96"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "R\nM\nR\nM": "0.59\n0.59\n0.59\n0.55\n0.68\n0.68\n0.65\n0.67\n0.70\n0.70\n0.67\n0.68"
        },
        {
          "R\nM\nR\nM": "0.02\n0.09\n0.18\n0.13\n0.17\n0.23\n0.26\n0.19\n1.01\n0.77\n0.93\n0.78\n0.99\n0.97\n1.10\n1.20"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "R\nM\nR\nM\nR\nM": "0.63\n0.59\n0.63\n0.57\n0.57\n0.59\n0.71\n0.66\n0.66\n0.66\n0.64\n0.41\n0.74\n0.67\n0.68\n0.68\n0.65\n0.42"
        },
        {
          "R\nM\nR\nM\nR\nM": "0.01\n0.03\n0.23\n0.19\n0.04\n0.07\n0.37\n0.27\n0.10\n0.13\n0.05\n0.06\n1.14\n1.00\n0.84\n0.75\n1.22\n0.84\n0.91\n0.86\n1.08\n1.20\n0.88\n1.17"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Machine learning: Algorithms, real-world applications and research directions",
      "authors": [
        "I Sarker"
      ],
      "year": "2021",
      "venue": "SN computer science"
    },
    {
      "citation_id": "2",
      "title": "Fairness in machine learning",
      "authors": [
        "S Barocas",
        "M Hardt",
        "A Narayanan"
      ],
      "year": "2017",
      "venue": "Nips tutorial"
    },
    {
      "citation_id": "3",
      "title": "Deep learning in mental health outcome research: a scoping review",
      "authors": [
        "C Su",
        "Z Xu",
        "J Pathak",
        "F Wang"
      ],
      "year": "2020",
      "venue": "Translational Psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Continual learning for affective robotics: A proof of concept for wellbeing",
      "authors": [
        "N Churamani",
        "M Axelsson",
        "A Caldır",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "5",
      "title": "Vita: A multi-modal llm-based system for longitudinal, autonomous, and adaptive robotic mental well-being coaching",
      "authors": [
        "M Spitale",
        "M Axelsson",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Vita: A multi-modal llm-based system for longitudinal, autonomous, and adaptive robotic mental well-being coaching"
    },
    {
      "citation_id": "6",
      "title": "Fairness and transparency in human-robot interaction",
      "authors": [
        "H Claure",
        "M Chang",
        "S Kim",
        "D Omeiza",
        "M Brandao",
        "M Lee",
        "M Jung"
      ],
      "year": "2022",
      "venue": "Fairness and transparency in human-robot interaction"
    },
    {
      "citation_id": "7",
      "title": "Unfair! perceptions of fairness in human-robot teams,\" in 2021 RO-MAN",
      "authors": [
        "M Chang",
        "G Trafton",
        "J Mccurry",
        "A Thomaz"
      ],
      "year": "2021",
      "venue": "Unfair! perceptions of fairness in human-robot teams,\" in 2021 RO-MAN"
    },
    {
      "citation_id": "8",
      "title": "Fair navigation planning: A resource for characterizing and designing fairness in mobile robots",
      "authors": [
        "M Brandao",
        "M Jirotka",
        "H Webb",
        "P Luff"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Ethics, equity, & justice in humanrobot interaction: A review and future directions,\" in 2022 RO-MAN",
      "authors": [
        "A Ostrowski",
        "R Walker",
        "M Das",
        "M Yang",
        "C Breazea",
        "H Park",
        "A Verma"
      ],
      "year": "2022",
      "venue": "Ethics, equity, & justice in humanrobot interaction: A review and future directions,\" in 2022 RO-MAN"
    },
    {
      "citation_id": "10",
      "title": "Bias mitigation for machine learning classifiers: A comprehensive survey",
      "authors": [
        "M Hort",
        "Z Chen",
        "J Zhang",
        "F Sarro",
        "M Harman"
      ],
      "year": "2022",
      "venue": "Bias mitigation for machine learning classifiers: A comprehensive survey",
      "arxiv": "arXiv:2207.07068"
    },
    {
      "citation_id": "11",
      "title": "The hitchhiker's guide to bias and fairness in facial affective signal processing: Overview and techniques",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "IEEE SPM"
    },
    {
      "citation_id": "12",
      "title": "Modeling user empathy elicited by a robot storyteller",
      "authors": [
        "L Mathur",
        "M Spitale",
        "H Xi",
        "J Li",
        "M Matarić"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "13",
      "title": "Affective robotics for wellbeing: A scoping review",
      "authors": [
        "M Spitale",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "14",
      "title": "Computational audio modelling for robot-assisted assessment of children's mental wellbeing",
      "authors": [
        "N Abbasi",
        "M Spitale",
        "J Anderson",
        "T Ford",
        "P Jones",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "15",
      "title": "it's not fair!\" -fairness for a small dataset of multi-modal dyadic mental well-being coaching",
      "authors": [
        "J Cheong",
        "M Spitale",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "it's not fair!\" -fairness for a small dataset of multi-modal dyadic mental well-being coaching"
    },
    {
      "citation_id": "16",
      "title": "Deploying a robotic positive psychology coach to improve college students' psychological well-being",
      "authors": [
        "S Jeong",
        "L Aymerich-Franch",
        "K Arias",
        "S Alghowinem",
        "A Lapedriza",
        "R Picard",
        "H Park",
        "C Breazeal"
      ],
      "year": "2023",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "17",
      "title": "Robotic coaches delivering group mindfulness practice at a public cafe",
      "authors": [
        "M Axelsson",
        "M Spitale",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "18",
      "title": "Evaluating and personalizing user-perceived quality of text-to-speech voices for delivering mindfulness meditation with different physical embodiments",
      "authors": [
        "Z Shi",
        "H Chen",
        "A.-M Velentza",
        "S Liu",
        "N Dennler",
        "A O'connell",
        "M Mataric"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "19",
      "title": "Robotic mental wellbeing coaches for the workplace: An in-the-wild study on form",
      "authors": [
        "M Spitale",
        "M Axelsson",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "20",
      "title": "A robotic companion for psychological well-being: A long-term investigation of companionship and therapeutic alliance",
      "authors": [
        "S Jeong",
        "L Aymerich-Franch",
        "S Alghowinem",
        "R Picard",
        "C Breazeal",
        "H Park"
      ],
      "year": "2023",
      "venue": "HRI"
    },
    {
      "citation_id": "21",
      "title": "Multimodal deep learning framework for mental disorder recognition",
      "authors": [
        "Z Zhang",
        "W Lin",
        "M Liu",
        "M Mahmoud"
      ],
      "venue": "Multimodal deep learning framework for mental disorder recognition"
    },
    {
      "citation_id": "22",
      "title": "Fairness definitions for digital mental health applications",
      "authors": [
        "S Ryan",
        "G Doherty"
      ],
      "year": "2022",
      "venue": "Fairness definitions for digital mental health applications"
    },
    {
      "citation_id": "23",
      "title": "Gender bias in depression detection using audio features",
      "authors": [
        "A Bailey",
        "M Plumbley"
      ],
      "year": "2021",
      "venue": "EUSIPCO"
    },
    {
      "citation_id": "24",
      "title": "Bias reducing multitask learning on mental health prediction",
      "authors": [
        "K Zanna",
        "K Sridhar",
        "H Yu",
        "A Sano"
      ],
      "venue": "ACII 2022. IEEE"
    },
    {
      "citation_id": "25",
      "title": "Towards gender fairness for mental health prediction",
      "authors": [
        "J Cheong",
        "S Kuzucu",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "IJCAI"
    },
    {
      "citation_id": "26",
      "title": "Fairness in mobile phone-based mental health assessment algorithms: Exploratory study",
      "authors": [
        "J Park",
        "R Arunachalam",
        "V Silenzio",
        "V Singh"
      ],
      "year": "2022",
      "venue": "JMIR formative research"
    },
    {
      "citation_id": "27",
      "title": "Fairness and bias in robot learning",
      "authors": [
        "L Londoño",
        "J Hurtado",
        "N Hertz",
        "P Kellmeyer",
        "S Voeneky",
        "A Valada"
      ],
      "year": "2022",
      "venue": "Fairness and bias in robot learning",
      "arxiv": "arXiv:2207.03444"
    },
    {
      "citation_id": "28",
      "title": "Causal-hri: Causal learning for human-robot interaction",
      "authors": [
        "J Cheong",
        "N Churamani",
        "L Guerdan",
        "T Lee",
        "Z Han",
        "H Gunes"
      ],
      "year": "2024",
      "venue": "Companion of ACM/IEEE HRI 2024"
    },
    {
      "citation_id": "29",
      "title": "Does removing stereotype priming remove bias? a pilot human-robot interaction study",
      "authors": [
        "T Ogunyale",
        "D Bryant",
        "A Howard"
      ],
      "year": "2018",
      "venue": "Does removing stereotype priming remove bias? a pilot human-robot interaction study",
      "arxiv": "arXiv:1807.00948"
    },
    {
      "citation_id": "30",
      "title": "Ai bias in human-robot interaction: An evaluation of the risk in gender biased robots",
      "authors": [
        "T Hitron",
        "B Megidish",
        "E Todress",
        "N Morag",
        "H Erel"
      ],
      "year": "2022",
      "venue": "Ai bias in human-robot interaction: An evaluation of the risk in gender biased robots"
    },
    {
      "citation_id": "31",
      "title": "Machine learning security as a source of unfairness in human-robot interaction",
      "authors": [
        "L Richards",
        "C Matuszek"
      ],
      "year": "2023",
      "venue": "(HRI) Workshop on Inclusive HRI"
    },
    {
      "citation_id": "32",
      "title": "Ffab-the form function attribution bias in human-robot interaction",
      "authors": [
        "K Haring",
        "K Watanabe",
        "M Velonaki",
        "C Tossell",
        "V Finomore"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "33",
      "title": "Differential biases in human-human versus humanrobot interactions",
      "authors": [
        "G Alarcon",
        "A Capiola",
        "I Hamdan",
        "M Lee",
        "S Jessup"
      ],
      "year": "2023",
      "venue": "Applied Ergonomics"
    },
    {
      "citation_id": "34",
      "title": "Encouraging human interaction with robot teams: Legible and fair subtask allocations",
      "authors": [
        "S Habibian",
        "D Losey"
      ],
      "year": "2022",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "35",
      "title": "Towards understanding the entanglement of human stereotypes and system biases in human-robot interaction",
      "authors": [
        "C Lachemaier",
        "E Lumer",
        "H Buschmeier",
        "S Zarrieß"
      ],
      "year": "2024",
      "venue": "Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "36",
      "title": "Counterfactual fairness for facial expression recognition",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "Counterfactual fairness for facial expression recognition"
    },
    {
      "citation_id": "37",
      "title": "Debiasing item-to-item recommendations with small annotated datasets",
      "authors": [
        "T Schnabel",
        "P Bennett"
      ],
      "year": "2020",
      "venue": "ACM RecSys 2020"
    },
    {
      "citation_id": "38",
      "title": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients",
      "authors": [
        "J An",
        "L Ying",
        "Y Zhu"
      ],
      "venue": "Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients"
    },
    {
      "citation_id": "39",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    },
    {
      "citation_id": "40",
      "title": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "authors": [
        "D Hendrycks",
        "N Mu",
        "E Cubuk",
        "B Zoph",
        "J Gilmer",
        "B Lakshminarayanan"
      ],
      "year": "2020",
      "venue": "Augmix: A simple data processing method to improve robustness and uncertainty"
    },
    {
      "citation_id": "41",
      "title": "Fair mixup: Fairness via interpolation",
      "authors": [
        "C.-Y Chuang",
        "Y Mroueh"
      ],
      "year": "2021",
      "venue": "ICLR 2021"
    },
    {
      "citation_id": "42",
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "authors": [
        "S Yun",
        "D Han",
        "S Oh",
        "S Chun",
        "J Choe",
        "Y Yoo"
      ],
      "year": "2019",
      "venue": "Cutmix: Regularization strategy to train strong classifiers with localizable features"
    },
    {
      "citation_id": "43",
      "title": "Mitigating biases in multimodal personality assessment",
      "authors": [
        "S Yan",
        "D Huang",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "44",
      "title": "More than miracles: The state of the art of solution-focused brief therapy",
      "authors": [
        "S Shazer",
        "Y Dolan"
      ],
      "year": "2012",
      "venue": "More than miracles: The state of the art of solution-focused brief therapy"
    },
    {
      "citation_id": "45",
      "title": "Development and validation of brief measures of positive and negative affect: the panas scales",
      "authors": [
        "D Watson",
        "L Clark",
        "A Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "46",
      "title": "Longitudinal evolution of coachees' behavioural responses to interaction ruptures in robotic positive psychology coaching",
      "authors": [
        "M Spitale",
        "M Axelsson",
        "N Kara",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "2023 32nd IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "47",
      "title": "oh, sorry, i think i interrupted you\": Designing repair strategies for robotic longitudinal well-being coaching",
      "authors": [
        "M Axelsson",
        "M Spitale",
        "H Gunes"
      ],
      "year": "2024",
      "venue": "oh, sorry, i think i interrupted you\": Designing repair strategies for robotic longitudinal well-being coaching"
    },
    {
      "citation_id": "48",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "FG 2018"
    },
    {
      "citation_id": "49",
      "title": "Investigating multisensory integration in emotion recognition through bio-inspired computational models",
      "authors": [
        "E Benssassi",
        "J Ye"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Computational audio modelling for robot-assisted assessment of children's mental wellbeing",
      "authors": [
        "N Abbasi",
        "M Spitale",
        "J Anderson",
        "T Ford",
        "P Jones",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "ICSR 2022"
    },
    {
      "citation_id": "51",
      "title": "Techniques for interpretable machine learning",
      "authors": [
        "M Du",
        "N Liu",
        "X Hu"
      ],
      "year": "2019",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "52",
      "title": "Multimodal fusion for multimedia analysis: a survey",
      "authors": [
        "P Atrey",
        "M Hossain",
        "A Saddik",
        "M Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia systems"
    },
    {
      "citation_id": "53",
      "title": "Bias in machine learning software: Why? how? what to do",
      "authors": [
        "J Chakraborty",
        "S Majumder",
        "T Menzies"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM joint meeting on European software engineering conference"
    },
    {
      "citation_id": "54",
      "title": "Spectral representation of behaviour primitives for depression analysis",
      "authors": [
        "S Song",
        "S Jaiswal",
        "L Shen",
        "M Valstar"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Introducing representations of facial affect in automated multimodal deception detection",
      "authors": [
        "L Mathur",
        "M Matarić"
      ],
      "year": "2020",
      "venue": "Introducing representations of facial affect in automated multimodal deception detection"
    },
    {
      "citation_id": "56",
      "title": "Adaptive data debiasing through bounded exploration",
      "authors": [
        "Y Yang",
        "Y Liu",
        "P Naghizadeh"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Fairness maximization among offline agents in online-matching markets",
      "authors": [
        "W Ma",
        "P Xu",
        "Y Xu"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Economics and Computation"
    },
    {
      "citation_id": "58",
      "title": "Value-sensitive design",
      "authors": [
        "B Friedman"
      ],
      "year": "1996",
      "venue": "interactions"
    },
    {
      "citation_id": "59",
      "title": "Robots as mental wellbeing coaches: Design and ethical recommendations",
      "authors": [
        "M Axelsson",
        "M Spitale",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Human-Robot Interaction"
    },
    {
      "citation_id": "60",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2021",
      "venue": "ACM CSUR"
    },
    {
      "citation_id": "61",
      "title": "Female, white, 27? bias evaluation on data and algorithms for affect recognition in faces",
      "authors": [
        "J Pahl",
        "I Rieger",
        "A Möller",
        "T Wittenberg",
        "U Schmid"
      ],
      "year": "2022",
      "venue": "FAccT"
    },
    {
      "citation_id": "62",
      "title": "Context sensitivity of eeg-based workload classification under different affective valence",
      "authors": [
        "S Grissmann",
        "M Spüler",
        "J Faller",
        "T Krumpe",
        "T Zander",
        "A Kelava",
        "C Scharinger",
        "P Gerjets"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "64",
      "title": "Causal structure learning of bias for fair affect recognition",
      "authors": [
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "WACV 2023"
    },
    {
      "citation_id": "65",
      "title": "Towards causal replay for knowledge rehearsal in continual learning",
      "authors": [
        "N Churamani",
        "J Cheong",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "PMLR"
    },
    {
      "citation_id": "66",
      "title": "Biasasker: Measuring the bias in conversational ai system",
      "authors": [
        "Y Wan",
        "W Wang",
        "P He",
        "J Gu",
        "H Bai",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"
    },
    {
      "citation_id": "67",
      "title": "Ai fairness in data management and analytics: A review on challenges, methodologies and applications",
      "authors": [
        "P Chen",
        "L Wu",
        "L Wang"
      ],
      "year": "2023",
      "venue": "Applied sciences"
    },
    {
      "citation_id": "68",
      "title": "Ethical design of artificial intelligence-based systems for decision making",
      "authors": [
        "G Biondi",
        "S Cagnoni",
        "R Capobianco",
        "V Franzoni",
        "F Lisi",
        "A Milani",
        "J Vallverdú"
      ],
      "year": "2023",
      "venue": "Ethical design of artificial intelligence-based systems for decision making"
    },
    {
      "citation_id": "69",
      "title": "Jiaee Cheong is a Turing doctoral student at the University of Cambridge. Her research interests lie at the intersection of machine learning, affective computing, fairness, causality and HRI",
      "venue": "Jiaee Cheong is a Turing doctoral student at the University of Cambridge. Her research interests lie at the intersection of machine learning, affective computing, fairness, causality and HRI"
    },
    {
      "citation_id": "70",
      "title": "Information and Bioengineering at the Politecnico di Milano, as well as a Visiting Affiliated Researcher at the University of Cambridge. In recent years, her research has been focused on the field of Social Robotics, Human-Robot Interaction, and Affective Computing, exploring ways to develop robots that are socio-emotionally adaptive and provide 'coaching' to promote wellbeing. Hatice Gunes is a Full Professor of Affective Intelligence and Robotics (AFAR) in the Department of Computer Science and Technology, University of Cambridge, leading the Cambridge AFAR Lab. She is a former President of the Association for the Advancement of Affective Computing",
      "venue": "Information and Bioengineering at the Politecnico di Milano, as well as a Visiting Affiliated Researcher at the University of Cambridge. In recent years, her research has been focused on the field of Social Robotics, Human-Robot Interaction, and Affective Computing, exploring ways to develop robots that are socio-emotionally adaptive and provide 'coaching' to promote wellbeing. Hatice Gunes is a Full Professor of Affective Intelligence and Robotics (AFAR) in the Department of Computer Science and Technology, University of Cambridge, leading the Cambridge AFAR Lab. She is a former President of the Association for the Advancement of Affective Computing"
    }
  ]
}