{
  "paper_id": "2308.11635v2",
  "title": "Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning For Cross-Subject Eeg-Based Emotion Recognition",
  "published": "2023-08-13T23:54:40Z",
  "authors": [
    "Weishan Ye",
    "Zhiguo Zhang",
    "Fei Teng",
    "Min Zhang",
    "Jianhong Wang",
    "Dong Ni",
    "Fali Li",
    "Peng Xu",
    "Zhen Liang"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "graph contrastive learning",
    "domain adaption",
    "semi-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments are conducted on three benchmark databases (SEED, SEED-IV and SEED-V) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation protocol. The results show that the proposed model outperforms existing methods under different incomplete label conditions with an average improvement of 2.17%, which demonstrates its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition. The source code is available at https://github.com/Vesan-yws/DS-AGC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "In recent years, there has been an increasing interest in the potential of EEG signals to identify emotional states of individuals  [8] ,  [9] ,  [10] ,  [11] ,  [12] ,  [13] ,  [14] ,  [15] . These studies have yielded valuable insights and paved the way for the development of affective brain-computer interface (aBCI) systems. Among the early contributions to this field, Duan et al. extracted emotion-related EEG features and employed support vector machines (SVM) for emotion classification  [8] . Since then, a number of studies based on various machine learning and deep learning methods have been developed to enhance the accuracy and usability of aBCI systems.\n\nThe recent advancements in deep learning have led to a surge of research on EEG-based emotion recognition using various neural network architectures. For example, Zhang et al. proposed a cascaded and parallel convolutional recursive neural network that can effectively learn discriminative EEG features  [12] . Song et al. introduced a dynamic graph convolutional neural network to dynamically capture intrinsic relationships among different EEG channels  [16] . Niu et al. developed a novel deep residual neural network by combining brain network analysis and channel-spatial attention mechanism  [17] . The above-mentioned models were robust in within-subject emotion recognition tasks, where the training and testing data come from the same subject. However, due to individual differences in EEG signals collected from different subjects, the model performance would significantly decrease in cross-subject emotion recognition tasks, when the training and testing data come from different subjects  [18] ,  [19] ,  [20] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cross-Subject Eeg-Based Emotion Recognition",
      "text": "To improve the generalizability of cross-subject EEG-based emotion recognition models, transfer learning is employed to mitigate individual differences by harmonizing the feature distribution across different subjects  [18] ,  [21] ,  [22] ,  [23] ,  [24] . Transfer learning methods, such as transfer component analysis (TCA) and transductive parameter transfer (TPT), have been incorporated in Zheng et al.'s work  [18]  to improve cross-subject emotion recognition performance from 56.73% to 76.31%, demonstrating the effectiveness of transfer learning in improving the generalizability of EEG-based emotion recognition models in the presence of individual differences among subjects.\n\nIn recent years, deep transfer learning methods have been proposed to further improve the model performance in cross-subject EEG-based emotion recognition tasks. In 2017, Jin et al.  [24]  introduced a deep transfer learning framework with DANN  [25]     [27] . Compared to the original DANN structure, these enhanced deep transfer learning frameworks have achieved better performance on the cross-subject EEG-based emotion recognition tasks.\n\nRecent deep transfer learning-based emotion recognition models have shown promising results in handling individual differences. Nonetheless, these models often necessitate a significant volume of labeled source data to establish consistent performance. Acquiring a sufficient amount of high-quality labeled EEG data is a challenging and timeconsuming task in aBCI systems. In light of this challenge, the development of a novel SSL framework for cross-subject EEG-based emotion recognition becomes important. Such a framework should effectively leverage a small quantity of labeled source data and a large amount of unlabeled source data. This represents a crucial research direction with the potential to mitigate the constraints imposed by the limited availability of labeled data in EEG-based emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Contrastive learning offers a promising alternative for feature learning, as it enables the extraction of meaningful representations directly from data without the need for manual annotation. This approach has found widespread application in various fields, including computer vision  [28] , natural language processing (NLP)  [29] , and bioinformatics  [30] ,  [31] . In recent years, researchers have also started exploring the potential of contrastive learning for EEGbased emotion recognition. For example, Mohsenvand et al.  [32]  utilized the SimCLR framework  [28]  to learn the similarity between augmented EEG samples from the same input, thereby enhancing the model's ability to capture distinctive features. Similarly, Shen et al.  [33]  proposed the CLISA model, which maximized feature representation similarity across subjects experiencing the same emotional stimuli, leading to improved emotion recognition performance. These studies highlight the efficacy of contrastive learning in the context of EEG-based emotion recognition. However, there are still challenges that need to be addressed in contrastive learning based EEG modeling, particularly with regard to effectively integrating structural information from EEG signals into the contrastive learning framework.\n\nExploring the structural information present in EEG signals is vital for capturing the complex connectivity patterns and topographical relationships within the brain. EEG signals exhibit natural spatial and temporal dependencies among different electrodes and brain regions, which can provide valuable insights into brain dynamics and emotional processes. Graph contrastive learning (GCL) has emerged as a powerful technique that leverages structured information in data, expanding the capabilities of contrastive learning to incorporate rich contextual information in graph data  [34] . Integrating GCL into EEG analysis holds the potential for facilitating a more comprehensive analysis of brain function and its implications, specifically by considering the interplay between different EEG channels within the complex and dynamic realm of emotional processing. The intricate connectivity patterns within the brain could be uncovered and the understanding of how different channels interact and contribute to the overall emotional experience could be deepened.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "An overview of the proposed DS-AGC is shown in Fig.  1 , which consists of three main parts. (1) Non-Structural Stream: extracting non-structural EEG features from the labeled source S, unlabeled source U, and unknown target T domains. Based on the extracted DE features at five different frequency bands, a non-structural feature extractor F N S (•) is defined to extract the features from each domain. A gradient reversal layer is used to reverse the gradients during the feature extraction process, allowing the model to learn domain-invariant features and ensuring that the extracted features from the three domains are indistinguishable. (2) Structural Stream: extracting structural EEG features from S, U, and T. Based on the extracted DE features at five different frequency bands, a graph convolution network (GCN) is constructed for spatial feature representation, and the corresponding positive samples are generated by data augmentation. Then, a structural feature extractor F S (•) is defined to characterize structural feature representation from the input, with a contrastive loss ensuring that the structural features extracted from the positive samples are consistent with each other. (3) Self-Attentive Fusion: fusing the extracted non-structural and structural features from the above two parallel streams. A concatenation of the extracted non-structural and structural features is fed into a multi-head self-attention mechanism. This fusion process generates a new feature representation that emphasizes the most discriminative features related to emotions and suppresses irrelevant information. To ensure the decodability of the features, we train a classifier on the labeled source data (S) using the fused features, with a classification loss.\n\nThe feature representation and the classifier are optimized jointly, ensuring the final feature representation is effective enough for emotion recognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Non-Structural Stream",
      "text": "Traditional EEG-based supervised domain adaptation methods primarily utilize DANN  [25] , aiming to align feature distributions between source and target domains  [35] ,  [36] ,  [37] ,  [38] ,  [39] ,  [40] . However, in semi-supervised learning, treating labeled and unlabeled source data as a single domain complicates adaptation and adversely affects performance of downstream tasks  [41] ,  [42] . To address distribution mismatches among different domains, this paper proposes a novel semi-supervised multi-domain adaptation method, which aligns feature representations among the labeled source domain S, the unlabeled source domain U, and the target domain T. The feature distribution discrepancies across these domains could be mitigated and the model's generalization capabilities could be further enhanced.\n\nIn the proposed model, three domains are defined below. The labeled source domain\n\n), which contains labeled samples with their corresponding emotion labels; the unlabeled source domain\n\n), which contains unlabeled samples without emotion labels; and the unknown target\n\n), which contains samples from an unseen domain that needs to be classified. Here, x s i , x u i , and x t i are the EEG data from the three domains, and y s i is the given emotion label of x s i in the labeled source domain. N s , N u , and N t are the corresponding sample sizes. It is noted that the emotion label information in the unlabeled source domain and the unknown target domain are not available during model training. Finally, the three domains, S, U, and T, constitute a multi-domain.\n\nFor the existing domain adaptation approaches involving two domains (the labeled source domain S and the target domain T), the corresponding target error ϵ T (h) is constrained by the following inequality  [43]\n\nwhere δ represents the difference in labeling functions across the two domains, typically assumed to be small under the covariate shift assumption  [44] . ϵ S (h) denotes the source error for a classification hypothesis h as determined by the source classifier. d H (D S , D T ) measures the divergence between the source domain distribution D S and the target domain distribution D T , estimated directly from the error of a trained binary classifier  [45] . In the semi-supervised learning, accurately estimating source error and H-divergence with limited labeled source data is highly challenging. To address this, our method utilizes both labeled and unla-beled source samples for estimating source error and Hdivergence. The source domain is defined as S * = {S, U}, combining labeled source data (S) and unlabeled source data (U). The convex hull Λ S * π of S * π is a set of mixed distributions, defined as\n\nwhere D S * π is a distribution calculated by the weighted sum of the labeled source domain distribution D S and the unlabeled source domain distribution D U . π S and π U are the respective weights, belonging to the simplex ∆ 1 . For the target domain T, D T is the nearest element to D T within Λ S * π , given as\n\nThen, based on prior domain adaptation methods  [45] ,  [46] ,  [47] , a generalization upper-bound for the target error ϵ T (h) can be derived as\n\nwhere ϵ S (h) is the error of the labeled source domain, and d H (•) denotes the H-divergence between the specified domains.\n\nis the labeling function for any x ∈ Supp DT , representing the target domain's labeling function. In extreme cases where all samples in the source domain are labeled (i.e., U is empty), the upper bound in Eq. 4 equals the bound given in the traditional supervised domain adaptation method. Next, we will demonstrate how to empirically optimize the target error in the downstream tasks.\n\nFor the non-structural stream, we focus on extracting non-structural EEG features. To address the distribution shift among the three domains, a multi-domain adversarial neural network is incorporated for feature adaptation. It aligns the feature distributions, making them more consistent and reliable across the three different domains. Specifically, we first flatten the extracted DE features into onedimensional feature vectors (termed as {f 1 , f 2 , ..., f m } in Fig.  1 , m is the feature dimensionality) and input them into a feature extractor F N S (•) for sample feature extraction. This produces the corresponding features as { f1 , f2 , ..., f ṁ}, where ṁ is the obtained feature dimensionality after F N S (•). To align the distribution shift among the extracted F N S (X s ), F N S (X u ), and F N S (X t ) for the labeled source domain, unlabeled source domain, and unknown target domain, we introduce a discriminator d(•) with parameters θ d to distinguish the domain from which the sample features originate. The distribution discrepancies among the three domains are minimized by optimizing the discriminator loss function. For an ideal joint hypothesis across the S, U, and T domains, the second term\n\nEq. 4 are assumed to be small under the covariate shift assumption  [43] ,  [44] ,  [46] ,  [48] .\n\nPrevious research has demonstrated that minimizing the H-divergence can be approximated by maximizing the classification error of the domain discriminator through adver-sarial training  [25] ,  [45] . Given the distribution differences among the three domains, the domain discriminator loss is redefined in this study as\n\nwhere l (x i ) is a one-hot domain label of the input sample data x i , and F N S (x i ) is the corresponding extracted nonstructural features. The domain adversarial training aligns the feature representation distribution across the three domains, making the non-structural features more robust to domain changes and more effective for downstream tasks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Structural Stream",
      "text": "We capture the structural information of EEG signals collected from multiple EEG channels and express the inherent relationships among the channels by extracting the structural features. It enables us to gain more valuable insights into the complex interconnections and dependencies within the EEG network. An undirected graph\n\nis the extracted DE features of the kth node.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Convolution Network",
      "text": "A GCN is developed to aggregate the neighbor information of the feature matrix Ψ i for spatial feature extraction, resulting in G. Specifically, we construct a channel-based graph, where the nodes V G correspond to EEG channels, and the node features are the extracted DE features at each channel termed as\n\nDifferent from the traditional GCNs that use a fixed adjacency matrix (such as the k-nearest neighbor graph  [49] ), we define a dynamic adjacency matrix A G as\n\nwhere ψ i j and ψ i k are the extracted DE features at the jth and kth channels. The linear rectification function (ReLU) is employed as an activation function here to ensure that the output of the linear operation (i.e., the dot product between the weight vector w and the node distance ||ψ i j -ψ i k ||) is non-negative, which introduces non-linearity into the model and improves its capacity to learn complex patterns. The weight vector w is learned by minimizing the GCN loss function as\n\nIn Eq. 7, the first term quantifies the similarities between any two nodes (ψ i j and ψ i k ). When the nodes exhibit dissimilar characteristics, it leads to a smaller adjacency value (A G jk ). As the brain network is known to exhibit sparse connectivity, we incorporate sparsity constraint into the graph learning process. A Frobenius norm term of the adjacency matrix A G is included in Eq. 7, which encourages the learned adjacency matrix to be sparse. λ ≥ 0 is a regularization parameter that controls the trade-off between graph learning and sparsity degree.\n\nBased on the learned adjacency matrix A G , the node representation could be characterized on the basis of the Chebyshev expansion of the graph Laplacian. The Chebyshev graph convolution  [50]  is defined as a Φ -1 degree polynomial, given as\n\nwhere θ ∈ R Φ is a Chebyshev coefficient vector, and x i is the input sample data. T φ ( L) ∈ R N G ×N G is the φth order Chebyshev polynomial with the variable L given as\n\nwhere λ max is the maximum eigenvalue of the Laplacian matrix. I N G is the identity matrix, and L is the Laplacian matrix calculated as\n\nwhere D is the degree matrix. The Chebyshev polynomials are given as\n\nwhere T 0 ( L) = 1 and T 1 ( L) = L. In the final obtained graph G, the corresponding node representation could well capture information about the φth order nodes of the graph and provide a richer and more comprehensive view of the graph.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Graph Contrastive Learning",
      "text": "To further enhance feature representation, GCL is introduced to learn representations that are robust to certain transformations or augmentations of the data. This process ensures that similar instances are brought closer together in the representation space, while dissimilar instances are pushed apart.\n\nBuilding upon the G obtained in subsection 3.2.1, two augmented graphs, denoted as Ĝi and Ĝj , are generated as positive samples. We randomly drop ζ% of the nodes from G, following a uniform dropout probability distribution, similar to  [34] . The node features of the augmented graphs are then flattened into one-dimensional feature vectors, denoted as {ĝ 1 , ..., ĝn } and {ĝ ′ 1 , ..., ĝ′ n }, respectively. n is the corresponding feature dimensionality. A feature extractor F S (•) is applied to generate high-level feature representation, resulting as {g 1 , ..., g ṅ} and {g ′ 1 , ..., g′ ṅ}. ṅ is the obtained feature dimensionality after the feature extraction. Then, a projector P (•) is used to further reduce the feature dimensionality, producing z i and z j for each augmented graph. To ensure consistency between the feature representation of the two augmented graphs generated from the same input, a contrastive learning loss L gcl is defined as a normalized temperature-scaled cross-entropy loss, given as\n\nwhere Sim refers to cosine similarity and τ is a temperature parameter to adjust the feature learning performance. L gcl encourages the similarity between z i and z j (positive samples) to be maximized, while pushing away the similarity between z i and z k (negative samples). B is the batch size.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Self-Attentive Fusion",
      "text": "Self-attentive fusion is introduced to effectively highlight important features and assign higher weights to the source data that is in closer proximity to the target data, resulting in more informative feature representation. Furthermore, to ensure that the extracted features are discriminant for emotion recognition, a supervised classification part is incorporated into the model learning process as well.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Informative Feature Fusion",
      "text": "The extracted non-structural features by F N S (•) and structural features by F S (•) are concatenated into a new feature representation, denoted as { f1 , . . . , f ṁ, g1 , . . . , g ṅ}. The multi-head self-attention mechanism is incorporated to emphasize the most crucial features relevant to emotions while reducing the influence of irrelevant information. It adapts to various viewpoints or focal points, autonomously determining the focus of feature extraction based on input data and task-specific requirements. As shown in Fig.  1 , we generate three matrices Q, K, and V from the input using linear transformations. Inspired from  [51] , the attention weights are given as\n\nThen, we further extend the attention mechanism to H heads over the three matrices. Each matrix is divided into H subspaces, termed as\n\nIn each subspace h ∈ H, we calculate A h using the attention formula, given as\n\nFinally, all H representations are concatenated together to obtain the final output M HA(X) = M HA(Q, K, V ) for classification as",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Informative Sample Selection",
      "text": "During the model training process, we place additional attention on weighing the contribution of each labeled source data. We assign higher weights to those labeled source data that offer more valuable information for effective emotion recognition. This process helps prioritize and focus on the most informative data during the optimization process. Specifically, based on the feature representation M HA(•), a fully connected layer ϕ(•) is designed as\n\nwhere B represents the batch size. For each labeled source data r b s (1 ≤ b ≤ B), we calculate the corresponding cosine similarity with all unknown target data as\n\nSoftmax is then applied on {Sim(r 1 s ), . . . , Sim(r B s )} for normalization. Finally, the normalized similarity weight Sim(r b s ) is used to adjust the sample contribution in the multi-class cross-entropy loss function as\n\nwhere y c b and ŷc b denote the actual emotion label and the predicted emotion label of b-th labeled source data, respectively. C is the total number of emotion categories.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Benchmark Databases",
      "text": "To assess the efficacy of the proposed DS-AGC model, comprehensive experiments are carried out on four benchmark EEG databases: SEED  [9] , SEED-IV  [52] , SEED-V  [53]  and FACED  [54] . In the SEED database, three emotional states (negative, neutral, and positive) were involved, and 15 subjects (7 males and 8 females) were recruited. In the SEED-IV database, four emotional states (happiness, sadness, fear, and neutral) were selected, and 15 subjects (7 males and 8 females) were recruited. In the SEED-V database, it included five emotional states (happiness, sadness, fear, disgust, and neutral), with EEG recordings from 16 subjects (6 males and 10 females). In the FACED database, it included nine emotional states (amusement, inspiration, joy, tenderness, anger, fear, disgust, sadness, and neutral), with EEG recordings from 123 subjects. To maintain consistency with previous research on the three benchmark databases and ensure fair comparisons, our study also utilizes the pre-computed DE features  [9] ,  [55] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details And Model Setting",
      "text": "In the implementation, the feature extractors F N S (•) and F S (•), as well as the domain discriminator d(•), are composed of fully connected layers with the ReLU activation function. Given the input DE features with a small feature dimensionality, a lightweight network architecture would be more appropriate as discussed in prior works  [56] ,  [57] . Specifically, F N S (•) is designed with 310 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-64 neurons (hidden layer 2)-ReLU activation-64 neurons (output feature layer). The probability of node dropout ζ% is set to 0.2, resulting in 49 remaining channels. F S (•) is designed with 245 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-64 neurons (hidden layer 2)-ReLU activation-64 neurons (output feature layer). The domain discriminator d(•) is designed with 64 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-dropout layer-64 neurons (hidden layer 2)-2 neurons (output layer) / 3 neurons (output layer)-Softmax activation. Gradient descent and parameter optimization are carried out using Fig.  2 : The cross-subject leave-one-subject-out crossvalidation experimental protocol with incomplete labels. S, U, and T represent the labeled source domain, unlabeled source domain, and unknown target domain. For the SEED and SEED-IV databases, which contain a total of 15 subjects, 14 -N subjects are allocated to S, and N subjects to U.\n\nSimilarly, for the SEED-V database with 16 subjects, 15 -N subjects are assigned to S, and N subjects to U. M denotes the total number of trials for each subject. L ce , L gcn , L gcl , and L disc are the classification loss, GCN loss, GCL loss, and discriminator loss, given in Eq. 18, Eq. 7, Eq. 12, and Eq. 5. In the implementation, L ce is calculated using only S (represented by blue stars), as the label information for U and T is unknown. L gcn , L gcl , and L disc , which do not depend on label information, are calculated using data from S (blue stars), U (pink circles), and T (gray triangles).\n\nthe RMSprop optimizer, with a learning rate set to 1e-3 and a batch size of 48. In the GCN architecture, we set φ in Eq. 8 as 3 to ensure the extraction of stable graph network features. This choice strikes a balance by incorporating relevant channel features while minimizing the introduction of noise. To further fine-tune the model, we set the balancing parameter λ in Eq. 7 to 0.01. For the multihead self-attention mechanism, we utilize H = 64 in Eq. 15. This configuration allows the model to capture multiple perspectives and enhance its ability to input data. All the models are trained using the PyTorch API on an NVIDIA GeForce RTX 1080 GPU, with CUDA version 11.7. During the model training process, we solely utilize the raw target data without any label information. This approach aligns with previous EEG-based emotion recognition methods that employ a transfer learning framework, as demonstrated in studies such as  [40] ,  [41] ,  [58] ,  [59] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Protocol With Incomplete Labels",
      "text": "As illustrated in Fig.  2 , we implement a strict leave-onesubject-out cross-validation experimental protocol. For the SEED and SEED-IV databases (each containing 15 subjects), we sequentially select 14 subjects as the source domain and the remaining one as the target domain for model testing. Following the same strategy, in the SEED-V database (containing 16 subjects), 15 subjects are used as the source domain, and the remaining subject is used as the target domain. In order to ensure each subject is rotated as the target domain, undergoing a total of 15 rounds (SEED and SEED-IV) or 16 rounds (SEED-V) of model training and testing. The final reported classification results are the average accuracy and standard deviation across 15 (SEED and SEED-IV) or 16 (SEED-V) rounds. Within the source domain, a subset of subjects (N subjects) is classified as the unlabeled source domain U, and the remaining 14 -N subjects (SEED and SEED-IV) or 15 -N (SEED-V) are classified as the labeled source domain S. For example, if the first subject is assigned to T, then subjects 2 to 2 + N -1 are assigned to U, and subjects 2 + N to 15 (SEED and SEED-IV) or 2 + N to 16 (SEED-V) belong to S. To fully evaluate the model's stability in various situations of label scarcity, we vary the N value from 1 to 13 for SEED and SEED-IV, and from 1 to 14 for SEED-V in the implementation.\n\nDuring the training process of the model, the unlabeled source domain U is excluded from the beginning and only the labeled source domain S and the unknown target domain T participate in the training for the first E t iterations. The output layer of the domain discriminator d(•) has two neurons at this point, performing binary classification between the S and T domains. After the model reaches a certain stability, the unlabeled source domain U is added to the training, and the output layer of the domain discriminator d(•) has three neurons, performing ternary classification among the S, U, and T domains. Notably, only the data from the labeled source domain S is used for calculating the cross-entropy loss L ce in the classifier throughout the entire process.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Recognition Performance With Incomplete Labels",
      "text": "As shown in Table  1 , the proposed DS-AGC model consistently outperforms existing machine learning and deep learning models on the SEED database, with an average improvement of 5.83%. Furthermore, even with an increasing number of incomplete labels, the model maintains relatively stable performance. Under the most challenging condition of extreme label scarcity (N = 13), the DS-AGC model achieves an improvement of 7.56%, demonstrating robust stability in situations with minimal labeled data. Table  2  shows the experimental comparison results on the SEED-IV database. Similarly, we vary the N value from 1 to 13. The corresponding average accuracy of DS-AGC on SEED-IV is 61.32 ± 10.41, with an average improvement of 0.19%. It shows DS-AGC exhibits particularly strong adaptability and efficiency with larger N value (less labeled source data). The evaluation performance on the SEED-V database is presented in Table  3 . The DS-AGC model achieves an average accuracy of 53.87±11.14, marking an average improvement of 0.48% over existing methods. Similar to the model's performance on SEED-IV, the superiority of the DS-AGC is particularly evident in situations of acute label scarcity (when the N value is larger than 7). These experimental results demonstrate that the proposed DS-AGC outperforms existing methods on average across different N values and exhibits particular superiority when labeled source data is limited.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Further Validation On The Faced Dataset",
      "text": "To further assess the effectiveness of the proposed DS-AGC semi-supervised learning model in handling large amounts of unlabeled data, we also validate it with the FACED dataset  [54] . For a total of 123 subjects, each subject underwent 28 trials corresponding to 9 different emotions: amusement, inspiration, joy, tenderness (categorized as positive emotions), anger, fear, disgust, sadness (categorized as negative emotions), and a neutral state. Following standard EEG preprocessing, similar to the SEED series database, DE features were extracted.\n\nHere, we evaluate the FACED database under a ten-fold cross-validation method. Specifically, the total of 123 subjects is randomly divided into ten groups. In each iteration, one group serves as the unknown target domain for testing, while the remaining nine groups serve as the source domain, and this process is repeated for ten rounds. This ensures that each subset is used as the test set once. In the source domain, the training data is further divided into labeled (S) and unlabeled (U) domains according to a specific ratio. To fully verify the model stability under different conditions of label scarcity, three semi-supervised cases are considered.\n\n(1) 66% of the data is assigned as labeled source data and 33% as unlabeled source data (S : U = 2 : 1). (2) 50% of the data is assigned as labeled source data and 50% as unlabeled source data (S : U = 1 : 1). (3) 33% of the data is assigned as labeled source data and 66% as unlabeled source data (S : U = 1 : 2). The third case is the most challenging, as it has the smallest amount of labeled source data. Additionally, two types of emotional classification tasks are conducted: a three-category classification (positive, neutral, and negative emotions) and a nine-category classification (amusement, inspiration, joy, tenderness, anger, fear, disgust, sadness, and neutral), denoted as Label 3 and Label 9 , respectively. As shown in Table  4 , the experimental results indicate that, compared to the existing methods, the proposed DS-AGC model achieves superior performance, with an average improvement of 0.43% in three-class classification results and 0.64% in nine-class classification results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "In order to thoroughly assess the performance of the proposed model, we conduct a series of experiments to evaluate the contribution of each module in the proposed model and also explore the effect of various hyperparameter settings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To assess the contribution of each component in the proposed model, a series of ablation studies are conducted. Specifically, we systematically removed different components to observe their impact on the overall performance. Table  5  reports the ablation results using cross-subject leaveone-subject-out cross-validation under N = 2. Firstly, we remove the discriminator from the model and find that the performance significantly drops when neglecting the distributional differences among domains. The model's performance decrease from 87.37 ± 06.19 to 78.71 ± 06.52 on SEED, from 66.00 ± 07.93 to 64.03 ± 08.60 on SEED-IV, and from 59.40 ± 09.99 to 53.32 ± 10.95 on SEED-V. Secondly, we evaluate the benefits of considering three domains (labeled source domain, unlabeled source domain, and target domain) and compare the proposed semi-supervised multi-domain adaptation method with traditional domain Equal weights (Sim(r b s ) = 1) in Eq. 18 are assigned to all samples. The results show that considering sample importance could be beneficial to the performance, bringing an average improvement in model performance by 0.45%, 0.57% and 0.97% on SEED, SEED-IV, and SEED-V databases, respectively. The above results provide a detailed breakdown of the performance of the model with and without specific components, indicating that each component in the proposed model plays an important role in improving the overall performance and addressing the challenges of emotion recognition in real-world scenarios with incomplete label information.\n\nAdditionally, we further assess the contribution of the dual-stream design and the incorporation of unlabeled data in the model learning process. To evaluate the dual-stream design, we analyze the model's performance with each stream separately under various N values. As shown in Fig.  3 , whether using a single structural stream (pink line with circle marker) or a single non-structural stream (green line with square marker), their performance across all settings (N = 1 : 13) is inferior to that of the complete dual-stream design (red line with triangle marker).\n\nIn evaluating the contribution of the unlabeled source data in modeling, we compare the model's performance both with and without the inclusion of the unlabeled source data. As shown in Fig.  3 , the model performance without unlabeled source data (blue line with diamond marker) shows poorer results compared to the performance with unlabeled source data. It demonstrates that including the unlabeled source data plays an important role in the model learning process. Additionally, we conduct supervised learning for comparison, where all the source data are labeled. Following the same cross-validation protocol, the supervised learning results on SEED, SEED-IV, and SEED-V are 87.72 ± 06.54, 67.95 ± 07.84, and 60.27 ± 11.92, respectively. These results indicate that the proposed DS-AGC model can achieve relatively stable results similar to supervised learning in a semi-supervised manner, even when labeled data is limited.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Model Performance On Unseen Target Data",
      "text": "We further explore the model stability on completely unseen target data. For this purpose, we divide the target domain data into two parts: one as a validation set (visible in the model learning phase) and the other as an unseen test set (completely invisible in the model learning phase). The ratio of the validation set to the unseen set is controlled by the parameter Z. When Z = 0, it indicates that all the target domain data are assigned as the unseen test set. When Z = 15 (SEED and SEED-V) or Z = 24 (SEED-IV), it indicates that all the target domain data are assigned as the validation set, as utilized in Section 4. The corresponding results under various Z values on the SEED, SEED-IV, and SEED-V databases are reported in Table  6 , 7, and 8, respectively. It shows that an increase in the Z value, which brings more information during the model learning phase, could be helpful in improving the model performance. Even in the extreme case with Z = 0 (all the target domain data is completely unseen during model learning), it still TABLE 3: The emotion recognition results on SEED-V database with incomplete label conditions, in terms of the average accuracy (%) and standard deviation (%). The model results reproduced by us are indicated by '*'. As shown in Fig.  4 , optimal results are obtained when U is incorporated once the model has achieved a certain degree of stability, rather than from the very beginning (E t = 0).\n\nIntroducing U at the onset can inject noise, potentially disrupting the model's initial learning phase.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Data Visualization",
      "text": "To delve deeper into the learning process, we employ the t-distributed stochastic neighbor embedding (t-SNE) algorithm  [73]  to visually compare the acquired feature representation at different stages. This analysis allows us to gain valuable insights into the model's learning dynamics. Specifically, we visualize the fused features obtained through M HA(•) (Fig.  5 ) and the final classification results (Fig.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which consists of three main parts. (1) Non-Structural",
      "page": 3
    },
    {
      "caption": "Figure 1: An overview of the proposed DS-AGC. It consists of three parts: non-structural stream, structural stream, and self-",
      "page": 4
    },
    {
      "caption": "Figure 1: , m is the feature dimensionality) and input them",
      "page": 5
    },
    {
      "caption": "Figure 1: , we generate",
      "page": 6
    },
    {
      "caption": "Figure 2: , we implement a strict leave-one-",
      "page": 7
    },
    {
      "caption": "Figure 3: , whether using a single structural stream (pink line with",
      "page": 9
    },
    {
      "caption": "Figure 3: , the model performance",
      "page": 10
    },
    {
      "caption": "Figure 4: , optimal results are obtained when U is",
      "page": 11
    },
    {
      "caption": "Figure 5: ) and the final classification results",
      "page": 11
    },
    {
      "caption": "Figure 6: ) at various learning stages: before training, at the",
      "page": 11
    },
    {
      "caption": "Figure 3: Experimental results under different settings on",
      "page": 12
    },
    {
      "caption": "Figure 4: Model performance under various Et values on the",
      "page": 13
    },
    {
      "caption": "Figure 5: A visualization of the obtained informative feature {ξ1, . . . , ξ ˙m+ ˙n} by MHA(·) at three different stages: (a) before",
      "page": 14
    },
    {
      "caption": "Figure 6: A visualization of the classification results at three different stages: (a) before training, (b) at the 30th training epoch,",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "...": "..."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "........."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "........."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "......"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "......"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "14 or 15 - N subjects",
          "Column_2": ""
        },
        {
          "Column_1": "",
          "Column_2": "Trial M"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Experimental results on the FACED database using cross-subject ten-fold cross-validation protocol with",
      "data": [
        {
          "\u00006\u0000L\u0000Q\u0000J\u0000O\u0000H\u0000\u0003\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000D\u0000O\u0000\u0003\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\n\u00006\u0000L\u0000Q\u0000J\u0000O\u0000H\u0000\u0003\u0000Q\u0000R\u0000Q\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000D\u0000O\u0000\u0003\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\n\u0000:\u0000L\u0000W\u0000K\u0000R\u0000X\u0000W\u0000\u0003\u0000X\u0000Q\u0000O\u0000D\u0000E\u0000H\u0000O\u0000H\u0000G\u0000\u0003\u0000V\u0000R\u0000X\u0000U\u0000F\u0000H\u0000\u0003\u0000G\u0000D\u0000W\u0000D\n\u0000&\u0000R\u0000P\u0000S\u0000O\u0000H\u0000W\u0000H\u0000\u0003\u0000G\u0000X\u0000D\u0000O\u0000\u0010\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\u0000\u0003\u0000G\u0000H\u0000V\u0000L\u0000J\u0000Q": "\u00006\u0000L\u0000Q\u0000J\u0000O\u0000H\u0000\u0003\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000D\u0000O\u0000\u0003\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\n\u00006\u0000L\u0000Q\u0000J\u0000O\u0000H\u0000\u0003\u0000Q\u0000R\u0000Q\u0000\u0010\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000X\u0000U\u0000D\u0000O\u0000\u0003\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\n\u0000:\u0000L\u0000W\u0000K\u0000R\u0000X\u0000W\u0000\u0003\u0000X\u0000Q\u0000O\u0000D\u0000E\u0000H\u0000O\u0000H\u0000G\u0000\u0003\u0000V\u0000R\u0000X\u0000U\u0000F\u0000H\u0000\u0003\u0000G\u0000D\u0000W\u0000D\n\u0000&\u0000R\u0000P\u0000S\u0000O\u0000H\u0000W\u0000H\u0000\u0003\u0000G\u0000X\u0000D\u0000O\u0000\u0010\u0000V\u0000W\u0000U\u0000H\u0000D\u0000P\u0000\u0003\u0000G\u0000H\u0000V\u0000L\u0000J\u0000Q"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "att\n14": "(a) before training (b) 30th training epoch (c) final trained model\nFusion Features by\nFig. 5: A visualization of the obtained informative feature {ξ 1 ,...,ξ m˙+n˙ } by MHA(·) at three different stages: (a) before",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "att\n14": "training, (b) at the 30th training epoch, and (c) in the final trained model. In this visualization, the circle, asterisk, and\ntriangle represent the labeled source domain (S), unlabeled source domain (U), and the unknown target domain (T),\npred\nrespectively.Thered,purple,andbluecolorscorrespondtonegative,neutral,andpositiveemotions,respectively.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "att\n14": "(a) before training (b) 30th training epoch (c) final trained model\nFinal Classification Results\nFig.6:Avisualizationoftheclassificationresultsatthreedifferentstages:(a)beforetraining,(b)atthe30thtrainingepoch,\nand(c)inthefinaltrainedmodel.Inthisvisualization,thecircle,asterisk,andtrianglerepresentthelabeledsourcedomain\n(S), unlabeled source domain (U), and the unknown target domain (T), respectively. The red, purple, and blue colors\ncorrespondtonegative,neutral,andpositiveemotions,respectively.\nInternationalConferenceonOrangeTechnologies(ICOT),pp.222–225, p.e1009284,2021.\n2017. [32] M. N. Mohsenvand, M. R. Izadi, and P. Maes, “Contrastive",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Adaboost"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Sa"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Tca"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Adaboost"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Svm"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Adaboost"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Adaboost"
      ],
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Adamatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Flexmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Softmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "",
      "authors": [
        "Mixmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "Adamatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "",
      "authors": [
        "Flexmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "Softmatch"
      ],
      "venue": ""
    },
    {
      "citation_id": "15",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "J Van Engelen",
        "H Hoos"
      ],
      "year": "2020",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "A novel semi-supervised deep learning framework for affective state recognition on EEG signals",
      "authors": [
        "X Jia",
        "K Li",
        "X Li",
        "A Zhang"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Bioinformatics and Bioengineering"
    },
    {
      "citation_id": "17",
      "title": "Holistic semi-supervised approaches for EEG representation learning",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "MixMatch: A holistic approach to semisupervised learning",
      "authors": [
        "D Berthelot",
        "N Carlini",
        "I Goodfellow",
        "N Papernot",
        "A Oliver",
        "C Raffel"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "FixMatch: Simplifying semi-supervised learning with consistency and confidence",
      "authors": [
        "K Sohn",
        "D Berthelot",
        "N Carlini",
        "Z Zhang",
        "H Zhang",
        "C Raffel",
        "E Cubuk",
        "A Kurakin",
        "C.-L Li"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "AdaMatch: A unified approach to semi-supervised learning and domain adaptation",
      "authors": [
        "D Berthelot",
        "R Roelofs",
        "K Sohn",
        "N Carlini",
        "A Kurakin"
      ],
      "year": "2021",
      "venue": "AdaMatch: A unified approach to semi-supervised learning and domain adaptation",
      "arxiv": "arXiv:2106.04732"
    },
    {
      "citation_id": "21",
      "title": "PARSE: Pairwise alignment of representations in semi-supervised EEG learning for emotion recognition",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "23",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "24",
      "title": "EEG-based mild depressive detection using feature selection methods and classifiers",
      "authors": [
        "X Li",
        "B Hu",
        "S Sun",
        "H Cai"
      ],
      "year": "2016",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "25",
      "title": "Real-time movie-induced discrete emotion recognition from EEG signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface",
      "authors": [
        "D Zhang",
        "L Yao",
        "X Zhang",
        "S Wang",
        "W Chen",
        "R Boots",
        "B Benatallah"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Multi-method fusion of cross-subject emotion recognition based on highdimensional EEG features",
      "authors": [
        "F Yang",
        "X Zhao",
        "W Jiang",
        "P Gao",
        "G Liu"
      ],
      "year": "2019",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "28",
      "title": "Adaptive tunable q wavelet transform-based emotion identification",
      "authors": [
        "S Khare",
        "V Bajaj",
        "G Sinha"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "29",
      "title": "EEG-based crosssubject emotion recognition using fourier-bessel series expansion based empirical wavelet transform and nca feature selection method",
      "authors": [
        "A Anuragi",
        "D Sisodia",
        "R Pachori"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "30",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "A brain network analysis-based double way deep neural network for emotion recognition",
      "authors": [
        "W Niu",
        "C Ma",
        "X Sun",
        "M Li",
        "Z Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "32",
      "title": "Personalizing EEG-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Transferring subspaces between subjects in brain-computer interfacing",
      "authors": [
        "W Samek",
        "F Meinecke",
        "K.-R Üller"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "34",
      "title": "Learning a common dictionary for subject-transfer decoding with resting calibration",
      "authors": [
        "H Morioka",
        "A Kanemura",
        "J -I. Hirayama",
        "M Shikauchi",
        "T Ogawa",
        "S Ikeda",
        "M Kawanabe",
        "S Ishii"
      ],
      "year": "2015",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "35",
      "title": "DAGAM: A domain adversarial graph attention model for subject independent EEGbased emotion recognition",
      "authors": [
        "T Xu",
        "W Dang",
        "J Wang",
        "Y Zhou"
      ],
      "year": "2023",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "36",
      "title": "Generator-based domain adaptation method with knowledge free for cross-subject EEG emotion recognition",
      "authors": [
        "D Huang",
        "S Zhou",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "37",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "38",
      "title": "EEG-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "EEG-based emotion recognition using domain adaptation network"
    },
    {
      "citation_id": "39",
      "title": "International Conference on Orange Technologies (ICOT)",
      "year": "2017",
      "venue": "International Conference on Orange Technologies (ICOT)"
    },
    {
      "citation_id": "40",
      "title": "Domainadversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "41",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "42",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Simple and deep graph convolutional networks",
      "authors": [
        "M Chen",
        "Z Wei",
        "Z Huang",
        "B Ding",
        "Y Li"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "44",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "45",
      "title": "An effective self-supervised framework for learning expressive molecular global representations to drug discovery",
      "authors": [
        "P Li",
        "J Wang",
        "Y Qiao",
        "H Chen",
        "Y Yu",
        "X Yao",
        "P Gao",
        "G Xie",
        "S Song"
      ],
      "year": "2021",
      "venue": "Briefings in Bioinformatics"
    },
    {
      "citation_id": "46",
      "title": "Deep geometric representations for modeling effects of mutations on proteinprotein binding affinity",
      "authors": [
        "X Liu",
        "Y Luo",
        "P Li",
        "S Song",
        "J Peng"
      ],
      "year": "2021",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "47",
      "title": "Contrastive representation learning for electroencephalogram classification",
      "authors": [
        "M Mohsenvand",
        "M Izadi",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "48",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Graph contrastive learning with augmentations",
      "authors": [
        "Y You",
        "T Chen",
        "Y Sui",
        "T Chen",
        "Z Wang",
        "Y Shen"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "WGAN Domain Adaptation for EEG-Based Emotion Recognition",
      "authors": [
        "Y Luo",
        "S Zhang",
        "W Zheng",
        "B Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "52",
      "title": "A novel bi-hemispheric discrepancy model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "53",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "54",
      "title": "Plug-and-play domain adaptation for cross-subject EEG-based emotion recognition",
      "authors": [
        "L Zhao",
        "X Yan",
        "B Lu"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "55",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Multisource transfer learning for cross-subject EEG emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "57",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "58",
      "title": "A theory of learning from different domains",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "A Kulesza",
        "F Pereira",
        "J Vaughan"
      ],
      "year": "2010",
      "venue": "Machine learning"
    },
    {
      "citation_id": "59",
      "title": "Impossibility theorems for domain adaptation",
      "authors": [
        "S David",
        "T Lu",
        "T Luu",
        "D Pál"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "60",
      "title": "Analysis of representations for domain adaptation",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "F Pereira"
      ],
      "year": "2006",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "Generalizing to unseen domains via distribution matching",
      "authors": [
        "I Albuquerque",
        "J Monteiro",
        "M Darvishi",
        "T Falk",
        "I Mitliagkas"
      ],
      "year": "2019",
      "venue": "Generalizing to unseen domains via distribution matching",
      "arxiv": "arXiv:1911.00804"
    },
    {
      "citation_id": "62",
      "title": "Adversarial multiple source domain adaptation",
      "authors": [
        "H Zhao",
        "S Zhang",
        "G Wu",
        "J Moura",
        "J Costeira",
        "G Gordon"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "63",
      "title": "Domain adversarial neural networks for domain generalization: When it works and how to improve",
      "authors": [
        "A Sicilia",
        "X Zhao",
        "S Hwang"
      ],
      "year": "2023",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "64",
      "title": "Graph-Laplacian PCA: Closed-form solution and robustness",
      "authors": [
        "B Jiang",
        "C Ding",
        "B Luo",
        "J Tang"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "65",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "66",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "67",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "68",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "69",
      "title": "A large finer-grained affective computing EEG dataset",
      "authors": [
        "J Chen",
        "X Wang",
        "C Huang",
        "X Hu",
        "X Shen",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "70",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "71",
      "title": "PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "72",
      "title": "EEGFuseNet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional eeg with an application to emotion recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "73",
      "title": "Transfer learning for EEG-based brain-computer interfaces: A review of progress made since 2016",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "74",
      "title": "Can emotion be transferred?-a review on transfer learning for EEG-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "75",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "76",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "77",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "78",
      "title": "Kernel pca and de-noising in feature spaces",
      "authors": [
        "S Mika",
        "B Sch Ölkopf",
        "A Smola",
        "K -R. M Üller",
        "M Scholz",
        "G Rätsch"
      ],
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "79",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "80",
      "title": "Multi-class adaboost",
      "authors": [
        "J Zhu",
        "A Arbor",
        "T Hastie"
      ],
      "year": "2006",
      "venue": "Statistics & Its Interface"
    },
    {
      "citation_id": "81",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "82",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "83",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "84",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "85",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "86",
      "title": "FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "authors": [
        "B Zhang",
        "Y Wang",
        "W Hou",
        "H Wu",
        "J Wang",
        "M Okumura",
        "T Shinozaki"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "87",
      "title": "SoftMatch: Addressing the quantityquality trade-off in semi-supervised learning",
      "authors": [
        "H Chen",
        "R Tao",
        "Y Fan",
        "Y Wang",
        "J Wang",
        "B Schiele",
        "X Xie",
        "B Raj",
        "M Savvides"
      ],
      "year": "2023",
      "venue": "SoftMatch: Addressing the quantityquality trade-off in semi-supervised learning",
      "arxiv": "arXiv:2301.10921"
    },
    {
      "citation_id": "88",
      "title": "Weishan Ye is a master's student in the Department of Biomedical Engineering at Shenzhen University, China",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Zhiguo Zhang received his B.Eng. degree from Tianjin University in 2000, his M.Phil. degree from the University of Science and Technology of China in 2003, and his Ph"
    }
  ]
}