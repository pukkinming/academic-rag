{
  "paper_id": "2411.13917v1",
  "title": "Spikemo: Enhancing Emotion Recognition With Spiking Temporal Dynamics In Conversations",
  "published": "2024-11-21T08:07:26Z",
  "authors": [
    "Xiaomin Yu",
    "Feiyang Wang",
    "Ziyue Qiao"
  ],
  "keywords": [
    "Emotion recognition in conversations",
    "Spiking neural network",
    "Long-tailed distribution Oh",
    "yeah! You fell asleep! Don't go",
    "I'm sorry. You liked it? You really liked it?"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In affective computing, the task of Emotion Recognition in Conversations (ERC) has emerged as a focal area of research. The primary objective of this task is to predict emotional states within conversations by analyzing multimodal data including text, audio, and video. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations. To address this challenge, we have developed the SpikEmo framework, which is based on spiking neurons and employs a Semantic & Dynamic Twostage Modeling approach to more precisely capture the complex temporal features of multimodal emotional data. Additionally, to tackle the class imbalance and emotional semantic similarity problems in the ERC tasks, we have devised an innovative combination of loss functions that significantly enhances the model's performance when dealing with ERC data characterized by longtail distributions. Extensive experiments conducted on multiple ERC benchmark datasets demonstrate that SpikEmo significantly outperforms existing state-of-the-art methods in ERC tasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversations (ERC) task  [1] -  [12]  aims to recognize emotions in each utterance based on text, audio, and visual information from the speaker, which is crucial for several applications such as Human-Computer Interaction and Mental Health Analysis. In conversations, emotional information is presented through text, audio, and visual modalities, exhibiting complex dynamic characteristics over time  [13] ,  [14] . Thus, accurately capturing the temporal features in multimodal data is crucial for developing effective emotion recognition models. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations.\n\nTo model the temporal information in the data, we propose a two stage modality-temporal modeling approach. In the first stage at the modality level, we employ targeted feature extraction models for different modality data characteristics to obtain modal representations. In the second stage at the temporal level, we introduce a feature-level dynamic contextualized modeling module, which combines the sequence Time Fig.  1 . An example of the ERC task from the MELD dataset.\n\nmodeling capabilities of the Transformer with the temporal dynamics handling of Spiking Neural Networks (SNNs)  [15] -  [17] , making it possible to effectively capture the complex temporal information in text, audio, and visual data. The essence of the feature-level dynamic contextualized modeling module is the spiking behavior of pulse neurons, which mimics the temporal dynamic characteristics of biological neurons. This dynamic nature allows the feature-level dynamic contextualized modeling module to capture changes in input data at different times, highlighting moments particularly important for emotion state recognition.\n\nIn addition, there are two inherent problems in the ERC dataset  [18] ,  [19] . First of all, In the ERC dataset, it is challenging to accurately classify certain semantically  [20] ,  [21]  related emotions such as disgust and anger. Although these emotions show subtle differences at the literal level, they exhibit significant similarities in human cognitive, emotional, and physiological characteristics, making their differentiation particularly complex in ERC tasks. Furthermore, natural tendencies in emotional expression, sociocultural factors, and data collection biases contribute to a long-tail distribution  [22] -  [26]  characteristic in ERC datasets. This is manifested as frequent occurrences of common emotions in everyday conversations within the dataset, while some emotions are rare. The data distribution shown in Fig.  2  illustrates this point well. The sample imbalance inherent in ERC tasks  [22]  leads to models that are prone to overfitting common emotions while struggling to accurately recognize rarer emotional categories. To address this issue, we employ a class-balancing loss that assigns higher weights to difficult-to-classify samples, enhancing the model's focus on these samples. In summary, we propose the SpikEmo framework to address the core challenges in the ERC task. This framework effectively handles the temporal dynamics and semantic complexity in dialogue, processing the rich variations in emotions. Experimental validation on standard datasets such as MELD  [18]  and IEMOCAP  [19]  demonstrates the potential of this approach in accurately recognizing and understanding diverse emotional states. The main contributions of this paper can be summarized in three areas:\n\n• We introduce a new framework called SpikEmo tailored for the ERC task, proposing a two-phase modal-temporal modeling approach to represent different modal representations and extract spiking temporal features. • We utilize state-of-the-art modal fusion techniques and design a combined optimization objective for the ERC task to address the issues of class imbalance and semantic similarity between different emotions. • We conducted extensive experiments on the standard ERC task datasets, MELD and IEMOCAP. Our method surpasses the existing state-of-the-art methods based on discriminative models and LLMs, confirming the efficacy of our model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Emotion Recognition in Conversation (ERC)  [1] -  [14]  aims to identify the emotional expressions in human dialogues such as happiness, sadness, anger, surprise, etc  [18] ,  [19] . It requires a comprehensive understanding of each participant's emotional attitude based on information sources from multiple modalities, including textual content of dialogues, audio for vocal emotions, and visual cues for facial expressions. Recent advancements in pre-trained large language models (LLMs)  [31]  have demonstrated exceptional performance in natural language processing tasks. Based on these developments, some studies have adopted a generative architecture based on LLMs for Emotion Recognition in Conversation (ERC) tasks. The notable improvement in performance with this approach has validated the potential of LLMs in this domain.  [30]  However, the effectiveness of LLMs heavily relies on substantial computational resources and extensive training data, which may not be feasible in resource-constrained environments. Therefore, continuing to explore the viability of traditional discriminative models for efficiently handling ERC tasks remains a critical area of research. Additionally, certain intense emotions being rare in everyday conversations because of their specific occurrence conditions, and the ERC datasets consequently still an issue of class imbalance  [22] -  [26] .\n\nSpiking Neural Networks (SNNs) Spiking Neural Networks (SNNs) are a novel type of neural network model, characterized by their use of discrete spike sequences for computation and information transmission  [15] . Inspired by the biological neural systems, these networks communicate through mechanisms that emulate the pulse transmissions of biological neurons. In SNNs, neurons convert continuous input values into spike sequences, efficiently encoding and responding to temporal changes through their spiking activity. This time-based signal processing capability enables SNNs to naturally adapt to the dynamics of data, especially excelling in handling time-related data. In recent years, researchers have begun to explore the application of the Transformer model to SNNs. The Transformer model, having achieved significant success in various natural language processing and computer vision tasks, has been integrated with SNNs  [16] ,  [17] . The first spiking Transformer model introduced spiking self-attention mechanisms to model visual features using sparse Query, Key, and Value matrices. Additionally, some studies have proposed variants using spatio-temporal attention to better incorporate the attention mechanism within the Transformer. These innovations open up potential for the application of pulsed attention mechanisms in complex tasks across natural language processing and computer vision.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Method A. Semantic & Dynamic Two-Stage Modeling",
      "text": "In the task of ERC, we consider a set of dialogues\n\nwhere k is the total number of turns in the dialogue, s i is the speaker of the i-th turn, and U i is the corresponding utterance. Each utterance\n\nrepresent the textual, audio, and visual sequences of the utterance U i , respectively. Each utterance U i is associated with an emotional label y i ∈ C, where C is a predefined set of emotional categories. The task is to use the combined multimodal information of U i and its context information in D to predict its emotional label y i . 1) Modality level Semantic modeling: In this section, we extract modality features for text, audio, and video modalities. Specifically, for text modality, given the textual utterance sequences {U t 1 , U t 2 , ..., U t k }, we concatenate the sequences together and employ the pre-trained RoBERTa  [27]  to process it into textual representations-{F\n\nSimilarly, for audio modality, we employ OpenSMILE  [28]  to extract 6373-dimensional features for each utterance audio. Then, we employ Dia-logueRNN  [29]    employ VisExtNet  [11]  to extract facial expression features of interlocutors from multiple frames, excluding redundant scene-related information Subsequently, DialogueRNN  [29]  is also used to learn contextualized visual representations {F v 1 , F v 2 , ..., F v k } for each video frame. Since the processed representation sequences F t i , F a i , F v i for textual, audio, and visual modalities may vary in sequence lengths and representation dimensions, we apply masking and use linear layers to transform these sequences into a uniform shape, R l * h , for subsequent unified processing.\n\n2) Feature level dynamic contextualized modeling: We introduce the dynamic spiking weight adaptation (DSWA) module to extract temporal features of inputs. The spiking selfattention layer is the core of DSWA, utilizing the dynamics of SNNs to simulate dependencies and interactions over time. In SSA, the self-attention mechanism is adjusted to accommodate the characteristics of spiking neurons, thus more effectively capturing the dynamic changes in time-series data. For each modality m ∈ {t, a, v}, we first compute its queries Q m i , keys K m i , and values V m i , which are obtained by transforming the original modality feature F m i through linear layers within the Spikformer. The SSA layer can be represented as:\n\nwhere S is the step function, simulating the firing behavior of spiking neurons; τ is a scaling factor used to adjust the intensity of the spike response. This process captures dependencies between different time points and encodes them into spike signals.\n\nBy stacking multiple SSA layers followed by Linear and Normalization layers, we obtain the output spiking representation E m i , we use the softmax function to determine the importance weights for each modality feature. Then, We multiply these weights with the original modality feature to highlight those moments that are particularly important for recognizing the emotional state. Finally, we introduce a residual structure to merge the time-weighted enhanced features with the original modality features. The final modality features can be written as follows:\n\nwhere δ(•) represents softmax function.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Heterogeneous Modality Feature Fusion",
      "text": "Then, We employ a multimodal fusion network called MultiAttn  [11]  to integrate the characteristics of these different modalities. MultiAttn utilizes a bidirectional multihead cross-attention mechanism. A complete MultiAttn architecture comprises three main components: M ultiAttn text , M ultiAttn audio , and M ultiAttn visual , each consisting of T layers, where the output of first stage attention layer is fed into the subsequent layer as the new Query. For each modalities x, y, and z, where x, y, and z each correspond to one of the modalities in m ∈ {t, a, v}. The first stage of the attention mechanism can be written as follows:\n\nwhere l is the index of the layer number and F x,(0) i = F x i , MultiAtten represents the multi-head self-attention layer.\n\nThen, the output from the first stage is utilized as the new Query, which can be written as follows:\n\nBy stacking multiple MultiAtten layers followed by Linear layers for textual, audio, and visual modalities, the final representations F t i , F a i , F c i for each utterance is obtained.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Optimization Object",
      "text": "To enhance the performance of the model on the ERC task, we considered two critical issues in the optimization process: (1) In ERC tasks, certain emotions (such as anger and disgust) exhibit highly similar semantic expressions in multimodal contexts;  (2)  As shown in the figure, the datasets for ERC tasks often exhibit a severe long-tail distribution problem.\n\nTo solve the problem (1), we introduce the L corr . Specifically, we use L corr to effectively capture and utilize the complex correlations between different Modality features. L corr consists of three parts:\n\nwhere each row in Equations (  5 ),  (6) , and (  7 ) uses the HGR maximal correlation implemented by deep learning to compute the correlation between F x and F y , F x and F z , as well as F y and F z on the complete data.\n\nwhere α 1 , α 2 , α 3 are the loss wrights.\n\nTo solve the problem (2), we introduce DSC Loss  [22] . The DSC loss adapts a self-regulating mechanism that reduces the focus on easily predictable samples (i.e., samples with prediction probabilities close to 1 or 0) by using 1 -p as a scaling factor. DSC loss achieves more balanced model optimization when dealing with imbalanced datasets. DSC Loss can be defined as follows:\n\nwhere y i,c is the ground-truth label of the c-th sentence in the i-th dialogue. α is a weighting factor used to enhance the impact of tail data, and γ is a smoothing parameter. Then, we adopted the cross-entropy (CE) loss function to reduce the discrepancy between the real and predicted labels. CE loss can be written as follows:\n\nwhere C is the number of emotional category. Finally, the overall optimization objective can be written as follows:\n\nwhere λ 1 , λ 2 are the loss wrights, and θ is overall parameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Final Emotional Classification",
      "text": "In conclusion, the final emotional labels are obtained from the following formula:\n\nwhere p i [t] represents the probability of the t-th category.   I .\n\nThe MELD dataset, derived from the TV series \"Friends\", is a multimodal and multiparty dataset focusing on emotion recognition from raw dialogue transcripts, audio, and video. It features over 13,000 utterances and 1,400 conversations annotated with one of seven emotional categories: anger, disgust, fear, happiness, neutrality, sadness, and surprise.\n\nThe IEMOCAP dataset is a comprehensive multimodal dataset developed by the SAIL Lab at the University of Southern California. It consists of approximately 12 hours of audiovisual data involving ten actors who engage in improvised dyadic interactions. These scenarios are specifically crafted to elicit a variety of emotional states such as happiness, sadness, anger, excitement, frustration, and neutral.\n\n2) Discriminative models Baselines: Discriminative models for ERC tasks, focus on directly mapping multimodal emotional data inputs to output sentiment labels. The defining characteristic of these models is their capacity to achieve promising results even with limited training data. Typically, they leverage various advanced architectures to capture contextual and speaker-specific information effectively, utilizing techniques such as recurrent neural networks  [1] ,  [2] ,  [4] , graph-based models  [3] ,  [6] ,  [8] ,  [10] ,  [12] , and transformer variants  [5] ,  [11] ,  [12] . These approaches often involve modeling dialogue context  [1] -  [3] ,  [8] , speaker states  [2] , and cross-modal interactions  [3] ,  [8] ,  [10] ,  [11]  through methods like graph convolution  [3] , positional encoding  [6] , multimodal fusion  [7] -  [11] , and quantum-inspired  [7]  frameworks.\n\n3) LLMs-based Baselines.: LLMs-based models, through pre-training on extensive datasets, capture a wealth of linguistic features and contextual information. Recent studies  [30]  have highlighted the advantages of these models in ERC tasks.\n\nA key advantage of these models is their ability to generalize based on the extensive prior knowledge accumulated during the pre-training process, thereby enhancing performance on target tasks.\n\n4) Implementation Details: For the MELD dataset, we utilize a batch size of 64 and assign the loss weights λ 1 , λ 2 , and λ 3 as 0.3, 1, and 0.4 respectively. For the IEMOCAP dataset, the batch size is set at 32, with loss weights λ 1 , λ 2 , and λ 3 configured to 0.4, 0.6, and 1, respectively. Regarding generic settings applicable across all datasets, the MultiAttn model consistently employs 6 layers, whereas the Spikformer model uses 2 layers with a time step of T=32. The parameters α and γ for the L corr loss function are set to 1.5 and 0.5, respectively. Training is conducted over 50 epochs using the Adam optimizer with β 1 = 0.9 and β 2 = 0.99; the initial learning rate is set at 0.0001, with a decay rate of 0.95 every 10 epochs. Additionally, the L2 regularization weight is set at 0.00001. 5) Metrix: We utilized the Weighted F1 score as the evaluation metric in our experiments.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Main Result",
      "text": "We systematically compare the performance of our model against two categories of baselines: traditional discriminative methods and those based on large language models (LLMs).\n\nCompare with discriminative models Baselines. We compared our approach with the existing SOTA methods on the MELD and IEMOCAP, as shown in TABLE II. On the MELD dataset, our method improved by 1.49% compared to the previous best model; on the IEMOCAP dataset, it improved by 1.50%. Moreover, in tail categories, such as the Fear category in the MELD dataset and the Happiness category in the IEMOCAP dataset, our model outperformed the previous best models by 0.17% and 2.41%, respectively, further validating the effectiveness of our model.\n\nCompared with LLM-based Baselines. We evaluated our model against state-of-the-art Large Language Model (LLM) baselines. As outlined in TABLE III, our model features significantly fewer parameters. While our scores may not be the highest, the efficiency of our model is markedly superior given the reduced complexity. Specifically, our model has only 0.5% of the parameter count of the leading LLMs but achieves commendably close performance metrics. This demonstrates our model's enhanced efficiency and underscores its potential for applications requiring lower computational resources and energy consumption.   IV . First, we notice that when any component is removed from the model, its performance on all datasets decreases; conversely, integrating all modules results in the best performance of the method. This directly validates the significant contribution of SpikEmo. Furthermore, when the training process includes L DSC , we clearly observe significant improvements in Impact of Different Modality Settings. In Table  V , we provides a detailed analysis of the model's performance under three types of modality settings: unimodal, bimodal, and trimodal. The findings indicate that the model achieves the best emotion recognition performance when utilizing information from all three modalities (text, audio, and video). This result shows that the integration of multimodal information significantly enhances the accuracy of emotion recognition. It is noteworthy that when the model uses only a single modality as input, the performance based on text data significantly outperforms that based on either audio or video data alone. This phenomenon indicates that the text modality possesses a higher discriminatory ability and informational value in emotion recognition tasks. Further analysis reveals that when the model is trained with combined modalities, its performance remains at a high level as long as the text modality is included. This finding further emphasizes the importance of providing explicit emotional cues in the text, enabling the model to capture the speaker's emotional variations more effectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Hyperparameter Experiments",
      "text": "Impact of Time Constant T. The time constant T is a critical parameter in spiking neural networks, determining the response window of spiking neurons to input signals. In spiking neural networks, the firing behavior of neurons (i.e., the generation of spikes) depends not only on the current input but also on the historical input, and the time constant T is used to modulate this dependency. In the hyperparameter experiments, we conducted an in-depth investigation into the impact of the time constant T on model performance. Specif-",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "We proposed the SpikEmo framework to address key challenges in Emotion Recognition in Conversations (ERC). Our two-stage modality-temporal modeling approach integrates targeted feature extraction and feature-level dynamic contextualized modeling. This enables SpikEmo to effectively capture temporal features and highlight critical emotional transitions. Our approach also addresses key challenges such as class imbalance and semantic similarity, significantly improving performance on ERC tasks across multiple datasets. Experimental validation on MELD and IEMOCAP demonstrated that SpikEmo surpasses existing state-of-the-art models, including those based on large language models (LLMs), confirming its potential for enhancing emotion recognition in humancomputer interaction and mental health analysis.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of the ERC task from the MELD dataset.",
      "page": 1
    },
    {
      "caption": "Figure 2: illustrates this point well. The",
      "page": 1
    },
    {
      "caption": "Figure 2: The data distribution of MELD and IEMOCAP. Both datasets exhibit",
      "page": 2
    },
    {
      "caption": "Figure 3: Overall framework of SpikEmo. The modality level Semantic modeling extracts the contextualized modality representations, the feature level dynamic",
      "page": 3
    },
    {
      "caption": "Figure 4: The impact of T settings on model performance.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "Abstract—In affective computing,\nthe task of Emotion Recog-",
          "ziyuejoe@gmail.com": "Surprise\nAnger"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "(Positive)\n(Negative)"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "nition\nin Conversations\n(ERC)\nhas\nemerged\nas\na\nfocal\narea",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "of\nresearch. The primary\nobjective\nof\nthis\ntask is\nto predict",
          "ziyuejoe@gmail.com": "You liked it?"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "You fell asleep!"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "You really liked it?"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "emotional\nstates within conversations by analyzing multimodal",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "data\nincluding\ntext,\naudio,\nand\nvideo. While\nexisting\nstudies",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "have progressed in extracting and fusing representations\nfrom",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "multimodal\ndata,\nthey\noften\noverlook\nthe\ntemporal\ndynamics",
          "ziyuejoe@gmail.com": "Dialogue\nTime"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "in\nthe\ndata\nduring\nconversations. To\naddress\nthis\nchallenge,",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "we\nhave\ndeveloped\nthe\nSpikEmo\nframework, which\nis\nbased",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "on spiking neurons and employs a Semantic & Dynamic Two-",
          "ziyuejoe@gmail.com": "Oh, yeah!"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "Don’t go, I’m sorry."
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "stage Modeling approach to more precisely capture the complex",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "temporal\nfeatures\nof multimodal\nemotional data. Additionally,",
          "ziyuejoe@gmail.com": "Joy\nSadness"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "(Positive)\n(Negative)"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "to tackle the class\nimbalance and emotional\nsemantic similarity",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "problems in the ERC tasks, we have devised an innovative com-",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "Fig. 1. An example of\nthe ERC task from the MELD dataset."
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "bination of\nloss functions that significantly enhances the model’s",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "performance when dealing with ERC data characterized by long-",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "tail distributions. Extensive experiments conducted on multiple",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "ERC benchmark datasets demonstrate that SpikEmo significantly",
          "ziyuejoe@gmail.com": "modeling capabilities of\nthe Transformer with the\ntemporal"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "outperforms existing state-of-the-art methods in ERC tasks. Our",
          "ziyuejoe@gmail.com": "dynamics handling of Spiking Neural Networks (SNNs) [15]–"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "code is available at https://github.com/Yu-xm/SpikEmo.git.",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "[17], making it possible\nto effectively capture\nthe\ncomplex"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "Index Terms—Emotion recognition in conversations, Spiking",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "temporal\ninformation\nin\ntext,\naudio,\nand\nvisual\ndata. The"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "neural network, Long-tailed distribution.",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "essence of\nthe feature-level dynamic contextualized modeling"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "module is the spiking behavior of pulse neurons, which mimics"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "I.\nINTRODUCTION",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "the\ntemporal\ndynamic\ncharacteristics\nof\nbiological\nneurons."
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "Emotion Recognition\nin Conversations\n(ERC)\ntask\n[1]–",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "This dynamic nature\nallows\nthe\nfeature-level dynamic\ncon-"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "[12] aims\nto recognize emotions\nin each utterance based on",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "textualized modeling module to capture changes in input data"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "text,\naudio,\nand visual\ninformation from the\nspeaker, which",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "at different times, highlighting moments particularly important"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "is crucial\nfor\nseveral applications\nsuch as Human-Computer",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "for emotion state recognition."
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "Interaction\nand Mental Health Analysis.\nIn\nconversations,",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "In addition,\nthere\nare\ntwo inherent problems\nin the ERC"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "emotional\ninformation is presented through text,\naudio,\nand",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "dataset\n[18],\n[19].\nFirst\nof\nall,\nIn\nthe ERC dataset,\nit\nis"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "visual modalities, exhibiting complex dynamic characteristics",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "challenging to accurately classify certain semantically [20],"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "over\ntime\n[13],\n[14]. Thus,\naccurately\ncapturing\nthe\ntem-",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "[21]\nrelated\nemotions\nsuch\nas\ndisgust\nand\nanger. Although"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "poral\nfeatures\nin multimodal data\nis\ncrucial\nfor developing",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "these emotions show subtle differences at the literal level, they"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "effective emotion recognition models. While existing studies",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "exhibit significant similarities in human cognitive, emotional,"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "have progressed in extracting and fusing representations from",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "and physiological characteristics, making their differentiation"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "multimodal data,\nthey often overlook the temporal dynamics",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "particularly complex in ERC tasks. Furthermore, natural\nten-"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "in the data during conversations.",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "dencies in emotional expression, sociocultural factors, and data"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "To model\nthe temporal\ninformation in the data, we propose",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "collection biases contribute to a long-tail distribution [22]–[26]"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "a\ntwo\nstage modality-temporal modeling\napproach.\nIn\nthe",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "characteristic in ERC datasets. This is manifested as frequent"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "first\nstage at\nthe modality level, we employ targeted feature",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "occurrences of common emotions\nin everyday conversations"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "extraction models\nfor different modality data\ncharacteristics",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "within the dataset, while\nsome\nemotions\nare\nrare. The data"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "to obtain modal\nrepresentations.\nIn the\nsecond stage\nat\nthe",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "distribution shown in Fig. 2 illustrates\nthis point well. The"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "temporal\nlevel, we\nintroduce\na\nfeature-level\ndynamic\ncon-",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "sample imbalance inherent\nin ERC tasks [22] leads to models"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "textualized modeling module, which combines\nthe sequence",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "that are prone to overfitting common emotions while struggling"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "",
          "ziyuejoe@gmail.com": "to accurately recognize rarer emotional categories. To address"
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "∗Equal Contribution",
          "ziyuejoe@gmail.com": ""
        },
        {
          "8207210931@csu.edu.cn\nyuxm02@gmail.com": "†Corresponding Author",
          "ziyuejoe@gmail.com": "this issue, we employ a class-balancing loss that assigns higher"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "focus on these samples.",
          "be feasible in resource-constrained environments. Therefore,": "continuing to explore the viability of traditional discriminative"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "models\nfor efficiently handling ERC tasks\nremains a critical"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "area of\nresearch. Additionally, certain intense emotions being"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "rare in everyday conversations because of their specific occur-"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "rence conditions, and the ERC datasets consequently still an"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "issue of class imbalance [22]–[26]."
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "Spiking Neural Networks\n(SNNs) Spiking Neural Net-"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "works\n(SNNs)\nare\na\nnovel\ntype\nof\nneural\nnetwork model,"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "characterized\nby\ntheir\nuse\nof\ndiscrete\nspike\nsequences\nfor"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "computation and information transmission [15].\nInspired by"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "the biological neural\nsystems,\nthese networks\ncommunicate"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "through mechanisms\nthat\nemulate\nthe\npulse\ntransmissions"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "Fig. 2.\nThe data distribution of MELD and IEMOCAP. Both datasets exhibit",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "of biological neurons.\nIn SNNs, neurons convert continuous"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "a noticeable long-tail distribution.",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "input\nvalues\ninto\nspike\nsequences,\nefficiently\nencoding\nand"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "responding to temporal changes through their spiking activity."
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "In summary, we propose\nthe SpikEmo framework to ad-",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "This time-based signal processing capability enables SNNs to"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "dress\nthe\ncore\nchallenges\nin the ERC task. This\nframework",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "naturally adapt\nto the dynamics of data, especially excelling"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "effectively handles the temporal dynamics and semantic com-",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "in\nhandling\ntime-related\ndata.\nIn\nrecent\nyears,\nresearchers"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "plexity in dialogue, processing the rich variations in emotions.",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "have\nbegun\nto\nexplore\nthe\napplication\nof\nthe Transformer"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "Experimental validation on standard datasets\nsuch as MELD",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "model\nto SNNs. The Transformer model,\nhaving\nachieved"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "[18]\nand IEMOCAP [19] demonstrates\nthe potential of\nthis",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "significant\nsuccess\nin\nvarious\nnatural\nlanguage\nprocessing"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "approach in accurately recognizing and understanding diverse",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "and computer vision tasks, has been integrated with SNNs"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "emotional states. The main contributions of\nthis paper can be",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "[16],\n[17]. The first\nspiking Transformer model\nintroduced"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "summarized in three areas:",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "spiking\nself-attention mechanisms\nto model\nvisual\nfeatures"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "• We introduce a new framework called SpikEmo tailored",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "using sparse Query, Key,\nand Value matrices. Additionally,"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "for the ERC task, proposing a two-phase modal-temporal",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "some\nstudies\nhave\nproposed\nvariants\nusing\nspatio-temporal"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "modeling approach to represent different modal represen-",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "attention to better incorporate the attention mechanism within"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "tations and extract spiking temporal\nfeatures.",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "the Transformer. These innovations open up potential\nfor\nthe"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "• We utilize\nstate-of-the-art modal\nfusion techniques\nand",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "application of pulsed attention mechanisms in complex tasks"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "design a\ncombined optimization objective\nfor\nthe ERC",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "across natural\nlanguage processing and computer vision."
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "task to address the issues of class imbalance and semantic",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "similarity between different emotions.",
          "be feasible in resource-constrained environments. Therefore,": "III. METHOD"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "• We\nconducted\nextensive\nexperiments\non\nthe\nstandard",
          "be feasible in resource-constrained environments. Therefore,": "A. Semantic & Dynamic Two-stage Modeling"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "ERC task datasets, MELD and IEMOCAP. Our method",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "In the task of ERC, we consider a set of dialogues D ="
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "surpasses\nthe existing state-of-the-art methods based on",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "{(s1, U1) , (s2, U2) , ..., (sk, Uk)}, where k is the total number"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "discriminative models and LLMs, confirming the efficacy",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "is the speaker of the i-th turn, and\nof turns in the dialogue, si"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "of our model.",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "includes\nUi\nis the corresponding utterance. Each utterance Ui"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "= {U t\n, U v\n, U v"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "three modalities—Ui\ni , U a\ni }, where U t\ni , U a\ni"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "II. RELATED WORKS",
          "be feasible in resource-constrained environments. Therefore,": ""
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "represent\nthe\ntextual,\naudio,\nand\nvisual\nsequences\nof\nthe"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "Emotion Recognition\nin Conversation\n(ERC)\n[1]–[14]",
          "be feasible in resource-constrained environments. Therefore,": "respectively. Each\nis\nassociated\nutterance Ui,\nutterance Ui"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "aims to identify the emotional expressions in human dialogues",
          "be feasible in resource-constrained environments. Therefore,": "with\nan\nemotional\nlabel\nis\na\npredefined\nyi ∈ C, where C"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "such as happiness,\nsadness, anger,\nsurprise, etc [18],\n[19].\nIt",
          "be feasible in resource-constrained environments. Therefore,": "set of emotional categories. The task is to use the combined"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "requires a comprehensive understanding of each participant’s",
          "be feasible in resource-constrained environments. Therefore,": "multimodal\nand its context\ninformation in\ninformation of Ui"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "emotional attitude based on information sources from multiple",
          "be feasible in resource-constrained environments. Therefore,": "D to predict\nits emotional\nlabel yi."
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "modalities,\nincluding textual content of dialogues, audio for",
          "be feasible in resource-constrained environments. Therefore,": "1) Modality level Semantic modeling:\nIn this section, we"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "vocal emotions, and visual cues for facial expressions. Recent",
          "be feasible in resource-constrained environments. Therefore,": "extract modality features\nfor\ntext,\naudio,\nand video modal-"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "advancements\nin pre-trained large\nlanguage models\n(LLMs)",
          "be feasible in resource-constrained environments. Therefore,": "ities.\nSpecifically,\nfor\ntext modality,\ngiven\nthe\ntextual\nut-"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "[31]\nhave\ndemonstrated\nexceptional\nperformance\nin\nnatural",
          "be feasible in resource-constrained environments. Therefore,": "{U t\nconcatenate\nthe\nse-\nterance\nsequences\n2, ..., U t\n1, U t"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "k}, we"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "language processing tasks. Based on these developments, some",
          "be feasible in resource-constrained environments. Therefore,": "quences\ntogether and employ the pre-trained RoBERTa [27]"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "studies have adopted a generative architecture based on LLMs",
          "be feasible in resource-constrained environments. Therefore,": "to process it\ninto textual\nrepresentations—{F t\n2, ..., F t\n1, F t"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "k} ="
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "for Emotion Recognition in Conversation (ERC)\ntasks. The",
          "be feasible in resource-constrained environments. Therefore,": "Similarly,\nfor\naudio modality,\nRoBERTa({U t\n1, U t\n2, ..., U t"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "k}),"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "notable improvement\nin performance with this approach has",
          "be feasible in resource-constrained environments. Therefore,": "we\nemploy OpenSMILE [28]\nto\nextract\n6373-dimensional"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "validated the potential of LLMs in this domain. [30] However,",
          "be feasible in resource-constrained environments. Therefore,": "features\nfor\neach\nutterance\naudio. Then, we\nemploy Dia-"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "the effectiveness of LLMs heavily relies on substantial compu-",
          "be feasible in resource-constrained environments. Therefore,": "logueRNN [29] to capture contextualized audio representations"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "tational\nresources and extensive training data, which may not",
          "be feasible in resource-constrained environments. Therefore,": "{F a\n1 , F a\n2 , ..., F a"
        },
        {
          "weights to difficult-to-classify samples, enhancing the model’s": "",
          "be feasible in resource-constrained environments. Therefore,": "k } for each audio clip. For visual modality, we"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Visual Branch\nVisual Branch": "SSA"
        },
        {
          "Visual Branch\nVisual Branch": "DSWA\nMultiAttn\nVisual"
        },
        {
          "Visual Branch\nVisual Branch": "Encoder"
        },
        {
          "Visual Branch\nVisual Branch": "Fig. 3. Overall framework of SpikEmo. The modality level Semantic modeling extracts the contextualized modality representations,\nthe feature level dynamic"
        },
        {
          "Visual Branch\nVisual Branch": "contextualized modeling extracts cross-modality temporal\ninformation, and the Lcorr\nand LDSC losses are proposed to capture correlations and avoid the"
        },
        {
          "Visual Branch\nVisual Branch": "long-tail problem in model\ntraining."
        },
        {
          "Visual Branch\nVisual Branch": "employ VisExtNet\n[11]\nto extract\nfacial\nexpression features\nmodality features. The final modality features can be written"
        },
        {
          "Visual Branch\nVisual Branch": "of\ninterlocutors\nfrom multiple\nframes,\nexcluding\nredundant\nas follows:"
        },
        {
          "Visual Branch\nVisual Branch": "scene-related\ninformation Subsequently, DialogueRNN [29]"
        },
        {
          "Visual Branch\nVisual Branch": "= δ(Em\n) · F m\n+ F m\n,\nF m\n(2)"
        },
        {
          "Visual Branch\nVisual Branch": "i\ni\ni\ni\nis\nalso\nused\nto\nlearn\ncontextualized\nvisual\nrepresentations"
        },
        {
          "Visual Branch\nVisual Branch": "{F v\n1 , F v\n2 , ..., F v"
        },
        {
          "Visual Branch\nVisual Branch": "k } for each video frame. Since the processed\nwhere δ(·) represents softmax function."
        },
        {
          "Visual Branch\nVisual Branch": ", F v"
        },
        {
          "Visual Branch\nVisual Branch": "for\ntextual,\naudio,\nand\ni , F a\ni"
        },
        {
          "Visual Branch\nVisual Branch": "B. Heterogeneous Modality Feature Fusion\nvisual modalities may vary in sequence lengths and represen-"
        },
        {
          "Visual Branch\nVisual Branch": "tation dimensions, we\napply masking and use\nlinear\nlayers"
        },
        {
          "Visual Branch\nVisual Branch": "Then, We\nemploy\na multimodal\nfusion\nnetwork\ncalled"
        },
        {
          "Visual Branch\nVisual Branch": "to transform these sequences into a uniform shape, Rl∗h,\nfor"
        },
        {
          "Visual Branch\nVisual Branch": "MultiAttn\n[11]\nto\nintegrate\nthe\ncharacteristics\nof\nthese\ndif-"
        },
        {
          "Visual Branch\nVisual Branch": "subsequent unified processing."
        },
        {
          "Visual Branch\nVisual Branch": "ferent modalities. MultiAttn\nutilizes\na\nbidirectional multi-"
        },
        {
          "Visual Branch\nVisual Branch": "2) Feature\nlevel\ndynamic\ncontextualized modeling: We\nhead\ncross-attention mechanism. A complete MultiAttn\nar-"
        },
        {
          "Visual Branch\nVisual Branch": "introduce\nthe\ndynamic\nspiking weight\nadaptation\n(DSWA)\nchitecture comprises three main components: M ultiAttntext,"
        },
        {
          "Visual Branch\nVisual Branch": "module to extract temporal features of inputs. The spiking self-\nM ultiAttnaudio, and M ultiAttnvisual, each consisting of T"
        },
        {
          "Visual Branch\nVisual Branch": "attention layer is the core of DSWA, utilizing the dynamics of\nlayers, where the output of first stage attention layer is fed into"
        },
        {
          "Visual Branch\nVisual Branch": "SNNs to simulate dependencies and interactions over time. In\nthe subsequent\nlayer as\nthe new Query. For each modalities"
        },
        {
          "Visual Branch\nVisual Branch": "SSA, the self-attention mechanism is adjusted to accommodate\nx, y, and z, where x, y, and z each correspond to one of\nthe"
        },
        {
          "Visual Branch\nVisual Branch": "the\ncharacteristics of\nspiking neurons,\nthus more\neffectively\nmodalities\nin m ∈ {t, a, v}. The first\nstage of\nthe attention"
        },
        {
          "Visual Branch\nVisual Branch": "capturing the dynamic changes\nin time-series data. For each\nmechanism can be written as follows:"
        },
        {
          "Visual Branch\nVisual Branch": "modality m ∈ {t, a, v}, we first compute its queries Qm\n, keys\ni"
        },
        {
          "Visual Branch\nVisual Branch": "K m\n, and values V m\n, which are obtained by transforming the"
        },
        {
          "Visual Branch\nVisual Branch": "i\ni"
        },
        {
          "Visual Branch\nVisual Branch": "F xy,(l)\n, F y,(l−1)\n, F y,(l−1)\n)\n= MultiAtten(F x,(l−1)\n(3)"
        },
        {
          "Visual Branch\nVisual Branch": "i\ni\ni\ni\noriginal modality feature F m\nthrough linear layers within the"
        },
        {
          "Visual Branch\nVisual Branch": "i"
        },
        {
          "Visual Branch\nVisual Branch": "Spikformer. The SSA layer can be represented as:"
        },
        {
          "Visual Branch\nVisual Branch": "= F x\nwhere l\nis\nthe index of\nthe layer number and F x,(0)\n,"
        },
        {
          "Visual Branch\nVisual Branch": "i\ni"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "STATISTICAL DATA FOR THE MELD AND IEMOCAP DATASETS. NOTE: T"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "to effectively capture and utilize the com-\ncally, we use Lcorr",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "REPRESENTS TEXT, A REPRESENTS AUDIO, AND V REPRESENTS VIDEO."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "plex correlations between different Modality features. Lcorr",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "consists of\nthree parts:",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "Dialogue\nUtterances\nAttributes"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "Datasets"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "Train\nValid\nTest\nTrain\nValid\nTest\nModality\nClasses"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "1 2\n(5)\nl1 = −EX,Y ∼D[( (cid:98)Fx)T (cid:98)Fy] +\ntr[cov( (cid:98)Fx)cov( (cid:98)Fy)]",
          "TABLE I": "MELD\n1038\n114\n280\n9989\n1109\n2610\nT, A, V\n6"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "108\n12\n31\n5163\n647\n1623\nT, A, V\n7\nIEMOCAP"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "1 2\n(6)\ntr[cov( (cid:98)Fx)cov( (cid:98)Fz)]\nl2 = −EX,Z∼D[( (cid:98)Fx)T (cid:98)Fz] +",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "IV. EXPERIMENTS"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "1 2\n(7)\nl3 = −EY,Z∼D[( (cid:98)Fy)T (cid:98)Fz] +\ntr[cov( (cid:98)Fy)cov( (cid:98)Fz)]",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "A. Setup"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "where each row in Equations (5),\n(6), and (7) uses the HGR",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "1) Datasets: To verify the model’s generality, we conducted"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "maximal correlation implemented by deep learning to compute",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "experiments\nusing\nthe widely\nrecognized\nstandard\ndatasets"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "the correlation between (cid:98)Fx and (cid:98)Fy, (cid:98)Fx and (cid:98)Fz, as well as (cid:98)Fy",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "MELD and IEMOCAP. The statistical overview of the related"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "and (cid:98)Fz on the complete data.",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "datasets is presented in Table I."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "The MELD dataset, derived from the TV series ”Friends”,"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "(8)\nLcorr = α1 · l1 + α2 · l2 + α3 · l3,",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "is a multimodal and multiparty dataset\nfocusing on emotion"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "where α1, α2, α3 are the loss wrights.",
          "TABLE I": "recognition from raw dialogue transcripts, audio, and video."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "To solve\nthe problem (2), we\nintroduce DSC Loss\n[22].",
          "TABLE I": "It\nfeatures\nover\n13,000\nutterances\nand\n1,400\nconversations"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "The DSC loss adapts a self-regulating mechanism that reduces",
          "TABLE I": "annotated with\none\nof\nseven\nemotional\ncategories:\nanger,"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "the\nfocus\non\neasily\npredictable\nsamples\n(i.e.,\nsamples with",
          "TABLE I": "disgust,\nfear, happiness, neutrality, sadness, and surprise."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "prediction probabilities\nclose\nto 1 or 0) by using 1 − p as",
          "TABLE I": "The\nIEMOCAP dataset\nis\na\ncomprehensive multimodal"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "a\nscaling\nfactor. DSC loss\nachieves more\nbalanced model",
          "TABLE I": "dataset\ndeveloped\nby\nthe\nSAIL Lab\nat\nthe University\nof"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "optimization when\ndealing with\nimbalanced\ndatasets. DSC",
          "TABLE I": "Southern California.\nIt\nconsists\nof\napproximately\n12\nhours"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "Loss can be defined as follows:",
          "TABLE I": "of\naudiovisual data\ninvolving ten actors who engage\nin im-"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "provised dyadic interactions. These scenarios are specifically"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "crafted to elicit a variety of emotional states such as happiness,"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "(cid:20)\n(cid:21)",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "2 · (1 − pi,c)α · pi,c · yi,c + γ",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "1 N\nN(cid:88) i\n1 −\n,\n(9)\nLDSC =",
          "TABLE I": "sadness, anger, excitement,\nfrustration, and neutral."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "(1 − pi,c)α · pi,c + yi,c + γ",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "=1",
          "TABLE I": "2) Discriminative models Baselines: Discriminative mod-"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "els\nfor ERC tasks,\nfocus\non\ndirectly mapping multimodal"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "is\nthe ground-truth label of\nthe\nc-th sentence\nin\nwhere yi,c",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "emotional data inputs to output sentiment\nlabels. The defining"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "the i-th dialogue. α is a weighting factor used to enhance the",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "characteristic\nof\nthese models\nis\ntheir\ncapacity\nto\nachieve"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "impact of\ntail data, and γ is a smoothing parameter.",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "promising results\neven with limited training data. Typically,"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "Then, we adopted the cross-entropy (CE)\nloss\nfunction to",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "they leverage various advanced architectures\nto capture con-"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "reduce the discrepancy between the real and predicted labels.",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "textual and speaker-specific information effectively, utilizing"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "CE loss can be written as follows:",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "techniques\nsuch\nas\nrecurrent\nneural\nnetworks\n[1],\n[2],\n[4],"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "graph-based models\n[3],\n[6],\n[8],\n[10],\n[12], and transformer"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "1 N\nN(cid:88) i\nC(cid:88) c\n(10)\nyi,c log pi,c,\nLCE = −",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "variants [5],\n[11],\n[12]. These approaches often involve mod-"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "=1\n=1",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "eling\ndialogue\ncontext\n[1]–[3],\n[8],\nspeaker\nstates\n[2],\nand"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "where C is the number of emotional category.",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "cross-modal\ninteractions\n[3],\n[8],\n[10],\n[11]\nthrough methods"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "Finally,\nthe overall optimization objective can be written as",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "like graph convolution [3], positional encoding [6], multimodal"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "follows:",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "fusion [7]–[11], and quantum-inspired [7]\nframeworks."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "3) LLMs-based Baselines.: LLMs-based models,\nthrough"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "L = min\n(11)\n{LCE + λ1 · LDSC + λ2 · Lcorr} ,",
          "TABLE I": "pre-training on extensive datasets, capture a wealth of linguis-"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "θ",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "tic\nfeatures\nand contextual\ninformation. Recent\nstudies\n[30]"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "where λ1, λ2 are the loss wrights, and θ is overall parameters.",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "have highlighted the advantages of these models in ERC tasks."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "A key advantage of these models is their ability to generalize"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "D. Final Emotional Classification",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "based on the extensive prior knowledge accumulated during"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "In conclusion,\nthe final emotional\nlabels are obtained from the",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "the pre-training process,\nthereby enhancing performance on"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "following formula:",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "target\ntasks."
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "4)\nImplementation Details:\nFor\nthe MELD dataset, we"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "(12)\npi = δ(ϕMLP([ (cid:98)F t\n; (cid:98)F c\ni ; (cid:98)F a\ni ])),",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "utilize a batch size of 64 and assign the loss weights λ1, λ2,"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "as 0.3, 1,\nand 0.4 respectively. For\nthe\nIEMOCAP\nand λ3"
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "(13)\nyi = arg max\n(pi[t]),",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "t",
          "TABLE I": ""
        },
        {
          "To solve the problem (1), we introduce the Lcorr. Specifi-": "",
          "TABLE I": "dataset,\nthe batch size is set at 32, with loss weights λ1, λ2,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Disgust Angry W-F1 Happiness Sadness Neutral Anger Excitement Frustration W-F1"
        },
        {
          "TABLE II": "38.40"
        },
        {
          "TABLE II": "46.76"
        },
        {
          "TABLE II": "43.03"
        },
        {
          "TABLE II": "48.88"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "43.17"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "42.55"
        },
        {
          "TABLE II": "47.82"
        },
        {
          "TABLE II": "52.91"
        },
        {
          "TABLE II": "48.52"
        },
        {
          "TABLE II": "54.20"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "initial"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "is set at"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "learning rate is set at 0.0001, with a decay rate of 0.95 every",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "IEMOCAP DATASETS."
        },
        {
          "Adam optimizer with β1": "10 epochs. Additionally,",
          "= 0.9\n= 0.99;\nand β2": "the L2 regularization weight",
          "the\ninitial": "is set at",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "0.00001.",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "IEMOCAP"
        },
        {
          "Adam optimizer with β1": "5) Metrix: We\nutilized",
          "= 0.9\n= 0.99;\nand β2": "F1\nthe Weighted\nscore",
          "the\ninitial": "as\nthe",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "LoRA + Backbone"
        },
        {
          "Adam optimizer with β1": "evaluation metric in our experiments.",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "17.98"
        },
        {
          "Adam optimizer with β1": "B. Main Result",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "52.88"
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "We systematically compare the performance of our model",
          "the\ninitial": "",
          "TABLE III": "55.81"
        },
        {
          "Adam optimizer with β1": "against\ntwo categories of baselines:",
          "= 0.9\n= 0.99;\nand β2": "traditional discriminative",
          "the\ninitial": "",
          "TABLE III": "55.96"
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "methods and those based on large language models (LLMs).",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "LoRA + InstructERC"
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "Compare with discriminative models Baselines. We com-",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "36.04"
        },
        {
          "Adam optimizer with β1": "pared our approach with the existing SOTA methods on the",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "67.54"
        },
        {
          "Adam optimizer with β1": "MELD and IEMOCAP, as shown in TABLE II. On the MELD",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "64.17"
        },
        {
          "Adam optimizer with β1": "dataset,\nour method",
          "= 0.9\n= 0.99;\nand β2": "improved\nby\n1.49% compared",
          "the\ninitial": "to\nthe",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "71.39"
        },
        {
          "Adam optimizer with β1": "previous best model; on the IEMOCAP dataset, it improved by",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": "Ours (Discriminative model)"
        },
        {
          "Adam optimizer with β1": "1.50%. Moreover,",
          "= 0.9\n= 0.99;\nand β2": "in tail categories, such as the Fear category",
          "the\ninitial": "",
          "TABLE III": ""
        },
        {
          "Adam optimizer with β1": "in\nthe MELD dataset",
          "= 0.9\n= 0.99;\nand β2": "and\nthe Happiness\ncategory",
          "the\ninitial": "in\nthe",
          "TABLE III": "71.50"
        },
        {
          "Adam optimizer with β1": "IEMOCAP dataset, our model outperformed the previous best",
          "= 0.9\n= 0.99;\nand β2": "",
          "the\ninitial": "",
          "TABLE III": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MELD": "",
          "IEMOCAP": ""
        },
        {
          "MELD": "Sadness",
          "IEMOCAP": "Anger"
        },
        {
          "MELD": "37.68",
          "IEMOCAP": "66.00"
        },
        {
          "MELD": "37.34",
          "IEMOCAP": "66.00"
        },
        {
          "MELD": "37.31",
          "IEMOCAP": "64.98"
        },
        {
          "MELD": "33.23",
          "IEMOCAP": "65.80"
        },
        {
          "MELD": "40.00",
          "IEMOCAP": "66.67"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "65.92\n71.50\nText + Audio + Visual"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "ically, we set\nthe time constant T to 2, 4, 8, 16, 32, and 64,"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "respectively, and the experimental\nresults are shown in Fig.4."
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "From the figure,\nit\nis clearly observed that as the value of T"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "increases,\nthe model’s performance exhibits a distinct upward"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "trend. This phenomenon indicates that a longer\ntime constant"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "helps better capture the long-term temporal dependencies\nin"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "the\ninput data,\nthereby significantly improving the\naccuracy"
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": "of emotion recognition."
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        },
        {
          "Text + Visual\n63.22\n65.15": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "the performance of SpikEmo on the MELD and IEMOCAP",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "TABLE V"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "THE RESULTS UNDER DIFFERENT MODALITY SETTINGS."
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "datasets,\nespecially\nin\nthe\n”Fear”\ncategory\nof MELD and",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "the ”Happiness” category of\nIEMOCAP,\nindicating that LDSC",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Modality\nMELD\nIEMOCAP"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "plays an active role in improving the issue of class imbalance.",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "Impact of Feature Level Dynamic Contextualized Mod-",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Audio\n35.91\n40.67"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "eling. To investigate\nthe\nimpact of Feature Level Dynamic",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Visual\n31.27\n27.44"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Text\n59.56\n64.77"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "Contextualized Modeling, we\nremoved\nthe\nparts\nrelated\nto",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Audio + Visual\n39.98\n46.80"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "it.\nIn the experimental\nresults presented in TABLE IV. After",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Text + Audio\n62.22\n67.06"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "removing this component,\nthe categorical performance on the",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Text + Visual\n63.22\n65.15"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "MELD and IEMOCAP datasets\nsignificantly decreases. This",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "65.92\n71.50\nText + Audio + Visual"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "effect\nis\nreflected\nin\nthe\ndecrease\nof\nthe Weight-F1\nscores",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "for\nboth\ndatasets, which\ndropped\nby\n3.48% and\n2.47%,",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "respectively.",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "ically, we set\nthe time constant T to 2, 4, 8, 16, 32, and 64,"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "Impact of Different Modality Settings.\nIn Table V, we",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "respectively, and the experimental\nresults are shown in Fig.4."
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "provides a detailed analysis of the model’s performance under",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "From the figure,\nit\nis clearly observed that as the value of T"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "three\ntypes\nof modality\nsettings:\nunimodal,\nbimodal,\nand",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "increases,\nthe model’s performance exhibits a distinct upward"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "trimodal. The findings\nindicate\nthat\nthe model\nachieves\nthe",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "trend. This phenomenon indicates that a longer\ntime constant"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "best\nemotion recognition performance when utilizing infor-",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "helps better capture the long-term temporal dependencies\nin"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "mation from all\nthree modalities (text, audio, and video). This",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "the\ninput data,\nthereby significantly improving the\naccuracy"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "result\nshows\nthat\nthe\nintegration of multimodal\ninformation",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "of emotion recognition."
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "significantly enhances the accuracy of emotion recognition. It",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "is noteworthy that when the model uses only a single modality",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "as\ninput,\nthe\nperformance\nbased\non\ntext\ndata\nsignificantly",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "outperforms\nthat based on either audio or video data alone.",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "This phenomenon indicates\nthat\nthe\ntext modality possesses",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "a\nhigher\ndiscriminatory\nability\nand\ninformational\nvalue\nin",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "emotion recognition tasks. Further analysis reveals that when",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "the model is trained with combined modalities, its performance",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "remains at a high level as long as the text modality is included.",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "This finding further emphasizes\nthe importance of providing",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "explicit\nemotional\ncues\nin\nthe\ntext,\nenabling\nthe model\nto",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "capture the speaker’s emotional variations more effectively.",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "D. Hyperparameter Experiments",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "Impact\nof Time Constant T. The\ntime\nconstant T\nis",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "Fig. 4.\nThe impact of T settings on model performance."
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "a critical parameter\nin spiking neural networks, determining",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "the response window of\nspiking neurons\nto input\nsignals.\nIn",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "V. CONCLUSION"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "spiking neural networks,\nthe firing behavior of neurons\n(i.e.,",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": ""
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "the\ngeneration\nof\nspikes)\ndepends\nnot\nonly\non\nthe\ncurrent",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "We proposed the SpikEmo framework to address key chal-"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "input but also on the historical\ninput, and the time constant",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "lenges in Emotion Recognition in Conversations (ERC). Our"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "T is used to modulate this dependency. In the hyperparameter",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "two-stage modality-temporal modeling\napproach\nintegrates"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "experiments, we conducted an in-depth investigation into the",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "targeted feature extraction and feature-level dynamic contextu-"
        },
        {
          "SpikEmo\n79.53\n58.40\n21.95\n40.00\n63.89\n28.85\n54.20": "impact of the time constant T on model performance. Specif-",
          "65.92\n58.16\n82.17\n72.61\n66.67\n79.01\n64.82\n71.50": "alized modeling. This enables SpikEmo to effectively capture"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "dyadic motion capture database[J]. Language resources and evaluation,"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "Our\napproach\nalso\naddresses\nkey\nchallenges\nsuch\nas\nclass",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "2008, 42: 335-359."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "imbalance\nand\nsemantic\nsimilarity,\nsignificantly\nimproving",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "[20] Wang L, Wu J, Huang S L, et al. An efficient approach to informative"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "performance on ERC tasks across multiple datasets. Experi-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "feature extraction from multimodal data[C]//Proceedings of\nthe AAAI"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "Conference on Artificial\nIntelligence. 2019, 33(01): 5281-5288."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "mental validation on MELD and IEMOCAP demonstrated that",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "[21] Ma F, Huang S L, Zhang L. An efficient approach for audio-visual emo-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "SpikEmo surpasses existing state-of-the-art models,\nincluding",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "tion recognition with missing labels\nand missing modalities[C]//2021"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "those\nbased\non\nlarge\nlanguage models\n(LLMs),\nconfirming",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "IEEE international conference on multimedia and Expo (ICME). IEEE,"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "2021: 1-6."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "its\npotential\nfor\nenhancing\nemotion\nrecognition\nin\nhuman-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[22]",
          "IEMOCAP:\nInteractive emotional": "Li X, Sun X, Meng Y, et al. Dice loss for data-imbalanced NLP tasks[J]."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "computer\ninteraction and mental health analysis.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "arXiv preprint arXiv:1911.02855, 2019."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[23]",
          "IEMOCAP:\nInteractive emotional": "Lin T Y, Goyal P, Girshick R, et al. Focal\nloss for dense object detec-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "tion[C]//Proceedings of\nthe IEEE international conference on computer"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "REFERENCES",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "vision. 2017: 2980-2988."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[24]",
          "IEMOCAP:\nInteractive emotional": "Zhang Y, Kang B, Hooi B, et al. Deep long-tailed learning: A survey[J]."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[1]\nPoria S, Cambria E, Hazarika D,\net\nal. Context-dependent\nsentiment",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023,"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "analysis\nin user-generated videos[C]//Proceedings of\nthe 55th annual",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "45(9): 10795-10816."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "meeting of the association for computational linguistics (volume 1: Long",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[25]",
          "IEMOCAP:\nInteractive emotional": "Zhang S, Li Z, Yan S, et al. Distribution alignment: A unified frame-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "papers). 2017: 873-883.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "work for\nlong-tail visual\nrecognition[C]//Proceedings of\nthe IEEE/CVF"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[2] Majumder N, Poria S, Hazarika D,\net\nal. Dialoguernn: An attentive",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "conference on computer vision and pattern recognition. 2021: 2361-2370"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "rnn for emotion detection in conversations[C]//Proceedings of the AAAI",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "[26] Huang Y, Giledereli B, K¨oksal A, et al. Balancing methods for multi-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "conference on artificial\nintelligence. 2019, 33(01): 6818-6825.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "label\ntext\nclassification with\nlong-tailed\nclass\ndistribution[J].\narXiv"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[3] Ghosal D, Majumder N, Poria S, et al. Dialoguegcn: A graph convolu-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "preprint arXiv:2109.04712, 2021."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "tional neural network for emotion recognition in conversation[J]. arXiv",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[27]",
          "IEMOCAP:\nInteractive emotional": "Liu Y. Roberta: A robustly optimized bert pretraining approach[J]. arXiv"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "preprint arXiv:1908.11540, 2019.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "preprint arXiv:1907.11692, 2019."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[4]\nLu X, Zhao Y, Wu Y, et al. An iterative emotion interaction network",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[28]",
          "IEMOCAP:\nInteractive emotional": "Eyben F, W¨ollmer M, Schuller B. Opensmile:\nthe munich\nversatile"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "for\nemotion recognition in conversations[C]//Proceedings of\nthe 28th",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "and fast open-source audio feature extractor[C]//Proceedings of the 18th"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "international conference on computational\nlinguistics. 2020: 4078-4088.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "ACM international conference on Multimedia. 2010: 1459-1462."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[5] Wang Y, Zhang J, Ma J, et al. Contextualized emotion recognition in",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[29] Majumder N, Poria S, Hazarika D,",
          "IEMOCAP:\nInteractive emotional": "et\nal. Dialoguernn: An attentive"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "conversation as\nsequence\ntagging[C]//Proceedings of\nthe 21th annual",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "rnn for emotion detection in conversations[C]//Proceedings of the AAAI"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "meeting of\nthe special\ninterest group on discourse and dialogue. 2020:",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "conference on artificial\nintelligence. 2019, 33(01): 6818-6825."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "186-195.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[30]",
          "IEMOCAP:\nInteractive emotional": "Lei S, Dong G, Wang X, et al. Instructerc: Reforming emotion recogni-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[6]\nIshiwatari T, Yasuda Y, Miyazaki T, et al. Relation-aware graph attention",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "tion in conversation with a retrieval multi-task llms framework[J]. arXiv"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "networks with\nrelational\nposition\nencodings\nfor\nemotion\nrecognition",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "preprint arXiv:2309.11911, 2023."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "in conversations[C]//Proceedings of\nthe 2020 conference on empirical",
          "[19] Busso C, Bulut M, Lee C C, et al.": "[31] Yu X, Wang Y, Chen Y,",
          "IEMOCAP:\nInteractive emotional": "et\nal. Fake\nartificial\nintelligence\ngenerated"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "methods in natural\nlanguage processing (EMNLP). 2020: 7360-7370.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "contents\n(faigc): A survey of\ntheories, detection methods, and oppor-"
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[7]\nLi Q, Gkoumas D, Sordoni A, et al. Quantum-inspired neural network",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": "tunities[J]. arXiv preprint arXiv:2405.00711, 2024."
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "for\nconversational\nemotion\nrecognition[C]//Proceedings\nof\nthe AAAI",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "Conference on Artificial\nIntelligence. 2021, 35(15): 13270-13278.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[8] Hu J, Liu Y, Zhao J, et al. MMGCN: Multimodal fusion via deep graph",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "convolution network for emotion recognition in conversation[J]. arXiv",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "preprint arXiv:2107.06779, 2021.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[9] Ma H, Wang J, Lin H, et al. A multi-view network for real-time emotion",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "recognition in conversations[J]. Knowledge-Based Systems, 2022, 236:",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "107751.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[10] Hu D, Hou X, Wei L,\net\nal. MM-DFN: Multimodal dynamic\nfusion",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "network\nfor\nemotion\nrecognition\nin\nconversations[C]//ICASSP 2022-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "2022 IEEE International Conference on Acoustics, Speech and Signal",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "Processing (ICASSP).\nIEEE, 2022: 7037-7041.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[11]\nShi T, Huang S L. MultiEMO: An attention-based correlation-aware",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "multimodal\nfusion\nframework\nfor\nemotion\nrecognition\nin\nconversa-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "tions[C]//Proceedings of\nthe 61st Annual Meeting of\nthe Association",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "for Computational Linguistics (Volume 1: Long Papers). 2023: 14752-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "14766.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[12]\nLi J, Wang X, Lv G, et al. GA2MIF: graph and attention based two-stage",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "multi-source information fusion for conversational emotion detection[J].",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "IEEE Transactions on affective computing, 2023, 15(1): 130-143.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[13]\nShirian A, Tripathi S, Guha T. Dynamic emotion modeling with learn-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "able\ngraphs\nand\ngraph\ninception\nnetwork[J].\nIEEE Transactions\non",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "Multimedia, 2021, 24: 780-790.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[14] Kim J, Cho J, Park J, et al. DEEPTalk: Dynamic Emotion Embedding",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "for Probabilistic Speech-Driven 3D Face Animation[J]. arXiv preprint",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "arXiv:2408.06010, 2024.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[15] Ghosh-Dastidar S, Adeli H. Spiking neural networks[J].\nInternational",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "journal of neural systems, 2009, 19(04): 295-308.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[16]\nZhou Z, Zhu Y, He C, et al. Spikformer: When spiking neural network",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "meets transformer[J]. arXiv preprint arXiv:2209.15425, 2022.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[17]\nZhou Z, Che K, Fang W, et al. Spikformer v2: Join the high accuracy",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "club on imagenet with an snn ticket[J]. arXiv preprint arXiv:2401.02020,",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "2024.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "[18]\nPoria S, Hazarika D, Majumder N, et al. Meld: A multimodal multi-",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "party dataset for emotion recognition in conversations[J]. arXiv preprint",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        },
        {
          "temporal\nfeatures and highlight critical emotional\ntransitions.": "arXiv:1810.02508, 2018.",
          "[19] Busso C, Bulut M, Lee C C, et al.": "",
          "IEMOCAP:\nInteractive emotional": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "2",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "3",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "4",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "X Lu",
        "Y Zhao",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "5",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th annual meeting of the special interest group on discourse and dialogue"
    },
    {
      "citation_id": "6",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "7",
      "title": "Quantum-inspired neural network for conversational emotion recognition",
      "authors": [
        "Q Li",
        "D Gkoumas",
        "A Sordoni"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "9",
      "title": "A multi-view network for real-time emotion recognition in conversations[J]. Knowledge-Based Systems",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin"
      ],
      "year": "2022",
      "venue": "A multi-view network for real-time emotion recognition in conversations[J]. Knowledge-Based Systems"
    },
    {
      "citation_id": "10",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S Huang",
        "Multiemo"
      ],
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "GA2MIF: graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "13",
      "title": "Dynamic emotion modeling with learnable graphs and graph inception network[J]",
      "authors": [
        "A Shirian",
        "S Tripathi",
        "T Guha"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation[J]",
      "authors": [
        "J Kim",
        "J Cho",
        "J Park"
      ],
      "year": "2024",
      "venue": "DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation[J]",
      "arxiv": "arXiv:2408.06010"
    },
    {
      "citation_id": "15",
      "title": "Spiking neural networks[J]",
      "authors": [
        "S Ghosh-Dastidar",
        "H Adeli"
      ],
      "year": "2009",
      "venue": "International journal of neural systems"
    },
    {
      "citation_id": "16",
      "title": "Spikformer: When spiking neural network meets transformer",
      "authors": [
        "Z Zhou",
        "Y Zhu",
        "C He"
      ],
      "year": "2022",
      "venue": "Spikformer: When spiking neural network meets transformer",
      "arxiv": "arXiv:2209.15425"
    },
    {
      "citation_id": "17",
      "title": "Spikformer v2: Join the high accuracy club on imagenet with an snn ticket",
      "authors": [
        "Z Zhou",
        "Che Fang"
      ],
      "year": "2024",
      "venue": "Spikformer v2: Join the high accuracy club on imagenet with an snn ticket",
      "arxiv": "arXiv:2401.02020"
    },
    {
      "citation_id": "18",
      "title": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C Lee"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "An efficient approach to informative feature extraction from multimodal data",
      "authors": [
        "L Wang",
        "J Wu",
        "S L Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "An efficient approach for audio-visual emotion recognition with missing labels and missing modalities",
      "authors": [
        "F Ma",
        "S L Huang",
        "L Zhang"
      ],
      "venue": "IEEE international conference on multimedia and Expo (ICME)"
    },
    {
      "citation_id": "22",
      "title": "Dice loss for data-imbalanced NLP tasks",
      "authors": [
        "X Li",
        "X Sun",
        "Y Meng"
      ],
      "year": "2019",
      "venue": "Dice loss for data-imbalanced NLP tasks",
      "arxiv": "arXiv:1911.02855"
    },
    {
      "citation_id": "23",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T Y Lin",
        "P Goyal",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "Focal loss for dense object detection"
    },
    {
      "citation_id": "24",
      "title": "Deep long-tailed learning: A survey",
      "authors": [
        "Y Zhang",
        "B Kang",
        "B Hooi"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Distribution alignment: A unified framework for long-tail visual recognition",
      "authors": [
        "S Zhang",
        "Z Li",
        "S Yan"
      ],
      "venue": "Distribution alignment: A unified framework for long-tail visual recognition"
    },
    {
      "citation_id": "26",
      "title": "Balancing methods for multilabel text classification with long-tailed class distribution",
      "authors": [
        "Y Huang",
        "B Giledereli",
        "A Köksal"
      ],
      "year": "2021",
      "venue": "Balancing methods for multilabel text classification with long-tailed class distribution",
      "arxiv": "arXiv:2109.04712"
    },
    {
      "citation_id": "27",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "28",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "30",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "31",
      "title": "Fake artificial intelligence generated contents (faigc): A survey of theories, detection methods, and opportunities",
      "authors": [
        "X Yu",
        "Y Wang",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "Fake artificial intelligence generated contents (faigc): A survey of theories, detection methods, and opportunities",
      "arxiv": "arXiv:2405.00711"
    }
  ]
}