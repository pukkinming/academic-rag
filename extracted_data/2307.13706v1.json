{
  "paper_id": "2307.13706v1",
  "title": "Introducing Calmed: Multimodal Annotated Dataset For Emotion Detection In Children With Autism ⋆ Annanda Sousa 1[0000-0002-0388-3641] , Karen Young 1[0000-0002-9452-2293] , Mathieu D'Aquin 2[0000-0001-7276-4702] , Manel Zarrouk 3[0000-0002-8160-5671] , And",
  "published": "2023-07-24T11:52:05Z",
  "authors": [
    "Annanda Sousa",
    "Karen Young",
    "Mathieu D'aquin",
    "Manel Zarrouk",
    "Jennifer Holloway"
  ],
  "keywords": [
    "Affective Computing",
    "Multimodal Emotion Detection",
    "Multimodal Dataset",
    "Autism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic Emotion Detection (ED) aims to build systems to identify users' emotions automatically. This field has the potential to enhance HCI, creating an individualised experience for the user. However, ED systems tend to perform poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create ED systems tailored to how people with autism express emotions. Previous works have created ED systems tailored for children with ASD but did not share the resulting dataset. Sharing annotated datasets is essential to enable the development of more advanced computer models for ED within the research community. In this paper, we describe our experience establishing a process to create a multimodal annotated dataset featuring children with a level 1 diagnosis of autism. In addition, we introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal emotion detection dataset featuring children with autism aged 8-12. CALMED includes audio and video features extracted from recording files of study sessions with participants, together with annotations provided by their parents into four target classes. The generated dataset includes a total of 57,012 examples, with each example representing a time window of 200ms (0.2s). Our experience and methods described here, together with the dataset shared, aim to contribute to future research applications of affective computing in ASD, which has the potential to create systems to improve the lives of people with ASD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing is a relatively new area in Computer Science that aims to create computer systems able to identify, process, respond to and generate emotions in human users  [3, 24] . One of the most commonly investigated areas of Affective Computing is automatic Emotion Detection (ED)  [3] , also referred to as affect recognition, affect detection, and emotion recognition. ED aims to automatically identify people's cognitive states or emotions, e.g. happiness, anger, and fear  [14] . ED systems, utilising different media inputs such as texts, video, audio and sensor signals, extract implicit cues from facial expression, eye gaze, and tone of voice. When combining more than one type of data, they are called Multimodal Emotion Detection systems, which tend to outperform unimodal systems  [6, 23] .\n\nMost of the advancement in ED has been focused on the general population, i.e. users with typical neurological development, usually referred to as neurotypicals. When applying those systems to a specific population, e.g. children with autism, the systems usually do not perform well, mainly because of this particular population's way of expressing emotions  [15] . Autism Spectrum Disorder (ASD) is a developmental disorder with a spectrum manifestation of traits characterised by impairments in social interaction, communication and repetitive patterns of behaviour and interests  [1] . Among the results of a recent metaanalysis  [25]  that compared the facial expression production between a typical development (TD) population and people with autism, they found evidence that people with autism display facial expressions less often and less frequently than people with TD. Also, people with ASD expressions are lower in quality and less accurate. In the work of  [10] , the results showed that a Random Forest model needs more facial landmarks to classify facial expressions from children with autism than it needs from children with typical development. Those works together provide additional evidence that ED systems developed for children with typical development do not perform well when applied to children with autism, motivating the need to develop ED systems specifically tailored to children with autism. \"Level 1 diagnosis of autism\" is a terminology defined by DSM-5R  [1]  referring to ASD without significant cognitive and language impairments. Also sometimes referred to as high-functioning autism, previously known as Asperger Syndrome  [9] . Throughout this paper, we will use the terms ASD and autism interchangeably.\n\nAnnotated datasets are a fundamental part of the creation of emotion detection systems. The emergence of more advanced computer models is heavily impacted by sharing these datasets within the research community. However, the creation and sharing of those datasets lead to a series of ethical challenges, requiring the research team to establish specific measures for protecting participants' rights, privacy, and well-being while maintaining the value of the created dataset to the research and the research community. Those issues appear from the first step of data collection, which often involves eliciting, capturing and tagging people's emotional expressions, to disseminating a sharable version of the data. When the emotion detection dataset features children with autism, representing a case of a vulnerable population with a medical condition, additional concerns must be considered. Examples of ethical matters in this scenario include:\n\nselecting tasks to evoke the target emotions without causing emotional harm to the children, addressing the participant's right to privacy as opposed to the important task of sharing resources in the research community, and designing a data protection plan to comply with the participants' rights as defined by the General Data Protection Regulation (GDPR).\n\nPrevious research works have investigated the challenges of developing an emotion detection system tailored for children with autism.  [15, 4, 5, 12, 20] . These studies created ED systems for children with ASD, demonstrating that it is viable to model how this population expresses emotions and automatically predicts their emotions. As part of their work, they created their own annotated datasets.\n\nIn addition, the works of  [19, 11]  specifically created annotated datasets featuring individuals with ASD. Samad's dataset  [19]  includes facial action units instead of emotions. Meanwhile, ElKaliouby's dataset  [11]  contains data on adults with ASD instead of children. All these works reported having had to go through the phases of creating an emotion detection annotated dataset. However, the authors of those works did not share their resulting dataset nor the resources they generated during the dataset creation.\n\nIn this paper, we describe our experience establishing a process and measures to design a data collection framework with the ultimate goal of creating a multimodal emotion detection annotated dataset featuring children with a level 1 diagnosis of autism.\n\nWe then introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal (video-audio) emotion detection dataset. Children from 8-12 years old with a level 1 diagnosis of autism were invited to perform computer-based emotion elicitation tasks to evoke each of the target emotion zones. The dataset is annotated with four emotion zones (green, yellow, red and blue) based on the \"emotion zones for regulation\" framework  [13] . The annotation task was performed by the participants' parents, who watched and selected which emotion zone their child was expressing at each moment.\n\nThe top four modalities of data input used in the emotion detection field are: 1. video, 2. physiological signals, 3. audio, and 4. text  [21] . Initially, CALMED was planned to include three modalities, video, audio and physiological signals. However, due to COVID-19 restrictions, the experiment setup required to be adapted and physiological signals input had to be excluded from the multimodal dataset since it would require close contact with participants to collect this data. Thus, our multimodal dataset includes two modalities of data input: video and audio. This bimodality is the most explored in multimodal emotion detection  [3] .\n\nCALMED, the annotated dataset for emotion detection described in this paper, has the following features:\n\n-Compliant with Ethics guidelines and recommendations, with a main focus on the participants' emotional well-being;\n\n-Compliant with Data Protection legislation for handling personal data; -Multimodal dataset (video and audio data inputs); -Features children (8-12 years old) with a diagnosis of level 1 autism; -Annotated by the participants' parents into four classes representing emotion zones; -Data collected in a naturalistic setup, not posed emotions; -Data collected using computer-based task environment, as opposed to dialect eliciting tasks, i.e., from a conversation with the participant.\n\nWe also share the resources we created during the process, with the hope that they will benefit future research in creating and sharing their own emotion detection dataset featuring particular population, especially ASD population. Thus, future researchers can reuse the resources here presented, or customise them to adapt to their specific research goals. The additional resources shared in this work are three artefacts, namely:\n\na computer-based task environment web system, for emotions' elicitation sessions; an annotation web system, to support the dataset annotation process; a working dataset generation system, to create an annotated dataset from the annotation process.\n\nBy sharing the dataset, this paper's contribution is unique since none of the previous research that created emotion detection datasets involving children with autism shared a dataset artefact with the research community, primarily due to privacy issues. While ED for the general population has plenty of available resources, e.g. annotated datasets, ED for ASD suffers from scarce available resources, with not even one annotated dataset available. Thus, the research team believes that making more resources available to support the creation and sharing of annotated datasets featuring populations with ASD can lead to advancing this specific application of the ED field.\n\nThe rest of the paper is organised as follows, Section 2 describes the process and methods used to elicit and capture the original data from the participants. Then, Section 3 describes the methodology to create and process the multimodal annotated dataset. This is followed in Section 4, with a description of the dataset characteristics and a discussion about its applicability, limitations and perspectives. Finally, the paper concludes with a discussion of future work directions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Collection",
      "text": "As introduced before, previous works  [19, 11, 15, 4, 5, 12, 20]  have created affective computing systems focussing on people with ASD; each of these was required to create their own dataset since none was available. In general, they followed the steps enumerated below.\n\n1. Modelling emotions definition, 2. define and design the eliciting tasks, 3. design the study session and task environment, 4. define data to be collected and create a data protection plan, 5. apply for ethics approval, 6. define inclusion and exclusion criteria of participation in the study, 7. recruit participants, 8. run study sessions, 9. define annotators, and running annotation sessions, 10. generate the final working annotated dataset from the original data collected. This section describes the methods used while creating CALMED for steps 1-8, while Section 3 describes the methodology for steps 9-10.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Modelling Emotions",
      "text": "Modelling and representing emotions is one of the inherent challenges in affective computing. Emotion definitions come from psychology and might be imprecise or fuzzy, making them difficult to apply to computer models  [3] . Usually, in affective computing, emotions are represented by the seven basic emotions, i.e. surprise, happiness, anger, disgust, contempt, sadness and fear, making them a set of categories. We can also find emotions represented according to the two dimensions of arousal (strong or weak) and valence (positive or negative), which usually take continuous value between -1 and 1  [17] .\n\nHowever, for emotion detection applied for ASD, researchers tend not to work with the basic emotions from the general ED field. They argue that basic emotions are not the best target emotional states for applications of ED for autism from a pragmatic perspective. They, instead, work with different emotion states, e.g. anxiety (a prevalent co-occurring condition to ASD) or engagement level for instance  [15] .\n\nFollowing the previous related works, which did not focus on the seven basic emotions, , we selected other emotional states as labelling targets for CALMED dataset. We labelled the data using a framework called \"the zones of regulation\"  [13] . This framework is extensively used in psychology to help children with ASD, and other neurological conditions learn emotion regulation since it is common for children with ASD to present impairments in emotion regulation  [22] .\n\nThe zones of regulation framework has four different zones represented by colours (See Figure  1 ). One of the emotion zones is the calming zone, represented by the green zone. This ideal state is where the child is calm, relaxed, and ready to work, listen, and interact. The warning zone (yellow zone) indicates that the child is presenting signals of agitation or excitement. This state can originate from both positive and negative emotions. It can start from intense happiness or excitement and also from frustration. The high-agitation zone (red zone) indicates that the child is upset or angry, presenting severe difficulties in keeping control of their emotions. The last zone is the slowing zone, the blue zone, in which the child is on low energy and showing emotional signals of being sad, tired, sick or bored. In this state, the child might move slower than usual, stop speaking or show delays in interactive responses.\n\nFig.  1 : The Four Emotion Zones, based on \"The Zones of Regulation\", a Social Emotional Learning Framework  [13] . The CALMED dataset is annotated with these four labels.\n\nWe obtain several benefits by using this emotions zones' framework as a target for labelling emotions. Firstly, the framework includes guidelines on activities to lead children back to the calming zone, and since the ultimate goal of creating a multimodal dataset is to develop affective computing systems to support children with ASD, using the \"zones of regulation framework\" makes it easy to incorporate the framework's activities to calm the child within an affect-sensitive interface. Second, parents of children with ASD are more likely to be familiarised with this framework because it is commonly used in the context of autism, hence making the labelling task by the parents more comfortable. Thirdly, considering the children's well-being, it is less harmful to the emotional comfort of children with ASD during the emotion elicitation experiment to elicit the four emotion zones than other strong negative emotions, e.g. fear and anxiety. This way we could elicit and observe all the four zones without causing a strong emotional discomfort on the participants.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ethics",
      "text": "This research is based in Ireland; thus, it must comply with Irish legislation and GDPR. The data collection involves children, which generates an additional challenge to address. Ireland has specific regulations for research involving children as participants. For instance, assent from the child is mandatory in addition to the parent's consent. Thus, if the parent allows the child to participate, but the child does not want to, the researcher can not go further with the data experiment. Such cases were encountered during this research's data collection. To obtain the assent from the child, we need to use age-appropriate language while being transparent and explaining what will be asked of them. Furthermore, we created age-appropriate materials to seek assent from children and to explain to them what the research participation would entail. We only talked to the children after obtaining permission from their parents.\n\nThe most critical ethical concern is not to cause harm during the data collection sessions. The data collection objective was to capture naturalistic manifestations of emotions from children with autism. So we needed to elicit these emotions to record them and later annotate them. There are positive and negative emotions, so we decided early on not to elicit strong negative emotions which could cause emotional discomfort for the children, e.g. fear and phobia.\n\nWe also put in place additional measures to protect the participants' emotional well-being. All sessions happened in the presence of a postgraduate psychology student who had experience working with children with ASD. This measure provided specialised support in case the participants got overwhelmed by emotions during the session and could not regulate themselves. As additional measures for emotional well-being, calming activities were included between each eliciting task to support the participants' emotion regulation, helping them calm themselves during the study session.\n\nThe University of Galway's research ethics committee reviewed and approved the proposed ethics measures.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Data Protection",
      "text": "Another important aspect of this data collection framework is the data protection plan. We needed to ensure that the personal data collected was secure and kept private from unauthorised access. Personal data is defined as any data that can be used to identify an individual, either by direct identification or through some sort of data engineering. To define the action and measures to put in place to protect the personal data and the participants' rights, we conducted a Data Protection Impact Assessment (DPIA), generating a plan with specific action points to follow on how to deal with the personal data resulting from this project. The university's data protection officer approved the DPIA. Some examples of actions we followed to ensure the participant's rights include:\n\n-Provide comprehensive information on all aspects of the research for participants' parents, with a project's website, information sheets and other materials. -Provide information on the participants' right to withdraw from the research.\n\n-Provide information on the participants' right to have their data deleted.\n\n-Protect the right to privacy, ensuring only the research team can access the participant's data.\n\nTo make the information more available to participants and the general public, we created a website with all the documents and relevant information about the project.  5  .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Privacy And Sharing Resources",
      "text": "None of the previous works which created annotated emotion detection datasets featuring children with autism shared their resulting dataset mainly due to privacy reasons. Since the raw recording files contain personal data, we needed to eliminate the identifiable aspect of the data to make it possible to share an annotated dataset. The approach we selected is, instead of creating a dataset featuring the raw video and audio files, to create a dataset with numerical and categorical features extracted from the original files. More detail on the feature generation can be found in the Sections 3.3 and 3.4.\n\nMaking only available the extracted features instead of the original files is not an ideal approach to sharing resources in the research community because the researchers will not be able to extract their features. However, that is a necessary step, and the benefits of having a dataset with extracted features outweigh the previous limitation of not having a dataset at all. Not having an available dataset resulted in each new research work ought to start from scratch, creating a new dataset with all the steps and procedures involving its creation, consuming much time, material and people resources, which might be a limiting factor when defining a research project, leading to less research being conducted in this specific application.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Study Session Setup",
      "text": "Our goal for the study session was to collect video and audio data from children with autism while they were engaged in the four emotion zones. Thus, we needed to elicit each emotion zone during a study session with participants. The elicitation is usually done by asking participants to perform evoking tasks. In order to create a more valuable dataset, it is essential to collect data following a setup as close as possible to the intended use-case scenario  [18] . For our case, we visioned a multimodal dataset as a resource to create affect-sensitive computer systems for children with ASD. Therefore, the closest scenario to elicit emotions is a computer-based task environment, i.e. where the participants interact with a computer system during the elicitation study session.\n\nThe study sessions happened entirely online, over a Zoom video call, which was recorded. Initially, the study session was planned to happen in a lab room prepared with a computer, a high-resolution camera and a semi-professional microphone. However, due to pandemic restrictions, we could not conduct faceto-face meetings, making it necessary to adapt the study session to an online remote setup. The session was then a Zoom video call in which the researcher first talked to the participant, using age-appropriate language, seeking their assent and explaining the session and tasks. The participant then accessed an URL  6  to the task environment software and clicked the \"Start button\" to start the session. The system is programmed to automatically follow each task in sequence without requiring manual human commands. While the child is engaged with the task environment software, the Zoom video call is still on, recording the participant's camera and audio.\n\nThe resulting video/audio file from the participant session is the raw file for the dataset creation. Using the participant's camera and microphone, on one side, does not guarantee the quality of the input, creating an additional challenge of noise and low-quality media. However, on the other side, it generates more realistic videos in a setup that an eventual affective computing system is more likely to encounter.\n\nWe invited children and their parents to participate in the data collection study. The inclusion criteria were that the child had to have a level 1 diagnosis of ASD and to be within the age range (8-12 years old). We limited the age range to before the teenage phase, where other co-occurring conditions usually manifest and influence how the person expresses emotions. In addition, we needed to have written consent from the parent or guardian and verbal assent from the child. An exclusion criterion was having a history of cognitive or language impairment that does not fall under the level 1 diagnosis of autism and not attaining assent from the child. The study session was designed to last for no more than 30 minutes, and we asked each participant to attend two different sessions.\n\nAs reported by all previous works, finding participants was challenging, especially because we were amidst a global pandemic. We had study sessions with four participants, one girl and three boys, with an average age of 10.25 (+-1.7). Table  1  summarises the participants' details and the number of sessions they attended. All the sessions generated close to four hours of recorded video/audio files. All study sessions followed the following agenda:\n\n1. Initial greetings with discussion on the child's interests (4 minutes).\n\n2. Explanation of the session using age-appropriate language (3 minutes).\n\n3. The session -the child accesses and follows the tasks on a web system (20 minutes). 4. Ending conversation (2 minutes). 5. Gifting the child with a certificate -with visual art involving the child's special interests (1 minute).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Elicitation Tasks",
      "text": "During the study session, the participant accessed a web-based software we created that encompasses four different eliciting tasks and calming activities. Each of the four eliciting tasks aims to evoke one of the emotion zones. Figure  2  shows screenshots of some parts of the Task Environment System. The eliciting tasks follow the sequence: green, yellow, red and blue zone. The order of the eliciting tasks focused on the participant's emotional well-being. It is expected that a participant already experiences some level of anxiety from participating in the study. They are talking to someone they do not know and doing something outside their routine, two things that tend to cause anxiety for individuals with ASD. We, therefore, did not want to aggravate it by starting the session by eliciting negative emotions. Hence, the first task is to calm the participant, eliciting the green zone, starting with a relaxed, happy and calm tone. Following the green zone, we elicit the yellow zone, which involves some uneasiness and agitation, and next, we elicit the red zone. The last emotion zone is the blue zone since we expected that by this point, the child would start to get tired, so we would naturally observe the blue zone. We defined this order to foster a more gentle progression towards the more challenging emotions so that the change to the red zone would not happen too abruptly, which could cause increased emotional discomfort.\n\nThe eliciting tasks are as follows: watching a video with funny and cute animals for the green zone (3 minutes), playing a game for the yellow zone (3 minutes), completing a challenging maths worksheet with a visible timer for the red zone (3 minutes, see Figure  2a ), and watching a boring video for the blue zone (3 minutes). Right after each eliciting task, the system includes a prompt for a simplified emotion state self-report with three options, happy face, neutral face and unhappy face (see Figure  2b ). The self-report does not include the four emotional zones because it could be too complex for the children, especially considering that alexithymia, difficulty in naming one's emotions, is common in individuals with autism.\n\nBetween each of the tasks, the system also includes a calming activity (1.5 minutes) which serves a twofold purpose: First, to help the child to calm and regulate their emotions, decreasing the possibility of emotional discomfort, and second, to return the participant's emotions to a baseline before moving forward to the next emotion zone elicitation (see Figure  2d ).\n\nIn addition, the system includes a visual schedule with the four tasks crossed throughout the session to help the participants have a sense of the time passing and what to expect during the session and when the session is close to the end. This technique is widely used in psychology to support children with ASD so they feel less anxious (see Figure  2c ).\n\nThe system at http://task-environment.datascienceinstitute.ie/ is configurable depending on the session number, i.e. one or two, the child's age for the maths worksheet, i.e., younger, default and older, and the time allocated for each task and activity. The system is currently configured for the first session, with \"older\" as a maths worksheet configuration and the default activity time (d) Screenshot of the calming activities to be selected by the participant between each eliciting task.\n\nFig.  2 : Screenshots of the Task Environment System, a web system created to support the study session with participants. Participants accessed the system through a web browser and completed the eliciting tasks. The system can be accessed at http://task-environment.datascienceinstitute.ie/.\n\n(3 minutes per task). The source code for the system is available for the research community by request. Hence other researchers can reproduce the same study session or configure it to meet their research goals.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Dataset Creation",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Annotation By Parents",
      "text": "Since children with autism can individually display emotions in a particular way, we asked each participant's parent to annotate their child's emotions throughout the study session, acknowledging that they know their child's emotions best. We created a system to support the annotation process. The parent's interface to the system is composed of the video recording of their child's study session with four buttons below it; each button represents an emotion zone to be clicked when the parent believes their child is displaying the given emotion zone. Figure  3  shows a screenshot of the system. Each study session recording was divided into videos of around five minutes to facilitate the annotation by the parent. We instructed the parent to watch the recording; they should click on the button representing the emotion as soon as they identified an emotion zone. At the end of the annotation, we also asked the parents to answer a brief questionnaire regarding the annotation session. None of the parents reported difficulty identifying their child's emotions during the annotation session. Fig.  3 : Screenshot of the Annotation System, a web system created to support the annotation process by the participants' parents. The parent watched their child's study session recording and selected the appropriate emotion zones by clicking on the buttons below the video.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset Creation Overview",
      "text": "The study sessions with the participants and the annotation sessions with the parents generated the raw data to be processed as a working annotated dataset. The raw files are converted into a working dataset via a pipeline process depicted in Figure  4 . First, following the diagram in Figure  4 , we conducted study sessions (1.1) with the participants and annotation sessions (1.2) with the parents, Fig.  4 : Pipeline with an overview of the CALMED dataset creation process, from data collection to dataset creation, including all the systems developed.\n\nobtaining the data for the dataset creation (1.3-1.5). We then used OpenFace  [2]  and OpenSMILE  [8]  to extract video and audio features, respectively (2.1 and 2.2). We then process the result of the parent's annotation sessions (2.5) into a dataset, i.e., a .CSV file with annotation from each session (2.6). In sequence, we clean, organise and synchronise the features datasets to the annotation datasets (2.7). At this point, we have a set of .CSV files with video, audio and annotation features for each session, all containing a column with the video timestamp that we use for synchronisation (2.8 and 2.9). These .CSV files represent the whole dataset without any split into different sets. Lastly, we created a split of the train, validation and test sets, with 80/10/10 proportions (2.10), resulting in the CALMED dataset.\n\nThe following sections present a detailed description of each of those steps.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Video Features Extraction",
      "text": "Each study session produces audio and video files of around 30 minutes in length.\n\nTo decrease the file size and facilitate the processing step, we split audio/video files into smaller files, each of around five minutes. We used OpenFace  [2]  to generate the visual features for the CALMED dataset. OpenFace is a toolkit composed of computer vision algorithms capable of executing essential tasks for visual affective computing, such as facial landmark, facial action unit detection, head pose, and eye gaze estimation. OpenFace has been widely used in the literature to generate visual features for analysing facial behaviour.\n\nWe passed the five-minute video files to the OpenFace tool for video feature extraction. The tool extracts visual features for each frame, returning a massive .CSV file with all the numerical visual features and timestamps (the output 2.3 in Figure  4 ). We use OpenFace's output as the input to a system created to clean and organise the visual features. In the Features Extraction system, 2.7 in the Figure  4 , we separate the visual features into groups, e.g., Facial Action Units (AU) and gaze. We also organise the output from OpenFace into our desired time window of 200 milliseconds, with the correct timestamp to be used to synchronise the visual data with the audio and annotation data. The visual features included in the resulting dataset are: Facial Action Units (FAUs), face landmarks, eye landmarks, gaze, and head pose, comprising 669 visual features in total. Table  2  summarises the number of features per group. A comprehensive description of the OpenFace features can be found in the project's publication  [2] .",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Audio Features Extraction",
      "text": "We similarly repeated the steps described in the last section for audio features extraction using OpenSMILE  [8]  for acoustic features creation. OpenSMILE is a robust system capable of extracting a vast set of audio features. We defined a selection of audio features to be included in the dataset based on the Extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS). The eGeMAPS is a set of features curated as a recommendation of audio parameters for voice research and affective computing. The parameters were selected based on: 1) how extensively and successfully these parameters were used in the literature, 2) the theoretical significance of the parameter, and 3) the potential of an acoustic parameter to represent physiological changes that occur in the voice during affective processes  [7] . From eGeMAPS, we selected 75 parameters resulting from Low-Level Descriptors (LLDs) extracted from audio files and sorted them into four groups: frequency, energy/amplitude, spectral (balance) parameters, and additional temporal features. Table  3  summarises the number of features per type. The methodology described above to create and synchronise the dataset's visual and audio features is based on the methodology used by AVEC-2018  [16]  when using the RECOLA  [17]  multimodal dataset, which shares some similarities with the CALMED dataset, including the video and audio data inputs.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Dataset Labelling",
      "text": "From the annotation sessions, we get a .sqlite file with each emotion zone selected by the annotator and the video's timestamp indicating when the emotion was chosen. From this file, we produce a .csv file where each row represents a time window of 200 milliseconds and the assigned label for the given time window. Each row corresponds to one example within the labels dataset.\n\nThe steps to create the labels dataset are as follows, they are executed in the Labelling creation system (marked as 2.6 in Figure  4 ):\n\n1. Create a .CSV file with a row representing each time window of 200 milliseconds of a given video file of a session with a participant; 2. Read the emotion zones selected by the annotations in the .sqlite file and; 3. Assign the corresponding annotation for each row in the dataset.\n\nOne challenge during this step was identifying how close the annotation time was to the real emotion utterance. During the annotation, we collect the time of the video when the annotator pressed the emotion button, which occurs with an inherent delay after the parent identifies the emotion. In other words, the emotion expression started some point before the time the parent marks it. For this study, we empirically defined the delay time as equal to one second. In practice, we marked in the dataset that the emotion started one second before the timestamp selected by the annotator.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Dataset Splitting",
      "text": "After creating a features dataset of audio, video and annotation labels, we then split it into three different sets: train, validation and test sets, with a proportion of 80/10/10, respectively, each of them containing the same four emotion zones distribution. To accomplish the same distribution of emotion within train, validation and test sets, we followed the steps described below. We utilised the sample method of Pandas Python library with a random state value of 25. Each participant's data appear in the train, validation and test sets, i.e., there are no participant data only present in one of the splitting sets.\n\nWithin a given session's data:\n\n1. We selected all the examples of a given emotion and split them randomly into train, validation and test with the proportion of 80/10/10, respectively, 2. We repeated the previous step with the other emotions, 3. We then concatenate all the same dataset types together, i.e., the train data from green, yellow, blue and red emotion zones, and so on, generating then train, validation and test set following the same distribution of labels.\n\nThe splitting method described above considers each time window of 200ms as an independent example in the dataset, not following the time sequence of the video. However, CALMED dataset includes all video/audio/labels timestamps, allowing researchers to create alternative splits according to their research needs. Any additional split type created by the research team will be available under request. The number of examples in the green emotion zone is significantly larger than in the red emotion zone, i.e. around ten times more. This unbalanced dataset characteristic was expected due to our decision to prioritise the participant's well-being. Since the red class models some negative and demanding emotions, during the sessions, we wanted to elicit the red zone in the least amount of time so as not to cause emotional harm to the participants. Thus, the resulting dataset presents a significantly lower number of examples of the red class than the green class. The CALMED dataset was created as part of a research project aiming to create a multimodal emotion detection system for children with ASD. Thus, we foresee that the main scenario in which CALMED can be applied is to train and test machine learning systems for affective computing tailored to children with ASD.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "Insights and knowledge are expected to emerge from creating a classifier model, such as, the most viable features, modalities and fusion layer techniques to be applied for emotion detection in the context of people with ASD. Moreover, this has the potential to generate knowledge for both the affective computing and psychology fields.\n\nThere are other subsets of the general population that are known to also express emotions differently from a typical population, e.g. people with schizophrenia, brain damage, and some mental health condition such as depression. All those populations could also benefit from the advancement of affective computing systems. However, to accomplish that, it would also be necessary to create annotated datasets featuring these populations. The methods described, and the systems shared in this paper could be extended, adapted and applied to designing and creating these annotated datasets.\n\nThe CALMED dataset could also be applied to evaluate the viability of creating a person-independent model, i.e. an emotion detection model applied to individuals whose data was not present in the training dataset. For this purpose, the dataset would need to split so that some participants' data do not appear in the training dataset, which is not the case now, as mentioned in Section 3.6.\n\nSharing only the extracted features as opposed to the original files can be considered a limitation since other researchers will not be able to extract their features. However, that was a middle-ground solution to both protect the par-ticipants' privacy and have a first-of-its-kind valuable resource shared within the research community.\n\nAs reported by previous works, the research team faced the same challenges in recruiting children as participants for creating a dataset. Thus, one of the main limitations of the dataset is the limited number of subjects, which implies that it does not encompass a wide diversity in the extracted data. However, we were able to extract meaningful knowledge from the data collected and expect to have more participants in the future to generate more data.\n\nAnother aspect to note concerns the annotation performed by participants' parents. Some of the previous works decided to use annotation by parents, while others selected annotation by specialists instead. We do not yet know if the parents' annotation carries any biases or noise or to what extent they are reliable, so it would be helpful to have an additional annotation from a specialist in ASD who does not know the participants and compare these two sets of annotation-checking for agreement rate, differences in the distribution of emotions, among others.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper introduced CALMED (Children, Autism, Multimodal, Emotion, Detection), a multimodal (video-audio) emotion detection dataset featuring children aged 8-12 years old with a level 1 diagnosis of autism. The multimodal dataset includes audio and video features extracted from recording files of study sessions with participants, together with annotation provided by their parents into four target classes (green, yellow, red and blue) based on the \"the zones of regulation\" framework  [13] . The generated dataset includes a total of 57,012 examples. Each example represents a time window of 200ms (0.2s) of audio and video extracted features and an emotion annotation. The resulting dataset is available for the research community upon request. This paper's contribution is unique since it shares an audio/video features dataset with the research community. By sharing the features dataset with the research community, we maintain the privacy of the subjects while still making available resources to facilitate further research in this emotion detection application, which has the potential to improve systems of automatic ED applied to children with ASD. We hope that our experience and methods described here contribute to future research applications of affective computing in this scenario, for instance, educational platforms, and self-identify emotions assistance tools, among others.\n\nThe small sample size of participants did not allow us to create a more diverse dataset regarding gender, culture, and language. Thus, further participant recruitment and data collection would benefit the dataset, including children from other cultures, countries and backgrounds. Using the task environment and annotation systems shared here, the data collected would be consistent, and other research groups could further expand the dataset.\n\nA further study will investigate how a specialist annotation compares with the parents' annotation of the participants' emotions. Our research group has already completed the annotation of emotion zones by such a specialist. It is now working on generating a new dataset which will be compared with the original annotation by the parents.\n\nA natural progression of this work is to create a multimodal ED machine learning system, an ongoing work in our research group. Initial results have been obtained, and further steps are being taken to improve the classification accuracy. The resulting system will feature in a future publication.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Acknowledgements",
      "text": "The research team would like to thank Dr Ciara Gunning who provided specialised advice, training on how to interact with children with ASD, and revision of the data collection experiment design and materials, together with support on the recruitment of participants. We also thank Aindrias Cullen for his comprehensive advice on data protection legislation, so we could design a project that is compliant with GDPR. We thank still Adhara Correa Soto, Alisha Garvey, Hannah Callanan, and Liam Finnerty for their participation and support during study sessions with participants. And lastly, we thank all the lovely participants and their parents who made this project possible. We hope you harvest the results of this research. It was a pleasure to meet and work with each of you.\n\nThis publication has emanated from research conducted with the financial support of Science Foundation Ireland under Grant number SFI/12/RC/2289 P2",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). One of the emotion zones is the calming zone, represented",
      "page": 5
    },
    {
      "caption": "Figure 1: The Four Emotion Zones, based on “The Zones of Regulation”, a Social",
      "page": 6
    },
    {
      "caption": "Figure 2: a), and watching a boring video for the blue",
      "page": 10
    },
    {
      "caption": "Figure 2: b). The self-report does not include the four",
      "page": 10
    },
    {
      "caption": "Figure 2: Screenshots of the Task Environment System, a web system created to",
      "page": 11
    },
    {
      "caption": "Figure 3: Screenshot of the Annotation System, a web system created to support",
      "page": 12
    },
    {
      "caption": "Figure 4: First, following the diagram in Figure 4, we conducted study ses-",
      "page": 12
    },
    {
      "caption": "Figure 4: Pipeline with an overview of the CALMED dataset creation process, from",
      "page": 13
    },
    {
      "caption": "Figure 4: ). We use OpenFace’s output as the input to a system created to",
      "page": 14
    },
    {
      "caption": "Figure 4: , we separate the visual features into groups, e.g., Facial Action",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Number of examples per class across the different sets of the data.",
      "data": [
        {
          "Green Yellow Red Blue": "30,882\n9,858",
          "Total": "3,179 13,093 57,012"
        },
        {
          "Green Yellow Red Blue": "24,704\n7,886\n3,089\n985\n319\n1,309\n3,089\n987\n317\n1,309",
          "Total": "2,543 10,475 45,608\n5,702\n5,702"
        },
        {
          "Green Yellow Red Blue": "5,158\n788\n390\n3,156\n5,158\n788\n390\n3,156",
          "Total": "9,492\n9,492"
        },
        {
          "Green Yellow Red Blue": "8,139\n3,788\n1482\n3,020\n3,664\n2,444\n829\n1,386\n4,475\n1,344\n653\n1,634",
          "Total": "16,429\n8,323\n8,106"
        },
        {
          "Green Yellow Red Blue": "8,534\n3,193\n595\n4,456\n4,939\n1,072\n516\n2,265\n3,595\n2,121\n79\n2,191",
          "Total": "16,778\n8,792\n7,986"
        },
        {
          "Green Yellow Red Blue": "9,051\n2,089\n714\n2,461\n4,483\n800\n278\n1,474\n4,568\n1,289\n436\n987",
          "Total": "14,315\n7,035\n7,280"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Diagnostic and statistical manual of mental disorders (DSM-5®)",
      "authors": [
        "A Association"
      ],
      "year": "2013",
      "venue": "Diagnostic and statistical manual of mental disorders (DSM-5®)"
    },
    {
      "citation_id": "2",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "3",
      "title": "The Oxford handbook of affective computing",
      "authors": [
        "R Calvo",
        "S D'mello",
        "J Gratch",
        "A Kappas"
      ],
      "year": "2015",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "4",
      "title": "Facial emotion recognition with transition detection for students with high-functioning autism in adaptive e-learning",
      "authors": [
        "H Chu",
        "W Tsai",
        "M Liao",
        "Y Chen"
      ],
      "year": "2018",
      "venue": "Soft Computing",
      "doi": "10.1007/s00500-017-2549-z"
    },
    {
      "citation_id": "5",
      "title": "Affective Computational Model to Extract Natural Affective States of Students with Asperger Syndrome (AS) in Computer-based Learning Environment",
      "authors": [
        "A Dawood",
        "S Turner",
        "P Perepa"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2879619"
    },
    {
      "citation_id": "6",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "S Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys (CSUR)",
      "doi": "10.1145/2682899"
    },
    {
      "citation_id": "7",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Cognitive behavioural therapy for adults with autism spectrum disorder",
      "authors": [
        "V Gaus"
      ],
      "year": "2011",
      "venue": "Advances in Mental Health and Intellectual Disabilities",
      "doi": "10.1108/20441281111180628"
    },
    {
      "citation_id": "10",
      "title": "Children with autism spectrum disorder produce more ambiguous and less socially meaningful facial expressions: An experimental study using random forest classifiers",
      "authors": [
        "C Grossard",
        "A Dapogny",
        "D Cohen",
        "S Bernheim",
        "E Juillet",
        "F Hamel",
        "S Hun",
        "J Bourgeois",
        "H Pellerin",
        "S Serret",
        "K Bailly",
        "L Chaby"
      ],
      "year": "2020",
      "venue": "Molecular Autism",
      "doi": "10.1186/s13229-020-0312-2"
    },
    {
      "citation_id": "11",
      "title": "Eliciting, capturing and tagging spontaneous facialaffect in autism spectrum disorder",
      "authors": [
        "R El Kaliouby",
        "A Teeters"
      ],
      "year": "2007",
      "venue": "Proceedings of the ninth international conference on Multimodal interfaces -ICMI '07 p",
      "doi": "10.1145/1322192.1322203"
    },
    {
      "citation_id": "12",
      "title": "A Kalman filtering framework for physiological detection of anxiety-related arousal in children with autism spectrum disorder",
      "authors": [
        "A Kushki",
        "A Khan",
        "J Brian",
        "E Anagnostou"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/tbme.2014.2377555"
    },
    {
      "citation_id": "13",
      "title": "The zones of regulation: A framework to foster self-regulation",
      "authors": [
        "L Kuypers"
      ],
      "year": "2013",
      "venue": "Sensory Integration Special Interest Section Quarterly"
    },
    {
      "citation_id": "14",
      "title": "Sentiment analysis: Mining opinions, sentiments, and emotions",
      "authors": [
        "B Liu"
      ],
      "year": "2015",
      "venue": "Sentiment analysis: Mining opinions, sentiments, and emotions"
    },
    {
      "citation_id": "15",
      "title": "Physiology-based affect recognition for computer-assisted intervention of children with Autism Spectrum Disorder",
      "authors": [
        "C Liu",
        "K Conn",
        "N Sarkar",
        "W Stone"
      ],
      "year": "2008",
      "venue": "International Journal of Human Computer Studies",
      "doi": "10.1016/j.ijhsc.2008.04.003"
    },
    {
      "citation_id": "16",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "17",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "18",
      "title": "Running behavioral studies with human participants: A practical guide",
      "authors": [
        "F Ritter",
        "J Kim",
        "J Morgan",
        "R Carlson"
      ],
      "year": "2012",
      "venue": "Running behavioral studies with human participants: A practical guide"
    },
    {
      "citation_id": "19",
      "title": "A Feasibility Study of Autism Behavioral Markers in Spontaneous Facial, Visual, and Hand Movement Response Data",
      "authors": [
        "M Samad",
        "N Diawara",
        "J Bobzien",
        "J Harrington",
        "M Witherow",
        "K Iftekharuddin"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2017.2768482"
    },
    {
      "citation_id": "20",
      "title": "Physiological detection of affective states in children with autism spectrum disorder",
      "authors": [
        "S Sarabadani",
        "L Schudlo",
        "A Samadani",
        "A Kushki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2018.2820049"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "22",
      "title": "Improving emotion regulation with cbt in young children with high functioning autism spectrum disorders: A pilot study",
      "authors": [
        "A Scarpa",
        "N Reyes"
      ],
      "year": "2011",
      "venue": "Behavioural and cognitive psychotherapy",
      "doi": "10.1017/s1352465811000063"
    },
    {
      "citation_id": "23",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/j.imavis.2017.08.003"
    },
    {
      "citation_id": "24",
      "title": "Affective computing: A review",
      "authors": [
        "J Tao",
        "T Tan"
      ],
      "year": "2005",
      "venue": "International Conference on Affective computing and intelligent interaction"
    },
    {
      "citation_id": "25",
      "title": "Facial expression production in autism: A meta-analysis",
      "authors": [
        "D Trevisan",
        "M Hoskyn",
        "E Birmingham"
      ],
      "year": "2018",
      "venue": "Autism Research"
    }
  ]
}