{
  "paper_id": "2308.06197v2",
  "title": "Complex Facial Expression Recognition Using Deep Knowledge Distillation Of Basic Features",
  "published": "2023-08-11T15:42:48Z",
  "authors": [
    "Angus Maiden",
    "Bahareh Nakisa"
  ],
  "keywords": [
    "Deep learning",
    "Knowledge transfer",
    "Neural networks",
    "Convolutional neural networks",
    "Human-centred artificial intelligence",
    "Multi-task learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in complex facial expression recognition as a human, it may need to synthesise knowledge and understand new concepts in real-time, as humans do. Humans are able to learn new concepts using only few examples by distilling important information from memories. Inspired by human cognition and learning, we propose a novel continual learning method for complex facial expression recognition that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. In this work, we also use GradCAM visualisations to demonstrate the relationship between basic and compound facial expressions. Our method leverages this relationship through knowledge distillation and a novel Predictive Sorting Memory Replay, to achieve the current stateof-the-art in continual learning for complex facial expression recognition, with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than noncontinual learning methods, improving on state-of-the-art noncontinual learning methods by 13.95%. Our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using only a single training sample per class. Impact Statement-Facial expressions are one of the most powerful signals for humans to convey emotional states, comprising over 55% of our emotional communication. By developing AI systems that can accurately recognise facial expressions at human performance level, they could be trusted to assist with functions and services that demand emotional communication, such as in healthcare and customer service. We propose a novel method of complex facial expression recognition using continual learning and few-shot learning, inspired by human cognition and learning. The method uses knowledge distillation of basic expressions and a novel Predictive Sorting Memory Replay to reduce the catastrophic forgetting associated with continual learning, achieving state-of-the-art performance compared with other methods. Our method demonstrates the increased performance of neural networks when recognising complex concepts by retaining and distilling the knowledge of basic concepts. This will pave the way for further research and applications in other domains using our method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "We are now entering the fourth industrial age, where artificial intelligence (AI) plays a crucial role in many of our activities and endeavours. By equalling or outperforming humans in cognitive tasks such as image recognition and natural language processing, and by completing physical and processing tasks with far greater precision and speed, AI can increasingly be used to assist with complex functions and services that were once the exclusive domain of humans, such as automobile driving, medical diagnosis, and customer service operations. However, many of these services rely not only on technical accuracy and precision, but on the very human elements of communication, empathy and compassion.\n\nCommunication between humans is fundamental to our ability to learn, work and build structures and societies, allowing us to survive, adapt, progress and prosper. According to Darwin  [1] , facial expressions are one of the most powerful signals for humans to convey emotional states and intentions, with over 55% of emotional communication being conveyed through facial expressions  [2] . Facial expression recognition (FER) is thus a crucial factor in our ability to operate in more complex and nuanced roles such as healthcare and customer service. By developing AI systems that can accurately recognise human emotional states at or above the level of human performance, they could be trusted to assist with these more complex functions in order to enhance these services.\n\nIn order for an AI system to recognise complex facial expressions at or above the level of human cognition, it may need to learn in a similar way to humans, learning new concepts from only a few examples by synthesising those concepts with existing knowledge. Continual learning and few-shot learning are two approaches to machine learning that are inspired by human cognition and learning patterns. Continual learning is an approach for incrementally learning new classes using a model that was previously trained on other classes. It uses techniques such as knowledge distillation and memory replay to retain the knowledge of known classes, such that it performs well in recognising both old and new classes. Few-shot learning involves training machine learning models using only a few examples. High-performing fewshot learning methods use novel feature extraction and data augmentation techniques to achieve high recognition accuracy using fewer training samples. Applying these methods of continual learning and few-shot learning to complex FER could lead to human-like performance on this challenging task.\n\nThis work investigates the hypothesis that by retaining the knowledge of basic facial expression features, a machine learning model will achieve better performance when learning new complex facial expression labels which share those features. The work is comprised of three phases. In the initial Basic FER Phase, we design and build a neural network model that can achieve high classification accuracy on six basic expression classes. In the Continual Learning Phase, new complex expression classes are iteratively learned by using knowledge distillation of basic expressions as well as a unique Predictive Sorting Memory Replay. In the Few-shot Learning Phase, we demonstrate how new complex expression classes can be learned to high accuracy with a very small number of training samples using knowledge distillation of basic expressions.\n\nThe main contributions of this work are:\n\n• We propose a novel method of complex facial expression recognition using continual learning and few-shot learning. The method uses data augmentation, knowledge distillation of basic expressions, and a novel Predictive Sorting Memory Replay to reduce catastrophic forgetting and improve performance using few training examples. • We demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on the state-of-the-art in non-continual learning methods by 13.95%. • We achieve the current state-of-the-art in continual learning for complex facial expression recognition, with 74.28% Overall Accuracy on new classes (an improvement of 0.67%) in a comprehensive analysis using experiments that compare with other continual learning methods.\n\n• Our method is the first to apply few-shot learning to complex facial expression recognition to the best of our knowledge, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Background And Scope",
      "text": "The majority of prior research on FER uses a categorical expression labelling system based on  [3] , who defined six basic emotions expressed on the human face: anger, disgust, fear, happiness, sadness, and surprise, with contempt later added by  [4] . These basic expressions are apparently conveyed and perceived similarly across different cultures  [5] . High recognition performance in recent years indicates that FER using basic expression labels is essentially solved (see Table  I ).\n\nHowever, human beings express a wide range of emotions through facial expressions that do not fit into predefined categories, and there is evidence that no such basic, prototypical emotion categories exist  [16] . Instead, FER develops naturally over time in humans, who are able to identify new, complex emotions on the fly as they appear  [1] ,  [13] . To approach human-like FER performance, a machine should be able to recognise complex expressions of emotion such as happily disgusted, and distinguish them from similar emotions like  happy, disgusted, or happily surprised. These compound expressions are more than the sum of their parts; they are distinct concepts which express a unique emotion  [15] . To humans, such synthesising of known concepts to form new ones comes relatively naturally, and we are able to learn, process and recognise new compound expressions using very little data. However, the state-of-the-art in FER still has difficulty with such cognitive complexity, due to the similarity of features across both basic and compound expressions. AI performs significantly worse at complex FER compared to basic FER, as seen in Table  I .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Fer System Architectures And Design",
      "text": "Complex facial expressions can be represented in a number of ways, such as combinations of basic expressions like happily surprised (called compound expressions), combinations of expressions in sequence such as surprise becoming anger, or as new expressions formed from adding a temporal dimension to basic ones, such as depression, which can be categorised as a persistent sadness over time  [17] . The Facial Action Coding System (FACS)  [3]  defines 44 action units (AUs), each one representing the activation of particular facial muscles groups. For example, a happy expression may be composed of AUs 12 (lip corner puller) and 25 (lips part) since this expression often involves the activation of these muscle groups, whereas a disgusted expression may be composed of AUs 10 (upper lip raiser) and 17 (chin raiser). The compound expression happily disgusted predominantly uses AUs 10, 12 and 25, which is the intersection of AUs found in each of the basic expressions happy and disgusted  [15] . Our work focuses solely on compound FER as a form of complex FER. Some traditional FER methods use manual feature extraction techniques such as local binary patterns  [7] ,  [18] , however deep learning is currently the most popular method, enabling automatic feature extraction through gradient descent backpropagation, and achieving the current state-of-the-art in FER (see Table  I ). High-performing deep learning architectures for computer vision such as ResNet  [19]  and Xception  [20]  can be used by pre-training a model on a large general image database like ImageNet  [21] , with fine-tuning to adapt to the FER task. This technique is used in a few state-of-the-art models such as  [12]  but is currently underutilised.\n\nThere are a number of common problems in attaining high accuracy for complex FER. Machine learning, particularly deep learning, requires a huge amount of training data to avoid over-fitting, however currently facial expression databases are not sufficient for this task compared with those used for general image recognition or object detection, such as ImageNet  [21] . Subject identity bias (differences in personal attributes of the subjects used for sample images, such as age, gender, ethnic background and level of expressiveness) reduces generalisation performance since there is increased bias towards recognising specific subjects or features (e.g. skin colour or wrinkles) in the training data. Many state-of-theart FER methods employ various pre-processing methods to reduce the effect of these issues and increase performance.\n\nFace detection and alignment can increase FER performance by creating consistency in training images. For example, the eyes are generally in the same region for images processed by face detection, allowing spatial models such as Convolutional Neural Networks (CNNs) to more easily learn these features. Some common facial detection and alignment techniques include the Viola-Jones face detector  [22]  as used by  [18] ,  [23] , generative object detection  [24]  as used by  [25] , supervised descent  [26]  as used by  [6] ,  [18] , and the RetinaFace face detector  [27] .\n\nData augmentation applies random transformations to the training image data, such as rotation, flipping and contrast adjustment, generating new images with the same semantic content but different data values. This alleviates the problem created by using smaller FER datasets for deep learning and improves generalisation performance, as the model is trained on more varied data. Data augmentation is used in some FER methods, such as  [6] ,  [9] ,  [14] , but is not used in many stateof-the-art methods, limiting their performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Few-Shot Learning And Continual Learning",
      "text": "As discussed in Section II-A, to improve the performance of complex FER systems, a novel approach is required which can incrementally learn new complex expressions as they appear, in the same way that a human might induce the expression happily surprised from its prior knowledge of happy and surprised, using only a few new examples.\n\nContinual learning is a group of methods which focuses on the problem of incrementally adding classes to a trained model. Most of these methods suffer from catastrophic forgetting, which is a reduction in performance on previously learned classes due to substantial weight changes when learning new ones  [13] . Recent advances in continual learning applied to other domains have partially solved the problem of catastrophic forgetting using such methods as memory replay  [28]  and knowledge distillation  [29] . Some pioneering methods of complex FER such as  [13] ,  [14] ,  [30]  also use continual learning.\n\nFew-shot learning is a research area focused on training machine learning models with a very small amount of training data, often down to a single sample, making them more adaptable to real-world applications such as streaming video data, security footage where the subject appears only briefly, or learning from a single passport or identification photo supplied by a user. Applying few-shot learning to the domain of complex FER could have a number of applications in areas such as human-computer interaction.\n\nFor example, a model that is able to recognise new complex expressions from only a few labelled images could learn a human's emotional state that it hasn't been trained on previously, if they state what they're feeling whilst looking at the machine's camera input. By capturing the facial expression images with some additional processing such as speech recognition, a machine that achieves high performance using few-shot learning could learn this new expression on-thefly and recognise it the next time it sees it. This approach could enable AI systems to perform better in applications of human-computer interaction such as AI assistants and robotic nurses, which would require human-like levels of emotional intelligence in communication.\n\nThe main problem with few-shot learning is under-fitting, whereby the model does not have enough varied training examples to generalise well on recognising unknown samples. A number of techniques are used to increase few-shot learning performance in state-of-the-art methods.  [31]  achieves high accuracy in one-shot learning for facial identity recognition using supervised auto-encoders to augment a single training sample, producing variations in illumination, expression, occlusion and pose.  [32]  uses a hybrid feature enhancement network to increase the importance of low-level features such as image textures for semantic segmentation tasks.  [33]  uses a pseudo-Siamese network to learn classes from a new domain by training one model branch on the original images, and another branch on augmented sketch map images produced by extracting the images' contour features. This method also cites as inspiration the ability of humans to build on prior knowledge when learning new tasks. Common themes amongst these methods of few-shot learning are data augmentation and enhanced feature extraction.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Knowledge Distillation",
      "text": "Knowledge distillation  [29]  is a method for transferring knowledge from a teacher model to a student model. The student uses the predicted probabilities from the teacher's output as a soft target in place of the usual hard target (ground truth labels) when calculating the loss between predicted and target outputs. Knowledge is distilled by the student model through gradient descent back-propagation of the distillation loss between the predictions and soft teacher targets. The student thus learns a mapping from its inputs to all the probable label outputs as learned by the teacher, enabling it to quickly form a rich representation of the teacher's knowledge. The distillation loss is a modified cross-entropy loss whereby the teacher model's prediction output is modified by a temperature T to attain the soft prediction output y sof t given by (  1 ), where k is the number of classes and z ∈ R k is a vector of logits (the output prior to the softmax activation layer). As seen in Figure  2 , higher temperatures smooth the probability distribution of the soft outputs, giving increased weight to smaller probabilities and decreased weight to the highest probability, whilst retaining the probabilities' relative ranking order.\n\n(2) demonstrates the categorical cross-entropy loss L used during standard model training, whilst (3) is the distillation loss L dist used for knowledge distillation, whereby y is the true label (one-hot encoded), ŷ is the prediction output, and ŷt-soft and ŷsoft are the soft teacher prediction and soft student prediction outputs, respectively.\n\nKnowledge distillation can reduce the effects of catastrophic forgetting in continual learning by reinforcing a model's knowledge of known classes when learning new classes.  [13]  uses knowledge distillation with continual learning for FER through an indicator loss function which is the weighted sum of the distillation loss and hard loss. An indicator function allows the model to treat each new training example differently depending on whether it is reinforcing prior knowledge or gaining new knowledge. For new classes, the hard loss is weighted more and for old classes the distillation loss is weighted more. FER accuracy with this method is reduced significantly less with each new class learned, compared to other methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Representative Memory Replay",
      "text": "Memory replay techniques are used in continual learning to store a subset of labelled training examples for known classes, which are added to the new class training data. This helps to prevent catastrophic forgetting by reinforcing the knowledge of known classes when learning new classes. It can also be used in conjunction with knowledge distillation, as in  [13] ,  [28] ,  [34] . Some memory replay methods use specific sample selection techniques with the aim of selecting the most representative samples of each label, such as  [28]  and  [34] . Other methods such as  [13]  instead use a randomly selected representative memory, stating that randomising the order of inputs when training a model improves generalisation performance. However, a major limitation of this work is that it is not true continual learning, as it draws from the full set of prior training examples using its random selection policy for memory replay. This effectively makes the results comparable to training a new model from the beginning with each new class. Our method aims to improve on  [13]  whilst adhering to the true principle of continual learning, that is in not having access to the prior training dataset except for a subset of data that is set aside and stored in a representative memory. We use a novel Predictive Sorting Memory Replay to select the samples which are most representative of their class. This enhances the knowledge distillation, thereby greatly reducing catastrophic forgetting and improving continual learning performance.\n\nOne of the advantages of continual learning is the potential for training a model to learn new classes in real-time without progressively using more resources. To this end, the representative memory is often kept to a fixed number of samples, K, over each continual learning iteration, as in  [13]  and  [34] . In this way m = K k samples are retained for each new class, where k is the number of observed classes so far. However, k increases with each continual learning iteration which progressively reduces the number of samples of each known class in the representative memory, and can reintroduce the effect of catastrophic forgetting. If instead K is allowed to increase, the representative memory can keep a constant number of samples for each label, reducing the effect of catastrophic forgetting at the expense of an ever-growing memory size.  [28]  evaluates both these methods and compares the results, which as expected shows a decrease in performance using a fixed K compared with an increasing K.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iii. Research Design And Methodology",
      "text": "In this section, the main components of a novel system architecture and method for compound FER are described. The code to reproduce this method is available at https://github. com/AngusMaiden/complex-FER. The proposed method has three phases:\n\n• A Basic FER Phase in which a Basic FER Model learns to recognise six basic expression classes from a dataset of labelled static images of facial expressions. The re-use of the trained Basic FER model in the Continual Learning and Few-shot Learning Phases is intended to somewhat mimic the human pattern of learning as we age, by learning complex concepts like compound facial expressions only after the basic concepts, i.e. basic expressions, are understood.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Basic Fer Phase",
      "text": "In this initial phase, a Basic FER Model learns the mapping between input images X basic and their ground truth expression labels y basic ∈ R k basic . Each image is labelled with one of k basic facial expression labels. The model has two main components:\n\n1) A feature extractor F E using a base residual network, ResNet50V2  [35] , learns feature mappings of images through stacked convolutional blocks with residual connections and dense layers on top. The network is pretrained on ImageNet  [21]  to extract common image features such as shapes and lines, then fine tuned on the FER dataset. The network's output is a feature vector\n\nwhere k F E is the number of output nodes of F E. 2) A classification layer CL basic with k basic output nodes.\n\nThis layer takes as input the output of F E(X basic ), and outputs a logit vector z ∈ R k basic , where k basic is the number of basic expressions from the Basic FER Phase. A forward pass of the Basic FER Model from inputs X basic to logit outputs z is given by (4). A standard softmax activation function as in  (1) , where T = 1, is applied to the logit vector to produce the probability vector ŷ ∈ R k basic .\n\nA categorical cross-entropy loss L cat calculates the error between the predicted labels ŷ and ground truth labels y basic for each input image, and is calculated according to  (2) .\n\nThe main aim of the Basic FER Phase is to train a model to achieve high FER accuracy with robust feature mappings from images of basic facial expressions. The feature mappings are then transferred using knowledge distillation to the models in the Continual Learning and Few-shot Learning Phases. It is hypothesised that compound FER accuracy can be improved using minimal new training examples through knowledge distillation of basic expressions, as compound and basic facial expressions share some basic features.\n\nIn order to achieve a high recognition accuracy in the Basic FER Phase, and subsequently in the Continual Learning and Few-shot Learning Phases, a good model architecture is needed. Based on empirical knowledge from state-of-the-art research in computer vision, the architecture of the Basic FER Model is designed as displayed in Figure  4 .\n\nThe input pipeline for the Basic FER model takes 3-channel (RGB) images of size 224 x 224 x 3 in batches. Each image is for batch in Batch(X basic ,y basic ) do\n\nfor x, y in batch do 4:\n\nx ← F aceDetection(x)\n\n5:\n\nx ← N ormalize(x)\n\n6:\n\nx ← Augment(x)\n\nŷ ← Sof tmax(z)",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "9:",
      "text": "L cat ← CrossEntropy(ŷ, y) Accuracy = Average(acc) 19: end for",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Continual Learning Phase",
      "text": "The Continual Learning Phase is an iterative cycle whereby labelled images from each complex expression class are added to the existing dataset and trained sequentially. At each iteration i, one new complex expression class is selected, which is comprised of images X newi and associated true labels\n\nwhere k i is the total number of expression classes available at this iteration (k i =k basic +i). The total number of known (trained) classes at the beginning of each iteration is k i-1 . The phase runs for k compound iterations, whereby k compound is the number of compound expression classes available. For each iteration i, a new node is added to the classification layer of the previous model, CL i-1 , with the new layer denoted as CL i . This new node's weights are randomly initialised using a Glorot Uniform distribution, whilst the other k i-1 nodes inherit their weights from the corresponding CL i-1 layer of the trained model from the previous iteration i-1. In the first iteration, whereby i = 1, the previous model is the trained model from the Basic FER Phase, such that CL 0 =CL basic . The structure of the feature extractor F E does not change at each iteration and is reused as-is. Figure  3  provides a visualisation of the continual learning method and process flow.\n\nA representative memory X memi is used to store a portion of training samples from the previous iteration i -1 together with the new class training samples X newi . A number of training samples X selecti , and their associated labels y selecti , are selected according to the Predictive Sorting Memory Replay (PSMR) selection policy. The pseudo-code for this policy is outlined in Algorithm 2.\n\nUsing this policy, the representative memory is therefore comprised of images that are the most representative of their respective class. When learning new classes, these representative memory samples are trained alongside the new class samples, which reinforces the knowledge of previous classes and reduces the effect of catastrophic forgetting. In a similar way, humans retain memories of only the most pronounced moments of an experience, which efficiently enables recognition and classification of the entire experience. Once initialised, the representative memory does not acquire any new samples from the Basic FER Phase data, to emulate human learning whereby the raw data from prior experiences is no longer available, and only memories are retained. This also ensures the method is aligned with practical applications that may have limiting memory and computation requirements, such as mobile computing, IoT and robotics.\n\nA forward pass of the continual learning model at iteration i from inputs X memi to output logits z i is given in  (5) . A Algorithm 2 Predictive Sorting Memory Replay (PSMR) Require: K ← number of samples in representative memory Require: k i ← number of classes at iteration i Require: m ← K ki-1\n\nRequire: X memi-1 , y memi-1 ← representative memory of previous iteration 1: if i = 1 then 2:\n\nX selecti , y selecti ← randomly select K X basic images and associated labels 3: else\n\nfor each ŷj in j classes where j = 1, ..., k i do 6:\n\nSort(ŷ j ) in order of prediction probability 7:\n\nAppend(y selecti ) ← Select top m sorted labels ŷj 8:\n\nAppend(X selecti ) ← images for y selecti labels 9:\n\nend for 10: end if 11: return X selecti , y selecti softmax activation function as in  (1) , where T = 1, is applied to the logit vector to produce the prediction vector ŷi ∈ R ki .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Distillation Loss",
      "text": "The loss function of a neural network calculates the difference between the target output, usually the ground truth labels, and the predicted output of a model. By minimising the loss through gradient descent optimisation, we aim to minimise this difference, bringing the predicted outputs closer to the real outputs with each gradient descent step. In this phase, the loss L is the weighted sum of a standard categorical crossentropy loss L cat and distillation loss L dist , with a distillation weighting factor, γ : 0 ≤ γ ≤ 1, as shown in  (6) .\n\nThe cross-entropy loss L cat calculates the error between the predicted labels ŷi and ground truth labels y i for each input image, as in  (2) . The distillation loss L dist calculates the error between ŷt-soft i and ŷsoft i , as in (3). Our method demonstrates a unique handling of the distillation loss whereby the teacher model is a static copy of the trained model attained in the Basic FER Phase. The weights of this teacher model are never updated in the Continual Learning Phase. A forward pass of the teacher model at iteration i from inputs X memi to teacher logits z t i is given in  (7) . A softmax activation function with temperature T , as in  (1) , is applied to the logits z i from (5) to produce student predictions ŷsoft i and to the teacher logits z t i from  (7)  to produce teacher predictions ŷt-soft i , which are used in the calculation of the distillation loss L dist  (6) .\n\nWith each iteration, L dist naturally increases, adding a higher penalty to the overall loss L. This is because L dist is calculated between the vectors ŷt-soft ∈ R k basic (zeropadded) and ŷsoft i ∈ R ki . Due to the zero-padding, there will be i constant zero values in ŷ, causing L dist to increase with each step i. To counteract this effect, a distillation weight decay is used, as in  (8) , which decreases the value of γ with each iteration i, thereby reducing the weight of L dist in the overall loss L (6). This function approaches but never equals the asymptote at γ = 0, ensuring there is always some amount of distilled knowledge of basic expressions contributing to the overall loss.\n\nAfter each epoch of training, the accuracy of the continual learning model is tested using a held-out test dataset. Training is stopped when the test accuracy is no longer improving over previous epochs. We then record the test accuracy for iteration i for all k i trained expression classes, as well as the single class test accuracy for the newest facial expression. Evaluating this method of knowledge distillation in the Continual Learning and Few-shot Learning Phases tests the hypothesis that compound FER accuracy can be improved using few new training examples through knowledge distillation of basic expressions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Few-Shot Learning Phase",
      "text": "In this phase, the hypothesis that better complex FER performance can be achieved through knowledge distillation of basic expressions is tested in the context of few-shot learning, whereby a very limited number of training examples are used. We run one experiment for each of the k compound compound facial expressions, with repeat trials of 5, 3 and 1 training examples. To train these expressions, the same system architecture and methodology is used as in the Continual Learning Phase, with the exception of the Representative Memory Replay component, which is excluded from this phase as it is specific to continual learning. Each few-shot learning experiment is equivalent to one iteration of the continual learning experiment. The model's parameters are reset after each experiment, before a new class is chosen. These experiments test the model's ability to learn each separate complex expression class using very few training examples through distilling the knowledge of basic facial expressions. The aggregate results of these separate experiments allows us to generalise about the model's capabilities for few-shot learning with compound FER.\n\nFor this phase, at the beginning of each experiment j ∈ [1, . . . , k compound ], one new compound expression class is selected which is comprised of images X newj and associated true labels\n\nAs with the first iteration of the Continual Learning Phase, the classification layer CL f ewshot is comprised of a new node added to the classification layer of the Basic FER model CL basic . For each experiment j, a forward pass of the fewshot learning model produces the output logits z j as in  (9) , whilst a forward pass of the teacher model produces the output logits z t j as in  (10) . A softmax activation function is applied to the logit vectors z j and z t j to produce the prediction vectors ŷsoft j and ŷt-soft j , respectively, as in  (1) . The cross-entropy X newi , y newi ← select images, labels of next expression in list 3:\n\ny memi ← Concatenate(y selecti , y newi )\n\n6:\n\nCL i ← Add one output node to CL i-1",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "7:",
      "text": "for epoch in no. of epochs do 8:\n\nfor batch in Batch(X memi , y memi ) do\n\nfor x, y in batch do 10:\n\nx ← F aceDetection(x)\n\n11:\n\nx ← N ormalize(x)\n\n12:\n\nx ← Augment(\n\nz ← CL i (F E(x))\n\n14:\n\nŷ ← Sof tmax(z)\n\n15:\n\nŷt-soft ← Sof tmax(z t , T )\n\nŷsoft ← Sof tmax(z, T )\n\nL cat ← CrossEntropy(ŷ, y)\n\nL dist ← CrossEntropy(ŷ t-sof t , ŷsoft )\n\nL ← (γ -1)\n\nend for",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "22:",
      "text": "L ← Average(L) over batch 23:\n\nUpdate model weights (back-propagation of L) Update weight decay γ according to (8) 32: end for loss L cat calculates the error between the predicted labels ŷj and ground truth labels y j for each input image, as in  (2) . The distillation loss L dist calculates the error between ŷt-soft j and y sof t j , as in (3). The loss function is given in  (6)  and calculated as in the Continual Learning Phase.\n\nAfter each epoch of training, the accuracy of the few-shot learning model is tested using the same held-out test dataset from the Continual Learning Phase. Training is stopped when the test accuracy is no longer improving over previous epochs. Few-shot learning experiments are conducted using 5, 3 and 1 training examples per class, with the single class test accuracy and number of training steps recorded for each j ∈ R k compound compound expression classes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Iv. Grad-Cam Visualisation Of Basic And Compound Features",
      "text": "This experimental setup was also used to visualise the features of basic expressions which are distilled into the model when learning new complex expressions. The Grad-CAM  [36]  method was applied to visualise the areas of each image that are most activated for particular expression classes. Using this method, we can visualise the features of basic expressions that are common to those compound expressions derived from them. Initially, Grad-CAM heat maps were produced from the Basic FER model for the k basic basic expressions. Similarly to the Few-Shot Learning Phase, here we use just one iteration of the Continual Learning Phase, without the representative memory, training the model for j ∈ R k compound compound expressions and resetting the weights each time, but using all available training examples in order to produce the strongest feature activations. The trained models were then used to produce Grad-CAM feature maps for each expression which highlights the areas of the image which are most activated when predicting that expression. Connections between the basic and compound expressions were drawn by identifying semantic relationships between them and their Grad-CAM feature maps in order to highlight facial expression features which are common to these expressions.\n\nFor example, Figure  5  demonstrates the Grad-CAM visualisation of the basic features angry, disgusted and fearful in relation to the compound expressions angrily disgusted and fearfully angry. In this visualisation, we can clearly see the features which are common to both the basic and compound expressions, such as in the furrowed brow which is common to the fearful, angry and fearfully angry expressions. We can also see how the angrily disgusted expression shares the downturned mouth with the disgusted expression and the furrowed brow with the angry expression. These common features also represent the knowledge which is transferred from the Basic FER model to the continual learning model using knowledge distillation. This visualisation, together with the improved performance in the Continual Learning and Few-Shot Learning Phases of the models which use knowledge distillation of basic features, supports the hypothesis that knowledge of basic features can improve the recognition of compound facial expression labels which share those features.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Empirical Evaluation And Results",
      "text": "The dataset used for evaluation is the Compound Facial Expressions of Emotion (CFEE) database  [15] . It contains 5044 images of 230 different subjects with acted facial expressions in a controlled lab environment. The dataset is labelled with 21 different facial expressions. Grad-CAM visualisation of basic features in angrily surprised disgustedly surprised and compound expressions For evaluation, a subject-independent k-fold cross validation method was used whereby the dataset is partitioned into 10 folds of 23 subjects each. 1 fold is held out as the test set, with the training data comprising the remaining 9 folds. The fold split points are chosen according to subject, such that the test data contains images of completely different subjects than the training data. This aims to reduce the effect of subject identity bias and improve the generalisation performance of the trained model, whilst also simulating the model's performance with real subjects that it has not seen before.\n\nIn the Basic FER Phase, the model is trained to completion, with the accuracy evaluated using the test dataset at the end of each epoch. The next test fold is then selected, with the training data comprising the remaining 9 folds, and the model is again trained and evaluated. This process is iterated over 10 times for the 10 possible test and training set combinations. The final evaluation results are aggregated to produce a maximum, mean and standard deviation for Basic FER accuracy. A initial range of good hyperparameters for the Basic FER Model were chosen based on empirical knowledge of good deep learning architectures. These hyperparameters form a multi-dimensional search space which can be searched to optimise the model. The Hyperband optimisation algorithm  [37]  was chosen as the search algorithm and implemented using KerasTuner  [38] . Hyperband uses an infinite-armed bandit method combined with successive halving to explore the search space and converge to an optimal set of hyperpa-rameters. Following this tuning process, the optimal hyperparameters were selected as in Table  II   For the Continual Learning Phase, the best performing test set from the Basic FER Phase cross validation was used. The continual learning model was evaluated using the method and metrics developed by  [13] . Using this method, the sequence of new classes learned in continual learning are randomised in order to test the model's invariance to the sequence order of expression classes. For each randomised list of complex expressions, out of C total lists (where C = 10 for our experiments), the accuracy at each continual learning step i, over N number of test samples, is recorded. The average step accuracy, aveSA i is the average accuracy at step i over all lists C, as shown in  (11) .\n\nWhere H(y j n , ŷj n ) is an indicator function that returns 1 if y j n = ŷj n and 0 if y j n ̸ = ŷj n . Furthermore, the OverallAccuracy is calculated as the average aveSA over all continual learning steps i, as demonstrated in  (12) .\n\nThe OverallAccuracy results obtained in the Continual Learning Phase are displayed in Table  IV , along with comparable baseline results as reported in  [13] . Furthermore, out of the 10 randomised complex expression lists, Figures  8  and 9  demonstrate the performance of the best, worst, and nearest to OverallAccuracy in terms of the average accuracy over each step i.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Method",
      "text": "Overall Accuracy Tree-CNN  [39]  (0.5107, 0.6437) Fine-tuning  [40]  (0.6837, 0.7418) LwF  [41]  (0.5373, 0.6638) TOPIC  [42]  (0.5268, 0.7168) Deep SLDA  [43]  (0.5387, 0.7478) REMIND  [44]  (0.5398, 0.7463) Lucir-CNN  [45]  (0.5698, 0.7639) PODNet-CNN  [46]  (0.5991, 0.8163) Lucir  [45]  /w AANets  [47]  (0.6414, 0.8598) PODNet-CNN  [46]  /w AANets  [47]  (0.6781, 0.8697) iCaRL  [48]  (0.7138, 0.8327) DCLEER  [13]  (0.7361, 0.8904) Our Method (0.7428, 0.7327) Our Method (excl. singular labels*) (0.8232, 0.7810) *Additionally, a second experiment was conducted using only compound facial expressions whose labels are composed of two basic expression labels, such that the expressions hatred, appalled, and awed are omitted and k compound = 12. The semantic relationship between these singular expressions and the six basic facial expressions is not as clear as with expressions like happily surprised, which is clearly related to happy and surprised. By removing these singular expressions, we can focus on FER for compound expressions and more thoroughly test the hypothesis that higher accuracy can be achieved for compound FER through knowledge distillation of the features of basic expressions which are common to related compound expressions.\n\nTo further evaluate the use of continual learning for compound FER over other methods, the baseline metrics reported by  [13]  are again used. Here, a number of state-of-of-theart non-continual learning methods are used to evaluate the 21 emotion labels of the CFEE  [15]  database. The results are compared with our method as evaluated after the final continual learning iteration with 21 labels, and are displayed in Table  V .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "Accuracy AlexNet  [49]  0.5637 VGG-16  [50]  0.5018 VGG-19  [50]  0.4971 Inception-v3  [51]  0.4288 DenseNet-201  [52]  0.4862 SCN  [53]  0.4621 PSR  [54]  0.5591 ESRs  [55]  0.5781 Our Method 0.7176 Our Method (excl. singular labels*) 0.7182",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Table V Avesa At Final Iteration Of Our Method And Non-Continual Learning Methods",
      "text": "To evaluate the Few-shot Learning Phase, the best performing test set from the Basic FER Phase was used. The",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "A novel method for compound facial expression recognition was developed by distilling the knowledge of basic expressions when learning new compound expressions. Two main experiments were conducted in continual learning and fewshot learning.\n\nWe demonstrate improvements in continual learning for complex FER through our novel knowledge distillation and Predictive Sorting Memory Replay techniques, achieving the state-of-the-art with 74.28% Overall Accuracy on new classes only (an improvement of 0.67%). The Overall Accuracy on all classes is 73.27% which is comparable to baseline results and improved by tuning the Temperature (T ) and Distillation Weight (γ) hyperparameters.\n\nOur method also demonstrates an improvement in accuracy over other state-of-the-art non-continual learning methods for facial expression recognition by 13.95%. This demonstrates the benefits of our approach to learning facial expressions through continual learning, by first learning to recognise basic facial expressions and then synthesising that knowledge to learn new complex facial expressions in a similar way to humans.\n\nOur method achieves 100% accuracy in all classes using only 5, 3 or 1 samples in few-shot learning, which is the state-of-the-art in the facial expression recognition domain to the best of our knowledge. These results also demonstrate the benefits of learning to recognise basic facial expressions prior to learning complex facial expressions, as the model was able to very quickly learn the new expression classes using only a limited number of image samples through knowledge distillation of basic features.\n\nBy visually inspecting the Grad-CAM heatmaps of basic expression images with those of compound expressions, a strong correlation was found between the activations of features in basic expressions and that of features in compound expressions. The activated areas also appear to align with the facial action encoding system  [3] . In future works, the use of action units as an additional feature extraction component may also be able to increase the accuracy of the model for both basic facial expression recognition and complex facial expression recognition.\n\nOne limitation of this work is in evaluating the model using only one dataset. By evaluating on multiple datasets, we could get a better picture of the model's generalisation performance with a wider variety of subjects and image conditions. Evaluating the model on in-the-wild complex emotion datasets such as EmotioNet  [56]  and AffectNet  [57]  would also prove useful in assessing the model's generalisation and suitability for practical applications such as human-computer interaction, as these datasets more closely resemble real facial expressions.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples from CFEE database [15]. Compound expressions such as",
      "page": 2
    },
    {
      "caption": "Figure 2: Softmax output of ’Surprised’ image with different temperatures.",
      "page": 4
    },
    {
      "caption": "Figure 2: , higher temperatures smooth the",
      "page": 4
    },
    {
      "caption": "Figure 3: System architecture and method",
      "page": 5
    },
    {
      "caption": "Figure 4: The input pipeline for the Basic FER model takes 3-channel",
      "page": 5
    },
    {
      "caption": "Figure 4: Basic FER model architecture",
      "page": 6
    },
    {
      "caption": "Figure 3: provides a visualisation of the continual learning",
      "page": 6
    },
    {
      "caption": "Figure 5: demonstrates the Grad-CAM visu-",
      "page": 8
    },
    {
      "caption": "Figure 5: Grad-CAM Visualisation of basic features in angrily disgusted and",
      "page": 9
    },
    {
      "caption": "Figure 6: Grad-CAM visualisation of basic features in angrily surprised",
      "page": 9
    },
    {
      "caption": "Figure 7: Grad-CAM visualisation of basic features in disgustedly surprised",
      "page": 9
    },
    {
      "caption": "Figure 8: Best, worst and near-average accuracy at each continual learning step i",
      "page": 11
    },
    {
      "caption": "Figure 9: Best, worst and near-average accuracy at each continual learning step i (excl. singular labels*)",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nDataset\nClasses\nAcc.": "Basic FER Methods"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Deep learning using CNN and incep-\ntion blocks [6]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Manual\nfeature extraction using Local\nBinary Patterns [7]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Deep neural network using multi-step\npre-processing\nand\nfeature\nextraction\n[8]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Deep network with joint fine-tuning [9]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Boosted deep belief network [10]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Complex FER Methods"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Deep CNN using Optical Flow feature\nextraction [11]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Deep CNN using knowledge distilla-\ntion of AUs and SVM classifier\n[12]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Continual\nlearning\nusing\nknowledge\ndistillation [13]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "Incremental\nactive\nlearning\nfrom\nsparsely annotated data [14]"
        },
        {
          "Method\nDataset\nClasses\nAcc.": "SVM classifier using extracted shape\nand appearance features [15]"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Basic FER Phase": "Initial Training epochs"
        },
        {
          "Basic FER Phase": "Fine Tuning Training epochs"
        },
        {
          "Basic FER Phase": "Batch size"
        },
        {
          "Basic FER Phase": "Optimisation function"
        },
        {
          "Basic FER Phase": "Initial Learning Rate"
        },
        {
          "Basic FER Phase": "Fine Tuning Learning Rate"
        },
        {
          "Basic FER Phase": "Continual Learning and Few-shot Learning Phases"
        },
        {
          "Basic FER Phase": "Training epochs"
        },
        {
          "Basic FER Phase": "Batch size"
        },
        {
          "Basic FER Phase": "Optimisation function"
        },
        {
          "Basic FER Phase": "Learning Rate"
        },
        {
          "Basic FER Phase": "Temperature (T)"
        },
        {
          "Basic FER Phase": "Distillation Weight\n(γ)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Tree-CNN [39]",
          "Overall Accuracy": "(0.5107, 0.6437)"
        },
        {
          "Method": "Fine-tuning [40]",
          "Overall Accuracy": "(0.6837, 0.7418)"
        },
        {
          "Method": "LwF [41]",
          "Overall Accuracy": "(0.5373, 0.6638)"
        },
        {
          "Method": "TOPIC [42]",
          "Overall Accuracy": "(0.5268, 0.7168)"
        },
        {
          "Method": "Deep SLDA [43]",
          "Overall Accuracy": "(0.5387, 0.7478)"
        },
        {
          "Method": "REMIND [44]",
          "Overall Accuracy": "(0.5398, 0.7463)"
        },
        {
          "Method": "Lucir-CNN [45]",
          "Overall Accuracy": "(0.5698, 0.7639)"
        },
        {
          "Method": "PODNet-CNN [46]",
          "Overall Accuracy": "(0.5991, 0.8163)"
        },
        {
          "Method": "Lucir\n[45]\n/w AANets [47]",
          "Overall Accuracy": "(0.6414, 0.8598)"
        },
        {
          "Method": "PODNet-CNN [46]\n/w AANets [47]",
          "Overall Accuracy": "(0.6781, 0.8697)"
        },
        {
          "Method": "iCaRL [48]",
          "Overall Accuracy": "(0.7138, 0.8327)"
        },
        {
          "Method": "DCLEER [13]",
          "Overall Accuracy": "(0.7361, 0.8904)"
        },
        {
          "Method": "Our Method",
          "Overall Accuracy": "(0.7428, 0.7327)"
        },
        {
          "Method": "Our Method (excl. singular labels*)",
          "Overall Accuracy": "(0.8232, 0.7810)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "AlexNet\n[49]",
          "Accuracy": "0.5637"
        },
        {
          "Method": "VGG-16 [50]",
          "Accuracy": "0.5018"
        },
        {
          "Method": "VGG-19 [50]",
          "Accuracy": "0.4971"
        },
        {
          "Method": "Inception-v3 [51]",
          "Accuracy": "0.4288"
        },
        {
          "Method": "DenseNet-201 [52]",
          "Accuracy": "0.4862"
        },
        {
          "Method": "SCN [53]",
          "Accuracy": "0.4621"
        },
        {
          "Method": "PSR [54]",
          "Accuracy": "0.5591"
        },
        {
          "Method": "ESRs [55]",
          "Accuracy": "0.5781"
        },
        {
          "Method": "Our Method",
          "Accuracy": "0.7176"
        },
        {
          "Method": "Our Method (excl. singular labels*)",
          "Accuracy": "0.7182"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expression": "",
          "5-shot": "Acc.",
          "3-shot": "Acc.",
          "1-shot": "Acc."
        },
        {
          "Expression": "Happily surprised",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Happily disgusted",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Sadly angry",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Angrily disgusted",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Appalled",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Hatred",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Angrily surprised",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Sadly surprised",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Disgustedly surprised",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Fearfully surprised",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Awed",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Sadly fearful",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Fearfully disgusted",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Fearfully angry",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        },
        {
          "Expression": "Sadly disgusted",
          "5-shot": "1.0",
          "3-shot": "1.0",
          "1-shot": "1.0"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "C Darwin"
      ],
      "year": "1872",
      "venue": "The Expression of the Emotions in Man and Animals"
    },
    {
      "citation_id": "2",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication without words"
    },
    {
      "citation_id": "3",
      "title": "Facial action coding system (facs),\" A human face",
      "authors": [
        "P Ekman"
      ],
      "year": "2002",
      "venue": "Facial action coding system (facs),\" A human face"
    },
    {
      "citation_id": "4",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "5",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "6",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter conference on applications of computer vision (WACV)"
    },
    {
      "citation_id": "7",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition using deep neural networks",
      "authors": [
        "J Li",
        "E Lam"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Imaging Systems and Techniques (IST)"
    },
    {
      "citation_id": "9",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "10",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "P Liu",
        "S Han",
        "Z Meng",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "A convolutional neural network for compound micro-expression recognition",
      "authors": [
        "Y Zhao",
        "J Xu"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "12",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Deep continual learning for emerging emotion recognition",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Complex emotion profiling: An incremental active learning based approach with sparse annotations",
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "16",
      "title": "Emotion and expression: Naturalistic studies",
      "authors": [
        "J.-M Fernández-Dols",
        "C Crivelli"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "17",
      "title": "Depression",
      "year": "2021",
      "venue": "Depression"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from facial expression based on fiducial points detection and using neural network",
      "authors": [
        "F Salmam",
        "A Madani",
        "M Kissi"
      ],
      "year": "2018",
      "venue": "International Journal of Electrical and Computer Engineering"
    },
    {
      "citation_id": "19",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein",
        "A Berg",
        "L Fei-Fei"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-015-0816-y"
    },
    {
      "citation_id": "22",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H.-W Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Deep learning for emotion recognition on small datasets using transfer learning",
      "doi": "10.1145/2818346.2830593"
    },
    {
      "citation_id": "24",
      "title": "A generative framework for real time object detection and classification",
      "authors": [
        "I Fasel",
        "B Fortenberry",
        "J Movellan"
      ],
      "year": "2005",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "25",
      "title": "Fully automatic recognition of the temporal phases of facial actions",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "26",
      "title": "Supervised descent method and its applications to face alignment",
      "authors": [
        "X Xiong",
        "F De"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "End-to-end incremental learning",
      "authors": [
        "F Castro",
        "M Marín-Jiménez",
        "N Guil",
        "C Schmid",
        "K Alahari"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "29",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "30",
      "title": "Wild facial expression recognition based on incremental active learning",
      "authors": [
        "M Ahmed",
        "K Woo",
        "K Hyeon",
        "M Bashar",
        "P Rhee"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "31",
      "title": "Single sample face recognition via learning deep supervised autoencoders",
      "authors": [
        "S Gao",
        "Y Zhang",
        "K Jia",
        "J Lu",
        "Y Zhang"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "32",
      "title": "Hybrid feature enhancement network for few-shot semantic segmentation",
      "authors": [
        "H Min",
        "Y Zhang",
        "Y Zhao",
        "W Jia",
        "Y Lei",
        "C Fan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Cross-domain few-shot learning based on pseudo-siamese neural network",
      "authors": [
        "Y Gong",
        "Y Yue",
        "W Ji",
        "G Zhou"
      ],
      "year": "2023",
      "venue": "Sci Rep"
    },
    {
      "citation_id": "34",
      "title": "icarl: Incremental classifier and representation learning",
      "authors": [
        "S.-A Rebuffi",
        "A Kolesnikov",
        "G Sperl",
        "C Lampert"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "36",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "37",
      "title": "Hyperband: A novel bandit-based approach to hyperparameter optimization",
      "authors": [
        "L Li",
        "K Jamieson",
        "G Desalvo",
        "A Rostamizadeh",
        "A Talwalkar"
      ],
      "year": "2017",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "38",
      "title": "",
      "authors": [
        "T O'malley",
        "E Bursztein",
        "J Long",
        "F Chollet",
        "H Jin",
        "L Invernizzi",
        "Kerastuner"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "39",
      "title": "Tree-cnn: a hierarchical deep convolutional neural network for incremental learning",
      "authors": [
        "D Roy",
        "P Panda",
        "K Roy"
      ],
      "year": "2020",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "40",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "authors": [
        "R Girshick",
        "J Donahue",
        "T Darrell",
        "J Malik"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Learning without forgetting",
      "authors": [
        "Z Li",
        "D Hoiem"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "42",
      "title": "Few-shot class-incremental learning",
      "authors": [
        "X Tao",
        "X Hong",
        "X Chang",
        "S Dong",
        "X Wei",
        "Y Gong"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Lifelong machine learning with deep streaming linear discriminant analysis",
      "authors": [
        "T Hayes",
        "C Kanan"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "44",
      "title": "Remind your neural network to prevent catastrophic forgetting",
      "authors": [
        "T Hayes",
        "K Kafle",
        "R Shrestha",
        "M Acharya",
        "C Kanan"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Learning a unified classifier incrementally via rebalancing",
      "authors": [
        "S Hou",
        "X Pan",
        "C Loy",
        "Z Wang",
        "D Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "46",
      "title": "Podnet: Pooled outputs distillation for small-tasks incremental learning",
      "authors": [
        "A Douillard",
        "M Cord",
        "C Ollion",
        "T Robert",
        "E Valle"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "47",
      "title": "Adaptive aggregation networks for classincremental learning",
      "authors": [
        "Y Liu",
        "B Schiele",
        "Q Sun"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "icarl: Incremental classifier and representation learning",
      "authors": [
        "S.-A Rebuffi",
        "A Kolesnikov",
        "G Sperl",
        "C Lampert"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "51",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "52",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "53",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "54",
      "title": "Pyramid with super resolution for in-the-wild facial expression recognition",
      "authors": [
        "T.-H Vo",
        "G.-S Lee",
        "H.-J Yang",
        "S.-H Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "55",
      "title": "Efficient facial feature learning with wide ensemble-based convolutional neural networks",
      "authors": [
        "H Siqueira",
        "S Magg",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "56",
      "title": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "Q Feng",
        "Y Wang",
        "A Martinez"
      ],
      "year": "2017",
      "venue": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "arxiv": "arXiv:1703.01210"
    },
    {
      "citation_id": "57",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}