{
  "paper_id": "2405.12853v2",
  "title": "Inconsistency-Aware Cross-Attention For Audio-Visual Fusion In Dimensional Emotion Recognition",
  "published": "2024-05-21T15:11:35Z",
  "authors": [
    "G Rajasekhar",
    "Jahangir Alam"
  ],
  "keywords": [
    "Audio-Visual Fusion",
    "Emotion Recognition",
    "Weak complementary relationships",
    "Cross-Attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Leveraging complementary relationships across modalities has recently drawn a lot of attention in multimodal emotion recognition. Most of the existing approaches explored cross-attention to capture the complementary relationships across the modalities. However, the modalities may also exhibit weak complementary relationships, which may deteriorate the crossattended features, resulting in poor multimodal feature representations. To address this problem, we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively select the most relevant features on-the-fly based on the strong or weak complementary relationships across audio and visual modalities. Specifically, we design a two-stage gating mechanism that can adaptively select the appropriate relevant features to deal with weak complementary relationships. Extensive experiments are conducted on the challenging Aff-Wild2 dataset to show the robustness of the proposed model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic Emotion Recognition (ER) is a challenging problem due to the complex and extremely diverse nature of expressions across individuals and cultures. Though emotion classification has been widely explored in the literature  [1] , they fail to capture the wide range of expressions on a finer granularity. Depending on the granularity of labels, regression of expressions can be formulated as ordinal (discrete) regression or dimensional (continuous) regression. Dimensional ER is even more challenging due to the high level of ambiguity in dimensional labels, often considered as valence and arousal. Valence reflects the wide range of emotions in the dimension of pleasantness, from being negative (sad) to positive (happy). On the other hand, arousal spans a range of intensities from passive (sleepiness) to active (high excitement). In this paper, we have focused on the problem of dimensional ER using audio-visual (A-V) fusion in the valence-arousal space.\n\nRecently, multimodal ER has achieved impressive performance by leveraging complementary relationships across audio and visual modalities in videos using Cross-Attention (CA)  [2, 3, 4, 5] . Though CA has been gaining momentum in capturing the complementary relationships, audio and visual modalities may not always pose strong complementary relationships with each other, they may also exhibit weak complementary relationships  [6] . It has been shown that the audio and visual modalities may demonstrate conflicting (when one of the modalities is noisy or paradoxical) or dominating (when one of the modalities is restrained or unemotional) relationships with each other for ER  [6] . When one of the modalities is extremely noisy or restrained (weak complementary relationships), leveraging the noisy modality to attend to a good modality may deteriorate the fused A-V feature representations  [7] . For a better understanding of the problem of weak complementary relationships for ER, we have provided an interpretability analysis by visualizing the attention scores of CA as shown in Fig 1 . \nWhen the modalities demonstrate strong complementary relationships, we can observe higher attention scores, thereby leveraging the complementary relationships (top image). On the other hand, when they do not strongly complement each other (bottom image), attending the rich vocal expressions with the noisy facial modality results in lower attention scores for audio, thereby degrading the audio representations also, losing the rich vocal expressions. Motivated by this insight, we investigate the prospect of adaptively choosing the most appropriate features based on their inter-modal relationships.\n\nIn this work, we propose an Inconsistency-Aware Cross-Attention (IACA) model, that focuses on intra-modal relationships using Self-Attention (SA) for weak complementary scenarios, while retaining the benefits of strong complementary relationships using CA. In particular, we propose a two-stage gating mechanism to collaboratively fuse the audio and visual features by automatically selecting the cross-attended or selfattended features based on strong or weak complementary rela-tionships respectively. The proposed model can also be adapted to any variant of the CA model, adding more flexibility to the CA framework by handling the weak complementary relationships. To the best of our knowledge, this is the first work to investigate the inconsistency issues (weak complementary relationships) across audio and visual modalities for ER. The major contributions of the paper can be summarized as follows: (i) We investigate the potential of CA and showed that weak complementary relationships degrade the audio-visual feature representations for dimensional ER. (ii) To deal with weak complementary relationships, we propose an IACA model, a two-stage gating mechanism to adaptively select the most appropriate features on-the-fly based on the strong or weak complementary relationships. (iii) A detailed set of experiments are conducted to evaluate the proposed model on the challenging Aff-Wild2 dataset. The proposed model achieves consistent improvement over multiple variants of CA models, achieving state-of-the-art performance for dimensional ER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Most conventional approaches for dimensional ER explored simple feature concatenation  [8, 9]  or LSTM-based fusion models  [10, 11] . In video-based ER, leveraging complementary relationships across the audio and visual modalities was found to be promising in outperforming the unimodal systems  [12] . With the massive success of transformers, several approaches have been proposed to exploit CA with transformers and found significant improvement for ER  [13] . Parthasarathy et al.  [14]  explored multimodal transformers, where CA is integrated with the SA module to obtain the A-V cross-modal feature representations. Praveen et al.  [4]  proposed a CA model to effectively capture the complementary relationships by allowing the modalities to interact with each other. They have further extended the approach by introducing joint feature representation to the CA framework  [3, 5]  and recursive fusion  [2] . Although these methods have shown remarkable success with CA, they rely on the assumption that the audio and visual modalities always strongly complement each other. When they pose weak complementary relationships, these methods will result in poor performance.\n\nGating mechanisms with attention have been widely explored for multimodal fusion to control the flow of modalities based on their importance to reduce redundancy  [15, 16]  or noisy modalities  [17, 18] . Aspandi et al.  [19]  proposed a gatedsequence neural network, where the gating mechanism is explored for temporal modeling to adaptively fuse the modalities based on their relative importance. Kumar et al.  [17]  explored the conditional gating mechanism using a nonlinear transformation by modulating the cross-modal interactions to learn the relative importance of modalities. Jiang et al.  [18]  also explored CA with a forget gate to discard the redundant information and focus on salient cross-modal representations for multimodal sentiment analysis. Unlike prior approaches, we have explored a conditional two-stage gating mechanism to tackle the problem of weak complementary relationships by dynamically selecting the semantic features pertinent to the synergic A-V relationships.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "A) Notations: For an input video sub-sequence S, L nonoverlapping video clips are uniformly sampled, and the corresponding deep feature vectors are obtained from the pre-trained models of audio and visual modalities. Let Xa and Xv denote the deep feature vectors of audio and visual modalities respectively for the given input video sub-sequence S of fixed size, which is expressed as Xa = {x 1 a , x 2 a , ..., x L a } ∈ R d×L and Xv = {x 1 v , x 2 v , ..., x L v } ∈ R d×L where d represents the dimension of the audio and visual feature vectors, and x l a and x l v denote the audio and visual feature vectors of the video clips, respectively, for l = 1, 2, ..., L clips. B) Preliminary-Cross Attention: In this section, we briefly introduce the CA  [4]  (baseline fusion model) as a preliminary to the proposed IACA model. Given the audio and visual feature vectors Xa and Xv for a video sub-sequence S, the cross-correlation across the modalities are computed as Z = X ⊤ a W Xv (W denotes cross-correlation weights). Next, we compute the CA weights of audio and visual features, Aa and Av by applying column-wise softmax of Z and Z ⊤ , respectively:\n\nAfter obtaining the CA weights, they are used to compute the attention maps of the audio and visual features to make them more comprehensive and discriminative as:\n\nThe attention maps are then added to the corresponding features to obtain the attended features, which is given by:\n\nC) Inconsistency-Aware Cross-Attention Model: To deal with weak complementary relationships, we design two-stage gating layers, where the first stage selects the cross-attended or self-attended features based on strong or weak complementary relationships, respectively for the individual modalities. In the second stage, we choose the most relevant features among the joint A-V features and the features of individual modalities to mitigate the impact of extremely noisy modality. The gating layers of the first stage are designed using a Fully Connected (FC) layer with two output units, one for the cross-attended feature and one for the self-attended feature, which is given by\n\nwhere W gl,a , W gl,v ∈ R d×2 , are the learnable weights of the gating layers and Wgo,a ∈ R L×2 , Wgo,v ∈ R L×2 are outputs of gating layers of audio and visual modalities, respectively. To obtain probabilistic attention scores, the output of the gating layers is activated using a softmax function with a small temperature T , as given by\n\nwhere Ga ∈ R L×2 , Gv ∈ R L×2 denotes the probabilistic attention scores of audio and visual modalities, respectively. K denotes the number of output units of the gating layer (here K = 2). The probabilistic attention scores of the gating layer help to estimate the relevance of cross-attended or self-attended features. The two columns of Ga correspond to the scores of self-attended features (first column) and cross-attended features (second column). To multiply with the corresponding feature vectors, each column is replicated to match the dimension of the corresponding feature vectors, denoted by Ga0, Ga1 and Gv0, Gv1 for audio and visual modalities, respectively, which is further fed to the ReLU activation function as:\n\nXatt,ga = ReLU(Xa ⊗ Ga0 + Xatt,a ⊗ Ga1)\n\nwhere ⊗ denotes element-wise multiplication. Now the attended features from the gating layers Xatt,ga and Xatt,gv are fed to the joint representation layer (concatenation + FC) to match the dimension d of individual modalities, denoted as Xatt,gav. In the second stage, we compute the attention scores for the features Xatt,ga, Xatt,gv and Xatt,gav using A-V gating layer, where the gating layer is designed with 3 output units. The A-V gating layer helps to mitigate the adverse effects of severely corrupted or missing modalities and focus more on the uncorrupted or contributing modality. Similar to the first gating layer, the softmax function is used and the probabilistic attention scores are replicated to multiply with the corresponding feature vectors, which is given by\n\nwhere Gav0, Gav1 and Gav2 denote the attention scores of Xatt,ga, Xatt,gv and Xatt,gav respectively. Finally, the feature vector Xatt,g obtained from the A-V gating layer is fed to the Multi Layer Perceptron (MLP) to obtain the final prediction of valence or arousal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results And Discussion",
      "text": "A) Datasets: Aff-Wild2 is the largest dataset, consisting of 594 videos collected from YouTube, all captured in the wild  [20] . The annotations are provided by four experts using a joystick and the final annotations are obtained as the average of the four raters. Sixteen of these videos display two subjects, both of which have been annotated. In total, there are  , Joint Cross-Attention (JCA)  [3] , Recursive Joint Cross-Attention (RJCA)  [2] , and Transformer-based Cross-Attention (TCA)  [14] . The proposed IACA model is added to each of the baselines to analyze the performance improvement. We can observe that the proposed model shows consistent improvement over all the baselines. Of all these baselines, CA and TCA rely only on cross-modal interactions, whereas JCA and RJCA exploit both intra-and intermodal relationships using joint feature representation in the CA framework. Since the proposed model is focused on dealing with weak complementary inter-modal relationships, there is a significant performance improvement (relative) for CA and TCA than that of JCA and RJCA as shown in Table  2 .\n\nThe proposed model has also been evaluated under the scenario of missing modality, in which a growing portion of audio modality is replaced by \"zeros\" to replicate the scenario of missing modality. Specifically, we chose the video sample \"131-30-1920x1080\" with 8012 frames and the audio modality (spectrograms) is dropped at varying percentages of 10, 20, 40, 60, and 80 in test mode. The CCC performance of the proposed approach is compared to that of RJCA for valence and arousal, where RJCA is considered as the baseline. We can observe that the performance of the proposed model has not decreased significantly compared to the baseline (RJCA) for both valence and arousal as depicted in   approaches on this dataset have been submitted to the ABAW challenges  [28, 29] , we compared the proposed approach with the relevant state-of-the-art models for A-V fusion in dimensional ER. Zhang et al.  [24]  explored a leader-follower attention strategy by exploiting the audio modality to attend to the visual modality to boost the performance of the visual modality. However, they do not show major improvement in fusion performance over the uni-modal performances. Karas et al.  [25]  and Meng et al.  [23]  explored the ensembling of fusion models with LSTMs and transformers, where  [23]  significantly improved the performance on both validation and test sets by leveraging multiple backbones for each modality and three external datasets to achieve better generalization. Zhou et al.  [26]  and Zhang et al.  [27]  also exploited transformer models, where the feature vectors of audio and visual modalities are concatenated and fed to the transformer model.  [26]  used Temporal Convolutional Networks along with transforms, whereas  [27]  leveraged large-scale pretraining using Masked Auto-encoders. Most of these methods  [23, 26, 27]  do not effectively capture the inter-modal relationships across the modalities and are geared towards achieving higher performance on the test set by exploiting large-scale pretraining on additional datasets and multiple backbones for each modality. Unlike these methods,  [3]  and  [2]  focused on improving the fusion performance by effectively capturing the inter-modal relationships, still retaining the intra-modal relationships. Both  [3]  and  [2]  improved the fusion performance by introducing the joint feature representation in the CA framework to capture both intra-and inter-modal relationships. Though they did not achieve significant improvement on the test set as they used only single backbones for each modality without any external datasets, their performance is solely attributed to the sophisticated fusion model by effectively leveraging the intra-and intermodal relationships. Since the primary focus of our work is to improve fusion performance by handling weak complementary relationships and severely corrupted modalities, the proposed model has been validated on these sophisticated CA-based fusion models  [2] . We can observe that the fusion performance of RJCA  [2]  has been further improved on both validation and test sets using the proposed IACA model as shown in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we investigated the prospect of retaining the benefits of strong complementary relationships across audio and visual modalities, while being robust to weak complementary relationships. In particular, we proposed an IACA model using a two-stage gating mechanism, where the first stage deals with weak complementary relationships by selecting the crossattended or self-attended features based on strong or weak complementary relationships, respectively. The features obtained from the first gating layer is further refined by focusing on the significant modality to suppress the impact of extremely corrupted or missing modalities. Extensive experiments were conducted to analyze the impact of the proposed IACA model on multiple variants of Cross-Attention (CA) and found consistent improvement on both valence and arousal. Leveraging sophisticated backbones for audio and visual modality is expected to further improve the performance of the model on the test set.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Attention scores are normalized between 0 and",
      "page": 1
    },
    {
      "caption": "Figure 1: When the modalities demonstrate strong complementary rela-",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of the proposed Inconsistency-Aware Cross-Attention model.",
      "page": 3
    },
    {
      "caption": "Figure 3: By exploiting the two-stage gat-",
      "page": 3
    },
    {
      "caption": "Figure 3: CCC Performance of the proposed model along with",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhang et al. [24]": "Karas et al. [25]",
          "Leader Follower Attention": "LSTM + Transformers",
          "0.469": "0.388",
          "0.649": "0.551",
          "0.463": "0.418",
          "0.492": "0.407"
        },
        {
          "Zhang et al. [24]": "Meng et al. [23]",
          "Leader Follower Attention": "LSTM + Transformers",
          "0.469": "0.605",
          "0.649": "0.668",
          "0.463": "0.606",
          "0.492": "0.596"
        },
        {
          "Zhang et al. [24]": "Zhou et al. [26]",
          "Leader Follower Attention": "TCN + Transformers",
          "0.469": "0.550",
          "0.649": "0.680",
          "0.463": "0.566",
          "0.492": "0.500"
        },
        {
          "Zhang et al. [24]": "Zhang et al. [27]",
          "Leader Follower Attention": "Transformers",
          "0.469": "0.648",
          "0.649": "0.705",
          "0.463": "0.523",
          "0.492": "0.545"
        },
        {
          "Zhang et al. [24]": "Praveen et al. [3]",
          "Leader Follower Attention": "JCA",
          "0.469": "0.657",
          "0.649": "0.580",
          "0.463": "0.451",
          "0.492": "0.389"
        },
        {
          "Zhang et al. [24]": "Praveen et al. [2]",
          "Leader Follower Attention": "RJCA",
          "0.469": "0.721",
          "0.649": "0.694",
          "0.463": "0.467",
          "0.492": "0.405"
        },
        {
          "Zhang et al. [24]": "Ours",
          "Leader Follower Attention": "RJCA + IACA",
          "0.469": "0.749",
          "0.649": "0.725",
          "0.463": "0.505",
          "0.492": "0.474"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "3",
      "title": "Recursive joint attention for audio-visual fusion in regression based emotion recognition",
      "authors": [
        "R Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention",
      "authors": [
        "R Praveen",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2023",
      "venue": "Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention"
    },
    {
      "citation_id": "5",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "venue": "IEEE FG, 2021"
    },
    {
      "citation_id": "6",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W De Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "P Cardinal",
        "E Granger"
      ],
      "year": "2022",
      "venue": "CVPRW"
    },
    {
      "citation_id": "7",
      "title": "M2lens: Visualizing and explaining multimodal models for sentiment analysis",
      "authors": [
        "X Wang",
        "J He",
        "Z Jin",
        "M Yang",
        "Y Wang",
        "H Qu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on VCG"
    },
    {
      "citation_id": "8",
      "title": "Leaky gated cross-attention for weakly supervised multi-modal temporal action localization",
      "authors": [
        "J.-T Lee",
        "S Yun",
        "M Jain"
      ],
      "year": "2022",
      "venue": "Leaky gated cross-attention for weakly supervised multi-modal temporal action localization"
    },
    {
      "citation_id": "9",
      "title": "Two-stream auralvisual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "Proc. of IEEE FG"
    },
    {
      "citation_id": "10",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "11",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE JSTSP"
    },
    {
      "citation_id": "12",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "S Liam",
        "O Alice",
        "A Hazem"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "13",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "S Liu",
        "P Gao",
        "Y Li",
        "W Fu",
        "W Ding"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "14",
      "title": "Robust audiovisual emotion recognition: Aligning modalities, capturing temporal information, and handling missing features",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "15",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "IEEE SLT Workshop"
    },
    {
      "citation_id": "16",
      "title": "Gated multimodal networks",
      "authors": [
        "J Arevalo",
        "T Solorio",
        "M Montes-Y Gómez",
        "F González"
      ],
      "year": "2020",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "17",
      "title": "Multimodal gated information fusion for emotion recognition from eeg signals and facial behaviors",
      "authors": [
        "S Rayatdoost",
        "D Rudrauf",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "18",
      "title": "Gated mechanism for attention based multi modal sentiment analysis",
      "authors": [
        "A Kumar",
        "J Vepa"
      ],
      "year": "2020",
      "venue": "Proc. of IEEE ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Cross-modality gated attention fusion for multimodal sentiment analysis",
      "authors": [
        "M Jiang",
        "S Ji"
      ],
      "year": "2022",
      "venue": "Cross-modality gated attention fusion for multimodal sentiment analysis"
    },
    {
      "citation_id": "20",
      "title": "Audiovisual gated-sequenced neural networks for affect recognition",
      "authors": [
        "D Aspandi",
        "F Sukno",
        "B Schuller",
        "X Binefa"
      ],
      "year": "2022",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "21",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proc. of IEEE CVPRW"
    },
    {
      "citation_id": "22",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. of IEEE CVPR"
    },
    {
      "citation_id": "24",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "C Liu",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "IEEE/CVF CVPR Workshop"
    },
    {
      "citation_id": "25",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "S Zhang",
        "Y Ding",
        "Z Wei",
        "C Guan"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "26",
      "title": "Time-continuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition",
      "authors": [
        "V Karas",
        "M Tellamekala",
        "A Mallol-Ragolta",
        "M Valstar",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proc. of the IEEE/CVF CVPRW"
    },
    {
      "citation_id": "27",
      "title": "Leveraging tcn and transformer for effective visual-audio fusion in continuous emotion recognition",
      "authors": [
        "W Zhou",
        "J Lu",
        "Z Xiong",
        "W Wang"
      ],
      "year": "2023",
      "venue": "CVPRW"
    },
    {
      "citation_id": "28",
      "title": "Multi-modal facial affective analysis based on masked autoencoder",
      "authors": [
        "W Zhang",
        "B Ma",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "CVPRW"
    },
    {
      "citation_id": "29",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proc. of IEEE ICCVW"
    },
    {
      "citation_id": "30",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proc. of the IEEE/CVF CVPRW"
    }
  ]
}