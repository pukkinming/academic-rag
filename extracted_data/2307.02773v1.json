{
  "paper_id": "2307.02773v1",
  "title": "Selinet: Sentiment Enriched Lightweight Network For Emotion Recognition In Images",
  "published": "2023-07-06T04:49:47Z",
  "authors": [
    "Tuneer Khargonkar",
    "Shwetank Choudhary",
    "Sumit Kumar",
    "Barath Raj KR"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a sentiment-enriched lightweight network SeLiNet and an end-to-end on-device pipeline for contextual emotion recognition in images. SeLiNet model consists of body feature extractor, image aesthetics feature extractor, and multitask learning-based fusion network which jointly estimates discrete emotion and human sentiments tasks. On the EMOTIC dataset, the proposed approach achieves an Average Precision (AP) score of 27.17 in comparison to the baseline AP score of 27.38 while reducing the model size by >85%. In addition, we report an on-device AP score of 26.42 with reduction in model size by >93% when compared to the baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Understanding the emotional states of the people in the images is an emerging research area in the domain of computer vision. Ability to correctly perceive emotions can help improve human-computer interactions. In the case of smartphones, several use cases can be built such as queries based on-device image search, dynamically uncluttering the notification panel based on user emotions, etc. Further, it has other advanced applications like modeling robot behavior as per the perceived emotion of the user.\n\nConventionally, researchers have used facial expressions  [14] ,  [19]  based features to process human emotions. Recently, scientific studies have established that perception of emotions is also influenced by context  [3] ,  [4]  such as background scene  [15] ,  [23] , body posture  [17] , image composition  [21] , gait analysis  [33]  etc. Several previous works have achieved better performance by considering these contexts.\n\nPrevious studies show that image aesthetics assessment  [11]  is a crucial cue to understand the emotions evoked by the images. Aesthetics response towards images may depend upon many components such as composition, colorfulness, spatial organization, emphasis, motion, depth, or presence of humans  [2] ,  [20] . Traditional works have used low-level handcrafted visual features  [1] ,  [10] ,  [31]  to understand the aesthetics and related image emotions. Recent works based on deep learning  [5] ,  [28] ,  [32]  extract mid and global-level features such as composition, semantics, and emphasis  [36]  to classify image emotions. These works try to understand human emotions evoked by the pictures and are able to achieve improved results by considering the aesthetics properties of the images. Understanding the composition and semantics of the images can help capture the high-level contextual properties like the object's spatial organization, the relationship between various local level features, etc., and thus can also be beneficial to the task of recognizing the emotional states of people in the images. Image aesthetics assessment has an impact on human sentiment also. It may be either positive, negative, or neutral. For example, images that convey a pleasant mood are generally rated high on the aesthetics scale, and vice-versa. Such images are also known to elicit positive emotions. Taking inspiration from these discussed studies, we explore image aesthetics assessment-based features along with body features to understand the sentiment and emotional states of the people in the images.\n\nSeveral studies show that privacy is one of the leading concerns among people  [22] ,  [24] . With the rise in ownership of smartphones  [8] , this concern is particularly high among smartphone users  [26] ,  [29] . To this end, we present an endto-end on-device novel pipeline consisting of the sentimentenriched lightweight model called SeLiNet for human emotion understanding from the image. Our main contributions can be summarized as:\n\n• We propose a sentiment-enriched lightweight model Se-LiNet and end-to-end pipeline for on-device emotion recognition in images which achieves comparable average precision(AP) score to the baseline system with a significant reduction in model size (85% ↓) and inference time(78% ↓).\n\n• We demonstrate that the image aesthetics features contribute in improving the overall performance of the task of emotion recognition in images. • We conceptualize the problem as multitask learning-based and make predictions for discrete emotions and related sentiments. We then use sentiment knowledge in the post-processing to enhance emotion predictions and show improved results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion recognition is a well-studied task in the vision field. Traditional works have used hand-crafted features  [25] ,  [37]  for the emotion recognition task. Deep learning networks have taken into account facial expressions  [14] ,  [19] , gait analysis  [33]  and body posture  [18] ,  [30]  etc. based features to predict emotions.  [7]  proposed facial expressions based on compound emotion detection such as 'happily disgusted' or 'angrily surprised' and thus provide deeper insights about expressed emotions. While most of these works have modeled emotion detection tasks as the categorical problem, some have tried to use the valence, arousal, and dominance (VAD) model  [27]  based on continuous emotional space.\n\nRecently, several works have also demonstrated the importance of contextual cues in interpreting emotional states.  [13]  presents two-stream encoding networks capturing facial and contextual features, followed by a fusion network to predict context-aware emotion recognition.  [12]  also proposes similar two-stream architecture in which one stream captures body features and the other stream captures scene context from the image. A fusion network consisting of both body and scene context features is jointly used to predict discrete emotion and VAD scores. They also create and publish the EMOTIC (from EMOTions In Context) Dataset and provide a CNN-based baseline system on the same dataset.  [16]  proposes context aware multi-modality-based network to predict emotion. They use several context-based modalities such as the face, gait analysis, semantic context, and depth maps to model sociodynamic interactions among agents.\n\nIn this work, we get inspiration from  [12]  to design our lightweight network and baseline this work for the comparison of our proposed approach on the EMOTIC dataset.  [12]  uses resnet50 as a feature extractor for both body and scene context. In contrast, we use the pretrained mobilenetV3 large model for extracting body features and a lightweight compositionbased aesthetics feature extractor (ReLIC  [35] ) to keep the model size small. This baseline  [12]  predicts emotion and VAD scores simultaneously. We, instead make sentiment and emotion predictions together and use sentiment predictions in the post-processing module to improve emotion prediction performance.  [16]  reports an AP score of 35.48 on the EMOTIC dataset. However, they have used several deep neural networks to derive multiple modalities-based contexts, making the overall architecture complex for training and inferencing. Architecture is also computationally intensive, with an overall model size of >500MB (includes face detection model size), making it unsuitable for low-resource devices such as smartphones.\n\nWe have used the aesthetics feature extractor as one of the branches for our SeLiNet model. Several previous works have shown that there is an explicit connection between image aesthetics and image emotion.  [6]  have used emotionassisted image aesthetics identification using multitask learning. They demonstrate that there is a link between image emotion and aesthetics, and that image emotion features can aid in aesthetics assessment tasks and vice versa.  [21]  use image semantics, image aesthetics, and other visual features to effectively classify the emotion types.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Dataset",
      "text": "We train and report our proposed approach performance on EMOTIC  [12]  dataset. EMOTIC is a benchmark dataset for the context-aware emotion recognition task. It is created by taking images from MSCOCO dataset  [38] , Ade20k dataset  [39]  and images downloaded from the Google search engine, which makes the overall dataset very diverse and increases its complexity. The dataset contains roughly 23,000 images and around 33,000 annotated instances of emotions. The dataset provides bounding box information of the target person in each image and the same has multi-label annotation with 26 possible emotion categories information of the bounding box. Dataset also has annotations for continuous emotion index such as Valance, Arousal, and Dominance (VAD). Emotions are quantified on these three indexes with their scales ranging from 0 to 10. In our work, we have only considered discrete emotions as ground truth and instead added sentiment prediction as an additional task. Since the EMOTIC dataset does not provide sentiment labeling, we use a study by  [9]  to label the sentiment of each image based on the ground truth emotion.  [9]  shows each emotion can be categorized into positive, negative, and neutral sentiments. So, we label possible sentiment for each image in the dataset. We use a multi-label strategy to label sentiment because each image can have multiple emotion labels and these labels can fall under more than one sentiment. Table  I  shows a few examples of ground truth emotion labels and corresponding one-hot encoding for the sentiment label. We report all our evaluation results on the test set of the EMOTIC dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. The Proposed Method",
      "text": "This section details the motivation behind the problem and our proposed method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Motivation And Problem Solving",
      "text": "Although multi-modality-based information  [12] ,  [13] ,  [16]  improves the performance of emotion task, the inclusion of these additional information makes the overall model architecture complex  [16]  and expensive in computation and memory and thus making it unfit for resource constraints systems such as mobile phones. Also, very few works focus on lightweight architectures suitable for the on-device system. Therefore, in this work, we attempted to develop a lightweight model for on-device inferencing. For our method, we derive the idea to employ image aesthetics for the emotion recognition task based on studies discussed in Section I.\n\nWe model the problem in this paper as multitask learning, which predicts both emotions and sentiments. The main idea behind predicting sentiments as an auxiliary task is: 1) To provide an additional loss factor to the emotion task during training in case of incorrect sentiment prediction; 2) To use the sentiment score in post-processing to further enhance the main task (emotion task) performance. It is also possible to infer only the sentiments of an image using the proposed multitask as standalone predictions.  Body feature extractor : This branch focuses on extracting facial and body features from the input image. Extracting these features is important because they provide crucial information about the emotional state of the person in the image. The branch is based on mobilenetV3 large, trained on the Ima-geNet dataset with person class. We freeze weights till the second last layer and take its output with a feature map of size (960*7*7) which is then fed to a self-attention network. The attention layer outputs an attentive vector of size 960 which is followed by a dense layer of 512 units whose output is then passed to the fusion model for further processing.\n\nAesthetics feature extractor: The aesthetics feature extractor uses the pretrained ReLIC architecture  [35]  as a backbone to extract image aesthetics features. ReLIC architecture  [35]  is based on several Convolutional Neural Networks and tries to learn both local and global features. Local features are used to understand image composition whereas global features contribute toward the overall image properties such as texture etc. We freeze weights till second last layer and take its output as a feature map of (1280*7*7) and apply self-attention to get attentive feature vectors. The aesthetics branch outputs a 1280size vector which is followed by a dense layer of 512 units. The output of the dense layer is then fed to the fusion model.\n\nFusion model : The fusion layer concatenates the output of the body and aesthetics feature extractors to get a 1024-size fused vector. This concatenation layer is then followed by two dense layers of 512 and 256 units. This last 256 dense layer is followed by two task-specific dense layers each of 128 units whose outputs are fed to emotion and sentiment classification layers respectively for the predictions. We perform detailed hyperparameter tuning to choose layers of the fusion model.\n\n3) Post-processing: We use a boosting algorithm to modify the confidence score of the emotion prediction based on the sentiment output. We consider the top 5 confidence scores by the emotion task E i as E = E 1 , E 2 , E 3 , E 4 , E 5 and predictions by sentiment task S j as S = S 1 , S 2 , S 3 . Then the boosting equation is as follows.\n\nOur boosting factor S j provides a relative boost to all emotions in E i that are predicted correctly in accordance with sentiment output.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Results And Experiments",
      "text": "This section describes the implementation details, comparison with previous works, and ablation study for our proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Implementation Details",
      "text": "We use the PyTorch framework for experimentation and model development. All training and testing are carried out on an Nvidia GPU GeForce GTX 1080 Ti 11178 MB card. The aesthetics branch takes the complete image of size 224 * 224 * 3 as input. The body branch, on the other hand, requires a 128 * 128 * 3 input image which is a cropped portion of the original image containing the whole body. We set the batch size to 26 and use stochastic gradient descent(SGD) optimizer in the training. The learning rate is initialized to 0.001 with a decay rate of 0.1. The model is trained for 25 epochs and is saved based on the best validation AP score.\n\nLoss Function : Since our problem statement is a multiclass multi-label on the EMOTIC dataset, we experimented with standard binary cross entropy(BCE) loss and L2 loss (suggested by  [12] ). We observe that L2 loss gives better results than BCE.\n\nWhere W i is dynamic weights per batch which are defined as:\n\nThe combined loss of emotion and sentiment task is referred to as the total loss. Based on the experiments, we set λ equals to 0.8 which gives better results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Comparison With Previous Works",
      "text": "As shown in Table  II ,  [12]  reports an AP score of 27.38 on the emotion recognition task using a CNN-based baseline system. We try to reproduce their work using the code available on Github 1 . Using the same configuration discussed in the work, we get an AP score of 25.38 with a model size of nearly 191MB and an inference time of 16.2 ms on GPU. In contrast, using our approach, we achieve an AP score of 27.17 with a model size of only 28.03 MB, a reduction of 85.32% when compared to the baseline. Our approach is faster by nearly 78% compared to the baseline. In Table  II , we also provide a comparison of our proposed model with other works. Although  [16]  shows better performance, it involves more than three modalities as input to the model making the overall system complex in computation and training and cost-intensive in terms of memory and inference time. In comparison, our work provides for the lightweight model with only two modalities as input and gives comparable performance to the baseline.\n\nFor our sentiment sub-task on the EMOTIC dataset, we achieve an AP score of 93.53, 73.82, and 19.13 for Positive, Negative, and Neutral sentiment respectively. Positive and Negative sentiments report better performance compared to Neutral. It is due to the small number of emotions categorized in the neutral sentiment leading to a lower training sample.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Ondevice Performance",
      "text": "Our end-to-end on-device pipeline is evaluated on a Samsung S21 smartphone (Android SDK 30, 12 GB RAM, 256 GB ROM, Octa-core, Exynos2100 chip). SeLiNet model, 1 https://github.com/Tandon-A/emotic quantized using the PyTorch framework, reports an on-device AP score of 26.42 on the same test set of EMOTIC dataset with a model size of 11.34 MB and on-device inference time of 65 ms. Although the AP score drops marginally by nearly 2.76 % due to quantization, there is a reduction of model size ( of the quantized model) by more than 52% which is a huge gain. Also, in comparison to the baseline system,  [12] , we achieve a comparable AP score by quantized model while reducing the model size by nearly 93%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ablation Study",
      "text": "Table  III  describes the ablation study regarding the different configurations of the architecture. The addition of the aesthetics branch results in a nearly 12% increase in AP score. Our proposed multi-task learning has improved the AP score by at least 3%. Thus, it demonstrates that all of the components are required to achieve the best results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Error Analysis",
      "text": "We observe that the performance of our model slightly degrades for the categories such as embarrassment, surprise, yearning, etc. compared to  [16] . Possible reasons are 1) The number of original training images is insufficient to learn diverse features. 2) Owing to complex nature of these emotions, additional cues may be required.  [16]  demonstrates that taking into account multiple modalities leads to better predictions.\n\nThe difficult nature of the emotion recognition task and the requirement of multiple contexts are supported by the study conducted by  [16]  where if only the facial context is considered out of three discussed contexts, then the AP score drops from 35.48 to 24.06. The baseline by  [12]  has used Resnet50 for both body and scene context feature extraction. But, when we replace Resnet50 with ResNet18, the AP score falls to 17.23, indicating that shallower models are insufficient for the task. Considering the above details, SeLiNet performs fairly well despite being lightweight.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Conclusion",
      "text": "We present a lightweight model SeLiNet and an end-to-end pipeline to predict the emotional states of people in images for on-device inferencing. Our proposed approach achieves an AP score of 27.17, which is comparable to the baseline system with a 85% smaller memory footprint and much faster inference time. We also show that aesthetics assessment of the images can be helpful information to understand image emotion. Using multitasking learning, we further improve our model results. In future work, we will like to capture additional contextual information such as object detection, deeper semantic analysis, and its impact on image emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of SeLiNet",
      "page": 3
    },
    {
      "caption": "Figure 1: shows our SeLiNet architecture",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Baseline CNN [12]",
          "AP Score": "27.38",
          "Model size": ">190 MB",
          "Inference time": "16.2 ms"
        },
        {
          "Model": "CAAGRER [34]",
          "AP Score": "28.42",
          "Model size": ">400 MB",
          "Inference time": "NA"
        },
        {
          "Model": "EmotiCon(GCN)\n[16]",
          "AP Score": "32.03",
          "Model size": ">500 MB",
          "Inference time": "NA"
        },
        {
          "Model": "EmotiCon(Depth)\n[16]",
          "AP Score": "35.48",
          "Model size": ">500 MB",
          "Inference time": "NA"
        },
        {
          "Model": "SeLiNet",
          "AP Score": "27.17",
          "Model size": "28.03 MB",
          "Inference time": "3.5 ms"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Body Model": "✓\n✓\n✓\n✓\n✓",
          "Aesthetic Model": "✗\n✓\n✓\n✓\n✓",
          "Attention": "✗\n✗\n✓\n✓\n✓",
          "Sentiment Task": "✗\n✗\n✗\n✓\n✓",
          "Post Processing": "✗\n✗\n✗\n✗\n✓",
          "Model Size": "12.53 MB\n27.23 MB\n27.91 MB\n28.03 MB\n28.03 MB",
          "AP Score": "22.71\n25.46\n26.30\n26.81\n27.17"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How we recognize angry and happy emotion in people, places, and things. Cross-cultural Research -CROSS-CULT RES",
      "authors": [
        "Joel Aronoff"
      ],
      "year": "2006",
      "venue": "How we recognize angry and happy emotion in people, places, and things. Cross-cultural Research -CROSS-CULT RES"
    },
    {
      "citation_id": "2",
      "title": "Towards a psychology of photography: Dimensions underlying aesthetic appeal of photographs",
      "authors": [
        "Osten Axelsson"
      ],
      "year": "2007",
      "venue": "Perceptual and motor skills"
    },
    {
      "citation_id": "3",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "4",
      "title": "Context in emotion perception",
      "authors": [
        "Lisa Barrett",
        "Batja Mesquita",
        "Maria Gendron"
      ],
      "year": "2011",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "5",
      "title": "Adaptive fractional dilated convolution network for image aesthetics assessment",
      "authors": [
        "Qiuyu Chen",
        "Wei Zhang",
        "Ning Zhou",
        "Peng Lei",
        "Yi Xu",
        "Yu Zheng",
        "Jianping Fan"
      ],
      "year": "2020",
      "venue": "Adaptive fractional dilated convolution network for image aesthetics assessment"
    },
    {
      "citation_id": "6",
      "title": "Image aesthetic assessment based on emotion-assisted multi-task learning network",
      "authors": [
        "Yunlong Chen",
        "Yuanyuan Pu",
        "Zhengpeng Zhao",
        "Dan Xu",
        "Wenhua Qian"
      ],
      "year": "2021",
      "venue": "Image aesthetic assessment based on emotion-assisted multi-task learning network"
    },
    {
      "citation_id": "7",
      "title": "Compound facial expressions of emotion: From basic research to clinical applications",
      "authors": [
        "Shichuan Du",
        "Aleix Martinez"
      ],
      "year": "2015",
      "venue": "Dialogues in Clinical Neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Global smartphone ownership, internet usage and their impacts on humans",
      "authors": [
        "Wadzani Gadzama",
        "Bitrus Joseph",
        "Ngubdo Aduwamai"
      ],
      "year": "2019",
      "venue": "Global smartphone ownership, internet usage and their impacts on humans"
    },
    {
      "citation_id": "9",
      "title": "A conceptual framework for emotional connection towards e-learning mobile application design for children",
      "authors": [
        "Magrizef Gasah",
        "Aslina Baharum"
      ],
      "venue": "Journal of Software and Systems Development"
    },
    {
      "citation_id": "10",
      "title": "Extracting moods from pictures and sounds: towards truly personalized tv",
      "authors": [
        "Alan Hanjalic",
        "Hanjalic"
      ],
      "year": "2006",
      "venue": "ieee signal processing magazine"
    },
    {
      "citation_id": "11",
      "title": "Aesthetics and emotions in images",
      "authors": [
        "Dhiraj Joshi",
        "Ritendra Datta",
        "Elena Fedorovskaya",
        "Quang-Tuan Luong",
        "J Wang",
        "Jia Li",
        "Jiebo Luo"
      ],
      "year": "2011",
      "venue": "Signal Processing Magazine"
    },
    {
      "citation_id": "12",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Àgata Lapedriza"
      ],
      "year": "2020",
      "venue": "Context based emotion recognition using emotic dataset"
    },
    {
      "citation_id": "13",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Context-aware emotion recognition networks"
    },
    {
      "citation_id": "14",
      "title": "Facial-componentbased bag of words and phog descriptor for facial expression recognition",
      "authors": [
        "Zisheng Li",
        "Jun-Ichi Imai",
        "Masahide Kaneko"
      ],
      "year": "2009",
      "venue": "The Journal of The Institute of Image Information and Television Engineers"
    },
    {
      "citation_id": "15",
      "title": "Placing the face in context: Cultural differences in the perception of facial emotion",
      "authors": [
        "Takahiko Masuda",
        "Phoebe Ellsworth",
        "Batja Mesquita",
        "Janxin Leu"
      ],
      "year": "2008",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "16",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Emoticon: Context-aware multimodal emotion recognition using frege's principle"
    },
    {
      "citation_id": "17",
      "title": "Individuality in Communicative Bodily Behaviours",
      "authors": [
        "Costanza Navarretta"
      ],
      "year": "2012",
      "venue": "Individuality in Communicative Bodily Behaviours"
    },
    {
      "citation_id": "18",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "T. Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Expert system for automatic analysis of facial expression",
      "authors": [
        "M Pantic",
        "Léon Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "20",
      "title": "Aesthetic primitives of images for visualization",
      "authors": [
        "Gabriele Peters"
      ],
      "year": "2007",
      "venue": "Aesthetic primitives of images for visualization"
    },
    {
      "citation_id": "21",
      "title": "Learning multi-level deep representations for image emotion classification",
      "authors": [
        "Tianrong Rao",
        "Min Xu",
        "Dezhi Xu"
      ],
      "year": "2016",
      "venue": "Learning multi-level deep representations for image emotion classification"
    },
    {
      "citation_id": "22",
      "title": "Information privacy concern at individual, group, organization, and societal level -a literature review",
      "authors": [
        "Dillip Kumar",
        "Ajit Kumar"
      ],
      "year": "2021",
      "venue": "Information privacy concern at individual, group, organization, and societal level -a literature review"
    },
    {
      "citation_id": "23",
      "title": "Rapid influence of emotional scenes on encoding of facial expressions: An erp study",
      "authors": [
        "Ruthger Righart",
        "Beatrice Gelder"
      ],
      "year": "2008",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "24",
      "title": "The impact of privacy concerns and perceived vulnerability to risks on users privacy protection behaviors on sns: A structural equation model",
      "authors": [
        "Noora Sami"
      ],
      "year": "2016",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "25",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "26",
      "title": "Privacy concerns associated with smartphone use",
      "authors": [
        "Janice Sipior",
        "Burke Ward",
        "Linda Volonino"
      ],
      "year": "2014",
      "venue": "Journal of Internet Commerce"
    },
    {
      "citation_id": "27",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "Mohammad Soleymani",
        "Sadjad Esfeden",
        "Yun Fu",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Nima: Neural image assessment",
      "authors": [
        "Hossein Talebi",
        "Peyman Milanfar"
      ],
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Reengineering the user: Privacy concerns about personal data on smartphones",
      "authors": [
        "Matina Tsavli",
        "Pavlos Efraimidis",
        "Vasilios Katos",
        "Lilian Mitrou"
      ],
      "year": "2015",
      "venue": "Information and Computer Security"
    },
    {
      "citation_id": "30",
      "title": "Recognizing emotions expressed by body pose: A biologically inspired neural model",
      "authors": [
        "Luc Van Gool",
        "Beatrice Gelder"
      ],
      "year": "2008",
      "venue": "Neural networks : the official journal of the International Neural Network Society"
    },
    {
      "citation_id": "31",
      "title": "A survey on emotional semantic image retrieval",
      "authors": [
        "Qianhua Weiningwang",
        "He"
      ],
      "year": "2008",
      "venue": "A survey on emotional semantic image retrieval"
    },
    {
      "citation_id": "32",
      "title": "Contextaware attention network for predicting image aesthetic subjectivity",
      "authors": [
        "Munan Xu",
        "Jia-Xing Zhong",
        "Yurui Ren",
        "Shan Liu",
        "Ge Li"
      ],
      "year": "2020",
      "venue": "Contextaware attention network for predicting image aesthetic subjectivity"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "Shihao Xu",
        "Jing Fang",
        "Xiping Hu",
        "Edith Ngai",
        "Yi Guo",
        "Victor Leung",
        "Jun Cheng",
        "Bin Hu"
      ],
      "year": "2020",
      "venue": "Emotion recognition from gait analyses: Current research and future directions"
    },
    {
      "citation_id": "34",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "Context-aware affective graph reasoning for emotion recognition"
    },
    {
      "citation_id": "35",
      "title": "Representation learning of image composition for aesthetic prediction",
      "authors": [
        "Lin Zhao",
        "Meimei Shang",
        "Fei Gao",
        "Rongsheng Li",
        "Fei Huang",
        "Jun Yu"
      ],
      "year": "2020",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "36",
      "title": "Exploring principles-of-art features for image emotion recognition",
      "authors": [
        "Sicheng Zhao",
        "Yue Gao",
        "Xiaolei Jiang",
        "Hongxun Yao"
      ],
      "year": "2014",
      "venue": "MM 2014 -Proceedings of the 2014 ACM Conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Learning multiscale active facial patches for expression analysis",
      "authors": [
        "Lin Zhong",
        "Qingshan Liu",
        "Peng Yang",
        "Bo Liu",
        "Dimitris Metaxas"
      ],
      "year": "2012",
      "venue": "Learning multiscale active facial patches for expression analysis"
    },
    {
      "citation_id": "38",
      "title": "Microsoft coco:Common objects in context",
      "authors": [
        "T Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan"
      ],
      "year": "2014",
      "venue": "Microsoft coco:Common objects in context"
    },
    {
      "citation_id": "39",
      "title": "Semantic understanding of scenes through the ade20k dataset",
      "authors": [
        "B Zhou",
        "H Zhao",
        "X Puig",
        "S Fidler",
        "A Barriuso",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "Semantic understanding of scenes through the ade20k dataset",
      "arxiv": "arXiv:1608.05442"
    }
  ]
}