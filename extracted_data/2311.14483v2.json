{
  "paper_id": "2311.14483v2",
  "title": "Ser_Ampel: A Multi-Source Dataset For Speech Emotion Recognition Of Italian Older Adults ‚ãÜ",
  "published": "2023-11-24T13:47:25Z",
  "authors": [
    "Alessandra Grossi",
    "Francesca Gasparini"
  ],
  "keywords": [
    "Speech emotion recognition",
    "older adults",
    "Italian language",
    "multisource dataset",
    "cross-language SER",
    "cross-corpus SER"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, SER_AMPEL, a multi-source dataset for speech emotion recognition (SER) is presented. The peculiarity of the dataset is that it is collected with the aim of providing a reference for speech emotion recognition in case of Italian older adults. The dataset is collected following different protocols, in particular considering acted conversations, extracted from movies and TV series, and recording natural conversations where the emotions are elicited by proper questions. The evidence of the need for such a dataset emerges from the analysis of the state of the art. Preliminary considerations on the critical issues of SER are reported analyzing the classification results on a subset of the proposed dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent decades, the elderly population has undergone continuous growth with a consequent increase in attention to well-being and active ageing. In this context, loneliness and social isolation of the elderly are factors to consider because they have a close relationship with depression, dementia and other serious medical conditions  [33] . The use of technology can reduce loneliness and social isolation, mitigating their negative outcomes on mental and physical health. In particular, there is a grown interest in studying social robots to increase psychological well being  [15]    [8] . As well as in the case of human-human interactions  [23] , emotions play a relevant role in creating an effective interaction between older adults and social robots. During social interactions, human beings convey their emotions not only by words but also by bodily, vocal or facial expressions  [23] .\n\nIn this work we focus on Speech Emotion Recognition (SER)  [40]    [1]  to recognize emotions in natural conversations, to provide a more natural and social communication in human-robot interactions  [18]    [21] .\n\nSeveral researches have been developed in the field of SER during the last three decades  [13] , starting from uni-modal approaches, that consider only linguistic or acoustic information, to multi-modal approaches that rely on both of them  [2] . Several traditional approaches that adopt handcrafted features, both in the temporal and/or frequency domains as well as deep learning ones, have been adopted and are summarized in review manuscripts such as the ones of Ak√ßay et al  [1]  and Wani et al  [39] .\n\nDespite the numerous works in the state of the art that face the SER task, there are several critical issues that make it difficult to recognize emotions especially in natural conversations. In the review paper by Fahad et al.  [13]  some of these challenges and the approaches developed to address them are summarized. In particular, SER models struggle to recognize emotions when considering people of different languages or ages. Moreover, most of the datasets available in the literature are composed of acted utterances  [30] , while only few of them consider natural conversations  [36]  [14]  [27] . Within this context, we address the problem of SER in the case of an elderly population that speaks Italian. To this end, the first urgency is the collection of a proper dataset of Italian conversations that considers different ages, in particular older people.\n\nThis paper is thus structured as follows. In section 2, the datasets available in the literature are introduced, analyzing their limits. In section 3, our multi-source SER_AMPEL dataset, collected to solve some of the critical issues found in the available datasets, is presented. Models previously trained on acted datasets are then applied on a portion of the SER_AMPEL data, and the obtained classification results are reported in section 4. From this analysis, interesting considerations can be drawn and future lines of research can be identified that are discussed in section 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ser Datasets",
      "text": "Several datasets have been considered in the literature for SER purpose. Following the suggestions proposed by Koolagudi et al.  [22] , they can be classified into three groups, depending on how the emotions are elicited.\n\n-Acted datasets: these datasets collect utterances from actors/actresses that try to simulate emotions; the utterances can be short phrases (few seconds), usually repeated with different emotions. The utterances can also be phrases without meaning. Most of the datasets available belong to this category  [5] [7]  [6] [25] -Evoked or Elicited datasets: conversations are recorded in situations properly created in order to evoke certain emotions  [26][14] . -Spontaneous or Natural datasets: conversations among different persons are recorded in real-world environments, such as call-centers, or public places  [13] [3]  [32] . These datasets are more realistic and authentic, but collecting balanced instances of each emotion is really difficult.\n\nWe have analyzed 46 datasets described in the literature and reported our analysis in a table available at the following link: https://mmsp.unimib.it/download-1/. In this table, among the various information, we have reported the name and reference of the dataset, if it is freely available, number of subjects, gender and age, language, type of collection process (with respect to the three categories introduced), and in particular if there are older adults among the subjects.\n\nFrom this analysis it emerges that: i) the languages of the collected datasets are mainly English and Chinese, as depicted in the pie chart of Figure  1 , right; ii) multilanguage datasets have been proposed  [28]  [19] as language could have an influence in how emotions are expressed  [24] ; iii) most of the available dataset are acted; iv) few works face the problem of SER in case of elderly, or varying the age  [4]  [20]  [38]  [34], and old subjects are rarely present in available datasets  [32]  [7]  [29]  [14], as also depicted in the pie chart of Figure  1 , left; v) the number of available datasets is 12 out of 46.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ser_Ampel: A Multi-Source Dataset",
      "text": "The SER_AMPEL is a multi-source dataset that has the intent of providing a reference in case of SER for Italian older adults. It is composed of different subsets, acquired following different protocols.\n\nThe three subsets of the SER_AMPEL dataset are:\n\n-NOLD evoked subset: this is a dataset of natural conversations among Auser 3  volunteers and older adults, where the emotions are elicited by proper questions and/or music. The list of all the 43 questions is reported in appendix A. Note that for each conversations only few questions, between 2 and 15 depending on the protocol, were used. Instead, the audio files adopted were chosen by the subject himself/herself. -NYNG evoked subset: this is a dataset of natural conversations among young adults where the emotions are elicited in the same way of NOLD.\n\n-AOLD acted subset: portion of conversations of Italian older dubbers segmented from Series and Movies.\n\nThe SER_AMPEL dataset is not yet completed, as the acquisition of some subsets is still ongoing, as well as the segmentation, labelling and transcription steps. In particular, the required steps are:\n\n1. segment all the recorded conversations into phrases of about 15 seconds; 2. label all the segments with respect to two different emotional models: the continuous Valence-Arousal-Dominance model, introduced by Mehrabian and Russell  [31]  and the categorical one defined by Ekman  [12] ; 3. transcribe the speech into text, automatically and/or manually.\n\nIn Table  1  a synthetic description of the subsets of data already collected is reported and described, considering the classification introduced in section 2, the number of subjects and gender, and type of transcription.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method And Results",
      "text": "A general SER model based on XGBoost  [9] , that considers only the acoustic part of the speech, was originally trained on a dataset of English utterances, acted by young and adult actors and actresses  [7] . The classifier has been defined in order to recognize the three sentiment classes: positive, neutral and negative. Starting from this model, two domain adaptation strategies (Kullback-Leiber Important Estimation Procedure (KLIEP)  [37]  and Transfer AdaBoost (TrAdaBoost)  [11] ) have been proposed to adapt the model to older people and to Italian language respectively  [16] . The dataset adopted to train the general model and the two adapted ones are CREMA-D  [7]  and EMOVO  [10] . To better understand what follows, a brief description of these two datasets is necessary. CREMA-D is an English acted dataset, that consists of 7442 audio and video recordings of professional actors, playing 12 utterances each one expressed in six emotional states (happy, sad, anger, fear, disgust and neutral) at different intensity levels. The utterances last about 5 seconds. All the acted sentences have a neutral semantic content. 91 actors and actresses have been involved (43 female), among them 6 are older persons (>= 60 years) and 85 adults (between 20 and 59 years). Data collected from adults are used to train the general model, while data coming from the older persons (hereafter called OLD) are used to adapt it.\n\nEMOVO is an Italian acted dataset of 588 audio recordings. Six young Italian actors (3 males and 3 females) with a mean age of 27.1 years, acted 14 utterances, simulating 7 emotional states: neutral, disgust, fear, anger, joy, surprise, and sadness. This dataset (hereafter called ITA) is used to adapt the general model for Italian speakers.\n\nThe best results obtained in the previous paper are reported in Table  2 . The models, without domain adaptation (DA) and with DA, trained and tested on the same English dataset (CREMA-D) perform better than the models where training and test sets are of different languages. The domain adaptation strategy applied to adapt the general model (trained on English utterances) to the Italian dataset (ITA test set), increases the performance with respect to the general model without DA. While, in the case of the OLD test set, it seems that there are no differences in the performance of the general model and the DA one. In Table  3 , the results obtained applying these models to AOLD subset are reported. AOLD is adopted as test set as it is already labelled and transcribed, while the rest of the SER_AMPEL dataset still requires either labelling and/or transcription.\n\nIn order to use the pre-trained models on this new dataset, the instances of AOLD have been labelled as positive, neutral and negative. The values of valence have been used to define these classes, considering the high valence scores (5-4) as positive sentiment, low valence scores (1-2) as negative sentiment, and the single central valence score (3) as neutral sentiment. Due to the fact that the dataset collection is still on going, the cardinality of the three groups is unbalanced, with 89 instances labeled as negative, 27 as positive and only 3 neutral. The performance while using the general model trained on English adults, is aligned with the results achieved with the same model on the ITA dataset. These low performances can be attributed both to the different language between training and test sets, but also to the presence of different utterances. Note that, the higher result obtained in Table  2  first row, is referred to a model where training and test set come from the same dataset, where the utterances pronounced by adults (training) and older people (test) are the same. Moreover the DA strategies, seem to be not effective, both in the case of trying to adapt to Italian language and to age. A difference between CREMA-D and EMOVO datasets and the AOLD one is related to the phrases pronounced. In fact the AOLD dataset is composed of realistic conversations of about 15 seconds, while the other two datasets contain only short utterances, of about 5 seconds, that in some cases correspond to no-sense phrases.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "The development of SER algorithms that adapt their behaviour to different conditions and scenarios is a fundamental step for the definition of systems able to interact in a natural and intuitive way with fragile people such as older adults. These technologies could be employed, for instance, in call centers or mobile communications for the definition of services that adapt their behaviour according to the interlocutor's emotions and mood  [17] . Furthermore, similar systems could be embedded into nursing-care or assistive robots that continuously interact with older adults or patients to help them in their daily lives. From the subject's voice, the robot could be able to infer the emotional state of the speaker and adapt its behaviour in order to define a more efficient and trustworthy interaction between robot and people  [35] . In this paper the SER task is specifically considered with respect to Italian older people. From the results here obtained, it emerges the need of a specific dataset, uniform both for what concerns the language and the age of the subjects. To guarantee the generalizability of the SER models that can be developed, multi sources for data collection should be considered. The SER_AMPEL dataset, here introduced and still ongoing, is conceived with these aims. Such a dataset could ensure a large collection of speech data useful for the definition of Italian SER algorithms that take into account several factors, for example, the subject's age, the mode of acquisition, or how the emotions are elicited. Furthermore, the dataset could allow the analysis of issues that may affect speech data collected in natural environments, such as the presence of different kinds of noise as well as the occurrence of utterances or words in dialect. Q_40 What is your perception of the area or the city where you live? How do they feel about your neighbourhood? Is there anything you like or dislike? How do you move within your neighbourhood, and from your home to the rest of the city? Q_41 What is your vision of the future city? How do you imagine the neighbourhood where you live, and the city in general, when your grandchildren will be older adults? Q_42 Think about of a song you like, would you describe an episode of your life related to that song? Q_43 What do you think about the experiment you have just performed?",
      "page_start": 6,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Pie charts depicting the SER datasets that include older subjects with respect to all the",
      "page": 3
    },
    {
      "caption": "Figure 1: , right; ii) multi-",
      "page": 3
    },
    {
      "caption": "Figure 1: , left; v) the number of available datasets is 12 out of",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subset": "NOLD\nNYNG\nAOLD",
          "No. of\nNo. of\nAge (in years):\nSpeakers\nFemale\nùúá ¬± ùúé": "81\n31\n73.03 ¬± 5.4\n10\n6\n31.09 ¬± 11,38\n10\n3\n71.4 ¬± 5.3",
          "No of\nAverage\nTranscriptions\nRecordings\nduration": "591\n40 sec\nmanual\n150\n1 min\nmanual\n120\n15 sec\nautomatic"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier DA Strategy": "XGBoost",
          "Test Set": "OLD\nNo DA\nITA",
          "Macro\nF1-score": "62%\n35%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,79\n0,55\n0,51\n0,71\n0,28\n0,07",
          "Accuracy": "70%\n57%"
        },
        {
          "Classifier DA Strategy": "",
          "Test Set": "OLD\nTrAdaBoost\nITA",
          "Macro\nF1-score": "62%\n44%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,79\n0,56\n0,52\n0,68\n0,25\n0,39",
          "Accuracy": "70%\n56%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Macro\nF1-score": "32%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,74\n0,00\n0,22",
          "Accuracy": "60%"
        },
        {
          "Macro\nF1-score": "36%\n30%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,85\n0,00\n0,35\n0,42\n0,14\n0,35",
          "Accuracy": "75%\n37%"
        },
        {
          "Macro\nF1-score": "36%\n31%",
          "Negative\nNeutral\nPositive\nF1-score\nF1-score\nF1-score": "0,67\n0,00\n0,40\n0,76\n0,00\n0,17",
          "Accuracy": "56%\n62%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "Q_01\nQ_02\nQ_03\nQ_04\nQ_05\nQ_06\nQ_07\nQ_08\nQ_09\nQ_10\nQ_11\nQ_12\nQ_13\nQ_14",
          "Questions": "Tell us about a fun adventure you had with your friends\nWhat is your favourite time of day? Please, describe it\nAre there any hobbies or activities you began during the lockdown? If so, please describe them\nWhere would you like to go on vacation?\nWhat is your favorite season and why?\nTell us about the happiest moment of your life\nDo you have grandchildren? Please, tell us about them\nDescribe a happy moment spent with your parents\nAre you married? If so, how did you meet each other?\nHave you ever traveled? If so, tell us about your most beautiful trip\nDescribe the happiest moment you experienced during the lockdown\nWere you able to keep in touch with friends and relatives during the lockdown? If so, please describe\nhow\nHas the lockdown period allowed you to rediscover activities or passions that you had set aside?\nWhat did you enjoy most during the lockdown period?"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "Q_15\nQ_16\nQ_17\nQ_18\nQ_19\nQ_20\nQ_21\nQ_22\nQ_23\nQ_24",
          "Questions": "Please, describe your ordinary day / Which activities do you daily perform?\nDescribe in detail the path to go shopping\nWhere do you usually read the news?\nPlease describe a room of your home in detail\nWhat did you have for lunch/dinner?\nAge, gender, place of birth, educational background and work history\nPlease, describe the place where you live\nPlease, describe the type of dwelling where you live\nPlease, describe what a car looks like\nPlease, describe what a chair looks like"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "Q_25\nQ_26\nQ_27\nQ_28\nQ_29\nQ_30\nQ_31\nQ_32\nQ_33\nQ_34\nQ_35\nQ_36\nQ_37",
          "Questions": "Tell us about a news story you recently heard that saddened you?\nIs there anything in your neighborhood you want to change?\nWhat did you miss most during the lockdown period?\nWhat do you think about the recent price increase?\nWhat is the most dramatic moment in someone‚Äôs life?\nWhat activities or hobbies have you been unable to carry out during the lockdown?\nHave you ever suffered the loss of a loved one?\nHave you ever been let down by someone you cared about?\nHas the economic situation ever affected your happiness during any moment of your life?\nDo you remember a moment of your childhood that saddened you deeply?\nHave you ever suffered a disappointment of love?\nWhat saddened you most during lockdown?\nWhat saddened you most during the pandemic from a global point of view?"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "Q_38\nQ_39\nQ_40\nQ_41\nQ_42\nQ_43",
          "Questions": "Do you think the experience of the pandemic could have affected you in any way?\nWhat is an object you particularly care about and what does it mean for you?\nWhat is your perception of the area or the city where you live? How do they feel about your neigh-\nbourhood? Is there anything you like or dislike? How do you move within your neighbourhood,\nand from your home to the rest of the city?\nWhat\nis your vision of the future city? How do you imagine the neighbourhood where you live,\nand the city in general, when your grandchildren will be older adults?\nThink about of a song you like, would you describe an episode of your life related to that song?\nWhat do you think about the experiment you have just performed?"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Ak√ßay",
        "K Oƒüuz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E N√∂th"
      ],
      "year": "2008",
      "venue": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition among elderly individuals using multimodal fusion and transfer learning",
      "authors": [
        "G Boateng",
        "T Kowatsch"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Social robots for depression in older adults: a systematic review",
      "authors": [
        "S Chen",
        "C Jones",
        "W Moyle"
      ],
      "year": "2018",
      "venue": "Journal of Nursing Scholarship"
    },
    {
      "citation_id": "9",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "10",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "11",
      "title": "Boosting for transfer learning",
      "authors": [
        "W Dai",
        "Q Yang",
        "G Xue",
        "Y Yu"
      ],
      "year": "2007",
      "venue": "Boosting for transfer learning",
      "doi": "10.1145/1273496.1273521"
    },
    {
      "citation_id": "12",
      "title": "Are there basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions"
    },
    {
      "citation_id": "13",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "year": "2021",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Lssed: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Socially facilitative robots for older adults to alleviate social isolation: A participatory design workshop approach in the us and japan",
      "authors": [
        "M Fraune",
        "T Komatsu",
        "H Preusse",
        "D Langlois",
        "R Au",
        "K Ling",
        "S Suda",
        "K Nakamura",
        "K Tsui"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "16",
      "title": "Sentiment recognition of italian elderly through domain adaptation on cross-corpus speech dataset",
      "authors": [
        "F Gasparini",
        "A Grossi"
      ],
      "year": "2022",
      "venue": "CEUR Workshop Proceeding"
    },
    {
      "citation_id": "17",
      "title": "Two-stream emotion recognition for call center monitoring",
      "authors": [
        "P Gupta",
        "N Rajput"
      ],
      "year": "2007",
      "venue": "Eighth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "18",
      "title": "Playing a different imitation game: Interaction with an empathic android robot",
      "authors": [
        "F Hegel",
        "T Spexard",
        "B Wrede",
        "G Horstmann",
        "T Vogt"
      ],
      "year": "2006",
      "venue": "2006 6th IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "19",
      "title": "Interface databases: Design and collection of a multilingual emotional speech database",
      "authors": [
        "V Hozjan",
        "Z Kacic",
        "A Moreno",
        "A Bonafonte",
        "A Nogueiras"
      ],
      "year": "2002",
      "venue": "Interface databases: Design and collection of a multilingual emotional speech database"
    },
    {
      "citation_id": "20",
      "title": "A speech emotion recognition method for the elderly based on feature fusion and attention mechanism",
      "authors": [
        "Q Jian",
        "M Xiang",
        "W Huang"
      ],
      "year": "2022",
      "venue": "Third International Conference on Electronics and Communication; Network and Computer Technology (ECNCT 2021)"
    },
    {
      "citation_id": "21",
      "title": "Affective human-robotic interaction",
      "authors": [
        "C Jones",
        "A Deeming"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "23",
      "title": "Reading emotions, reading people: Emotion perception and inferences drawn from perceived emotions",
      "authors": [
        "J Lange",
        "M Heerdink",
        "G Van Kleef"
      ],
      "year": "2022",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "24",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "25",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "26",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd international conference on data engineering workshops (ICDEW'06)"
    },
    {
      "citation_id": "27",
      "title": "Ensemble methods for spoken emotion recognition in call-centres",
      "authors": [
        "D Morrison",
        "R Wang",
        "L De Silva"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "28",
      "title": "Categorical vs dimensional perception of italian emotional speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Categorical vs dimensional perception of italian emotional speech"
    },
    {
      "citation_id": "29",
      "title": "Toronto emotional speech set (TESS",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "30",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "31",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "32",
      "title": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "E Messner",
        "A Hamilton",
        "S Amiriparian",
        "A Baird",
        "G Rizos",
        "M Schmitt",
        "L Stappen"
      ],
      "year": "2020",
      "venue": "The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks"
    },
    {
      "citation_id": "33",
      "title": "Social isolation and loneliness in older adults: Opportunities for the health care system",
      "authors": [
        "E Medicine"
      ],
      "year": "2020",
      "venue": "Social isolation and loneliness in older adults: Opportunities for the health care system"
    },
    {
      "citation_id": "34",
      "title": "Is everything fine, grandma? acoustic and linguistic modeling for robust elderly speech emotion recognition",
      "authors": [
        "G Soƒüancƒ±oƒülu",
        "O Verkholyak",
        "H Kaya",
        "D Fedotov",
        "T Cad√©e",
        "A Salah",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Is everything fine, grandma? acoustic and linguistic modeling for robust elderly speech emotion recognition",
      "arxiv": "arXiv:2009.03432"
    },
    {
      "citation_id": "35",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "36",
      "title": "Automatic classification of emotion related user states in spontaneous children's speech",
      "authors": [
        "S Steidl"
      ],
      "year": "2009",
      "venue": "Automatic classification of emotion related user states in spontaneous children's speech"
    },
    {
      "citation_id": "37",
      "title": "Direct importance estimation with model selection and its application to covariate shift adaptation",
      "authors": [
        "M Sugiyama",
        "S Nakajima",
        "H Kashima",
        "P Buenau",
        "M Kawanabe"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Age driven automatic speech emotion recognition system",
      "authors": [
        "D Verma",
        "D Mukhopadhyay"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Computing, Communication and Automation (ICCCA)"
    },
    {
      "citation_id": "39",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    }
  ]
}