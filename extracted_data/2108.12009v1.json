{
  "paper_id": "2108.12009v1",
  "title": "Emoberta: Speaker-Aware Emotion Recognition In Conversation With Roberta",
  "published": "2021-08-26T19:34:26Z",
  "authors": [
    "Taewoon Kim",
    "Piek Vossen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition in conversation) task. By simply prepending speaker names to utterances and inserting separation tokens between the utterances in a dialogue, EmoBERTa can learn intra-and inter-speaker states and context to predict the emotion of a current speaker, in an end-to-end manner. Our experiments show that we reach a new state of the art on the two popular ERC datasets using a basic and straight-forward approach. We've open sourced our code and models at https://github.com/tae898/erc.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The scope of emotion recognition is very wide, ranging from stills of face images, audio data to the actual utterances or text as in tweets. In this paper, we focus on emotion recognition in conversation (ERC), which is a subfield of emotion recognition. More specifically, the task is to predict the emotion of a current speaker who's engaging in a conversation with one person or more. Recognizing emotion is important to areas such as affective computing and human-robot communication, in which it can be an important feedback mechanism.\n\nAs humans use multiple sensory inputs to have a conversation  (e.g., vision, voice, etc.)  the ERC task can also include multiple modalities  (e.g., visual, audio, text, etc.) . Here, we report on our first experiments on the text modality, leaving using multiple modalities within our framework to future work.\n\nSince the introduction of the Transformer  (Vaswani et al., 2017) , transformer-based deep neural network models have become the dominating neural network model in sequence modeling. Especially, pretrained encoder-only models such as BERT  (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019)  have shown that they can be success-fully fine-tuned for downstream tasks, such as sentence classification and question answering. ERC can be seen as a special case of sequence modeling, since emotions are expected to be triggered by a preceding event in any modality.\n\nOur approach to ERC enriches such transformer models by including the speaker identity in the sequence information spanning multiple utterances. By adapting the RoBERTa sequence representation, we improve the SOTA on two popular benchmark datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Most of the existing works on ERC combine different kinds of neural network architectures (e.g., CNNs, RNNs, Transformers, GNNs, etc.)  (Li et al., 2020b) ,  (Li et al., 2020c) ,  (Ishiwatari et al., 2020) ,  (Wang et al., 2020) ,  (Hazarika et al., 2021) ,  (Sheng et al., 2020) ,  (Ghosal et al., 2020) ,  (Ghosal et al., 2019) . The biggest downside of such approaches is that each part of the model is responsible for extracting their own features. These extracted features might not be ideal for the other parts of the model. Also, since these models are combinations of sub-models, it is hard to understand what each sub-model is contributing and how to improve the overall model. Some of the approaches try to take advantage of external knowledge bases  (Ghosal et al., 2020) ,  (Zhong et al., 2019a) , which adds even more complexity to the model.\n\nA substantial number of approaches are heavily based on RNNs (e.g., GRU) to model the sequence  (Jiao et al., 2019) ,  (Lu et al., 2020) . The biggest problem with this is that it inherently decouples the word embedding extraction and the sequence modeling, whereas BERT-like models tackle them at once, often leading to a better performance. Also, they have to rely on external decontextualized word embedding extractors (e.g., GloVe  (Pennington et al., 2014)  or word2vec  (Mikolov et al., 2013) ). Furthermore, training an RNN is very inefficient since \"backpropagation through time\" has to wait until the last input of a sequence has been processed.\n\nThe approaches that are most closely related to us are HiTrans  (Li et al., 2020a)  and DialogXL  (Shen et al., 2020) . HiTrans packs multiple utterances with [CLS] tokens prepended into one input sequence. This sequence is first fed into a BERT and then to another transformer. DialogXL is based on XLNet  (Yang et al., 2019) . Our approach differs in that we just use RoBERTa and encode the speaker information with multiple utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Definition",
      "text": "Let's say a dialogue of M utterances is given and I interlocutors are engaging in a conversation. Then the dialogue can be expressed as a list of vectors: dialogue = [x 1 , x 2 , ..., x M ], where an utterance x t contains several words (tokens). Each utterance x t is spoken by a unique speaker ∈ I. Since this is a supervised setup, every utterance x t has one corresponding label y t that is annotated by a human.\n\nThe simplest way to solve this problem is to come up with a function f that takes x t as an input and outputs the correct label y t . However, this doesn't take context into account. It's easily conceivable that the function f should also consider the past utterances [x 1 , x 2 , ..., x t-1 ] or even the future utterances [x t+1 , x t+2 , ..., x M ] to model the context.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emoberta",
      "text": "EmoBERTa starts from the pretrained roberta-large model  (Liu et al., 2019) . Since the task is basically a sequence classification task, we simply add a randomly initialized linear layer with the softmax nonlinearity to the first hidden state (this state corresponds to the [CLS] token) of the last layer of the pretrained model.\n\nWe chose RoBERTa, among the many BERTlike models, because its structure is not only relatively simple, but also it can deal with more than two segments. The original authors of RoBERTa simply used two </s> tokens consecutively as [SEP] token, which separates the first and the second segments. Although the pretrained model has not been trained on more than two segments, we show that EmoBERTa can be generalized to three segments per input sequence.\n\nThe first, second, and third segments contain the past utterances, the current utterance, and the future utterances, respectively, in a dialogue. Each utterance is prepended with the name of a speaker so that the model is aware which utterance is spoken by whom. The task is to predict the emotion of the current utterance.\n\nRoBERTa uses <s> and </s> as [CLS] and [EOS] tokens, respectively. Building an input sequence for EmoBERTa is outlined in Algorithm 1. Example sequences can be found in Figure  1 .\n\nAlgorithm 1: Building an input sequence 1  Given the current utterance",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training",
      "text": "The loss is calculated as the sum of cross entropy loss and L 2 weight decay  (Krogh and Hertz, 1991) . We use adaptive gradient descent  (Kingma and Ba, 2015) ,  (Loshchilov and Hutter, 2019)  with gradual linear warmup learning rate scheduling  (Goyal et al., 2017) . The peak learning rate was determined using Optuna  (Akiba et al., 2019) . Mixed floating point precision was used to reduce the training time and increase the batch size  (Micikevicius et al., 2017) . See Appendix A.1 for the details (e.g., hyperparameters, training time, hardware, etc.). We mostly used the huggingface transformer pytorch library for training  (Wolf et al., 2020) .\n\nThen the training loss function is\n\n1 The pretrained RoBERTa model can have a maximum of 512 tokens in one input sequence. The while loop terminates if there are no more available past / future utterances to add in the dialogue. We empirically found that capitalizing the names of the interlocutors leads to slightly better results.\n\n(1) where y (i) , is a one-hot label vector, ŷ(i) is the softmax output vector given the input x (i) , N is the number of training data samples, C is the number of classes, λ is a L 2  regularization rate, and w are the weights of the model. In practice, the data samples are batched, and stochastic gradient descent is used.\n\nAlthough the weights w were tuned to minimize the loss value, the final model is chosen where the weighted f 1 score on the validation split is the highest, since that's the metric that we report.  We test EmoBERTa on the two popular ERC datasets. MELD  (Poria et al., 2019)  is a multimodal (visual, audio, and text) and multi-party (more than two interlocutors in a dialogue) conversational dataset. It was collected from the TV series Friends. The seven emotions are neutral, joy, surprise, anger, sadness, disgust, and fear. Weighted f 1 score is used to evaluate the performance, as the class distribution is highly imbalanced.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "IEMOCAP  (Busso et al., 2008 ) is a multimodal (visual, audio, and text) and dyadic (only two interlocutors in a dialogue) conversational dataset. Ten actors participated in the data collection. Although the original dataset contains 11 different emotions, only six of them are used for evaluation. They are neutral, frustration, sadness, anger, excited, and happiness. As MELD, the class distribution is highly imbalanced and thus a weighted f 1 score will be used for evaluation. Unlike MELD, IEMOCAP does not officially have the names of the speakers. Therefore, we gave each actor a random name. See Appendix A.2 for the details.\n\nSome statistics on train, val, and test splits of both datasets can be found at Table  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baselines",
      "text": "We compare our model with the models we mentioned in Section 2.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quantitative Analysis",
      "text": "Table  2  shows the performance of our models and the baselines. Our models outperform the other models on both MELD and IEMOCAP.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "MELD IEMOCAP BERT+MTL  (Li et al., 2020b)  61.90 -BiERU-lc  (Li et al., 2020c)  60.84 64.65 DialogueGCN  (Ghosal et al., 2019)  58.1 64.18 RGAT  (Ishiwatari et al., 2020)  60.91 65.22 CESTa  (Wang et al., 2020)  58.36 67.1 VHRED  (Hazarika et al., 2021)  -58.6 SumAggGIN  (Sheng et al., 2020)  58.45 66.61 COSMIC  (Ghosal et al., 2020)  65.21 65.28 KET  (Zhong et al., 2019b)  58.18 59.56 BiF-AGRU  (Jiao et al., 2019)  58.1 63.5 Iterative  (Lu et al., 2020)  60.72 64.37 HiTrans  (Li et al., 2020a)  61.94 64.5 DialogXL  (Shen et al., 2020)  62 EmoBERTa shows very good results: max weighted f 1 scores (%) of 65.61 (MELD) and 68.57 (IEMOCAP) respectively and above the best reported SOTA, especially considering that no modifications were made to the original RoBERTa model architecture. We also trained a model without the speaker names prepended, which drops the performance: weighted f 1 scores (%) of 65.07 (MELD) and 64.02 (IEMOCAP) respectively, pro-viding evidence that encoding the speaker information helps.\n\nEspecially on IEMOCAP dataset, although we had to come up with random names for the actors, EmoBERTa was able to learn what's important. As for IEMOCAP, the results were better using only past utterances, rather than using both past and future utterances. We believe that this is due to the fact that IEMOCAP has many more utterances than MELD, and thus we couldn't fit all the past and future utterances in one sequence, meaning that only using past utterances can fit more past utterances than aiming for both. Past utterances were apparently more useful than future utterances to predict the emotion of a current utterance.\n\nThere was a bigger performance gain by incorporating past and/or future utterances in IEMOCAP than  MELD (12.48 vs. 2.15) . We believe that this is due to the fact that the nature of IEMOCAP is more contextual than that of MELD.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "To get more insight in the value of encoding the speaker, we did a qualitative analysis on 10 correctly and 10 incorrectly classified random samples from each test split.\n\nOur manual inspection shows that the model tries to learn the dynamics of the interlocutors in the beginning layer, as the current speaker tokens attend to the interlocutor tokens (We observed this behavior from all the 20 MELD and 20 IEMOCAP random test samples). As for MELD, in all 100% of the correctly classified samples, based on the top 10 attended tokens, the <s> token of the last layer attended to the speaker token of the target (current) utterance, whereas this ratio was only 60% for the incorrectly classified samples. This verifies that the current speaker token increasingly contains important information, as the tokens move on to the higher layers.\n\nFigure  1  shows a visualization of one correctly and one incorrectly classified random samples.\n\nAs the <s> token in the last layer focuses on the current speaker and his/her utterance, we believe that this information is what the model finds most useful to make the final prediction.\n\nNote that in the incorrectly classified example, the <s> token in the last layer does not focus on the current speaker but some random punctuation marks throughout the conversation, thus leading to an incorrect prediction. See Appendix A.3 for the random IEMOCAP test samples. <s>PHOEBE: I think she took it pretty well. You know Paolo's over there right now, so... ROSS: Ah...ooh! Well, looks like, uh, we kicked your butts. JOEY: No-no, she kicked our butts. You could be on the Olympic standing-there team. ROSS: Come on, two on one. CHANDLER: What are you still doing here? She just broke up with the guy, it's time for you to swoop in! ROSS: What, now? JOEY: Yes, now is when you swoop!</s></s>JOEY: You gotta make sure that when Paolo walks out of there, the first guy Rachel sees is you, She's gotta know that you're everything he's not!</s></s>JOEY: You're like, like the anti-Paolo! CHANDLER: My Catholic friend is right. CHANDLER: She's distraught. CHANDLER: You're there for her. CHANDLER: You pick up the pieces, and then you usher in the age of Ross!</s> <s>PHOEBE: I think she took it pretty well. You know Paolo's over there right now, so... ROSS: Ah...ooh! Well, looks like, uh, we kicked your butts. JOEY: No-no, she kicked our butts. You could be on the Olympic standing-there team. ROSS: Come on, two on one. CHANDLER: What are you still doing here? She just broke up with the guy, it's time for you to swoop in! ROSS: What, now? JOEY: Yes, now is when you swoop!</s></s>JOEY: You gotta make sure that when Paolo walks out of there, the first guy Rachel sees is you, She's gotta know that you're everything he's not!</s></s>JOEY: You're like, like the anti-Paolo! CHANDLER: My Catholic friend is right. CHANDLER: She's distraught. CHANDLER: You're there for her. CHANDLER: You pick up the pieces, and then you usher in the age of Ross!</s> (a) A correctly classified example. Both the prediction and the truth are joy.\n\n<s>CHANDLER: And you're upset because you didn't make your best friend cry? MONICA: I mean, all I'm asking for is just a little emotion! MONICA: Is that too much to ask after six years?! MONICA: I mean what? MONICA: Are-are-are Rachel and I not as close as you guys?! MONICA: I mean do we not have as much fun?! MONICA: Don't I deserve a few tears?!! MONICA: I mean we-we told Joey, he cried his eyes out!</s></s>JOEY: Hey!</s></s>JOEY: I did not cry my eyes out!! JOEY: Come on! JOEY: It's like the end of an era! JOEY: No more J-man and Channie's! CHANDLER: Okay, I gotta ask, who calls us that?!</s> <s>CHANDLER: And you're upset because you didn't make your best friend cry? MONICA: I mean, all I'm asking for is just a little emotion! MONICA: Is that too much to ask after six years?! MONICA: I mean what? MONICA: Are-are-are Rachel and I not as close as you guys?! MONICA: I mean do we not have as much fun?! MONICA: Don't I deserve a few tears?!! MONICA: I mean we-we told Joey, he cried his eyes out!</s></s>JOEY: Hey!</s></s>JOEY: I did not cry my eyes out!! JOEY: Come on! JOEY: It's like the end of an era! JOEY: No more J-man and Channie's! CHANDLER: Okay, I gotta ask, who calls us that?!</s> (b) An incorrectly classified example. The prediction is joy while the truth is anger.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we showed that our new model, EmoBERTa, outperforms other models in the ERC task. Since EmoBERTa can directly attend to the input tokens and interlocutor names, we can easily observe the attention coefficients to see which part of the dialogue the model finds most important to make a final classification.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A Appendix",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A.1 Training Details",
      "text": "We used GCP (Google Cloud Platform) Compute Engine to carry out our experiments. We used an NVIDIA Tesla V100 machine (disclaimer: we are supported by neither Google nor NVIDIA). This GPU has 16 GB of memory and depending on the length of the input sequence, we were able to fit 4 to 16 samples in one batch, using mixed precision. The pretrained roberta-large model has about 355 million parameters. We found that without mixed precision, it's very difficult to train this model, since it's a pretty big model.\n\nWe set the value of L 2 regularization rate as 0.01. Training was done for five epochs. No weights were frozen during the training. The learning rate scheduler was set to linearly increase in the first 20% of training and then linearly decrease in the remaining 80%.\n\nSince the optimal peak learning rate highly depends on the batch size and the other hyperparameters, we used Optuna  (Akiba et al., 2019)  to find its best value. 10% of the training data and the same amount of validation data were used to search for the best value. Optuna ran five trials and looked for the best learning rate, between 1e -6 and 1e -4, that minimizes the cross entropy loss on the validation data split.\n\nThe hyperparameters not mentioned here are all set to the default values.\n\nOne full five-epoch training took about 45 minutes.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A.2 Iemocap Speaker Names",
      "text": "Since the IEMOCAP dataset was created in the US, we used the top five male and female American names over the past 100 years (https://www.ssa.gov/oact/ babynames/decades/century.html).\n\nThe female names used are Mary, Patricia, Jennifer, Linda, and Elizabeth. The male names used are James, John, Robert, Michael, and William.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A.3 Qualitative Analysis On Iemocap",
      "text": "Unlike MELD, only 20% of the correctly classified samples showed the behavior of the last layer <s> token attending to the current target speaker. This ratio was 10% for the incorrectly classified samples. We believe this is due to the fact that the speaker names in the test split of IEMOCAP (i.e., WILLIAM and ELIZABETH) were never seen during training.)\n\nFigure  2  gives you a visualization of the qualitative analysis on the IEMOCAP dataset. <s>WILLIAM: Okay. That's okay, spontaneous is great. Spontaneous is awesome. ELIZABETH: It was really beautiful. WILLIAM: You've got to love that. ELIZABETH: Yeah, we went to a really great jazz show, and um-WILLIAM: Nice, you love the jazz. ELIZABETH: Yeah, and after, we went up to this um, this place like a restaurant on the roof kind of thing. WILLIAM: Oh, okay, yeah. ELIZABETH: So you know like late dinner and champagne and what not. WILLIAM: Were you in Chicago? ELIZABETH: Mm-hmm. WILLIAM: Downtown, was it beautiful? of course it was beautiful ELIZABETH: so beautiful, full moon. WILLIAM: What a guy. I can't believe it. So okay, so do you know any details? When's it going to be? Anything? ELIZABETH: I don't know. I guess next summer. WILLIAM: Am I in it? ELIZABETH: Yeah. Yeah, of course. WILLIAM: Okay, I certainly hope so, of course. ELIZABETH: So next fall I guess. WILLIAM: Next fall. ELIZABETH: Yeah, I like the autumn. WILLIAM: Are you going to do it in town or are you out of town? ELIZABETH: oh, We'll do it in town. WILLIAM: Oh, okay. ELIZABETH: Yeah, his family doesn't live too far away, so I think-WILLIAM: Yeah, where's he from again? ELIZABETH: He's from Chicago as well. WILLIAM: Oh he is, okay. ELIZABETH: Yeah.</s></s>WILLIAM: Cool, perfect. Another Chicagoite, got to love it.</s> <s>WILLIAM: Okay. That's okay, spontaneous is great. Spontaneous is awesome. ELIZABETH: It was really beautiful. WILLIAM: You've got to love that. ELIZABETH: Yeah, we went to a really great jazz show, and um-WILLIAM: Nice, you love the jazz. ELIZABETH: Yeah, and after, we went up to this um, this place like a restaurant on the roof kind of thing. WILLIAM: Oh, okay, yeah   We see a similar behavior as in MELD. Again, in the incorrectly classified example, the <s> in the last layer does not attend to the current speaker.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Algorithm 1: Building an input sequence1",
      "page": 2
    },
    {
      "caption": "Figure 1: shows a visualization of one correctly",
      "page": 4
    },
    {
      "caption": "Figure 1: Two examples from the 20 randomly selected",
      "page": 4
    },
    {
      "caption": "Figure 2: gives you a visualization of the qualita-",
      "page": 7
    },
    {
      "caption": "Figure 2: Two examples from the 20 randomly selected",
      "page": 7
    },
    {
      "caption": "Figure 2: a and 2b, re-",
      "page": 7
    },
    {
      "caption": "Figure 1: , there is only one [SEP] token (i.e.,",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The upper half of the table shows the num- VHRED(Hazarikaetal.,2021) - 58.6",
      "data": [
        {
          "Model": "BERT+MTL (Li et al., 2020b)\nBiERU-lc (Li et al., 2020c)\nDialogueGCN (Ghosal et al., 2019)\nRGAT (Ishiwatari et al., 2020)\nCESTa (Wang et al., 2020)\nVHRED (Hazarika et al., 2021)\nSumAggGIN (Sheng et al., 2020)\nCOSMIC (Ghosal et al., 2020)\nKET (Zhong et al., 2019b)\nBiF-AGRU (Jiao et al., 2019)\nIterative (Lu et al., 2020)\nHiTrans (Li et al., 2020a)\nDialogXL (Shen et al., 2020)",
          "MELD": "61.90\n60.84\n58.1\n60.91\n58.36\n-\n58.45\n65.21\n58.18\n58.1\n60.72\n61.94\n62.41",
          "IEMOCAP": "-\n64.65\n64.18\n65.22\n67.1\n58.6\n66.61\n65.28\n59.56\n63.5\n64.37\n64.5\n65.94"
        },
        {
          "Model": "No past and future utterances\nOnly past utterances\nEmoBERTa\nOnly future utterances\nBoth past and future utterances\n→ without speaker names",
          "MELD": "63.46\n64.55\n64.23\n65.61\n65.07",
          "IEMOCAP": "56.09\n68.57\n66.56\n67.42\n64.02"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Optuna: A next-generation hyperparameter optimization framework",
      "authors": [
        "Takuya Akiba",
        "Shotaro Sano",
        "Toshihiko Yanase",
        "Takeru Ohta",
        "Masanori Koyama"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "Chi-Chun Lee",
        "Ebrahim Kazemzadeh",
        "Emily Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "4",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "5",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation"
    },
    {
      "citation_id": "6",
      "title": "Accurate, large minibatch SGD: training imagenet in 1 hour",
      "authors": [
        "Priya Goyal",
        "Piotr Dollár",
        "Ross Girshick",
        "Pieter Noordhuis",
        "Lukasz Wesolowski",
        "Aapo Kyrola",
        "Andrew Tulloch",
        "Yangqing Jia",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "Accurate, large minibatch SGD: training imagenet in 1 hour"
    },
    {
      "citation_id": "7",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2020.06.005"
    },
    {
      "citation_id": "8",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "9",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2019",
      "venue": "Real-time emotion recognition via attention gated hierarchical memory network"
    },
    {
      "citation_id": "10",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "11",
      "title": "A simple weight decay can improve generalization",
      "authors": [
        "Anders Krogh",
        "John Hertz"
      ],
      "year": "1991",
      "venue": "Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS'91"
    },
    {
      "citation_id": "12",
      "title": "HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.370"
    },
    {
      "citation_id": "13",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis"
    },
    {
      "citation_id": "15",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "16",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "7th International Conference on Learning Representations, ICLR 2019"
    },
    {
      "citation_id": "17",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.360"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Paulius Micikevicius",
        "Sharan Narang",
        "Jonah Alben",
        "Gregory Diamos",
        "Erich Elsen",
        "David García",
        "Boris Ginsburg",
        "Michael Houston",
        "Oleksii Kuchaiev",
        "Ganesh Venkatesh",
        "Hao Wu"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "21",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "22",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P16-1162"
    },
    {
      "citation_id": "23",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.367"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "27",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "citation_id": "28",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Salakhutdinov",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Xlnet: Generalized autoregressive pretraining for language understanding"
    },
    {
      "citation_id": "29",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations"
    },
    {
      "citation_id": "30",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1016"
    }
  ]
}