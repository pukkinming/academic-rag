{
  "paper_id": "2203.15829v1",
  "title": "An Eeg-Based Multi-Modal Emotion Database With Both Posed And Authentic Facial Actions For Emotion Analysis",
  "published": "2022-03-29T18:02:12Z",
  "authors": [
    "Xiaotian Li",
    "Xiang Zhang",
    "Huiyuan Yang",
    "Wenna Duan",
    "Weiying Dai",
    "Lijun Yin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion is an experience associated with a particular pattern of physiological activity along with different physiological, behavioral and cognitive changes. One behavioral change is facial expression, which has been studied extensively over the past few decades. Facial behavior varies with a person's emotion according to differences in terms of culture, personality, age, context, and environment. In recent years, physiological activities have been used to study emotional responses. A typical signal is the electroencephalogram (EEG), which measures brain activity. Most of existing EEG-based emotion analysis has overlooked the role of facial expression changes. There exits little research on the relationship between facial behavior and brain signals due to the lack of dataset measuring both EEG and facial action signals simultaneously. To address this problem, we propose to develop a new database by collecting facial expressions, action units, and EEGs simultaneously. We recorded the EEGs and face videos of both posed facial actions and spontaneous expressions from 29 participants with different ages, genders, ethnic backgrounds. Differing from existing approaches, we designed a protocol to capture the EEG signals by evoking participants' individual action units explicitly. We also investigated the relation between the EEG signals and facial action units. As a baseline, the database has been evaluated through the experiments on both posed and spontaneous emotion recognition with images alone, EEG alone, and EEG fused with images, respectively. The database will be released to the research community to advance the state of the art for automatic emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are complex responses to significant internal and external events. They are states of feeling that result in physical and psychological changes that influence our behavior  [23] . They involve different components, such as subjective experience, cognitive processes, expressive behavior, psychophysiological changes, and instrumental behavior. Most of existing research attempts to identify the emotion with one of the components, for example, behavioral signs (e.g., facial expressions) or physiological signals (e.g., EEG). Recent work (e.g., Soleymani et al  [24] ) has started to investigate emotion by combination of both EEG and facial expressions. However, the facial expression variation is very trivial, and there are no EEG associated with individual action units  [6]  included. Lack of multi-modal data with action units, expressions, and EEG signals impedes the development of the field. This motivates us to create a new database of both modalities with explicit action units and spontaneous expressions, in an attempt to explore the fusion of both external facial activity and internal brain activity in order to advance the study and understanding of emotion recognition.\n\nFor context, we present an overview of the state-of-the-art databases for facial expression and physiology based emotion recognition in Table  I .\n\n1. Facial expression analysis: Facial expression analysis (FEA) has been studied from both posed expressions and spontaneous expressions along with their facial action units. Several existing databases have been used as testbeds for FEA (Table  1 ), for example, CK+  [16] , Oulu-CASIA  [33] , MMI  [21] , BU-3DFE  [31] , BU-4DFE  [29] , BP4D+  [32] , DISFA  [18] . Some of them have been used in multi-modal systems  [3]  with both videos/images and other modalities (e.g., geometry, audio, 2D, 3D) for facial expressions recognition (FER), nevertheless, most of them relied on the visual information from videos or images. For example, FER accuracy 94%∼99% has been achieved on CK+ for 6∼8 expressions  [11] ,  [34] ,  [14] , 80%∼92% with 6 classes on MMI  [30] ,  [26] , and 80%∼92% on Oulu-CASIA  [30] ,  [14] . It still poses a challenge in identifying emotions from facial expressions.\n\n2. Affective analysis: To address the issue, other modalities such as physiological signals have been employed for emotion analysis, typically, for example, the EEG signals  [13] ,  [5] ,  [35] ,  [25] . The EEG-based emotion recognition achieved 80% accuracy in arousal and valence  [6]  dimension  [27] ,  [1] , 50%∼80% for classifying four classes of emotions  [15] ,  [1] , and 64% for classifying six emotions  [1] . However, it is still challenging for realizing a reliable emotion analysis from physiology data.\n\nMulti-modal fusion from visual modality and physiology modality seems a promising method to address this issue  [10] . We propose the creation of a new database with visual data and EEG data for both posed facial actions and elicited spontaneous emotions. To verify the correlation of EEG and facial actions, we also collect EEG data associating with each action unit individually. We further validate and compare the results from single modality (e.g., expression alone, EEG alone) and multi-modality (e.g., combined expression and EEG) for recognition of both posed expressions and spontaneous emotions.\n\nThe contribution of this work is three-fold:\n\n• This is the first data corpus with EEGs associating with individual AUs explicitly, which enables the study of relation of facial actions and EEG responses, thus allowing the information compensation as well as combination from both facial expressions and EEGs for emotion recognition.    [12]  and CK+  [16]  On command and Naturally occur 97 and 26 2D videos facial expression and action units DISFA  [18]  Induced 27 2D videos action units MMI  [21]  On command and Induced 210 and 25 2D videos, audio, physiological signal facial expression and action units BP4D+  [32]  Induced 140 3D geometric facial sequences, 2D facial videos, thermal videos, physiological data sequence facial expression and action units Databases for Affective Analysis SEED  [5]  Movie induced 15 32-channel EEG arousal, valence SEED IV  [35]  Movie induced 15 32-channel EEG and eye track data arousal, valence MAHNOB HCI  [25]  Movie induced The EMG-like artifacts and EOG-like artifacts can be used as complementary information to benefit the expression analysis. The findings leads to the fusion of facial expressions and EEGs to improve the affection analysis. The following sections will elaborate the data acquisition and validation, followed by a conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Data Acquisition A. Participants",
      "text": "Twenty-nine participants from the authors' institute were recruited for participating in the experiment of data collection. There are 22 males and 7 females, with ages ranging from 18 to 38 years old. Ethnic Ancestries include Asian, Mid-Eastern, White and Hispanic/Latino. The ethnic distribution is showed in Table  II . Following the IRB-approved protocol, all participants signed an informed consent form before the start of experimentations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Recording System",
      "text": "The EGI's GTEN™ 100 Research Neuromodulation System  [17]  has been used for EEG data collection. It has 128 sensors to both record brain electrical activity and modulate it without additional sponge pads or electrodes. EGI's Net Station Acquisition software is designed for the acquisition of dense-array EEG data. Different from previous EEG-based  affective databases, e.g., DEAP  [13]  which uses International 10-20 System with 32 sensors, we used equipment with 128 sensors to record the EEG signal. The 128 sensors can cover the forehead and cheeks, providing more sensitive and reliable capture ability for facial action analysis. We set the sample frequency at 1000 Hz and chose the cut-off frequency for the Net Amps high-pass filter, with a 0.1 Hz cutoff by default. Fig.  1  shows the data collection at work, corresponding EEG electrodes with a frontal view, and EEG electrodes location information, respectively. System synchronization is a critical process for data col- lection from multi-modality sensors. Both the EEG recording system and video recording software can generate timestamps when starting and ending the recording process. By comparing and calculating the timestamps of starting time from different sensors, we can obtain the synchronized signal accurately.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Experiment Setup And Emotion Stimuli",
      "text": "There are three sessions in the experiment for simultaneous collection of EEG signals and facial action videos, including posed expressions, action units, and spontaneous emotions, respectively. A total of 2,320 experiment trails were recorded, which is a considerably sized database for research. The whole procedure of experiment is shown in the Fig.  2 .\n\n1) In the first session, a video illustrating six prototypical facial expressions (e.g., Anger, disgust, fear, happiness, sadness, and surprise) was shown to a participant. The participant was allowed to mimic the expressions before the start of data collection. Once the data collection starts, the participant was asked to follow the display order of six facial expressions, one-by-one, to imitate the corresponding facial expression, respectively. Each facial expression was performed three times with lasting 2∼8 seconds each trial. During this time, EEG signals and facial expression videos were recorded accordingly. After an expression was imitated, and before moving on to the next expression, there were about 10∼15 seconds for relax in-between.\n\n2) In the second session, a video of displaying 10 facial action units (AU1, AU2, AU4, AU5, AU9, AU12, AU15, AU17, AU23, AU25, AU27) was shown to a participant. The participant followed the order of the 10 AUs one by one, and performed the respective facial action for 2 ∼ 8 seconds each. Every AU was imitated 5 times. In between two AUs, there was 5 ∼ 10 seconds gap to relax.\n\n3) The last session collects authentic emotions elicited from two parts: meditation and pain by cold-pressor. (i) In the first part, the participant had a 5∼10 seconds rest, then closed his/her eyes for meditation. During the meditation of about one minute, he/she could think of anything, and slight head or face muscle movements were allowed. (ii) In the second part, the participant was asked to submerge their hand into ice water for 90 seconds. This is to elicit an emotional response during the physical pain by the cold-pressor, followed by a self-report to rate the numerical scale of pain (0∼10) by the participant. One example of a subject's signal along with face video in each session is shown in Fig.  3 . It shows the signal significantly waved up when actions occurred, and the signal is at low level as participant felt very peaceful.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Data Annotation And Selection",
      "text": "For the posted facial expressions and action units, since the participants followed each video clip sample, their facial action and EEG signals are tagged as the designated expressions or AUs. For the spontaneous emotion (pain and neutral meditation), the self-reports are used to be the reference to rate and label the data. After the pain task, every participant was asked to provide us a brief report about the painful feeling they had, if they had one. All participants manually rate the physical pain level from 0 to 10, where 0 means no-pain and 10 means the worst possible pain. Considering the self-reports are relatively subjective judgment, we also review their corresponding video clips to annotate the data by analysing participants' facial expressions and facial actions. During the meditation task, subjects exhibit a natural state that can be viewed as a reference, which will be used as a neutral affection for the subsequent comparative study.\n\nNote that in order to extract the EEG signals during the event of actions or expressions, we check the video sequences and the associated EEGs sequences, and manually extract the segments of EEGs corresponding to the period of clear facial action or expressions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Data Processing And Feature Extraction",
      "text": "It is necessary to pre-process the collected data before feeding the data to our baseline model. In this section, we elaborate the details for data processing and feature extraction methods. It is worth noting that the collected EEGs' signals show a strong correlation with facial actions and eye blinking of both posed and spontaneous facial expressions. Such EMG-like artifacts (Electromyography) with frequency above 40 Hz and EOG-like artifacts (Electrooculography) with frequency below 4 Hz reflect the facial muscle movement and eye movement/blinking  [8] ,  [7] ,  [20] . Such a finding leads us to use all these signals for emotional facial expression analysis. Therefore, unlike the traditional EEG signal processing that removes those \"noises\" explicitly, it is in our belief that those peripheral signals can be used as complementary information for benefiting the analysis of both posed and spontaneous expressions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Eeg Pre-Processing And Feature Extraction",
      "text": "In order to represent the EEG features in a twodimensional format which is compatible to the 2D images, we take the following steps to process the EEG signals for feature extraction and feature map generation. First, we apply a band-pass filter on the EEG data. Second, we extract the features and generate the feature map. Third, we apply the Kalman filter to smooth the extracted features map. Last, we normalize the extracted features and save them to 2D gray images as the extracted feature maps. Fig.  4  shows a example of the generated EEG feature map. It also shows the pipeline from data acquisition to the subsequent experiment.\n\n1) EEG Pre-processing: Many works  [36] ,  [2] ,  [19]  used to apply a 50 Hz band filter to extract the specific EEG frequency bands (Delta:0.1∼4 Hz, Theta:4∼8 Hz, Alpha:8∼14 Hz, Beta:14∼31 Hz, Gamma:31∼50 Hz). It is to average the specific values in the corresponding frequency band respectively, which has been proved to be an efficient way for down sample. For our assumption, if EEG has the capacity to recognize the external facial expressions with the artifacts from high frequency bands, which are usually realized as noise, then the models may achieve better performance. Therefore, we apply a filter between 0.1 to 100 Hz, keeping most high frequencies information.\n\n2) Feature Extraction: After applying the band-pass filter, we use the short-term Fourier transform (STFT) to extract Power spectral density (PSD) features, and calculate the Differential Entropy (DE)  [35] . The Power spectral density function (PSD) shows the strength of the variations (energy) as a function of frequency. The unit of PSD is energy (variance) per frequency (width). The PSD features can be extracted through STFT, which is a Fourier-related transform that is used to determine the sinusoidal frequency and the phase content of the local sections of a signal as it changes over time. In this paper, we apply a 1 s time window without overlap in meditation and pain tasks. Considering these two tasks are long-time period, to balance the data for each task, we set the same time window but with 0.9s overlap in the other tasks.\n\nDifferential entropy (DE)  [35]  is used to measure the average surprisal of a continuous random variable. Its formula can be expressed as\n\nwhere X is a random variable, and f (x) is the probability density function of X. In this paper, f (x) is the PSD feature extracted by STFT, so we can extract DE features though,\n\nAfter extracting the DE feature, we generate three different feature maps (FP), namely is Feature A, Feature B, and Feature C, respectively. Feature A is derived by the DE feature from 5 frequency bands (Delta:0.1∼4 Hz, Theta:4∼8 Hz, Alpha:8∼14 Hz, Beta:14∼31 Hz, and Gamma:31∼50 Hz). Feature B is derived by 7 frequency bands (0.1∼4 Hz, 4∼8, 8∼14 Hz, 14∼31 Hz, 31∼50 Hz, 50∼75 Hz, and 75∼100 Hz). And Feature C is same as the extracted DE feature with 100 frequency bands. Combining with 128 location channels, the three feature maps are in size of 5×128, 7×128, and 100×128, respectively. The specification of EEG features is shown in Table  III . Based on the existing works  [35] ,  [13] ,  [1]  which shows the five bands combination (Delta, Theta, Alpha, Beta and Gamma) performs better than each individual band, we apply the whole frequency bands in our experiment, instead of testing the independent frequency band respectively. Note that Feature A is generated by the traditional method, thus it is used for comparison in the subsequent experiment. Afterwards, we implement the Kalman filter  [22]  for feature map smoothing. The Kalman filter has been proved to be an efficient recursive filter that estimates the internal state of a linear dynamic system (LSD)  [35]  from a series of noisy measurements.\n\nFinally, we apply the normalization to the extracted smoothed DE feature and save it to a 2D gray image as the feature map.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Facial Image Processing",
      "text": "We record a video of every subject from a frontal camera with 24 frames per second. During the data processing, we extract all frames corresponding to the selected EEG data. We then crop the faces and resize them by 128×128 using the OpenCV face detector and landmark detector.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments And Validation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Classifiers",
      "text": "In this paper, we apply three classifiers as baseline models to evaluate the performance of the new dataset: a linear SVM (EEG), a deep CNN model (static facial image), and a DANN model (both). All the experiments we conducted in this paper are with a subject-independent manner.\n\n1) SVM: We adopt the Support Vector Machine (SVM) to be the baseline algorithm for EEG based experiments. We apply it to our EEG extracted features for classification. The linear SVM kernels are imported from scikit-learn to train and evaluate the model. We search the parameter C from 0.001 to 0.2 to find the optimal value.\n\n2) CNN: A CNN model is deployed as a baseline model in the classification of static facial expression. We build a 4 layer convolution neural network, with each layer consisting by a conv2d, batch normalization, Relu activation, and max pooling. The out channels are set by 64, 128, 256 and 512 specifically. We set the first 3 max-pooling with kernel size 2 and the last one with kernel size 4. For the last two layers, we set a dropout for each layer. The learning rate we set up is 10 -5 with learning rate decay of every 30 steps with a gamma of 0.8. We implement the CNN model through the PyTorch framework.\n\n3) DANN: Considering the variant of EEG representation from different subjects, we adopt Domain-Adversarial Neural Network (DANN)  [28]  as the comparison algorithm. DANN uses the idea of generative adversarial networks (GAN)  [9]  which designs an adversarial process that learns both generative model and discriminative model, in which the discriminative model estimates the probabilistic distribution of a sample from a real dataset or a fake generative data set but the generative model aims to estimate the distribution of real training data. Based on the GAN method, the DANN method aims to generate domain-invariant data features that are discriminative for the classification task whereas indiscriminative for the shift between the source and target domains. In this paper, we treat the training instances as the source domain data and validation instances as the target domain data. The specification of the DANN structure is as follows: the feature extractor has 4 layers, their corresponding channels are 3 to 50, 50 to 100, 100 to 200, 200 to 400, and the kernel size is 5 × 5. We adopt max pooling to reduce the feature dimension, and their max-pooling sizes are 2, 2, 2 and 4. We also set the dropout layer to avoid overfitting, and they are located after layer 3 and layer 4. The dropout rate is 0.5. The activation function is Relu. We do batch normalization after each convolutions layer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Fusion Strategy",
      "text": "Feature level fusion (FLF) was employed to fuse the features from two modalities (facial expression images and EEG signals). Multimodal feature fusion is expected to bring more considerable performance improvement of recognition for the spacial and temporal information they carry. We concatenate the facial expression and their corresponding EEG feature map directly to form a fused feature map before feeding them into the model. Since there are three types of EEG features as shown in Table  III , we employed three types of fusion features, which are the combinations of EEG Feature A, B and C. The concatenated fusion features were resized to the same 2D dimension (128 × 128). We used the same DANN model for a fair evaluation of the performance of both fused features and single modality features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Session 1: Posed Facial Expression Recognition",
      "text": "We applied a linear SVM using three types of EEG features from Table III for 7 class facial expression recognition (Neutral, Sadness, Fear, Happiness, Anger, Disgust, and Surprise). Using leave-one-out cross-validation, the average accuracy of EEG Feature A, Feature B, and Feature C are 70.3%, 73.8%, and 71.8% respectively. Accordingly, the validation results are 68.4%, 71.9%, and 71.1% when the 4-fold cross-validation is applied. The CNN-based approach achieves 70.3% accuracy in leave-one-out cross-validation and 68.4% accuracy in the 4-fold cross-validation. The experimental results shows that DANN outperforms the linear SVM and CNN in both modalities, thus we apply   the DANN to compare the single modal features and multimodal features.\n\nTable  IV  shows the performance of single modal features and two-modal fused features for posed expression recognition using DANN. First, it clearly shows that EEG feature B and EEG feature C perform better than EEG Feature A because the high-frequency EEG signals (over 50HZ) are included in the feature maps. Such high-frequency EEG signals provide necessary complementary information associating with individual facial expressions to improve the classification performance significantly. Second, the performance based on the facial expression images is superior to the the performance of EEG-based single modal features by using DANN. Finally, the two-modal fusion based method generally outperforms the single modal feature based method for posed facial expression recognition. Fig.  5  shows the data distribution with 4-fold cross-validation using the t-SNE embedding method  [4]  on facial expression images and fusion features. As we can see that the separability of fusion based method is much better than the facial expression based method. Table V illustrates the confusion matrix of the fusion method.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Session 2:Facial Action Units Analysis",
      "text": "To investigate the relation of AUs and the corresponding EEGs, For each AU, we calculate the average map of EEG Feature C from all subjects, and show the corresponding histogram of the corresponding average feature map. Fig.  6  shows average feature map of each AU and the corresponding histogram, which are distinguishable each other. Fig.  6a  shows the average feature maps of AUs in the same part of the face have more similar patterns. For example, AU2 to AU9 are action units around eyes and nose at the upper part of a face, while AU12 to AU27 are around the mouth and chin at the lower part of face. The distribution from AU2 to AU9 are centralized around left sides in the average feature maps, which represent the lower numbers of the geodesic sensor net. AU12 to AU27 are located more in the right and middle area, meaning that the higher number of geodesic sensors are more active. Moreover, AU12 to AU25 have more activities in high frequency bands around 80Hz to 100 Hz.\n\nIn addition, in order to show the dissimilarity of any two AUs corresponding to the two feature maps, we calculate the independent (uncorrelated) coefficient of each pair of the feature maps. It is clearly shown in Table  VI , that the 5 feature maps correspond to the 5 AUs of the upper faces have the most dissimilarity with the 5 feature maps of AUs of the lower part of the face. For example, AU1 (Inner Brow Raiser) is most independent with AU12 (Lip Corner Puller) (e.g., 47.9%) but is most correlated with AU2 (Outer Brow Raiser) (e.g., 1.8%).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Session 3:Spontaneous Emotion Recognition",
      "text": "We applied a binary emotion classification for recognizing pain versus meditation. Using leave-one-out cross-validation in linear SVM, the average accuracies of EEG Feature A, Feature B and Feature C are 88.2%, 91.7%, and 91.9%, respectively. The results are 89.2%, 91.8% and 92.3% when we apply the 4 fold cross-validation. Meanwhile, through training a CNN model on the face expression images, we get accuracies of leave-one-out cross-validation and the 4 fold cross-validation with 85.2% and 78.4%.\n\nTable  VII  shows the performance of single modal features and two-modal fused features for spontaneous emotion recognition using DANN. In this part, it shows that EEG Feature B and Feature C performs generally better than EEG Feature A because the high-frequency EEG signals (over 50HZ) are included in the feature maps. We have consistent results with posed facial expression recognition showing that high-frequency EEG signals also contribute to the spontaneous emotion recognition. However, we have split results about the performance of two-modal fused features in leave-one-out cross-validation and 4 fold cross-validation (see Table  VII ). This may caused by the conflict property of spontaneous emotion, which means participant yields some composite emotion to confuse our model. Thus, further research needs to be designed to verify our assumption in spontaneous emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusions And Future Works",
      "text": "In this paper, we present a new EEG-based multi-modal emotion database with posed expressions, action units, and spontaneous emotions. We show the strong correlation of AUs and EEGs, and thus have applied the EEGs for AU representation. The validation result shows that the peripheral information e.g., EOG-like and EMG-like artifacts can be used as complementary features for benefiting both posed facial expression and spontaneous emotion analysis. Our validation experiments shows that the two-modality feature fusion performs better than the single-modality feature alone in terms of the facial expression classification when facial movements are not trivial.\n\nThis work gives rise to a new investigation on how to utilize EEG signal frequency to correlate the facial behavior and emotion, with an attempt to improve the emotion analysis. Our future work will expand the data size to a larger scale, and will conduct EEG based AU detection and EEGexpression based fusion for AU detection. All the data will be made available to the research community.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Experiment scene and EEG electrodes location",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the data collection at work,",
      "page": 2
    },
    {
      "caption": "Figure 2: Protocol of the data acquisition (Procedure of experiment).",
      "page": 3
    },
    {
      "caption": "Figure 2: 1) In the ﬁrst session, a video illustrating six prototypi-",
      "page": 3
    },
    {
      "caption": "Figure 3: It shows the",
      "page": 3
    },
    {
      "caption": "Figure 3: Examples of EEG signal along with expression, action",
      "page": 3
    },
    {
      "caption": "Figure 4: shows a example",
      "page": 4
    },
    {
      "caption": "Figure 4: EEG data processing, feature extraction and expres-",
      "page": 5
    },
    {
      "caption": "Figure 5: The t-SNE visualization. The ﬁrst shows the data distributions of static facial expression feature in 4 fold cross-",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the",
      "page": 6
    },
    {
      "caption": "Figure 6: shows average feature map of each AU and the corresponding",
      "page": 7
    },
    {
      "caption": "Figure 6: Feature map and Histogram of the Feature C from the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Databases for Facial Expressions Analysis": "Name"
        },
        {
          "Databases for Facial Expressions Analysis": "CK [12] and CK+ [16]"
        },
        {
          "Databases for Facial Expressions Analysis": "DISFA [18]"
        },
        {
          "Databases for Facial Expressions Analysis": "MMI\n[21]"
        },
        {
          "Databases for Facial Expressions Analysis": "BP4D+ [32]"
        },
        {
          "Databases for Facial Expressions Analysis": "Databases for Affective Analysis"
        },
        {
          "Databases for Facial Expressions Analysis": "SEED [5]"
        },
        {
          "Databases for Facial Expressions Analysis": "SEED IV [35]"
        },
        {
          "Databases for Facial Expressions Analysis": "MAHNOB HCI\n[25]"
        },
        {
          "Databases for Facial Expressions Analysis": "DEAP [13]"
        },
        {
          "Databases for Facial Expressions Analysis": "Ours"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ethnicity": "Asian",
          "Participant Number": "22",
          "Proportion": "75.9%"
        },
        {
          "Ethnicity": "White",
          "Participant Number": "2",
          "Proportion": "6.9%"
        },
        {
          "Ethnicity": "Mid-Eastern",
          "Participant Number": "4",
          "Proportion": "13.8%"
        },
        {
          "Ethnicity": "Other",
          "Participant Number": "1",
          "Proportion": "3.45%"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Map\nType": "Feature Map\nSize",
          "Feature A": "5×128",
          "Feature B": "7×128",
          "Feature C": "100×128"
        },
        {
          "Feature Map\nType": "Description",
          "Feature A": "5 frequency\nbands (0.1∼4\nHz, 4∼8 Hz,\n8∼14 Hz,\n14∼31 Hz,\n31∼50 Hz)",
          "Feature B": "7 frequency\nbands (0.1∼4\nHz, 4∼8,\n8∼14 Hz,\n14∼31 Hz,\n31∼50 Hz,\n50∼75 Hz,\n75∼100 Hz)",
          "Feature C": "100 frequency\nbands (0.1∼1\nHz, 1∼2 Hz,\n2∼3\nHz...,99∼100\nHz)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluation\nmethod": "LOOCV",
          "Evaluation\ncriteria": "ACC",
          "EEG Fea-\nture A": "77.12%",
          "EEG Fea-\nture B": "80.51%",
          "EEG Fea-\nture C": "82.82%",
          "Facial\nexpression\nimage": "88.15%",
          "Fusion\nFeature A": "67.32%",
          "Fusion\nFeature B": "85.96%",
          "Fusion\nFeature C": "95.02%"
        },
        {
          "Evaluation\nmethod": "",
          "Evaluation\ncriteria": "STD",
          "EEG Fea-\nture A": "0.1146",
          "EEG Fea-\nture B": "0.1484",
          "EEG Fea-\nture C": "0.1255",
          "Facial\nexpression\nimage": "0.0757",
          "Fusion\nFeature A": "0.1719",
          "Fusion\nFeature B": "0.0909",
          "Fusion\nFeature C": "0.0546"
        },
        {
          "Evaluation\nmethod": "4 fold CV",
          "Evaluation\ncriteria": "ACC",
          "EEG Fea-\nture A": "61.85%",
          "EEG Fea-\nture B": "66.98%",
          "EEG Fea-\nture C": "69.68%",
          "Facial\nexpression\nimage": "72.85%",
          "Fusion\nFeature A": "74.28%",
          "Fusion\nFeature B": "72.59%",
          "Fusion\nFeature C": "76.68%"
        },
        {
          "Evaluation\nmethod": "",
          "Evaluation\ncriteria": "STD",
          "EEG Fea-\nture A": "0.0689",
          "EEG Fea-\nture B": "0.0749",
          "EEG Fea-\nture C": "0.0819",
          "Facial\nexpression\nimage": "0.0837",
          "Fusion\nFeature A": "0.0521",
          "Fusion\nFeature B": "0.0759",
          "Fusion\nFeature C": "0.0769"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral": "84.51%",
          "Sadness": "5.94%",
          "Fear": "1.12%",
          "Happy": "0.88%",
          "Anger": "2.33%",
          "Disgust": "0.00%",
          "Surprise": "5.22%"
        },
        {
          "Neutral": "0.71%",
          "Sadness": "71.71%",
          "Fear": "3.00%",
          "Happy": "1.12%",
          "Anger": "17.88%",
          "Disgust": "1.12%",
          "Surprise": "4.47%"
        },
        {
          "Neutral": "0.42%",
          "Sadness": "3.19%",
          "Fear": "65.37%",
          "Happy": "6.94%",
          "Anger": "4.53%",
          "Disgust": "4.18%",
          "Surprise": "15.37%"
        },
        {
          "Neutral": "0.11%",
          "Sadness": "2.56%",
          "Fear": "8.35%",
          "Happy": "86.12%",
          "Anger": "2.21%",
          "Disgust": "0.00%",
          "Surprise": "0.65%"
        },
        {
          "Neutral": "0.99%",
          "Sadness": "11.82%",
          "Fear": "2.31%",
          "Happy": "0.92%",
          "Anger": "67.99%",
          "Disgust": "8.84%",
          "Surprise": "7.13%"
        },
        {
          "Neutral": "0.58%",
          "Sadness": "4.67%",
          "Fear": "4.90%",
          "Happy": "6.57%",
          "Anger": "15.74%",
          "Disgust": "64.42%",
          "Surprise": "3.11%"
        },
        {
          "Neutral": "0.36%",
          "Sadness": "3.17%",
          "Fear": "9.43%",
          "Happy": "0.04%",
          "Anger": "2.21%",
          "Disgust": "0.72%",
          "Surprise": "84.07%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AU1": "0.0%",
          "AU2": "1.8%",
          "AU4": "7.1%",
          "AU5": "5.7%",
          "AU9": "10.4%",
          "AU12": "47.9%",
          "AU15": "38.5%",
          "AU17": "34.4%",
          "AU25": "27.9%",
          "AU27": "39.4%"
        },
        {
          "AU1": "1.8%",
          "AU2": "0.0%",
          "AU4": "7.1%",
          "AU5": "3.4%",
          "AU9": "9.9%",
          "AU12": "38.4%",
          "AU15": "29.5%",
          "AU17": "26.0%",
          "AU25": "21.5%",
          "AU27": "30.7%"
        },
        {
          "AU1": "7.1%",
          "AU2": "7.1%",
          "AU4": "0.0%",
          "AU5": "5.1%",
          "AU9": "3.0%",
          "AU12": "39.5%",
          "AU15": "30.4%",
          "AU17": "26.0%",
          "AU25": "18.4%",
          "AU27": "33.6%"
        },
        {
          "AU1": "5.7%",
          "AU2": "3.4%",
          "AU4": "5.1%",
          "AU5": "0.0%",
          "AU9": "7.3%",
          "AU12": "29.9%",
          "AU15": "20.9%",
          "AU17": "17.1%",
          "AU25": "11.1%",
          "AU27": "22.8%"
        },
        {
          "AU1": "10.4%",
          "AU2": "9.9%",
          "AU4": "3.0%",
          "AU5": "7.3%",
          "AU9": "0.0%",
          "AU12": "32.0%",
          "AU15": "24.7%",
          "AU17": "21.6%",
          "AU25": "16.0%",
          "AU27": "28.2%"
        },
        {
          "AU1": "47.9%",
          "AU2": "38.4%",
          "AU4": "39.5%",
          "AU5": "29.9%",
          "AU9": "32.0%",
          "AU12": "0.0%",
          "AU15": "2.5%",
          "AU17": "4.4%",
          "AU25": "12.9%",
          "AU27": "13.6%"
        },
        {
          "AU1": "38.5%",
          "AU2": "29.5%",
          "AU4": "30.4%",
          "AU5": "20.9%",
          "AU9": "24.7%",
          "AU12": "2.5%",
          "AU15": "0.0%",
          "AU17": "1.2%",
          "AU25": "6.9%",
          "AU27": "8.4%"
        },
        {
          "AU1": "34.4%",
          "AU2": "26.0%",
          "AU4": "26.0%",
          "AU5": "17.1%",
          "AU9": "21.6%",
          "AU12": "4.4%",
          "AU15": "1.2%",
          "AU17": "0.0%",
          "AU25": "4.3%",
          "AU27": "8.3%"
        },
        {
          "AU1": "27.9%",
          "AU2": "21.5%",
          "AU4": "18.4%",
          "AU5": "11.1%",
          "AU9": "16.0%",
          "AU12": "12.9%",
          "AU15": "6.9%",
          "AU17": "4.3%",
          "AU25": "0.0%",
          "AU27": "9.7%"
        },
        {
          "AU1": "39.4%",
          "AU2": "30.7%",
          "AU4": "33.6%",
          "AU5": "22.8%",
          "AU9": "28.2%",
          "AU12": "13.6%",
          "AU15": "8.4%",
          "AU17": "8.3%",
          "AU25": "9.7%",
          "AU27": "0.0"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluation\nmethod": "LOOCV",
          "Evaluation\ncriteria": "ACC",
          "EEG Fea-\nture A": "94.24%",
          "EEG Fea-\nture B": "95.69%",
          "EEG Fea-\nture C": "97.15%",
          "Facial\nexpression\nimage": "92.13%",
          "Fusion\nFeature A": "93.38%",
          "Fusion\nfeature B": "92.90%",
          "Fusion\nfeature C": "98.60%"
        },
        {
          "Evaluation\nmethod": "",
          "Evaluation\ncriteria": "STD",
          "EEG Fea-\nture A": "0.0959",
          "EEG Fea-\nture B": "0.1013",
          "EEG Fea-\nture C": "0.0860",
          "Facial\nexpression\nimage": "0.0963",
          "Fusion\nFeature A": "0.0890",
          "Fusion\nfeature B": "0.1009",
          "Fusion\nfeature C": "0.0481"
        },
        {
          "Evaluation\nmethod": "4 fold CV",
          "Evaluation\ncriteria": "ACC",
          "EEG Fea-\nture A": "91.71%",
          "EEG Fea-\nture B": "94.28%",
          "EEG Fea-\nture C": "90.54%",
          "Facial\nexpression\nimage": "86.90%",
          "Fusion\nFeature A": "87.02%",
          "Fusion\nfeature B": "92.44%",
          "Fusion\nfeature C": "91.00%"
        },
        {
          "Evaluation\nmethod": "",
          "Evaluation\ncriteria": "STD",
          "EEG Fea-\nture A": "0.0738",
          "EEG Fea-\nture B": "0.0443",
          "EEG Fea-\nture C": "0.0424",
          "Facial\nexpression\nimage": "0.0984",
          "Fusion\nFeature A": "0.1028",
          "Fusion\nfeature B": "0.0663",
          "Fusion\nfeature C": "0.0637"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Human emotion recognition and analysis in response to audio music using brain signals",
      "authors": [
        "A Bhatti",
        "M Majid",
        "S Anwar",
        "B Khan"
      ],
      "year": "2016",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "3",
      "title": "Survey on RGB, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Simon"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
      "authors": [
        "J Donahue",
        "T Darrell"
      ],
      "year": "2013",
      "venue": "Decaf: A deep convolutional activation feature for generic visual recognition"
    },
    {
      "citation_id": "5",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R Duan",
        "B Lu"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Neural Engineering"
    },
    {
      "citation_id": "6",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "7",
      "title": "EMG and EOG artifacts in brain computer interface systems: A survey",
      "authors": [
        "M Fatourechi",
        "A Bashashati",
        "R Ward",
        "G Birch"
      ],
      "year": "2007",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "8",
      "title": "EMG contamination of EEG: spectral and topographical characteristics",
      "authors": [
        "I Goncharova",
        "D Mcfarland",
        "T Vaughan",
        "J Wolpaw"
      ],
      "year": "2003",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "9",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair"
      ],
      "year": "2014",
      "venue": "Generative adversarial networks"
    },
    {
      "citation_id": "10",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee"
      ],
      "year": "2015",
      "venue": "IEEE ICCV"
    },
    {
      "citation_id": "12",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade"
      ],
      "year": "2000",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "13",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2012",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C.-M Kuo"
      ],
      "year": "2018",
      "venue": "IEEE CVPRW"
    },
    {
      "citation_id": "15",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "16",
      "title": "The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE CVPRW"
    },
    {
      "citation_id": "17",
      "title": "Slow-frequency pulsed transcranial electrical stimulation for modulation of cortical plasticity based on reciprocity targeting with precision electrical head modeling",
      "authors": [
        "P Luu",
        "D Tucker"
      ],
      "year": "2016",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "A novel feature extraction method based on late positive potential for emotion recognition in human brain signal patterns",
      "authors": [
        "R Mehmood",
        "H Lee"
      ],
      "year": "2016",
      "venue": "Computers & Electrical Engineering"
    },
    {
      "citation_id": "20",
      "title": "High-frequency brain activity and muscle artifacts in MEG/EEG: a review and recommendations",
      "authors": [
        "S Muthukumaraswamy"
      ],
      "year": "2013",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "22",
      "title": "Solutions to the linear smoothing problem",
      "authors": [
        "H Rauch"
      ],
      "year": "1963",
      "venue": "IEEE Transactions on Automatic Control"
    },
    {
      "citation_id": "23",
      "title": "Psychology. Macmillan Education UK",
      "authors": [
        "D Schacter",
        "D Gilbert",
        "D Wegner",
        "B Hood"
      ],
      "year": "2016",
      "venue": "Psychology. Macmillan Education UK"
    },
    {
      "citation_id": "24",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer"
      ],
      "year": "2012",
      "venue": "IEEE Transaction on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Deep spatial-temporal feature fusion for facial expression recognition in static images",
      "authors": [
        "N Sun",
        "Q Li",
        "R Huan",
        "J Liu",
        "G Han"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "27",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on deap dataset",
      "authors": [
        "S Tripathi"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "28",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "G Yaroslav"
      ],
      "year": "2017",
      "venue": "Advances in Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "A high-resolution 3d dynamic facial expression database",
      "authors": [
        "L Yin",
        "X Chen",
        "Y Sun",
        "T Worm",
        "M Reale"
      ],
      "year": "2008",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "31",
      "title": "spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "32",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang",
        "J Cohn",
        "Q Ji",
        "L Yin"
      ],
      "year": "2016",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "33",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "34",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "X Zhao",
        "X Liang"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "35",
      "title": "EmotionMeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "36",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on AMD"
    }
  ]
}