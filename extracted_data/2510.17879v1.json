{
  "paper_id": "2510.17879v1",
  "title": "Decoding Listener'S Identity: Person Identification From Eeg Signals Using A Lightweight Spiking Transformer",
  "published": "2025-10-17T08:20:01Z",
  "authors": [
    "Zheyuan Lin",
    "Siqi Cai",
    "Haizhou Li"
  ],
  "keywords": [
    "Brain-computer interface",
    "EEG",
    "Person identification",
    "Spiking neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Electroencephalography (EEG) serves as a non-invasive method for monitoring and interpreting brain activity, offering insights into cognitive states and enabling the development of brain-computer interfaces (BCIs)  [1] . Within this domain, EEG-based person identification has emerged as a critical challenge, aiming to recognize individuals through their unique neurophysiological signatures  [2] . This capability is crucial for personalized BCI applications, where robust user-specific adaptation is required.\n\nNeuroscience research suggests that auditory stimuli can evoke individualized neural responses  [3] .\n\nStudies on song identification, stimulus-response relationships, and intersubject variability during music listening  [4, 5]  have demonstrated distinct neural patterns influenced by subjective experiences. These findings support the feasibility of identifying listeners based on EEG signals recorded during music engagement.\n\nTraditional machine learning approaches, particularly artificial neural networks (ANNs), have been extensively employed for EEG-based person identification  [6, 7] . For instance, convolutional neural networks (CNNs) have demonstrated success in user identification tasks due to their ability to extract spatial features from high-dimensional EEG data  [8] . A large-scale study  [9]  analyzed EEG recordings from 109 subjects using a multilayer fully connected neural network, achieving competitive classification accuracy across all subjects. However, these approaches often rely on handcrafted features (e.g., statistical, frequency, or wavelet descriptors), which limits generalizabil-ity. Additionally, ANNs typically require substantial computational resources and may not fully capture the temporal dynamics inherent in EEG signals.\n\nSpiking Neural Networks (SNNs), inspired by the brain's neural architecture, offer a promising alternative due to their event-driven nature and potential for energy-efficient processing. Previous studies have demonstrated the effectiveness of SNNs in EEG classification tasks, which show higher performance with less energy consumption than conventional ANNs  [10, 11] . Recent advances in SNNs include models like the Spike-driven Transformer  [12] , which integrates transformer mechanisms with spiking neurons to efficiently process sequential data and achieve state-of-the-art performance across various tasks.\n\nThis study investigates SNN-based person identification using EEG recordings collected during music listening-a challenging benchmark task that demands biometric identification across different individuals. To our knowledge, this represents the first exploration of SNNs for EEG-based person identification. We adopt a lightweight spiking transformer, an endto-end architecture optimized for efficiency and performance. This architecture is designed to efficiently process temporal sequences, making it particularly suitable for handling EEG data, which inherently involves complex temporal dynamics. The results demonstrate SNNs' potential as a robust tool for biometric recognition, advancing the development of more personalized and adaptive BCI systems.\n\nThe remainder of this paper is organized as follows: Section 2 outlines the methodology, detailing the spiking transformer architecture for EEG analysis. Section 3 describes the experimental setup, including the EEG dataset specifications, data preprocessing, and training protocol. Results are reported in Section 4. Section 5 discusses the implications of our findings and concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "As illustrated in Figure  1 , the proposed lightweight spiking transformer takes a window of EEG signals as input and performs person identification through multi-class classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Spiking Neurons",
      "text": "The core of SNNs is built on spiking neurons, which simulate the behavior of biological neurons more closely than traditional artificial neurons  [13] . The spiking neuron layer adopted in our model follows the standard Leaky Integrate-and-Fire (LIF) neuron model  [14] . It integrates temporal and spatial inputs; its membrane potential inherently performs temporal integration, crucial for capturing the dynamics of EEG data. Spikes are generated based on this potential, driving computations forward. Its synergy can be represented by:\n\nThe membrane potential V of each neuron is updated based on the previous potential, the weighted sum of incoming spikes, and a reset term. β represents the leakage factor, wj are the synaptic weights, and S in j [n] denotes the incoming spikes to the neuron at time step n. V th denotes the firing threshold. Reset  [n]  ensures that the neuron's voltage is set to a lower value after firing. A spike is emitted when the membrane potential exceeds a threshold, at which point an output spike S out [n+1] is activated. This is controlled by θ, the Heaviside step function.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overall Model",
      "text": "As illustrated in Figure  1 , we adopt a lightweight spiking transformer for EEG-based person identification. This architecture is based on the established designs of Spike-driven Transformer V2  [12]  and Spikformer  [15] . The model combines both Convbased and Transformer-based SNN blocks to enhance performance and versatility, drawing inspiration from both traditional CNN and Vision Transformer (ViT)  [16]  architectures. The first two stages use Conv-based SNN blocks, while the last two leverage Transformer-based SNN blocks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conv-Based Snn Block",
      "text": "Separable Convolution (SepConv) is used to extract features from each channel and merge features across channels in the Conv-based SNN blocks. It combines depthwise convolution (kernel size 7x7, stride 1), which performs convolution independently for each input channel, and pointwise convolution, which merges the channels:\n\n(3) where E represents a window of EEG signal E ∈ R C×T , with C denoting the number of EEG channels and T representing the Channel Convolution combines spatial and channel-wise information, allowing the model to learn deeper, more abstract features. It applies standard convolutions (kernel 3x3, stride 1) to process the output from the SepConv layer, ensuring that spatial patterns are effectively captured:\n\nAfter the two convolution-based SNN blocks, the input EEG E is transformed into the feature map feature F.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transformer-Based Snn Block",
      "text": "Spike-driven self-attention (SDSA) replaces the traditional Query (Q), Key (K), and Value (V) matrix operations used in standard attention mechanisms, as illustrated in Figure  2 . It operates on the spike tensor form Q, K, and V directly, and computes interactions between tokens using sparse additions instead of the dense, computationally expensive dot products found in standard self-attention:\n\nwhere RepConv(•) is re-parameterization convolution (kernel size 3x3, stride 1)  [17] . Unlike conventional Transformers, this avoids expensive operations like softmax and scaling, as spikeform matrix multiplication can be converted to additions using addressing algorithms  [18] , making it energy-efficient. Following the SDSA operation, a two-layer MLP is applied to the output. It operates on the channel dimension of the feature maps, improving the model's ability to understand the interactions between different features across channels:\n\nwhere W1 ∈ R C×rC and W2 ∈ R rC×C are learnable parameters with MLP expansion ratio r = 4.\n\nAfter passing through the two Transformer-based SNN blocks, the feature map F was further modified as G, which serves as the refined feature representation.\n\nSubsequently, G is processed by a linear classification layer, producing the final prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Parameter Reduction",
      "text": "To enhance efficiency in EEG-based person identification, we streamlined our model by reducing channel dimensions and optimizing downsampling parameters. Compared to the Spikedriven Transformer V2 baseline  [12] , which contains 30.9 million parameters, our approach achieves computational feasibility with just 3.91M parameters while preserving discriminative EEG feature learning and maintaining competitive performance.\n\nOverall, this novel architecture, combining the strengths of spiking neural networks with transformer-based attention mechanisms, enables efficient and accurate processing of EEG signals for person identification tasks. The integration of spiking neurons allows the model to process temporal information in a biologically plausible manner, while the transformer mechanism ensures that long-range dependencies within the data are captured effectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "This study utilizes the EEG-Music Emotion Recognition Challenge dataset 1  for the task of Person Identification-determining the identities of 26 subjects based on EEG responses elicited by musical stimuli.\n\nEEG signals were recorded in a controlled laboratory environment using a 32-channel EPOC Flex system with saline sensors, with data sampled at 128 Hz. Electrode placements followed the international 10-20 system. Each subject participated in 16 trials lasting 90 seconds each. The trials were divided into two categories: personal trials, selected based on the subject's own music preferences, and non-personal trials, chosen randomly from the playlists of other participants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Preprocessing",
      "text": "The EEG signals utilized in this study were obtained from the official EEG-Music Emotion Recognition Challenge dataset, which were underwent preprocessing using an FIR filter (0.5-40 Hz) and artifact removal through Independent Component Analysis (ICA). Moreover, the EEG signals were normalized for each trial within each subject. Each raw EEG trial, approximately 90 seconds in length, was segmented into 10-second windows, each containing 1280 time points.\n\nThe training dataset comprises 294 trials collected from 26 subjects, with each subject contributing roughly 12 labeled trials. These trials are used to train models to identify subjects based on their distinct EEG signatures. For validation, a set was extracted from the training data (10%), ensuring a diverse and representative sample to evaluate model performance during training.\n\nThe test dataset consists of 104 held-out trials, all coming from the 26 subjects seen during training. Additionally, 44 extra trials are included in the test set, some of which lack stimulus information or self-assessed emotion labels. This additional data further challenges the model, testing its robustness and generalization capabilities. Person identification results are reported on the testing set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Protocol",
      "text": "The training of the proposed lightweight spiking transformer was conducted over 300 epochs using surrogate gradient  [19] , which facilitates backpropagation through the non-differentiable spiking activity inherent in SNNs. This approach enables efficient learning while preserving the temporal characteristics of the EEG data.\n\nFor the loss function, we employed a CrossEntropyLoss with class count balancing to mitigate any potential class imbalances within the dataset. This ensured that the model did not favor any particular subject during training, maintaining fairness and robustness in classification.\n\nOptimization was carried out using the Adam optimizer, chosen for its adaptive learning rate capabilities and overall efficiency in handling sparse gradients. In addition, a ReduceL-ROnPlateau learning rate scheduler was integrated into the training process. This scheduler dynamically adjusts the learning rate when the validation performance plateaued, thereby enhancing convergence and preventing overfitting.\n\nTogether, these components of the training protocol-surrogate gradient-based learning, balanced CrossEn-tropyLoss, Adam optimization, and adaptive learning rate scheduling-enabled robust and efficient training of the SNN model, culminating in high-performance EEG-based person identification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Person Identification Accuracy",
      "text": "The performance of the models was evaluated using the EEG-Music Emotion Recognition Challenge dataset, with the primary objective of accurately identifying the subject based on EEG signals. In this study, we assessed our proposed lightweight spiking transformer and compared its performance against several other models, including conventional ANNs like the Compact Convolutional Transformer, as well as SNNs such as the Spike-driven Transformer V2  [12] .\n\nCompact Convolutional Transformer: CCT model has demonstrated superior performance in a variety of tasks, outperforming many contemporary transformer models  [20] . In this study, we incorporate the CCT model in our experimental setup and further reduce its size to align with the scale of our proposed lightweight transformer.\n\nSpike-driven Transformer V2: Spike-driven Transformer V2 combines transformer-based architecture with spike-driven processing, improving computational efficiency while maintaining high accuracy across various vision tasks  [12] . We compared its performance to that of our proposed lightweight spiking transformer to evaluate both efficiency and effectiveness in the task of person identification.\n\nAs summarized in Table  1 , our lightweight spiking transformer achieved 100% accuracy in the person identification task. Compared to Spike-driven Transformer V2, which also reached 100% accuracy, our model significantly reduces the parameter count from 30.9 million to 3.91 million-a nearly 90% reduction in model size-while maintaining the same level of performance.\n\nIn contrast, both variants of the CCT exhibited lower accuracy. The smaller CCT model, despite having a similar parameter count to ours, achieved only 98.65%. This result further underscores the effectiveness of SNNs in balancing computational efficiency and classification accuracy. 1 CCT model with reduced model size.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Efficiency",
      "text": "We further compare the computational costs of different models, evaluating their inference cost for a single forward pass. The inference cost is measured by the number of floating-point operations per second (FLOPs), based on the standard 45nm CMOS process  [21, 22] . Notably, for SNNs, neurons are activated only when the input spikes surpass a threshold, allowing inactive neurons to enter low-power modes, thus are expected to reduce power consumption.\n\nAs shown in Table  1 , our proposed lightweight spiking transformer, with 3.91 million parameters, demonstrates remarkable energy efficiency, consuming only 760.7 µJ during inference. This low energy consumption can be attributed to the inherent efficiency of our SNN design, which simplifies traditional multiply-accumulate (MAC) operations into more energy-efficient accumulation (AC) operations. Moreover, the SNN operates with an average firing rate of just 7.18%, meaning that only a small fraction of the network's spiking neurons are active at any given time, further minimizing both computational load and energy usage.\n\nIn contrast, the energy consumption of the CCT model was significantly higher than that of the SNN transformer models, with the 22.0 million parameter version consuming 38,414.6 µJ per forward pass. Even the smaller 3.24 million parameter variant, while more efficient, still required 8,248.3 µJ-substantially more than the SNN-based models. Notably, our model consumed only about 10% of this energy, highlighting its superior efficiency.\n\nIn summary, our proposed model distinguishes itself through its exceptional energy efficiency. Its low power consumption, enabled by streamlined computational operations and minimal neuronal firing rates, makes it well-suited for real-time, energy-constrained applications such as BCIs, where both high performance and low energy usage are essential for effective deployment.\n\nThe optimized SNN transformer model stands out for its superior energy efficiency. Its low energy consumption, driven by simplified computational operations and low neuronal firing rates, makes it a highly attractive option for real-time, energy-constrained applications such as brain-computer interfaces, where both high performance and low power consumption are crucial.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the impact of architectural modifications on the spiking transformer, we conducted an ablation study focusing on two critical aspects: (1) downsampling strategy and (2) the role of Conv-based SNN blocks.\n\nDownsampling Strategy and Parameter Efficiency: Compared to Spike-driven Transformer V2, one of the key modifications in our lightweight spiking transformer was the selection of smaller downsampling dimensions. This adjustment reduced the model size from 30.9 million to 3.91 million pa-rameters, significantly lowering computational and energy costs while maintaining 100% accuracy. The results emphasize the effectiveness of downsampling and parameter pruning in creating a lightweight yet highly accurate model with improved efficiency. Further reductions in downsampling dimensions were explored but led to severe accuracy degradation, indicating that the 3.91 million parameter configuration achieves an optimal balance between efficiency and performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Impact Of Conv-Based Snn Blocks:",
      "text": "To evaluate the necessity of the dual Conv-based SNN blocks in the original architecture, we tested a variant with only one Conv-based block. This further reduced the model size to 2.85 million parameters (a 27% decrease from the 3.91M version) but resulted in a 2.03% drop in accuracy. These findings highlight the role of Conv-based SNN blocks in capturing hierarchical temporal features and demonstrate the trade-off between extreme parameter reduction and performance retention.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "SNNs offer distinct advantages for EEG-based classification tasks due to their ability to capture the temporal dynamics inherent in neural signals. EEG data is rich with time-dependent patterns, and SNNs, which process information through eventdriven, spike-based mechanisms, are particularly well-suited to model these temporal dependencies. This alignment enables SNNs to effectively represent the brain's intricate activity over time, leading to enhanced accuracy in classifying EEG signals.\n\nAnother significant benefit of SNNs is their efficiency. The lightweight SNN transformer model in this study achieved high performance while significantly reducing computational and energy costs compared to analogous ANN models. This efficiency is critical for deploying EEG-based systems on edge devices, such as wearables or speech processors, where power consumption and real-time processing are key constraints. The low firing rate of spiking neurons, coupled with simplified operations, makes SNNs particularly suitable for deployment in resource-constrained environments, ensuring that they can operate effectively in portable and energy-efficient settings. However, despite these promising advantages, several limitations must be considered. While the model performed well on the 26-subject dataset used in this study, its generalizability to larger and more diverse populations remains uncertain. To fully assess the robustness and scalability of the model, further testing with more extensive datasets that incorporate a wider range of demographic and physiological variability is needed. Additionally, future research should investigate the adaptability of SNNs to other EEG-based applications and explore their performance in environments with varying noise levels and signal quality. This would provide a more comprehensive understanding of the potential of SNNs in real-world EEG applications.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed lightweight spiking",
      "page": 1
    },
    {
      "caption": "Figure 1: Architecture overview of the proposed lightweight",
      "page": 2
    },
    {
      "caption": "Figure 1: , we adopt a lightweight spiking trans-",
      "page": 2
    },
    {
      "caption": "Figure 2: Outline of the Spike-Driven Self-Attention (SDSA)",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3Department of ECE, National University of Singapore, Singapore": "caisiqi@ieee.org,\nhaizhouli@cuhk.edu.cn"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ity. Additionally, ANNs typically require substantial computa-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "tional resources and may not fully capture the temporal dynam-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ics inherent in EEG signals."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "Spiking Neural Networks (SNNs),\ninspired by the brain’s"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "neural architecture, offer a promising alternative due to their"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "event-driven nature and potential\nfor energy-efficient process-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ing.\nPrevious studies have demonstrated the effectiveness of"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "SNNs\nin EEG classification tasks, which show higher per-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "formance with\nless\nenergy\nconsumption\nthan\nconventional"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ANNs\n[10, 11].\nRecent\nadvances\nin SNNs\ninclude models"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "like the Spike-driven Transformer [12], which integrates trans-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "former mechanisms with spiking neurons to efficiently process"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "sequential data and achieve state-of-the-art performance across"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "various tasks."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "This study investigates SNN-based person identification us-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ing EEG recordings collected during music listening—a chal-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "lenging benchmark task that demands biometric identification"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "across different\nindividuals. To our knowledge,\nthis represents"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "the first exploration of SNNs\nfor EEG-based person identifi-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "cation. We adopt a lightweight\nspiking transformer, an end-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "to-end architecture optimized for efficiency and performance."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "This architecture is designed to efficiently process temporal se-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "quences, making it particularly suitable for handling EEG data,"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "which inherently involves complex temporal dynamics. The re-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "sults demonstrate SNNs’ potential as a robust tool for biometric"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "recognition, advancing the development of more personalized"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "and adaptive BCI systems."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "The remainder of this paper is organized as follows: Section"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "2 outlines the methodology, detailing the spiking transformer"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "architecture for EEG analysis.\nSection 3 describes the exper-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "imental\nsetup,\nincluding the EEG dataset\nspecifications, data"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "preprocessing, and training protocol.\nResults are reported in"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "Section 4. Section 5 discusses the implications of our findings"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "and concludes the paper."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "2. Methodology"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "As\nillustrated in Figure 1,\nthe proposed lightweight\nspiking"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "transformer\ntakes a window of EEG signals as input and per-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "forms person identification through multi-class classification."
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "2.1.\nSpiking Neurons"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": ""
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "The core of SNNs is built on spiking neurons, which simulate"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "the behavior of biological neurons more closely than traditional"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "artificial neurons [13]. The spiking neuron layer adopted in our"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "model follows the standard Leaky Integrate-and-Fire (LIF) neu-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "ron model\n[14].\nIt\nintegrates temporal and spatial\ninputs;\nits"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "membrane potential\ninherently performs temporal\nintegration,"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "crucial for capturing the dynamics of EEG data. Spikes are gen-"
        },
        {
          "3Department of ECE, National University of Singapore, Singapore": "erated based on this potential, driving computations forward. Its"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "block, RepConv stands for re-parameterization convolution."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "samples within a window. SN (·) represents a layer of LIF neu-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "rons as in Section 2.1. ConvPW and ConvDW denote the point-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "wise and depthwise convolution operations, respectively."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Channel Convolution combines\nspatial and channel-wise"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "information, allowing the model\nto learn deeper, more abstract"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "features.\nIt applies standard convolutions (kernel 3x3, stride 1)"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "to process the output from the SepConv layer, ensuring that spa-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "tial patterns are effectively captured:"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "ChannelConv(E′′) = Conv (cid:0)SN (cid:0)Conv (cid:0)SN(E′)(cid:1)(cid:1)(cid:1)\n(4)"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "After\nthe\ntwo convolution-based SNN blocks,\nthe\ninput"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "EEG E is transformed into the feature map feature F."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "2.2.3.\nTransformer-based SNN block"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Spike-driven\nself-attention\n(SDSA)\nreplaces\nthe\ntraditional"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Query (Q), Key (K), and Value (V) matrix operations used in"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "standard attention mechanisms, as illustrated in Figure 2. It op-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "erates on the spike tensor form Q, K, and V directly, and com-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "putes interactions between tokens using sparse additions instead"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "of the dense, computationally expensive dot products found in"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "standard self-attention:"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Qs = SN(RepConv(F))"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "(5)\nKs = SN(RepConv(F))"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Vs = SN(RepConv(F))"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "(6)\ns Vs)))\nSDSA(F) = RepConv(SN(Qs(KT"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "where RepConv(·) is re-parameterization convolution (kernel"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "size 3x3, stride 1) [17]. Unlike conventional Transformers, this"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "avoids expensive operations like softmax and scaling, as spike-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "form matrix multiplication can be converted to additions using"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "addressing algorithms [18], making it energy-efficient."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Following the SDSA operation, a two-layer MLP is applied"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "to the output. It operates on the channel dimension of the feature"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "maps,\nimproving the model’s ability to understand the interac-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "tions between different features across channels:"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "(7)\nMLP(F′) = SN(SN(F′)W1)W2"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "where W1 ∈ RC×rC and W2 ∈ RrC×C are learnable parame-"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "ters with MLP expansion ratio r = 4."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": ""
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "After\npassing\nthrough\nthe\ntwo Transformer-based SNN"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "blocks,\nthe feature map F was\nfurther modified as G, which"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "serves as the refined feature representation."
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "Subsequently, G is\nprocessed\nby\na\nlinear\nclassification"
        },
        {
          "the Spike-Driven Self-Attention (SDSA)\nFigure 2: Outline of": "layer, producing the final prediction."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , our lightweight spiking trans-",
      "data": [
        {
          "2.2.4. Parameter Reduction": "To enhance efficiency in EEG-based person identification, we",
          "3.3. Training Protocol": "The\ntraining\nof\nthe\nproposed\nlightweight\nspiking\ntrans-"
        },
        {
          "2.2.4. Parameter Reduction": "streamlined our model by reducing channel dimensions and op-",
          "3.3. Training Protocol": "former was\nconducted\nover\n300\nepochs\nusing\nsurrogate"
        },
        {
          "2.2.4. Parameter Reduction": "timizing downsampling parameters.\nCompared to the Spike-",
          "3.3. Training Protocol": "gradient[19], which\nfacilitates\nbackpropagation\nthrough\nthe"
        },
        {
          "2.2.4. Parameter Reduction": "driven Transformer V2 baseline [12], which contains 30.9 mil-",
          "3.3. Training Protocol": "non-differentiable spiking activity inherent\nin SNNs. This ap-"
        },
        {
          "2.2.4. Parameter Reduction": "lion parameters, our approach achieves computational feasibil-",
          "3.3. Training Protocol": "proach enables efficient learning while preserving the temporal"
        },
        {
          "2.2.4. Parameter Reduction": "ity with just 3.91M parameters while preserving discrimina-",
          "3.3. Training Protocol": "characteristics of the EEG data."
        },
        {
          "2.2.4. Parameter Reduction": "tive EEG feature learning and maintaining competitive perfor-",
          "3.3. Training Protocol": "For\nthe loss\nfunction, we employed a CrossEntropyLoss"
        },
        {
          "2.2.4. Parameter Reduction": "mance.",
          "3.3. Training Protocol": "with class count balancing to mitigate any potential class im-"
        },
        {
          "2.2.4. Parameter Reduction": "Overall, this novel architecture, combining the strengths of",
          "3.3. Training Protocol": "balances within the dataset. This ensured that the model did not"
        },
        {
          "2.2.4. Parameter Reduction": "spiking neural networks with transformer-based attention mech-",
          "3.3. Training Protocol": "favor any particular subject during training, maintaining fair-"
        },
        {
          "2.2.4. Parameter Reduction": "anisms, enables efficient and accurate processing of EEG sig-",
          "3.3. Training Protocol": "ness and robustness in classification."
        },
        {
          "2.2.4. Parameter Reduction": "nals for person identification tasks. The integration of spiking",
          "3.3. Training Protocol": "Optimization was carried out using the Adam optimizer,"
        },
        {
          "2.2.4. Parameter Reduction": "neurons allows the model\nto process temporal\ninformation in",
          "3.3. Training Protocol": "chosen for its adaptive learning rate capabilities and overall ef-"
        },
        {
          "2.2.4. Parameter Reduction": "a biologically plausible manner, while the transformer mecha-",
          "3.3. Training Protocol": "ficiency in handling sparse gradients.\nIn addition, a ReduceL-"
        },
        {
          "2.2.4. Parameter Reduction": "nism ensures that\nlong-range dependencies within the data are",
          "3.3. Training Protocol": "ROnPlateau\nlearning\nrate\nscheduler was\nintegrated\ninto\nthe"
        },
        {
          "2.2.4. Parameter Reduction": "captured effectively.",
          "3.3. Training Protocol": "training process. This scheduler dynamically adjusts the learn-"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "ing rate when the validation performance plateaued, thereby en-"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "hancing convergence and preventing overfitting."
        },
        {
          "2.2.4. Parameter Reduction": "3. Dataset and Experiments",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "Together,\nthese\ncomponents\nof\nthe\ntraining\nproto-"
        },
        {
          "2.2.4. Parameter Reduction": "3.1. Dataset",
          "3.3. Training Protocol": "col—surrogate\ngradient-based\nlearning,\nbalanced CrossEn-"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "tropyLoss, Adam optimization,\nand\nadaptive\nlearning\nrate"
        },
        {
          "2.2.4. Parameter Reduction": "This\nstudy\nutilizes\nthe\nEEG-Music\nEmotion\nRecogni-",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "scheduling—enabled robust and efficient\ntraining of\nthe SNN"
        },
        {
          "2.2.4. Parameter Reduction": "tion Challenge\ndataset1\nfor\nthe\ntask\nof\nPerson\nIdentifica-",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "model,\nculminating\nin\nhigh-performance EEG-based\nperson"
        },
        {
          "2.2.4. Parameter Reduction": "tion—determining the identities of 26 subjects based on EEG",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "identification."
        },
        {
          "2.2.4. Parameter Reduction": "responses elicited by musical stimuli.",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "EEG signals were recorded in a controlled laboratory en-",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "4. Results"
        },
        {
          "2.2.4. Parameter Reduction": "vironment using a 32-channel EPOC Flex system with saline",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "sensors, with data sampled at 128 Hz. Electrode placements fol-",
          "3.3. Training Protocol": "4.1. Person Identification Accuracy"
        },
        {
          "2.2.4. Parameter Reduction": "lowed the international 10-20 system. Each subject participated",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "The performance of the models was evaluated using the EEG-"
        },
        {
          "2.2.4. Parameter Reduction": "in 16 trials lasting 90 seconds each.\nThe trials were divided",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "Music Emotion Recognition Challenge dataset, with the pri-"
        },
        {
          "2.2.4. Parameter Reduction": "into two categories: personal\ntrials, selected based on the sub-",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "mary\nobjective\nof\naccurately\nidentifying\nthe\nsubject\nbased"
        },
        {
          "2.2.4. Parameter Reduction": "ject’s own music preferences, and non-personal\ntrials, chosen",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "on EEG signals.\nIn\nthis\nstudy, we\nassessed\nour\nproposed"
        },
        {
          "2.2.4. Parameter Reduction": "randomly from the playlists of other participants.",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "lightweight spiking transformer and compared its performance"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "against several other models, including conventional ANNs like"
        },
        {
          "2.2.4. Parameter Reduction": "3.2. Data Preprocessing",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "the Compact Convolutional Transformer, as well as SNNs such"
        },
        {
          "2.2.4. Parameter Reduction": "The EEG signals utilized in this study were obtained from the",
          "3.3. Training Protocol": "as the Spike-driven Transformer V2 [12]."
        },
        {
          "2.2.4. Parameter Reduction": "official EEG-Music Emotion Recognition Challenge dataset,",
          "3.3. Training Protocol": "Compact Convolutional Transformer: CCT model has"
        },
        {
          "2.2.4. Parameter Reduction": "which were underwent preprocessing using an FIR filter (0.5-",
          "3.3. Training Protocol": "demonstrated superior performance in a variety of\ntasks, out-"
        },
        {
          "2.2.4. Parameter Reduction": "40 Hz) and artifact\nremoval\nthrough Independent Component",
          "3.3. Training Protocol": "performing many contemporary transformer models\n[20].\nIn"
        },
        {
          "2.2.4. Parameter Reduction": "Analysis (ICA). Moreover,\nthe EEG signals were normalized",
          "3.3. Training Protocol": "this study, we incorporate the CCT model\nin our experimental"
        },
        {
          "2.2.4. Parameter Reduction": "for each trial within each subject. Each raw EEG trial, approx-",
          "3.3. Training Protocol": "setup and further reduce its size to align with the scale of our"
        },
        {
          "2.2.4. Parameter Reduction": "imately 90 seconds in length, was segmented into 10-second",
          "3.3. Training Protocol": "proposed lightweight transformer."
        },
        {
          "2.2.4. Parameter Reduction": "windows, each containing 1280 time points.",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "Spike-driven Transformer V2: Spike-driven Transformer"
        },
        {
          "2.2.4. Parameter Reduction": "The training dataset comprises 294 trials collected from 26",
          "3.3. Training Protocol": "V2 combines transformer-based architecture with spike-driven"
        },
        {
          "2.2.4. Parameter Reduction": "subjects, with each subject contributing roughly 12 labeled tri-",
          "3.3. Training Protocol": "processing,\nimproving computational\nefficiency while main-"
        },
        {
          "2.2.4. Parameter Reduction": "als.\nThese trials are used to train models to identify subjects",
          "3.3. Training Protocol": "taining high accuracy across various vision tasks [12]. We com-"
        },
        {
          "2.2.4. Parameter Reduction": "based on their distinct EEG signatures.\nFor validation, a set",
          "3.3. Training Protocol": "pared its performance to that of our proposed lightweight spik-"
        },
        {
          "2.2.4. Parameter Reduction": "was extracted from the training data (10%), ensuring a diverse",
          "3.3. Training Protocol": "ing transformer to evaluate both efficiency and effectiveness in"
        },
        {
          "2.2.4. Parameter Reduction": "and representative sample to evaluate model performance dur-",
          "3.3. Training Protocol": "the task of person identification."
        },
        {
          "2.2.4. Parameter Reduction": "ing training.",
          "3.3. Training Protocol": "As summarized in Table 1, our\nlightweight spiking trans-"
        },
        {
          "2.2.4. Parameter Reduction": "The test dataset consists of 104 held-out\ntrials, all coming",
          "3.3. Training Protocol": "former\nachieved 100% accuracy in the person identification"
        },
        {
          "2.2.4. Parameter Reduction": "from the 26 subjects seen during training. Additionally, 44 extra",
          "3.3. Training Protocol": "task. Compared to Spike-driven Transformer V2, which also"
        },
        {
          "2.2.4. Parameter Reduction": "trials are included in the test set, some of which lack stimulus in-",
          "3.3. Training Protocol": "reached 100% accuracy, our model significantly reduces the pa-"
        },
        {
          "2.2.4. Parameter Reduction": "formation or self-assessed emotion labels. This additional data",
          "3.3. Training Protocol": "rameter count from 30.9 million to 3.91 million—a nearly 90%"
        },
        {
          "2.2.4. Parameter Reduction": "further challenges the model,\ntesting its robustness and gener-",
          "3.3. Training Protocol": "reduction in model size—while maintaining the same level of"
        },
        {
          "2.2.4. Parameter Reduction": "alization capabilities. Person identification results are reported",
          "3.3. Training Protocol": "performance."
        },
        {
          "2.2.4. Parameter Reduction": "on the testing set.",
          "3.3. Training Protocol": "In contrast, both variants of the CCT exhibited lower accu-"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "racy. The smaller CCT model, despite having a similar param-"
        },
        {
          "2.2.4. Parameter Reduction": "",
          "3.3. Training Protocol": "eter count\nto ours, achieved only 98.65%.\nThis result\nfurther"
        },
        {
          "2.2.4. Parameter Reduction": "1EEG-Music Emotion Recognition Challenge.\n(n.d.). EEG-Music",
          "3.3. Training Protocol": ""
        },
        {
          "2.2.4. Parameter Reduction": "Emotion Recognition. Retrieved February 18, 2025,\nfrom https://eeg-",
          "3.3. Training Protocol": "underscores the effectiveness of SNNs in balancing computa-"
        },
        {
          "2.2.4. Parameter Reduction": "music-challenge.github.io/eeg-music-challenge/dataset",
          "3.3. Training Protocol": "tional efficiency and classification accuracy."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , our proposed lightweight spiking cessityofthedualConv-basedSNNblocksintheoriginalar-",
      "data": [
        {
          "Spike-driven transformer V2[12]": "SNN",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "Ours (Lightweight spiking transformer)",
          "30.9\n6065.1\n100%": "760.7\n100%\n3.91"
        },
        {
          "Spike-driven transformer V2[12]": "1 CCT model with reduced model size.",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "4.2. Model Efficiency",
          "30.9\n6065.1\n100%": "rameters, significantly lowering computational and energy costs"
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "while maintaining 100% accuracy. The results emphasize the"
        },
        {
          "Spike-driven transformer V2[12]": "We further compare the computational costs of different mod-",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "effectiveness of downsampling and parameter pruning in creat-"
        },
        {
          "Spike-driven transformer V2[12]": "els, evaluating their\ninference cost\nfor a single forward pass.",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "ing a lightweight yet highly accurate model with improved ef-"
        },
        {
          "Spike-driven transformer V2[12]": "The inference cost is measured by the number of floating-point",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "ficiency. Further reductions in downsampling dimensions were"
        },
        {
          "Spike-driven transformer V2[12]": "operations per\nsecond (FLOPs), based on the standard 45nm",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "explored but led to severe accuracy degradation, indicating that"
        },
        {
          "Spike-driven transformer V2[12]": "CMOS process [21, 22]. Notably, for SNNs, neurons are acti-",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "the 3.91 million parameter configuration achieves an optimal"
        },
        {
          "Spike-driven transformer V2[12]": "vated only when the input spikes surpass a threshold, allowing",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "balance between efficiency and performance."
        },
        {
          "Spike-driven transformer V2[12]": "inactive neurons to enter low-power modes,\nthus are expected",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "to reduce power consumption.",
          "30.9\n6065.1\n100%": "Impact of Conv-based SNN Blocks: To evaluate the ne-"
        },
        {
          "Spike-driven transformer V2[12]": "As\nshown in Table 1,\nour proposed lightweight\nspiking",
          "30.9\n6065.1\n100%": "cessity of the dual Conv-based SNN blocks in the original ar-"
        },
        {
          "Spike-driven transformer V2[12]": "transformer, with 3.91 million parameters,\ndemonstrates\nre-",
          "30.9\n6065.1\n100%": "chitecture, we tested a variant with only one Conv-based block."
        },
        {
          "Spike-driven transformer V2[12]": "markable energy efficiency, consuming only 760.7 µJ during",
          "30.9\n6065.1\n100%": "This\nfurther\nreduced the model\nsize to 2.85 million parame-"
        },
        {
          "Spike-driven transformer V2[12]": "inference.\nThis low energy consumption can be attributed to",
          "30.9\n6065.1\n100%": "ters (a 27% decrease from the 3.91M version) but\nresulted in"
        },
        {
          "Spike-driven transformer V2[12]": "the inherent efficiency of our SNN design, which simplifies",
          "30.9\n6065.1\n100%": "a 2.03% drop in accuracy. These findings highlight\nthe role of"
        },
        {
          "Spike-driven transformer V2[12]": "traditional multiply-accumulate\n(MAC) operations\ninto more",
          "30.9\n6065.1\n100%": "Conv-based SNN blocks in capturing hierarchical temporal fea-"
        },
        {
          "Spike-driven transformer V2[12]": "energy-efficient accumulation (AC) operations. Moreover,\nthe",
          "30.9\n6065.1\n100%": "tures and demonstrate the trade-off between extreme parameter"
        },
        {
          "Spike-driven transformer V2[12]": "SNN operates with an average firing rate of just 7.18%, meaning",
          "30.9\n6065.1\n100%": "reduction and performance retention."
        },
        {
          "Spike-driven transformer V2[12]": "that only a small fraction of the network’s spiking neurons are",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "active at any given time, further minimizing both computational",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "load and energy usage.",
          "30.9\n6065.1\n100%": "5. Discussion and Conclusion"
        },
        {
          "Spike-driven transformer V2[12]": "In contrast,\nthe\nenergy consumption of\nthe CCT model",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "was\nsignificantly\nhigher\nthan\nthat\nof\nthe SNN transformer",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "SNNs offer distinct\nadvantages\nfor EEG-based classification"
        },
        {
          "Spike-driven transformer V2[12]": "models, with the 22.0 million parameter version consuming",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "tasks due to their ability to capture the temporal dynamics in-"
        },
        {
          "Spike-driven transformer V2[12]": "38,414.6 µJ per\nforward pass.\nEven the smaller 3.24 million",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "herent\nin neural signals. EEG data is rich with time-dependent"
        },
        {
          "Spike-driven transformer V2[12]": "parameter variant, while more efficient,\nstill\nrequired 8,248.3",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "patterns, and SNNs, which process information through event-"
        },
        {
          "Spike-driven transformer V2[12]": "µJ—substantially more than the SNN-based models. Notably,",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "driven, spike-based mechanisms, are particularly well-suited to"
        },
        {
          "Spike-driven transformer V2[12]": "our model consumed only about 10% of this energy, highlight-",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "model\nthese temporal dependencies.\nThis alignment enables"
        },
        {
          "Spike-driven transformer V2[12]": "ing its superior efficiency.",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "SNNs to effectively represent\nthe brain’s intricate activity over"
        },
        {
          "Spike-driven transformer V2[12]": "In\nsummary,\nour\nproposed model\ndistinguishes\nitself",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "time, leading to enhanced accuracy in classifying EEG signals."
        },
        {
          "Spike-driven transformer V2[12]": "through its exceptional energy efficiency.\nIts low power con-",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "sumption, enabled by streamlined computational operations and",
          "30.9\n6065.1\n100%": "Another significant benefit of SNNs is their efficiency. The"
        },
        {
          "Spike-driven transformer V2[12]": "minimal neuronal firing rates, makes it well-suited for real-time,",
          "30.9\n6065.1\n100%": "lightweight SNN transformer model in this study achieved high"
        },
        {
          "Spike-driven transformer V2[12]": "energy-constrained applications such as BCIs, where both high",
          "30.9\n6065.1\n100%": "performance while\nsignificantly reducing computational\nand"
        },
        {
          "Spike-driven transformer V2[12]": "performance and low energy usage are essential\nfor effective",
          "30.9\n6065.1\n100%": "energy costs compared to analogous ANN models.\nThis ef-"
        },
        {
          "Spike-driven transformer V2[12]": "deployment.",
          "30.9\n6065.1\n100%": "ficiency is critical\nfor deploying EEG-based systems on edge"
        },
        {
          "Spike-driven transformer V2[12]": "The optimized SNN transformer model stands out\nfor\nits",
          "30.9\n6065.1\n100%": "devices, such as wearables or speech processors, where power"
        },
        {
          "Spike-driven transformer V2[12]": "superior energy efficiency.\nIts low energy consumption, driven",
          "30.9\n6065.1\n100%": "consumption and real-time processing are key constraints. The"
        },
        {
          "Spike-driven transformer V2[12]": "by simplified computational operations and low neuronal fir-",
          "30.9\n6065.1\n100%": "low firing rate of spiking neurons, coupled with simplified op-"
        },
        {
          "Spike-driven transformer V2[12]": "ing rates, makes\nit\na highly attractive option for\nreal-time,",
          "30.9\n6065.1\n100%": "erations, makes SNNs particularly suitable for deployment\nin"
        },
        {
          "Spike-driven transformer V2[12]": "energy-constrained applications such as brain-computer\ninter-",
          "30.9\n6065.1\n100%": "resource-constrained environments, ensuring that\nthey can op-"
        },
        {
          "Spike-driven transformer V2[12]": "faces, where both high performance and low power consump-",
          "30.9\n6065.1\n100%": "erate effectively in portable and energy-efficient settings."
        },
        {
          "Spike-driven transformer V2[12]": "tion are crucial.",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "However, despite these promising advantages, several limi-"
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "tations must be considered. While the model performed well on"
        },
        {
          "Spike-driven transformer V2[12]": "4.3. Ablation Study",
          "30.9\n6065.1\n100%": ""
        },
        {
          "Spike-driven transformer V2[12]": "",
          "30.9\n6065.1\n100%": "the 26-subject dataset used in this study,\nits generalizability to"
        },
        {
          "Spike-driven transformer V2[12]": "To evaluate the impact of architectural modifications on the",
          "30.9\n6065.1\n100%": "larger and more diverse populations remains uncertain. To fully"
        },
        {
          "Spike-driven transformer V2[12]": "spiking transformer, we conducted an ablation study focusing",
          "30.9\n6065.1\n100%": "assess the robustness and scalability of the model, further test-"
        },
        {
          "Spike-driven transformer V2[12]": "on two critical aspects:\n(1) downsampling strategy and (2) the",
          "30.9\n6065.1\n100%": "ing with more extensive datasets that incorporate a wider range"
        },
        {
          "Spike-driven transformer V2[12]": "role of Conv-based SNN blocks.",
          "30.9\n6065.1\n100%": "of demographic and physiological variability is needed. Addi-"
        },
        {
          "Spike-driven transformer V2[12]": "Downsampling\nStrategy\nand\nParameter\nEfficiency:",
          "30.9\n6065.1\n100%": "tionally,\nfuture research should investigate the adaptability of"
        },
        {
          "Spike-driven transformer V2[12]": "Compared to Spike-driven Transformer V2, one of the key mod-",
          "30.9\n6065.1\n100%": "SNNs to other EEG-based applications and explore their per-"
        },
        {
          "Spike-driven transformer V2[12]": "ifications\nin our\nlightweight\nspiking transformer was\nthe se-",
          "30.9\n6065.1\n100%": "formance in environments with varying noise levels and signal"
        },
        {
          "Spike-driven transformer V2[12]": "lection of smaller downsampling dimensions. This adjustment",
          "30.9\n6065.1\n100%": "quality. This would provide a more comprehensive understand-"
        },
        {
          "Spike-driven transformer V2[12]": "reduced the model size from 30.9 million to 3.91 million pa-",
          "30.9\n6065.1\n100%": "ing of the potential of SNNs in real-world EEG applications."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "A. Maida, “Deep learning in spiking neural networks,” Neural"
        },
        {
          "6. Acknowledgements": "This work is\nsupported by National Natural Science Foun-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "Networks, vol. 111, pp. 47–63, 2019."
        },
        {
          "6. Acknowledgements": "dation of China\n(Grant No.\n62401377),\ninternal Project of",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[14]\nP. Dayan and L. F. Abbott, Theoretical neuroscience: Computa-"
        },
        {
          "6. Acknowledgements": "Shenzhen Research Institute of Big Data,\ninternal project of",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "tional and mathematical modeling of neural systems. Cambridge:"
        },
        {
          "6. Acknowledgements": "the Guangdong Provincial Key Laboratory of Big Data Com-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "MIT Press, 2001."
        },
        {
          "6. Acknowledgements": "puting under\nthe Grant No.\nB10120210117-KP02, The Chi-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[15]\nZ.\nZhou,\nY\n.\nZhu,\nC.\nHe,\nY\n. Wang,\nS.\nYan,\nY\n.\nTian,"
        },
        {
          "6. Acknowledgements": "nese University of Hong Kong, Shenzhen (CUHK-Shenzhen),",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "and\nL. Yuan,\n“Spikformer:\nWhen\nspiking\nneural\nnetwork"
        },
        {
          "6. Acknowledgements": "Shenzhen Stability Science Program 2023, Shenzhen Key Lab",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "meets transformer,” ArXiv, vol. abs/2209.15425, 2022. [Online]."
        },
        {
          "6. Acknowledgements": "of Multi-Modal Cognitive Computing, Shenzhen Science and",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "Available: https://api.semanticscholar.org/CorpusID:252668422"
        },
        {
          "6. Acknowledgements": "Technology Program (Shenzhen Key Laboratory, Grant No.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[16] A. Dosovitskiy, “An image is worth 16x16 words: Transformers"
        },
        {
          "6. Acknowledgements": "ZDSYS20230626091302006), Shenzhen Science and Technol-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "for image recognition at scale,” arXiv preprint arXiv:2010.11929,"
        },
        {
          "6. Acknowledgements": "ogy Research Fund (Fundamental Research Key Project, Grant",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "2020."
        },
        {
          "6. Acknowledgements": "No.\nJCYJ20220818103001002), Program for Guangdong In-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[17] X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg:"
        },
        {
          "6. Acknowledgements": "troducing Innovative\nand Entrepreneurial Teams, Grant No.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "Making vgg-style convnets great again,” in 2021 IEEE/CVF Con-"
        },
        {
          "6. Acknowledgements": "2023ZT10X044,\nand Guangdong S&T Program under Grant",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "ference on Computer Vision and Pattern Recognition (CVPR),"
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "2021, pp. 13 728–13 737."
        },
        {
          "6. Acknowledgements": "2023B0303040002.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[18] C.\nFrenkel, D. Bol,\nand G.\nIndiveri,\n“Bottom-up\nand\ntop-"
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "down\napproaches\nfor\nthe\ndesign\nof\nneuromorphic\nprocessing"
        },
        {
          "6. Acknowledgements": "7. References",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "systems: Tradeoffs and synergies between natural and artificial"
        },
        {
          "6. Acknowledgements": "[1] Y. An, J. Wong, and S. H. Ling, “Development of real-time brain-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "Proceedings\nof\nthe\nintelligence,”\nIEEE,\nvol.\n111,\npp.\n623–"
        },
        {
          "6. Acknowledgements": "computer interface control system for robot,” Applied Soft Com-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "652, 2021.\n[Online]. Available:\nhttps://api.semanticscholar.org/"
        },
        {
          "6. Acknowledgements": "puting, vol. 159, p. 111648, 2024.\n[Online]. Available:\nhttps://",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "CorpusID:258685271"
        },
        {
          "6. Acknowledgements": "www.sciencedirect.com/science/article/pii/S1568494624004228",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[19]\nE. O. Neftci, H. Mostafa, and F. Zenke, “Surrogate gradient learn-"
        },
        {
          "6. Acknowledgements": "[2] Y.-Y. Yang, A. Hwang, C.-T. Wu,\nand T.-R. Huang,\n“Person-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "ing in spiking neural networks: Bringing the power of gradient-"
        },
        {
          "6. Acknowledgements": "identifying brainprints are stably embedded in eeg mindprints,”",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "based optimization to spiking neural networks,” IEEE Signal Pro-"
        },
        {
          "6. Acknowledgements": "Scientific Reports, vol. 12, 10 2022.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "cessing Magazine, vol. 36, no. 6, pp. 51–63, 2019."
        },
        {
          "6. Acknowledgements": "[3]\nP. Pandey, G. Sharma, K. P. Miyapuram, R. Subramanian, and",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[20] A. Hassani,\nS. Walton,\nN.\nShah,\nA. Abuduweili,\nJ.\nLi,"
        },
        {
          "6. Acknowledgements": "D. Lomas, “Music identification using brain responses to initial",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "and H. Shi,\n“Escaping\nthe\nbig\ndata\nparadigm with\ncompact"
        },
        {
          "6. Acknowledgements": "snippets,” in ICASSP 2022-2022 IEEE International Conference",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "transformers,”\nArXiv,\nvol.\nabs/2104.05704,\n2021.\n[Online]."
        },
        {
          "6. Acknowledgements": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "Available: https://api.semanticscholar.org/CorpusID:233210459"
        },
        {
          "6. Acknowledgements": "2022, pp. 1246–1250.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[21] M. Horowitz, “1.1 computing’s energy problem (and what we can"
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "do about it),” in 2014 IEEE international solid-state circuits con-"
        },
        {
          "6. Acknowledgements": "Clinical\n[4]\nJ. Warren,\n“How does\nthe\nbrain\nprocess music?”",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "ference digest of\ntechnical papers\n(ISSCC).\nIEEE, 2014, pp."
        },
        {
          "6. Acknowledgements": "Medicine, vol. 8, no. 1, pp. 32–36, 2008.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "10–14."
        },
        {
          "6. Acknowledgements": "[5]\nP. Vuust, O. A. Heggli, K. J. Friston, and M. L. Kringelbach, “Mu-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "[22] M. Yao, G. Zhao, H. Zhang, Y. Hu, L. Deng, Y. Tian, B. Xu, and"
        },
        {
          "6. Acknowledgements": "sic in the brain,” Nature Reviews Neuroscience, vol. 23, no. 5, pp.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "G. Li, “Attention spiking neural networks,” IEEE Transactions"
        },
        {
          "6. Acknowledgements": "287–305, 2022.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "on Pattern Analysis and Machine Intelligence, vol. 45, no. 8, pp."
        },
        {
          "6. Acknowledgements": "[6] B. Kaur, D. Singh, and P. Roy, “A novel framework of EEG-based",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": "9393–9410, 2023."
        },
        {
          "6. Acknowledgements": "user identification by analyzing music-listening behavior,” Multi-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "media Tools and Applications, vol. 76, 12 2017.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[7]\nI. Jayarathne, M. Cohen, and S. Amarakeerthi, “Person identifi-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "cation from EEG using various machine learning techniques with",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "inter-hemispheric\namplitude\nratio,” PLoS ONE, vol. 15,\n2020.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "221636638",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[8] G. Sharma, P. Pandey, R. Subramanian, K. P. Miyapuram, and",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "A. Dhall, “Neural encoding of songs is modulated by their enjoy-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "the 2022 International Conference on\nment,” in Proceedings of",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "Multimodal Interaction, 2022, pp. 414–419.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[9] Y. Akbarnia and M. R. Daliri, “EEG-based identification system",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "using deep neural networks with frequency features,” Heliyon,",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "vol. 10, no. 4, p. e25999, 2024. [Online]. Available: https://www.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "sciencedirect.com/science/article/pii/S2405844024020309",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[10]\nS. Cai, P. Li, and H. Li, “A bio-inspired spiking attentional neu-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "ral network for attentional selection in the listening brain,” IEEE",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "Transactions on Neural Networks and Learning Systems, vol. 35,",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "no. 12, pp. 17 387–17 397, 2024.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[11]\nF. Faghihi, S. Cai, and A. A. Moustafa, “A neuroscience-inspired",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "spiking neural network for EEG-based auditory spatial attention",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "detection,” Neural Networks, vol. 152, pp. 555–565, 2022.",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "[12] M. Yao,\nJ. Hu,\nT. Hu, Y. Xu,\nZ. Zhou, Y. Tian, B. XU,",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "and\nG.\nLi,\n“Spike-driven\ntransformer\nv2:\nMeta\nspiking",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "neural\nnetwork\narchitecture\ninspiring\nthe\ndesign\nof\nnext-",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "International\ngeneration\nneuromorphic\nchips,”\nin The Twelfth",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "Conference\non\nLearning\nRepresentations,\n2024.\n[Online].",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        },
        {
          "6. Acknowledgements": "Available: https://openreview.net/forum?id=1SIBN5Xyw7",
          "[13] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Development of real-time braincomputer interface control system for robot",
      "authors": [
        "Y An",
        "J Wong",
        "S Ling"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "2",
      "title": "Personidentifying brainprints are stably embedded in eeg mindprints",
      "authors": [
        "Y.-Y Yang",
        "A Hwang",
        "C.-T Wu",
        "T.-R Huang"
      ],
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "3",
      "title": "Music identification using brain responses to initial snippets",
      "authors": [
        "P Pandey",
        "G Sharma",
        "K Miyapuram",
        "R Subramanian",
        "D Lomas"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "How does the brain process music?",
      "authors": [
        "J Warren"
      ],
      "year": "2008",
      "venue": "Clinical Medicine"
    },
    {
      "citation_id": "5",
      "title": "Music in the brain",
      "authors": [
        "P Vuust",
        "O Heggli",
        "K Friston",
        "M Kringelbach"
      ],
      "year": "2022",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "6",
      "title": "A novel framework of EEG-based user identification by analyzing music-listening behavior",
      "authors": [
        "B Kaur",
        "D Singh",
        "P Roy"
      ],
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "7",
      "title": "Person identification from EEG using various machine learning techniques with inter-hemispheric amplitude ratio",
      "authors": [
        "I Jayarathne",
        "M Cohen",
        "S Amarakeerthi"
      ],
      "year": "2020",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "8",
      "title": "Neural encoding of songs is modulated by their enjoyment",
      "authors": [
        "G Sharma",
        "P Pandey",
        "R Subramanian",
        "K Miyapuram",
        "A Dhall"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "9",
      "title": "EEG-based identification system using deep neural networks with frequency features",
      "authors": [
        "Y Akbarnia",
        "M Daliri"
      ],
      "year": "2024",
      "venue": "Heliyon"
    },
    {
      "citation_id": "10",
      "title": "A bio-inspired spiking attentional neural network for attentional selection in the listening brain",
      "authors": [
        "S Cai",
        "P Li",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "11",
      "title": "A neuroscience-inspired spiking neural network for EEG-based auditory spatial attention detection",
      "authors": [
        "F Faghihi",
        "S Cai",
        "A Moustafa"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of nextgeneration neuromorphic chips",
      "authors": [
        "M Yao",
        "J Hu",
        "T Hu",
        "Y Xu",
        "Z Zhou",
        "Y Tian",
        "B Xu",
        "G Li"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "Deep learning in spiking neural networks",
      "authors": [
        "A Tavanaei",
        "M Ghodrati",
        "S Kheradpisheh",
        "T Masquelier",
        "A Maida"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Theoretical neuroscience: Computational and mathematical modeling of neural systems",
      "authors": [
        "P Dayan",
        "L Abbott"
      ],
      "year": "2001",
      "venue": "Theoretical neuroscience: Computational and mathematical modeling of neural systems"
    },
    {
      "citation_id": "15",
      "title": "Spikformer: When spiking neural network meets transformer",
      "authors": [
        "Z Zhou",
        "Y Zhu",
        "C He",
        "Y Wang",
        "S Yan",
        "Y Tian",
        "L Yuan"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "16",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "17",
      "title": "Repvgg: Making vgg-style convnets great again",
      "authors": [
        "X Ding",
        "X Zhang",
        "N Ma",
        "J Han",
        "G Ding",
        "J Sun"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Bottom-up and topdown approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence",
      "authors": [
        "C Frenkel",
        "D Bol",
        "G Indiveri"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "19",
      "title": "Surrogate gradient learning in spiking neural networks: Bringing the power of gradientbased optimization to spiking neural networks",
      "authors": [
        "E Neftci",
        "H Mostafa",
        "F Zenke"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "20",
      "title": "Escaping the big data paradigm with compact transformers",
      "authors": [
        "A Hassani",
        "S Walton",
        "N Shah",
        "A Abuduweili",
        "J Li",
        "H Shi"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "21",
      "title": "1.1 computing's energy problem (and what we can do about it),\" in 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)",
      "authors": [
        "M Horowitz"
      ],
      "year": "2014",
      "venue": "1.1 computing's energy problem (and what we can do about it),\" in 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)"
    },
    {
      "citation_id": "22",
      "title": "Attention spiking neural networks",
      "authors": [
        "M Yao",
        "G Zhao",
        "H Zhang",
        "Y Hu",
        "L Deng",
        "Y Tian",
        "B Xu",
        "G Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}