{
  "paper_id": "2105.06421v3",
  "title": "Using Self-Supervised Auxiliary Tasks To Improve Fine-Grained Facial Representation",
  "published": "2021-05-13T16:56:36Z",
  "authors": [
    "Mahdi Pourmirzaei",
    "Gholam Ali Montazer",
    "Farzaneh Esmaili"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, at first, the impact of ImageNet pre-training on fine-grained Facial Emotion Recognition (FER) is investigated which shows that when enough augmentations on images are applied, training from scratch provides better result than fine-tuning on ImageNet pre-training. Next, we propose a method to improve fine-grained and in-the-wild FER, called Hybrid Multi-Task Learning (HMTL). HMTL uses Self-Supervised Learning (SSL) as an auxiliary task during classical Supervised Learning (SL) in the form of Multi-Task Learning (MTL). Leveraging SSL during training can gain additional information from images for the primary fine-grained SL task. We investigate how proposed HMTL can be used in the FER domain by designing two customized version of common pre-text task techniques, puzzling and in-painting. We achieve stateof-the-art results on the AffectNet benchmark via two types of HMTL, without utilizing pre-training on additional data. Experimental results on the common SSL pre-training and proposed HMTL demonstrate the difference and superiority of our work. However, HMTL is not only limited to FER domain. Experiments on two types of fine-grained facial tasks, i.e., head pose estimation and gender recognition, reveals the potential of using HMTL to improve finegrained facial representation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotions are important factors in human communication to help people convey their emotional states and intentions. One-third of these communications are verbal components and two-third of residues are nonverbal. Among the non-verbal components, facial emotions have a key role in communications between people  [1] .\n\nRecently, various systems have been developed for recognizing facial emotions in the field of computer vision and deep learning  [1] . To detect emotion of a person in an image, a lot of visual information can be found such as appearance, gesture, behavior, and context of the scene. But facial emotions are dramatically the most important visual cue for analyzing basic human emotions  [2] .\n\nThere are two models to explain facial emotions in computer vision: categorical and circumplex (dimensional)  [3] :\n\nIn categorical model, Ekman  [4]  has defined a list of affective-related categories (Anger, Happy, Sad, Surprise, Disgust, Neutral, Fear, and Contempt), which emotions have been chosen from this list. The facial action coding system (FACS) depends on action units (AUs) or a set of facial muscle movements being determined to display the emotions of participants. Ekman used FACS to recognize emotions.\n\nThe circumplex model of emotion was developed by James Russell  [3] . This model suggests that emotions are distributed in a two-dimensional circular space, containing arousal and valence dimensions. To be precise, the vertical and the horizontal axis determine valence and arousal respectively and the center of this circle represents the normal state of valence and arousal  [5] . This model is shown in figure  1 .\n\nAccording to Russel and Ekman model, Data labeling requires experts and it is not as simple as annotating like the other computer vision tasks such as object detection. The inherent uncertainty in facial emotion recognition is the reason for the difficulty and the cost of labeling. This uncertainty is even more for the Russell's model. Thus, unlike other computer vision issues, collecting large amounts of labeled data in the field of Facial Emotion Recognition (FER) is challenging and may adversely impact on quality of annotating. This makes the utilize of deep learning or specifically Supervised Learning (SL), become less effective in FER. Recently, with progress in Self-Supervised Learning (SSL), results show that in an end-to-end learning manner, SSL methods can help us to overcome needing lots of labels  [6] [7] [8] . While recent contrastive SSL methods have demonstrated capability of them, they still have some drawbacks which did not solve completely, such as requiring large batch sizes to train  [6] , need for accessing huge amount of computation power and data to work well  [6, 7, 9] , and placing far behind of SL methods on fine-grained visual tasks  [10] . The last two ones are considered as the main hurdle to improve FER performance by SSL. In fact, as far as we know, there is no work which seriously attempt to use SSL on fine-grained FER. Furthermore, FER is one of the most challenging fine-grained problems because of its uncertainty. For example, in collecting and labeling AffectNet dataset, their collectors found that even with human training, there were about 60% to 70% agreement in their labeling among different emotions  [11] . In other words, it shows the uncertainty and ambiguity of emotion information through visual channel.\n\nIn some studies  [12, 13] , by the another type of SSL methods named pre-text task SSL, they could extract valuable features for many downstream tasks, even by doing pre-training on one image  [14] . In fact, in this type of pre-training, middle layers show better features concerning final layers. Likewise, by moving from low level layer to high level layer, increase and then decrease could be seen via linear evaluation of feature map  [14] . Since 1990s, different studies have shown the ability of Multi-Task Learning (MTL) in Supervised settings  [15, 16] . Yet, it is not quite obvious how to pick the proper tasks due to the fact that, before a certain point, two tasks enhance shareable features for each other and after that point, they start hurting each other which is described as cooperation and competition between tasks  [15] . Therefore, from another view, using SSL tasks can be looked through the MTL lens. So, why do not we use SL with auxiliary tasks of SSL concurrently in the MTL setting?\n\nIn this study, we used SSL besides SL in the MTL setting called Hybrid Multi-Task Learning (HMTL). The proposed HMTL was considered during the training procedures. It means that the SSL tasks were placed on top of the backbone and they could be removed from it during testing or inference. The purpose of appending SSL tasks was to help the backbone to build better features for the fine-grained FER. One important thing in the proposed HMTL was that it basically is not a pre-training technique like other studies in this major. Mainly, there are two ways to use SSL:  (1)  Using it to pre-train the weights, (2) leveraging SSL as an auxiliary task besides of SL which we showed the advantage of the second one. Need to mention that in this study, \"HMTL\", \"SL+SSL\" and \"SL with auxiliary SSL task\" are used interchangeably and refer to a same thing.\n\nThe contributions of this work are summarized as follows:\n\n1. We showed the effect of augmentation intensity on FER using pre-trained ImageNet weights and random weights. It showed that the combination of random weights and strong augmentation is superior to the fine-tuning on ImageNet weights procedure with any sorts of augmentation. 2. We did pre-training with some SSLs pre-text tasks. Results showed that two of our proposed SSL methods can extract better and useful features for the FER problem. They both were also good to use as the pre-training step. 3. We proposed a hypothesis which says: \"leveraging proper auxiliary tasks of SSL alongside with SL in MTL manner can improve performance of the downstream supervised task\". 4. Results showed that by utilizing our HMTL approach on AffectNet dataset, the performance of both types of emotion recognition (i.e., dimensional and categorical) on all the augment levels remarkably increased, even when 20% of the heavily imbalanced training set was used. 5. The impact of using auxiliary SSL tasks on two fine-grained head pose estimation and gender recognition were scrutinized which concluded to substantial average error reduction in head pose estimation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Methods of emotion recognition from faces can be divided into two general categories: conventional methods and end-to-end learning methods, in which end-to-end based models have been able to achieve better performance in many computer vision problems  [1] .\n\nIn both approaches deep learning has been a crucial part. And also, among several deep learning models, convolutional neural networks (CNNs) have played important roles in FER for many years  [1] . CNNs completely reduce preprocessing techniques by providing end-to-end learning from inputs  [17] .\n\nEven though, some studies did not go only for CNN features. For example, in  [18] , they combined two types of feature extraction modes. First, they used features learned by CNN models through pre-trainings. Second, they used handcrafted features subtracted by a bag of visual words.\n\nBut, as we said, end-to-end learning methods have been superior than traditional methods. End-to-end methods mostly focused on designing different deep architecture  [19] [20] [21] . In a work  [19] , they used manifold networks in connection with CNN and showed that manifold networks of covariance pooling could get better performance than CNN networks with Softmax layers. Another work  [20] , introduced two CNN-based models for FER using different kernel sizes and numbers of filters.\n\nVideos can also be used to recognize emotions from faces as well  [22] . In videos, frames are classified to various emotions such as happy, sad, etc. Although Inception and ResNet networks have had significant results in the field of FER, these two architectures did not use temporal aggregation. To solve this problem, a three-dimensional (3D) Inception-ResNet architecture was introduced  [23] . In this type of architecture, geometric and temporal features were extracted in a sequence of frames with the three-dimensional model.\n\nAnother study  [24] , used two different types of CNN networks on videos that improve the FER performance. First model extracted the temporal characteristics features of the images and the second one extracted the temporal geometric characteristics of the Facial Landmarks (FLs) points over time.\n\nIn addition to CNN, combining CNNs with RNN based models (GRU, LSTM) also could improve the performance of the networks on videos. In a study  [25] , three architectures that consist of a combination of CNN and RNN to recognize dimensional emotions in a MTL manner were used. To join CNN and RNN, each video frame was given to the CNN model, then several levels of features were extracted from it to feed each of them to multi RNNs. Combining end-toend approaches like LSTMs with CNNs and supporting the variable-length inputs and outputs are the most important advantages of using LSTM  [1] . In a study  [26] , a hybrid model in the form of LSTM-CNN was proposed which showed that this hybrid architecture could outperform previous 3D-CNN over time.\n\nJust like CNNs, using attentional models have achieved significant improvements over other methods in recent years  [27] [28] [29] [30] . A study  [27] , used CNN with visual attention for feature extraction and detection of important regions. Another study  [31] , proposed a CNN with attention mechanisms that could understand occlusion regions in the face. Also, they created an end-toend trainable Patch-Gated CNNs to recognize facial expressions from occluded faces, and the model could automatically focus on unoccluded regions.\n\nHowever, there have been a few methods which have done novel experiment such as using graph convolutional neural networks  [32] . In fact, undirected graphs were constructed from faces. Likewise, another study  [33]  used an identity-free conditional Generative Adversarial Network (IF-GAN) to detect facial expressions by distinguishing two types of features in faces: Information related to personal identity and information related to facial expressions. This work tried to reduce the effect of identity features in facial images as much as possible, and then the features related to facial expressions were used to categorize emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Learning",
      "text": "SSL methods are used to learn features from unlabeled data without using annotated labels. This method of learning is used as a subset of unsupervised learning methods to avoid the large cost of gathering and labeling huge scales of datasets. SL methods need data pairs of (x, y) as the input and output which usually are annotated by a human. SSL methods are also learned from the input x, but unlike SL, the labels y are automatically generated from the inputs, without participating in any human annotation  [34] . In the variety of scales including the robustness of adversarial examples, label corruption, Semi-Supervised learning, etc. SSL methods are useful.\n\nMany SSL methods have been proposed in the computer vision field  [34]  which in general, three types of approaches exist:\n\n1. Contrastive learning 2. Non-contrastive learning 3. Pre-text task learning\n\nIn the mid-2010s, pre-text task learning, including colorization, in-painting, and self-supervised jigsaw puzzle became popular in computer vision field  [35, 36] . However, recently, contrastive and non-contrastive SSL methods have achieved better results on challenging datasets such as ImageNet  [6, 8, 9] . Although these methods have been successful to achieve better results, they need more data and more computation cost to work well. Also, the training process of them are rather hard and tricky. However, these methods could not work well on finegrain problems such as FER which shows that there is a room for improvement  [10] .\n\nOne of the most well-known pre-text task techniques is random rotation. In  [12] , they used 2d image rotation, which was applied to input images to learn direction with CNN training. In addition to random rotation self-supervision, several papers used jigsaw puzzles in a SSL manner. For instance,  [37, 38]  used a jigsaw puzzle that shredded each image, then shuffled it and fed it to a Siamese network. Also,  [13]  used SSL to solve jigsaw puzzles, in-painting, and colorization together.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Imbalance Dataset",
      "text": "In classification problems, when the number of samples is unevenly distributed in classes, the network learns classes with more samples better than other classes, and performance decreases for the classes with fewer samples. If the imbalance is high, it can affect the performance of the classifier and cause the network to bias towards the larger class  [39] . Therefore, there are several ways to deal with this problem:\n\n1. Up sampling 2. Down sampling 3. Customize loss function It has been shown that the customized loss function performs better in FER  [11] . In this work, we consider weighted loss and focal loss as two popular ones.\n\nIn weighted loss method, a higher loss is assigned to the samples belonging to the classes with fewer samples. Focal loss  [40]  reduces the effect of error calculation when the probability of predicting output p increased with adjustable alpha and gamma coefficients. If the network has more confidence in predicting samples the loss function will get lower and smaller coefficients and consequently, difficult samples will get bigger coefficients.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Before starting our work, three augmentation levels are defined (appendix A).\n\nThe purpose of selecting these three levels is to investigate the impact of augmentation on the FER problem. At first step, a neural network is considered to compare fine-tuning and training from scratch on augmentation levels. Then, we evaluate features from SSL pre-trained methods. At the end, we propose our Hybrid Learning architecture for FER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Supervised Learning Approach",
      "text": "To compare different augmentation settings, we test them under ImageNet weights and random weights (figure  2 ). We use EfficientNet B0 and B2  [41]  as our neural network backbones.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Supervised Learning Approach",
      "text": "In this section, three SSL methods are selected: Rotating, Puzzling and Inpainting.\n\nIn rotation method, images multiply at N√ó45 degrees which N is chosen randomly from zero to seven. It means images rotate at one of eight directions and this direction is considered as the output label. Therefore, this is a classification problem with eight categories.\n\nOur customized proposed puzzling pre-text task is somehow different from the original jigsaw puzzling method. Unlike general jigsaw puzzling which different parts of the images are fed to a Siamese network separately, in our method (figure  3 ) at first, an image is sliced to N identical size square regions, where N must have a second root (4, 9, etc.). Then, all pieces are randomly shuffled and for each region, a label is created to show the original location of it. In the end, regions are merge into a single image and a neural network tries to learn the correct location of each region in the image. For this purpose, we need to create classification heads according to the number of regions. For example, as we illustrate in figure  3 , the image is divided into four regions which get shuffled and then, they merge into a single image. The neural network (EfficientNet) gets the puzzled images and tries to find the correct location of each region based on its head.\n\nIn contrast to previous methods, in-painting is not a classification problem. In this study, we consider two types of in-painting. The first one has one stage and the second one has two stages. The first one is a common SSL pre-text task which has been used a lot. We use it for both pre-training and HMTL that only has Pixel Wise Loss (PWL) function like mean square error (MSE). This is different from the two-stage type. We consider two-stage type due to the fact that we want to add a FER perceptual loss (PL) to it. The PL uses features of a pre-trained model which is trained on the same dataset in an ordinary supervised manner. In other words, PL function tries to reconstruct the missing part of an image with respect to the representation of a supervised model we already trained (figure  4 ). In this type of in-painting, we only use the PL which is defined as one of the HMTL methods.\n\nFor the erasing procedure of in-painting, we determine a fixed region in the image consisting of face attributes (figure  4 .a). Then, we cut a square with fix side randomly inside of the region (figure 4.b). In other words, the input images are cut out partially. Then, a SSH is built in the form of a deconvolutional decoder. The decoder tries to reconstruct the original image. Here the difference of two methods is shown up. The first one uses MSE loss function to fill the cutout image with respect to the original image, but, in the second type of inpainting (e.g., PL), at first, a pre-trained EfficientNet model as an offline teacher is considered to create representation for the loss function. Then, the SSH's output is trained only by representation of the offline teacher (figure  6 ).\n\nIn HMTL, the decoder head tries to close the distances of representation for generated image with the original one (the input image w/o cutout). Equation  3 shows the total loss function. The important point is that the FER model for creating representation is trained with SL only on AffectNet under the same augmentation setting (w/o cutout). Therefore, this means no additional information from different augmenting levels or different datasets are going through the error signal.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hybrid Multi-Task Learning Approach (Adding Auxiliary Ssl)",
      "text": "The impact of multitask learning in neural networks is palpable  [16] . As we look furthermore into this issue, there are a lot of information in images that can increase the performance of deep networks in computer vision indirectly. This information cannot correctly be recognized in SL because of their indirect effects on the loss function. For example, simultaneously recognition of facial features along with emotion could improve the final performance of emotion recognition  [42][43] . We believe that these features are not clear enough to be accessible from emotion labels. With consideration that gathering and annotating multi labels in FER (e.g., AU, landmark, etc.) are expensive and hard to access with hand labeling, we propose a hypothesis that says: \"leveraging proper tasks of SSL as auxiliary tasks beside the SL in the MTL setting can improve supervised task representation\". This statement means that if SSHs are put alongside SH, it helps the networks to create a better representation of images for the task. To choose the best HMTL approach, each method of self-supervision can be separately performed and the effectiveness of extracted features on solving the main problem can be examined as well. A SSL method that find valuable features for a downstream task (here FER), can be used besides of SL head (figure  5, 6 ).\n\nWhere:\n\n‚Ä¢ ùêø ùëÜùêø : weighted categorical cross entropy for supervised head ‚Ä¢ ùêø ùëÜùëÜùêø : categorical cross entropy for puzzling self-supervised heads\n\nWhere:\n\n‚Ä¢ ùêø ùëÜùêø : weighted categorical cross entropy for supervised head ‚Ä¢ ùêø ùëÜùëÜùêø : categorical cross entropy for puzzling self-supervised heads and rotation selfsupervised head\n\nWhere:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Categorical And Dimensional Models",
      "text": "In categorical model, we only have one SH. That is to say, eight Ekman's emotions are added to the network as the supervised classification head with eight categories. In addition to correctly learning the SL task, the network is forced to solve SSL tasks as well (Equations 1 to 3). Unlike the training step, in the evaluation step, images are given to the network without SSL pre-task such as puzzling or cutout. Only SH is going to consider during the evaluation step.\n\nWhere: Figure  7  shows the procedure of calculating the loss function (equation 4) in the training process of continuous mode. The alpha coefficient is considered as a regulator for the attention of the network to the classification part and its value can be changed. This method is similar to HopeNet  [44]  method which tries to transform continues labels to both categorical and continues.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this paper, we used two versions of EfficientNet architectures, B0 and B2 among eight versions. we have considered EfficientNet architecture due to being better than other CNN architectures in terms of performance, number of parameters and flops in various domains and benchmarks  [41] . The resolution sizes have been set to 224√ó224 and 260√ó260 for B0 and B2 respectively. In this study, a 1080 Ti GPU card has been used to train neural network models. For training all models in this section, an Adabelief optimizer has been used  [45] . Also, the batch size has been set to 64 for all models except for the in-painting which is 32. For all the experiments, the TensorFlow framework have been used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affectnet Dataset",
      "text": "The AffectNet is one of the largest FER databases with more than 1 million images gathered from three search engines with querying emotion-related tags in different languages. 450,000 images are labeled in two modes: categorical and dimensional. In the categorical mode there are 11 labels in which eight of them are the basic emotions suggested by Ekman. Although labels in the training set are highly imbalanced (figure  8 ), in the validation set for each category 500 images are prepared. The test set has not been published yet  [11] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Supervised Learning",
      "text": "In this part, the EfficientNet B0 has been used as the backbone of SL to extract features for a linear classifier to predict eight emotions which is placed on top of it. In the training part, we set a learning rate of 0.0001 when using ImageNet weights and 0.001 when using random weights mode. The number of epochs based on the augmentation level and the weights were considered between 20 and 100. Weight decay was not used and we only used the step decay method for learning rate schedule. We considered the AffectNet training set for the training and validation set for the evaluation step. In the training set, there are approximately 287,000 images based on the eight emotions of Ekman model (Figure  8 ). In the validation set, there are 500 images for each category. Since the training set is very imbalanced, we used the weighted cross entropy. Also, the dropout and label smoothing methods with different intensities have been used. Each training mode has been repeated three times on three fixed random seeds and the model with the highest accuracy has been selected as the reported result. The results of each training mode are shown in table 1 and figure  9 . As the results show, increasing augmentation intensity on the fine-tuning mode has not have remarkable impact on the results. In contrast, the consequence of that on the random weights mode has been more recognizable. As a matter of fact, when the strong level was used, it achieved higher accuracy compared to the ImageNet fine-tuning. In the learning procedure, before feeding inputs to the networks, images are divided into nine regions and these regions are shuffled and then merged. In the validation step, augmentation and SSL tasks do not perform on images. The puzzling pre-text block can be replaced with other pretext methods like rotation.  ) ,œÉ(\n\n) )\n\nCutout",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Self-Supervised Learning",
      "text": "We have used four pretext task learning approaches in this part:\n\n1. Puzzling 2. Random rotation 3. Puzzling + random rotation 4. In-painting-pwl\n\nIn the first one, heads have been added on top of the EfficientNet backbone with the number of puzzle pieces (e.g., 4 or 9 heads). In the second approach, only one head has been placed on top of the backbone which the number of classes has been equal to the number of rotation directions. In the third one, all heads of the first and second approaches have been joined together (e.g., 5 or 10 heads). Lastly, the pre-training procedure of the fourth approach is based on a CNN decoder which has been placed on the backbone and tried to only reconstruct the original image with respect to the RMSE pixel-wise loss function. As the first three methods are relatively easy for a deep network to learn, the strong augmentation level has been considered for all three of them. Needless to say, that in the second and third approaches, random rotation augmentation has been removed. Moreover, in the last approach, no augment level has been used. Like previous part, each method has been trained three times on three pre-defined random seeds. Considering methods in this section do not require labels, all the AffectNet images (more than 1 million) can be suitable for self-supervised training. But all images in AffectNet are not face images. In other words, in 450,000 images with labels, 89,000 of them are faceless. This means that in 550,000 unlabeled images; there are over 100,000 faceless images which can be counted as noises and they are worthless for emotion recognition task as well as self-supervised pre-training. Thus, the 361,000 labeled images which include faces have been used for the training. We used the focal loss function for puzzle classification losses due to the fact that some face images are more different from the others. After training based on all the approaches and reaching convergence, the SSHs have been removed from top of the backbone, then its output has been evaluated in a nonlinear way on the eight AffectNet emotions. That is to say, we have trained a neural network with two layers to classify the AffectNet training set on top of the frozen pre-trained backbone of for all four SSL approaches. Results are shown in Table  2 . Additionally, we have fine-tuned all layers with SSL pre-trained backbones on AffectNet (table  3 ). In both nonlinear evaluation and fine-tuning settings, the no augment level has been selected. On one hand, as table 2 shows, features which have been trained by the SSL methods have been effective compared to the random classification. The puzzling approach and in-painting-pwl have gained more accuracy thought.\n\nOn the other hand, based on the fine-tuning results (table  3 ), we could see a backbone which has been pre-trained by a SSL task like in-painting-pwl could act differently on a downstream task whether doing fine-tuning or feature evaluation.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Hybrid Multi-Task Learning",
      "text": "HMTL of this section has been done in recognizing emotions from single images in categorical and dimensional model. In this paper, the focus of HMTL has been on the categorical model. To show the effectiveness of our proposed HMTL, it has been tested on the circumplex model using 3√ó3 puzzling auxiliary task, nonetheless.\n\nThe categorical model is considered with four SSL methods:\n\nIn-painting-pl (Equation  3 )\n\nIn-painting-pwl\n\nThe dimensional mode consists of three methods:\n\nLike previous parts, each method has been trained three times on three fixed random seeds and the best results have been reported.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Categorical",
      "text": "In this section, as figure  6  and figure  7  are illustrating, the SSHs are put alongside the SH in the training procedures of the categorical model. Here, in all the puzzling SSHs, categorical cross-entropy has been used (Equation  1 ). We did not use focal loss due to the lower value compare to the cross-entropy loss function. It means that the SSHs loss values would decrease significantly after few epochs and the network could be forced towards the SH. Moreover, in the decoder loss function of the in-painting methods, the error rate has been enlarged with Œª coefficient to make it be equal SH in the initial epochs (Equation  3 ).\n\nIn HMTL approach, due to hardware limitation we have considered random weights with the no and weak augment levels. Despite we have to recognize the location of regions in permuted images and the emotion label at the same time in the training step, we only give the original images to the network without doing permutation or cutout in the validation step, nevertheless. Accordingly, the validation step is only considered for the SH. Results are shown in figure  10  and table  5 .\n\nWe need to emphasize that the number of emotions is an important factor in the final result (Accuracy). When the contempt emotion is added to the labels and the number of emotion categories increases from seven to eight, the accuracy decreases 3% to 4%. In this paper, due to more complexity and generalization of eight labels, only the eight categories have been considered.\n\nAnother important point in the use of SSHs is the less tendency of the network to reach overfitting. Because deep networks with a small amount of data tend to overfit, this may indicate that SSHs improving performance on low data. To test this claim, we have trained by the proposed HMTL with a small percentage of the AffectNet training set images. That is to say, eight Ekman labels in the AffectNet training set have been considered and 20% of them have been randomly selected with a fixed random seed for training process. Need to mention that the distribution of emotion labels in the new training set is also similar to the original training set which means the subset is heavily imbalanced. Similarly, the weighted cross entropy loss function has been utilized for the SH. We have compared the results based on the SL and SL+SSL approaches. Due to the effect of augmentation on overfitting, the training process was performed with the no and weak augment levels. It is important to separate our work on low data regimes from semi-supervised learning where the number of labels and images for training varies. In fact, in the semisupervised learning method, a model is first pre-trained on usually large amounts of data and then fine-tuned on a limited number of labels. That is, the number of tags and images in semi-supervised learning is different while it is the same in our work.\n\nWe saw that if the Softmax layer removed from the Equation  3 , which acts as a normalizer for the representation, the training process becomes very unstable and sometimes collapses. In particular, this happens in low data regimes.\n\nThe results in table  4  show that when we are using hybrid architecture, the performance increases significantly. To illustrate the important regions in the images, several examples have been selected and the GradCam  [46]  method has been performed on them (figure  11 ). As figure 11 displays, by using HMTL, more attention is paid to the different parts of the images and consequently, the network has a better power in selecting the relevant features from faces.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dimensional",
      "text": "In this section, all the images are included in the AffectNet training set with eight Ekman emotions and the \"None\" label. We have set the alpha coefficient equal to one and used 20 bins to construct categorical labels. Table  4  shows the results.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Feeding puzzled images or cutout images to the network without adding SSH may have the same effect on the SH performance. To look further more into that, at first, the effect of puzzling sizes has been examined. Next, we have looked at the impact of SSHs on the SH.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Puzzle Sizes",
      "text": "In this part, we have showed the effect of puzzle sizes on the SH performance. For this purpose, a no puzzling mode as well as three different puzzle sizes of 2√ó2, 3√ó3, and 4√ó4 have been investigated. In the no puzzling mode, only SH and for the rest, HMTL of puzzling have been considered. In both, the no augment level has been selected and all networks have been trained for 20 epochs. When the puzzle size was set to 4√ó4, the performance of the emotion recognition head showed a great decline. We think it may be due to the presence of unrelated puzzle pieces in the images to learn emotion labels which distracts the focus of learning procedure to different information. As shown in figure  12 , in the 4√ó4 puzzle size, some regions could contain unrelated information for emotion recognition. To look furthermore at this issue, we have given different weights to the SSHs of the puzzled images (Equation  5 ).\n\nWhere:\n\n‚Ä¢ ùúÜ ùëÜùêø : weight for the supervised head ‚Ä¢ ùúÜ ùëó : weight for each self-supervised head\n\nIn fact, in the learning process, these weights are assigned to the SSHs with respect to the shuffling of puzzle pieces. We find these values experimentally.\n\nThe results are shown in figure  13 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Self-Supervised Heads On Fer",
      "text": "Here, the SSHs are removed, but puzzled images are given to the network during the training process. As shown in figure  14 , during the training, the error rate decreases slower than when SSHs are next to SH, making it harder to recognize emotions. In other words, when we only have the SH head, it takes almost twice as many steps to achieve an accuracy of 50% in the validation set. We observed that as the number of puzzles and augmentation intensity is increased, this difference becomes greater. Similarly, when the average accuracy in SSHs reaches above 80%, the emotion loss function decreases more rapidly. It shows the important role of SSHs in distinguishing different parts of faces before recognizing emotions (table  6 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Barlow Twins Self-Supervised Learning",
      "text": "The new wave of self-supervised learning methods (i.e., non-contrastive selfsupervised learning methods) is closing the gap between self-supervised and supervised learning  [7, 52, 53] . Barlow twins  [8]  is a great example of them which makes the self-supervised pre-training stage to be simpler and easier than other techniques, i.e., BYOL, DINO. In fact, Barlow Twins essentially is a siamese network which tries to find mutual information between two views of an image based on the covariance-correlation matrix via redundancy reduction.\n\nTo demonstrate a comparison, we pre-trained a B0 network using Barlow Twins SSL on the AffectNet training set in similar configuration of section 4.2. In addition, we used the strong augment level twice to create two different views for every sample in the training set. Despite Barlow Twins extracted good features in pre-training stage, it did not perform better than SL when doing finetuning on AffectNet labels. In table  7  we have compared Barlow Twins features using linear evaluation and fine-tuning on the AffectNet training set and then evaluated on the AffectNet validation set. Table  6  The difference between using SSHs. The backbone of all approaches is B0 and All of them are trained from the scratch with the no augment level except the in-painting method which uses cutout. Need to mention that the \"SL + in-painting w/o SSL\" is identical to the \"SL + cutout\".   7  The backbone of all approaches is B0. All of the puzzling methods are 3√ó3. The In-painting pre-training method is a CNN decoder on top of the backbone which is trained based on the PWL function. In SSL Barlow twins, feature evaluation is a linear classifier on top of the backbone and the rest is based on section 4.3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Accuracy",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Macro",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Facial Emotion Recognition Benchmarks",
      "text": "AffectNet is a kind of challengeable dataset due to its different kinds of faces within, as well as its diversity. A network that creates a general representation in the field of FER should be able to identify important features in different benchmarks. Our view for AffectNet is that a network that can extract main features in this dataset, also would act as a domain generalization in emotion recognition and potentially, dealing better with out-of-distribution samples. Therefore, the pre-train network trained by the HMTL 3√ó3 puzzling approach (weak augmentation and random weights) is selected.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Affwild",
      "text": "AffWild  [54]  is an in-the-wild database for analysis of continuous emotion dimensions (e.g., arousal and valence) in which more than 15 hours of data in 300 videos are annotated with regards to arousal and valence values. Also, 252 of the videos are provided as the training set that contains the videos and their corresponding annotation. Since, the competition has been ended, we have selected 16 videos from the training set to create a validation set (appendix part D). During the preprocessing step, we encountered with a problem in annotations which is described in the appendix part D.\n\nTo build our model, similar to the baseline paper  [55] , faces in a video are cropped and then, the sequence of frames is split into window sizes of 32 to train by a bidirectional GRU with two layers of 64 units. At first, the model encodes faces with the B0 backbone to vectors (representation) and next, gives the representation of faces to the GRU network to estimate arousal and valence values for each frame. We initialized the backbone with different weights to compare each method together. To prevent the model from overfitting, we set a dropout layer (0.3) after the encoder model. The results are placed in table  8 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Afew-Va",
      "text": "AFEW-VA  [56]  is based on video sequences. First, faces region in the frames are cropped based on the bounding boxes and converted to fix vectors by the frozen pre-trained HMTL 3√ó3 puzzling and SL models. In this dataset, we consider the task of estimating the values of valence and arousal in each frame according to the Circumplex model. We used a bidirectional LSTM with a single layer with the window size of 32. Table  9  shows the results.\n\nTable  9  Evaluation of the frozen HMTL 3√ó3 puzzling and SL features which pre-trained on AffectNet and then evaluated on AFEW-VA dataset using 10-fold cross validation method. The compared method is trained directly on AFEW-VA dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Methods",
      "text": "Valence (RMSE) Arousal (RMSE)  [56]  0.26 0.22 SL-eval (B0) 0.269 0.252 SL+ SSL puzzling-eval (B0) 0.261 0.243",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ck+",
      "text": "CK+  [57]  is based on video sequences for 10 subjects. First, faces region in the frames are cropped based on the bounding boxes and converted to vectors by the frozen pre-trained HMTL 3√ó3 puzzling and SL models. In this dataset, we consider the task of classifying of frame sequences into seven emotion categories. We used bidirectional LSTM with a single layer. Since there are 10 subjects in CK+, 10-Fold cross validation method has been used which each subject is placed in each Fold. Table  10  shows the results.\n\nTable  10  Evaluation of the frozen HMTL 3√ó3 puzzling and SL features which pre-trained on AffectNet and then evaluated on CK + dataset based on 10-fold cross validation. The two compared methods are trained directly on CK+ dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Methods",
      "text": "Accuracy (%)  [29]  98  [58]  98.06 SL-eval (B0) 97.87 SL+ SSL puzzling (B0)-eval 98.23",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Jaffe",
      "text": "For JAFEE samples, each image converted to a vector by the frozen pre-trained HMTL 3√ó3 puzzling and SL models. Vectors are trained by linear classification into seven categories of emotion. In order to evaluate, like CK+, a 10-Fold cross validation method is used that each subject is placed in each fold. Table  11shows  the result.\n\nTable  11  Linear evaluation of the frozen HMTL 3√ó3 puzzling and SL features which pretrained on AffectNet and then evaluated on JAFFE using 10-fold cross validation evaluation. The compared method is trained directly on the dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Methods",
      "text": "Accuracy (%)  [29]  92.8 SL-eval (B0) 77.6 SL+ SSL puzzling-eval (B0) 79.88",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Hybrid Multi-Task Learning On Other Facial Tasks",
      "text": "In this section, we have investigated our HMTL approach for two face related tasks: head pose estimation and gender recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Hybrid Multi-Task Learning On Fine-Grained Head Pose Estimation",
      "text": "We have investigated our hybrid approach on the head pose estimation problem. The HopeNet method  [44]  is considered as a baseline for our comparison. The 300W-LP dataset  [59]  is chosen to train all methods. 300W-LP synthesizes faces to generate 61,225 samples across various poses. We have used random zoom, down sampling, image blurring, and cutout augmentation during training. After training, the mean average error of Euler angles on the AFLW2000 dataset  [59]  has been reported. Also, just like the baseline, we have removed the images with labels of larger than the absolute value of 99 degrees.\n\nIn table  12  and figure 15 methods on three SHs (yaw, roll, and pitch) are reported. Furthermore, to show the impact of HMTL, the SSHs are removed from the backbone at the same puzzling configuration.    All the models are selected based on the lowest loss value on the validation set. All the above methods are trained on the no augment setting. Need to be mentioned that the \"SL + in-painting w/o SSL\" is identical to the \"SL + cutout\".",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Adversarial Robustness",
      "text": "An adversarial attack consists of subtly modifying an original image which changes are almost undetectable to the human eye. The modified image is called an adversarial image, and when submitted to a classifier, the network is going to misclassify it, while the original one is correctly classified. One of the wellknown and fast methods to create adversarial examples is the Fast gradient sign method (FGSM) which first has been introduced in [61] (Equation  6 ).\n\nWhere: We have used this method to create adversarial examples when evaluating the models on the AffectNet validation set. For this purpose, we have taken trained models and made adversarial examples with different epsilons and then, evaluated the networks on them. The evaluation is performed on the AffectNet validation set (figure  16 ). B0 model is chosen for all backbones. All models are trained with the no augment level and dropout using Adabelief optimizer. Although the SL+SSL 3√ó3 puzzling got better results than the SL, as the puzzle size increased, it has shown more performance loss when using the FGSM attack. In contrast to puzzling, via the in-painting auxiliary head, adversarial robustness is hugely improved.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "In this article, three subjects are investigated:\n\n‚Ä¢ The effect of ImageNet transfer learning and random weights on the FER with different levels of augmentation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "‚Ä¢",
      "text": "The effectiveness of the puzzling, rotating, and in-painting selfsupervision pre-training features on the FER.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "‚Ä¢",
      "text": "The impact of adding auxiliary self-supervised tasks on the FER.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Imagenet Transfer Learning Vs Training From Scratch",
      "text": "With the advent of deep learning, transfer learning has become extremely popular and has shown good performance in many datasets  [62, 63] . In this paper, besides the random weights, ImageNet weights are chosen as the first transfer learning option due to the challenging of ImageNet dataset. Also, the effect of different levels of augmentation is examined. The results of this section are summarized as follows:\n\n‚Ä¢ When the data size is small and the augmentation intensity is weak, finetuning on the ImageNet weights helps to increase the accuracy compared to the random weights.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "‚Ä¢",
      "text": "By intensify the augmentation, training from the scratch have better results than fine-tuning on the ImageNet weights.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Self-Supervised Pre-Training",
      "text": "SSL has shown significant results in recent years and its distance from the SL has been reduced in the computer vision benchmarks. In this paper, two types of SSL methods are investigated on AffectNet, pre-text task and nonecontrastive SSL. Each type is evaluated on the AffectNet benchmark by a classifier (i.e., evaluating the fixed image representations of the frozen backbones). The results of this evaluation showed that the puzzling and inpainting with pixel-wise loss methods provide a better representation for emotion recognition from faces compared to the rotation and puzzling-rotation methods. In the following, the effect of fine-tuning on the pre-train methods has been investigated which has exposed that fine-tuning on puzzling pre-training weights and puzzling-rotation pre-training weights can gain better results versus training from scratch.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Hybrid Multi-Task Learning",
      "text": "According to our proposed hypothesis, using HMTL can improve the results for the SHs. This hypothesis is tested on the FER, as well as head pose estimation and gender recognition benchmarks. By adding the auxiliary puzzling or inpainting with perceptual loss heads in the training, the accuracy and error rate are increased which concludes to the state-of-the-arts result on AffectNet.\n\nThe important points in this section are as follows:\n\n‚Ä¢ As puzzle size is increased, the error rate for three tasks of FER, head pose estimation and gender recognition is going to decrease.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "‚Ä¢",
      "text": "On all the augment levels, reaching to overfitting is significantly reduced.\n\nIn our opinion, this can at least have two reasons in the puzzling methods. First, when using the multi-tasking learning method, the sensitivity to overfitting is decreased. Second, the puzzling method creates a lot of perturbation. For example, in the 3√ó3 puzzling approach, each image can be puzzled in 9! ways.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "‚Ä¢",
      "text": "As the input images get puzzled, by removing SSHs from the backbone, the performance slightly decreases compared to the classical SL. Additionally, it takes almost twice as many steps to achieve its best result.\n\nAs the number of puzzle slices gets bigger and the intensity of the augmentation increases, the amounts of steps to achieve convergence increase. At the same configuration, on average, when it switches from the SL to the HMTL, the steps for reaching the best result increased by 30%. Yet, on AffectNet, when 3√ó3 puzzle size is selected, the error rate is reduced to its best result. Due to the two-stage training of the in-painting method, we cannot compare it directly.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "‚Ä¢",
      "text": "In the low data regime, using selected 20% of training data randomly (i.e., with the same distribution of the original), the effect of the hybrid learning is more tangible, especially when least augmentation is used.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitations And Future Works",
      "text": "We believe that using auxiliary self-supervised tasks is going to improve the performance on the other domains as well, but we could not test it except three types of fine-grained facial benchmarks. In this article, we encountered problems such as selecting appropriate SSL tasks, adjusting tasks weights in multi-task learning and instability in HMTL training. Items that can impact on overall HMTL performance and need further investigation are listed down below:\n\n1. How to select best architecture when using self-supervised auxiliary tasks?\n\nSelecting appropriate SSL tasks alongside SL requires reviewing and testing each of them precisely. As different tasks have different impact on SHs, finding the best backbone architecture is crucial and may show different result  [16] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "How To Assign Weights For Different Tasks In Mtl Setting?",
      "text": "Tuning weights of self-surprise heads have had an impact on the results. Finding the appropriate weights for each task is a problem we encountered with. For example, in head pose estimation, the decoder error value is very small compared to the SHs based on the HopeNet loss function and when we assign a bigger coefficient for the SSHs, training become very unstable to converge. This issue also happens more often at low data regime for the in-painting with perceptual loss HMTL. One interesting solution for this can be using different losses like weighting by uncertainty  [64]  or dynamic weight assigning  [65] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "How Ssl Can Be Helpful For Downstream Tasks?",
      "text": "We have seen some interesting points from our study which shows the difference between using SSL pre-training and adding SSL auxiliary tasks for a downstream supervised task (here emotion recognition). In contrast to our initial thought, when we are pre-training based on a SSL task which can extract good features for the downstream task, that is not necessarily the best option to be used as the fine-tuning on that task as we see the inpainting-pwl results in table 2 and 3. Similarly, it is observed that if finetuning on a SSL pre-training can improve the performance, it does not mean it is always good to be used as the auxiliary task besides SL.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Which Ssl Task Should Be Used Besides Sl?",
      "text": "In AffectNet, the puzzling and the in-painting with perceptual loss help to reduce the error rate, and on the flip side, the in-painting method with pixel wise loss, rotating and rotating-puzzle neither change the results and sometimes make it worse than using classical SL. Fortunately, there are bunch of SSL methods which have been created in recent years. Finding ways of adding those to SL is a good way to show the effectiveness of using HMTL.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this article, we examined the effect of transfer learning and random weights on AffectNet, and we observed that using random weights could be better than transfer learning when enough augmentation is applied. Moreover, we suggested that using HMTL (i.e., adding auxiliary self-supervised learning tasks to a supervised task) could improve the supervised task representation. SSHs only are used in the training stage and in the testing stage or inference time they are removed from the trained model. To do this, we chose proposed puzzling and in-painting SSL pre-text tasks to add them as the auxiliary heads to the supervised emotion recognition head. Results showed that utilizing proper self-supervised tasks could increase the accuracy of emotion recognition in different benchmarks both at different levels of augmentation and low amounts of data. With two proposed HMTL methods, we reached two new state-of-the-art result on AffectNet in the eight-emotion mode without using additional training data. We also evaluated our method on the head pose estimation and gender recognition datasets which concluded to decreasing in error rate.\n\nWe believe utilizing auxiliary self-supervised task besides a supervised task would impact many fields even beyond face-related problems. This article just scratched the surface and we hope it sets new directions for future research.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: According to Russel and Ekman model, Data labeling requires experts and it is",
      "page": 1
    },
    {
      "caption": "Figure 1: Circumplex model with arousal and valence axis [3].",
      "page": 1
    },
    {
      "caption": "Figure 2: ). We use EfficientNet B0 and B2 [41] as",
      "page": 3
    },
    {
      "caption": "Figure 2: Supervised Learning (SL) procedure with different augmentation levels and weights.",
      "page": 3
    },
    {
      "caption": "Figure 3: ) at first, an image is sliced to N identical size square regions,",
      "page": 3
    },
    {
      "caption": "Figure 3: , the image is divided into four regions",
      "page": 3
    },
    {
      "caption": "Figure 4: ). In this type of in-painting, we only use the PL which is",
      "page": 3
    },
    {
      "caption": "Figure 4: a). Then, we cut a square with fix",
      "page": 3
    },
    {
      "caption": "Figure 4: b). In other words, the input images",
      "page": 3
    },
    {
      "caption": "Figure 3: In our SSL puzzling procedure, at first, an image split into many pieces. Next, the",
      "page": 3
    },
    {
      "caption": "Figure 4: Our proposed two stage in-painting with PL pre-task consists of two stages. (a) An",
      "page": 4
    },
    {
      "caption": "Figure 7: shows the procedure of calculating the loss function (equation 4) in",
      "page": 4
    },
    {
      "caption": "Figure 8: ), in the validation set for each",
      "page": 4
    },
    {
      "caption": "Figure 8: ). In the validation set, there are 500 images for each category. Since",
      "page": 4
    },
    {
      "caption": "Figure 5: Training SL by combining auxiliary SSL task to it. In the learning procedure, before feeding inputs to the networks, images are divided into nine regions and these regions are shuffled",
      "page": 5
    },
    {
      "caption": "Figure 6: Training SL by combining auxiliary SSL task to it. Training consists of two stages. First, we train an offline backbone from the scratch under a pre-defined augmentation level. Then, in the",
      "page": 5
    },
    {
      "caption": "Figure 7: Converting regression to the categorical-regression of valence and arousal values in",
      "page": 5
    },
    {
      "caption": "Figure 8: AffectNet labels distribution in the training set. The training set is heavily",
      "page": 5
    },
    {
      "caption": "Figure 9: Comparing the fine-tuning and the training from scratch modes in different levels of",
      "page": 6
    },
    {
      "caption": "Figure 6: and figure 7 are illustrating, the SSHs are put",
      "page": 6
    },
    {
      "caption": "Figure 10: and table 5.",
      "page": 7
    },
    {
      "caption": "Figure 11: ). As figure 11 displays, by using HMTL,",
      "page": 7
    },
    {
      "caption": "Figure 10: Comparing all settings of SL and HMTL methods with each other. The in-painting-",
      "page": 7
    },
    {
      "caption": "Figure 11: Using GradCam method to show important regions on emotion classification. The",
      "page": 8
    },
    {
      "caption": "Figure 13: 4.5.2. Effect of self-supervised heads on FER",
      "page": 8
    },
    {
      "caption": "Figure 14: , during the training, the error",
      "page": 8
    },
    {
      "caption": "Figure 12: In the 4√ó4 puzzling setting (a) Every piece has a different emotion information",
      "page": 8
    },
    {
      "caption": "Figure 13: Effect of increasing of puzzle sizes on emotion recognition in the AffectNet",
      "page": 8
    },
    {
      "caption": "Figure 14: Effect of the SSHs on training with 2√ó2 and 3√ó3 puzzling images. (a) is loss curve",
      "page": 9
    },
    {
      "caption": "Figure 15: methods on three SHs (yaw, roll, and pitch) are",
      "page": 9
    },
    {
      "caption": "Figure 15: The effect of adding SSHs on the SH based on the average error rate in different",
      "page": 9
    },
    {
      "caption": "Figure 16: The results on the AffectNet validation set with different epsilons for FGSM",
      "page": 10
    },
    {
      "caption": "Figure 16: ). B0 model is chosen for all backbones. All models are",
      "page": 10
    },
    {
      "caption": "Figure 1: AffWild original face annotations. In many frames, bounding boxes and",
      "page": 13
    },
    {
      "caption": "Figure 2: Impact of SSH on training with different puzzling sizes for average error of head",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results of SL methods with the difference in weights and augmentation level. The backbones on AffectNet (table 3). In both nonlinear evaluation and fine-tuning",
      "data": [
        {
          "Approach": "SL \nSL \nSL \nSL \nSL \nSL",
          "Augment level": "No  \nWeak \nStrong \nNo \nWeak \nStrong",
          "Pre-training weights": "- \n- \n- \nImageNet \nImageNet \nImageNet",
          "Accuracy (%)": "57.03 \n60.09 \n60.34 \n59.3 \n59.57 \n60.17"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Results of SL methods with the difference in weights and augmentation level. The backbones on AffectNet (table 3). In both nonlinear evaluation and fine-tuning",
      "data": [
        {
          "Methods": "SL \nSL \nSL \nSL \nSL \nSL",
          "Augment level": "No  \nNo \nNo \nNo \nNo \nNo",
          "Pre-training weights": "Random initialization \nAffectNet (SSL puzzling) \nAffectNet (SSL rotation) \nAffectNet (SSL puzzling-rotation) \nAffectNet (SSL inpainting-pwl) \nImageNet (SL)",
          "Accuracy (%)": "57.03 \n57.56 \n54.26 \n58.86 \n51.84 \n59.3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: We need to emphasize that the number of emotions is an important factor in the",
      "data": [
        {
          "Approach": "Supervised Learning",
          "Method": "ResNet50 [11] \nESR-9 [47] \nRAN [49] \nPSR [51]",
          "Classes": "8 \n8 \n8 \n8",
          "Augment level": "‚âàWeak \nUnknown \nUnknown \nUnknown",
          "Backbone pre-training weights": "- \n- \nMS-Celeb-1M [48] \nDIV2K (STN) [50]",
          "Accuracy (%)": "58.0 \n59.3 \n59.5 \n60.68",
          "Difference (%)": "-2.09 \n-0.79 \n-0.59 \n0.59"
        },
        {
          "Approach": "Supervised Learning",
          "Method": "SL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B0) \nSL (B2)",
          "Classes": "8 \n8 \n8 \n8 \n8 \n8 \n8 \n8 \n8 \n8",
          "Augment level": "No \nWeak \nStrong \nNo \nNo \nNo \nNo \nWeak \nStrong \nWeak",
          "Backbone pre-training weights": "- \n- \n- \nAffectNet (SSL puzzling) \nAffectNet (SSL rotation) \nAffectNet (SSL puzzling-rotation) \nImageNet \nImageNet \nImageNet \nImageNet",
          "Accuracy (%)": "57.03 \n60.09 \n60.34 \n57.56 \n54.26 \n58.86 \n59.3 \n59.57 \n60.17 \n60.35",
          "Difference (%)": "-3.06 \n0.00 \n0.25 \n-2.53 \n-5.83 \n-1.23 \n-0.79 \n-0.52 \n0.08 \n0.26"
        },
        {
          "Approach": "Hybrid Multi-Task Learning",
          "Method": "SL + SSL puzzling-rotation (B0) \nSL + SSL in-panting-pwl (B0) \nSL + SSL in-panting-pl (B0) \nSL + SSL in-panting-pl (B0) \nSL + SSL puzzling (B0) \nSL + SSL puzzling (B0) \nSL + SSL puzzling (B2)",
          "Classes": "8 \n8 \n8 \n8 \n8 \n8 \n8",
          "Augment level": "No \nNo + Cutout \nNo + Cutout \nWeak + Cutout \nNo \nWeak \nWeak",
          "Backbone pre-training weights": "- \n- \n- \n- \n- \n- \n-",
          "Accuracy (%)": "55.21 \n56.78 \n58.76 \n61.72 \n58.11 \n61.09 \n61.32",
          "Difference (%)": "-4.88 \n-3.31 \n-1.33 \n1.63 \n-1.98 \n1.00 \n1.23"
        },
        {
          "Approach": "20% of Training Data",
          "Method": "SL (B0) \nSL (B0) \nSL+ SSL puzzling (B0) \nSL+ SSL puzzling (B0) \nSL+ SSL in-painting-pl (B0)",
          "Classes": "8 \n8 \n8 \n8 \n8",
          "Augment level": "No \nWeak \nNo \nWeak \nWeak + Cutout",
          "Backbone pre-training weights": "- \n- \n- \n- \n-",
          "Accuracy (%)": "43.59 \n52.46 \n52.11 \n54.98 \n55.36",
          "Difference (%)": "- \n- \n- \n- \n-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: we have compared Barlow Twins features",
      "data": [
        {
          "0.15": "0.35",
          "0.50": "1.00"
        },
        {
          "0.15": "0.35",
          "0.50": "1.00"
        },
        {
          "0.15": "0.15",
          "0.50": "0.80"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: we have compared Barlow Twins features",
      "data": [
        {
          "Methods": "SL",
          "Accuracy (%)": "57.03"
        },
        {
          "Methods": "SL + 2√ó2 puzzling w/o SSL \nSL + 3√ó3 puzzling w/o SSL \nSL + in-painting w/o SSL",
          "Accuracy (%)": "55.89 \n55.69 \n57.31"
        },
        {
          "Methods": "SL + SSL 2√ó2 puzzling \nSL + SSL 3√ó3 puzzling \nSL + SSL in-painting",
          "Accuracy (%)": "57.36 \n58.11 \n58.71"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: we have compared Barlow Twins features",
      "data": [
        {
          "Backbone weights": "Barlow Twins \nIn-painting-pwl \nPuzzling \nRandom initialization",
          "Methods": "Linear eval \nNonlinear eval \nNonlinear eval \nNonlinear eval",
          "Augment": "No \nNo \nNo \nNo",
          "Accuracy (%)": "41.51 \n34.41 \n32.98 \n12.5",
          "Macro F1": "0.4116 \n0. 3372 \n0.3227 \n0.04"
        },
        {
          "Backbone weights": "AffectNet SSL Barlow Twins  \n(fine-tuning)",
          "Methods": "SL \nSL",
          "Augment": "Weak \nStrong",
          "Accuracy (%)": "59.11 \n59.94",
          "Macro F1": "- \n-"
        },
        {
          "Backbone weights": "ImageNet SL (fine-tuning)",
          "Methods": "SL \nSL",
          "Augment": "Weak \nStrong",
          "Accuracy (%)": "59.57 \n60.17",
          "Macro F1": "- \n-"
        },
        {
          "Backbone weights": "Random initialization",
          "Methods": "SL \nSL \nSL + SSL puzzling \nSL + SSL in-panting-pl",
          "Augment": "Weak \nStrong \nWeak \nWeak + Cutout",
          "Accuracy (%)": "60.09 \n60.34 \n61.09 \n61.72",
          "Macro F1": "- \n- \n- \n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 10: shows the results.",
      "data": [
        {
          "Backbone weights": "",
          "Resolution": "",
          "Valence": "CCC \nMSE",
          "Arousal": "CCC \nMSE",
          "Mean": "CCC \nMSE"
        },
        {
          "Backbone weights": "Random",
          "Resolution": "112√ó112 \n224√ó224",
          "Valence": "0.158 \n0.148 \n0.268 \n0.093",
          "Arousal": "0.199 \n0.199 \n0.245 \n0.102",
          "Mean": "0.179 \n0.118 \n0.256 \n0.097"
        },
        {
          "Backbone weights": "SL pre-training",
          "Resolution": "112√ó112 \n224√ó224",
          "Valence": "0.165 \n0.137 \n0.281 \n0.11",
          "Arousal": "0.205 \n0.091 \n0.292 \n0.102",
          "Mean": "0.185 \n0.114 \n0.286 \n0.106"
        },
        {
          "Backbone weights": "HMTL pre-training",
          "Resolution": "112√ó112 \n224√ó224",
          "Valence": "0.194 \n0.124 \n0.342 \n0.09",
          "Arousal": "0.204 \n0.094 \n0.312 \n0.103",
          "Mean": "0.199 \n0.109 \n0.327 \n0.097"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 12: The effect of HMTL on the mean average error of head pose estimation. All We have used this method to create adversarial examples when evaluating the",
      "data": [
        {
          "Method": "SL (35 epochs)",
          "Accuracy (%)": "91.51 (¬±0.02)"
        },
        {
          "Method": "SL + in-painting w/o SSL (40 epoch) \nSL + SSL in-painting-pl",
          "Accuracy (%)": "91.59 (¬±0.02) \n92.12 (¬±0.01)"
        },
        {
          "Method": "SL + 2√ó2 puzzling w/o SSL (35 epochs) \nSL + SSL 2√ó2 puzzling (25 epochs)",
          "Accuracy (%)": "91.33 (¬±0.03) \n91.98 (¬±0.01)"
        },
        {
          "Method": "SL + 3√ó3 puzzling w/o SSL (45 epochs) \nSL + SSL 3√ó3 puzzling (35 epochs)",
          "Accuracy (%)": "91.58 (¬±0.04) \n92.41 (¬±0.01)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: found them quite inaccurate and noisy, nevertheless. Additionally, in some",
      "data": [
        {
          "Transformation": "Horizontal flip",
          "Level 1 \n(No \naugment)": "‚úì",
          "Level 2 \n(Weak \naugment)": "‚úì",
          "Level 3 \n(Strong augment)": "‚úì"
        },
        {
          "Transformation": "Central Zoom",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "‚úì (0.69 to 100)",
          "Level 3 \n(Strong augment)": "‚úì (0.69 to 100)"
        },
        {
          "Transformation": "Contrast",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "‚úì (0.6 to 1.4)",
          "Level 3 \n(Strong augment)": "‚úì (0.6 to 1.4)"
        },
        {
          "Transformation": "Rotation",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "‚úì (-15¬∞ to 15¬∞)",
          "Level 3 \n(Strong augment)": "‚úì (-20¬∞ to 20¬∞)"
        },
        {
          "Transformation": "Brightness",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "ÔÉè",
          "Level 3 \n(Strong augment)": "‚úì (-0.05 to 0.05)"
        },
        {
          "Transformation": "RGB channel swap",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "ÔÉè",
          "Level 3 \n(Strong augment)": "‚úì"
        },
        {
          "Transformation": "Blurring",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "ÔÉè",
          "Level 3 \n(Strong augment)": "‚úì (1, 3, 5 filter size)"
        },
        {
          "Transformation": "Gaussian noise",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "ÔÉè",
          "Level 3 \n(Strong augment)": "‚úì (mean=0, var=0.05)"
        },
        {
          "Transformation": "Cutout",
          "Level 1 \n(No \naugment)": "ÔÉè",
          "Level 2 \n(Weak \naugment)": "ÔÉè",
          "Level 3 \n(Strong augment)": "‚úì (60√ó60)"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A brief review of facial emotion recognition based on visual information. Sensors (Switzerland)",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "A brief review of facial emotion recognition based on visual information. Sensors (Switzerland)"
    },
    {
      "citation_id": "2",
      "title": "Image based static facial expression recognition with multiple deep network learning",
      "authors": [
        "Z Yu",
        "C Zhang"
      ],
      "year": "2015",
      "venue": "ICMI 2015 -Proceedings of the 2015 ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J Pers Soc Psychol"
    },
    {
      "citation_id": "4",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "J Pers Soc Psychol"
    },
    {
      "citation_id": "5",
      "title": "Reexamining the circumplex model of affect",
      "authors": [
        "N Remington",
        "L Fabrigar",
        "P Visser"
      ],
      "year": "2000",
      "venue": "J Pers Soc Psychol"
    },
    {
      "citation_id": "6",
      "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
      "authors": [
        "T Chen",
        "S Kornblith",
        "K Swersky",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Bootstrap your own latent a new approach to self-supervised learning",
      "authors": [
        "J Grill",
        "F Strub",
        "F Altch√©",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya"
      ],
      "year": "2020",
      "venue": "Adv Neural Inf Process Syst"
    },
    {
      "citation_id": "8",
      "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
      "authors": [
        "J Zbontar",
        "L Jing",
        "I Misra",
        "Y Lecun",
        "S Deny"
      ],
      "venue": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction"
    },
    {
      "citation_id": "9",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "M Caron",
        "H Touvron",
        "I Misra",
        "H J√©gou",
        "J Mairal",
        "P Bojanowski"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "When does contrastive visual representation learning work? arXiv Prepr arXiv210505837",
      "authors": [
        "E Cole",
        "X Yang",
        "K Wilber",
        "Mac Aodha",
        "O Belongie"
      ],
      "year": "2021",
      "venue": "When does contrastive visual representation learning work? arXiv Prepr arXiv210505837"
    },
    {
      "citation_id": "11",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Trans Affect Comput"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised representation learning by predicting image rotations",
      "authors": [
        "S Gidaris",
        "P Singh",
        "N Komodakis"
      ],
      "year": "2018",
      "venue": "Unsupervised representation learning by predicting image rotations",
      "arxiv": "arXiv:1803.07728"
    },
    {
      "citation_id": "13",
      "title": "Learning image representations by completing damaged jigsaw puzzles",
      "authors": [
        "D Kim",
        "D Cho",
        "D Yoo",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "14",
      "title": "A critical analysis of selfsupervision, or what we can learn from a single image",
      "authors": [
        "Y Asano",
        "C Rupprecht",
        "A Vedaldi"
      ],
      "year": "2019",
      "venue": "A critical analysis of selfsupervision, or what we can learn from a single image"
    },
    {
      "citation_id": "15",
      "title": "Which tasks should be learned together in multi-task learning?",
      "authors": [
        "T Standley",
        "A Zamir",
        "D Chen",
        "L Guibas",
        "J Malik",
        "S Savarese"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Multi-task learning with deep neural networks: A survey",
      "authors": [
        "M Crawshaw"
      ],
      "venue": "Multi-task learning with deep neural networks: A survey",
      "arxiv": "arXiv:2009.09796.2020"
    },
    {
      "citation_id": "17",
      "title": "Deep structured learning for facial action unit intensity estimation",
      "authors": [
        "R Walecki",
        "V Pavlovic",
        "B Schuller",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Covariance pooling for facial expression recognition",
      "authors": [
        "D Acharya",
        "Z Huang",
        "Pani Paudel",
        "Van Gool"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "20",
      "title": "Using CNN for facial expression recognition: a study of the effects of kernel size and number of filters on accuracy",
      "authors": [
        "A Agrawal",
        "N Mittal"
      ],
      "venue": "Using CNN for facial expression recognition: a study of the effects of kernel size and number of filters on accuracy"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "Vis Comput"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "Fine-grained facial expression analysis using dimensional emotion model",
      "authors": [
        "F Zhou",
        "S Kong",
        "C Fowlkes",
        "T Chen",
        "B Lei"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "23",
      "title": "A deep learning perspective on the origin of facial expressions",
      "authors": [
        "R Breuer",
        "R Kimmel"
      ],
      "year": "2017",
      "venue": "A deep learning perspective on the origin of facial expressions"
    },
    {
      "citation_id": "24",
      "title": "Facial expression recognition using enhanced deep 3D convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "25",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Joint fine-tuning in deep neural networks for facial expression recognition"
    },
    {
      "citation_id": "26",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Trans Affect Comput"
    },
    {
      "citation_id": "27",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "Ebrahimi Kahou",
        "S Michalski",
        "V Konda",
        "K Memisevic",
        "R Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "28",
      "title": "A visual attention based ROI detection method for facial expression recognition",
      "authors": [
        "W Sun",
        "H Zhao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "Lossless attention in convolutional networks for facial expression recognition in the wild. arXiv Prepr arXiv",
      "authors": [
        "C Wang",
        "R Hu",
        "M Hu",
        "J Liu",
        "T Ren",
        "S He"
      ],
      "year": "2020",
      "venue": "Lossless attention in convolutional networks for facial expression recognition in the wild. arXiv Prepr arXiv"
    },
    {
      "citation_id": "30",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "31",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "D Meng",
        "X Peng",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "IEEE international conference on image processing (ICIP)"
    },
    {
      "citation_id": "32",
      "title": "Occlusion aware facial expression recognition using CNN with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Trans Image Process"
    },
    {
      "citation_id": "33",
      "title": "Facial Expression Recognition using Convolutional Neural Network on Graphs",
      "authors": [
        "C Wu",
        "L Chai",
        "J Yang",
        "Y Sheng"
      ],
      "year": "2019",
      "venue": "Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "34",
      "title": "Identity-free facial expression recognition using conditional generative adversarial network",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "O' Reilly",
        "J Li",
        "Z Han"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Self-supervised visual feature learning with deep neural networks: A survey",
      "authors": [
        "L Jing",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "IEEE Trans Pattern Anal Mach Intell"
    },
    {
      "citation_id": "36",
      "title": "A framework for contrastive self-supervised learning and designing a new approach",
      "authors": [
        "W Falcon",
        "K Cho"
      ],
      "year": "2020",
      "venue": "A framework for contrastive self-supervised learning and designing a new approach"
    },
    {
      "citation_id": "37",
      "title": "Context Encoders: Feature Learning by Inpainting",
      "authors": [
        "D Pathak",
        "P Krahenbuhl",
        "J Donahue",
        "T Darrell",
        "A Efros"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised learning visual representations by solving jigsaw puzzles",
      "authors": [
        "M Noroozi",
        "P Favaro"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "39",
      "title": "Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning",
      "authors": [
        "C Wei",
        "L Xie",
        "X Ren",
        "Y Xia",
        "C Su",
        "J Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "A systematic review on imbalanced data challenges in machine learning: Applications and solutions",
      "authors": [
        "H Kaur",
        "H Pannu",
        "A Malhi"
      ],
      "year": "2019",
      "venue": "ACM Comput Surv"
    },
    {
      "citation_id": "41",
      "title": "Focal Loss for Dense Object Detection",
      "authors": [
        "T Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollar"
      ],
      "year": "2020",
      "venue": "IEEE Trans Pattern Anal Mach Intell"
    },
    {
      "citation_id": "42",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Le Efficientnet"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "43",
      "title": "Deep multi-task learning to recognise subtle facial expressions of mental states",
      "authors": [
        "G Hu",
        "L Liu",
        "Y Yuan",
        "Z Yu",
        "Y Hua",
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "44",
      "title": "Increasingly packing multiple facial-informatics modules in a unified deep-learning model via lifelong learning",
      "authors": [
        "Scy Hung",
        "J-H Lee",
        "Tst Wan",
        "C-H Chen",
        "Y-M Chan",
        "C-S Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "45",
      "title": "Fine-grained head pose estimation without keypoints",
      "authors": [
        "N Ruiz",
        "E Chong",
        "J Rehg"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "46",
      "title": "AdaBelief optimizer: Adapting stepsizes by the belief in observed gradients",
      "authors": [
        "J Zhuang",
        "T Tang",
        "Y Ding",
        "S Tatikonda",
        "N Dvornek",
        "X Papademetris"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Grad-cam: Visual explanations from deep networks via gradientbased localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Grad-cam: Visual explanations from deep networks via gradientbased localization"
    },
    {
      "citation_id": "48",
      "title": "Efficient facial feature learning with wide ensemble-based convolutional neural networks",
      "authors": [
        "H Siqueira",
        "S Magg",
        "S Wermter"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition"
    },
    {
      "citation_id": "50",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Trans Image Process"
    },
    {
      "citation_id": "51",
      "title": "Ntire 2017 challenge on single image superresolution: Dataset and study",
      "authors": [
        "E Agustsson",
        "R Timofte"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "52",
      "title": "Pyramid with super resolution for in-the-wild facial expression recognition",
      "authors": [
        "T-H Vo",
        "G-S Lee",
        "H-J Yang",
        "S-H Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Emerging Properties in Self-Supervised Vision Transformers",
      "authors": [
        "M Caron",
        "H Touvron",
        "I Misra",
        "H J√©gou",
        "J Mairal",
        "P Bojanowski"
      ],
      "year": "2021",
      "venue": "Proc IEEE/CVF Int Conf Comput Vis"
    },
    {
      "citation_id": "54",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Data2vec: A general framework for self-supervised learning in speech, vision and language"
    },
    {
      "citation_id": "55",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Int J Comput Vis"
    },
    {
      "citation_id": "56",
      "title": "Aff-wild: valence and arousal'In-the-Wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "57",
      "title": "AFEW-VA database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image Vis Comput"
    },
    {
      "citation_id": "58",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognitionworkshops"
    },
    {
      "citation_id": "59",
      "title": "Facial motion prior networks for facial expression recognition",
      "authors": [
        "Y Chen",
        "J Wang",
        "S Chen",
        "Z Shi",
        "J Cai"
      ],
      "year": "2019",
      "venue": "IEEE Visual Communications and Image Processing"
    },
    {
      "citation_id": "60",
      "title": "Face alignment across large poses: A 3d solution",
      "authors": [
        "X Zhu",
        "Z Lei",
        "X Liu",
        "H Shi",
        "S Li"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "61",
      "title": "Face attribute dataset for balanced race, gender, and age. arXiv Prepr arXiv",
      "authors": [
        "K K√§rkk√§inen",
        "J Joo",
        "Fairface"
      ],
      "year": "2019",
      "venue": "Face attribute dataset for balanced race, gender, and age. arXiv Prepr arXiv"
    },
    {
      "citation_id": "62",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples"
    },
    {
      "citation_id": "63",
      "title": "A survey on deep transfer learning",
      "authors": [
        "C Tan",
        "F Sun",
        "T Kong",
        "W Zhang",
        "C Yang",
        "C Liu"
      ],
      "year": "2018",
      "venue": "International conference on artificial neural networks"
    },
    {
      "citation_id": "64",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H-W Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "65",
      "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
      "authors": [
        "R Cipolla",
        "Y Gal",
        "A Kendall"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Dynamic multi-task learning for face recognition with facial expression",
      "authors": [
        "Z Ming",
        "J Xia",
        "M Luqman",
        "J-C Burie",
        "K Zhao"
      ],
      "year": "2019",
      "venue": "Dynamic multi-task learning for face recognition with facial expression"
    }
  ]
}