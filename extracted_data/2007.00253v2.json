{
  "paper_id": "2007.00253v2",
  "title": "Private Speech Classification With Secure Multiparty Computation",
  "published": "2020-07-01T05:26:06Z",
  "authors": [
    "Kyle Bittner",
    "Martine De Cock",
    "Rafael Dowsley"
  ],
  "keywords": [
    "convolutional neural network",
    "deep learning",
    "emotion recognition",
    "privacy",
    "secure multiparty computation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning in audio signal processing, such as human voice audio signal classification, is a rich application area of machine learning. Legitimate use cases include voice authentication and emotion recognition. While there are clear advantages to automated human speech classification, application developers can gain knowledge beyond the professed scope from unprotected audio signal processing. In this paper we propose the first privacy-preserving solution for deep learning based audio classification with Secure Multiparty Computation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech technology is becoming increasingly prevalent and intrusive  [1] . Speech data, i.e. recordings of human speech, are automatically classified for various purposes, extending from user authentication, to control of services and devices, surveillance, and marketing. The developing prevalence of speech audio processing technology stems from the ever-increasing demand of devices and programs that are \"always-listening\" -such as smartphones, televisions, and intelligent digital voice assistants -and the technological improvements in speech technology. Beyond applications that aim to automatically classify speakers or speech, e.g. for authentication or for emotion detection, respectively, there are countless interesting sound classification tasks 2  that may include speech audio processing.\n\nThese include gunfire detection in surveillance, cough sensing in healthcare, and noise mitigation enabled by smart acoustic sensor networks  [2, 3] .\n\nWhile there are apparent benefits to automated speech audio signal recognition,  3  application developers can gain knowledge beyond the professed scope from unprotected audio signals. A wealth of personal data can be extracted from speech audio signals, including age and gender, health and emotional state, racial or ethnic origin, geographical background, social identity, and socio-economic status  [4] . As stated in the recent survey paper by Nautsch et al.  [1] , the continued success of speech technologies hinges upon the development of reliable and efficient privacy-preservation capabilities, specifically designed for the automatic processing of speech signals. Efforts to safeguard the privacy of users in data-driven applications are underway along at least three dimensions:\n\n(1) by laws and regulations such as the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA); (2) by anonymization techniques that aim to suppress personally identifiable information in data  4  ; and (3) by protecting sensitive data through encryption.\n\nIn this paper, we focus on the latter, using techniques from Secure Multiparty Computation (MPC). MPC is an umbrella term for cryptographic approaches that allow two or more parties to jointly compute a specified output from their private information in a distributed fashion, without actually revealing the private information to each other  [5] . As illustrated in Figure  1 , speech classification is inherently a two-party computation (2PC) problem, where one party -nicknamed Alice henceforth -has a speech signal or sound fragment that needs to be classified, and another party -nicknamed Bob -has a machine learning (ML) classifier that can be used to this end. Similar to how Alice does not want to disclose her speech data to Bob, Bob may not want to disclose his ML model to Alice for a variety of reasons. ML models can be expensive to train and usually constitute a competitive advantage. For example, as reported by Dalskov et al.  [6] , the network by Yang et al.  [7]  costs between $61,000 and $250,000 to train  [8] . Furthermore, deep learning models are powerful enough to memorize specific examples from the training data  [9] , hence disclosing a trained model can leak very specific information about the training data, which might be sensitive in itself. Finally, disclosing the trained ML model increases the likelihood that adversaries can develop successful evasion attacks. In the context of speaker or speech characterization, such attacks could consist of altering speech signals to bypass speaker verification systems or to bypass classifiers that detect \"fake speech\", i.e. that detect the use of speech synthesis tools for malicious purposes such as spreading misinformation, harassment and intimidation  [10] .\n\nMPC allows oblivious speech classification through computations over encrypted data. In this way, Alice can classify her speech signal using Bob's model, without Alice revealing her speech signal to anyone in plaintext, and without Bob disclosing his ML model to anyone in-the-clear, i.e. without encryption.\n\nTo this end, Alice and Bob engage in computations, and they exchange intermediate encrypted results by communicating with each other. At the end of the oblivious speech classification protocol, Alice and Bob each have \"shares\" of the inferred class label (e.g. the emotion state of Alice). The true class label is revealed only when these shares are combined, e.g. when, depending on the application, (1) Bob sends his shares to Alice, or  (2)  Alice sends his shares to Bob, or (3) both Alice and Bob send their shares to a third party, like a health care provider who might need to be informed when Alice is not doing well.\n\nMPC has already been used for speaker and speech classification with hidden Markov models (HMMs) and Gaussian mixture models (GMMs)  [11, 12, 13, 14] .\n\nWhile HMMs and GMMs were popular techniques for speech classification in the 1980s and 1990s, more recently deep learning has emerged as a state-of-theart technique in this field. To the best of our knowledge, MPC-based secure classification of speech with deep neural networks has never been studied. It is this gap in the literature, which is also called out by Nautsch et al.  [1] , that we fill in this paper.\n\nSeveral kinds of neural network architectures can be used for speech classification. As cryptographic methods are known to result in significant increases to computational complexity and/or communication overheads  [4] , we choose convolutional neural networks (CNNs), which are computationally less intensive than for instance long short-term memory networks (LSTMs), even without encryption. To the best of our knowledge, all existing work on MPC-based classification with CNNs is developed for and focused on 2-dimensional CNNs, which are commonly used for classification of images. In this paper, we adapt the work that was done in SecureQ8  [6]  for 2-dimensional CNNs (image classification) to 1-dimensional CNNs (speech classification). While speech classification can be done with 2-dimensional inputs (spectrograms), for privacy-preserving speech classification we advocate the use of 1-dimensional CNNs for their smaller model size and efficiency during the secure inference process.\n\nAfter describing the relationships between this paper and existing work in Section 2, in Section 3 we present details about the proposed methods. These include the pre-processing of the audio and the proposed MPC-friendly neural network architecture, a description of the security settings, and MPC-based protocols for secure classification with 1-dimensional CNNs. We implemented our approach on top of the MP-SPDZ framework  [15] . In Section 4, we present accuracy and runtime results on the RAVDESS benchmark data set  [16]  for emotion detection from speech. In the active-security setting, i.e. with malicious adversaries that may deviate from the protocol, a speech signal of 3.5 sec is classified in ∼1.6 sec (i.e. real-time factor 0.46). In the passive-security setting, i.e. with semi-honest adversaries that adhere to the protocol instructions but try to learn additional information, we can classify a speech signal in 0.26 sec (real-time factor 0.07). The accuracy in both cases is 70.32%, which is in the range of state-of-the-art approaches for emotion recognition from the RAVDESS dataset in-the-clear. Furthermore, our approach is provably secure: during the secure inference, nobody other than Alice learns anything about her speech signal, and nobody other than Bob learns anything about his model parameters.\n\nObviously, disclosing the inferred class label at the end of the protocol to Bob reveals information about Alice. Similarly, disclosing class labels to Alice could reveal some information about Bob's model, which could allow membership inference and/or model inversion attacks. To protect against these, Differential Privacy (DP) could be used to add controlled noise to the gradients during DNN training, which would lead to a decrease in accuracy. We consider this to be orthogonal to, and outside of, the scope of this paper.\n\nAs we highlight in Section 5, our results answer a question that has remained open in the literature thus far, namely to what extent MPC-based protocols can enable provably secure and highly accurate real-time speech classification  [1] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We refer to the work of Nautsch et al.  [1]  for an excellent and comprehensive survey of existing work on privacy-preserving speaker and speech characterization. Below we focus on what is most relevant for our work, namely (1) existing approaches to speech classification that are based on MPC, and (2) existing work on secure inference with a trained deep learning model based on MPC, for applications other than speech or speaker characterization. To the best of our knowledge, none of the existing work in category (1) is based on deep learning, while none of the existing work in (2) has been applied to 1-dimensional CNNs in general, or to speech classification in specific. This is the gap we close in our work. We note that, prior to our work, an MPC based approach for speech classification based on neural networks has been considered highly impractical due to an assumed massive overhead in computation time and communication costs  [17] . This has prompted research into the use of trusted execution environments, such as real-time speech classification based on Intel SGX  [17] , and real-time keyword recognition on ARM-based (mobile) devices using TrustZone capabilities  [18] . Contrary to this, the solution we present in this paper does not require special hardware, and, as we demonstrate in Section 4, is fast enough for use in real-time.\n\n(1) MPC-based speech classification. In the clear, i.e. without concern for user privacy, there are several successful ML approaches for speech classification. Well-known ML work horses that gained popularity in the 1980s and 1990s are hidden Markov models (HMMs) and Gaussian mixture models (GMMs). The earliest work on privacy-preserving speech classification based on MPC focused on the design of cryptographic protocols to make training and inference with HMMs and GMMs secure in the semi-honest setting  [11, 12, 13] .\n\nThese early approaches were based on homomorphic encryption (HE) and slow because of the large computation costs. Portêlo et al.  [14]  substantially improved upon this computational cost by using Garbled Circuits (GC) instead of HE, in a GMM based protocol specifically for speaker verification, i.e. voice based authentication. Similarly, Treiber et al.  [19]  introduced an MPC approach for privacy-preserving speaker verification based on secure computation of the cosine of a vector with biometric characteristics extracted from the speaker's audio signal on one hand, and a stored reference embedding on the other hand; we note that the comparison of two vectors for speaker verification is computationally a less involved task than inference with a trained ML model for speech classification, as we do in this paper.\n\nWhile up until a decade ago, HMM used to be popular for speech processing and audio classification, more recently deep learning has been acknowledged as a state-of-the-art ML approach in this field  [20, 21] . The CNN approach that we follow in this paper adheres to the latter.\n\n(2) MPC-based classification with CNNs. The problem of doing privacy-preserving inference with trained neural networks has received a lot of attention in the literature recently, and a variety of MPC-based approaches and frameworks have been proposed. Most of these, including MiniONN  [22] ,\n\nSecureML  [23] , DeepSecure  [24] , Chameleon  [25] , Gazelle  [26] , Quotient  [27] , XONN  [28] , and Delphi  [29] , are limited to the semi-honest security setting, i.e. they guarantee that no information is leaked as long as the parties honestly execute the protocols. CrypTFlow  [30] , based in part on SecureNN  [31] , is an interesting recent addition to the growing body of MPC-based secure inference frameworks. In addition to the semi-honest case, CrypTFlow also guarantees security in the malicious case, where parties may deviate arbitrarily from the protocols. To this end, CrypTFlow uses a combination of cryptographic techniques and secure hardware (Intel SGX). To the best of our knowledge, SecureQ8  [6]  is the only work so far on MPC-based secure inference with trained CNNs in both the semi-honest and malicious case that does not require special secure hardware. In this paper, we adapt the work that was done in SecureQ8 for 2-dimensional CNNs (image classification) to 1-dimensional CNNs (speech classification).\n\nOutside of MPC, we mention the research by Dias et al.  [32]  and Teixeira et al.  [33]  who combine neural networks with (leveled) fully homomorphic encryption (FHE) for privacy-preserving detection of emotion and of voice-affecting diseases such as a cold, a depression, and Parkinson's disease. The main difference between their work, which builds on Cryptonets  [34] , and ours, is that in  [32, 33] , Alice encrypts her input feature vector and sends it to Bob, who uses FHE to perform computations over the encrypted data, while in our MPC approach both Alice and Bob perform computations. FHE comes with lower communication costs than MPC, at the expense of substantially higher computation costs, which could make it prohibitively expensive for audio classification.\n\nDias et al.  [32]  use a relatively simple architecture, namely a multi-layer perceptron (MLP) with two hidden layers, and no convolutional layers. Based on runtime results reported in  [34] , the CNN approach that we propose in this pa-per, is faster than the MLP approach of Dias et al.  [32] , even for active security (i.e. malicious parties), and an estimated two orders of magnitude faster for passive security (i.e. semi-honest parties).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "Our approach for private speech characterization consists of two phases:  Our assumption is that Bob has a set of audio files (speech signals) that are each annotated with a label, and he uses these to train an ML model that can assign a correct label to a previously unseen audio file (Alice's input). It is common in speech processing for classifiers to work on features extracted from the speech signal as opposed to on the raw speech signal itself. These features and the software to extract them are widely known and publicly available. It is for example very common to convert a speech signal into a sequence of feature vectors of mel-frequency cepstral coefficients (MFCC)  [35]  that are extracted from sliding windows of consecutive speech. We assume that Bob converts each audio file from his training data into a sequence of r feature vectors, each of length m, and subsequently averages them to obtain one feature vector of length m per audio file. Similarly, Alice converts her speech signal into a sequence of r vectors of MFCC coefficients, averages them, and uses the resulting feature vector of length m as her input to the protocol for speech classification (see Figure  1 ). As we demonstrate in Section 4, we can train highly accurate ML models for speech classification based on these extracted feature vectors. That in itself is clear evidence that the feature vectors contain meaningful, private information that needs to be kept private during inference, as we do with the technique described in Section 3.2.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Mpc-Friendly Cnn Model Architecture",
      "text": "We propose the use of a standard, MPC-friendly CNN model architecture.\n\nBy \"MPC-friendly\" we mean that the operations to be performed when doing inference with the trained CNN are chosen purposefully among operations for which efficient MPC protocols exist. A standard CNN contains one or more blocks that each have a convolutional layer, followed by an activation layer, and an optional pooling layer.\n\nThe difference between the more commonly used 2-dimensional CNNs on one hand, and the 1-dimensional CNNs that we use in this paper, is that in a 1dimensional CNN, convolutional operations are performed across one dimension.\n\nIn 2-dimensional CNNs, the shape of the input of a convolutional layer is defined in terms of its height H, width M, and depth D. In a 1-dimensional CNN, the height is always 1, hence the input X is a D × M matrix, as illustrated in Figure  2 . In this paper, the input to the first convolutional layer in the CNN is a vector of m MFCC values as explained in Section 3.1.1, hence D= 1 and M= m. In further convolutional layers, the depth D is typically larger, and determined by the number of filters (kernels) in a previous layer. A convolutional layer is defined by F filters, each of size D × L, with L the width of the filters. In addition, for each filter W i , i = 1, . . . , F, the convolutional layer contains a bias term b i ∈ R. The values of the weights in the filters and the bias are, as usual, learned during training. The output produced by a convolutional layer with F filters W i of width L, when applied to an input X of size D × M, is a matrix Z of size F × M, which is computed as: 5 4, in the first convolutional layer (line 5 For example, in the CNN in Figure\n\nIn this pseudocode, X[j : j + L -1] denotes the submatrix of X that consists of column j through column j + L -1 of X,  6  while denotes the Frobenius inner product (a generalization to matrices of the dot product of vectors). The computation of the ith row of Z is illustrated in Figure  2 . In privacy-preserving speech classification, the input X into the first convolutional layer is known to Alice, while the values of W i and b i are known to Bob. We address in Section 3.2 how in this case Z can be computed, and subsequent CNN operations can be performed, without the need for Alice to disclose X and without the need for Bob to disclose W i and b i .\n\nAs the activation function in the convolutional blocks, we choose the RELU function f (z) = max(0, z), which means that all negative values are mapped to 0. For the pooling layer, we select average pooling instead of max pooling, because in an MPC setting additions and division by a publicly known constant (as is needed to compute an average) are computationally less expensive than performing comparisons (which would be needed to find a maximum). Applying RELU and average pooling with size 2 to the output in Figure  2  would yield [10, 0, 2.5].\n\nThe stacked convolutional blocks are followed by a dense layer, the application of which comes down to a product of two matrices. The activation function on the final layer is typically a logistic function (for binary classifica-\n\nFigure  2 : Illustration of a 1-dimensional convolution on an input width M=6, height H=1, and depth D=4. This is equivalent to a 2-dimensional convolution on an input of width 6, height 4, and depth 1. The filter \"slides\" from the left to the right.\n\ntion problems) or a softmax operation (for multi-class classification problems).\n\nThe output of the softmax operation is a probability for each of the possible class labels; the label with the highest probability is returned as the final result.\n\nWhile the use of a softmax function is important during training, we note that during inference it can be replaced by an argmax function. Indeed, the softmax operation does not change the ordering among the logits, i.e. the values that are passed into it from the previous layer. Argmax is computationally much less expensive to compute in a privacy-preserving manner. Finally, any dropout layers that are used to improve the training process, are omitted during inference, which means that we do not need to include MPC-based protocols for these layers when doing secure inference (see Section 3.2). We refer to Section 4 for more details about the exact CNN architecture that we used in our experiments.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Privacy-Preserving Inference With A 1-Dimensional Cnn",
      "text": "After giving a high level overview in Section 3.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Security Settings",
      "text": "There exist a variety of MPC schemes, designed for different numbers of participants and offering various levels of security that correspond to different threat models. In the scenario of privacy-preserving speech classification that we consider in this work, there are two participants, Alice and Bob, and one of them may be corrupted. When Alice and Bob execute a secure MPC protocol between themselves to perform the privacy-preserving speech classification, as illustrated in Figure  1 , one corrupted party means that we are in the so-called scenario of dishonest majority. In general, a dishonest majority setting is one where an adversary can corrupt a fraction of the protocol participants that is equal to or greater than 1/2. In our two-party computation (2PC) setting this means that each party can only trust itself and assumes that the other party may be corrupted. We describe the MPC protocols that we use for the dishonest-majority setting in Section 3.2.2.\n\nMPC protocols in the dishonest-majority setting such as the 2PC scenario from Figure  1  are much more computationally expensive than protocols in an honest-majority setting, i.e. when more than half of the protocol participants are honest. Therefore, many works on privacy-preserving inference have considered the setting in which Alice and Bob outsource the secure computations to a set of 3 or more servers, of which a majority is assumed to be honest (e.g.,  [6, 25, 30, 31] ). In this work we also evaluate the performance of privacypreserving speech classification in the scenario in which Alice and Bob outsource the secure classification to 3 servers (three-party computation, 3PC), one of which can be corrupted. The protocols that we use for this scenario, which is illustrated in Figure  3 , are described in Section 3.2.3.\n\nFurthermore, a party can be corrupted in different ways. In the passivesecurity setting (also known as semi-honest or honest-but-curious adversaries), the corrupted parties follow the specified protocol instructions, but they may try to learn additional information (i.e., information other than what can be inferred from their specified inputs and outputs) from the messages exchanged during the protocol execution. Secure MPC protocols prevent such information leakage. In the active-security setting (also known as malicious adversaries), the parties may deviate from the protocol instructions in arbitrary ways, for instance by providing incorrect values on purpose. In this case, secure MPC protocols should prevent information leakage and detect devious behavior. Protection against such a stronger threat model comes at a higher computational cost.\n\nIn this paper, we evaluate multiple MPC schemes and their efficiency-securityaccuracy trade-off for privacy-preserving speech classification.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Secret Sharing-Based Mpc For Dishonest Majority",
      "text": "In the MPC schemes that we use, all computations are done on integers, modulo an integer q. The modulo q is a hyperparameter that defines the algebraic structure in which the computations are done, which in turn has a direct effect on the efficiency of the MPC protocols for different tasks. In Section 4, we evaluate MPC schemes where q is a prime number as well as where q is a power of 2.\n\nFurthermore, all MPC schemes for the dishonest-majority scenario that we use are based on additive sharing. A value x in Z q = {0, 1, . . . , q -1} is secret shared between Alice and Bob by picking uniformly random values x 1 , x 2 ∈ Z q such that (for efficiency gains) using matrix multiplication triples, and keeps its security even when composed with other arbitrary building blocks  [37, 38] .\n\nActive security. In the case of active security, the main idea to prevent the players from cheating is to use a Message Authentication Code (MAC). We focus first on the case of a prime field Z q , with q a prime number. To verify the correctness of the computations, Alice and Bob each have a share of a fixed MAC key α ∈ Z q , i.e. Alice has α 1 and Bob has α 2 such that α 1 + α 2 = α mod q. When a value x is secret shared between Alice and Bob, they also get shares m 1 and m 2 , respectively, of a MAC such that • Addition of a constant (z = x + c): Alice and Bob compute (\n\n. Note that the MAC relation remains satisfied, since\n\n• The notations we use for the operations are the same as in the passivesecurity case. In the case of the protocol with active security using binary fields Z q , with q a power of 2, there are a few additional technical details regarding the MAC, but the MPC scheme provides the same set of basic local operations that are described above. We refer interested readers to  [39]  for further details.\n\nIn the case of active security, the multiplication of secret-shared values can also be performed as described above using multiplication triples, but the multiplication triples must be generated together with the respective MACs.\n\nAdditionally, since in the case of active security all secret-shared values x that are used in the computations must contain a corresponding MAC (defined by m 1 and m 2 as in Equation  3 ), a procedure for the parties to obtain a MAC for their inputs must be used. This is done as follows. During the offline phase described below, a secret sharing [[r]] (with a MAC) of a random value r ∈ Z q is generated and distributed to Alice and Bob. If Alice has an input a ∈ Z q , the secret sharing [[r]] is opened towards her and she sends c = a -r to Bob. They then compute the secret sharing [[a]] ← [[r]] + c, which contains a MAC. Note that the value c is uniformly random and independent from a, and therefore does not reveal any information to Bob.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Generation Of Multiplication Triples And Random Values During",
      "text": "the Offline Phase. For performance reasons, modern MPC schemes are normally divided in two phases: the offline and online phases. The offline phase only performs computations that are independent from the specific inputs of the parties to the protocol (Alice's speech signal and Bob's trained model parameters), and therefore can be executed far before the inputs are fixed. Modern MPC protocols try to perform as much of the computation as possible in the offline phase, so that the online phase can be faster, improving the responsiveness of the MPC solution.\n\nIn the case of the secret-shared based schemes that we consider, the computationally heavy operations are the generation of the multiplication triples and of the random values, and both of them are independent of the specific inputs of parties and can be delegated to the offline phase, whose main purpose is to generate these values. we use in this paper. The MPC schemes for passive security provide protection against semi-honest adversaries, while the MPC schemes for active security provide protection against malicious adversaries. The distinction between the underlying algebraic structures Z p and Z 2 k is meaningful because of its potential impact on the efficiency of the protocols. We briefly describe each MPC scheme for the dishonest majority scenario here (the ones for the honest majority scenario are described in Section 3.2.3):\n\n• In the case of active security using a prime field, we use MASCOT  [40] ,\n\nan MPC scheme with an improved offline phase based on oblivious transfer techniques to generate the necessary values for the online phase of the SPDZ protocol  [41]  (which is the online phase described above). Note that the offline phase is also performed between Alice and Bob, and one of them may act maliciously. Therefore it is necessary to use a series of mechanisms (such as consistency checking, privacy amplification techniques, and oblivious transfer checks) in order to guarantee that correct multiplication triples and random values are generated and that nothing about them leaks to Alice or Bob. We point interested readers to  [40]  for further details about how these values are generated in the offline phase.\n\n• In the case of active security using a binary field, we use SPDZ2K  [39] . It adapts the offline phase of MASCOT to generate multiplication triples and random values for a binary field, which are then consumed by its online phase (which is an adaptation of SPDZ to the setting of binary fields). See  [39]  for further details.\n\n• For passive security we use SEMI for the case of prime fields, and SEMI2K\n\nfor binary fields. Both schemes generate multiplication triples using techniques based on oblivious transfer. SEMI is a cut-down version of MAS-COT, which eliminates all the additional machinery of MASCOT that is only necessary for the case of active security (such as consistency checking, privacy amplification techniques, the generation and use of message authentication codes, and oblivious transfer checks). Similarly, SEMI2K is a cutdown version of SPDZ2K to focus on passive security.\n\nMany previous works on privacy-preserving machine learning have assumed the existence of a trusted initializer (e.g.,  [42, 43, 44, 45, 46, 47, 48] ), who pre-distributes correlated randomness to the protocol participants at a setup phase and does not participate in any other part of the protocol execution.\n\nNote that such a trusted initializer would completely eliminate the need of executing the offline phase of the above protocols, as the trusted initializer can pre-distribute all necessary multiplication triples and random values to Alice and Bob. However, we are interested in evaluating the performance of secure classification in the setting in which no such trusted initializer is available to the model and data owners and they have to execute the complete two-party computation solution between themselves.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Secret Sharing-Based Mpc For Honest Majority",
      "text": "In the setting with 3 computing servers and at most 1 corruption (i.e., honest majority setting), we use MPC schemes based on replicated secret sharing, which allow much faster solutions than in the two-party setting.\n\nIn a replicated secret sharing scheme, a value x in Z q = {0, 1, . . . , q -1} is secret shared among servers S 1 , S 2 and S 3 by picking uniformly random values\n\nand distributing (x 1 , x 2 ) to S 1 , (x 2 , x 3 ) to S 2 , and (x 3 , x 1 ) to S 3 . Note that no single server can obtain any information about x given its share. We continue to use  [[x] ] as a shorthand for a secret sharing of x in this case.\n\nPassive security. As in the case of additive secret sharings, the 3 parties can easily perform the following operations through carrying out local computations: addition of a constant, addition of secret-shared values, and multiplication by a constant. The biggest advantage of this replicated secret sharing scheme is that it enables a more efficient procedure for multiplying secret-shared values.\n\nWhen multiplying x • y = (x 1 + x 2 + x 3 )(y 1 + y 2 + y 3 ), the servers can locally perform the following computations:\n\nAfter performing these local computations, the servers obtain an additive secret sharing of x • y without needing any interactions. Next, they just need to convert from the additive secret sharing representation back to a replicated secret sharing representation, so that it is possible to perform more multiplications in the same way. In order to securely do this conversion, the servers obtain an additive secret sharing of 0 by picking uniformly random u 1 , u 2 , u 3 such that u 1 + u 2 + u 3 = 0, which can be locally done with computational security by using pseudorandom functions, and S i locally computes v i = z i + u i . Finally, Active security. In the case of malicious adversaries, the MPC scheme PsReplPrime that we consider for prime fields uses the approach introduced by Lindell and Nof  [50]  of generating multiplication triples optimistically in the offline phase (i.e., running the multiplication protocol that is secure against semi-honest adversaries), performing the triple verification via sacrificing (i.e., one additional triple is used to verify the triple in question and this additional triple is then discarded  [50] ), and then using Beaver's protocol for multiplication of secret-shared values. For more details, we refer to  [50] . In the case of binary fields, the MPC scheme PsRepl2k that we use was recently proposed by Eerikson et al.  [51] ; we evaluate the option with preprocessing for generation of the multiplication triples that is available in MP-SPDZ  [15] . Note that in the three-party computation setting, the generation of the multiplication triples does not require any expensive public-key encryption operations.",
      "page_start": 20,
      "page_end": 22
    },
    {
      "section_name": "Quantization",
      "text": "MPC based on secret sharing, as explained in Section 3.2.2 and 3.2.3, provides a mechanism to perform secure computations on integers modulo q. The parameter values of a trained neural network, i.e. the values in the filters in the convolutional layers, the weights on the dense layers etc., are natively real numbers and need to be converted to integers. For this conversion process, we leverage existing research on quantization of neural networks. In deep learning, the conversion of floating-point (FP) data in the network to integers (INT) is studied as an effective way to shrink the model size and to accelerate computation, e.g. on edge devices with limited memory and computational power  [52] . The use of quantization is growing in popularity in research on privacy-preserving deep learning as well, for instance in XONN  [28] , where neural network parameters are restricted to take binary values {-1, 1}, in Quotient  [27]  with ternarized network weights in {-1, 0, 1}, and in SecureQ8  [6]  where network weights are reduced to 8-bit integers. We adhere to the latter.\n\nQuantization allows to represent a set of real numbers {α 1 , ...., α n } ∈ R as a set of integers {a 1 , ...., a n } ∈ Z q . In this work we use the 8-bit quantization method implemented in TensorFlow Lite,  9  which was designed in the work of Jacob et al.  [53]  and used previously in SecureQ8  [6] . Let us define the dequantization function\n\nwhere m ∈ R is a scale and z ∈ {0, . . . , 2 8 -1} is a zero-point. The quantization Dot product is an important operation in CNNs, both for the convolutional layers and the dense layers. We use the same method as SecureQ8  [6]  to compute dot products by using only integer arithmetic to sum the products of the vector elements (in Z q for q 2 8 ) and a single fixed-point multiplication to adjust to the proper scale for the output. Adding bias is handled by setting the scale of the bias representation to be the same as the scale of the output, and its zero-point to 0. Layers that only involve comparisons, such as RELU, can be directly implemented on the quantized values if they share the same scale and zero-point.\n\nWhen a fixed-point multiplication is performed, it is necessary to truncate the result by a number of bits equal to the number of bits that is used to represent the fractional part, so that the output does not use twice as many bits to represent the fractional part as the inputs. In the case of prime fields this is done using either the deterministic truncation protocol of Catrina and De Hoogh  [54]  or the probabilistic truncation protocol of Catrina and Saxena  [55] . The protocol of Catrina and De Hoogh computes the exact truncation result, but needs to invoke a secure bitwise comparison protocol (which increases the overall runtime of the truncation) in order to verify if a modular reduction modulo q occurred in a certain step of the secret-shared based protocol or not. We refer interested readers to the original paper for details: the deterministic truncation protocol is described as Protocol 3.3, and its building blocks are explained in Sections 2, 3 and 4 of that paper. In the case of the probabilistic truncation protocol of Catrina and Saxena, the probabilities that a number is rounded up or down are proportional to its distance to those bounds. The probabilistic truncation protocol eliminates the invocation of the underlying secure bitwise comparison protocol, and therefore improves the efficiency. We refer interested readers to the original paper of Catrina and Saxena: the probabilistic truncation protocol is described as Protocol 3.1, and its building blocks are described in Section 2.\n\nThe probabilistic truncation affects negatively the accuracy of the secure classification as we will show in Section 4. In the case of binary fields, the truncation is done using the adaptations of the above deterministic and probabilistic truncation protocols that were introduced by Dalskov et al.  [6] . In the procedures in which the amount of bits to be truncated needs to be kept secret, we use the protocol of Dalskov et al.  [6]  to perform deterministic truncation by a secret value.\n\nWe refer interested readers to  [6, 53]  for further details.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Using An Mpc Scheme To Securely Classify",
      "text": "Classification of Alice's speech signal vector X with Bob's model can, at a high level of abstraction, be thought of as the evaluation of a function f (X, θ)\n\nthat depends both on Alice's input X and on proprietary model parameters θ that were learned during training and that are only known to Bob. In the following description, we focus on the case of two-party computation for concreteness, but the case of outsourced three-party computation can be handled similarly. Designing a secure solution based on MPC for the classification comes down to representing the function that needs to be privately computed using the basic operations that are provided by the underlying MPC scheme (i.e., the addition and multiplication gates). Once this representation is found, the parties evaluate it gate by gate using existing procedures for private addition and private multiplication as explained in Section 3.2.2. This classification is performed during the online phase of protocol, consuming the necessary values that were generated during the offline phase, i.e. the multiplication triples that are needed for multiplication of secret-shared values, as well as the random values that are needed for Alice to secret share her speech signal vector X, and for Bob to secret share his model parameters θ. During the secure classification process, Alice and Bob jointly go through the following steps:\n\n1. Input. Alice secret shares her MFCC vector X, and Bob secret shares his model parameters θ using the technique for secret sharing described in Section 3.2.2. Since secret sharing is done over integers, this requires that Alice and Bob first convert their real valued numbers into integers, using either a fixed-point representation  [55]  or, as we do in this paper, using a quantization scheme (Section 3.2.4).",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Results",
      "text": "Experimental setup: all benchmark and accuracy tests were completed on co-located F32s V2 (VM1) and F72s V2 (VM2) Azure virtual machines. We  The accuracy results were obtained by holding 33% of the data out as test data.\n\nThe classification runtimes are computed as an average over 10 inferences.\n\ndrop in accuracy to 64.38%. These numbers are interesting by themselves: while Dalskov et al.  [6]  write that the use of a probabilistic truncation protocol may hurt classification accuracy, to the best of our knowledge, we are the first to evaluate and measure this drop in accuracy experimentally on a real-life data set.\n\nThe absolute runtimes that we obtain are, even on the more modest VM, an order of magnitude smaller (better) than the runtimes reported for image classification in  [6] . This is because our overall neural network architecture is far more compact; the fact that we choose to use a 1-dimensional CNN instead of a 2-dimensional CNN contributes to this gain in speed. Beyond that, our runtime results are in line with what is reported in  [6] . For the 2PC setting (Table  2 ) we observe the following:\n\n• The probabilistic truncation protocol allows faster secure inferences than the deterministic truncation protocol. The price paid for this gain in speed, is a loss in accuracy (in our data set, a loss of ∼6%).\n\n• Among the MPC schemes for passive security, SEMI is 2-4x slower than SEMI2K. Among the MPC schemes for active security, the difference in runtime between SPDZ2K and MASCOT is minor (one slightly better with the deterministic truncation, the other slightly better with the probabilistic truncation).\n\n• SEMI2K (passive security) is around 7-10x faster than SPDZ2K/MASCOT (active security). The price paid for this gain in speed is a weaker security setting, in which it is assumed that the adversary tries to gain additional information, but nevertheless follows the protocol specifications.\n\nThe protocols in the three-party outsourced computation setting with honest majority execute between 16x and 29x faster than their counterparts in the twoparty computation setting. This is expected given the performance differences between state-of-art MPC protocols in the 2PC with dishonest-majority and 3PC with honest-majority settings. Beyond that, we have that • Among the MPC schemes for active security, PsReplPrime (which uses a prime field) performs slightly better than PsRepl2k (which uses a binary field) in all tests.\n\n• On the other hand, among the MPC schemes for passive security, Repl2k\n\noutperforms ReplPrime in all tests, running around 2-3x faster.\n\n• Repl2k executes around 6-9x faster than PsReplPrime.\n\nConsidering passive security in both the 2PC and 3PC settings, performing the secure classification using computations on a binary field is far more efficient than using a prime field. On the other hand, in the active-security setting, the secure classification achieves a comparable running time on both binary and prime fields, the winner depending on the number of parties running the MPC scheme and the type of truncation. Note that, in the passive-security setting the overall procedures required for performing a multiplication of secret-shared values are far less complicated than in the active-security setting, and in the active-security setting those procedures are more in the case of binary fields.\n\nTowards deployment in a real-time privacy-preserving speech classification application, the 3PC setting with three semi-honest computational servers is a very viable option (Table  3 ). The gain in speed compared to the 2PC setting stems from the use of cryptographic protocols that leverage the availability of three instead of only two players to secret share the values with, and the removal of the need for expensive public key encryption, rather than the availability of more hardware in the form of a third server. It is important to stress that, since the three servers only receive shares of Alice's and Bob's information, the servers do not learn anything about the speech signal nor the trained model parameters.\n\nThis holds true as long as not more than one of the three servers is corrupted.\n\nThe 3PC setting is a good fit for applications where the user (Alice) and the application developer (Bob) have access to three reliable computational servers in the cloud, and the application developer wants to offer a speech classification service without becoming liable for invading the user's privacy.\n\nIn settings where there is no configuration available of three computational servers with an honest majority, and where each party can only trust itself, one can resort to the MPC schemes from the 2PC setting at a higher runtime cost (see Table  2 ). These may be suitable for sensitive applications where real-time speech classification is not a requirement, such as healthcare applications or empathy based AI systems where one can afford several seconds of even half a minute to detect a disease or the user's general mood in a privacy-preserving manner.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented the first privacy-preserving approach to deep learning based speech classification that is provably secure. To this end, we have proposed the first application of privacy-preserving classification with 1dimensional CNNs based on Secure Multiparty Computation (MPC). In terms of privacy, MPC is very reliable: other than the result of the classification (which can be selectively revealed to the model owner, data owner, or a third party depending on the application), no information about the speech signal or the trained model parameters is leaked to any participant of the protocol.\n\nWhen performing oblivious speech classification, the price paid for keeping the data and the model private, is an increase in computational cost and runtime.\n\nOur results answer a question that has remained open in the literature thus far, namely whether MPC based protocols are efficient enough to enable highly accurate real-time speech classification as would be needed for instance for digital voice assistants such as Apple's Siri, Amazon's Alexa, Google Home, and Microsoft's Cortana. Our results show that this is clearly within reach.\n\nIn our experiments for a passive-security setting, i.e. with semi-honest parties who follow the instructions of the cryptographic protocols, an audio file of 3.5 sec is classified with high accuracy in 0.26 sec, and in 0.15 sec with lower accuracy.\n\nThese results were obtained with a CNN that we optimized for high accuracy as well as high efficiency in the MPC setting, through deliberate design choices in the CNN architecture, and the use of quantization. We ran the protocols in MP-SPDZ, an existing framework for MPC that is not optimized in any specific way for speech classification. That means that, in addition to the optimization efforts we made in this paper on the machine learning side, there is room to bring the secure inference runtimes down even further by optimizations on the MPC side, for instance by replacing the division algorithm in the average pooling layer by multiplication with a constant.\n\nThe fastest results mentioned above are obtained when Alice and Bob outsource the computations to three semi-honest servers (3PC). As long as these servers do not collude with each other, they do not learn anything about Alice's speech signal or about Bob's trained model parameters. We have also included scenarios with stronger security assumptions in our study, namely, in increasing order of runtime: malicious adversaries with an honest majority (3PC), semihonest adversaries with a dishonest majority (2PC), and malicious adversaries with a dishonest majority (2PC). Actively secure protocols remain secure even if one of the parties is a malicious adversary who deviates from the protocol specification. This makes these protocols most suitable for sensitive applications, even if they come at a notably higher computational cost.",
      "page_start": 28,
      "page_end": 34
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Oblivious speech classiﬁcation as a two-party computation (2PC) problem in the",
      "page": 4
    },
    {
      "caption": "Figure 1: ). As we demonstrate in Section 4, we can train highly accurate ML",
      "page": 10
    },
    {
      "caption": "Figure 2: In this paper, the input to the ﬁrst convolutional layer in the CNN is a vector",
      "page": 10
    },
    {
      "caption": "Figure 2: In privacy-preserving",
      "page": 11
    },
    {
      "caption": "Figure 2: would yield",
      "page": 11
    },
    {
      "caption": "Figure 2: Illustration of a 1-dimensional convolution on an input width M=6, height H=1,",
      "page": 12
    },
    {
      "caption": "Figure 1: , one corrupted party means that we are in the so-called",
      "page": 13
    },
    {
      "caption": "Figure 1: are much more computationally expensive than protocols in an",
      "page": 13
    },
    {
      "caption": "Figure 3: Oblivious speech classiﬁcation as a three-party computation (3PC) problem in",
      "page": 14
    },
    {
      "caption": "Figure 3: , are described in Section 3.2.3.",
      "page": 14
    },
    {
      "caption": "Figure 4: CNN architecture and Keras code snippet used to train the model.",
      "page": 28
    },
    {
      "caption": "Figure 4: We hold out 33% of the data as test data and train on the rest. The",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Accuracy and runtime results for privacy-preserving emotion detection in the",
      "data": [
        {
          "Active Security": "SPDZ2K",
          "Passive Security": "SEMI2K"
        },
        {
          "Active Security": "250.9 sec\n370.0 sec",
          "Passive Security": "27.6 sec\n40.5 sec"
        },
        {
          "Active Security": "26.01 sec\n33.30 sec",
          "Passive Security": "2.77 sec\n4.17 sec"
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Active Security": "PsRepl2k",
          "Passive Security": "Repl2k"
        },
        {
          "Active Security": "10.16 sec\n12.72 sec",
          "Passive Security": "1.24 sec\n2.06 sec"
        },
        {
          "Active Security": "1.35 sec\n1.61 sec",
          "Passive Security": "0.15 sec\n0.26 sec"
        }
      ],
      "page": 31
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Preserving privacy in speaker and speech characterisation",
      "authors": [
        "A Nautsch",
        "A Jiménez",
        "A Treiber",
        "J Kolberg",
        "C Jasserand",
        "E Kindt",
        "H Delgado",
        "M Todisco",
        "M Hmani",
        "A Mtibaa"
      ],
      "year": "2019",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "2",
      "title": "The implementation of low-cost urban acoustic monitoring devices",
      "authors": [
        "C Mydlarz",
        "J Salamon",
        "J Bello"
      ],
      "year": "2017",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "3",
      "title": "Deep convolutional neural networks and data augmentation for environmental sound classification",
      "authors": [
        "J Salamon",
        "J Bello"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "4",
      "title": "The VoicePrivacy 2020 challenge evaluation plan",
      "authors": [
        "N Tomashenko",
        "B Srivastava",
        "X Wang",
        "E Vincent",
        "A Nautsch",
        "J Yamagishi",
        "N Evans",
        "J.-F Bonastre",
        "P.-G Noé",
        "M Todisco",
        "J Patino"
      ],
      "year": "2020",
      "venue": "The VoicePrivacy 2020 challenge evaluation plan"
    },
    {
      "citation_id": "5",
      "title": "Secure Multiparty Computation and Secret Sharing",
      "authors": [
        "R Cramer",
        "I Damgård",
        "J Nielsen"
      ],
      "year": "2015",
      "venue": "Secure Multiparty Computation and Secret Sharing"
    },
    {
      "citation_id": "6",
      "title": "Secure evaluation of quantized neural networks",
      "authors": [
        "A Dalskov",
        "D Escudero",
        "M Keller"
      ],
      "year": "2020",
      "venue": "Proceedings on Privacy Enhancing Technologies 2020"
    },
    {
      "citation_id": "7",
      "title": "Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "8",
      "title": "The staggering cost of training SOTA AI models",
      "authors": [
        "T Peng"
      ],
      "year": "2019",
      "venue": "SyncedReview"
    },
    {
      "citation_id": "9",
      "title": "The secret sharer: Evaluating and testing unintended memorization in neural networks",
      "authors": [
        "N Carlini",
        "C Liu",
        "Ú Erlingsson",
        "J Kos",
        "D Song"
      ],
      "year": "2019",
      "venue": "th USENIX Security Symposium"
    },
    {
      "citation_id": "10",
      "title": "Learning efficient representations for fake speech detection",
      "authors": [
        "N Subramani",
        "D Rao"
      ],
      "year": "2020",
      "venue": "34th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "A framework for secure speech recognition",
      "authors": [
        "P Smaragdis",
        "M Shashanka"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Privacy-preserving speech processing: cryptographic and string-matching frameworks show promise",
      "authors": [
        "M Pathak",
        "B Raj",
        "S Rane",
        "P Smaragdis"
      ],
      "year": "2013",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "13",
      "title": "Privacy-preserving speaker verification and identification using Gaussian mixture models",
      "authors": [
        "M Pathak",
        "B Raj"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Privacy-preserving speaker verification using garbled GMMs",
      "authors": [
        "J Portêlo",
        "B Raj",
        "A Abad",
        "I Trancoso"
      ],
      "year": "2014",
      "venue": "22nd European Signal Processing Conference"
    },
    {
      "citation_id": "15",
      "title": "MP-SPDZ: A versatile framework for multi-party computation",
      "authors": [
        "M Keller"
      ],
      "year": "2020",
      "venue": "Cryptology ePrint Archive"
    },
    {
      "citation_id": "16",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "venue": "PloS one"
    },
    {
      "citation_id": "17",
      "title": "VoiceGuard: Secure and private speech processing",
      "authors": [
        "F Brasser",
        "T Frassetto",
        "K Riedhammer",
        "A.-R Sadeghi",
        "T Schneider",
        "C Weinert"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Offline model guard: Secure and private ML on mobile devices, in: Design, Automation & Test in Europe Conference & Exhibition (DATE)",
      "authors": [
        "S Bayerl",
        "T Frassetto",
        "P Jauernig",
        "K Riedhammer",
        "A.-R Sadeghi",
        "T Schneider",
        "E Stapf",
        "C Weinert"
      ],
      "year": "2020",
      "venue": "Offline model guard: Secure and private ML on mobile devices, in: Design, Automation & Test in Europe Conference & Exhibition (DATE)"
    },
    {
      "citation_id": "19",
      "title": "Privacypreserving PLDA speaker verification using outsourced secure computation",
      "authors": [
        "A Treiber",
        "A Nautsch",
        "J Kolberg",
        "T Schneider",
        "C Busch"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Using representation learning and out-of-domain data for a paralinguistic speech task",
      "authors": [
        "B Milde",
        "C Biemann"
      ],
      "year": "2015",
      "venue": "16th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "22",
      "title": "Oblivious neural network predictions via MiniONN transformations",
      "authors": [
        "J Liu",
        "M Juuti",
        "Y Lu",
        "N Asokan"
      ],
      "year": "2017",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "23",
      "title": "SecureML: A system for scalable privacy-preserving machine learning",
      "authors": [
        "P Mohassel",
        "Y Zhang"
      ],
      "year": "2017",
      "venue": "IEEE Symposium on Security and Privacy"
    },
    {
      "citation_id": "24",
      "title": "Scalable provablysecure deep learning",
      "authors": [
        "B Rouhani",
        "M Riazi",
        "F Koushanfar"
      ],
      "year": "2018",
      "venue": "th Annual Design Automation Conference (DAC)"
    },
    {
      "citation_id": "25",
      "title": "Chameleon: A hybrid secure computation framework for machine learning applications",
      "authors": [
        "M Riazi",
        "C Weinert",
        "O Tkachenko",
        "E Songhori",
        "T Schneider",
        "F Koushanfar"
      ],
      "year": "2018",
      "venue": "Asia Conference on Computer and Communications Security"
    },
    {
      "citation_id": "26",
      "title": "GAZELLE: A low latency framework for secure neural network inference",
      "authors": [
        "C Juvekar",
        "V Vaikuntanathan",
        "A Chandrakasan"
      ],
      "year": "2018",
      "venue": "th USENIX Security Symposium"
    },
    {
      "citation_id": "27",
      "title": "QUOTIENT: two-party secure neural network training and prediction",
      "authors": [
        "N Agrawal",
        "A Shahin",
        "M Shamsabadi",
        "A Kusner",
        "Gascón"
      ],
      "year": "2019",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "28",
      "title": "XONN: XNOR-based oblivious deep neural network inference",
      "authors": [
        "M Riazi",
        "M Samragh",
        "H Chen",
        "K Laine",
        "K Lauter",
        "F Koushanfar"
      ],
      "year": "2019",
      "venue": "th USENIX Security Symposium"
    },
    {
      "citation_id": "29",
      "title": "Delphi: A cryptographic inference service for neural networks",
      "authors": [
        "P Mishra",
        "R Lehmkuhl",
        "A Srinivasan",
        "W Zheng",
        "R Popa"
      ],
      "year": "2020",
      "venue": "th USENIX Security Symposium"
    },
    {
      "citation_id": "30",
      "title": "st IEEE Symposium on Security and Privacy",
      "authors": [
        "N Kumar",
        "M Rathee",
        "N Chandran",
        "D Gupta",
        "A Rastogi",
        "R Sharma"
      ],
      "year": "2020",
      "venue": "st IEEE Symposium on Security and Privacy"
    },
    {
      "citation_id": "31",
      "title": "SecureNN: 3-party secure computation for neural network training",
      "authors": [
        "S Wagh",
        "D Gupta",
        "N Chandran"
      ],
      "year": "2019",
      "venue": "Proceedings on Privacy Enhancing Technologies 2019"
    },
    {
      "citation_id": "32",
      "title": "Exploring hashing and CryptoNet based approaches for privacy-preserving speech emotion recognition",
      "authors": [
        "M Dias",
        "A Abad",
        "I Trancoso"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Privacy-preserving paralinguistic tasks",
      "authors": [
        "F Teixeira",
        "A Abad",
        "I Trancoso"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy",
      "authors": [
        "R Gilad-Bachrach",
        "N Dowlin",
        "K Laine",
        "K Lauter",
        "M Naehrig",
        "J Wernsing"
      ],
      "year": "2016",
      "venue": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy"
    },
    {
      "citation_id": "35",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal processing"
    },
    {
      "citation_id": "36",
      "title": "Efficient multiparty protocols using circuit randomization",
      "authors": [
        "D Beaver"
      ],
      "year": "1992",
      "venue": "Advances in Cryptology -CRYPTO '91"
    },
    {
      "citation_id": "37",
      "title": "Efficient and private scoring of decision trees, support vector machines and logistic regression models based on pre-computation",
      "authors": [
        "M Cock",
        "R Dowsley",
        "C Horst",
        "R Katti",
        "A Nascimento",
        "W.-S Poon",
        "S Truex"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "38",
      "title": "Cryptography based on correlated data: Foundations and practice",
      "authors": [
        "R Dowsley"
      ],
      "year": "2016",
      "venue": "Cryptography based on correlated data: Foundations and practice"
    },
    {
      "citation_id": "39",
      "title": "SPDZ 2 k : Efficient MPC mod 2 k for dishonest majority",
      "authors": [
        "R Cramer",
        "I Damgård",
        "D Escudero",
        "P Scholl",
        "C Xing"
      ],
      "year": "2018",
      "venue": "Annual International Cryptology Conference"
    },
    {
      "citation_id": "40",
      "title": "MASCOT: Faster malicious arithmetic secure computation with oblivious transfer",
      "authors": [
        "M Keller",
        "E Orsini",
        "P Scholl"
      ],
      "year": "2016",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "41",
      "title": "Multiparty computation from somewhat homomorphic encryption",
      "authors": [
        "I Damgård",
        "V Pastro",
        "N Smart",
        "S Zakarias"
      ],
      "year": "2012",
      "venue": "Annual Cryptology Conference"
    },
    {
      "citation_id": "42",
      "title": "Efficient unconditionally secure comparison and privacy preserving machine learning classification protocols",
      "authors": [
        "B David",
        "R Dowsley",
        "R Katti",
        "A Nascimento"
      ],
      "year": "2015",
      "venue": "International Conference on Provable Security"
    },
    {
      "citation_id": "43",
      "title": "Fast, privacy preserving linear regression over distributed datasets based on predistributed data",
      "authors": [
        "M Cock",
        "R Dowsley",
        "A Nascimento",
        "S Newman"
      ],
      "year": "2015",
      "venue": "ACM Workshop on Artificial Intelligence and Security"
    },
    {
      "citation_id": "44",
      "title": "Privacy-preserving scoring of tree ensembles: A novel framework for AI in healthcare",
      "authors": [
        "K Fritchman",
        "K Saminathan",
        "R Dowsley",
        "T Hughes",
        "M De Cock",
        "A Nascimento",
        "A Teredesai"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Big Data"
    },
    {
      "citation_id": "45",
      "title": "Protecting privacy of users in brain-computer interface applications",
      "authors": [
        "A Agarwal",
        "R Dowsley",
        "N Mckinney",
        "D Wu",
        "C.-T Lin",
        "M De Cock",
        "A Nascimento"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "46",
      "title": "Privacypreserving classification of personal text messages with secure multi-party computation",
      "authors": [
        "D Reich",
        "A Todoki",
        "R Dowsley",
        "M De Cock",
        "A Nascimento"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "47",
      "title": "Secure multiparty computations in floating-point arithmetic",
      "authors": [
        "C Guo",
        "A Hannun",
        "B Knott",
        "L Van Der Maaten",
        "M Tygert",
        "R Zhu"
      ],
      "venue": "Secure multiparty computations in floating-point arithmetic",
      "arxiv": "arXiv:2001.03192"
    },
    {
      "citation_id": "48",
      "title": "High performance logistic regression for privacy-preserving genome analysis",
      "authors": [
        "M Cock",
        "R Dowsley",
        "A Nascimento",
        "D Railsback",
        "J Shen",
        "A Todoki"
      ],
      "year": "2020",
      "venue": "High performance logistic regression for privacy-preserving genome analysis"
    },
    {
      "citation_id": "49",
      "title": "High-throughput semi-honest secure three-party computation with an honest majority",
      "authors": [
        "T Araki",
        "J Furukawa",
        "Y Lindell",
        "A Nof",
        "K Ohara"
      ],
      "year": "2016",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "50",
      "title": "A framework for constructing fast MPC over arithmetic circuits with malicious adversaries and an honest-majority",
      "authors": [
        "Y Lindell",
        "A Nof"
      ],
      "year": "2017",
      "venue": "ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "51",
      "title": "Use your brain! Arithmetic 3PC for any modulus with active security",
      "authors": [
        "H Eerikson",
        "M Keller",
        "C Orlandi",
        "P Pullonen",
        "J Puura",
        "M Simkin"
      ],
      "year": "2020",
      "venue": "st Conference on Information-Theoretic Cryptography (ITC 2020), Schloss Dagstuhl-Leibniz-Zentrum für Informatik"
    },
    {
      "citation_id": "52",
      "title": "Training high-performance and large-scale deep neural networks with full 8-bit integers",
      "authors": [
        "Y Yang",
        "L Deng",
        "S Wu",
        "T Yan",
        "Y Xie",
        "G Li"
      ],
      "year": "2020",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "53",
      "title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference",
      "authors": [
        "B Jacob",
        "S Kligys",
        "B Chen",
        "M Zhu",
        "M Tang",
        "A Howard",
        "H Adam",
        "D Kalenichenko"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Improved primitives for secure multiparty integer computation",
      "authors": [
        "O Catrina",
        "S Hoogh"
      ],
      "year": "2010",
      "venue": "International Conference on Security and Cryptography for Networks"
    },
    {
      "citation_id": "55",
      "title": "Secure computation with fixed-point numbers",
      "authors": [
        "O Catrina",
        "A Saxena"
      ],
      "year": "2010",
      "venue": "International Conference on Financial Cryptography and Data Security"
    },
    {
      "citation_id": "56",
      "title": "Primitives and applications for multi-party computation",
      "authors": [
        "T Toft"
      ],
      "year": "2007",
      "venue": "Primitives and applications for multi-party computation"
    },
    {
      "citation_id": "57",
      "title": "Proceedings of the 14th Python in Science Conference",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "58",
      "title": "Emotion detection from speech signals using voting mechanism on classified frames",
      "authors": [
        "A Zamil",
        "S Hasan",
        "S Baki",
        "J Adam",
        "I Zaman"
      ],
      "year": "2019",
      "venue": "International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST)"
    },
    {
      "citation_id": "59",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}