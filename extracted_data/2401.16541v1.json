{
  "paper_id": "2401.16541v1",
  "title": "Guret: Distinguishing Guilt And Regret Related Text",
  "published": "2024-01-29T20:20:44Z",
  "authors": [
    "Sabur Butt",
    "Fazlourrahman Balouchzahi",
    "Abdul Gafar Manuel Meque",
    "Maaz Amjad",
    "Hector G. Ceballos Cancino",
    "Grigori Sidorov",
    "Alexander Gelbukh"
  ],
  "keywords": [
    "Emotion Classification",
    "Guilt Detection",
    "Regret Detection",
    "Text Classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The intricate relationship between human decision-making and emotions, particularly guilt and regret, has significant implications on behavior and well-being. Yet, these emotions' subtle distinctions and interplay are often overlooked in computational models. This paper introduces a dataset tailored to dissect the relationship between guilt and regret and their unique textual markers, filling a notable gap in affective computing research. Our approach treats guilt and regret recognition as a binary classification task and employs three machine learning and six transformer-based deep learning techniques to benchmark the newly created dataset. The study further implements innovative reasoning methods like chain-of-thought and tree-of-thought to assess the models' interpretive logic. The results indicate a clear performance edge for transformer-based models, achieving a 90.4% macro F1 score compared to the 85.3% scored by the best machine learning classifier, demonstrating their superior capability in distinguishing complex emotional states.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human beings need to make several decisions every day to survive. Some decisions are routine, while others are crucial in determining human behavior. Decisions are essentially a pathway through which emotions shape our everyday efforts to avoid negative sentiments (i.e., guilt and regret) while promoting positive sentiments (i.e., hope, pride, and happiness), even when this process occurs unconsciously  [1, 2] . Similarly, decisions can also serve as a channel for intensifying negative emotions or diminishing positive ones linked to mental health issues. Regardless of whether these decisions are advantageous or not, when the consequences of our choices materialize, we typically experience new emotions like elation, surprise, or regret  [3] . Therefore, emotions and decision-making are intricately interconnected.\n\nGuilt and regret are strongly linked to decision-making because they are emotions that arise in response to our choices and actions. The earliest analysis of regret that took lexicographical, theoretical, and empirical considerations was proposed by Janet Landman  [4] , distinguishing it with similar concepts such as shame, guilt, and remorse.\n\nRegret is a more or less painful cognitive/affective state of feeling sorry for losses, transgressions, shortcomings, or mistakes. The regretted matters may have been sins of commission as well as sins of omission; they may range from the entirely voluntary to the accidental; they may have been executed deeds or entirely mental ones; they may have been committed by oneself or by another person or group; they may be moral or legal transgressions or morally and legally neutral; and the regretted matters may have occurred in the past, the present, or the future.\n\nSimilarly, an elaborate definition  [4]  of guilt would have the following:\n\nDelinquency or failure in respect to one's duty: offense; responsibility for an offense: fault; state of deserving punishment: deserts; the fact of having committed a breach of conduct; and the state of consciousness of one who has committed an offense.\n\nBased on the definition, we can infer that guilt encompasses two states: (i) states of being, and (ii) states of mind. Moreover, guilt pertains to ethical and legal concerns. The primary challenge is how guilt can be differentiated in natural language. The reason is that these emotions in a limited context might overlap and, in many cases, are indistinguishable. In general, it seems impossible to imagine an instance of guilt without regret; however, it is quite possible to imagine an instance of regret without guilt. Thus, regret is once again the broader concept. Regret cannot be only limited to instances of legal, moral, or psychological culpability, but it also includes instances of legally, morally, and subjectively innocuous acts. Furthermore, unlike guilt, regret cannot be limited to one's own free and voluntary actions and failures to act but also includes the acts and omissions of others and deeds over which one had no control. In contrast to psychology, which employs varying evaluation metrics, natural language lacks explicit indicators of underlying morality and ethics. Consequently, the lexicons for both emotions exhibit significant overlap, and distinguishing between these two emotions within a sentence or paragraph poses a complex challenge. In this paper, we attempt to understand how Natural language processing (NLP) can help us to disambiguate regret and guilt-related text.\n\nThis paper addresses the problem of guilt and regret detection as a binary classification task. Our main contributions are as follows:\n\n• We elucidate the textual cues that facilitate the discrimination of guilt from regret (See Section 2). • We created a novel dataset for classifying guilt and regret (See Section 4).\n\n• We trained and validated the proposed binary classification dataset using machine learning and transformer-based classifiers (See Section 5). • We trained and validated the proposed dataset using three large language models (LLMs) reasoning capabilities: (i) Zero shot, (ii) Few shot Chain-of-thought (CoT), and (iii) Tree-of-thought (ToT)). This is done to assess the reasoning capabilities of large language models in tackling intricate emotion-related tasks. We also present detailed error analyses (See Section 6).\n\n2 Regret and Guilt Markers in Text",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Outcome Of Decision",
      "text": "In accordance with regret theories  [5] , the expected utility associated with making a choice can be expressed as a mathematical function of the probability of the choice multiplied by the value of the choice made, minus the level of regret experienced for not choosing the alternative (the superior option that was not selected). Hence, the outcome of regret always has a degree of uncertainty and it is not easily predictable in both actions and inactions. Whereas guilt  [4] , defined as doing something that is below the set standards of the individual, infers that the outcome will be predictable, yet the person still proceeds to do it. This textual marker is visible in many text examples and can be a distinguishing factor.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Focus Of Emotion",
      "text": "The focus of the emotional intensity of regret and guilt differ  [6] . Although both emotions come from negative decision-making, the focus on emotional negativity that stems from regret emphasizes the negative outcome, whereas guilt emphasizes selfblame and responsibility.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Harm",
      "text": "It has been proven that guilt and regret both result from the types of harm (i.e., intrapersonal or interpersonal)  [7, 8] . Guilt is primarily associated with scenarios of harm inflicted upon another individual, whereas regret is associated with intrapersonal and interpersonal harm situations. Although when taken alone, harm is a weak indicator of guilt when combined with other textual markers, it becomes a strong indicator of the distinct emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Discrepancies",
      "text": "In 1987, Higgins  [9]  introduced the idea that two separate types of self-discrepancies are connected to distinct emotional experiences. The first type arises from comparing an individual's real self to their ideal self, representing what they aspire to become. This is referred to as an actual/ideal self-discrepancy. The second type of selfdiscrepancy exists between an individual's real self and what they believe they should be based on societal norms, obligations, and responsibilities, known as actual/ought self-discrepancy.\n\nThe research proposed by Davidai and Gilovich  [10]  found that people actually feel more regret about not being the person they could have been than over not being the person they should have been. Regret thus appears to be more related to ideal selfdiscrepancies than to ought self-discrepancies. While guilt would be associated more closely with ought self-discrepancies  [11] . The research also suggested that guilt association with ought self-discrepancies may vary with culture as in a different culture, it also positively correlated with ideal self-discrepancies. Hence, the discrepancies backed by numerous pieces of research can be an ideal text marker for differentiating guilt and regret.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Future Decision Making",
      "text": "Regret indicates correctness in future decision-making based on experienced or anticipated regret  [12] . In texts, modal verbs such as \"could have,\" \"would have,\" and \"should have\" indicate past experiences and hint at altering decisions based on past events. On the other hand, guilt is a painful judgment, and a person committing the act knows the outcome of the decision and knows that it is wrong, yet still decides to do it  [4] . Hence, fewer words related to \"what could have been\" are seen. The focus of guilt-related actions then becomes how to alter the situation later, apologize, or escape the reality of it.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Moral Self-Blame",
      "text": "Guilt occurs when an individual concentrates on the particular wrongdoing they committed (\"I engaged in a wrongful action\")  [13]  / the focus of the negative energy results in moral self-blaming. This typically occurs when someone has caused harm to a significant relationship and generally instigates efforts to make amends, such as apologizing, rectifying, or reversing the blameworthy behavior.\n\nThis feeling of moral self-blame is significantly higher and more predominant in guilt than regret. The related studies focused on exploring the factors that lead to the emotions of guilt and regret rather than delving into the specific reasons or subjects for which individuals experience regret and guilt. In other words, they looked at what leads people to feel these emotions but did not investigate the specific things that trigger these feelings of regret and guilt.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Literature Review",
      "text": "The concept of regret, often intertwined with guilt, has been examined through various lenses, shedding light on its multifaceted nature. Regret, in its essence, encompasses cognitive and affective dimensions, with varying degrees of emphasis. It emerges as a result of cognitive appraisals, reflective judgment, and critical thinking, standing as a testament to human rationality. Unlike certain emotions, such as anger or fear, regret leans more towards \"cool\" cognitive assessment than \"warm\" emotional reactivity. Hence, the anticipation of regret provides a reason to avoid excessive risk-taking  [3] . It frequently accompanies self-reflection, making it an intensely personal experience. Regret's link to the concept of \"possible selves\" suggests its potential to invoke emotions and emphasizes its role as an affective phenomenon within the realm of human emotions  [4] .\n\nIn contrast, guilt, another closely related emotion, often involves a significant degree of arousal and centers around moral concerns and a sense of duty and obligation. Guilt typically arises from a violation of moral standards or a sense of responsibility for an offense  [1] . Regret encompasses a broad spectrum, including both voluntary and involuntary actions, personal and external circumstances, moral and non-moral domains. Unlike regret, guilt is more confined to situations involving transgressions against moral or legal precepts. Solomon's distinction between the two emotions underscores this difference, where guilt entails \"extreme\" blame, whereas regret is devoid of blame, as it often concerns circumstances beyond one's control  [4] .\n\nThus, regret and guilt, although sharing certain commonalities in their distressing nature and emotional unpleasantness, possess distinct characteristics and triggers. Regret, characterized by its cognitive and affective dimensions, is a versatile emotion that can encompass a wide range of situations, including those without moral or legal implications. Guilt, on the other hand, is more tightly linked to moral concerns and transgressions against established standards, carrying a heavier sense of duty and responsibility  [4] . These nuances highlight the complexity of human emotions and showcase how different emotions (e.g., regret and guilt) serve unique roles in shaping human behavior and decision-making  [3] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Techniques And Dataset For Emotions, Regret And Guilt",
      "text": "Emotion analysis in the realm of digital communication has undergone significant methodological shifts. Early on, emotion analysis was limited in its approach (relay on traditional machine learning algorithms), primarily distinguishing sentiments as positive or negative  [14, 15] . However, the linguistic diversity and complexity in communications presented new challenges as the digital landscape expanded, and binary classification proved inadequate in capturing the depth of human emotional expression  [16] . Therefore, more sophisticated models were required for emotion analysis.\n\nDeep learning, as a transformative approach, introduced a multitude of nuanced techniques (e.g., neural networks) for emotion analysis  [17] . A number of neural networks based approaches have been proposed, such as the Deep Averaging Networks (DAN)  [18] , Recurrent Neural Networks (RNN)  [19] , and Convolutional Neural Networks (CNN)  [20] . These foundational models, along with more sophisticated model, such as the Long Short Term Memory networks (LSTM)  [21]  and attention mechanisms  [22] , found applications in diverse platforms. Furthermore, more recent advancements led to the development of transformer-based models (e.g., LLMs), such as Step-bystep reasoning, which further rely on self-consistency of thought  [23] , chain-of-thought (CoT)  [24] , and tree-of-thought (ToT)  [25] . Therefore, all these techniques and models (e.g., DialogueGCN  [26] , DialogueRNN  [27] , and PAN  [28] ) exemplify the adaptability, highlight deep learning's efficacy in different communication contexts, and pave the way for a more granular understanding of digital sentiment.\n\nLarge Language Models (LLMs) have demonstrated good results in understanding and generating human-like text due to their reasoning abilities  [29] . In the realm of emotion-related tasks, particularly regret and guilt classification, LLMs have been harnessed for their advanced reasoning abilities  [29, 30] . There are two methodologies used to enhance the reasoning capabilities of large language models (LLMs): (i) Stepby-step reasoning, and (ii) chain-of-thought (CoT) reasoning. Both methods aim to improve the model's ability to solve complex problems by generating intermediate reasoning steps, but they differ in their approach and implementation  [31] . In step-by-step reasoning, the model breaks down complex emotional nuances into sequential logical steps and generates intermediate reasoning steps automatically  [32] . This approach often involves integrating external tools, and it has shown substantial improvements over few-shot prompting and chain-of-thought (CoT)  [24, 31, 32] . This approach allows for a detailed analysis of the emotional context, enabling the LLM to discern subtle distinctions between regret and guilt.\n\nOn the other hand, CoT reasoning is a technique that enables LLMs to articulate a series of intermediate reasoning steps. Unlike Step-by-step reasoning which often integrates external tools and has shown improvements over CoT, CoT prompting guides the model to decompose complex problems into intermediate steps, mimicking an intuitive thought process  [24] . This method does not require a large training dataset or modifying the language model's weights, and it is particularly effective when combined with few-shot prompting to capture intricate emotions  [24, 31] .\n\nFurthermore, the exploration of tree-of-thought (ToT) reasoning in LLMs adds another layer of sophistication to emotion-related tasks. By structuring the reasoning process as a tree, the model can allow exploration over coherent units of text and enable deliberate decision-making through considering multiple reasoning paths and the ability to backtrack when necessary  [33] . This hierarchical approach aids in capturing the multi-faceted nature of these emotions, allowing for a nuanced and comprehensive classification. Therefore, these methodologies, including CoT, step-by-step reasoning, and ToT, are crucial for understanding and classification of emotions. Table  1  summarizes existing emotion analysis models along with their respective datasets. Though a wide variety of emotion-related datasets are available. The majority of them are limited to Ekman  [34]  and Plutchik's emotions  [35] . Whereas, the current datasets and approaches used to classify regret and guilt exhibit a limited understanding of regret and guilt and overlap several emotions that are distinct in their expression. In this section, we embark on a comprehensive exploration of the datasets pivotal to our research endeavors. We begin by presenting the Regret Detection and Domain Identification Dataset (ReDDIT)  [17] , followed by an introduction to the Guilt Detection in Text Dataset (VIC)  [37] . After performing thorough pre-processing steps, we tailored the datasets to our specific needs, re-annotated the dataset, and created a unified dataset for guilt and regret detection experiments. We present you the details of annotation and data statistics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Corpora Ekmans Emotions Other Emotions Number",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Collection And Pre-Processing",
      "text": "The ReDDIT dataset introduced in  [17, 29] , serves as a valuable resource for understanding the expression of regret on social media, particularly on the Reddit platform. This dataset offers a fine-grained analysis of regret, classifying texts into three distinct categories: Regret by Action, Regret by Inaction, and No Regret. The data collection process involved scraping posts from three relevant subreddits, including \"regret\", \"regretfulparents\", and \"confession\" spanning from January 1, 2000, to September 10, 2022. The dataset has undergone a meticulous annotation process, with three annotators having backgrounds in information technology and computer science, ensuring its reliability. It provides valuable insights into the domains most commonly associated with regret, especially in the realm of relationships. The dataset's statistics reveal the distribution of posts across different regret types and domains, making it a valuable resource for researchers developing and evaluating natural language processing algorithms focused on detecting and understanding emotional language in online texts.\n\nThe VIC dataset, first introduced in  [37]  and also featured in  [38] , is a pioneering resource that addresses the previously understudied area of guilt detection in text using Natural Language Processing (NLP). This dataset, created by combining and binarizing samples from three existing emotion detection datasets, namely VENT  [39] , ISEAR  [40] , and CEASE  [41] , provides a comprehensive foundation for exploring guilt as an emotion in textual content. The dataset development process involved selecting relevant samples from these datasets, emphasizing guilt-labeled categories. Careful data cleaning and balancing techniques resulted in a dataset with a balanced distribution of guilt and no-guilt samples, making it suitable for machine learning experiments. The VIC dataset's statistics showcase its origin and characteristics, highlighting differences in average sentence and word lengths between guilt and no-guilt samples. Researchers can leverage this dataset to delve into the complex realm of guilt detection in the text, laying the groundwork for future research and advancements in understanding this intricate emotion through NLP methods.\n\nBoth dataset were subjected to meticulous preprocessing to address class imbalance and ensure a judicious distribution of samples. For VIC, we initiated the procedure by grouping the dataset by the \"origin\" column, subsequently applying random sampling with replacement to select 500 samples from each group. These operations were followed by a judicious selection of pertinent columns, namely \"text\" and \"label\", signifying textual content and corresponding labels, respectively. To rectify the class imbalance in ReDDIT and align the dataset with our research objectives, we initiated the process by grouping the dataset by the \"label\". We then executed random sampling with replacement to select 500 samples from each group. Furthermore, to enhance label interpretability, we employed a label transformation scheme, replacing numeric labels (0, 1, and 2) with descriptive labels, specifically \"no regret\" and \"regret.\"\n\nThe culminating step in our dataset preparation involved the concatenation of \"VIC\" and \"ReDDIT\" along the rows, a procedure executed employing. The resultant dataset is distinguished by its balanced composition, affording parity between the various class categories. Table  2  shows the statistics of the dataset after pre-processing and unification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Annotator Selection And Procedure",
      "text": "Three annotators were selected based on their background in multidisciplinary research in psychology and computation, all three held a Masters's degree. Thirty samples were provided to the qualified annotators for annotation. The annotations were evaluated manually and the common problems in understanding were discussed. One annotator was female, while the other two were male. Each annotator was given a subset of the dataset and was asked to mark the most suitable label according to the provided guideline explained in Section 2.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Inter-Annotator Agreement",
      "text": "Inter-annotator agreement (IAA) measures the degree of consensus among annotators, taking into account the possibility of random agreement. For the final labels, three annotators achieved a 90.67% Cohen's Kappa Score  [42] . These scores demonstrate the reliability of the datasets and reflect the rigorous annotation process that was followed. While, for the labels that were previously annotated in the original dataset, we calculated Cohen's Kappa Coefficient which reached 85.57% with our final labels after we removed and filtered all instances which were insufficient for the distinction between regret and guilt.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Methodology",
      "text": "The section presents the details of deep learning and machine learning models used for binary classification. We also employed three prompting techniques in our experimental design using large language models, utilizing GPT-3.5-Turbo 1  , as our language models (LLM). All LLM experiments were carried out using LangChain 2  .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Pre-Processing",
      "text": "We followed conventional pre-processing methods, including the removal of duplicate instances, punctuations, URLs, and usernames. Nevertheless, we chose not to incorporate additional pre-processing steps such as stopword removal and lemmatization. This choice was made because language models and transformers rely on sentences in their authentic, unaltered form.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Algorithm Design",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Machine Learning",
      "text": "We explored a variety of traditional machine learning algorithms to compare and contrast with the emerging transformer architectures, including large language models. Central to our assessment is the utilization of diverse ensemble learning models which have historically demonstrated robust performance across a plethora of tasks. Specifically, we examine the efficacy of Random Forest  [43] , AdaBoost  [44] , and XGBoost  [45]  classifiers for the emotion classification task, leveraging their inherent strengths in handling complex decision spaces.\n\nInitial processing involved converting the textual data into a numerical format amenable to machine learning models. We derive vector representations of the text by averaging word embeddings using FastText  [46]  word vectors, effectively capturing the semantic essence of each sentence within a high-dimensional feature space. This transformation results in embeddings that serve as inputs to our classifiers.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Transformers",
      "text": "The advent of Transformer-based models has revolutionized the field of Natural Language Processing (NLP). Leveraging the powerful representational capabilities of these pre-trained models, our study evaluates their performance on the complex task of emotion classification in text. We utilize a suite of Transformer architectures, including BERT  [47] , RoBERTa  [48] , AlBERT  [49] , XLNet  [50] , DistilBERT  [51] , and ELEC-TRA  [52] , to probe the breadth and depth of each model's understanding of nuanced emotional constructs. We instantiate a designated Transformer model and fine-tune it to the emotion classification task at hand. The fine-tuning process employs a learning rate of 3 × 10 -5 over 15 epochs, with sequence lengths and batch sizes carefully calibrated to balance computational efficiency and model performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Llms",
      "text": "1. Zero-shot prompting: The Zero-Shot  [53]  (ZS) approach aims to directly classify the sentiment of a text as \"Regret\" or \"Guilt\" without explicitly modeling the underlying reasoning process. In this approach, we leverage the powerful pretrained language model GPT-3.5-Turbo to perform one-shot text classification. We constructed a simple template prompt for GPT-3.5-Turbo:\n\nGiven a text, identify if it represents ''Regret'' or ''Guilt''. Only write the final class. Text: <text to classify> Final Class: output We accessed GPT-3.5-Turbo via OpenAI's API, calibrated at temperature 0 for determinism and reduced diversity. The model was not explicitly trained on our data, relying solely on pre-trained knowledge and one-shot generalization. 2. Few-shot Chain of Thought prompting: The Chain of Thought prompting strategy  [24] , as applied within our research, constitutes a template-driven approach that impels a language model to simulate a cognitive process akin to human reflection and reasoning. The approach is crafted to distill the model's rationale behind its classification of emotional sentiments into discernible \"Thoughts,\" explicitly delineating a synthesized logical derivation that leads to its final choice between \"regret\" and \"guilt.\" In the prompt format used for CoT, the model is provided with five examples of regret and guilt with their respective thoughts and text instances that capture a human experience or sentiment. Following this, the model is required to classify the sentiment as either \"regret\" or \"guilt.\" Most critically, it must augment its classification with an articulated \"Thought,\" a narrative element that methodically elucidates the reasoning behind its decision. This constructed narrative mirrors the logical steps that individuals typically employ when interpreting complex emotions and providing justifications for their interpretations.\n\n3. Tree of Thought prompting: For this approach  [25] , we encoded the key expert knowledge provided in the prompt about differentiating \"Regret\" and \"Guilt\" into seven distinct rules: The resulting ToT simulates a round-robin discussion where each expert analyzes the text, applies relevant rules, and explains their reasoning. They build upon each other's contributions, acknowledging and correcting misunderstandings until a consensus on the final class (\"Regret\" or \"Guilt\") is reached.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Evaluation",
      "text": "Our evaluation strategy for machine and deep learning models employed stratified K-Fold cross-validation with five folds, ensuring that each class's distribution is uniform across each partition. This method lends statistical rigor to our analysis, providing a cross-sectional view of model performance while mitigating sampling variability. Each fold undergoes meticulous processing where the train set is used to fit the model, and the resulting classifier is used to predict labels on the test set. We report on a suite of metrics -accuracy, precision, recall, and F1 score to provide a comprehensive understanding of model behavior. Scores are calculated both with weighted and macroaverage options, offering insight not only into the aggregate performance but also into how each model performs in balance and fairness across classes. These metrics collectively give us an opportunity to scrutinize the models' decision-making fabric, benchmarking them against advanced NLP models. For LLMs, all the data was used as the test set. ToT needed manual evaluation to come up with the final answers on many of the instances where the output was not provided as described in the prompted structure",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "The tables 3, 4, and 5 present the results of the binary classification for machine learning, deep learning, and large language models. We can see that the machine learning models, including Random Forest, AdaBoost, and XGBoost, demonstrate comparable performance with accuracy ranging from 0.840 to 0.845. These models exhibit balanced precision, recall, and F1 scores for both weighted (W) and macro (M) averages. In contrast, the transformer models, such as Bert, Roberta, Albert, Xlnet, Electra, and Distilbert, outperform the traditional machine learning models, achieving accuracy scores between 0.894 and 0.908. These transformers consistently show higher precision, recall, and F1 scores across both weighted and macro averages. Lastly, the language model GPT-3.5-turbo performs variably across different tasks. In zero-shot learning, shows moderate accuracy and F1 scores, while few-shot learning improves accuracy and F1 scores. However, in the task-oriented transfer (ToT) setting, the model's performance drops, indicating an inability to reason for our task. Overall, the transformer models outshine traditional machine learning models and GPT-3.5-turbo in the given evaluation metrics.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Error Analyses And Future Work",
      "text": "We conducted an error analysis on our best-performing model (AlBERT) and large language model (GPT-3.5-Turbo). Figure  1  shows the confusion metrics between labels for every fold. In many of the instances of guilt that were identified as regret, the algorithm failed to understand the \"moral self-blame\" indicator. Although compared to Machine Learning, transformers were able to understand the context better to make the distinction. On the other hand, instances of regret that are identified as guilt contain apologetic keywords such as \"sorry\", however, in reality, do not have strong indicators supporting guilt i.e. \"I hurt myself again. I stopped using this for a little while, cause of school and stuff, and a crap tom happened while I was gone. I hurt myself, got a crush, confessed to that crush, got good grades at school, and through all of that I was feeling miserable.\", where the focus of emotion is the negative outcome. In contrast, large language models presented several problems in understanding complex contexts. Tree of Thought method, often made a point and then deviated from it, or was inconclusive to which class it should belong to. The ambiguity of the responses also included responses such as \"i don't know\" or \"difficult to determine\", which in reality are clear examples of their respective classes. The LLM methods also presented repetitive patterns and unstructured responses. All of this made it difficult to evaluate and determine the final class automatically. Although we believe, fine-tuning LLM's might counter some of these problems, however, in terms of reasoning abilities LLM's lacked understanding of the complex emotions in our task.\n\nIn the future, we would like to extend the dataset by providing neutral instances and negative emotions that are distinct in their understanding but can overlap with the existing labels. We also want to experiment thoroughly on fine-tuning the existing LLMs to see the potential of LLMs of all sizes and types in understanding these complex emotions.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "Drawing from comprehensive research, empirical analysis, and nuanced discussions on the intricate relationship between emotions and decision-making, this paper underscores the symbiotic connection between emotions and the choices we make. It delves into the depths of human psychology, drawing a fine line between the feelings of guilt and regret that arise from our actions and choices. The paper elucidates the covert indicators that help distinguish between these emotions, which, despite their apparent similarities, emerge from different cognitive processes and moral evaluations.\n\nThe paper contributes to the field by successfully mapping textual cues to emotional states, distinguishing between guilt and regret with considerable accuracy. Testing the boundaries of large language models and traditional machine learning algorithms provides compelling evidence for the superiority of transformer-based models in understanding the nuances of emotional language. Furthermore, the development of a novel dataset, tailored annotation processes, and meticulous error analysis add structural integrity to this work.\n\nIn closing, this paper not only bridges the gap between theoretical understandings of emotions and their linguistic realizations but also paves the way for technological solutions that can perceive and process complex human emotions with unprecedented depth and sensitivity. The future work proposed promises to extend the reach of this research, inviting more comprehensive explorations and technological advancements that can fine-tune the discernment of AI to interpret emotions as delicately as the human mind does. Within the intricate tapestry of language, emotion, and technology, this paper marks a significant stride in the journey toward a more emotionally intelligent future.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the confusion metrics between labels",
      "page": 12
    },
    {
      "caption": "Figure 1: Confusion matrix of every 5-fold evaluation with AlBert",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes existing emotion analysis models along with their respective datasets.",
      "page": 6
    },
    {
      "caption": "Table 1: Publicly Available Datasets [36] for Textual Emotion Detection. Ekman’s 6 basic",
      "page": 7
    },
    {
      "caption": "Table 2: shows the statistics of the dataset after pre-processing",
      "page": 8
    },
    {
      "caption": "Table 3: The table presents the machine learning results. Evaluation metrics",
      "page": 12
    },
    {
      "caption": "Table 4: The table presents the transformers results. Evaluation",
      "page": 12
    },
    {
      "caption": "Table 5: The table presents the LLM results. Evaluation metrics are denoted as Acc",
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion. Handbook of Social Psychology",
      "authors": [
        "D Keltner",
        "J Lerner"
      ],
      "year": "2010",
      "venue": "Emotion. Handbook of Social Psychology"
    },
    {
      "citation_id": "2",
      "title": "PolyHope: Two-level hope speech detection from tweets",
      "authors": [
        "F Balouchzahi",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Emotion and decision making",
      "authors": [
        "J Lerner",
        "Y Li",
        "P Valdesolo",
        "K Kassam"
      ],
      "year": "2015",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "4",
      "title": "Regret: A Theoretical and Conceptual Analysis",
      "authors": [
        "J Landman"
      ],
      "year": "1987",
      "venue": "Journal for the Theory of Social Behaviour"
    },
    {
      "citation_id": "5",
      "title": "Prediction of some stochastic events: A regret equalization model",
      "authors": [
        "M Schoeffler"
      ],
      "year": "1962",
      "venue": "Journal of experimental psychology"
    },
    {
      "citation_id": "6",
      "title": "What I did\" versus \"what I might have done\": Effect of factual versus counterfactual thinking on blame, guilt, and shame in prisoners",
      "authors": [
        "D Mandel",
        "M Dhami"
      ],
      "year": "2005",
      "venue": "Journal of Experimental Social Psychology"
    },
    {
      "citation_id": "7",
      "title": "Guilt and regret: The determining role of interpersonal and intrapersonal harm",
      "authors": [
        "M Berndsen",
        "J Van Der Pligt",
        "B Doosje",
        "A Manstead"
      ],
      "year": "2004",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "8",
      "title": "The role of interpersonal harm in distinguishing regret from guilt",
      "authors": [
        "M Zeelenberg",
        "S Breugelmans"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "9",
      "title": "Self-discrepancy: a theory relating self and affect",
      "authors": [
        "E Higgins"
      ],
      "year": "1987",
      "venue": "Psychological review"
    },
    {
      "citation_id": "10",
      "title": "The ideal road not taken: The self-discrepancies involved in people's most enduring regrets",
      "authors": [
        "S Davidai",
        "T Gilovich"
      ],
      "year": "2018",
      "venue": "Emotion"
    },
    {
      "citation_id": "11",
      "title": "The role of self-discrepancies in distinguishing regret from guilt",
      "authors": [
        "X Zhang",
        "M Zeelenberg",
        "A Summerville",
        "S Breugelmans"
      ],
      "year": "2021",
      "venue": "Self and Identity"
    },
    {
      "citation_id": "12",
      "title": "Seeing the future: Theoretical perspectives on future-oriented mental time travel",
      "authors": [
        "C Hoerl",
        "T Mccormack"
      ],
      "year": "2016",
      "venue": "Seeing the future: Theoretical perspectives on future-oriented mental time travel"
    },
    {
      "citation_id": "13",
      "title": "Shame and the motivation to change the self",
      "authors": [
        "B Lickel",
        "K Kushlev",
        "V Savalei",
        "S Matta",
        "T Schmader"
      ],
      "year": "2014",
      "venue": "Emotion"
    },
    {
      "citation_id": "14",
      "title": "Text emotion analysis: A survey",
      "authors": [
        "R Li",
        "Z Lin",
        "H Lin",
        "W Wang",
        "D Meng"
      ],
      "year": "2018",
      "venue": "Journal of Computer Research and Development"
    },
    {
      "citation_id": "15",
      "title": "Survey of Fake News Datasets and Detection Methods in European and Asian Languages",
      "authors": [
        "M Amjad",
        "S Butt",
        "A Zhila",
        "G Sidorov",
        "L Chanona-Hernandez",
        "A Gelbukh"
      ],
      "year": "2022",
      "venue": "Acta Polytechnica Hungarica"
    },
    {
      "citation_id": "16",
      "title": "Borrow from rich cousin: transfer learning for emotion detection using cross lingual embedding",
      "authors": [
        "Z Ahmad",
        "R Jindal",
        "A Ekbal",
        "P Bhattachharyya"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "17",
      "title": "Regret detection and domain identification from text",
      "authors": [
        "F Balouchzahi",
        "S Butt",
        "G Sidorov",
        "Gelbukh Reddit"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "18",
      "title": "Deep unordered composition rivals syntactic methods for text classification",
      "authors": [
        "M Iyyer",
        "V Manjunatha",
        "J Boyd-Graber",
        "Iii Daumé"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing"
    },
    {
      "citation_id": "19",
      "title": "Dynamic recurrent neural networks: Theory and applications",
      "authors": [
        "C Giles",
        "G Kuhn",
        "R Williams"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "20",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "21",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "22",
      "title": "Computational modelling of visual attention",
      "authors": [
        "L Itti",
        "C Koch"
      ],
      "year": "2001",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "authors": [
        "X Wang",
        "J Wei",
        "D Schuurmans",
        "Q Le",
        "Chi Narang"
      ],
      "year": "2022",
      "venue": "Self-consistency improves chain of thought reasoning in language models"
    },
    {
      "citation_id": "24",
      "title": "Chain-ofthought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "authors": [
        "S Yao",
        "D Yu",
        "J Zhao",
        "I Shafran",
        "T Griffiths",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "Tree of thoughts: Deliberate problem solving with large language models"
    },
    {
      "citation_id": "26",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation"
    },
    {
      "citation_id": "27",
      "title": "An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "Cambria Dialoguernn"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "28",
      "title": "Gated recurrent neural network approach for multilabel emotion detection in microblogs",
      "authors": [
        "P Rathnayaka",
        "S Abeysinghe",
        "C Samarajeewa",
        "I Manchanayake",
        "M Walpola",
        "R Nawaratne"
      ],
      "year": "2019",
      "venue": "Gated recurrent neural network approach for multilabel emotion detection in microblogs"
    },
    {
      "citation_id": "29",
      "title": "Regret and Hope on Transformers: An Analysis of Transformers on Regret and Hope Speech Detection Datasets",
      "authors": [
        "G Sidorov",
        "F Balouchzahi",
        "S Butt",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "30",
      "title": "Transformer-based extractive social media question answering on TweetQA",
      "authors": [
        "S Butt",
        "N Ashraf",
        "Mhf Siddiqui",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2021",
      "venue": "Computación y Sistemas"
    },
    {
      "citation_id": "31",
      "title": "Language Models Perform Reasoning via Chain of Thought",
      "authors": [
        "J Wei",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Google AI Blog Google Research Online verfügbar unter"
    },
    {
      "citation_id": "32",
      "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
      "authors": [
        "B Paranjape",
        "S Lundberg",
        "S Singh",
        "H Hajishirzi",
        "L Zettlemoyer",
        "M Ribeiro"
      ],
      "year": "2023",
      "venue": "ART: Automatic multi-step reasoning and tool-use for large language models"
    },
    {
      "citation_id": "33",
      "title": "Towards better chain-of-thought prompting strategies: A survey",
      "authors": [
        "Z Yu",
        "L He",
        "Z Wu",
        "X Dai",
        "J Chen"
      ],
      "year": "2023",
      "venue": "Towards better chain-of-thought prompting strategies: A survey"
    },
    {
      "citation_id": "34",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "35",
      "title": "Emotions: A general psychoevolutionary theory. Approaches to emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1984",
      "venue": "Emotions: A general psychoevolutionary theory. Approaches to emotion"
    },
    {
      "citation_id": "36",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Guilt Detection in Text: A",
      "authors": [
        "Agm Meque",
        "N Hussain",
        "G Sidorov",
        "A Gelbukh"
      ],
      "year": "2023",
      "venue": "Step Towards Understanding Complex Emotions"
    },
    {
      "citation_id": "38",
      "title": "Leveraging the power of transformers for guilt detection in text",
      "authors": [
        "Agm Meque",
        "J Angel",
        "G Sidorov",
        "A Gelbukh"
      ],
      "venue": "Leveraging the power of transformers for guilt detection in text"
    },
    {
      "citation_id": "39",
      "title": "Sharing emotions at scale: The vent dataset",
      "authors": [
        "N Lykousas",
        "C Patsakis",
        "A Kaltenbrunner",
        "V Gómez"
      ],
      "year": "2019",
      "venue": "Proceedings of the International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "40",
      "title": "Evidence for universality and cultural variation of differential emotion response patterning: Correction",
      "authors": [
        "K Scherer",
        "H Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.67.1.55"
    },
    {
      "citation_id": "41",
      "title": "Cease, a corpus of emotion annotated suicide notes in English",
      "authors": [
        "S Ghosh",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "42",
      "title": "Beyond kappa: A review of interrater agreement measures",
      "authors": [
        "M Banerjee",
        "M Capozzoli",
        "L Mcsweeney",
        "D Sinha"
      ],
      "year": "1999",
      "venue": "Canadian journal of statistics"
    },
    {
      "citation_id": "43",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "44",
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "authors": [
        "Y Freund",
        "R Schapire"
      ],
      "year": "1997",
      "venue": "Journal of computer and system sciences"
    },
    {
      "citation_id": "45",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "46",
      "title": "zip: Compressing text classification models",
      "authors": [
        "A Joulin",
        "E Grave",
        "P Bojanowski",
        "T Mikolov",
        "Fasttext"
      ],
      "year": "2016",
      "venue": "5th International Conference on Learning Representations, ICLR 2017 -Workshop Track Proceedings"
    },
    {
      "citation_id": "47",
      "title": "Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "48",
      "title": "A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen"
      ],
      "year": "2019",
      "venue": "A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "49",
      "title": "Lite BERT for Self-supervised Learning of Language Representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut",
        "Albert"
      ],
      "year": "2019",
      "venue": "Lite BERT for Self-supervised Learning of Language Representations"
    },
    {
      "citation_id": "50",
      "title": "Generalized Autoregressive Pretraining for Language Understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Le Xlnet"
      ],
      "year": "2019",
      "venue": "Generalized Autoregressive Pretraining for Language Understanding"
    },
    {
      "citation_id": "51",
      "title": "a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "Wolf Distilbert"
      ],
      "year": "2019",
      "venue": "a distilled version of BERT: smaller, faster, cheaper and lighter"
    },
    {
      "citation_id": "52",
      "title": "Pre-training Text Encoders as Discriminators Rather Than Generators",
      "authors": [
        "K Clark",
        "M Luong",
        "Q Le",
        "C Manning",
        "Electra"
      ],
      "year": "2020",
      "venue": "Pre-training Text Encoders as Discriminators Rather Than Generators"
    },
    {
      "citation_id": "53",
      "title": "Zero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly",
      "authors": [
        "Y Xian",
        "C Lampert",
        "B Schiele",
        "Z Akata"
      ],
      "venue": "Zero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly"
    }
  ]
}