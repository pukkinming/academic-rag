{
  "paper_id": "2304.06910v2",
  "title": "Hcam -Hierarchical Cross Attention Model For Multi-Modal Emotion Recognition",
  "published": "2023-04-14T03:25:00Z",
  "authors": [
    "Soumya Dutta",
    "Sriram Ganapathy"
  ],
  "keywords": [
    "Hierarchical learning",
    "Co-attention models",
    "Multi-modal fusion",
    "Emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network parameters in the audio layers, text layers as well as the multi-modal co-attention layers, are hierarchically trained for the emotion classification task. We perform experiments on three established datasets namely, IEMOCAP, MELD and CMU-MOSI, where we illustrate that the proposed model improves significantly over other benchmarks and helps achieve state-ofart results on all these datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "H UMAN emotions, expressed in a complex multi-modal manner, play a central role in human interactions and inter-person communications. As automated systems play a ubiquitous role in day-to-day lives, the understanding of the human emotions becomes a crucial step in the design of these systems. The machines capable of performing emotion recognition can enable the development of personalized humancomputer interfaces like conversational agents  [1] , social media analytics  [2] , customer call centres  [3]  and mental health monitoring systems  [4] . The key task is termed as emotion recognition in conversation (ERC).\n\nEmotion recognition in conversational data entails a number of challenges, namely multiple (overlapping) speakers, shortterm and long-term dependencies  [5] , short duration of turn events, background noise in audio etc. Further, emotion recognition task is inherently multi-modal, where the information is expressed in a variety of ways such as facial expressions  [6] , speech  [7] , gestures  [8] , physiological signals  [9]  or through a combination of these. The different modalities contain varying degrees of information relating to the underlying emotion and hence, designing a joint multi-modal approach for emotion recognition is usually considered  [10] .\n\nThe previous works have explored the use of audio with visual signals  [11] ,  [12]  and text with audio signals  [13] ,  [14] . Further, there have been attempts to use all the three modalities namely visual, audio and text  [15] . While the ability to perceive emotions in a multi-modal way is required, it is also necessary to recognize the emotions from each modality in a robust manner, for scenarios where the data from some of the modalities is unavailable or noisy.\n\nIn this paper, we propose a novel hierarchical cross attention model (HCAM) for the problem of multi-modal emotion recognition. Our modeling framework, involving large representation learning networks from speech and text, uses a hierarchical training process. The training paradigm consists of three distinct stages, where the first stage trains unimodal predictors in a context-agnostic fashion. The contextual information, being an essential component in conversational emotion recognition, is added subsequently in the second stage. The final stage involves the fusion of the acoustic and textual streams using a co-attention module. The audio features are extracted using a wav2vec model  [16]  while textual representations are derived using RoBERTa  [17]  model. The co-attention is a mechanism that jointly models auditory and textual information. With a natural symmetry between the audio based and text based embeddings, this fusion technique guides the model to attend to the multi-modal contextual information for emotion recognition. In order to train this model in a hierarchical fashion, we explore the usage of the supervised contrastive loss  [18] .\n\nThe experiments are performed on three established datasets, namely IEMOCAP  [19] , MELD  [20]  and CMU-MOSI  [21] . Although the type of emotions, duration of the utterances, length of the conversations, noise or distortion in the audio and style of the conversations are different across the datasets, the same HCAM architecture is proposed.\n\nThe key contributions from this work are:\n\n• We propose a hierarchical approach for the multi-modal emotion recognition, where the information is first processed at the utterance level in each of the modalities, followed by inter-utterance conversation modeling and subsequently, multi-modal processing with crossattention. We experimentally establish that the hierarchical modeling is important for improving the emotion recognition performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works A. Audio Feature Extraction",
      "text": "The initial approaches for emotion recognition used features that were mostly knowledge driven like amplitude, pitch and spectral profile, as proposed by Sauter et. al.  [22] . Luengo et. al.  [23]  used prosodic parameters for SER, while segment level prosodic features were used by Koolagudi et. al.  [24] . The melfrequency cepstral coefficients (MFCCs) were highlighted to provide the best emotion classification performance by Eyben et. al  [25] . Recently, the statistical descriptors of a number of knowledge driven features were found to be significantly better for the SER task by Schuller et. al, as part of the Interspeech para-linguistics challenge  [26] . These features were further refined  [27]  to create a minimalist set of parameters (Opensmile toolkit  [28] ).\n\nThe progress in deep learning in the recent years have motivated researchers to develop audio feature extractors which are learnable in nature, like the SincNet features by Ravanelli et. al.  [29] , learnable audio front-end (LEAF) by Zeghidour et. al.  [30]  and interpretable Gaussian filters by Agrawal et.al  [31] ,  [32] . Recently, unsupervised and self-supervised approaches have been proposed using a large corpus of unlabeled speech data  [33] ,  [16] . Several recent works in emotion recognition have explored the use of these representations  [34] ,  [35] ,  [36] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Text Feature Extraction",
      "text": "Early approaches for text feature extraction used the bagof-words approach in conjunction with word relation features, as proposed by Xia et. al.  [37] . The recent models, inspired by deep learning, use prediction tasks to learn text embeddings. One such attempt, termed the word2vec model by Mikolov et. al  [38] , is widely used for feature extraction in text sentiment analysis  [39] ,  [40] ,  [41] . With the development of recurrent and attention networks such as bidirectional long short term memory networks (B-LSTM), gated recurrent unit (GRU) and transformers  [42] , improved language models were developed like BERT  [43]  and RoBERTa  [17] . The BERT embeddings have resulted in improvements for a variety of downstream tasks like sentiment analysis  [44] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Multi-Modal Fusion",
      "text": "The fusion of multiple modalities has been shown to be effective in emotion recognition  [45] . The early attempts using a concatenation of the representations has been replaced with sophisticated techniques like multi-modal transformers  [46] ,  [47] . A multi-view sequential learning architecture was proposed by Zadeh et. al  [48] . This was further improved using a dynamic fusion graph  [49] . The attention based fusion approach is also pursued in various vision-language tasks  [50] ,  [51] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Incorporating Contextual Information In Erc",
      "text": "The presence of multiple speakers in a conversation may result in short-term and long-term dependencies. This presents a significant challenge in ERC, as highlighted by Poria et. al  [5] . To address this, Poria et. al  [45]  used LSTM networks to capture inter-utterance context in the conversation. This was further refined by Majumder et. al  [52]  in their model, named DialogueRNN. The work by Ghosal et. al proposed the use of graph convolutional networks  [53] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Contrast With Prior Works",
      "text": "In contrast to the prior works, the proposed HCAM framework is novel in the following aspects.\n\n• We propose a curriculum learning  [54]  based design of the modeling stages. In this design, the easy task of recognizing the emotion states at an utterance level of a conversation is learned initially. The more complex task of inter-utterance contextual modeling is designed on top of the utterance level model with recurrent layers. Further, the neural attention based modeling layers enable the multi-modal integration. This hierarchical modeling framework is shown to efficiently learn the underlying emotion labels from speech and text representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Background",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Wav2Vec",
      "text": "The wav2vec is a representation learning framework based on principles of self-supervision  [33] . In the wav2vec 2.0 model  [16] , the audio signal is windowed into short overlapping frames. Each windowed segment is passed through convolutional feature extractor layers, following which a quantization module allows encoding of the representations in a discrete space. These representations are contextualized by means of transformer encoder layers. The network is pretrained with a self-supervised contrastive loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Roberta",
      "text": "In the recent years, one of the significant contributions towards creating a large scale language model, is by Devlin et.al  [43] . This architecture, called bidirectional encoder representations from transformer (BERT), was trained with two tasks on a corpus of textual data, namely, predicting a word masked out in a sentence (called masked language modeling) and to predict whether two sentences semantically follow each other (referred to as next sentence prediction). Liu et. al  [17]  trained this architecture on a larger corpus of textual data after removing the next sentence prediction task. This pretrained model, known as robust optimized BERT approach (RoBERTa), also optimizes several other hyper-parameters in the design of the model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Self And Cross Attention",
      "text": "The self-attention mechanism, as proposed by Vaswani et. al  [42] , considers a sequence of length N , and of dimension of d k . This sequence is converted into three matrices,\n\nSelf-attention is involved in computing the similarity between query representation (denoted by Q) with key representation (denoted by K). This similarity matrix, converted to a probability distribution by the softmax function, is then used to take a weighted sum of the value representations (denoted by V ). Finally, the query matrix (Q) is added to this weighted sum followed by a layer normalization block.\n\nThe cross-attention network is similar to the self-attention module with a key difference. Here, the query matrix and the key/value matrices are constructed from representations of different modalities. If we consider the query matrix from audio modality (denoted as Q A ), and the key/value matrices from the text modality (denoted as K T /V T ), the cross-attention network operations are,",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Proposed Architecture",
      "text": "The block diagram of the proposed model is shown in Fig.  1 . As the entire model is a relatively large architecture, driven by the two modalities of text and audio, our model has three distinct stages which are trained hierarchically, where each stage uses the pre-trained model parameters of the previous stage without fine-tuning.\n\nAll the stages are trained hierarchically using a weighted combination of cross-entropy loss and the supervised contrastive loss function (Sec. IV-D). The model implementation is made available publicly  1  .\n\nSelf attention block",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self-Attention Block",
      "text": "Bi-GRU Position-wise FF network",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Hcam Stage I",
      "text": "In the first stage, we train utterance-level embedding extractors from audio and text. The models are trained to classify individual utterances without considering the interutterance conversational context. The models trained in this stage classify the individual utterances into the corresponding emotion classes based on cues present in either the audio signal or the text transcript. The pre-trained feature extractors are generally models with large computational requirements, and this constrains the number of utterances that can be processed in every iteration. The large number of utterances in a single conversation inhibits the fine-tuning of these feature extractors in previous works, as they process conversations as a whole. Our hierarchical modeling allows us to fine-tune the embeddings from the pre-trained feature extractors for improved emotion classification for each individual utterance in this training stage.\n\n1) Audio embedding extractor: The audio is input to the wav2vec2.0 large model  [16] , pre-trained on Libri-light  [55] , CommonVoice  [56] , Switchboard  [57]  and Fisher  [58]  datasets. The model is further fine-tuned on 300 hours of noisy telephone conversation data in the Switchboard corpus. Inspired by the strategy proposed in Pepino et. al.  [36] , we fine-tune the transformer layers in the wav2vec2.0 network while keeping the lower convolutional layers unchanged. The hidden layer outputs from the wav2vec model, for all the transformer layers, are summed at the frame-level, and passed through a 1-D CNN network. Finally, the embeddings from the CNN network are average pooled over the utterance level to generate audio embeddings for the given utterance. The embeddings obtained at the output of the 1D-CNN network are considered for the contextual GRU layer.\n\n2) Text embedding extractor: The embedding extractor on the text data follows a similar architecture to that of the audio feature extraction. We obtain the word embeddings through a pre-trained RoBERTa model  [17]  and splice the",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Cross-Attention Block",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Attention Block",
      "text": "Self-attention block Self-attention block",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Hcam Stage Ii",
      "text": "The utterance-level embeddings obtained from the previous modeling stage are used in this stage, where we introduce inter-utterance context by means of a bidirectional gated recurrent unit (Bi-GRU) architecture with self-attention mechanism. The representations extracted from models trained on each individual utterance in stage I are further enhanced with the conversational context information.\n\n1) Inter-utterance contextual GRU: We propose a simple block, called the contextual-GRU, which takes into account the information from all utterances in the conversation. The block diagram of the contextual GRU is shown in Fig.  2 . While the bi-GRU itself can incorporate contextual information, it may not capture long term dependencies in the conversations that have a large number of utterances. For this reason, selfattention is used, which allows effective modeling of the long-term context. The output from the self-attention block is processed by a position wise feed forward layer with ReLU activation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Hcam Stage Iii",
      "text": "The third stage of the model consists of the effective fusion of the embeddings from the different modalities.\n\n1) Multi-modal fusion: The network architecture of the co-attention block, is shown in Fig.  3 , and it consists of two sub-blocks, namely cross-attention and self-attention. The self-attention and cross attention schemes were discussed in Sec. III-C. In the co-attention network, we exploit the crossattention between the modalities. There are two ways of performing the cross-attention between audio and text, one where the query representations are derived from the audio data, while the key and value representations are derived from the text data. The other way is the reversal of the roles of the audio and text. The cross-modal embeddings at the output of the cross-attention blocks are further enriched by adding a self-attention block. The two arms of cross-attention, as shown in Fig.  3 , are concatenated and this forms the multi-modal representation.\n\nThe final representations, which combines the information from audio and text, is passed through a position-wise feed forward layer. D. Loss function 1) Supervised contrastive loss across conversations: We explore the supervised contrastive loss function, proposed by Khosla et.al  [18] , that encourages similarity between utterance representations from the same emotion class. Let us consider a mini-batch size of B, where the utterancelevel representations, derived from multiple conversations, are denoted as {x 1 , x 2 , . . . , x B }. The features that appear in this loss formulation are normalized, that is, ||x i || = 1 ∀i = {1, 2, . . . , B}. We denote the corresponding labels as {y 1 , y 2 , . . . , y B }. Considering the sample with index j, the set of positive examples from the mini-batch is denoted by P j : {i ∈ B s.t. y i = y j }. The supervised contrastive loss is,\n\nwhere, τ is a hyper-parameter indicating the temperature of this loss. Generally, contrastive losses have been used in representation learning tasks. The dependence of the contrastive loss on the ground truth labels enables one to use these losses in supervised settings too. In addition to the cross-entropy loss, this loss makes the system focus on the hard-to-classify samples. However, unlike in representation learning tasks, the datasets for ERC are not large enough to be able to classify the utterances by means of a contrastive loss alone. We therefore, use the supervised contrastive loss in combination with the cross-entropy loss for training our emotion recognition modules.\n\n2) Combined loss: We use a convex combination of the the cross entropy loss and the supervised contrastive loss. The final loss, used to train all the stages of our model, is given by,\n\nwhere β is a hyper-parameter in the range of [0, 1].",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "E. Inference",
      "text": "During the inference in a particular stage, we combine the predictions of the model from the previous stage. Thus, once the contextual GRU model is trained in stage II, we combine the predictions of the audio and text models from stage I. During the inference in stage III, after fusion of the audio and text representations, we combine the predictions of the contextual GRU model (for audio and text separately). Let the softmax outputs for an utterance x at the end of stage I be p 1 a (for audio) and p 1 t (for text). These are combined with the unimodal contextual GRU predictions (denoted by p 2 a and p 2 t ), with weights denoted by α 1→2 a and α 1→2 t , as follows,\n\nSimilarly, we combine the outputs from stage II with the predictions from the co-attention module (p c ) as follows,\n\nwhere ŷ(x) refers to the final predictions used for classification. All the combination weights used in equations (  9 ),  (10)  and  (11)  are decided based on validation set performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluate our work on three widely used datasets, IEMO-CAP  [19] , MELD  [20]  and CMU-MOSI  [21] . Unlike the first two, CMU-MOSI is not multi-speaker in nature as it has single speaker monologues. Further, we use only text and audio modalities for emotion recognition task in this paper.\n\n1) IEMOCAP: The IEMOCAP dataset consists of 151 video recordings split into 5 sessions. Each of these sessions is a conversation between a pair of people, one male and one female. Each recording is split into multiple utterances. There are a total of 10, 039 utterances, each of which is labeled by human annotators as belonging to one of the 10 emotions -angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted or \"other\". Keeping in line with previous works, we do a four-way classification task where we consider angry, happy, sad, neutral and excited categories (with excited and happy categories merged). We have a total of 5531 utterances from the four emotion labels. We also have a separate setting of 6 emotional classes, as has been done in some of the prior works such as  [52] . The first 6 emotion classes are considered resulting in a total of 7433 utterances. The dataset is imbalanced with the least number of samples for the happy emotion (648 utterances during training). For both these cases, we consider session 5 for testing purposes. We choose session 1 for validating our models and sessions 2 -4 for training.\n\n2) MELD: The MELD dataset is a multi-party dataset created from video clippings of the popular TV show, \"Friends\". The training data consists of 9988 utterances, validation data consists of 1108 utterances and test data consists of 2610 utterances. A seven way classification task is performed on this dataset, with each utterance being labeled as one of the 7 emotions -angry, sad, joy, neutral, fear, surprise or disgust. Like the IEMOCAP 6-way classification problem, this is also an imbalanced dataset with neutral being the most dominant class label and disgust being the least frequent label (4710 and 271 training utterances respectively).\n\n3) CMU-MOSI: The CMU-MOSI dataset has a total of 93 monologues divided into 2199 utterances. Each monologue is divided into several utterances and is labeled in the range of  [-3, 3] . Following previous works, we treat this as a binary classification problem with utterances having sentiment values in the range [-3, 0) being classified as negative sentiment and those with values in the range [0, 3] as positive sentiment.\n\nFor dataset partitioning, we follow the prior work by Poria et. al  [45] , where the first 62 monologues are used for training and validation while the last 31 monologues are used for testing.\n\nOf the 62 monologues, we use 49 for training our model and the rest 13 for validation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "The models are trained with Adam optimizer using a learning rate of 1e -5 and a batch size of 32 in MELD and IEMOCAP dataset, while the batch size is reduced to 8 for the CMU-MOSI dataset. All the experiments reported in this work use 5 random weight initialization choices. The mean performance using the random initializations are reported in all the experiments below. We run the different stages of our model for 100 epochs as we found the validation performance to saturate within this limit. We also employ gradient clipping with a L2 norm of 0.25 in all our implementation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Results",
      "text": "We report the performance of the proposed model for each individual modality followed by the performance of the model on the multi-modal setting. The key results on the three datasets are shown in Table  I .\n\nThe following are the observations from these results, (i) In the IEMOCAP dataset, the audio and the text modalities perform relatively similarly, while in the MELD and CMU-MOSI datasets, the audio results in an inferior performance compared to the text domain (ii) The context addition framework proves to be effective for all the three datasets. The proposed contextual GRU architecture with self-attention, though a simple architecture, leads to an improvement for both the modalities. For audio, the relative improvement over the stage I performance is 40.3%, 22.7%, 3.7% and 8.4% for IEMO-CAP 4-way, IEMOCAP 6-way, MELD and CMU-MOSI datasets, respectively. Similarly, for the textual modality, we notice a relative improvement of 41.1%, 22.9%, 6.3% and 7% respectively. IEMOCAP has the largest number of utterances per conversation among the three datasets while MELD has the lowest. As the number of utterances increase in the datasets, the contextual modeling becomes more effective (iii) The final multi-modal fusion achieves the best performance on all the three datasets over any of the individual  modality. The relative improvements of 24.2%, 14%, 0.6% and 2.7% are observed for multi-modal results over the best individual modality in the IEMOCAP 4way, IEMOCAP 6-way, MELD and CMU-MOSI datasets, respectively. These results show that, even when some modalities are inferior to the others in the emotion classification task, the co-attention mechanism is able to effectively improve over the best individual modality VI. DISCUSSION",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison With Other Works",
      "text": "We present a comparative study of our work along with other existing works in the literature. The comparison with the relevant works are shown in Table  II  for the IEMOCAP dataset with 4-way classification. We see that the performance of our model on audio signals is significantly better than other works (by a relative margin of about 20%). This is attributed to the efficient contextual modeling of the audio representations from the wav2vec model.\n\nWhile we do not improve the state-of-the-art results in the text modality, we achieve state-of-the-art results on the multimodal fusion task by a relative margin of 7% over the previous best result. We also show a comparison with prior works reporting on the 6-way classification in the IEMOCAP dataset (Table  III ). For this unbalanced classification setting, we notice that our model matches the results reported by Lian et al  [44] .\n\nWe compare with other relevant works on the MELD dataset in Table  IV . Similar to IEMOCAP, we notice a considerable improvement in the audio only performance. We improve the current state-of-the-art results in audio signal performance by a relative margin of approximately 14%. We improve the performance of our model on textual modality alone as   well, where we achieve a relative improvement of 8.5%. Subsequently we get a relative improvement of approximately 4% after the fusion module. It is noteworthy that, MELD is the most imbalanced of the three datasets with maximum number of speakers in a conversation. Our model, however, does not depend on any speaker information for emotion modeling unlike some prior works  [63] .\n\nIn Table  V , we compare our proposed model with other works on the CMU-MOSI dataset. Due to the small size of the dataset (only 2199 utterances), the proposed model is prone to overfitting in this dataset. However, we note that the performance of the model in the audio modality is comparable to the previously reported best results on audio inputs alone. We improve upon the best reported text-only performance by a relative margin of 30%, while the model working on audio and text together improves upon the previous state-of-the-art performance by a relative margin of 26%.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Impact Of Hierarchical Modeling",
      "text": "In order to understand the advantages of our hierarchical modeling approach, we modify our training paradigm where we combine stages II and III of training. The results for these modifications in the training paradigm are shown in Table  VI . We note that, for all the four dataset settings, we achieve a better performance with the proposed hierarchical modeling with no change in the model architecture. These experiments highlight the benefits of a curriculum style design of the stages proposed in the HCAM framework.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Importance Of Self-Attention In Stage Ii",
      "text": "In order to understand the role of self-attention in the contextual GRU, we run an ablation experiment where we remove the self-attention block from the contextual GRU. The results from this experiment are shown in Table  VII . We note that self-attention improves the performance for both the modalities in IEMOCAP. This is expected as the conversation length is more in the case of IEMOCAP (conversation length of 110 utterances). In a departure from the other datasets, we see an absolute drop of 0.4% for the textual modality for CMU-MOSI dataset on the introduction of the self-attention block. This may partly be due to the small size of this dataset, which leads the Bi-GRU model to overfit.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Testing With Asr Generated Transcripts",
      "text": "In order to understand the robustness of our model to noise in the text modality, we test our model with ASR transcripts in place of the provided transcripts. The training of the model is not modified from the previous experiments\n\nWe use a pre-trained ASR system 2  for providing the transcripts. The word error rate (WER) of this ASR system is reported in Table VIII for each of the three datasets. As seen here, the WER on emotional conversational speech is significantly higher than those seen on other controlled datasets. The ASR performance in the case of MELD is the lowest, partly due to the high levels of background noise in the dataset. In spite of the high WER, the model performs well on the IEMOCAP and CMU-MOSI datasets (with an absolute drop of 2.4% for IEMOCAP 6-way classification and 5.7% for the CMU-MOSI) as shown in Table  IX . For the baseline result in this setting, we use our previous work  [47] , as we have not found a similar inference setting elsewhere. We achieve a significant relative improvement of 32% over our previous work.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Role Of The Supervised Contrastive Loss",
      "text": "The performance of the proposed model is evaluated when the training is performed without and with the supervised contrastive loss. These results are shown in Table  X . For all the datasets, we observe an improvement in the performance of our model with the inclusion of the supervised loss.\n\nHowever, the performance improvements from the supervised contrastive loss varies from one dataset to the other. The improvement after adding the sup-con loss is only 0.6% and 0.3% for IEMOCAP-6 and MELD (in absolute terms). When the number of classes reduce, the introduction of sup-con loss improves the performance of the model by a significant margin (1.4% and 1.1% for IEMOCAP-4 way classification and CMU-MOSI respectively). This loss involves two hyperparameters, namely the temperature and the weight used for combining with the cross-entropy loss. We show the variation in the test performance with change in these parameters for IEMOCAP 4-way classification and CMU-MOSI in Fig.  5 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Performance On Different Emotion Classes",
      "text": "In order to show the performance of our models in recognizing the different classes, we show the confusion matrices for the 6-way classification in IEMOCAP in Fig.  4 . It is seen that  the models have considerable error in differentiating happy and excited class. This is somewhat expected as these emotional classes are closely related to each other. The confusion matrices also highlight that the proposed model is not biased towards any particular emotion category class.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "G. Role Of Test Time Ensembling",
      "text": "As mentioned in Sec.IV-E, we take an weighted combination of the predictions of the model in a particular stage of training with those of the previous stage. The effect of this test time combination is shown in Table  XI . We compare this strategy with the one when we do not use any ensembling in stage II and III of inference. We note that for CMU-MOSI, this improves the performance of the audio modality by 4.4% in stage II. While a small improvement is noticed for MELD, the ensemble predictions does not yield any improvement for IEMOCAP. For the combination in stage III, we note that with the exception of CMU-MOSI, we see a slight improvement for all the other test settings. We show the variation of the test performance for the audio modality at the end of stage II for CMU-MOSI in Fig.  6 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, a hierarchical multi-modal neural architecture is proposed for conversational emotion recognition task. The proposed architecture improves the representation of the utterance level speech and text, first by learning the representations and then, by employing self-attention over other utterances in the recording. A co-attention mechanism is used for effective multi-modal fusion of the two modalities. On three benchmark datasets, we establish new state-of-the-art results. We further show the robustness of our model when tested with ASR generated text transcripts. Through extensive ablation studies, we also show the impact of different aspects of the modeling framework and the hyper-parameter choices. In future, we plan to extend these approaches to also incorporate the visual modality for emotion recognition.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed model. Here, S1, S2 and S3 refer to the speech utterances in a conversation. Similarly,",
      "page": 3
    },
    {
      "caption": "Figure 1: As the entire model is a relatively large architecture, driven",
      "page": 3
    },
    {
      "caption": "Figure 2: Block diagram of the contextual GRU with self-",
      "page": 4
    },
    {
      "caption": "Figure 3: The co-attention network used in the proposed model.",
      "page": 4
    },
    {
      "caption": "Figure 3: , and it consists of",
      "page": 4
    },
    {
      "caption": "Figure 3: , are concatenated and this forms the multi-modal",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrices for the different stages of our model when run on IEMOCAP dataset with 6 classes. Abbreviations",
      "page": 8
    },
    {
      "caption": "Figure 5: Variation of the test performance with change in β",
      "page": 8
    },
    {
      "caption": "Figure 6: Variation of the test performance with change in α1→2",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Splits\nHap. Sad Neu. Ang. Exc. Fru. Ang. Sad Neu. Fear Sur. Dis. Joy Pos. Neg.\nTrain 50.1 50.1 50.2 54.8 59 47.5 54.1 37.3 37.9\n36 44 31.2 23.4 36.5 28\nVal 47.4 49.9 46.4 52.8 58.8 46.6 50.7 34 37.9\nTest 34.5 47.5 30.2 27.3 36 30.5 50.5 49.2 48.1 59.5 61.2 49.3 54.5 38.2 40.4\n(a) Audio (Stage 1) (b) Text (Stage 1) (c) Audio (Stage 2) (d) Text (Stage 2) (e) Fusion (Stage 3)\nFig. 4: Confusion matrices for the different stages of our model when run on IEMOCAP dataset with 6 classes. Abbreviations\nused: Happy:Hap., Neutral:Neu., Angry:Ang., Excited:Exc., Frustrated:Fru.\nTABLE IX: Weighted F1 score of our system with ASR\nIEMOCAP 4-way CMU-MOSI\ntranscripts during test time on the datasets.\n86\n)": "",
          "Column_2": "ASR\nIEMOCAP 4-way CMU-MOSI\n86\n)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yes\nNo": "Yes",
          "78.7%\n78.7%": "81.4%",
          "65.7%\n65.7%": "64.4%",
          "50.1%\n49.6%": "65.6%"
        },
        {
          "Yes\nNo": "No",
          "78.7%\n78.7%": "81.4%",
          "65.7%\n65.7%": "64.4%",
          "50.1%\n49.6%": "65.4%"
        },
        {
          "Yes\nNo": "Yes\nNo",
          "78.7%\n78.7%": "85.9%\n85.3%",
          "65.7%\n65.7%": "70.5%\n70.3%",
          "50.1%\n49.6%": "65.8%\n65.5%"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "Maja Pantic",
        "Nicu Sebe",
        "Jeffrey Cohn",
        "Thomas Huang"
      ],
      "year": "2005",
      "venue": "ACM international conference on Multimedia"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection and analysis on social media",
      "authors": [
        "Bharat Gaind",
        "Varun Syal",
        "Sneha Padgalwar"
      ],
      "year": "2019",
      "venue": "Emotion detection and analysis on social media",
      "arxiv": "arXiv:1901.08458"
    },
    {
      "citation_id": "3",
      "title": "Acoustic and lexical sentiment analysis for customer service calls",
      "authors": [
        "Bryan Li",
        "Dimitrios Dimitriadis",
        "Andreas Stolcke"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Emokey: An emotion-aware smartphone keyboard for mental health monitoring",
      "authors": [
        "Surjya Ghosh",
        "Sumit Sahu",
        "Niloy Ganguly",
        "Bivas Mitra",
        "Pradipta De"
      ],
      "year": "2019",
      "venue": "COMSNETS"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "Paweł Tarnowski",
        "Marcin Kołodziej",
        "Andrzej Majkowski",
        "Remigiusz Rak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "7",
      "title": "Vocal expression of emotion",
      "authors": [
        "Klaus Scherer",
        "Tom Johnstone",
        "Gundrun Klasmeyer"
      ],
      "year": "2003",
      "venue": "Vocal expression of emotion"
    },
    {
      "citation_id": "8",
      "title": "Individuality in communicative bodily behaviours",
      "authors": [
        "Costanza Navarretta"
      ],
      "year": "2012",
      "venue": "Cognitive Behavioural Systems"
    },
    {
      "citation_id": "9",
      "title": "Physiological signals and their use in augmenting emotion recognition for human-machine interaction",
      "authors": [
        "Benjamin Knapp",
        "Jonghwa Kim",
        "Elisabeth André"
      ],
      "year": "2011",
      "venue": "Emotion-oriented systems"
    },
    {
      "citation_id": "10",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "Isla: Temporal segmentation and labeling for audio-visual emotion recognition",
      "authors": [
        "Yelin Kim",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Jun Du",
        "Yuanyuan Zhang",
        "Qing Wang",
        "Qing-Feng Liu",
        "Chin-Hui Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "SLT"
    },
    {
      "citation_id": "14",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustín Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "16",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "18",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "LREC"
    },
    {
      "citation_id": "20",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "21",
      "title": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "22",
      "title": "Perceptual cues in nonverbal vocal expressions of emotion",
      "authors": [
        "A Disa",
        "Frank Sauter",
        "Andrew Eisner",
        "Sophie Calder",
        "Scott"
      ],
      "year": "2010",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "23",
      "title": "Automatic emotion recognition using prosodic parameters",
      "authors": [
        "Iker Luengo",
        "Eva Navas",
        "Inmaculada Hernáez",
        "Jon Sánchez"
      ],
      "year": "2005",
      "venue": "Automatic emotion recognition using prosodic parameters"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition using segmental level prosodic analysis",
      "authors": [
        "Nitin Shashidhar G Koolagudi",
        "K Sreenivasa Kumar",
        "Rao"
      ],
      "year": "2011",
      "venue": "ICDeCom. IEEE"
    },
    {
      "citation_id": "25",
      "title": "Affect recognition in real-life acoustic conditions-a new perspective on feature selection",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Affect recognition in real-life acoustic conditions-a new perspective on feature selection"
    },
    {
      "citation_id": "26",
      "title": "The Interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Alessandro Vinciarelli",
        "Klaus Scherer",
        "Fabien Ringeval",
        "Mohamed Chetouani",
        "Felix Weninger",
        "Florian Eyben",
        "Erik Marchi"
      ],
      "year": "2013",
      "venue": "The Interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "27",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Opensmile: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "SLT"
    },
    {
      "citation_id": "30",
      "title": "LEAF: A Learnable Frontend for Audio Classification",
      "authors": [
        "Neil Zeghidour",
        "Olivier Teboul",
        "Félix De Chaumont",
        "Marco Quitry",
        "Tagliasacchi"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "31",
      "title": "Modulation filter learning using deep variational networks for robust speech recognition",
      "authors": [
        "Purvi Agrawal",
        "Sriram Ganapathy"
      ],
      "year": "2019",
      "venue": "IEEE journal of selected topics in signal processing"
    },
    {
      "citation_id": "32",
      "title": "Interpretable representation learning for speech and audio signals based on relevance weighting",
      "authors": [
        "Purvi Agrawal",
        "Sriram Ganapathy"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "34",
      "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "35",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "Manon Macary",
        "Marie Tahon",
        "Yannick Estève",
        "Anthony Rousseau"
      ],
      "year": "2021",
      "venue": "SLT"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "37",
      "title": "Exploring the use of word relation features for sentiment classification",
      "authors": [
        "Rui Xia",
        "Chengqing Zong"
      ],
      "year": "2010",
      "venue": "Coling 2010: Posters"
    },
    {
      "citation_id": "38",
      "title": "Advances in pre-training distributed word representations",
      "authors": [
        "Tomáš Mikolov",
        "Édouard Grave",
        "Piotr Bojanowski",
        "Christian Puhrsch",
        "Armand Joulin"
      ],
      "year": "2018",
      "venue": "LREC"
    },
    {
      "citation_id": "39",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "40",
      "title": "Vector representation of words for sentiment analysis using glove",
      "authors": [
        "Yash Sharma",
        "Gaurav Agrawal",
        "Pooja Jain",
        "Tapan Kumar"
      ],
      "year": "2017",
      "venue": "ICCT"
    },
    {
      "citation_id": "41",
      "title": "Sentiment analysis of citations using word2vec",
      "authors": [
        "Haixia Liu"
      ],
      "year": "2017",
      "venue": "Sentiment analysis of citations using word2vec",
      "arxiv": "arXiv:1704.00177"
    },
    {
      "citation_id": "42",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "44",
      "title": "SMIN: Semi-supervised Multimodal Interaction Network for Conversational Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "46",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "47",
      "title": "Multimodal transformer with learnable frontend and self attention for emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "48",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "49",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "50",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-andlanguage tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "51",
      "title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "52",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "53",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "54",
      "title": "Curriculum learning",
      "authors": [
        "Yoshua Bengio",
        "Jérôme Louradour",
        "Ronan Collobert",
        "Jason Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "55",
      "title": "Libri-light: A benchmark for ASR with limited or no supervision",
      "authors": [
        "Jacob Kahn",
        "Morgane Riviere",
        "Weiyi Zheng",
        "Evgeny Kharitonov",
        "Qiantong Xu",
        "Pierre-Emmanuel Mazaré",
        "Julien Karadayi",
        "Vitaliy Liptchinsky",
        "Ronan Collobert",
        "Christian Fuegen"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "56",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Kohler",
        "Josh Meyer",
        "Michael Henretty",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "LREC"
    },
    {
      "citation_id": "57",
      "title": "Switchboard: Telephone speech corpus for research and development",
      "authors": [
        "John J Godfrey",
        "Edward Holliman",
        "Jane Mcdaniel"
      ],
      "year": "1992",
      "venue": "ICASSP"
    },
    {
      "citation_id": "58",
      "title": "The fisher corpus: A resource for the next generations of speech-to-text",
      "authors": [
        "Christopher Cieri",
        "David Miller",
        "Kevin Walker"
      ],
      "year": "2004",
      "venue": "LREC"
    },
    {
      "citation_id": "59",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "60",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia",
        "Yaohua Bu",
        "Sheng Zhao",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "61",
      "title": "Locally confined modality fusion network with a global perspective for multimodal human affective computing",
      "authors": [
        "Sijie Mai",
        "Songlong Xing",
        "Haifeng Hu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "63",
      "title": "Dialoguetrm: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "64",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "65",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "66",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "67",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Mazumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ICDM"
    },
    {
      "citation_id": "68",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "69",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "Minghai Chen",
        "Sen Wang",
        "Paul Liang",
        "Tadas Baltrušaitis",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "ICMI"
    }
  ]
}