{
  "paper_id": "2504.19423v2",
  "title": "Mer 2025: When Affective Computing Meets Large Language Models",
  "published": "2025-04-28T02:14:08Z",
  "authors": [
    "Zheng Lian",
    "Rui Liu",
    "Kele Xu",
    "Bin Liu",
    "Xuefei Liu",
    "Yazhou Zhang",
    "Xin Liu",
    "Yong Li",
    "Zebang Cheng",
    "Haolin Zuo",
    "Ziyang Ma",
    "Xiaojiang Peng",
    "Xie Chen",
    "Ya Li",
    "Erik Cambria",
    "Guoying Zhao",
    "Bj√∂rn W. Schuller",
    "Jianhua Tao"
  ],
  "keywords": [
    "MER2025",
    "semi-supervised learning",
    "fine-grained emotion recognition",
    "descriptive emotion understanding",
    "emotion-enhanced personality recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "MER2025 is the third year of our MER series of challenges, aiming to bring together researchers in the affective computing community to explore emerging trends and future directions in the field. Previously, MER2023 1 focused on multi-label learning, noise robustness, and semi-supervised learning, while MER2024 2 introduced a new track dedicated to open-vocabulary emotion recognition. This year, MER2025 centers on the theme \"When Affective Computing Meets Large Language Models (LLMs)\". We aim to shift the paradigm from traditional categorical frameworks reliant on predefined emotion taxonomies to LLM-driven generative methods, offering innovative solutions for more accurate and reliable 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays a pivotal role in human-computer interactions  [24, 33] , drawing increasing attention within the field of artificial intelligence. Already today, humans interact more frequently with AI systems than through direct human-to-human interaction, and this trend is expected to grow even more pronounced in the coming years. Emotion-aware interaction systems hold significant potential across various domains, including healthcare, banking, transportation, and education  [7, 8] .\n\nHuman emotion expression is a complex process that typically involves multiple modalities, including facial expressions, vocal tones, body movements, gestures, and even physiological signals  [12] . This complexity has spurred the development of Multimodal Emotion Recognition (MER), a critical task that aims to integrate cross-modal cues to identify human emotions. Recently, MER research has evolved in two key directions: a) from coarse-grained  [18]  to fine-grained emotion recognition  [16] , and b) from categorical approaches  [14]  to descriptive emotion understanding  [4, 13] , aiming to enhance both prediction accuracy and interpretability. In line with these advancements, MER2025@ACM Multimedia introduces four tracks aligned with current research priorities:\n\nTrack 1. MER-SEMI. Recent studies have demonstrated that pre-training on large-scale unlabeled data  [22] , especially domainmatched data, can significantly enhance model performance  [5, 18, 29] . This track provides a substantial collection of unlabeled samples from the same domain as the labeled data. Participants are encouraged to leverage semi-supervised learning techniques, such as masked auto-encoders  [30]  or contrastive learning  [26] , to achieve better results.\n\nTrack 2. MER-FG. Current frameworks primarily focus on basic emotions, often failing to capture the complexity and subtlety of human emotions. This track shifts the emphasis to fine-grained MER, enabling the prediction of a broader range of emotions. Following previous works  [13, 16] , participants are encouraged to leverage large language models (LLMs) for this purpose. Given that LLMs possess extensive vocabularies, they hold the potential to generate more diverse emotion categories beyond basic labels.\n\nTrack 3. MER-DES. The first two tracks primarily focus on emotion words, neglecting the integration of multimodal clues during the inference process. This omission results in prediction outcomes that lack interpretability. Moreover, emotion words struggle to fully capture the dynamic, diverse, and sometimes ambiguous nature of human emotions. This track seeks to leverage free-form, natural language descriptions to represent emotions  [13, 19] , offering greater flexibility to achieve more accurate emotion representations and enhance model interpretability.\n\nSubtitle: Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\n\nTask: select the most likely label from six categories: worried, happy, neutral, angry, surprise, and sad Label: surprise",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Mer-Semi",
      "text": "Task: predict any emotion labels without restrictions on category or quantity Label: surprise, nervous, dissatisfied",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mer-Fg",
      "text": "Task: integrate multimodal clues to enhance the interpretability of each emotion label Label: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mer-Des",
      "text": "Task: predict five-dimensional personality traits Label: five floating-point values, each ranging from [0, 1] Track 4. MER-PR. Personality and emotion are deeply intertwined in human behavior and social interactions, yet current research often treats them as separate tasks, neglecting their inherent correlations. This track seeks to investigate the interplay between emotion and personality, exploring whether emotion recognition can enhance the accuracy of personality predictions. Participants are encouraged to employ techniques such as multi-task learning to analyze the influence of emotion on personality prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer-Pr",
      "text": "Figure  1  shows the differences between these tracks. MER2025 builds upon the success of MER 2023@ACM Multimedia  [15]  and MER 2024@IJCAI  [17] . Over the course of these challenges, participation has steadily grown, increasing from 76 teams in MER2023 to 94 teams in MER2024. This year, we aim to attract more teams to join our challenge. Through the MER series of challenges, we strive to establish a unified platform for comparing different systems and further advancing the development of MER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mer-Semi 2.1 Dataset",
      "text": "MER-SEMI spans three consecutive MER challenges, aiming to enhance the performance of categorical emotion recognition algorithms through semi-supervised learning and unlabeled data. This year, we expanded the dataset by incorporating more labeled and unlabeled samples. Our raw data comes from two sources: a) conversational emotion datasets MC-EIU  [20]  and M3ED  [31] , with explicit approval from dataset owners; and b) 19 Chinese TV series sourced from publicly available platforms. Then, we follow the video segmentation and filtering process used in prior MER challenges  [15, 17] , ensuring each video contains predominantly one speaker with relatively complete speech content. Compared   1 .\n\nFor evaluation purposes, manually annotating 124k unlabeled data is impractical due to the substantial time and financial costs. Thus, we select a subset of the unlabeled data for performance assessments. To ensure label reliability, we recruit nine annotators and conduct a preliminary test to evaluate their alignment with emotion experts. In this process, two annotators are excluded based on insufficient agreement, leaving seven annotators for the labeling task. Then, we randomly sample 10k instances from the unlabeled subset and ask the annotators to choose the most likely label from eight categories: neutral, happy, angry, sad, surprise, worry, others, unknown. In this process, each annotator is assigned a portion of the 10k samples to reduce annotation time. Meanwhile, we ensure that each sample is labeled by at least five annotators. Samples that receive at least 80% agreements and whose majority vote is neither others nor unknown are included in the test set. This process yields 2,026 high-quality test samples, ensuring strong inter-annotator agreement and reliable ranking results.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "This track aims to identify the most likely label from six candidate emotions: neutral, happy, angry, sad, surprise, and worry. For evaluation purposes, we use the same metrics as previous MER challenges  [15, 17] : accuracy and weighted average F1-score (WAF). Given the inherent class imbalance in the dataset, we prioritize WAF as the primary metric for final ranking.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Framework",
      "text": "A categorical model primarily relies on two key components: feature selection and model architecture. For feature selection, we evaluate the performance of both handcrafted and model-driven features. For model architecture, MERBench highlights that a simple attention mechanism, which maps different unimodal features to the same dimension and then fuses them using attention weights, can already achieve strong performance  [18] . In contrast, more complex fusion architectures may lead to overfitting problems and are not suitable for MER, where labeled data is usually limited. For implementation details, please refer to MERBench and our baseline code.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Results",
      "text": "Our baseline code is designed to automatically and randomly select hyperparameters. In practice, we execute each command 50 times, identify the optimal hyperparameter combination, and then run",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer-Fg 3.1 Dataset",
      "text": "Unlike MER-SEMI, which restricts the prediction scope to six candidates, MER-FG does not limit the label space, allowing predictions for any number and any emotion categories for each sample. In this track, we utilize two recently released datasets, OV-MERD  [16]  and MER-Caption+  [13] , as training datasets. Their statistics are summarized in Table  3 . Specifically, OV-MERD employs a humanled, model-assisted annotation strategy, where MLLMs first provide pre-extracted multimodal clues and then rely heavily on manual verification to ensure label quality. In contrast, MER-Caption+ adopts a human-assisted, model-led annotation strategy, leveraging human priors to guide description generation and sample filtering, ultimately achieving an automatic annotation process. Consequently, OV-MERD provides small-scale but high-quality labels, whereas MER-Caption+ offers large-scale labels that may contain some errors in emotion annotations. Figure  2  summarizes the sample-wise label number distribution of these datasets. We observe that they provide rich emotion labels for each sample, offering potential for complex emotion modeling.\n\nFor evaluation purposes, we randomly selected 1,200 samples from the unlabeled set in MER-SEMI and engaged four annotators to label them. These annotators were distinct from those involved in the MER-SEMI dataset and had also successfully passed the preliminary qualification test described in MER-SEMI. Figure  3  presents the detailed annotation pipeline. Specifically, we first aggregated the Top-6 teams' submission results from the previous year's MER-OV track to generate initial labels. Each annotator was then tasked with either selecting labels they deemed correct or adding any additional labels they considered appropriate but not included in the provided candidate list. This approach enabled us to obtain richer emotion annotations for each sample compared to directly asking annotators to provide labels without candidate options. Our manual verification process consisted of two rounds. In the first round, we preserved all labels selected by different annotators to ensure comprehensiveness. In the second round, we retained only those labels confirmed by at least two annotators to ensure accuracy. Through this two-step verification process, each final label was validated at least three times, ensuring both comprehensiveness and accuracy of the annotation results.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "This track does not restrict the number or category of labels during prediction. Therefore, traditional metrics like accuracy are not suitable for MER-FG. For evaluation purposes, we follow the previous work  [16]  and compute results in two stages.\n\n3.2.1 Grouping. First, we apply a three-level grouping strategy to reduce the impact of synonyms:\n\nM1. We normalize different forms of emotion words to their base form. For example, we map happier and happiness to happy. This function is denoted as ùêπ (‚Ä¢).\n\nM2. We map synonyms to a unified label. For example, we map happy and joyful to happy. We call this function ùê∫ (‚Ä¢).\n\nM3. The emotion wheel provides natural hierarchical grouping, with basic emotions located in the innermost layer and more nuanced labels arranged in the outer layers  [25] . Figure  4  shows two emotion wheels. First, we group the labels by their levels from the innermost to the outermost as ùêø 1  ùë§ , ùêø 2 ùë§ , and ùêø 3 ùë§ . Next, we define a mapping function ùëö ùëñ‚Üíùëó ùë§ (‚Ä¢) that transforms labels from level ùêø ùëñ ùë§ to their corresponding labels in level ùêø ùëó ùë§ . We then introduce two grouping functions, ùëä 1 and ùëä 2 . For ùëä 1 , all labels are mapped to their corresponding labels in ùêø 1 ùë§ :\n\nFor ùëä 2 , all labels are mapped to corresponding labels in ùêø 2 ùë§ :\n\nThe above grouping functions can be summarized as:\n\nIn this paper, we employ five emotion wheels {ùë§ ùëñ } 5 ùëñ=1 , following the approach of previous works  [16] . This process mitigates the influence of the choice of emotion wheels, thereby yielding more reliable evaluation results. Consequently, the aforementioned grouping function is dependent on the specific emotion wheel used. Therefore, we denote the final grouping functions as ùêø 1  ùë§ ùëñ (‚Ä¢) and ùêø 2\n\nùë§ ùëñ (‚Ä¢), where the former focuses on coarse-grained grouping, while the latter emphasizes more fine-grained grouping.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Metrics.",
      "text": "For each sample, the number of labels is variable. Let the dataset consist of ùëÅ samples. For sample ùë• ùëñ , the true labels are denoted as Y ùëñ and the predicted labels are denoted as ≈∂ùëñ .   For a grouping function ùëÄ ‚àà {ùêø 1 ùë§ ùëñ , ùêø 2 ùë§ ùëñ }, we define the following evaluation metrics:\n\nHere, Precision ùëÄ s represents the proportion of correctly predicted labels; Recall ùëÄ s measures whether the prediction covers all ground truth labels; and F ùëÄ s is the harmonic mean of these two metrics, providing a more comprehensive evaluation. In this paper, we define the following scores ùëÜ 1 and ùëÜ 2 based on different grouping functions, and use their average results for the final ranking:\n\n3.3 Baseline Framework 3.3.1 Zero-shot Baselines. The primary objective of MER-FG is to generate appropriate emotion labels for a given sample without being constrained by a predefined emotion taxonomy. Consequently, LLM-driven baselines are well-suited for this task, as they have extensive vocabularies that enable the generation of finegrained emotion labels. Given that emotions are often expressed through multimodal cues, we primarily select multimodal LLMs (MLLMs) as our baselines, including representative frameworks such as SALMONN and Chat-UniVi. To extract emotion-related clues, we employ the following prompt: As an expert in the field of emotions, please focus on the facial expressions, body movements, environment, acoustic information, subtitle content, etc., in the video to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual in the video.\n\nNext, we use Qwen2.5 to extract emotion labels from the above descriptions using the following prompt: Please assume the role of an expert in the field of emotions. We provide clues that may be related to the emotions of the characters. Based on the provided clues, please identify the emotional states of the main characters. Please separate different emotional categories with commas and output only the clearly identifiable emotional categories in a list format. If none are identified, please output an empty list.\n\nFinally, we obtain the emotion prediction results for MER-FG. This process does not involve any training and operates in a zeroshot setup. For MLLMs, we use their 7B parameter weights by default. All models are implemented in PyTorch, and all inference processes are conducted on an A100 GPU. Additionally, we leverage the vLLM toolkit to accelerate the inference process.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Affectgpt.",
      "text": "We also evaluate the performance of an emotionspecific MLLM, AffectGPT  [13] . This framework employs a prefusion mechanism to enhance multimodal integration. During our experiments, we train the model on two datasets: OV-MERD and MER-Caption+. All pretrained models are available in our official baseline code repository. For detailed implementation instructions, please refer to the baseline code.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Results",
      "text": "Table  4  presents all baseline results. In Table  4 , AffectGPT outperforms the zero-shot baselines, achieving higher scores in both coarse-grained scores ùëÜ 1 and fine-grained scores ùëÜ 2 . Additionally, the model trained on MER-Caption+ demonstrates superior performance compared to the one trained on OV-MERD. These findings indicate that although OV-MERD has high-quality labels, its limited dataset size is insufficient to support effective training. In contrast, MER-Caption+ offers a larger number of samples with relatively accurate labels, resulting in better performance on MER-FG.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Mer-Des 4.1 Dataset",
      "text": "ER-SEMI and MER-FG focus on emotion word prediction, whereas MER-DES extends beyond emotion words by integrating multimodal clues to enhance the interpretability of each emotion label. Figure  1  illustrates the task differences among these tracks. Since OV-MERD and MER-Caption+, which are used in MER-FG, also provide emotion descriptions, they are employed as the training set for MER-DES. Figure  5  presents the distribution of description lengths in these datasets. We observe that both datasets provide detailed emotional descriptions for each sample.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "MER-DES leverages multimodal cues to enhance the interpretability of each emotion label. Given that interpretability is inherently a relatively subjective metric, we plan to recruit annotators to manually evaluate the quality of the emotion descriptions. Simultaneously, we will invite two members from each team to assist with the scoring process. The evaluation will be conducted along two dimensions: First, we will evaluate whether the submission results include interpretable clues. This criterion distinguishes MER-DES from MER-SEMI and MER-FG by ensuring that participants do not merely provide emotion labels without supporting evidence.\n\nSecond, we will assess the quality of the emotion descriptions from two perspectives: 1) whether the provided emotion clues are present in the video; and 2) whether the emotion and its associated clues are logically connected.\n\nInitially, we plan to provide real descriptions and calculate similarity scores between real and predicted descriptions, as done in EMER  [19] . However, human emotional expression is complex and difficult to fully capture all emotion-related visual and acoustic cues. Therefore, we shift our focus to manual evaluation. During the ranking process, each team is required to participate in the labeling process. We will exclude annotators with low inter-annotator agreement compared to others to ensure annotation quality. Additionally, we will mask the sample names and require participants to submit results in English while using their translated Chinese versions for ranking. This approach prevents participants from identifying their own submissions and deliberately assigning inflated scores. The final rankings will be determined based on these manual annotations.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Baseline",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Mer-Pr 5.1 Dataset",
      "text": "This track aims to explore whether emotion prediction results can enhance the performance of personality recognition. For this purpose, we utilize the MDPE dataset  [2] , which provides annotations for both emotion and personality traits. The dataset and baseline code are available on GitHub  6  . Table  5  provides dataset statistics.\n\nFor personality traits, we used a Big Five personality questionnaire consisting of 60 items. Responses were used to derive scores for five personality dimensions (openness, conscientiousness, extraversion, agreeableness, and neuroticism), with each dimension represented as a floating-point value ranging from 0 to 1. The dataset includes 233 participants, all native Chinese speakers from diverse backgrounds.\n\nFor emotion labels, each participant watched 16 emotion-induction videos, comprising two videos designed to elicit each of the eight target emotions: sadness, happiness, relaxation, surprise, fear, disgust, anger, and neutral. After watching, participants described their Table  6 : Unimodal results for MER-PR. In this table, the abbreviations \"O\", \"C\", \"E\", \"A\", and \"N\" represent the openness, conscientiousness, extraversion, agreeableness, and neuroticism, respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Metric",
      "text": "This track focuses on personality recognition. In the dataset, personality traits are quantified using five floating-point values. For evaluation, we employ the Root Mean Square Error (RMSE) metric to measure the discrepancy between true and predicted personality scores. The RMSE is calculated as follows:\n\nwhere ùëÅ is the dataset size. ùë¶ ùëñ and ≈∑ùëñ denote the true and predicted personality scores, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Framework",
      "text": "For unimodal features, we employ fully connected layers to extract hidden representations and predict personality scores:\n\nwhere ‚Ñé ùëö ùëñ ‚àà R ‚Ñé is the hidden feature for each modality, and ≈∑ùëñ ‚àà R 5 is the estimated personality probabilities.\n\nFor multimodal features, we use concatenation for feature fusion:\n\nwhere ùõº ùëñ ‚àà R 5 is the estimated personality probabilities.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Unimodal",
      "text": "Results. We establish a unimodal benchmark for personality trait prediction across visual, acoustic, and textual modalities (see Table  6 ). For the visual modality, the ViT feature achieves competitive performance, particularly in predicting conscientiousness. Among acoustic features, Wav2Vec2-base yields the highest average results. The textual modality outperforms all other unimodal features, with Baichuan-13B achieving the best average scores on the test set (0.156), excelling in extraversion (0.157), agreeableness (0.214), and neuroticism (0.157). Meanwhile, although textual cues are the most informative, visual and acoustic signals provide complementary trait-specific insights. For instance, ViT is particularly effective for conscientiousness, whereas Wav2Vec2-base performs well for agreeableness.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Results",
      "text": ". In Table  7 , we present the multimodal fusion results based on several best-performing unimodal features. While multimodal fusion does not consistently outperform individual modalities in terms of average test scores, it yields improvements for specific personality traits. This underscores the potential advantages of tailoring fusion strategies to individual traits.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Challenge Protocol",
      "text": "For MER-SEMI, MER-FG, and MER-DES, to download the dataset, participants must complete an End User License Agreement (EULA) available on Hugging Face 7 . The EULA clearly states that the dataset is for academic research purposes only and prohibits any modifications or uploads to the Internet. We emphasize that manual annotation of MER2025 samples is strictly prohibited. For these tracks, the test samples are selected from the 124k unlabeled samples (see Table  1 ). To reduce the task difficulty, we reduce the evaluation scope from 124k to 20k samples. Participants must submit predictions for these 20k samples, which cover all the test samples of these tracks. As shown in Figure  1 , each track has distinct objectives. For MER-SEMI, participants must predict the most likely label from six predefined categories: worried, happy, neutral, angry, surprised, and sad; For MER-FG, participants can freely predict any emotion labels without restrictions on category or quantity; For MER-DES, participants are required to submit both multimodal evidence and corresponding emotion labels to improve model interpretability.\n\nFor MER-PR, the dataset and baseline code are available on GitHub 8 . We provide an official test set, and participants can directly submit predictions for the test set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "This year's MER2025 focuses on the theme \"When Affective Computing Meets Large Language Models\" and contains four distinct tracks. This paper presents the datasets, baselines, evaluation metrics, and experimental results for all tracks. For MER-SEMI, we evaluate various unimodal features and multimodal fusion approaches, providing a strong baseline for categorical emotion recognition under a fixed taxonomy. For MER-FG and MER-DES, we employ LLM-driven baselines to generate fine-grained emotions with corresponding multimodal clues, ensuring accurate and interpretable emotion understanding. For MER-PR, we test diverse unimodal features for personality detection. To promote reproducibility, we have open-sourced the code and pretrained models on our official GitHub repository. We hope all participants enjoy this year's challenge! Your continued support and engagement make this challenge truly meaningful.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demo for four tracks.",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the differences between these tracks. MER2025",
      "page": 2
    },
    {
      "caption": "Figure 2: Label number distribution for MER-FG.",
      "page": 4
    },
    {
      "caption": "Figure 2: summarizes the sample-wise",
      "page": 4
    },
    {
      "caption": "Figure 4: shows two",
      "page": 4
    },
    {
      "caption": "Figure 3: Annotation pipeline for MER-FG.",
      "page": 5
    },
    {
      "caption": "Figure 4: Example of two emotion wheels.",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotion description length distribution.",
      "page": 6
    },
    {
      "caption": "Figure 1: illustrates the task differences among these tracks. Since",
      "page": 6
    },
    {
      "caption": "Figure 5: presents the distribution of description",
      "page": 6
    },
    {
      "caption": "Figure 1: , each track has distinct objectives. For",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "",
          "Val": "O(‚Üì)\nC (‚Üì)\nE (‚Üì)\nA (‚Üì)\nN (‚Üì)\nAvg (‚Üì)",
          "Test": "O (‚Üì)\nC (‚Üì)\nE (‚Üì)\nA (‚Üì)\nN(‚Üì)\nAvg (‚Üì)"
        },
        {
          "Feature": "VIT\nClipVIT-B16\nClipVIT-L14",
          "Val": "0.159\n0.179\n0.174\n0.231\n0.183\n0.185\n0.171\n0.169\n0.177\n0.225\n0.187\n0.185\n0.180\n0.172\n0.168\n0.180\n0.230\n0.186",
          "Test": "0.145\n0.156\n0.106\n0.178\n0.217\n0.160\n0.158\n0.108\n0.180\n0.220\n0.165\n0.166\n0.173\n0.114\n0.176\n0.229\n0.166\n0.172"
        },
        {
          "Feature": "HUBERT-base\nHUBERT-large\nWav2vec2-base\nWav2vec2-large\nWavLM-base\nWavLM-large",
          "Val": "0.173\n0.162\n0.176\n0.228\n0.184\n0.185\n0.200\n0.171\n0.194\n0.240\n0.200\n0.201\n0.172\n0.221\n0.180\n0.169\n0.160\n0.182\n0.198\n0.168\n0.190\n0.237\n0.200\n0.198\n0.178\n0.164\n0.179\n0.227\n0.184\n0.186\n0.191\n0.174\n0.191\n0.246\n0.206\n0.201",
          "Test": "0.098\n0.160\n0.183\n0.218\n0.167\n0.165\n0.189\n0.150\n0.210\n0.228\n0.174\n0.190\n0.159\n0.092\n0.177\n0.215\n0.158\n0.160\n0.190\n0.146\n0.205\n0.223\n0.173\n0.187\n0.156\n0.101\n0.180\n0.219\n0.162\n0.164\n0.200\n0.147\n0.209\n0.251\n0.205\n0.202"
        },
        {
          "Feature": "Sentence-BERT\nChatGLM2-6B\nBaichuan-13B",
          "Val": "0.181\n0.181\n0.176\n0.234\n0.181\n0.190\n0.188\n0.180\n0.195\n0.248\n0.202\n0.203\n0.167\n0.172\n0.180\n0.160\n0.229\n0.182",
          "Test": "0.159\n0.104\n0.181\n0.219\n0.163\n0.165\n0.182\n0.133\n0.200\n0.240\n0.180\n0.187\n0.098\n0.157\n0.214\n0.156\n0.156\n0.157"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Features\nV\nA\nT": "",
          "Val": "O(‚Üì)\nC (‚Üì)\nE (‚Üì)\nA (‚Üì)\nN (‚Üì)\nAvg (‚Üì)",
          "Test": "O (‚Üì)\nC (‚Üì)\nE (‚Üì)\nA (‚Üì)\nN(‚Üì)\nAvg (‚Üì)"
        },
        {
          "Features\nV\nA\nT": "VIT\nW2V\n‚Äî\n‚Äî\nW2V\nBAI\nVIT\n‚Äî\nBAI\nVIT\nW2V\nBAI",
          "Val": "0.160\n0.223\n0.171\n0.175\n0.185\n0.183\n0.160\n0.171\n0.223\n0.182\n0.170\n0.184\n0.167\n0.182\n0.182\n0.161\n0.172\n0.228\n0.160\n0.172\n0.175\n0.224\n0.187\n0.184",
          "Test": "0.158\n0.159\n0.095\n0.177\n0.213\n0.160\n0.159\n0.096\n0.175\n0.215\n0.160\n0.161\n0.154\n0.158\n0.097\n0.175\n0.216\n0.160\n0.093\n0.171\n0.212\n0.158\n0.156\n0.160"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "Mdpe: A multimodal deception dataset with personality and emotional characteristics",
      "authors": [
        "Cong Cai",
        "Shan Liang",
        "Xuefei Liu",
        "Kang Zhu",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Heng Xie",
        "Jizhou Cui",
        "Yiming Ma",
        "Zhenhua Cheng",
        "Hanzhe Xu",
        "Ruibo Fu",
        "Bin Liu",
        "Yongwei Li"
      ],
      "year": "2024",
      "venue": "Mdpe: A multimodal deception dataset with personality and emotional characteristics",
      "arxiv": "arXiv:2407.12274"
    },
    {
      "citation_id": "3",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "7",
      "title": "A review of emotion-aware systems for e-learning in virtual environments",
      "authors": [
        "Michalis Feidakis"
      ],
      "year": "2016",
      "venue": "Formative assessment, learning data analytics and gamification"
    },
    {
      "citation_id": "8",
      "title": "Emotion-Aware Interfaces: Empirical Methods for Adaptive User Interface",
      "authors": [
        "Syrine Haddad",
        "Olfa Daassi",
        "Safya Belghith"
      ],
      "year": "2024",
      "venue": "International Conference on Computer-Human Interaction Research and Applications"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion theory and research: Highlights, unanswered questions, and emerging issues",
      "authors": [
        "Carroll E Izard"
      ],
      "year": "2009",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "13",
      "title": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models",
      "authors": [
        "Zheng Lian",
        "Haoyu Chen",
        "Lan Chen",
        "Haiyang Sun",
        "Licai Sun",
        "Yong Ren",
        "Zebang Cheng",
        "Bin Liu",
        "Rui Liu",
        "Xiaojiang Peng",
        "Jiangyan Yi",
        "Jianhua Tao"
      ],
      "year": "2025",
      "venue": "AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models",
      "arxiv": "arXiv:2501.16566"
    },
    {
      "citation_id": "14",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mingyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao",
        "Ye Liu",
        "Bin Liu",
        "Jiangyan Yi",
        "Meng Wang",
        "Erik Cambria",
        "Guoying Zhao",
        "Bj√∂rn Schuller",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Open-vocabulary Multimodal Emotion Recognition: Dataset, Metric, and Benchmark",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Lan Chen",
        "Haoyu Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Siyuan Zhang",
        "Hailiang Yao",
        "Mingyu Xu",
        "Kang Chen",
        "Bin Liu",
        "Rui Liu",
        "Shan Liang",
        "Ya Li",
        "Jiangyan Yi",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Open-vocabulary Multimodal Emotion Recognition: Dataset, Metric, and Benchmark",
      "arxiv": "arXiv:2410.01495"
    },
    {
      "citation_id": "17",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen",
        "Jiangyan Yi",
        "Rui Liu",
        "Kele Xu",
        "Bin Liu",
        "Erik Cambria",
        "Guoying Zhao",
        "Bj√∂rn Schuller",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "19",
      "title": "Explainable Multimodal Emotion Reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable Multimodal Emotion Reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "20",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Xiaofen Xing",
        "Bj√∂rn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "21",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "22",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of The 62rd Annual Meeting of the Association for Computational Linguistics (ACL 2024"
    },
    {
      "citation_id": "23",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "Maxime Oquab",
        "Timoth√©e Darcet",
        "Th√©o Moutakanni",
        "V Huy",
        "Marc Vo",
        "Vasil Szafraniec",
        "Pierre Khalidov",
        "Daniel Fernandez",
        "Francisco Haziza",
        "Alaaeldin Massa",
        "Mido El-Nouby",
        "Nicolas Assran",
        "Wojciech Ballas",
        "Russell Galuba",
        "Po-Yao Howes",
        "Shang-Wen Huang",
        "Ishan Li",
        "Michael Misra",
        "Vasu Rabbat",
        "Gabriel Sharma",
        "Hu Synnaeve",
        "Herv√© Xu",
        "Julien J√©gou",
        "Patrick Mairal",
        "Armand Labatut",
        "Piotr Joulin",
        "Bojanowski"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "24",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "25",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Emotion: Theory, research, and experience"
    },
    {
      "citation_id": "26",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "27",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "28",
      "title": "2022. Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilic",
        "Roman Hesslow",
        "Alexandra Castagn√©",
        "Fran√ßois Sasha Luccioni",
        "Matthias Yvon",
        "Jonathan Gall√©",
        "Alexander Tow",
        "Stella Rush",
        "Albert Biderman",
        "Pawan Webson",
        "Thomas Sasanka Ammanamanchi",
        "Beno√Æt Wang",
        "Niklas Sagot",
        "Albert Muennighoff",
        "Olatunji Villanova Del Moral",
        "Rachel Ruwase",
        "Stas Bawden",
        "Angelina Bekman",
        "Iz Mcmillan-Major",
        "Huu Beltagy",
        "Lucile Nguyen",
        "Samson Saulnier",
        "Pedro Tan",
        "Victor Suarez",
        "Hugo Sanh",
        "Yacine Lauren√ßon",
        "Julien Jernite",
        "Margaret Launay",
        "Mitchell"
      ],
      "year": "2022",
      "venue": "2022. Bloom: A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "29",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Shanmin Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "Hao Zhou",
        "Minlie Huang",
        "Tianyang Zhang",
        "Xiaoyan Zhu",
        "Bing Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}