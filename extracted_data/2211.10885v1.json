{
  "paper_id": "2211.10885v1",
  "title": "Contrastive Regularization For Multimodal Emotion Recognition Using Audio And Text",
  "published": "2022-11-20T06:56:26Z",
  "authors": [
    "Fan Qian",
    "Jiqing Han"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "contrastive learning",
    "regularization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition is a challenge and an important step towards more natural human-computer interaction (HCI). The popular approach is multimodal emotion recognition based on model-level fusion, which means that the multimodal signals can be encoded to acquire embeddings, and then the embeddings are concatenated together for the final classification. However, due to the influence of noise or other factors, each modality does not always tend to the same emotional category, which affects the generalization of a model. In this paper, we propose a novel regularization method via contrastive learning for multimodal emotion recognition using audio and text. By introducing a discriminator to distinguish the difference between the same and different emotional pairs, we explicitly restrict the latent code of each modality to contain the same emotional information, so as to reduce the noise interference and get more discriminative representation. Experiments are performed on the standard IEMOCAP dataset for 4-class emotion recognition. The results show a significant improvement of 1.44% and 1.53% in terms of weighted accuracy (WA) and unweighted accuracy (UA) compared to the baseline system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the human-centered era, natural human-computer interaction (HCI) becomes more and more important. An ideal HCI system is required to detect the users emotion for enriching their experience, so as to provide better feedback. Many studies  [1, 2, 3]  show that a prominent automatic emotion recognition system should be multimodal since it is closer to the human sensor system.\n\nThe core challenge of multimodal emotion recognition is how to integrate the emotional knowledge from various modalities. With the popularity of deep learning, modellevel fusion has become the current mainstream method of multimodal emotion recognition, which is implemented by concatenating embeddings obtained from different modalities into a combined representation as the input of a classification model for end-to-end training  [4, 5, 6] . It can capture com-plex non-linear multimodal feature correlations and make better advantages of deep neural networks.\n\nHowever, inevitably, there are several samples that contain noise, which affects the complementarity between the modalities. Specifically, the noise may cause the various modalities of a sample to tend to different emotion categories. For example, when a model is well-trained on the 4-class emotional recognition task using audio and text, the audio score of a noisy sample indicates that the speech is neutral, but its text score indicates that the speech is happy. Although the model prediction based on scores summation of two modalities may be correct, there is a conflict between them. This phenomenon shows that, due to noise or other factors, the complementarity between various modalities of a sample is lost, resulting in the failure of learning good representation, which greatly affects the generalization of a model.\n\nTo alleviate the above problem, we propose a novel contrastive regularization method. Our motivation is that the learned embeddings should be restricted to contain the same emotional information as much as possible, so as to reduce the influence of noise and improve the system robustness. Contrastive learning  [7]  is a paradigm that leverages abundant negative example pairs to constrain information contained in the learned representation, which satisfies our demand well. More specifically, a discriminator is introduced to compare positive and negative example pairs as the explicit constraint with the same emotion, where the positive example pair is the embeddings obtained by encoding the audio and text from the same sample, and the negative example pair is two modal embeddings that one is the audio embedding in the positive pair, and the other is the text embedding from different samples as well as different emotions. Therefore, the complementarity between the two modalities is greatly enhanced.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "There are currently three main fusion methods in multimodal emotion recognition: feature-level fusion, decision-level fusion and model-level fusion. Feature-level fusion  [8, 9, 10] , a.k.a. early fusion, is implemented by simply concatenating features from multiple modalities into a combined vector as the input of a predictive model. However, this type of fu-sion method might suffer from the curse of dimensionality on small datasets, meanwhile larger dimensional features might stress computational resources during model training. Decision-level fusion  [10, 11, 12] , a.k.a. late fusion, on the contrary, combines predictions obtained from different modalities for a final prediction via a voting strategy. Nevertheless, it does not take into account the interaction between various modalities. As a compromise of early fusion and late fusion, model-level fusion which fuses the intermediate representations of different modalities may be the best choice for the fusion strategy  [4] .\n\nHowever, most previous works based on model-level fusion for emotion recognition seldom take into account the effect of noise on modal complementarity. In fact, there are always samples whose multiple modalities tend to be different emotional categories due to the effect of noise, no matter how well the model is trained. Our intuition is that, by adding the regularization constraint of the same emotion explicitly, the performance of model-level fusion method can be improved. To this end, we draw inspirations from the idea of contrastive learning  [7] , where the difference between positive and negative examples is captured, and propose a novel regularization method. In contrast to random negative sampling commonly used in self-supervised learning  [13] , we draw negative examples from different emotions to restrict the learned representation to contain the same emotional information and experimentally demonstrate the effectiveness of our proposed method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model-Level Fusion",
      "text": "We start by introducing the model-level fusion for emotion recognition using audio and text. The audio features x a and text features x t pass through an individual encoder to obtain the embedding vectors e a and e t , respectively. Then the embedding vectors are concatenated together to classify emotions. We argue that, when embeddings are learned well enough, only a linear classifier is required,\n\nwhere s is the score vector and W is the weight matrix. Eq.\n\n(1) can also be written as:\n\nThe weight matrix W is the concatenation of W 1 and W 2 .\n\nIn other words, the score of audio and text connected vector is the summation of their respective scores. For i-th sample, the classification loss is cross-entropy as follows,\n\nwhere C is the number of categories and s ij is the score of i-th sample j-th class.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Regularization",
      "text": "Due to the influence of noise or other factors, the categories corresponding to the largest scores of s a and s t are inconsistent. As a result, the complementarity between the two modalities is destroyed, which greatly affect the generalization of a model. To overcome the above problem, we propose a contrastive regularization method and its model framework is shown in More specifically, a four-layers CNN is leveraged as the audio encoder for audio spectrogram features to capture the emotional information. Meanwhile, a two-layers LSTM with attention mechanism is used as the text encoder. The input of LSTM is GloVe  [14]  word embeddings which embed a sequence of individual words from video segment transcripts into a sequence of word vectors that represent spoken text. The ensemble of CNN and LSTM is a backbone framework of both the baseline and our method. In addition, a discriminator is introduced and fed by either a positive example pair or negative example pairs and trained to separate them. Specifically, for each sample, we select its corresponding audio and text features as a positive pair, and at the same time select multiple text features with other different emotions as negative pairs. Through the comparison of positive and negative pairs, the learned representations are constrained to contain the same emotional information. Therefore, the noise is reduced and the loss of complementarity can be well mitigated.\n\nFor i-th sample, the contrastive loss is similar to the form of cross-entropy, which is called infoNCE by  [15] ,\n\nwhere d(, ) is the discriminator with the input of either positive example pair (e i a , e i t ) or negative example pair (e i a , e n - t ), and N is the number of negative pairs. In this work, a single hidden layer fully connected neural network is selected as the discriminator where the output is a real value, and N is equal to batch size.\n\nwhere α is a hyperparameter that controls the strength of the regularization term. In this work, we use a grid search with a step size of 0.1 to find the optimal α.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "We use the IEMOCAP database  [16]  to evaluate the performance of different methods, which contains approximate 12 hours of audio-video recordings of dyadic interactions with 10 different speakers split in pair over 5 sessions in English.\n\nThere are about 10000 utterances in the database that has been given emotion labels by at least 3 annotators. 12 unique raters annotate the database by choosing labels out of the 9 possible emotional labels per utterance. Emotional labels are happiness, anger, sadness, neutral, fear, surprise, frustration, excitement, and others.\n\nIn order to compare with majority of previous works  [17, 18] , we select utterances annotated with four basic emotions: neutral, happiness, sadness and anger. Samples with excitement are merged with happiness resulting in a total of 5531 utterances (neutral: 1708, happiness: 1636, sadness: 1084, anger: 1103).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing",
      "text": "For audio features, we extract spectrograms using the python toolkit, pylab  [19] . We calculate a short-time Fourier transform with 256 FFT points and the hop length is 50% of the number of FFT points. In addition, all audio utterances are divided into a uniform length of approximate one second. Thus, the input audio features are 128×128 spectrograms.\n\nFor text features, we extract GloVe  [14]  word embeddings utilizing gensim toolkit  [20] . The word2vec-google-news-300  [21] , a mass of pre-trained vectors trained on a part of the Google News dataset (about 100 billion words), is loaded. We split or pad each sentence into 30 words and acquire a 300-dimensional word embedding sequence of length 30.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "For the CNN block used to process the spectrograms, a 3×3 convolution kernel is utilized with padding 1, and the maxpooling kernel is 2×2 with a 2×2 stride. Moreover, the activation function is ReLU and the output channels are all 8. The LSTM which is used to process word embedding sequences has 200 hidden nodes. It is a single-directional framework with two layers. An Adam optimizer is used with the learning rate of 0.001, and batch size is set to 64. The hyperparameter α is selected as 0.1 through a grid search in the range of [0.0, 1.0] with a step size of 0.1.\n\nFollowing  [17, 18] , we utilize 10-fold cross validation with a 8:1:1 split for training, validation and testing, respectively. The performance of the system is measured in terms of weighted accuracy (WA) and unweighted accuracy (UA)  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "Table  1  presents the performance of our approach for emotion recognition compared with other methods. The baseline model is based on model-level fusion that concatenate     [23]  72.7 73.3 MDRE  [17]  71.8 -MHA-2  [18]  76.5 77.6 Contrastive regularization (Proposed) 76.06 77.45 embeddings obtained from audio and text modalities into a combined representation as the input of a linear classification model. DS-LSTM  [23]  is a dual-level model that predicts emotions based on both MFCC features and melspectrograms, and it is the state-of-the-art unimodal emotion recognition only using audio. MDRE  [17]  encodes the information from audio and text sequences using dual recurrent neural network (RNN) and then combines the information from these sources to predict the emotion class. MHA-2  [18]  uses a multi-hop attention mechanism to exploit both textual and acoustic information in tandem.\n\nAs illustrated in Table  1 , our proposed method outperforms DS-LSTM by 3.36% and 4.15% in terms of WA and UA, which further demonstrates the previous conclusion that the multimodal method is better than the unimodal method. In addition, it is worth noting that MHA-2, the state-of-theart multimodal emotion recognition using audio and text, achieves 76.5% in WA and 77.6% in UA. Although our results are not directly comparable due to distinct model framework used and domain specialized optimizations, the idea behind our contrastive regularization method, which leverages abundant negative example pairs to acquire stronger representations, can be characterized as an additional contrastive loss and serve as a regularization term. Therefore, our proposed method is more applicable than MHA-2 due to the limitation of multi-hop attention mechanism which is only used for audio and text sequential data.\n\nMore notably, the 1.44% and 1.53% improvement in terms of WA and UA compared to the baseline are realized by our proposed method. Meanwhile, as confusion matrices shown in Fig.  2 , our proposed method acquires the same accuracy 68.77% compared to the baseline method in classifying neutral emotion and outperforms the baseline method in other three types of emotions. Moreover, the baseline model incorrectly classifies more examples of one emotion as another emotion than our proposed method, e.g.,  16 .08% to 14.15% for happy, 11.01% to 9.63% for angry, except for misclassification of neutral as happy (14.52% to 16.44%). For this type of misclassification, we guess that randomly sampling other different emotions against neutral class may sample more happy emotions due to the slight imbalance of the samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed a novel regularization method via contrastive learning for multimodal emotion recognition task. The proposed method is designed to constrain latent representations of audio and text to contain the same emotional information, so that it enhances the complementarity between the two modalities and improves the generalization of a model. Experiments show that the proposed method outperforms the baseline system in classifying the four emotion categories on IEMOCAP database by 1.44% and 1.53% in terms of WA and UA respectively and is competitive with the state-of-the-art system.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The upper block diagram represents a Convolution",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of our model framework",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrices of baseline and our method. Left and right ﬁgures represent baseline and our method respectively.",
      "page": 4
    },
    {
      "caption": "Figure 2: , our proposed method acquires the same",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: presents the performance of our approach for emo-",
      "page": 3
    },
    {
      "caption": "Table 1: Methods performance comparison on IEMOCAP",
      "page": 4
    },
    {
      "caption": "Table 1: , our proposed method outper-",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "Y Hillel Aviezer",
        "A Trope",
        "Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "4",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "N Sebe",
        "J Cohn",
        "T Huang"
      ],
      "year": "2005",
      "venue": "MULTI-MEDIA '05"
    },
    {
      "citation_id": "5",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "H Nicolaou",
        "M Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "Lang He",
        "Dongmei Jiang",
        "Le Yang",
        "Ercheng Pei",
        "P Wu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks"
    },
    {
      "citation_id": "7",
      "title": "An investigation of annotation delay compensation and outputassociative fusion for multimodal continuous emotion prediction",
      "authors": [
        "Zhaocheng Huang",
        "Ting Dang",
        "Nicholas Cummins",
        "Brian Stasak",
        "P Le",
        "V Sethu",
        "J Epps"
      ],
      "year": "2015",
      "venue": "An investigation of annotation delay compensation and outputassociative fusion for multimodal continuous emotion prediction"
    },
    {
      "citation_id": "8",
      "title": "Contrastive self-supervised learning",
      "authors": [
        "Ankesh Anand"
      ],
      "year": "2020",
      "venue": "Contrastive self-supervised learning"
    },
    {
      "citation_id": "9",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Zhigang Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "Shrikanth Narayanan"
      ],
      "year": "2004",
      "venue": "Analysis of emotion recognition using facial expressions, speech and multimodal information"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal multi-cultural dimensional continues emotion recognition in dyadic interactions",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Shizhe Chen",
        "Q Jin"
      ],
      "year": "2018",
      "venue": "Multi-modal multi-cultural dimensional continues emotion recognition in dyadic interactions"
    },
    {
      "citation_id": "11",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "Fabien Ringeval",
        "D Lalanne",
        "Mercedes Torres",
        "S Scherer",
        "Giota Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "13",
      "title": "From hard to soft: Towards more humanlike emotion recognition by modelling the perception uncertainty",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Selforganizing neural network that discovers surfaces in random-dot stereograms",
      "authors": [
        "Suzanna Becker",
        "Geoffrey Hinton"
      ],
      "year": "1992",
      "venue": "Nature"
    },
    {
      "citation_id": "15",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "R Jeffrey Pennington",
        "Christopher Socher",
        "Manning"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "16",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "Chi-Chun Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Subhadeep Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Matplotlib: A 2d graphics environment",
      "authors": [
        "J Hunter"
      ],
      "year": "2007",
      "venue": "Computing in Science & Engineering"
    },
    {
      "citation_id": "21",
      "title": "Gensim -statistical semantics in python",
      "authors": [
        "Radim Rehurek",
        "P Sojka"
      ],
      "year": "2011",
      "venue": "Gensim -statistical semantics in python"
    },
    {
      "citation_id": "22",
      "title": "Linguistic regularities in continuous space word representations",
      "authors": [
        "Tomas Mikolov",
        "W Yih",
        "G Zweig"
      ],
      "year": "2013",
      "venue": "HLT-NAACL"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}