{
  "paper_id": "2401.08095v4",
  "title": "Durflex-Evc: Duration-Flexible Emotional Voice Conversion Leveraging Discrete Representations Without Text Alignment",
  "published": "2024-01-16T03:39:35Z",
  "authors": [
    "Hyung-Seok Oh",
    "Sang-Hoon Lee",
    "Deok-Hyeon Cho",
    "Seong-Whan Lee"
  ],
  "keywords": [
    "emotional voice conversion",
    "self-supervised representation",
    "style disentanglement",
    "duration control"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional voice conversion (EVC) involves modifying various acoustic characteristics, such as pitch and spectral envelope, to match a desired emotional state while preserving the speaker's identity. Existing EVC methods often rely on text transcriptions or time-alignment information and struggle to handle varying speech durations effectively. In this paper, we propose DurFlex-EVC, a duration-flexible EVC framework that operates without the need for text or alignment information. We introduce a unit aligner that models contextual information by aligning speech with discrete units representing content, eliminating the need for text or speech-text alignment. Additionally, we design a style autoencoder that effectively disentangles content and emotional style, allowing precise manipulation of the emotional characteristics of the speech. We further enhance emotional expressiveness through a hierarchical stylize encoder that applies the target emotional style at multiple hierarchical levels, refining the stylization process to improve the naturalness and expressiveness of the converted speech. Experimental results from subjective and objective evaluations demonstrate that our approach outperforms baseline models, effectively handling duration variability and enhancing emotional expressiveness in the converted speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTIONAL voice conversion (EVC) involves modifying various acoustic characteristics of a voice, such as pitch and spectral envelope, to match a desired emotional state while preserving the speaker's identity  [1] . EVC has gained prominence, particularly in the realm of voice-interactive technologies such as virtual assistants and internet of things (IoT) devices, improving the human-like and emotionally resonant aspects of digital interactions  [2] -  [5] .\n\nIn the context of EVC, a crucial objective is to preserve the speaker identity and content of the original speech while modifying only those speech attributes that convey emotion  [1] ,  [6] . This necessitates an adjustment of prosody to align with the intended emotion. Prosody elements, including intonation, rhythm, and energy, play a critical role in both conveying and recognizing emotions in speech. Although the concept of controlling prosody for emotional conversion is intuitively appealing  [7] , refining each prosody component presents a significant challenge.\n\nThe field of EVC has been revolutionized by advances in deep learning  [8] ,  [9] . Initial approaches employed Gaussian mixture models  [10]  to convert spectral and prosody features, producing more expressive voices. Subsequent developments led to autoencoder-based methods  [11] -  [13] , enabling learning in non-parallel data-driven EVC. VAE-based approaches  [14]  and GAN-based frameworks  [15] -such as Cycle-GAN  [16] , StarGAN  [17] , and VAE-GAN  [18] -represent further advances. However, these methods often overlook the importance of rhythm in expressing emotion, as they typically support emotional conversion with fixed durations.\n\nSequence-to-sequence (Seq2Seq) models, capable of implicitly modeling duration, have become a notable development  [19] ,  [20] . These models often employ strategies, such as a two-stage learning approach integrating a text-to-speech (TTS) model, to improve stability  [21] ,  [22] . However, Seq2Seq models face challenges like long-term dependency and repetition, necessitating parallel generation for efficiency and reliability. Parallel generation requires explicit duration modeling, which many voice conversion models  [23]  achieve through phoneme duration derived from TTS alignments. Obtaining phoneme duration often requires encoder-decoder attention from pretrained autoregressive TTS models  [24]  or external forced alignment tools  [25] .\n\nRecently, discrete speech units derived from self-supervised learning representations have shown promise in addressing parallel generation challenges  [26] -  [29] . Certain studies  [28] ,  [29]  have proposed methods that model speech emotion conversion as a translation task, thereby enabling parallel audio generation. However, these methods still rely on autoregressive models for emotional translation, limiting their effectiveness in fully parallel generation.\n\nIn this paper, we propose a duration-flexible EVC framework, DurFlex-EVC, that eliminates the need for text or alignment information while supporting parallel generation. We use discrete speech units to model content and incorporate arXiv:2401.08095v4 [cs.SD] 21 Jan 2025 self-supervised representations to enrich acoustic details. Our model predicts unit sequences and their durations, which allows for flexible duration control.\n\nOur main contributions are as follows:\n\n‚Ä¢ We propose DurFlex-EVC, a duration-flexible EVC framework that uses discrete speech units for content modeling, eliminating the need for text or external alignment information while supporting efficient parallel generation.\n\n‚Ä¢ We introduce a unit aligner that models contextual relationships between speech features and unit sequences, enabling effective duration control without relying on text-based alignment. ‚Ä¢ We develop a style autoencoder that separates and reintroduces emotional styles in the input features, facilitating content-style disentanglement for emotional voice conversion.\n\n‚Ä¢ We design a hierarchical stylize encoder that captures both global and local emotional patterns, thereby enhancing the expressiveness of emotional speech. ‚Ä¢ We perform extensive subjective and objective evaluations, demonstrating the superior performance of our method compared to existing approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "A. Exploring Self-Supervised Learning in Speech\n\nSelf-supervised learning (SSL) is a machine learning paradigm in which models are trained on unlabeled datasets to generate meaningful representations. This approach is particularly advantageous in speech processing, where labeling data is labor-intensive and expensive. Models like wav2vec 2.0  [30]  and vq-wav2vec  [31]  have demonstrated SSL's potential, with wav2vec 2.0 leveraging contrastive learning and vq-wav2vec introducing quantization techniques for discrete representations. Subsequent developments, such as XLS-R  [32] , extend these ideas to cross-lingual applications, while models like Hidden-unit BERT (HuBERT)  [33]  and ContentVec  [34]  focus on masked prediction and speaker disentanglement, respectively.\n\nSSL representations have proven valuable across a wide range of speech-related tasks, including automatic speech recognition  [30] , voice conversion  [35] , speaker verification  [36] , and speech emotion recognition  [37] . The increasing adoption of SSL techniques motivates further exploration into how these representations can be leveraged for more complex and resource-efficient speech applications, such as emotional voice conversion, where conventional supervised methods are often limited by data availability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Discrete Units In Speech Processing",
      "text": "Discrete unit representations have emerged as a versatile tool in audio and speech processing  [38] -  [40] . Techniques like SoundStream  [38]  and EnCodec  [39]  focus on high-fidelity audio compression, employing methods such as residual vector quantization (RVQ), while UniAudio  [40]  serves as a generalpurpose audio generation model. These neural codec-based methods, aimed at audio compression and restoration, utilize large codebooks and compact dimensions. Although these approaches excel in compression, they are less suited for tasks that require detailed modeling of linguistic and prosodic content.\n\nIn contrast, some methods emphasize encoding speech into semantic units to facilitate applications like speech synthesis  [29]  and emotion conversion  [33] . For instance, techniques have been developed to decompose and reconstruct speech into discrete units for content, pitch, and speaker identity  [26] . To improve naturalness and intelligibility, soft speech units were introduced  [28]  for enhanced content capture. Speech emotion conversion has also been framed as a language translation task  [27] , leveraging discrete representations of phonetic content, prosody, speaker, and emotion alongside neural vocoders for waveform generation. UnitSpeech  [29]  has demonstrated effectiveness in personalized TTS and voice conversion, using self-supervised units to fine-tune a diffusion-based TTS model with minimal data, thus bypassing the need for retraining across tasks. This focus on discrete units offers a promising solution to the challenges of aligning speech content, offering the potential for more efficient and accurate modeling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Parallel Speech Generation",
      "text": "Non-autoregressive speech synthesis models generate speech frames in parallel, significantly reducing inference time compared to autoregressive methods  [24] ,  [41] ,  [42] . Recent studies have demonstrated that parallel generation methods can offer superior performance and reliability over autoregressive approaches for applications like text-to-speech, voice conversion, and vocoders. In text-to-speech systems, parallel generation typically relies on precise alignments between text and speech. These alignments are often derived using pretrained autoregressive teacher models  [24] , external aligners like the Montreal Forced Aligner (MFA)  [43] , or algorithms such as the monotonic alignment search (MAS)  [41] . Voice conversion methods often attempt to leverage the benefits of parallel generation. While many approaches  [44] ,  [45]  have adopted parallel generation strategies, certain models remain autoregressive  [46]  due to complexities in handling variable durations. To address these challenges, some solutions  [23] ,  [47]  have integrated variable-duration modeling with parallel generation. However, this often requires text-speech alignment similar to TTS frameworks. Additionally, vocoders have used techniques like GANs  [48] , normalizing flows  [49] , and generative diffusion models  [50]  to efficiently produce high-quality waveforms. The success of parallel generation frameworks has inspired further exploration into adapting these techniques for emotional voice conversion, with the goal of achieving efficient and expressive speech synthesis without relying on traditional alignment methods.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Duration Modeling In Speech Processing",
      "text": "Duration modeling is crucial in speech synthesis, particularly in TTS systems, where discrepancies between character length and audio signals often occur  [24] . Early approaches ùë§ ùë†ùëüùëê ùë§ ùë°ùëîùë° Fig.  1 . Overall framework of the proposed method. The feature extractor transforms the source audio into input features. These features are subsequently disentangled and reconditioned by the style autoencoder. The unit aligner is responsible for providing unit-level context and performing duration modeling. In addition, the hierarchical style encoder encodes features at both the unit and frame levels. Mel-spectrogram is subsequently produced by the generator. In this figure, \"DP\" represents the duration predictor, LR denotes the length regulator, while \"Q\", \"K\" and \"V\" represent the query, key and value of the cross-attention in the unit aligner, respectively. ‚äó denotes the concatenate operation. wsrc represents the source style vector and wtgt represents the target style vector. The style autoencoder disentangles the source style from the features and applies the target style, while the hierarchical stylize encoder and generator take the target style as a condition. used autoregressive models to implicitly handle duration, generating speech one frame at a time  [51] ,  [52] . FastSpeech  [24]  improved upon these methods by leveraging alignments from an autoregressive teacher model to explicitly model phoneme duration, thereby enabling parallel generation. Building on this, FastSpeech 2  [25]  introduced phoneme duration extraction using external forced alignment. Glow-TTS  [41]  further refined this approach by employing a monotonic alignment search to find optimal matches between text and latent representations. Duration modeling has also been explored in voice conversion, where seq2seq models are often used to manage variable durations  [46] ,  [53] . The DCVC model  [23]  utilized a phoneme-based information bottleneck for style transfer and speech speed control in voice conversion. Discrete speech units have also been leveraged in voice conversion for modeling duration by counting consecutive occurrences of each unit. This approach allows for effective duration control while bypassing the need for explicit text or phoneme alignment  [26] ,  [29] . In EVC, seq2seq structures have become a popular choice for managing duration variations  [22] ,  [54] .  [27]  leveraged discrete units for parallel emotional voice conversion but still incorporated seq2seq models for the translation of unit sequences, introducing challenges inherent to autoregressive architectures. Our work aims to address these limitations by employing discrete units to model duration, eliminating the need for text-speech alignment and enabling a more flexible parallel generation framework.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "Our work introduces DurFlex-EVC, a duration-flexible and parallel generation framework for emotional voice conversion. We designed this system to overcome limitations of previous approaches, such as the reliance on seq2seq structures and the need for external text or alignment information. The key components of our model and their roles are as follows:\n\n‚Ä¢ Feature extractor: Transforms raw audio waveforms into acoustic features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Overview",
      "text": "Fig.  1a  illustrates the overall architecture of DurFlex-EVC. The process begins with the feature extractor. It transforms raw audio waveforms into acoustic features. While various feature representations, such as Mel-spectrogram or SSL outputs, can be utilized, our approach employs the final layer representations from HuBERT  [33] . This choice provides continuous representations that retain comprehensive acoustic information, essential for accurately modeling emotional characteristics in speech. Next, the style autoencoder separates the emotional style from the input features and applies the desired target emotion. This disentanglement ensures that content and style are managed independently, allowing for precise emotional adaptation without relying on seq2seq structures. The unit aligner then processes the stylized features by aggregating contextual information at the frame level through a cross-attention module. This aggregated representation is compressed to the unit level, preparing it for further processing by the hierarchical stylize encoder. The hierarchical stylize encoder operates at both unit and frame levels, refining the emotional style to ensure consistency and expressiveness across different scales. Finally, the diffusion-based generator synthesizes a highquality Mel-spectrogram from the output of the hierarchical stylize encoder and the style vector. This Mel-spectrogram is subsequently converted into a raw waveform using a pretrained vocoder, completing the emotional voice conversion process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Style Autoencoder",
      "text": "The feature extractor generates representations that encompass both content and style aspects of the speech. In this context, content refers to the linguistic information, while style includes emotional expression and speaker-specific characteristics. To model styles, we distinguish between speaker style and emotional style. The style autoencoder is designed to separate the source emotional style from the input features and apply a target emotional style, facilitating content-style disentanglement. Fig.  1b  shows the structure of the style autoencoder, which consists of two primary components: the de-stylize transformer and the stylize transformer.\n\nLayer Normalization (LN) is employed as a fundamental technique in both transformers, defined as:\n\nwhere z represents the input vector, ¬µ is the mean, and œÉ is the standard deviation. LN ensures that the inputs to each layer have a consistent distribution, facilitating stable and efficient training.\n\nDe-stylize Transformer: This component removes the source emotional style from the input features, isolating the content. It employs mix-style layer normalization (MixLN)  [55] , which blends the original style vector with a shuffled style vector to disrupt the association between features and the source style, as shown in Fig.  1f . The MixLN is defined as:\n\nwhere\n\n. w is the original style vector, w is the shuffled style vector, and Œª is a parameter sampled from a Beta distribution, Beta(Œ±, Œ±). This process helps in disentangling style-independent content by preventing the model from encoding style-specific features.\n\nStylize Transformer: This component applies the target emotional style to the content features. It employs conditional layer normalization (CLN)  [56] , which adjusts the normalized  input based on the target style vector w, as illustrated in Fig.  1g . The CLN is defined as:\n\nwhere Œ≥(w) and Œ≤(w) are adaptive parameters derived from the style vector w. CLN allows the model to incorporate the desired emotional style into the content features seamlessly.\n\nThe style autoencoder consists of N de-stylize transformers and N stylize transformers. We construct the source style vector w src by combining the speaker vector s src and the emotion vector e src , and the target style vector w tgt by combining s src with the desired emotion vector e tgt :\n\nBoth the speaker vector s * and the emotion vector e * are obtained from the embedding look-up table. The de-stylize transformer uses w src to disentangle the source emotional style from the input features, while the stylize transformer applies w tgt to encode the target emotional style into the features. This design enables the model to adapt emotional styles effectively without relying on seq2seq structures, thereby supporting parallel processing and flexible duration modeling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Unit Aligner",
      "text": "The unit aligner is responsible for predicting unit sequences by modeling the semantic context of the input speech. As illustrated in Fig.  1d , the unit aligner leverages a crossattention mechanism combined with learnable embeddings to generate attention weights that guide unit prediction.\n\nWe use the output of the style autoencoder as a query (Q) and introduce learnable embeddings e unit as keys (K) and values (V ) to cross-attention. The attention weights A unit are computed as follows:\n\nwhere d is the dimension of Q and K. Subsequently, these weights A unit are integrated with the value matrix V to produce the attention output z attn .\n\nFig.  1e  shows the process of learning attention weights to predict units and the modeling of the duration of the predicted units. We introduce an additional loss term to guide the attention module to learn semantic information. This approach implies a direct classification task by aligning the attention weights A unit with the target unit sequence y:\n\nwhere L is the length of the unit sequence and C is the number of unit classes, A i,j unit represents the predicted probability of the i-th element for class j from the attention weights A unit , and y i,j is the one-hot encoded class label for the ith unit for class j. We adopt the HuBERT unit as our target unit. This is an intended design feature that ensures that the context reflected in the unit aligner is consistent with the target style, rather than based on the context of the input speech. During the emotional voice conversion process, the style autoencoder first removes the source emotion from the input features and applies the target emotion. The unit aligner then processes these stylized features to predict the corresponding unit sequences. This design enables the model to transform context efficiently. It also supports parallel processing without relying on seq2seq structures.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Duration Modeling",
      "text": "Once the unit aligner has predicted the unit sequence, the duration modeling component determines the duration of each unit based on consecutive identical units. This allows the model to adjust the generated speech length to align with the target emotional style. The unit sequence ≈∑ is obtained by selecting the unit with the highest probability for each frame:\n\nwhere ≈∑i is the i-th predicted unit and A i unit represents the attention weights for the i-th frame. To extract a distinct sequence of units and their consecutive counts, a deduplication operation is applied:\n\nwhere ≈∑uniq contains the sequence of unique units, and n count represents the number of consecutive occurrences for each unit. For example, given an input sequence ≈∑ = [4, 4, 2, 2, 2, 2, 1, 1], the deduplication results in ≈∑uniq = [4, 2, 1] and n count =  [2, 4, 2] . This indicates two instances of unit 4, followed by four instances of unit 2 and two instances of unit 1.\n\nThe duration predictor is trained using n count as the target durations. This predictor estimates the number of consecutive units based on the unit-level representations generated by the unit aligner. Fig.  2a  explains unit-level pooling. The output of the unit aligner, z attn , is averaged based on the duration of the unit and results in downsampling the sequence length for alignment at the unit level.\n\nwhere z u is the latent downsampled at the unit level. For example, given that z attn = [0.2, 0.2, 0.1, 0.4, 0.5, 0.2, 0.3, 0.5] and n count = [2, 4, 2], the result of unit-level pooling is",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Hierarchical Stylize Encoder",
      "text": "The hierarchical stylize encoder  [57]  functions at two levels: the unit level and the frame level. It consists of two components: the unit-level stylize transformer (UST) and the framelevel stylize transformer (FST). UST processes z u into a z us , denoted as z us = UST(z u , w tgt ), focusing on unit-specific features. This refined variable z u is scaled to the frame level through a length regulator LR, depicted in Fig.  2b .\n\nwhere z f represents the latent variable at the frame level. For example, if z us = [0. The duration predictor takes z us and w tgt as input and is trained to predict the unit-level duration n count . For emotionbased duration dynamics, we introduce the flow-based stochastic duration predictor proposed in  [58]  to add duration uncertainty. The duration predictor training objective L dur follows a negative variational lower bound.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F. Diffusion-Based Mel-Spectrogram Generator",
      "text": "We use a diffusion framework based on stochastic differential equations (SDE) to generate high-quality speech with expressive emotions. The diffusion-based model gradually transforms the Mel-spectrogram into Gaussian noise in a forward process and generates samples from the noise in a reverse process. We adopt the standard normal distribution as the prior distribution, as in  [29] . The model is trained to minimize the mean square error (MSE) loss L dif f between the ground truth noise and the estimated noise. For score estimation, our model incorporates a network denoted by s Œ∏ based on the U-net architecture with linear attention used in Grad-TTS  [42] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "G. Training Objective",
      "text": "Consequently, the model is trained using the following loss function:\n\nwhere Œª dif f , Œª unit , and Œª dur are the loss weights, which we set to 1.0, 0.1, and 0.1, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "H. Emotion Voice Conversion Process",
      "text": "The process of converting the emotion in the input speech to the target emotion is as follows.\n\n1) The input waveform is converted into input features by the feature extractor.  2) The features are de-stylized from the source style vector and stylized to the target style vector by the style autoencoder. The source style vector is obtained from the source emotion vector and the speaker vector. The target style vector is obtained from the target emotion vector and the speaker vector. The source style is disentangled by the MixLN of the style autoencoder, and the target style is applied by the CLN. 3) Unit-level features according to the target style are obtained with the unit aligner. 4) The hierarchical stylize encoder adapts the target style to the features at the unit-level and the frame-level. 5) The diffusion-based generator produces a Melspectrogram conditioned on the feature and target style vector. 6) The waveform is synthesized by pre-trained vocoder.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "We conducted experiments using the emotional speech dataset (ESD) 1   [59] , which contains 350 parallel utterances spoken by 10 native Mandarin speakers and 10 English speakers with 5 emotional states (neutral, happy, angry, sad, and surprise). Following the data partitioning guidelines provided by ESD, we constructed the training set with 300 samples per emotion per speaker, for a total of 15,000 samples. The validation set included 20 samples for each emotion per speaker, totaling 1,000 samples, and the test set comprised 30 samples for each emotion per speaker, totaling 1,500 samples. Audio was sampled at 16,000 Hz and transformed into an 80-bin Mel-spectrogram using a short-time Fourier transform (STFT) with a window length of 1,024 and a hop size of 256.\n\nThe experiments included transformations between all possible emotional states, not just limited from neutral to other states. This approach was designed to cover all possible emotional state conversions, ensuring a comprehensive assessment of the model's performance. For subjective evaluation, 10 sentences were randomly selected for each of the 5 emotions. These sentences were then adapted to reflect each of the four other emotional states, resulting in a total of 200 samples (10 √ó 5 √ó 4 = 200). For objective evaluation, each of the 1500 test samples was transformed into the four other emotional states, resulting in a total of 6000 samples (1500 √ó 4 = 6000).\n\n1 https://github.com/HLTSingapore/Emotional-Speech-Data This setup provided a thorough assessment of the model's performance across all possible emotional state conversions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In our experimental setup, we configured both the de-stylize and stylize transformers with specific parameters: the hidden dimension, kernel size, number of heads, FFN kernel size, and feed forward network (FFN) hidden size were set to 256, 5, 2, 9, and 1024, respectively. The Œ± parameter of the Beta distribution for MixLN was fixed at 0.1. All transformers used in our model were organized into N layers, with N set to 4. The unit aligner featured multi-head attention with 16 heads. We set T = 1, Œ≤ t = Œ≤ 0 + (Œ≤ 1 -Œ≤ 0 )t, Œ≤ 0 = 0.05, and Œ≤ 1 = 20 as noise schedules. The U-Net in our model was set to downsample four times and had a hidden dimension of 128. We evaluated our model with two different time steps: 4 and 100. The duration predictor, which comprises residual blocks using dilated and depth-separable convolution, was structured in four layers. To address the resolution disparity between the HuBERT unit and the Mel-spectrogram, we expanded the hidden representation with a length regulator and employed linear interpolation for upsampling. In training the generator, we utilized random segments, setting the segment size to 32 frames of the Mel-spectrogram. The AdamW optimizer was used, with a learning rate of 1√ó10 - 4  . We set the batch size to 16 and the number of training steps to 500K. We trained the vocoder using the official BigVGAN 2    [60]  implementation, incorporating the LibriTTS  [61] , VCTK  3  , and ESD datasets. All experiments were conducted on an Intel¬Æ Xeon¬Æ Gold 6148 CPU @ 2.40 GHz and a single NVIDIA RTX A6000 GPU. For broader accessibility, the code 4 and a demo 5  of our proposed method are available online.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Evaluation 1) Subjective Metrics:",
      "text": "We conducted subjective evaluations using Amazon Mechanical Turk (mTurk). Our analysis included the mean opinion score (MOS) for naturalness (nMOS) and speaker similarity (sMOS), using a 9-point scale ranging from 1 to 5, with increments of 0.5 units. The results are presented with a confidence interval (CI) of 95%. Furthermore, we use the emotion mean opinion classification (eMOC) as suggested in  [27] .\n\n2) Objective Metrics: For our objective evaluation, we incorporate a range of metrics: predicted mean opinion score, phoneme error rate (PER), character error rate (CER), word error rate (WER), emotion classification accuracy (ECA), and speaker embedding cosine similarity (SECS). The predicted MOS was assessed using UTMOS  [62]    6  . For the PER calculation, we used a wav2vec2.0-based phoneme recognition model from Hugging Face  [63] . CER and WER were determined using Whisper 7    [64] . In assessing SECS, we extracted speaker embeddings from both target and generated audio using Resemblyzer  8  , subsequently computing their cosine similarity. This similarity measure ranges from -1 to 1, where higher values denote greater similarity. We evaluated the similarity for samples that shared the same speaker and emotion and then averaged these across all speakers. The objective evaluation of emotions in the generated speech was conducted using a pre-trained speech emotion recognition (SER) model.\n\nTo measure SER accuracy, we employed emotion2vec  [65] . We used emotion2vec+ base  9  , a pre-trained model that supports nine classes, and only used the five sentiment classes in the ESD dataset for evaluation. We propose the emotion embedding cosine similarity (EECS) to evaluate the emotion  of synthesized speech. The EECS is obtained by computing the cosine similarity of the emotion embedding between the synthesized audio and arbitrary reference audio with the target emotion. The emotion embedding was obtained using emotion2vec. Fig.  3  is a visualization of the features in emotion2vec, which shows that it encodes emotions independently of the speaker. We also evaluated the root mean square error (RMSE) for pitch and energy and calculated the difference of duration (DDUR) to assess prosody. The pitch was extracted using parselmouth  10  in Hz, and the energy was obtained as the L2-norm of the absolute value of the linear spectrogram. DDUR was obtained in the same way as in  [22] . We measured the real-time factor (RTF) of inference time for computational analysis. Table  I  presents the results of each pre-trained baseline model on the test set, comprising 1500 samples, for both ground-truth and vocoded samples.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Comparison Models",
      "text": "To benchmark the efficacy of our proposed method, we trained and compared it against several existing models.   [27] : This model approaches speech synthesis by deconstructing the speech signal into discrete learned representations. These include speech content units, prosody features, speaker identity, and emotions. Each element is modified to align with the target emotion before being synthesized back into speech. ‚Ä¢ DurFlex-EVC: Our proposed model includes a unit aligner, style autoencoder, stochastic duration predictor, hierarchical stylize encoder, and a diffusion-based generator. This model stands out with its comprehensive and integrated approach to emotional speech synthesis. All comparison models were trained using the official implementation. We used the same vocoder to generate the waveforms except for Textless-EVC, which generates the waveform directly, and adjusted the hyperparameters to match the vocoder settings.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Result",
      "text": "This section contains the results and discussion of the extensive experiments. We compared our proposed model with previous EVC models to evaluate its quality. We then conducted experiments to demonstrate the effectiveness of 11",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Comparison Of Evaluation Results For Baseline Models",
      "text": "To evaluate the performance of the proposed model, we conducted both objective and subjective evaluations to compare it with the baseline models. Table  II  shows the results of subjective evaluation. Table  III  shows the objective evaluation results. As the nMOS results show, the naturalness of the speech generated by our method outperforms that of other models. In terms of speaker similarity, our approach scored the highest on both sMOS and SECS. Fig.  4  shows the resulting SECS for all combinations of emotional conversions. This demonstrates that the proposed model is more robust in terms of speaker similarity than the comparison models for all combinations of transformations. Furthermore, the results of eMOC, ECA and EECS demonstrate that our method performs better in terms of perceptual quality as well as objective metrics. Fig.  5  shows the resulting EECS for all combinations of emotion transformations. This indicates that the proposed model synthesized speech with a higher emotional similarity than the comparison models for all combinations of transformations. The ASR evaluation also shows that our method achieves lower values in PER, CER, and WER compared to other models. This emphasizes the ability of our method to synthesize precisely pronounced speech.\n\nWe evaluated the computational efficiency of each model by comparing their RTF. Autoregressive models, including Seq2seq-EVC, Emovox, Mixed Emotion, and Textless-EVC, generally demonstrated slower inference speeds compared to parallel generation models such as StarGAN-EVC. DurFlex-EVC, as a diffusion-based model, exhibited varying RTFs depending on the number of sampling time steps. With just four time steps, DurFlex-EVC achieved an RTF of 0.1334, significantly faster than all compared autoregressive models. At this setting, DurFlex-EVC maintained competitive performance and exceeded the speed and performance metrics of other autoregressive models. However, when using 100 time steps, DurFlex-EVC achieved the highest performance, though at a slower RTF of 1.9200 due to the increased computational demand of the diffusion process.\n\nFor a quantitative assessment of prosody, we also compared pitch and energy duration. Table  IV  shows the evaluation results for each emotion. We found that the proposed model scored better than the other models in all prosody evaluations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Experiments For Analyzing Model Architectures",
      "text": "We conducted an analysis of the model design and additional experiments.   SAE indicates the importance of disentangling content and style from input features. Fig.  6  shows the change in cross entropy for styles as input features pass through SAE. To obtain cross entropy, we trained a classifier for emotion style on features extracted from all layers of SAE. The black line is the result for the model with SAE and the orange line is the result for the model without SAE. The w/o SAE model shows high cross entropy across the layers, while the model with SAE shows increasing cross entropy as it passes through the de-stylize transformer layers and decreasing cross entropy as it passes through the stylize transformer layers. This means that the style is disentangled and conditioned from the feature by SAE. We also experimented with adversarial training strategies to remove source styles from input features. This replaces the de-stylize transformer in the style autoencoder with a standard transformer, and adds a gradient reversal layer and classifier to prevent its output from learning the style. This adversarial training strategy is often used to disentangle specific styles  [67] -  [69] .\n\nThe results for this are shown in Table V as w/ adv, and show a performance degradation on all metrics except EECS.\n\n2) Effectiveness of Unit Aligner: The unit aligner (UA) was introduced to model stylized contexts. Table V w/o UA shows the results of the ablation experiment for this. The results showed that w/o UA performed better in terms of voice quality and pronunciation but not in terms of emotion conversion. This indicates that there remains a lot of information about the source style in the features and that the UA functions as a bottleneck and has a significant impact on style control.\n\nWe measured the Jensen-Shannon divergence (JSD) to validate that the predicted units in UA reflect the target emotion. Table  VI  shows the JSD results of the units for each emotion in Textless-EVC and DurFlex-EVC. DurFlex-EVC achieved a better JSD than Textless-EVC for all emotions, indicating that it provides an appropriate representation of the context.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "3) Effectiveness Of Hierarchical Stylize Encoder:",
      "text": "The output of the unit aligner is a representation in units, which is expanded to frame-level by a duration predictor and length regulator. For the frame-level stylization in this representation, we introduce a hierarchical stylize encoder (HSE). The results of the ablation study without the HSE are shown in Table V under the entry labeled w/o HSE. The evaluation results showed an overall performance degradation with w/o HSE. This is inferred to be due to HSE reducing the burden on the Mel-spectrogram generator, resulting in improved generation    We introduced a stochastic duration predictor (SDP) for duration modeling to represent the diversity of emotions. To evaluate, we compared our method with widely used deterministic duration predictors (DDP). The DDP follows the structure described in FastSpeech  [24] , which includes two convolutional layers, ReLU activation, layer normalization, and dropout. Tables V, w/ DDP shows the evaluation results of the model using DDP. The evaluation results show that the model using SDP is better than the model using DDP. We compared the JSD of unit duration for each emotion for each duration predictor. Table  VII  shows the JSD results for unit duration for models with stochastic duration predictors, w/ SDP, and deterministic duration predictors, w/ DDP.\n\nIn our experiments, we found that DDP does not always output the same length when generating the same sentence with the same conditions. We discovered that it was caused by a dropout within the DDP. We experimented with repeatedly generating the same speech, and Fig.  7  shows a histogram of  Diffusion-based generators have been shown to produce highquality and diverse results in a wide range of domains. We introduced a diffusion-based structure to generate more expressive speech for each emotion. We conducted experiments to compare our decoder with a feed-forward transformer (FFT) based structure, which is widely used in conventional speech synthesis studies for parallel generation. Table V w/ FFT shows the objective evaluation results of the model using the FFT-based decoder. The w/ FFT scored better in CER and WER for pronunciation, but lower on UTMOS for quality, and worse on ECA and EECS for emotion expression.\n\nTo verify the expressiveness of each emotion, we computed the JSD over pitch for each emotion. In Table VIII, w/ FFT is the model using an FFT decoder and w/ diffusion is the model using a diffusion-based structure. w/ diffusion shows better JSD on neutral, angry, sad, while w/ FFT shows better JSD in happy and surprise. Although w/ FFT outscores w/ diffusion in Happy and Surprise, it does not mean that FFT is more  expressive. Fig.  8  shows the pitch of the speech generated by multiple iterations of the same sentence. Models using FFT produce consistent pitch tracks that remain stable over repeated generations, while models using diffusion produce more dynamic results. This suggests that the diffusion-based structure captures higher expressiveness, resulting in a higher JSD compared to the FFT-based structure, which tends to average over dynamic emotions like happy and surprise.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "6) Comparison For Input Features:",
      "text": "The proposed model takes HuBERT features as input and outputs a Melspectrogram. The reason for outputting Mel-spectrogram is for compatibility with pre-trained vocoders. To explain the reasoning behind our choice of HuBERT features, we present a comparison of the input features. We compared Mel-spectrogram and linear spectrogram as features using conventional signal processing, and wav2vec 2.0  [31] , wavLM  [70] , and HuBERT  [33]  as SSL features that have recently been used as linguistic representations  [35] ,  [71] .\n\nAlthough the input feature dimensions differ in each experiment, they are aligned to the same hidden dimension through a linear layer.\n\nTable  IX  shows the results of the experiments by input feature. The results show that models using Mel-spectrogram and linear-spectrogram have lower UTMOS than SSL feature models. Models using wavLM performed better overall than those using wav2vec 2.0. The model using HuBERT outperformed the others in all metrics except ECA and EECS. In particular, the model using HuBERT shows a significant improvement in pronunciation, which is interpreted as advantageous for linguistic learning over other features because the target units used for training are obtained from the clustering of HuBERT features. To compare the linguistic modeling ability of each input feature, we calculated the BLEU score  [72]  and the unit error rate (UER) of the predicted units. Table  X  shows the BLEU score and UER of the predicted units for each feature.  We found that the prediction of units was correlated with the pronunciation accuracy of the synthesized speech, and this was due to the fact that HuBERT predicted units more accurately than the other features.\n\nWe also experimented with a model that takes unit input and generates speech. In Table  V , w/ unit2mel is a model that receives unit input and generates Mel-spectrogram. This model has the same structure, with the addition of a unit embedding layer to accept units as input. Furthermore, we experimented with models that generate waveforms directly from unit. w/ unit2wav is a modification of HiFi-GAN  [48]  to take unit as input, which is equivalent to not including f0 in Textless-EVC generator.\n\nThe w/ unit2mel shows a worse overall performance, except for ECA and EECS. The w/ unit2wav shows significantly worse metrics across the board, except for CER and WER. We interpret this to mean that the speech unit has enough information about pronunciation but not enough other speech information to generate a waveform. To overcome this, Textless-EVC uses additional information such as pitch and timbre to generate the waveform.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Unseen Speaker Emotion Conversion",
      "text": "We extended our experiments to apply our proposed model to an unseen speaker scenario. To make it possible, we modified the model structure to allow speaker information to encode speaker embedding from reference audio instead of speaker IDs. We adopt the style encoder structure from Meta-StyleSpeech  [73]  as the speaker encoder. We added a gradient reversal layer (GRL)  [74]  and a linear layer to prevent the speaker encoder from learning information about emotion. The linear layer performs the emotion classification task, where the losses are reversed by the GRL to prevent the speaker encoder from learning about emotion. Fig.  9  shows the speaker encoder and emotion embedding designed in this way to model style embedding. We set the weight for adversarial losses due to GRL to 0.001. We trained our model on the ESD dataset. For the unseen speaker test, we composed the test set by randomly selecting five sentences for each speaker from the VCTK dataset that were not used for training. We set the emotion of the test set to neutral and converted all other emotions. Table  XI  shows the evaluation results for seen and unseen speakers for the modified model. The modified model scored better UTMOS, PER, CER, and WER than the original version, while performing weaker on ECA, EECS, and SECS. We infer that the synthesis quality and pronunciation are better due to the additional information encoded from the reference audio that helps with speech synthesis. However, the lack of style disentanglement leads to decreased performance on emotion and speaker-related metrics. The results of the unseen speaker show that the modified structure allows for the emotion conversion of a new speaker without losing quality. Fig.  10  shows the pitch tracks of the transformed samples of the unseen speaker for each emotion, showing distinct differences for each emotion. We found that the speaker similarity for unseen speakers was lower compared to seen speakers. Table  XI  shows the speaker similarity for each emotion. We observe a decrease in the speaker similarity for all emotions. Fig.  11  shows a t-SNE visualization of speaker embedding for the ESD dataset (green), the VCTK dataset (purple), and the results of the unseen speaker experiment (yellow). The model synthesized speech that was closer to the speaker in the ESD, which was the training set.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vi. Discussion",
      "text": "Our model is influenced by the framework described in  [75] , which focuses on modeling emotional pronunciation. However, our approach differs substantially; whereas their model directly addresses pronunciation, our focus is on converting SSL features at the unit level. This perspective aligns more closely with the emotional translation mechanisms inherent in Textless-EVC  [27] . The primary distinction in our approach lies in the utilization of the cross-attention output as the input for our model, rather than relying on the predicted units.\n\nWe designed our model to generate a Mel-spectrogram. This was more efficient than generating the waveform directly, and allowed us to do more experimentation. For example, Textless-EVC, which generates waveforms directly, spent two weeks training, while our model required three days.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Limitations",
      "text": "In experimental results, the diffusion-based model demonstrated improved results. However, the limitations of diffusionbased structures include their extensive computational demand and time-consuming nature. We anticipate that this challenge will be alleviated by the advent of the recent fast sampling method  [76] . Some applications have been developed in speech research  [77] . The scope of our experiments also encompassed speaker generalization. Although we successfully observed emotional transformations in the voices of unseen speakers, a discernible lack of speaker similarity was apparent, indicating the need for further refinement. Empirically, our observations suggest that models tested on unseen data often reflect the voice distribution of the data used for their pretraining. Expanding the dataset is expected to increase the representational capacity of the generator, and the performance of zero-shot emotion conversion is also expected to improve.\n\nIn the configuration of our style autoencoder, we utilized MixLN for de-stylizing and CLN for stylizing. It was observed that the perturbations introduced by MixLN resulted in a compromise that affected the equilibrium between expressiveness and pronunciation accuracy within our model. In addition, the task of effectively separating style from content remains a significant challenge  [78] ,  [79] , requiring ongoing research and development to address this issue. Although the diffusion model produces high-quality speech, it has limitations due to emotion datasets typically having a 16k sampling rate. Audio super-resolution models, such as  [80] , are expected to solve this problem. Although we only experimented with English, our model has the potential to be extended by considering a wide range of languages and combining it with speech-to-speech translations  [81] . The expression of emotions differs between people, languages, and cultures, and research is needed to reflect these differences.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Future Works",
      "text": "Fundamentally, the proposed model depends on the performance of the SSL model because it utilizes discrete units. In addition to semantic units such as HuBERT, we also plan to investigate structures that exploit neural audio codecs such as  [38] ,  [39] . Recent research in the field of speech synthesis has increasingly focused on controlling emotion intensity. Some works, such as  [82]  and  [83] , have adopted the method of modeling emotion intensity using relative attribute ranking functions. Alternatively, some studies, such as  [84] , have explored the modeling of intensity through interpolation of embeddings. Various approaches, such as  [85] , have tried to control the intensity of emotions. Although these studies have shown that modeling emotion intensity is possible, precise control of intensity remains a challenge. In our future work, we will explore methods to control emotional intensity. We will also investigate emotional voice conversion for crosslanguage. Furthermore, it is expected to be applied to singing voice synthesis tasks  [86]  to express emotions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this work, we proposed DurFlex-EVC, a robust emotional voice conversion framework that supports flexible durations without relying on text or alignment. Exploiting discrete speech units derived from HuBERT, our model encodes content at the unit level and allows flexible duration control by extending unit-level duration prediction to the frame level. We developed a style autoencoder to disentangle emotional style from content, allowing precise manipulation of target styles. Additionally, we introduced a unit aligner to model emotional context at the unit level. A hierarchical stylize encoder further enhances emotional expressiveness by refining style application at both unit and frame levels. To improve synthesis quality, we incorporated a stochastic duration predictor and a diffusion-based generator, ensuring naturalness and diversity in generated speech. Through extensive experiments, DurFlex-EVC demonstrated superior performance over existing models, excelling in both emotional reproduction accuracy and speech quality. The framework effectively generalized to unseen speakers, preserving pronunciation and emotional clarity. These results demonstrate its potential for advancing emotional voice conversion, particularly in applications requiring duration flexibility and expressive emotional transformations.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall framework of the proposed method. The feature extractor transforms the source audio into input features. These features are subsequently",
      "page": 3
    },
    {
      "caption": "Figure 1: a illustrates the overall architecture of DurFlex-EVC.",
      "page": 3
    },
    {
      "caption": "Figure 1: b shows the structure of the style",
      "page": 4
    },
    {
      "caption": "Figure 1: f. The MixLN is defined",
      "page": 4
    },
    {
      "caption": "Figure 2: Unit-level pooling and frame-level scaling. (a) Latent is pooled",
      "page": 4
    },
    {
      "caption": "Figure 1: g. The CLN is defined as:",
      "page": 4
    },
    {
      "caption": "Figure 1: d, the unit aligner leverages a cross-",
      "page": 4
    },
    {
      "caption": "Figure 1: e shows the process of learning attention weights to",
      "page": 5
    },
    {
      "caption": "Figure 2: a explains unit-level pooling. The output of",
      "page": 5
    },
    {
      "caption": "Figure 3: Visualize t-SNE of emotion2vec features for speaker and emotion.",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of the SECS scores of the comparison models for all",
      "page": 7
    },
    {
      "caption": "Figure 5: Comparison of the EECS scores of the comparison models for all",
      "page": 7
    },
    {
      "caption": "Figure 3: is a visualization of the features in",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the resulting EECS for all combinations",
      "page": 8
    },
    {
      "caption": "Figure 6: Cross-entropy change across layers for emotion classification. Models",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the change in cross",
      "page": 9
    },
    {
      "caption": "Figure 7: Histogram of duration by emotion based on the structure of the",
      "page": 10
    },
    {
      "caption": "Figure 7: shows a histogram of",
      "page": 10
    },
    {
      "caption": "Figure 8: Pitch track of the same speech converted multiple times for each",
      "page": 10
    },
    {
      "caption": "Figure 9: Style embedding modeling for unseen speaker setting.",
      "page": 11
    },
    {
      "caption": "Figure 8: shows the pitch of the speech generated",
      "page": 11
    },
    {
      "caption": "Figure 10: Pitch track of unseen speaker speech converted to each emotion.",
      "page": 11
    },
    {
      "caption": "Figure 9: shows the speaker encoder",
      "page": 11
    },
    {
      "caption": "Figure 11: Visualization of t-SNEs of speaker embedding for the ESD dataset",
      "page": 12
    },
    {
      "caption": "Figure 10: shows the pitch tracks of the transformed samples of the",
      "page": 12
    },
    {
      "caption": "Figure 11: shows a t-SNE visualization of speaker embedding for",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Overallframework\nSource audio\nFeature\nExtractor\nùë§ ùë°ùëîùë° ùë§ ùë†ùëüùëê\nStyle\nAutoencoder\n.\nUnit Aligner\nunit-level pooling\nHierarchical\nStylize encoder\nGenerator\nMel-spectrogram": "",
          "(b) Style Autoencoder (f) De-stylize Transformer (d) Unit Aligner\nùë§ ùë†ùëüùëê ùëß Q ùë§ ùë†ùëüùëê ùëß De-stylize √óùëÅ Multi-Head ùëßùëñùëõ r a e n iL elacS x am tfo Attention weights\nùë§ Transformer Self-Attention ùë°ùëîùë° ùêæ r a e n S\niL ùëßùëúùë¢ùë°\nStylize √óùëÅ Add & MixLN raen\nTransformer ùëíùë¢ùëõùëñùë° ùëâiL : concatenate\nFFN\n(e) Duration Modeling Source audio\nAdd & MixLN\n(c) Hierarchical Stylize Encoder\nùë§ ùë°ùëîùë° ùëß ùë¢ HuBERT\nGround truth units K-means clustering (g) Stylize Transformer\nStylize √óùëÅ ùë§ ùëß 4 4 2 2 2 2 1 1\nTransformer Cross entropy loss ùë°ùëîùë°\nDP Multi-Head 0.1 0.0 0.2 0.1 0.0 0.2 0.2 0.1\n0.1 0.1 0.2 0.0 0.0 0.1 0.0 0.9\nSelf-Attention\n0.2 0.0 0.5 0.8 0.9 0.5 0.6 0.0\n[2,5,1] ‚Ñí‚Ñõ 0.1 0.1 0.0 0.0 0.1 0.1 0.1 0.0\nUnit duration Add & CLN 0.4 0.7 0.0 0.1 0.0 0.0 0.1 0.0\nAttention weights 0.1 0.1 0.1 0.0 0.0 0.1 0.0 0.0\nPredicted units\n4 4 2 2 2 2 2 1 FFN\nStylize √óùëÅ Deduplication\nTransformer [2,5,1] Add & CLN 4 2 1\nUnique units Unit duration": "",
          "(f) De-stylize Transformer\nùë§ ùëß\nùë†ùëüùëê\nMulti-Head\nSelf-Attention\nAdd & MixLN\nFFN\nAdd & MixLN": "(g) Stylize Transformer\nùë§ ùëß\nùë°ùëîùë°\nMulti-Head\nSelf-Attention\nAdd & CLN\nFFN\nAdd & CLN"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Stylize √óùëÅ\nTransformer": "DP"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.1": "0.9",
          "0.0": "0.0",
          "0.2": "0.0"
        },
        {
          "0.1": "0.0\n0.0",
          "0.0": "0.9\n0.1",
          "0.2": "0.6\n0.1"
        },
        {
          "0.1": "0.0\n0.0",
          "0.0": "0.0\n0.0",
          "0.2": "0.1\n0.0"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FFN": "Add & CLN"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.60",
          "0.63": "0.00",
          "0.62": "0.62",
          "0.59": "0.60"
        },
        {
          "0.00": "0.59",
          "0.63": "0.62",
          "0.62": "0.62",
          "0.59": "0.58"
        },
        {
          "0.00": "0.61",
          "0.63": "0.63",
          "0.62": "0.62",
          "0.59": "0.00"
        },
        {
          "0.00": "0.60",
          "0.63": "0.63",
          "0.62": "0.00",
          "0.59": "0.59"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.63",
          "0.62": "0.63",
          "0.63": "0.63"
        },
        {
          "0.00": "0.63",
          "0.62": "0.62",
          "0.63": "0.63"
        },
        {
          "0.00": "0.63",
          "0.62": "0.63",
          "0.63": "0.00"
        },
        {
          "0.00": "0.63",
          "0.62": "0.00",
          "0.63": "0.62"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.33",
          "0.34": "0.00",
          "0.32": "0.32",
          "0.21": "0.22",
          "0.45": "0.48"
        },
        {
          "0.00": "0.42",
          "0.34": "0.34",
          "0.32": "0.00",
          "0.21": "0.21",
          "0.45": "0.46"
        },
        {
          "0.00": "0.43",
          "0.34": "0.38",
          "0.32": "0.27",
          "0.21": "0.00",
          "0.45": "0.40"
        },
        {
          "0.00": "0.34",
          "0.34": "0.36",
          "0.32": "0.26",
          "0.21": "0.25",
          "0.45": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.14",
          "0.38": "0.00",
          "0.45": "0.45",
          "0.57": "0.56",
          "0.42": "0.43"
        },
        {
          "0.00": "0.14",
          "0.38": "0.39",
          "0.45": "0.00",
          "0.57": "0.55",
          "0.42": "0.41"
        },
        {
          "0.00": "0.16",
          "0.38": "0.38",
          "0.45": "0.40",
          "0.57": "0.00",
          "0.42": "0.43"
        },
        {
          "0.00": "0.18",
          "0.38": "0.41",
          "0.45": "0.40",
          "0.57": "0.62",
          "0.42": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.70",
          "0.69": "0.69",
          "0.68": "0.69"
        },
        {
          "0.00": "0.70",
          "0.69": "0.00",
          "0.68": "0.68"
        },
        {
          "0.00": "0.69",
          "0.69": "0.68",
          "0.68": "0.68"
        },
        {
          "0.00": "0.69",
          "0.69": "0.68",
          "0.68": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.68",
          "0.66": "0.66",
          "0.68": "0.68",
          "0.67": "0.67"
        },
        {
          "0.00": "0.67",
          "0.66": "0.65",
          "0.68": "0.00",
          "0.67": "0.66"
        },
        {
          "0.00": "0.68",
          "0.66": "0.00",
          "0.68": "0.67",
          "0.67": "0.67"
        },
        {
          "0.00": "0.67",
          "0.66": "0.66",
          "0.68": "0.67",
          "0.67": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.26",
          "0.43": "0.00",
          "0.63": "0.62",
          "0.64": "0.64",
          "0.44": "0.45"
        },
        {
          "0.00": "0.25",
          "0.43": "0.41",
          "0.63": "0.00",
          "0.64": "0.52",
          "0.44": "0.43"
        },
        {
          "0.00": "0.32",
          "0.43": "0.47",
          "0.63": "0.55",
          "0.64": "0.00",
          "0.44": "0.45"
        },
        {
          "0.00": "0.34",
          "0.43": "0.51",
          "0.63": "0.63",
          "0.64": "0.68",
          "0.44": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.36",
          "0.45": "0.00",
          "0.72": "0.73",
          "0.76": "0.77",
          "0.46": "0.47"
        },
        {
          "0.00": "0.32",
          "0.45": "0.47",
          "0.72": "0.00",
          "0.76": "0.77",
          "0.46": "0.43"
        },
        {
          "0.00": "0.38",
          "0.45": "0.44",
          "0.72": "0.69",
          "0.76": "0.00",
          "0.46": "0.44"
        },
        {
          "0.00": "0.37",
          "0.45": "0.49",
          "0.72": "0.69",
          "0.76": "0.81",
          "0.46": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.68",
          "0.68": "0.68",
          "0.69": "0.68",
          "0.67": "0.67"
        },
        {
          "0.00": "0.69",
          "0.68": "0.68",
          "0.69": "0.00",
          "0.67": "0.67"
        },
        {
          "0.00": "0.69",
          "0.68": "0.00",
          "0.69": "0.68",
          "0.67": "0.68"
        },
        {
          "0.00": "0.69",
          "0.68": "0.68",
          "0.69": "0.69",
          "0.67": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.76",
          "0.75": "0.00",
          "0.73": "0.73",
          "0.78": "0.78",
          "0.74": "0.75"
        },
        {
          "0.00": "0.75",
          "0.75": "0.75",
          "0.73": "0.00",
          "0.78": "0.78",
          "0.74": "0.75"
        },
        {
          "0.00": "0.76",
          "0.75": "0.75",
          "0.73": "0.74",
          "0.78": "0.00",
          "0.74": "0.74"
        },
        {
          "0.00": "0.75",
          "0.75": "0.75",
          "0.73": "0.73",
          "0.78": "0.78",
          "0.74": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.79",
          "0.50": "0.00",
          "0.41": "0.43",
          "0.67": "0.73",
          "0.42": "0.44"
        },
        {
          "0.00": "0.80",
          "0.50": "0.56",
          "0.41": "0.00",
          "0.67": "0.72",
          "0.42": "0.43"
        },
        {
          "0.00": "0.79",
          "0.50": "0.54",
          "0.41": "0.33",
          "0.67": "0.00",
          "0.42": "0.46"
        },
        {
          "0.00": "0.80",
          "0.50": "0.56",
          "0.41": "0.37",
          "0.67": "0.73",
          "0.42": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00": "0.93",
          "0.68": "0.00",
          "0.96": "0.96",
          "0.88": "0.87",
          "0.81": "0.82"
        },
        {
          "0.00": "0.91",
          "0.68": "0.70",
          "0.96": "0.00",
          "0.88": "0.88",
          "0.81": "0.77"
        },
        {
          "0.00": "0.89",
          "0.68": "0.66",
          "0.96": "0.94",
          "0.88": "0.00",
          "0.81": "0.79"
        },
        {
          "0.00": "0.93",
          "0.68": "0.70",
          "0.96": "0.94",
          "0.88": "0.87",
          "0.81": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: ) EffectivenessofUnitAligner: Theunitaligner(UA)was V under the entry labeled w/o HSE. The evaluation results",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "AE",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "Column_1": "",
          "Column_2": "DurFlex\nDurFlex",
          "Column_3": "-EVC\n-EVC w/o S",
          "Column_4": "AE",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
      "authors": [
        "A Triantafyllopoulos",
        "B Schuller",
        "G ƒ∞ymen",
        "M Sezgin",
        "X He",
        "Z Yang",
        "P Tzirakis",
        "S Liu",
        "S Mertes",
        "E Andr√©",
        "R Fu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "Training Socially Engaging Robots: Modeling Backchannel Behaviors with Batch Reinforcement Learning",
      "authors": [
        "N Hussain",
        "E Erzin",
        "T Sezgin",
        "Y Yemez"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "3",
      "title": "Speech Synthesis for the Generation of Artificial Personality",
      "authors": [
        "M Aylett",
        "A Vinciarelli",
        "M Wester"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Real-time deep neurolinguistic learning enhances noninvasive neural language decoding for brain-machine interaction",
      "authors": [
        "J.-H Jeong",
        "J.-H Cho",
        "B.-H Lee",
        "S.-W Lee"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Cybernetics"
    },
    {
      "citation_id": "6",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E Andr√©",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "Interdependencies among Voice Source Parameters in Emotional Speech",
      "authors": [
        "J Sundberg",
        "S Patel",
        "E Bjorkner",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "8",
      "title": "Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional Voice Conversion",
      "authors": [
        "H Ming",
        "D Huang",
        "L Xie",
        "J Wu",
        "M Dong",
        "H Li"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Expressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer",
      "authors": [
        "Z Du",
        "B Sisman",
        "K Zhou",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE Autom. Speech Recognit. Underst. Workshop"
    },
    {
      "citation_id": "10",
      "title": "GMM-based emotional voice conversion using spectrum and prosody features",
      "authors": [
        "R Aihara",
        "R Takashima",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2012",
      "venue": "American Journal of Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Nonparallel Emotional Speech Conversion",
      "authors": [
        "J Gao",
        "D Chakraborty",
        "H Tembine",
        "O Olaleye"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Vaw-Gan For Disentanglement And Recomposition Of Emotional Elements In Speech",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE Spok. Lang. Technol. Workshop"
    },
    {
      "citation_id": "14",
      "title": "Nonparallel Emotional Speech Conversion Using VAE-GAN",
      "authors": [
        "Y Cao",
        "Z Liu",
        "M Chen",
        "J Ma",
        "S Wang",
        "J Xiao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Stargan for Emotional Speech Conversion: Validated by Data Augmentation of End-To-End Emotion Recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Elliott",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "16",
      "title": "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "17",
      "title": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Imageto-Image Translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "Proc. IEEE/CVF Conf. Compt. Vis. Pattern Recognit"
    },
    {
      "citation_id": "18",
      "title": "CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training",
      "authors": [
        "J Bao",
        "D Chen",
        "F Wen",
        "H Li",
        "G Hua"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "19",
      "title": "Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion",
      "authors": [
        "C Robinson",
        "N Obin",
        "A Roebel"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process"
    },
    {
      "citation_id": "20",
      "title": "Emotional Voice Conversion Using Multitask Learning with Text-To-Speech",
      "authors": [
        "T.-H Kim",
        "S Cho",
        "S Choi",
        "S Park",
        "S.-Y Lee"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "21",
      "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Emotion Intensity and its Control for Emotional Voice Conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "23",
      "title": "Duration Controllable Voice Conversion via Phoneme-Based Information Bottleneck",
      "authors": [
        "S.-H Lee",
        "H.-R Noh",
        "W.-J Nam",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "24",
      "title": "FastSpeech: Fast, Robust and Controllable Text to Speech",
      "authors": [
        "Y Ren",
        "Y Ruan",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2019",
      "venue": "Proc"
    },
    {
      "citation_id": "25",
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
      "authors": [
        "Y Ren",
        "C Hu",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "26",
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "authors": [
        "A Polyak",
        "Y Adi",
        "J Copet",
        "E Kharitonov",
        "K Lakhotia",
        "W.-N Hsu",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Textless Speech Emotion Conversion using Discrete & Decomposed Representations",
      "authors": [
        "F Kreuk",
        "A Polyak",
        "J Copet",
        "E Kharitonov",
        "T Nguyen",
        "M Rivi√®re",
        "W.-N Hsu",
        "A Mohamed",
        "E Dupoux",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "Proc. Conf. Empir. Methods Nat. Lang. Process"
    },
    {
      "citation_id": "28",
      "title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion",
      "authors": [
        "B Van Niekerk",
        "M.-A Carbonneau",
        "J Za√Ødi",
        "M Baas",
        "H Seut√©",
        "H Kamper"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "29",
      "title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data",
      "authors": [
        "H Kim",
        "S Kim",
        "J Yeom",
        "S Yoon"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc"
    },
    {
      "citation_id": "31",
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "32",
      "title": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "XLS-R: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "33",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "34",
      "title": "ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers",
      "authors": [
        "K Qian",
        "Y Zhang",
        "H Gao",
        "J Ni",
        "C.-I Lai",
        "D Cox",
        "M Hasegawa-Johnson",
        "S Chang"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "35",
      "title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer",
      "authors": [
        "S.-H Lee",
        "H.-Y Choi",
        "H.-S Oh",
        "S.-W Lee"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification",
      "authors": [
        "Z Chen",
        "S Chen",
        "Y Wu",
        "Y Qian",
        "C Wang",
        "S Liu",
        "Y Qian",
        "M Zeng"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "37",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "38",
      "title": "SoundStream: An End-to-End Neural Audio Codec",
      "authors": [
        "N Zeghidour",
        "A Luebs",
        "A Omran",
        "J Skoglund",
        "M Tagliasacchi"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "39",
      "title": "High Fidelity Neural Audio Compression",
      "authors": [
        "A D√©fossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "year": "2023",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "40",
      "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "authors": [
        "D Yang",
        "J Tian",
        "X Tan",
        "R Huang",
        "S Liu",
        "X Chang",
        "J Shi",
        "S Zhao",
        "J Bian",
        "X Wu"
      ],
      "year": "2023",
      "venue": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "arxiv": "arXiv:2310.00704"
    },
    {
      "citation_id": "41",
      "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
      "authors": [
        "J Kim",
        "S Kim",
        "J Kong",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Proc"
    },
    {
      "citation_id": "42",
      "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
      "authors": [
        "V Popov",
        "I Vovk",
        "V Gogoryan",
        "T Sadekova",
        "M Kudinov"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "43",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "M Mcauliffe",
        "M Socolof",
        "S Mihuc",
        "M Wagner",
        "M Sonderegger"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "44",
      "title": "Stargan-vc: nonparallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "H Kameoka",
        "T Kaneko",
        "K Tanaka",
        "N Hojo"
      ],
      "year": "2018",
      "venue": "IEEE Spok. Lang. Technol. Workshop"
    },
    {
      "citation_id": "45",
      "title": "AutoVC: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "K Qian",
        "Y Zhang",
        "S Chang",
        "X Yang",
        "M Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "46",
      "title": "ATTS2S-VC: Sequence-to-sequence Voice Conversion with Attention and Context Preservation Mechanisms",
      "authors": [
        "K Tanaka",
        "H Kameoka",
        "T Kaneko",
        "N Hojo"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process"
    },
    {
      "citation_id": "47",
      "title": "Nonautoregressive sequence-to-sequence voice conversion",
      "authors": [
        "T Hayashi",
        "W.-C Huang",
        "K Kobayashi",
        "T Toda"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "48",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Proc"
    },
    {
      "citation_id": "49",
      "title": "Waveglow: A flow-based generative network for speech synthesis",
      "authors": [
        "R Prenger",
        "R Valle",
        "B Catanzaro"
      ],
      "year": "2019",
      "venue": "Proc"
    },
    {
      "citation_id": "50",
      "title": "Diffwave: A versatile diffusion model for audio synthesis",
      "authors": [
        "Z Kong",
        "W Ping",
        "J Huang",
        "K Zhao",
        "B Catanzaro"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "51",
      "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
      "authors": [
        "J Shen",
        "R Pang",
        "R Weiss",
        "M Schuster",
        "N Jaitly",
        "Z Yang",
        "Z Chen",
        "Y Zhang",
        "Y Wang",
        "R Skerrv-Ryan",
        "R Saurous",
        "Y Agiomvrgiannakis",
        "Y Wu"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process"
    },
    {
      "citation_id": "52",
      "title": "Neural Speech Synthesis with Transformer Network",
      "authors": [
        "N Li",
        "S Liu",
        "Y Liu",
        "S Zhao",
        "M Liu"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "53",
      "title": "Rhythm-Flexible Voice Conversion Without Parallel Data Using Cycle-GAN Over Phoneme Posteriorgram Sequences",
      "authors": [
        "P.-C Yeh",
        "J.-C Hsu",
        "H Chou",
        "L.-S -Y. Lee",
        "Lee"
      ],
      "year": "2018",
      "venue": "IEEE Spok. Lang. Technol. Workshop"
    },
    {
      "citation_id": "54",
      "title": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion",
      "authors": [
        "Z Yang",
        "X Jing",
        "A Triantafyllopoulos",
        "M Song",
        "I Aslan",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "55",
      "title": "GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech",
      "authors": [
        "R Huang",
        "Y Ren",
        "J Liu",
        "C Cui",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Proc"
    },
    {
      "citation_id": "56",
      "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice",
      "authors": [
        "M Chen",
        "X Tan",
        "B Li",
        "Y Liu",
        "T Qin",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "57",
      "title": "Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis with Diffusion Models",
      "authors": [
        "M Kang",
        "D Min",
        "S Hwang"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "58",
      "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
      "authors": [
        "J Kim",
        "J Kong",
        "J Son"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "59",
      "title": "Emotional voice conversion: Theory, databases and ESD",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "60",
      "title": "BigV-GAN: A Universal Neural Vocoder with Large-Scale Training",
      "authors": [
        "S Lee",
        "W Ping",
        "B Ginsburg",
        "B Catanzaro",
        "S Yoon"
      ],
      "year": "2023",
      "venue": "The Eleventh Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "61",
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Textto-Speech",
      "authors": [
        "H Zen",
        "V Dang",
        "R Clark",
        "Y Zhang",
        "R Weiss",
        "Y Jia",
        "Z Chen",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "62",
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "authors": [
        "T Saeki",
        "D Xin",
        "W Nakata",
        "T Koriyama",
        "S Takamichi",
        "H Saruwatari"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "63",
      "title": "Simple and Effective Zero-shot Crosslingual Phoneme Recognition",
      "authors": [
        "Q Xu",
        "A Baevski",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "64",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "65",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Proc. ACL Findings"
    },
    {
      "citation_id": "66",
      "title": "Speech Synthesis with Mixed Emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "67",
      "title": "Adversarially learning disentangled speech representations for robust multi-factor voice conversion",
      "authors": [
        "J Wang",
        "J Li",
        "X Zhao",
        "Z Wu",
        "S Kang",
        "H Meng"
      ],
      "venue": "Adversarially learning disentangled speech representations for robust multi-factor voice conversion"
    },
    {
      "citation_id": "68",
      "title": "Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "X Wang",
        "Q Xie",
        "Z Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "69",
      "title": "iemotts: Toward robust cross-speaker emotion transfer and control for speech synthesis based on disentanglement between prosody and timbre",
      "authors": [
        "G Zhang",
        "Y Qin",
        "W Zhang",
        "J Wu",
        "M Li",
        "Y Gai",
        "F Jiang",
        "T Lee"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "70",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process"
    },
    {
      "citation_id": "71",
      "title": "HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis",
      "authors": [
        "S.-H Lee",
        "S.-B Kim",
        "J.-H Lee",
        "E Song",
        "M.-J Hwang",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "Proc"
    },
    {
      "citation_id": "72",
      "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ser. ACL '02"
    },
    {
      "citation_id": "73",
      "title": "Meta-stylespeech : Multi-speaker adaptive text-to-speech generation",
      "authors": [
        "D Min",
        "D Lee",
        "E Yang",
        "S Hwang"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "74",
      "title": "Unsupervised Domain Adaptation by Backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "Proc. Int. Conf. on Mach. Learn"
    },
    {
      "citation_id": "75",
      "title": "Can We Generate Emotional Pronunciations for Expressive Speech Synthesis?",
      "authors": [
        "M Tahon",
        "G Lecorv√©",
        "D Lolive"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "76",
      "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
      "authors": [
        "Z Xiao",
        "K Kreis",
        "A Vahdat"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf. Learn. Repr"
    },
    {
      "citation_id": "77",
      "title": "Diffprosody: Diffusion-based latent prosody generation for expressive speech synthesis with prosody conditional adversarial training",
      "authors": [
        "H.-S Oh",
        "S.-H Lee",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "78",
      "title": "Multispectrogan: High-diversity and high-fidelity spectrogram generation with adversarial style combination for speech synthesis",
      "authors": [
        "S.-H Lee",
        "H.-W Yoon",
        "H.-R Noh",
        "J.-H Kim",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "79",
      "title": "Voicemixer: Adversarial voice style mixup",
      "authors": [
        "S.-H Lee",
        "J.-H Kim",
        "H Chung",
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "Proc"
    },
    {
      "citation_id": "80",
      "title": "Audio superresolution with robust speech representation learning of masked autoencoder",
      "authors": [
        "S.-B Kim",
        "S.-H Lee",
        "H.-Y Choi",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "81",
      "title": "Transentence: speech-to-speech translation via language-agnostic sentence-level speech encoding without language-parallel data",
      "authors": [
        "S.-B Kim",
        "S.-H Lee",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process"
    },
    {
      "citation_id": "82",
      "title": "Controlling Emotion Strength with Relative Attribute for End-to-End Speech Synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "IEEE Autom. Speech Recognit. Underst. Workshop"
    },
    {
      "citation_id": "83",
      "title": "Fine-Grained Emotion Strength Transfer, Control and Prediction for Emotional Speech Synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "IEEE Spok. Lang. Technol. Workshop"
    },
    {
      "citation_id": "84",
      "title": "Emotional Speech Synthesis with Rich and Granularized Control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "85",
      "title": "EMOQ-TTS: Emotion Intensity Quantization for Fine-Grained Controllable Emotional Text-to-Speech",
      "authors": [
        "C.-B Im",
        "S.-H Lee",
        "S.-B Kim",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process."
    },
    {
      "citation_id": "86",
      "title": "Hyung-Seok Oh received the B.S. degree in Computer Science and Engineering from Konkuk University, Seoul, South Korea, in 2021. He is currently working toward an integrated master's and Ph",
      "authors": [
        "D.-M Byun",
        "S.-H Lee",
        "J.-S Hwang",
        "S.-W Lee"
      ],
      "year": "2024",
      "venue": "IEEE Int. Conf. Acoust., Speech, Signal Process"
    },
    {
      "citation_id": "87",
      "title": "His current research interests include artificial intelligence and audio signal processing. Deok-Hyeon Cho received the B.S. degree in Applied Mathematics from Hanyang University ER-ICA Campus, Ansan, South Korea, in 2022. He is currently working toward an integrated master's and Ph",
      "year": "2016",
      "venue": "His research interests include artificial intelligence and audio signal processing"
    },
    {
      "citation_id": "88",
      "title": "His current research interests include artificial intelligence, pattern recognition, and brain engineering",
      "authors": [
        "Seong-Whan Lee"
      ],
      "year": "1986",
      "venue": "1984, and the M.S. and Ph"
    }
  ]
}