{
  "paper_id": "2109.04316v1",
  "title": "Accounting For Variations In Speech Emotion Recognition With Nonparametric Hierarchical Neural Network",
  "published": "2021-09-09T14:46:46Z",
  "authors": [
    "Lance Ying",
    "Amrit Romana",
    "Emily Mower Provost"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Bayesian Nonparametric Method",
    "Clustering"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, deep-learning-based speech emotion recognition models have outperformed classical machine learning models. Previously, some neural network designs, such as Multitask Learning, have accounted for variations in emotional expressions due to demographic and contextual factors. However, existing models face a few constraints: 1) they rely on a clear definition of domains (e.g. gender, culture, noise condition, etc.) and the availability of domain labels. 2) they often attempt to learn domain-invariant features while emotion expressions can be domain-specific. In the present study, we propose the Nonparametric Hierarchical Neural Network (NHNN), a lightweight hierarchical neural network model based on Bayesian nonparametric clustering. In comparison to Multitask Learning approaches, the proposed model does not require domain/task labels. In our experiments, the NHNN models outperform the models with similar levels of complexity and state-of-the-art models in within-corpus and crosscorpus tests. Through clustering analysis, we show that the NHNN models are able to learn group-specific features and bridge the performance gap between groups.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion can greatly influence people's mental processes and behaviors and is also a predictor of physical and psychological well being. Speech Emotion Recognition (SER) has become an important research topic in human-computer interaction.\n\nAlthough considerable research effort has been put into developing models capable of recognizing and predicting human emotions, much of the work focuses on lab-produced datasets  [3]  and robust speech emotion recognition on in-thewild speech with diverse confounding acoustic elements remains a challenge  [32] . Previous work has used a variety of machine learning techniques such as multi-task learning  [32]  and domain generalization  [31]  that attempt to learn a more robust and generalizable representation of speech features, yet few studies have successfully accounted for variations in speech emotion corpora due to the complexity of emotion expressions and a variety of moderating variables such as gender, culture, etc.\n\nIn this paper, we propose a novel lightweight architecture based on unsupervised non-parametric clustering and supervised convolutional neural networks (CNN) in order to account for inter-group differences in emotional expressions. We test the model on both lab-produced and in-the-wild speech emotion datasets and show our proposed model outperforms stateof-the-art methods on most within-corpus and cross-corpus tests. Our work has the potential to achieve robust SER performances in groups with a variety of emotional expressions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Speech Emotion Recognition",
      "text": "Speech Emotion Recognition (SER) has attracted enormous interest in the past decades. Past research has experimented with different machine learning algorithms, such as Hidden Markov Models (HMM)  [16] , Artificial Neural Networks (ANN)  [4] , Support Vector Machines (SVM)  [10] . In recent years, deep learning approaches have become more popular and have reached state-of-the-art performances on SER  [9] . For instance, Zhao et al. proposed the CNN-LSTM model, which showed better accuracy than conventional CNN and LSTM models  [33] . Liu et al. proposed a temporal attention CNN model, which shows stable and state-of-the-art performances on SER tasks  [15]  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustic Features And Variations",
      "text": "A considerable amount of research from the speech signal processing community has explored relevant acoustic features for SER  [27] . Some common features used for classifications often include prosody features, such as pitch and loudness, voice quality features, such as the first three formants and the signal-to-noise ratio, and spectral features, such as the Melfrequency Cepstral Coefficients (MFCC) and Linear Prediction Cepstral Coefficients (LPCC).\n\nHowever, robustly extracting emotion from these acoustic features can be quite challenging. The relationship between experienced emotions and observed acoustic features can be moderated by multiple contextual and demographic factors. These variations create challenges for robust automatic speech recognition systems. One source of variations is environmental conditions. In-the-wild speech is often recorded with different recording devices, producing different sound qualities. A variety of background noise, such as other human speech and music, can also introduce noise into the extracted acoustic features. Additionally, researchers have shown that the experience of emotion and its expressions are often influenced by biological, social, and cultural factors. Kring and Gordon found that, when presented with emotional stimuli, female participants were more expressive than male participants, even though they reported the same type and intensity of experienced emotions  [13] . Matsumoto et al. found that cultural display rules differ systematically between Japan and the United States  [17] . Gender differences have also been well documented in SER applications. For instance, Alghowinem et al. found that best features for depression detection from speech differed across genders  [26] .\n\nDespite advances in speech emotion recognition systems, few model designs have properly accounted for these factors. One popular approach is to partition audio segments to be part of a certain domain, such as gender, corpus, noise condition, etc., and apply domain adaptation  [30]  or multi-task learning during training  [32] . Some have trained separate models on each domain and then applied model fusion in a later stage  [24] . However, one key assumption of such approaches is that there exist pre-specified domains for audio. For human speech, the domains can be non-exhaustive. For instance, gender, race, language, age, environmental conditions are all possible criteria for categorization, yet a combination of these criteria would generate an exponential amount of permutations with multiple labels (e.g. Male, Caucasian, adult, etc.). Training separate models for each permutation or applying domain adaptation among such a large number of domains are simply impractical.\n\nIn addition, the goal of domain adaptation approaches is to find some shared representation that exists across domains. However, some acoustic features may be domain-specific. Past research has shown that negative transfer can occur where transfer methods decrease performances  [28] .\n\nLast but not least, working with pre-specified domains assumes sufficient information about the corpus. However, when training on in-the-wild speech or real-world speech recordings, researchers often don't have explicit and perfect knowledge on subjects' demographics or speech recording conditions, and many of these labels may be incomplete.\n\nIn the present study, we introduce the Bayesian Nonparametric method (BNP) as a way to account for acoustic variations. Our proposed method offers a more flexible alternative than Multi-task Learning or domain adaptation as it captures the latent structure of the dataset through unsupervised means and does not rely on any additional labels except ground-truth emotion categories. The number of domains can grow as new data becomes available. The Bayesian nonparametric (BNP) method was first proposed by Ferguson  [8] , who introduced the Dirichlet process, denoted as DP(ğ›¼ 0 , ğº 0 ), where ğ›¼ 0 is the scaling parameter, and ğº 0 is the probability measure. An explicit formulation was provided by Sethuraman  [25]  with the famous \"stick-breaking construction\", which uses an independent sequence of i.i.d. random variables ğœ· = (ğ›½ 1 , ğ›½ 2 , ...) and components ğ“ = (ğœ™ 1 , ğœ™ 2 , ...) where",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Bayesian Nonparametric Method",
      "text": "Then we can define a discrete distribution ğº as\n\nwhere ğ›¿ ğœ™ ğ‘˜ is a probability measure concentrated at ğœ™ ğ‘˜ . One of the most popular application of the Dirichlet process is to serve as a nonparametric prior for the latent mixing measure in a mixture model, which can be expressed as follows:\n\nwhere ğ‘¥ ğ‘– is the observation, and ğ¹ (ğœƒ ğ‘– ) denotes the distribution of ğ‘¥ ğ‘– given ğœƒ ğ‘– . This is referred as the Dirichlet Process Mixture Model (DPMM). Since ğº can be represented with the stickbreaking construction process defined earlier, we can express ğœƒ ğ‘– with atoms ğœ™ ğ‘˜ . This results in an alternative representation:\n\nBased on this formulation, Antoniak  [1]  shows that the likelihood of the number of mixtures ğ‘˜, given ğ‘› observations, is calculated to be\n\nwhere ğ‘§ ğ‘›ğ‘˜ is the unsigned Stirling number of the first kind. The expected number of mixtures is\n\nIn comparison to a standard Gaussian Mixture Model (GMM), the DPMM is able to infer the number of components from the data where in a GMM, the number of components is a predetermined hyperparameter. DPMM is preferred in our study because the groups with variations in acoustic emotion expression are latent. The DPMM can account for such latent structures within the acoustic data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Inference Of Dpmm.",
      "text": "Given observations, the inference of the Dirichlet Process Mixture Model often uses one of the two methods: Markov Chain Monte Carlo  [20]  and Variational Inference  [2] . In our experiments, the inference of the mixtures is estimated by BNP packages in sklearn  [22] , which uses the Variational Inference method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Applications Of Bnp.",
      "text": "Due to the variable and complex nature of human behaviors, BNP has been commonly applied to model human activities. Navarro et al. introduced a framework with the Dirichlet Process to model individual differences in category learning, publication habits of psychologists, and web browsing activities  [19] . Chen et al. applied the Dirichlet Process Mixture Model to learn Pedestrian Motion Patterns  [5] . These past research showed that BNP was a powerful tool that could be capable of capturing the similarities and differences among emotional domains. Some previous work has incorporated BNP in SER. For instance, Wang et al. used a hierarchical Dirichlet Process mixture model on music emotion labeling and showed better performance on music emotion annotation and retrieval than GMM  [29] . However, the overall application of BNP in SER is quite limited and none has attempted using BNP in modeling individual/group differences in corpora.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Model",
      "text": "In this section, we present the baseline models and the proposed Nonparametric Hierarchical Neural Network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Models",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dilated Convolutional Neural Network (Dcnn).",
      "text": "Convolutional Neural Network(CNN) is one of the most popular deep learning methods. In many applications, CNN has well surpassed the performances of classical machine learning methods because of its ability to construct rich representations from data  [11] .\n\nThe CNN model consists of a feature encoder, which is a stack of convolutional and maxpool layers, and a classifier, which is a stack of fully-connected layers and a softmax layer. For classifications, the classifier generates an n-class probability distribution.\n\nIn the present study, we use a Dilated Convolutional Neural Network (DCNN), which uses two 1D dilated convolutional layers as the feature encoder for inputs. In comparison to a CNN, the dilation in the DCNN allows for a more inclusive receptive field, which is commonly used in SER  [11, 14] . We use ReLU as the activation function, except the softmax at the end. The channel size for the convolutional layers and fully-connected layers are both 128, as used in  [11] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multitask Cnn (Mtl-Cnn).",
      "text": "Multitask Learning attempts to learn models that perform well on multiple tasks simultaneously. Through this paradigm, models can learn shared representations for multiple tasks with better generalizability. In a simple Multitask CNN, different tasks share the same feature encoder while having task-unique classification layers. We include Multitask CNN as a baseline because it also attempts to account for variations in speech domains and its architecture have a similar amount of parameters and training time as our proposed methods. In the present study, we follow existing work  [21, 32]  and use gender classification as an auxiliary task in addition the main task of valence classification.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cnn With",
      "text": "Long Short-Term Memory (CNN-LSTM). Zhao et al.  [33]  proposed a CNN-LSTM model which achieved stateof-the-art performance on SER tasks. In this paper, we use the same 1D CNN-LSTM model, which consists of 4 convolutional layers, each followed by a max pooling and dropout layer, 1 LSTM layer and a fully-connected layer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Nonparametric Hierarchical Neural Network",
      "text": "The NHNN model (Fig.  1 ) is a hierarchical model that consists of a shared feature encoder and domain-specific classifiers. The feature encoder aims to learn a shared representation for all domains while the domain-specific classifier learns a domainspecific mapping from the bottleneck features to outputs.\n\nOnce the model is trained, the model performs the classification task as follows.Suppose the classification problem has ğ‘› classes and assume ğ‘˜ mixture components are inferred from a Gaussian DPMM with parameters ğ, ğˆ and cluster weights ğ. Denote the set of mixtures as ğ“ = (ğœ™ 1 , ğœ™ 2 , ..., ğœ™ ğ‘˜ ), the observation as ğ‘¥, the class of the observation as ğ‘§, and the latent discrete allocation variable as ğœƒ . Each of the classifiers produce a probability distribution ğœ‹ (ğœ™ ğ‘– ) = (ğœ‹ 1 (ğœ™ ğ‘– ), ğœ‹ 2 (ğœ™ ğ‘– )...ğœ‹ ğ‘› (ğœ™ ğ‘– )) from the softmax layer, where ğœ‹ ğ‘— (ğœ™ ğ‘– ) denotes the j-th output from the softmax probability distribution associated with the cluster ğœ™ ğ‘– . Here we can equivalently express this as a conditional probability ğœ‹ ğ‘— (ğœ™ ğ‘– ) = ğ‘ƒ (ğ‘§ = ğ‘— |ğœƒ = ğ‘–). Then the weighted conditional probability is,\n\nSince each cluster is a Gaussian, we can substitute the conditional probability for the latent variable.\n\nThen the predicted label ğœ of observation ğ‘¥ is essentially\n\nwhere ğ‘› ğ‘—=1 ğœ‹ ğ‘— (ğœ™ ğ‘– ) = 1. Through its hierarchical structure, NHNN is able to extract common features across domains while preserving domainspecific characteristics. The structure can be adapted to a variety of neural networks such as CNN, RNN, and DNN.\n\nIn the present study, we use a CNN-based NHNN for SER. The pipeline of the model is shown in Fig 2 . The clustering of the audio is based on eGeMAPS feature vectors and the MFB features are passed into the feature encoder and emotion classifier. Both eGeMAPS and MFB features are commonly used in SER work and will be introduced in detail in Section 4.5. We use the feature encoder from the DCNN model to construct a shared bottleneck representation.\n\nIn our preliminary clustering analysis, we find that, in each dataset, the clustering yields two major components that account for over 90% of the utterances. Clusters with little data (< 10% of the corpus) are therefore removed to ensure each cluster has sufficient data to fit the neural network. In experiments, the data are reassigned to existing cluster.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Complexity",
      "text": "The DCNN model has the least number of parameters (n_parameters =175,619). The MTL-CNN model and the proposed NHNN FC model have a similar amount of parameters (n_parameters",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We select three datasets in the present study: IEMOCAP  [3] , PRIORI Emotion  [12] , and PRIORI R21 . All three datasets are in English. IEMOCAP is a popular dataset produced in a lab, where both PRIORI Emotion and PRIORI R21 are in-the-wild datasets, which include phone calls with spontaneous speech and various background noises. We choose the three datasets so that the proposed model can be tested on both lab-recorded and in-the-wild datasets. Each dataset will be introduced in detail below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iemocap",
      "text": "The Interactive Emotional Dyadic MOtion Capture Database (IEMOCAP) is a common lab-recorded multimodal dataset. Ten actors (five male and five female) performed a series of scripts or improvisational scenarios recorded over five sessions. The audios are recorded at a 48 kHz sampling rate and then downsampled to 16 kHz. The IEMOCAP dataset includes 10,039 audio segments (5,255 scripted utterances and 4,784 improvised utterances). The segments are annotated by two to four annotators on valence, activation, and dominance on a 5-point Likert Scale.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Priori Emotion",
      "text": "The",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Labels",
      "text": "SER work often uses one of two emotion labels: categorical and dimensional labels. The categorical labels include emotions such as happy, sad, angry, etc. where dimensional labels typically include valence and activation. Valence refers to the perceived pleasantness of the speech and activation refers to the degree of arousal. Valence and activation are often scored on a Likert scale, which maps each utterance onto a 2-D valence-activation space.\n\nAlthough much of the work on IEMOCAP uses categorical labels in classification tasks, in the present study, however, we choose to use dimensional valence scores because the PRIORI datasets are only annotated on dimensional scores. Previous research has also shown that dimensional labels are more consistently interpretable across datasets  [9, 23] .\n\nWe follow the same preprocessing procedure as Gideon et al.  [9]  by converting each annotation of valence into three bins. The middle bin corresponds to medium valence with a rating of 3 for IEMOCAP and a rating of 5 for PRIORI Emotion and PRIORI R21. The other two bins correspond to valence ratings below and above the middle points. We then create a final label for an utterance by aggregating over all evaluations for that utterance and identifying the most commonly selected (majority) bin. We exclude all utterances without a majority",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction And Preprocessing",
      "text": "All audio segments from the datasets are normalized to 0 dBFS using the Sox command line tool. We extract two sets of features for our model: eGeMAPS and MFB. eGeMAPS -The eGeMAPS feature set is a commonly used feature set designed for SER. It includes 88 acoustic features such as energy, excitation, spectral, cepstral, etc. In the present study, the eGeMAPS feature vectors are extracted by the openS-MILE toolkit with default parameters  [6, 7] .\n\nMFB -We extract 40-dimensional MFB features with 25ms frame length and 10ms frame shift using the Librosa package  [18] . The MFB features are then z-normalized and padded to the same length during training to account for variations in audio lengths.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We designed two experiments to test the within-corpus and cross-corpus performance of NHNN against the baseline. In the present study, we train and test two variants of NHNN by freezing different layers of the DCNN model. The NHNN FC variant freezes all the convolutional layers and adapts the fully-connected layers to each cluster. The NHNN FC+Conv variant freezes one out of the two convolutional layers of the feature encoder.\n\nIn both experiments, the classifiers are trained on a single corpus. We randomly take out 1/4 of the training set to be the validation set. We implement both experiments with Pytorch. We use early stopping with a patience of 5 epochs and a maximum epoch of 50. The model with the best validation loss is selected. Training and testing is repeated with different random seeds. In both experiments, hyperparameters including batch size(32/64) and learning rate(0.001/0.0005/0.0001) are tuned for all models. We choose a batch size of 64 and a learning rate of 0.0001 for the final implementation.\n\nFor performance evaluation, we use the Unweighted Average Recall (UAR) as the performance metric. UAR ensures that each valence class is given equal weight at performance evaluation to account for possible data imbalance. A random prediction would result in a UAR of 0.333 in our experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 1",
      "text": "In Experiment 1, we investigate the performance of NHNN on a single corpus. We train and test classifiers within-corpus for each of the three datasets. For each dataset, we use Leave-One-Subject-Out (LOSO) testing scheme and the performances are averaged over the number of subjects. We repeat the experiments with different random seeds and the final results are averaged. Since each model is tested with the same testing scheme, to evaluate the significance of performance difference, we use a paired t-test between results of the models models, where the UAR performance on each subject is paired from the LOSO testing results.\n\nWe also break down the UAR performances by groups including gender, language, and subject to compare the performances. We relate any performance discrepancies to differences between clusters to see if the clustering is improving the model performances on subsets/groups within the dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 2",
      "text": "In Experiment 2, we investigate the cross-corpus performances by training classifiers on one dataset and testing their crosscorpus performances on each of the other two datasets. Since the PRIORI datasets and IEMOCAP are sampled with different rates, the IEMOCAP dataset is downsampled to 8 kHz in Experiment 2. Each model is trained 30 times and the results are averaged for comparison. Similar to experiment 1, we perform a paired t-test to compare the results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 1",
      "text": "The results of Experiment 1 are summarized in Table  2 . The two proposed NHNN models achieve the best UAR performances in all three datasets when compared with other models. Between the two variants, the NHNN FC+Conv model shows superior UAR on all three datasets, compared to the NHNN FC variant. The Multi-task CNN model, when treating gender recognition as the auxiliary task, shows similar performance as the DCNN model. The CNN-LSTM model has a similar performance as the two NHNN models on IEMOCAP and PRIORI Emotion, where the differences are not statistically significant, but the CNN-LSTM performed poorly on PRIORI R21.\n\nIn the R21 dataset, the DCNN baseline model performance has a higher UAR for native English speakers (UAR=0.4017) than non-native speakers (UAR=0.3856) with statistical significance (p=0.0055). In NHNN FC, the performance gap decreases slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it increases to 2.05%. However, none of the changes in performance gaps is statistically significant.\n\nThe comparison analysis between the two gender groups shows that the performance gaps between male and female speech in PRIORI Emotion and PRIORI R21 shrink under our proposed models. In PRIORI Emotion, the UAR performance on female speech (UAR=0.4731) is 3.73% higher than on male speech (UAR=0.4358). The gender-group UAR difference shrinks to 3.42% under NHNN FC and 3.26% under NHNN FC+Conv. In PRIORI R21, the UAR performance on male speech (UAR=0.3990) is 1.61% higher than on female speech (UAR=0.3876). The gender-group UAR difference shrinks to 0.38% under NHNN FC and -0.54% under NHNN FC+Conv. No significant changes in gender-group UAR differences are observed for IEMOCAP when comparing two NHNN models and the DCNN model.\n\nTable  3  shows the group-level performances improvement between female and male subjects against the DCNN model. In PRIORI Emotion and PRIORI R21, both variants of NHNN showed statistically significant performance boost on one of the gender groups but not the other, whereas the performance changes for the two gender groups is similar and both statistically significant on IEMOCAP.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Clustering Analysis",
      "text": "The two mixture components have similar weights across three datasets, which roughly follow an 80/20 split. We first relate the clustering results to attributes of the audio segments. In particular, we look at the gender ratio, language ratio, subject audio distribution, and mood severity in each mixture. In the R21 dataset, we also analyze the proportion of audio segments from native English speakers in the mixtures. As gender and native-language are binary variables in our study, we simply compare the ratio of the two attributes in a dataset/mixture. The gender ratio is defined as the number of female audio segments divided by the number of male audio segments. The language ratio is calculated by dividing the number of audio segments from native English speakers from those from nonnative speakers. As for subjects, there are great variations in the number of audio segments belonged to each subject in each dataset while the mixtures are of different sizes. To analyze the distributions of subject_id, we divide the maximum number of audio segments belonged to a single subject by the minimum number as a surrogate of the dispersion of subject audio segments in a dataset/mixture. If the clustering is not based on a certain attribute, we would expect the relevant ratios to be similar in cluster 1, cluster 2, and the dataset as a whole.\n\nIn the R21 dataset, the clusters seem to be indicative of subjects' demographics, where gender ratio difference and native-language language difference are observed between the two clusters. In cluster 1, the Female/Male ratio is 0.65, and 1.53 in cluster 2. The English/Arabic as first language ratio is 2.24 in the first cluster and 0.95 in the second cluster.\n\nIn the PRIORI Emotion dataset, the gender ratios are 1.96 and 0.69, respectively. Significant variations in subject_id distributions are found in the two clusters. In the PRIORI Emotion dataset, the max/min subject audio segments ratio is 2.77 in cluster 1 and 9.00 in cluster 2, while the ratio is 1.56 in the V1 dataset as a whole.\n\nAs for IEMOCAP, the gender ratio follows an even split in each mixture and the dataset as a whole. We also did not observe any significant variations in the number of subject audio segments among clusters. It's likely that the clustering of IEMOCAP depends more on lower-level acoustic features than subject group differences.\n\nThe valence rating distributions are similar in all mixture components in each of the datasets.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiment 2",
      "text": "Table  4  shows the results of Experiment 2. The two NHNN models have better average performances than other models. In particular, NHNN FC+Conv outperforms DCNN and MTL-CNN on all cross-corpus tests, and CNN-LSTM when trained on IEMOCAP and PRIORI Emotion. The CNN-LSTM model performs relatively poorly on IEMOCAP and PRIORI Emotion against other models but has the best UAR performance when trained on PRIORI R21 and tested on the other two datasets. The differences are not statistically significant between CNN-LSTM and NHNN FC+Conv when trained on PRIORI R21 (p=0.1401 when tested on IEMOCAP and p=0.0986 when tested on PRIORI Emotion). Between the two variants, NHNN FC+Conv has on average better cross-corpus performances than NHNN FC model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "In the present study, we introduce the Nonparametric Hierarchical Neural Network, which learns latent structures from the dataset with a Dirichlet Process Mixture Model to build domain-specific classifiers from a shared feature encoder.\n\nThe two experiments we conducted show that NHNN, despite being a lightweight model, has superior within-corpus performance than DCNN and MTL-CNN. Its cross-corpus test performance is on average better than DCNN, MTL-CNN and CNN-LSTM, which indicates a better generalizability on unseen data. The performance increase over the DCNN, which uses the same feature encoder as the NHNN model, is observed for both lab-produced and in-the-wild datasets. In addition, NHNN FC+Conv shows a slightly better average performance than NHNN FC in both experiments. This is likely because NHNN FC+Conv allows for a more versatile classifier with more free parameters.\n\nComparing results across two experiments, we notice that the model trained on PRIORI Emotion and tested on PRIORI R21 has a significantly better performance than within-corpus UAR of PRIORI R21. This suggests that the low R21 withincorpus performance relative to PRIORI Emotion is likely due to a lack of training samples.\n\nIn the cluster analysis, we show that the clustering criteria vary depending on the dataset, which may be a combination of higher-level attributes or lower-level acoustic features. subsequent analysis also confirms that the performance improvement of the NHNN models tend to be different depending on domains. In Experiment 1, we found that the performance improvement on one gender is significantly better than the other when the dataset has gender imbalances in utterances and the clustering is partially based on gender. When the clustering is not based on gender, and the gender ratio is close to 1, as is the case for IEMOCAP, the model showed a similar performance increase for both genders. This implies that the clustering-based approach has an effect on the model performance on some selective gender-related data with local characteristics that are not properly accounted for in a general CNN model when there is a gender imbalance. Through clustering, we build customized classifiers for each mixture component to account for these variations, while using a shared feature encoder to preserve group-invariant characteristics. Compared with a typical blackbox machine learning algorithm, our proposed model and subsequent analysis provide a more intuitive and explainable approach.\n\nIn addition to performance, compared with multi-tasking learning, which has a similar training time and model complexity, our model is more flexible as it does not require any group labels for training and achieves better within-corpus and cross-corpus performances with statistical significance.\n\nLastly, as much of the emotional experiences, expressions and perceptions are biologically, socially and culturally dependent, the ability to understand and account for domain-specific characteristics is crucial in building equitable SER systems for in-the-wild speech by speakers from all backgrounds. The NHNN model has the potential of accounting for variations and reducing model biases. Future studies can explore in detail how the clustering of acoustic features is related to sources of variation. This will help us better understand domain-specific features in speech emotion expressions. In our experiments, we used a standard DPMM with a full set of eGeMAPS features. Future studies can conduct feature selection and regularization techniques to test different variants of NHNN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose the Nonparametric Hierarchical Neural Network to account for variations in emotional expressions in speech. In our experiments, we show the proposed model surpass state-of-the-art models on most of within-corpus and cross-corpus SER tests.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) is a hierarchical model that consists",
      "page": 3
    },
    {
      "caption": "Figure 2: The clustering",
      "page": 3
    },
    {
      "caption": "Figure 1: Architecture of Nonparametric Hierarchical",
      "page": 4
    },
    {
      "caption": "Figure 2: CNN-based NHNN implementation pipeline",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lance Ying\nAmrit Romana": "University of Michigan\nUniversity of Michigan",
          "Emily Mower Provost": "University of Michigan"
        },
        {
          "Lance Ying\nAmrit Romana": "Ann Arbor, Michigan, USA\nAnn Arbor, Michigan, USA",
          "Emily Mower Provost": "Ann Arbor, Michigan, USA"
        },
        {
          "Lance Ying\nAmrit Romana": "ABSTRACT",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "the model on both lab-produced and in-the-wild speech emo-"
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "tion datasets and show our proposed model outperforms state-"
        },
        {
          "Lance Ying\nAmrit Romana": "In recent years, deep-learning-based speech emotion recog-",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "of-the-art methods on most within-corpus and cross-corpus"
        },
        {
          "Lance Ying\nAmrit Romana": "nition models have outperformed classical machine learning",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "tests. Our work has the potential to achieve robust SER per-"
        },
        {
          "Lance Ying\nAmrit Romana": "models. Previously, some neural network designs, such as",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "formances in groups with a variety of emotional expressions."
        },
        {
          "Lance Ying\nAmrit Romana": "Multitask Learning, have accounted for variations in emo-",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "tional expressions due to demographic and contextual factors.",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "2\nRELATED WORK"
        },
        {
          "Lance Ying\nAmrit Romana": "However, existing models face a few constraints: 1) they rely",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "on a clear definition of domains (e.g. gender, culture, noise",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "2.1\nSpeech Emotion Recognition"
        },
        {
          "Lance Ying\nAmrit Romana": "condition, etc.) and the availability of domain labels. 2) they",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "Speech Emotion Recognition (SER) has attracted enormous in-"
        },
        {
          "Lance Ying\nAmrit Romana": "often attempt to learn domain-invariant features while emo-",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "terest in the past decades. Past research has experimented with"
        },
        {
          "Lance Ying\nAmrit Romana": "tion expressions can be domain-specific. In the present study,",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "different machine learning algorithms, such as Hidden Markov"
        },
        {
          "Lance Ying\nAmrit Romana": "we propose the Nonparametric Hierarchical Neural Network",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "Models (HMM)[16], Artificial Neural Networks (ANN) [4], Sup-"
        },
        {
          "Lance Ying\nAmrit Romana": "(NHNN), a lightweight hierarchical neural network model",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "port Vector Machines (SVM) [10]. In recent years, deep learn-"
        },
        {
          "Lance Ying\nAmrit Romana": "based on Bayesian nonparametric clustering. In comparison",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "ing approaches have become more popular and have reached"
        },
        {
          "Lance Ying\nAmrit Romana": "to Multitask Learning approaches, the proposed model does",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "state-of-the-art performances on SER [9]. For instance, Zhao"
        },
        {
          "Lance Ying\nAmrit Romana": "not require domain/task labels. In our experiments, the NHNN",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "et al. proposed the CNN-LSTM model, which showed better"
        },
        {
          "Lance Ying\nAmrit Romana": "models outperform the models with similar levels of complex-",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "accuracy than conventional CNN and LSTM models [33]. Liu"
        },
        {
          "Lance Ying\nAmrit Romana": "ity and state-of-the-art models in within-corpus and cross-",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "et al. proposed a temporal attention CNN model, which shows"
        },
        {
          "Lance Ying\nAmrit Romana": "corpus tests. Through clustering analysis, we show that the",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "stable and state-of-the-art performances on SER tasks [15] ."
        },
        {
          "Lance Ying\nAmrit Romana": "NHNN models are able to learn group-specific features and",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "bridge the performance gap between groups.",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "2.2\nAcoustic Features and Variations"
        },
        {
          "Lance Ying\nAmrit Romana": "KEYWORDS",
          "Emily Mower Provost": "A considerable amount of research from the speech signal"
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "processing community has explored relevant acoustic features"
        },
        {
          "Lance Ying\nAmrit Romana": "Speech Emotion Recognition, Bayesian Nonparametric Method,",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "for SER [27]. Some common features used for classifications"
        },
        {
          "Lance Ying\nAmrit Romana": "Clustering",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "often include prosody features, such as pitch and loudness,"
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "voice quality features, such as the first three formants and the"
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "signal-to-noise ratio, and spectral features, such as the Melfre-"
        },
        {
          "Lance Ying\nAmrit Romana": "1\nINTRODUCTION",
          "Emily Mower Provost": ""
        },
        {
          "Lance Ying\nAmrit Romana": "",
          "Emily Mower Provost": "quency Cepstral Coefficients (MFCC) and Linear Prediction"
        },
        {
          "Lance Ying\nAmrit Romana": "Emotion can greatly influence peopleâ€™s mental processes and",
          "Emily Mower Provost": "Cepstral Coefficients (LPCC)."
        },
        {
          "Lance Ying\nAmrit Romana": "behaviors and is also a predictor of physical and psychological",
          "Emily Mower Provost": "However, robustly extracting emotion from these acoustic"
        },
        {
          "Lance Ying\nAmrit Romana": "well being. Speech Emotion Recognition (SER) has become an",
          "Emily Mower Provost": "features can be quite challenging. The relationship between"
        },
        {
          "Lance Ying\nAmrit Romana": "important research topic in human-computer interaction.",
          "Emily Mower Provost": "experienced emotions and observed acoustic features can be"
        },
        {
          "Lance Ying\nAmrit Romana": "Although considerable research effort has been put into",
          "Emily Mower Provost": "moderated by multiple contextual and demographic factors."
        },
        {
          "Lance Ying\nAmrit Romana": "developing models capable of recognizing and predicting hu-",
          "Emily Mower Provost": "These variations create challenges for robust automatic speech"
        },
        {
          "Lance Ying\nAmrit Romana": "man emotions, much of\nthe work focuses on lab-produced",
          "Emily Mower Provost": "recognition systems. One source of variations is environmen-"
        },
        {
          "Lance Ying\nAmrit Romana": "datasets[3] and robust speech emotion recognition on in-the-",
          "Emily Mower Provost": "tal conditions. In-the-wild speech is often recorded with dif-"
        },
        {
          "Lance Ying\nAmrit Romana": "wild speech with diverse confounding acoustic elements re-",
          "Emily Mower Provost": "ferent recording devices, producing different sound qualities."
        },
        {
          "Lance Ying\nAmrit Romana": "mains a challenge [32]. Previous work has used a variety of",
          "Emily Mower Provost": "A variety of background noise, such as other human speech"
        },
        {
          "Lance Ying\nAmrit Romana": "machine learning techniques such as multi-task learning [32]",
          "Emily Mower Provost": "and music, can also introduce noise into the extracted acous-"
        },
        {
          "Lance Ying\nAmrit Romana": "and domain generalization [31] that attempt to learn a more",
          "Emily Mower Provost": "tic features. Additionally, researchers have shown that\nthe"
        },
        {
          "Lance Ying\nAmrit Romana": "robust and generalizable representation of speech features,",
          "Emily Mower Provost": "experience of emotion and its expressions are often influ-"
        },
        {
          "Lance Ying\nAmrit Romana": "yet few studies have successfully accounted for variations in",
          "Emily Mower Provost": "enced by biological, social, and cultural\nfactors. Kring and"
        },
        {
          "Lance Ying\nAmrit Romana": "speech emotion corpora due to the complexity of emotion ex-",
          "Emily Mower Provost": "Gordon found that, when presented with emotional stimuli,"
        },
        {
          "Lance Ying\nAmrit Romana": "pressions and a variety of moderating variables such as gender,",
          "Emily Mower Provost": "female participants were more expressive than male partici-"
        },
        {
          "Lance Ying\nAmrit Romana": "culture, etc.",
          "Emily Mower Provost": "pants, even though they reported the same type and intensity"
        },
        {
          "Lance Ying\nAmrit Romana": "In this paper, we propose a novel lightweight architecture",
          "Emily Mower Provost": "of experienced emotions [13]. Matsumoto et al.\nfound that"
        },
        {
          "Lance Ying\nAmrit Romana": "based on unsupervised non-parametric clustering and super-",
          "Emily Mower Provost": "cultural display rules differ systematically between Japan and"
        },
        {
          "Lance Ying\nAmrit Romana": "vised convolutional neural networks (CNN) in order to account",
          "Emily Mower Provost": "the United States [17]. Gender differences have also been well"
        },
        {
          "Lance Ying\nAmrit Romana": "for inter-group differences in emotional expressions. We test",
          "Emily Mower Provost": "documented in SER applications. For instance, Alghowinem"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "et al. found that best features for depression detection from": "speech differed across genders [26]."
        },
        {
          "et al. found that best features for depression detection from": "Despite advances in speech emotion recognition systems,"
        },
        {
          "et al. found that best features for depression detection from": "few model designs have properly accounted for these factors."
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "One popular approach is to partition audio segments to be part"
        },
        {
          "et al. found that best features for depression detection from": "of a certain domain, such as gender, corpus, noise condition,"
        },
        {
          "et al. found that best features for depression detection from": "etc., and apply domain adaptation [30] or multi-task learning"
        },
        {
          "et al. found that best features for depression detection from": "during training [32]. Some have trained separate models on"
        },
        {
          "et al. found that best features for depression detection from": "each domain and then applied model fusion in a later stage"
        },
        {
          "et al. found that best features for depression detection from": "[24]. However, one key assumption of such approaches is that"
        },
        {
          "et al. found that best features for depression detection from": "there exist pre-specified domains for audio. For human speech,"
        },
        {
          "et al. found that best features for depression detection from": "the domains can be non-exhaustive. For instance, gender, race,"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "language, age, environmental conditions are all possible crite-"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "ria for categorization, yet a combination of these criteria would"
        },
        {
          "et al. found that best features for depression detection from": "generate an exponential amount of permutations with multi-"
        },
        {
          "et al. found that best features for depression detection from": "ple labels (e.g. Male, Caucasian, adult, etc.). Training separate"
        },
        {
          "et al. found that best features for depression detection from": "models for each permutation or applying domain adaptation"
        },
        {
          "et al. found that best features for depression detection from": "among such a large number of domains are simply impractical."
        },
        {
          "et al. found that best features for depression detection from": "In addition, the goal of domain adaptation approaches is to"
        },
        {
          "et al. found that best features for depression detection from": "find some shared representation that exists across domains."
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "However, some acoustic features may be domain-specific. Past"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "research has shown that negative transfer can occur where"
        },
        {
          "et al. found that best features for depression detection from": "transfer methods decrease performances [28]."
        },
        {
          "et al. found that best features for depression detection from": "Last but not least, working with pre-specified domains as-"
        },
        {
          "et al. found that best features for depression detection from": "sumes sufficient information about the corpus. However, when"
        },
        {
          "et al. found that best features for depression detection from": "training on in-the-wild speech or real-world speech recordings,"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "researchers often donâ€™t have explicit and perfect knowledge on"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "subjectsâ€™ demographics or speech recording conditions, and"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "many of these labels may be incomplete."
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "In the present study, we introduce the Bayesian Nonpara-"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "metric method (BNP) as a way to account for acoustic varia-"
        },
        {
          "et al. found that best features for depression detection from": "tions. Our proposed method offers a more flexible alternative"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "than Multi-task Learning or domain adaptation as it captures"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "the latent structure of the dataset through unsupervised means"
        },
        {
          "et al. found that best features for depression detection from": "and does not rely on any additional labels except ground-truth"
        },
        {
          "et al. found that best features for depression detection from": "emotion categories. The number of domains can grow as new"
        },
        {
          "et al. found that best features for depression detection from": "data becomes available."
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "2.3\nBayesian Nonparametric Method"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "2.3.1\nDirichlet Process and Dirichlet Process Mixture Model."
        },
        {
          "et al. found that best features for depression detection from": "The Bayesian nonparametric (BNP) method was first proposed"
        },
        {
          "et al. found that best features for depression detection from": "by Ferguson [8], who introduced the Dirichlet process, de-"
        },
        {
          "et al. found that best features for depression detection from": "noted as DP(ğ›¼0, ğº0), where ğ›¼0 is the scaling parameter, and"
        },
        {
          "et al. found that best features for depression detection from": "ğº0 is the probability measure. An explicit formulation was"
        },
        {
          "et al. found that best features for depression detection from": "provided by Sethuraman [25] with the famous â€œstick-breaking"
        },
        {
          "et al. found that best features for depression detection from": "constructionâ€, which uses an independent sequence of i.i.d. ran-"
        },
        {
          "et al. found that best features for depression detection from": "dom variables ğœ· = (ğ›½1, ğ›½2, ...) and components ğ“ = (ğœ™1, ğœ™2, ...)"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "where"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "ğ›½ğ‘˜ |ğ›¼0, ğº0 âˆ¼ ğµğ‘’ğ‘¡ğ‘(1, ğ›¼0)"
        },
        {
          "et al. found that best features for depression detection from": "(1)"
        },
        {
          "et al. found that best features for depression detection from": "ğœ™ğ‘˜ |ğ›¼0, ğº0 âˆ¼ ğº0"
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": ""
        },
        {
          "et al. found that best features for depression detection from": "Then we can define a discrete distribution ğº as"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Some previous work has incorporated BNP in SER. For": "instance, Wang et al. used a hierarchical Dirichlet Process",
          "domains while the domain-specific classifier learns a domain-": "specific mapping from the bottleneck features to outputs."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "mixture model on music emotion labeling and showed better",
          "domains while the domain-specific classifier learns a domain-": "Once the model is trained, the model performs the classifi-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "performance on music emotion annotation and retrieval than",
          "domains while the domain-specific classifier learns a domain-": "cation task as follows.Suppose the classification problem has"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "GMM [29]. However, the overall application of BNP in SER is",
          "domains while the domain-specific classifier learns a domain-": "ğ‘› classes and assume ğ‘˜ mixture components are inferred from"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "quite limited and none has attempted using BNP in modeling",
          "domains while the domain-specific classifier learns a domain-": "a Gaussian DPMM with parameters ğ, ğˆ and cluster weights"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "individual/group differences in corpora.",
          "domains while the domain-specific classifier learns a domain-": "ğ. Denote the set of mixtures as ğ“ = (ğœ™1, ğœ™2, ..., ğœ™ğ‘˜ ), the ob-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "servation as ğ‘¥, the class of the observation as ğ‘§, and the latent"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3\nCLASSIFICATION MODEL",
          "domains while the domain-specific classifier learns a domain-": "discrete allocation variable as ğœƒ . Each of the classifiers produce"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "In this section, we present the baseline models and the pro-",
          "domains while the domain-specific classifier learns a domain-": "a probability distribution ğœ‹ (ğœ™ğ‘– ) = (ğœ‹1 (ğœ™ğ‘– ), ğœ‹2 (ğœ™ğ‘– )...ğœ‹ğ‘› (ğœ™ğ‘– ))"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "from the softmax layer, where ğœ‹ ğ‘— (ğœ™ğ‘– ) denotes the j-th output"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "posed Nonparametric Hierarchical Neural Network.",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "from the softmax probability distribution associated with the"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3.1\nBaseline Models",
          "domains while the domain-specific classifier learns a domain-": "cluster ğœ™ğ‘– . Here we can equivalently express this as a condi-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "tional probability ğœ‹ ğ‘— (ğœ™ğ‘– ) = ğ‘ƒ (ğ‘§ = ğ‘— |ğœƒ = ğ‘–). Then the weighted"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3.1.1\nDilated Convolutional Neural Network (DCNN). Con-",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "conditional probability is,"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "volutional Neural Network(CNN)\nis one of\nthe most popu-",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "lar deep learning methods.\nIn many applications, CNN has",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğ‘˜âˆ‘ï¸ ğ‘–\nğ‘ƒ (ğ‘§ = ğ‘— |ğœƒ = ğ‘–)ğ‘ƒ (ğœƒ = ğ‘– |ğ‘¥, ğ, ğˆ, ğ)\nğ‘ƒ (ğ‘§ = ğ‘— |ğ‘¥, ğ, ğˆ, ğ) ="
        },
        {
          "Some previous work has incorporated BNP in SER. For": "well surpassed the performances of classical machine learning",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "=1"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "methods because of its ability to construct rich representations",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(7)"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "from data [11].",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "Since each cluster is a Gaussian, we can substitute the condi-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "The CNN model consists of a feature encoder, which is a",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "tional probability for the latent variable."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "stack of convolutional and maxpool\nlayers, and a classifier,",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "which is a stack of fully-connected layers and a softmax layer.",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ”ğ‘– ğ‘ (ğ‘¥ |ğœ‡ğ‘–, ğœğ‘– )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "For classifications, the classifier generates an n-class probabil-",
          "domains while the domain-specific classifier learns a domain-": "ğ‘˜âˆ‘ï¸ ğ‘–\nğ‘ƒ (ğ‘§ = ğ‘— |ğœƒ = ğ‘–)\nğ‘ƒ (ğ‘§ = ğ‘— |ğ‘¥, ğ, ğˆ, ğ) ="
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(cid:205)ğ‘˜"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ”ğ‘™ ğ‘ (ğ‘¥ |ğœ‡ğ‘™ , ğœğ‘™ )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "ity distribution.",
          "domains while the domain-specific classifier learns a domain-": "=1\nğ‘™=1"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(8)"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "In the present study, we use a Dilated Convolutional Neural",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "Network (DCNN), which uses two 1D dilated convolutional",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ”ğ‘– ğ‘ (ğ‘¥ |ğœ‡ğ‘–, ğœğ‘– )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "layers as the feature encoder for inputs. In comparison to a",
          "domains while the domain-specific classifier learns a domain-": "ğ‘˜âˆ‘ï¸ ğ‘–\nğœ‹ ğ‘— (ğœ™ğ‘– )\n=\n(9)"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(cid:205)ğ‘˜"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ”ğ‘™ ğ‘ (ğ‘¥ |ğœ‡ğ‘™ , ğœğ‘™ )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "CNN, the dilation in the DCNN allows for a more inclusive",
          "domains while the domain-specific classifier learns a domain-": "=1\nğ‘™=1"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "receptive field, which is commonly used in SER [11, 14]. We",
          "domains while the domain-specific classifier learns a domain-": "Then the predicted label ğœ of observation ğ‘¥ is essentially"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "use ReLU as the activation function, except\nthe softmax at",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ”ğ‘– ğ‘ (ğ‘¥ |ğœ‡ğ‘–, ğœğ‘– )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "the end. The channel size for the convolutional\nlayers and",
          "domains while the domain-specific classifier learns a domain-": "ğ‘˜âˆ‘ï¸ ğ‘–"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ğœ = argmax\nğ‘— = 1, 2, ...ğ‘›"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(cid:205)ğ‘˜"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "fully-connected layers are both 128, as used in [11].",
          "domains while the domain-specific classifier learns a domain-": "ğ‘—\nğœ”ğ‘™ ğ‘ (ğ‘¥ |ğœ‡ğ‘™ , ğœğ‘™ )"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "=1\nğ‘™=1"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(10)"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3.1.2\nMultitask CNN (MTL-CNN). Multitask Learning attempts",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "where (cid:205)ğ‘›\nğœ‹ ğ‘— (ğœ™ğ‘– ) = 1."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "to learn models that perform well on multiple tasks simultane-",
          "domains while the domain-specific classifier learns a domain-": "ğ‘—=1"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "ously. Through this paradigm, models can learn shared repre-",
          "domains while the domain-specific classifier learns a domain-": "Through its hierarchical structure, NHNN is able to extract"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "sentations for multiple tasks with better generalizability. In a",
          "domains while the domain-specific classifier learns a domain-": "common features across domains while preserving domain-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "simple Multitask CNN, different tasks share the same feature",
          "domains while the domain-specific classifier learns a domain-": "specific characteristics. The structure can be adapted to a vari-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "encoder while having task-unique classification layers. We in-",
          "domains while the domain-specific classifier learns a domain-": "ety of neural networks such as CNN, RNN, and DNN."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "clude Multitask CNN as a baseline because it also attempts to",
          "domains while the domain-specific classifier learns a domain-": "In the present study, we use a CNN-based NHNN for SER."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "account for variations in speech domains and its architecture",
          "domains while the domain-specific classifier learns a domain-": "The pipeline of the model\nis shown in Fig 2. The clustering"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "have a similar amount of parameters and training time as our",
          "domains while the domain-specific classifier learns a domain-": "of the audio is based on eGeMAPS feature vectors and the"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "proposed methods. In the present study, we follow existing",
          "domains while the domain-specific classifier learns a domain-": "MFB features are passed into the feature encoder and emotion"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "work [21, 32] and use gender classification as an auxiliary task",
          "domains while the domain-specific classifier learns a domain-": "classifier. Both eGeMAPS and MFB features are commonly"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "in addition the main task of valence classification.",
          "domains while the domain-specific classifier learns a domain-": "used in SER work and will be introduced in detail in Section"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "4.5. We use the feature encoder from the DCNN model\nto"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3.1.3\nCNN with Long Short-Term Memory (CNN-LSTM). Zhao",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "construct a shared bottleneck representation."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "et al. [33] proposed a CNN-LSTM model which achieved state-",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "In our preliminary clustering analysis, we find that, in each"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "of-the-art performance on SER tasks. In this paper, we use the",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "dataset, the clustering yields two major components that ac-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "same 1D CNN-LSTM model, which consists of 4 convolutional",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "count for over 90% of the utterances. Clusters with little data"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "layers, each followed by a max pooling and dropout layer, 1",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "(< 10% of the corpus) are therefore removed to ensure each"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "LSTM layer and a fully-connected layer.",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "cluster has sufficient data to fit the neural network. In experi-"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "",
          "domains while the domain-specific classifier learns a domain-": "ments, the data are reassigned to existing cluster."
        },
        {
          "Some previous work has incorporated BNP in SER. For": "3.2\nNonparametric Hierarchical Neural",
          "domains while the domain-specific classifier learns a domain-": ""
        },
        {
          "Some previous work has incorporated BNP in SER. For": "Network",
          "domains while the domain-specific classifier learns a domain-": "3.3\nModel Complexity"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "The NHNN model (Fig. 1) is a hierarchical model that consists",
          "domains while the domain-specific classifier learns a domain-": "The DCNN model has the least number of parameters (n_parameters"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "of a shared feature encoder and domain-specific classifiers. The",
          "domains while the domain-specific classifier learns a domain-": "=175,619). The MTL-CNN model and the proposed NHNN FC"
        },
        {
          "Some previous work has incorporated BNP in SER. For": "feature encoder aims to learn a shared representation for all",
          "domains while the domain-specific classifier learns a domain-": "model have a similar amount of parameters (n_parameters"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1\nIEMOCAP": "The Interactive Emotional Dyadic MOtion Capture Database"
        },
        {
          "4.1\nIEMOCAP": "(IEMOCAP)\nis a common lab-recorded multimodal dataset."
        },
        {
          "4.1\nIEMOCAP": "Ten actors (five male and five female) performed a series of"
        },
        {
          "4.1\nIEMOCAP": "scripts or improvisational scenarios recorded over five ses-"
        },
        {
          "4.1\nIEMOCAP": "sions. The audios are recorded at a 48 kHz sampling rate and"
        },
        {
          "4.1\nIEMOCAP": "then downsampled to 16 kHz. The IEMOCAP dataset includes"
        },
        {
          "4.1\nIEMOCAP": "10,039 audio segments (5,255 scripted utterances and 4,784"
        },
        {
          "4.1\nIEMOCAP": "improvised utterances). The segments are annotated by two"
        },
        {
          "4.1\nIEMOCAP": "to four annotators on valence, activation, and dominance on a"
        },
        {
          "4.1\nIEMOCAP": "5-point Likert Scale."
        },
        {
          "4.1\nIEMOCAP": "4.2\nPRIORI Emotion"
        },
        {
          "4.1\nIEMOCAP": "The PRIORI Emotion dataset\nis an annotated subset of\nthe"
        },
        {
          "4.1\nIEMOCAP": "larger PRIORI (Predicting Individual Outcomes for Rapid Inter-"
        },
        {
          "4.1\nIEMOCAP": "vention) bipolar mood dataset [cite]. It consists of phone calls"
        },
        {
          "4.1\nIEMOCAP": "from 19 subjects. The utterances are annotated on a 9-point"
        },
        {
          "4.1\nIEMOCAP": "Likert scale on valence and activation. The dataset includes"
        },
        {
          "4.1\nIEMOCAP": "18,388 annotated audio segments."
        },
        {
          "4.1\nIEMOCAP": "4.3\nPRIORI R21"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "PRIORI R21 is a multicultural subset of PRIORI. The PRIORI"
        },
        {
          "4.1\nIEMOCAP": "R21 dataset includes 9 subjects, 5 are native English speakers"
        },
        {
          "4.1\nIEMOCAP": "and 4 are native Arabic speakers. The countries of birth in-"
        },
        {
          "4.1\nIEMOCAP": "clude Palestine, Lebanon, Saudi Arabia, Yemen, and the United"
        },
        {
          "4.1\nIEMOCAP": "States. Similar to PRIORI Emotion,\nthe audio segments are"
        },
        {
          "4.1\nIEMOCAP": "annotated on a 9-point Likert scale on valence and activation."
        },
        {
          "4.1\nIEMOCAP": "During annotation, non-English speech or empty segments"
        },
        {
          "4.1\nIEMOCAP": "are removed. The R21 dataset includes 4,148 annotated audio"
        },
        {
          "4.1\nIEMOCAP": "segments. In the present study, we exclude two subjects, one"
        },
        {
          "4.1\nIEMOCAP": "due to poor audio quality and the other due to a lack of audio"
        },
        {
          "4.1\nIEMOCAP": "segments (33 segments)."
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "4.4\nEmotion Labels"
        },
        {
          "4.1\nIEMOCAP": "SER work often uses one of two emotion labels: categorical"
        },
        {
          "4.1\nIEMOCAP": "and dimensional\nlabels. The categorical\nlabels include emo-"
        },
        {
          "4.1\nIEMOCAP": "tions such as happy, sad, angry, etc. where dimensional\nla-"
        },
        {
          "4.1\nIEMOCAP": "bels typically include valence and activation. Valence refers"
        },
        {
          "4.1\nIEMOCAP": "to the perceived pleasantness of\nthe speech and activation"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "refers to the degree of arousal. Valence and activation are of-"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "ten scored on a Likert scale, which maps each utterance onto"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "a 2-D valence-activation space."
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "Although much of the work on IEMOCAP uses categorical"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "labels in classification tasks, in the present study, however, we"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "choose to use dimensional valence scores because the PRIORI"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "datasets are only annotated on dimensional scores. Previous"
        },
        {
          "4.1\nIEMOCAP": "research has also shown that dimensional\nlabels are more"
        },
        {
          "4.1\nIEMOCAP": ""
        },
        {
          "4.1\nIEMOCAP": "consistently interpretable across datasets [9, 23]."
        },
        {
          "4.1\nIEMOCAP": "We follow the same preprocessing procedure as Gideon et"
        },
        {
          "4.1\nIEMOCAP": "al. [9] by converting each annotation of valence into three"
        },
        {
          "4.1\nIEMOCAP": "bins. The middle bin corresponds to medium valence with a"
        },
        {
          "4.1\nIEMOCAP": "rating of 3 for IEMOCAP and a rating of 5 for PRIORI Emotion"
        },
        {
          "4.1\nIEMOCAP": "and PRIORI R21. The other two bins correspond to valence"
        },
        {
          "4.1\nIEMOCAP": "ratings below and above the middle points. We then create a"
        },
        {
          "4.1\nIEMOCAP": "final label for an utterance by aggregating over all evaluations"
        },
        {
          "4.1\nIEMOCAP": "for that utterance and identifying the most commonly selected"
        },
        {
          "4.1\nIEMOCAP": "(majority) bin. We exclude all utterances without a majority"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "bin. Our final datasets include 6,816 segments for IEMOCAP,"
        },
        {
          "Table 1: Summary of datasets": "13,822 segments for PRIORI Emotion, and 2,677 segments for"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "PRIORI R21. The breakdown of utterance annotations and"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "demographics are shown in Table 1."
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "4.5\nFeature Extraction and Preprocessing"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "All audio segments from the datasets are normalized to 0 dBFS"
        },
        {
          "Table 1: Summary of datasets": "using the Sox command line tool. We extract two sets of fea-"
        },
        {
          "Table 1: Summary of datasets": "tures for our model: eGeMAPS and MFB."
        },
        {
          "Table 1: Summary of datasets": "eGeMAPS â€“ The eGeMAPS feature set is a commonly used"
        },
        {
          "Table 1: Summary of datasets": "feature set designed for SER. It includes 88 acoustic features"
        },
        {
          "Table 1: Summary of datasets": "such as energy, excitation, spectral, cepstral, etc. In the present"
        },
        {
          "Table 1: Summary of datasets": "study, the eGeMAPS feature vectors are extracted by the openS-"
        },
        {
          "Table 1: Summary of datasets": "MILE toolkit with default parameters [6, 7]."
        },
        {
          "Table 1: Summary of datasets": "MFB â€“ We extract 40-dimensional MFB features with 25ms"
        },
        {
          "Table 1: Summary of datasets": "frame length and 10ms frame shift using the Librosa package"
        },
        {
          "Table 1: Summary of datasets": "[18]. The MFB features are then z-normalized and padded to"
        },
        {
          "Table 1: Summary of datasets": "the same length during training to account for variations in"
        },
        {
          "Table 1: Summary of datasets": "audio lengths."
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "5\nEXPERIMENTAL SETUP"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "We designed two experiments to test the within-corpus and"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "cross-corpus performance of NHNN against the baseline. In"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "the present study, we train and test two variants of NHNN"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "by freezing different layers of the DCNN model. The NHNN"
        },
        {
          "Table 1: Summary of datasets": "FC variant freezes all the convolutional layers and adapts the"
        },
        {
          "Table 1: Summary of datasets": "fully-connected layers to each cluster. The NHNN FC+Conv"
        },
        {
          "Table 1: Summary of datasets": "variant freezes one out of the two convolutional layers of the"
        },
        {
          "Table 1: Summary of datasets": ""
        },
        {
          "Table 1: Summary of datasets": "feature encoder."
        },
        {
          "Table 1: Summary of datasets": "In both experiments, the classifiers are trained on a single"
        },
        {
          "Table 1: Summary of datasets": "corpus. We randomly take out 1/4 of the training set to be"
        },
        {
          "Table 1: Summary of datasets": "the validation set. We implement both experiments with Py-"
        },
        {
          "Table 1: Summary of datasets": "torch. We use early stopping with a patience of 5 epochs and"
        },
        {
          "Table 1: Summary of datasets": "a maximum epoch of 50. The model with the best validation"
        },
        {
          "Table 1: Summary of datasets": "loss is selected. Training and testing is repeated with different"
        },
        {
          "Table 1: Summary of datasets": "random seeds. In both experiments, hyperparameters includ-"
        },
        {
          "Table 1: Summary of datasets": "ing batch size(32/64) and learning rate(0.001/0.0005/0.0001)"
        },
        {
          "Table 1: Summary of datasets": "are tuned for all models. We choose a batch size of 64 and a"
        },
        {
          "Table 1: Summary of datasets": "learning rate of 0.0001 for the final implementation."
        },
        {
          "Table 1: Summary of datasets": "For performance evaluation, we use the Unweighted Av-"
        },
        {
          "Table 1: Summary of datasets": "erage Recall (UAR) as the performance metric. UAR ensures"
        },
        {
          "Table 1: Summary of datasets": "that each valence class is given equal weight at performance"
        },
        {
          "Table 1: Summary of datasets": "evaluation to account for possible data imbalance. A random"
        },
        {
          "Table 1: Summary of datasets": "prediction would result in a UAR of 0.333 in our experiments."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "creases to 2.05%. However, none of the changes in performance",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "and 0.69, respectively. Significant variations in subject_id dis-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "gaps is statistically significant.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "tributions are found in the two clusters. In the PRIORI Emotion"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "The comparison analysis between the two gender groups",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "dataset, the max/min subject audio segments ratio is 2.77 in"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "shows that the performance gaps between male and female",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "cluster 1 and 9.00 in cluster 2, while the ratio is 1.56 in the V1"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "speech in PRIORI Emotion and PRIORI R21 shrink under",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "dataset as a whole."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "our proposed models.\nIn PRIORI Emotion,\nthe UAR perfor-",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "As for IEMOCAP, the gender ratio follows an even split"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "mance on female speech (UAR=0.4731) is 3.73% higher than",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "in each mixture and the dataset as a whole. We also did not"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "on male speech (UAR=0.4358). The gender-group UAR dif-",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "observe any significant variations in the number of subject"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "ference shrinks to 3.42% under NHNN FC and 3.26% under",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "audio segments among clusters. Itâ€™s likely that the clustering"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "NHNN FC+Conv.\nIn PRIORI R21, the UAR performance on",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "of IEMOCAP depends more on lower-level acoustic features"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "male speech (UAR=0.3990)\nis 1.61% higher than on female",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "than subject group differences."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "speech (UAR=0.3876). The gender-group UAR difference shrinks",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "The valence rating distributions are similar in all mixture"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "to 0.38% under NHNN FC and -0.54% under NHNN FC+Conv.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "components in each of the datasets."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "No significant changes in gender-group UAR differences are",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "observed for IEMOCAP when comparing two NHNN models",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "6.3\nExperiment 2"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "and the DCNN model.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "Table 4 shows the results of Experiment 2. The two NHNN"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "Table 3 shows the group-level performances improvement",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "models have better average performances than other mod-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "between female and male subjects against the DCNN model.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "els. In particular, NHNN FC+Conv outperforms DCNN and"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "In PRIORI Emotion and PRIORI R21, both variants of NHNN",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "MTL-CNN on all cross-corpus tests, and CNN-LSTM when"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "showed statistically significant performance boost on one of",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "trained on IEMOCAP and PRIORI Emotion. The CNN-LSTM"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "the gender groups but not the other, whereas the performance",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "model performs relatively poorly on IEMOCAP and PRIORI"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "changes for the two gender groups is similar and both statisti-",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "Emotion against other models but has the best UAR perfor-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "cally significant on IEMOCAP.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "mance when trained on PRIORI R21 and tested on the other"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "two datasets. The differences are not statistically significant"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "between CNN-LSTM and NHNN FC+Conv when trained on"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "PRIORI R21 (p=0.1401 when tested on IEMOCAP and p=0.0986"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "6.2\nClustering Analysis",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "when tested on PRIORI Emotion). Between the two variants,"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "NHNN FC+Conv has on average better cross-corpus perfor-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "The two mixture components have similar weights across three",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "mances than NHNN FC model."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "datasets, which roughly follow an 80/20 split. We first relate",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "the clustering results to attributes of the audio segments. In",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "7\nDISCUSSION"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "particular, we look at the gender ratio, language ratio, subject",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": ""
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "audio distribution, and mood severity in each mixture. In the",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "In the present study, we introduce the Nonparametric Hier-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "R21 dataset, we also analyze the proportion of audio segments",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "archical Neural Network, which learns latent structures from"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "from native English speakers in the mixtures. As gender and",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "the dataset with a Dirichlet Process Mixture Model to build"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "native-language are binary variables in our study, we simply",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "domain-specific classifiers from a shared feature encoder."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "compare the ratio of the two attributes in a dataset/mixture.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "The two experiments we conducted show that NHNN, de-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "The gender ratio is defined as the number of\nfemale audio",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "spite being a lightweight model, has superior within-corpus"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "segments divided by the number of male audio segments. The",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "performance than DCNN and MTL-CNN. Its cross-corpus test"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "language ratio is calculated by dividing the number of audio",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "performance is on average better than DCNN, MTL-CNN and"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "segments from native English speakers from those from non-",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "CNN-LSTM, which indicates a better generalizability on un-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "native speakers. As for subjects,\nthere are great variations",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "seen data. The performance increase over the DCNN, which"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "in the number of audio segments belonged to each subject",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "uses the same feature encoder as the NHNN model, is observed"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "in each dataset while the mixtures are of different sizes. To",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "for both lab-produced and in-the-wild datasets. In addition,"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "analyze the distributions of subject_id, we divide the maximum",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "NHNN FC+Conv shows a slightly better average performance"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "number of audio segments belonged to a single subject by the",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "than NHNN FC in both experiments. This is likely because"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "minimum number as a surrogate of the dispersion of subject",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "NHNN FC+Conv allows for a more versatile classifier with"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "audio segments in a dataset/mixture. If the clustering is not",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "more free parameters."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "based on a certain attribute, we would expect\nthe relevant",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "Comparing results across two experiments, we notice that"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "ratios to be similar in cluster 1, cluster 2, and the dataset as a",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "the model trained on PRIORI Emotion and tested on PRIORI"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "whole.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "R21 has a significantly better performance than within-corpus"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "In the R21 dataset,\nthe clusters seem to be indicative of",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "UAR of PRIORI R21. This suggests that the low R21 within-"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "subjectsâ€™ demographics, where gender ratio difference and",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "corpus performance relative to PRIORI Emotion is likely due"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "native-language language difference are observed between the",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "to a lack of training samples."
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "two clusters. In cluster 1, the Female/Male ratio is 0.65, and",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "In the cluster analysis, we show that the clustering criteria"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "1.53 in cluster 2. The English/Arabic as first language ratio is",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "vary depending on the dataset, which may be a combination"
        },
        {
          "slightly from 1.61% to 1.33%, whereas in NHNN FC+Conv, it in-": "2.24 in the first cluster and 0.95 in the second cluster.",
          "In the PRIORI Emotion dataset, the gender ratios are 1.96": "of higher-level attributes or lower-level acoustic features. Our"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Performance (UAR)": "CNN-"
        },
        {
          "Performance (UAR)": "LSTM"
        },
        {
          "Performance (UAR)": "0.5517"
        },
        {
          "Performance (UAR)": "0.4639"
        },
        {
          "Performance (UAR)": ""
        },
        {
          "Performance (UAR)": "0.3876"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": "Dataset"
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": "IEMOCAP"
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": "PRIORI Emotion"
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": "PRIORI R21"
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        },
        {
          "Table 2: Within-corpus test performances of models. Best UAR performances are bolded. * indicates the performance": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "improvements (p<0.05) are bolded": "Dataset"
        },
        {
          "improvements (p<0.05) are bolded": ""
        },
        {
          "improvements (p<0.05) are bolded": "IEMOCAP"
        },
        {
          "improvements (p<0.05) are bolded": ""
        },
        {
          "improvements (p<0.05) are bolded": ""
        },
        {
          "improvements (p<0.05) are bolded": "PRIORI Emotion"
        },
        {
          "improvements (p<0.05) are bolded": ""
        },
        {
          "improvements (p<0.05) are bolded": ""
        },
        {
          "improvements (p<0.05) are bolded": "PRIORI R21"
        },
        {
          "improvements (p<0.05) are bolded": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "subsequent analysis also confirms that the performance im-": "provement of the NHNN models tend to be different depending",
          "with a typical blackbox machine learning algorithm, our pro-": "posed model and subsequent analysis provide a more intuitive"
        },
        {
          "subsequent analysis also confirms that the performance im-": "on domains. In Experiment 1, we found that the performance",
          "with a typical blackbox machine learning algorithm, our pro-": "and explainable approach."
        },
        {
          "subsequent analysis also confirms that the performance im-": "improvement on one gender is significantly better than the",
          "with a typical blackbox machine learning algorithm, our pro-": "In addition to performance, compared with multi-tasking"
        },
        {
          "subsequent analysis also confirms that the performance im-": "other when the dataset has gender imbalances in utterances",
          "with a typical blackbox machine learning algorithm, our pro-": "learning, which has a similar training time and model com-"
        },
        {
          "subsequent analysis also confirms that the performance im-": "and the clustering is partially based on gender. When the clus-",
          "with a typical blackbox machine learning algorithm, our pro-": "plexity, our model is more flexible as it does not require any"
        },
        {
          "subsequent analysis also confirms that the performance im-": "tering is not based on gender, and the gender ratio is close to",
          "with a typical blackbox machine learning algorithm, our pro-": "group labels for training and achieves better within-corpus"
        },
        {
          "subsequent analysis also confirms that the performance im-": "1, as is the case for IEMOCAP, the model showed a similar",
          "with a typical blackbox machine learning algorithm, our pro-": "and cross-corpus performances with statistical significance."
        },
        {
          "subsequent analysis also confirms that the performance im-": "performance increase for both genders. This implies that the",
          "with a typical blackbox machine learning algorithm, our pro-": "Lastly, as much of the emotional experiences, expressions"
        },
        {
          "subsequent analysis also confirms that the performance im-": "clustering-based approach has an effect on the model perfor-",
          "with a typical blackbox machine learning algorithm, our pro-": "and perceptions are biologically, socially and culturally depen-"
        },
        {
          "subsequent analysis also confirms that the performance im-": "mance on some selective gender-related data with local charac-",
          "with a typical blackbox machine learning algorithm, our pro-": "dent, the ability to understand and account for domain-specific"
        },
        {
          "subsequent analysis also confirms that the performance im-": "teristics that are not properly accounted for in a general CNN",
          "with a typical blackbox machine learning algorithm, our pro-": "characteristics is crucial in building equitable SER systems for"
        },
        {
          "subsequent analysis also confirms that the performance im-": "model when there is a gender imbalance. Through clustering,",
          "with a typical blackbox machine learning algorithm, our pro-": "in-the-wild speech by speakers from all backgrounds. The"
        },
        {
          "subsequent analysis also confirms that the performance im-": "we build customized classifiers for each mixture component to",
          "with a typical blackbox machine learning algorithm, our pro-": "NHNN model has the potential of accounting for variations"
        },
        {
          "subsequent analysis also confirms that the performance im-": "account for these variations, while using a shared feature en-",
          "with a typical blackbox machine learning algorithm, our pro-": "and reducing model biases."
        },
        {
          "subsequent analysis also confirms that the performance im-": "coder to preserve group-invariant characteristics. Compared",
          "with a typical blackbox machine learning algorithm, our pro-": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Performance (UAR)": "CNN-"
        },
        {
          "Performance (UAR)": "LSTM"
        },
        {
          "Performance (UAR)": "0.3622"
        },
        {
          "Performance (UAR)": ""
        },
        {
          "Performance (UAR)": "0.4027"
        },
        {
          "Performance (UAR)": "0.3873"
        },
        {
          "Performance (UAR)": "0.4399"
        },
        {
          "Performance (UAR)": "0.3572"
        },
        {
          "Performance (UAR)": "0.3752"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Future studies can explore in detail how the clustering of"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "acoustic features is related to sources of variation. This will"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "help us better understand domain-specific features in speech"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "emotion expressions. In our experiments, we used a standard"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "DPMM with a full set of eGeMAPS features. Future studies"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "can conduct feature selection and regularization techniques"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "to test different variants of NHNN."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "8\nCONCLUSION"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "In this study, we propose the Nonparametric Hierarchical Neu-"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "ral Network to account for variations in emotional expressions"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "in speech. In our experiments, we show the proposed model"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "surpass state-of-the-art models on most of within-corpus and"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "cross-corpus SER tests."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "REFERENCES"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[1] Charles E Antoniak. Mixtures of dirichlet processes with applications to"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "bayesian nonparametric problems. The annals of statistics, pages 1152â€“1174,"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "1974."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[2] David M Blei and Michael\nI Jordan. Variational\ninference for dirichlet"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "process mixtures. Bayesian analysis, 1(1):121â€“143, 2006."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Narayanan.\nIemocap: Interactive emotional dyadic motion capture data-"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "base. Language resources and evaluation, 42(4):335â€“359, 2008."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Lijiang Chen, Xia Mao, Yuli Xue, and Lee Lung Cheng. Speech emotion\n[4]"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "recognition: Features and classification models. Digital signal processing,"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "22(6):1154â€“1160, 2012."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[5] Yufan Chen, Miao Liu, Shih-Yuan Liu, Justin Miller, and Jonathan P How."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Predictive modeling of pedestrian motion patterns with bayesian nonpara-"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "metrics.\nIn AIAA guidance, navigation, and control conference, page 1861,"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "2016."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[6]\nFlorian Eyben, Klaus R Scherer, BjÃ¶rn W Schuller, Johan Sundberg, Elisa-"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "beth AndrÃ©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka,"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "Shrikanth S Narayanan, et al. The geneva minimalistic acoustic parameter"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "set (gemaps) for voice research and affective computing.\nIEEE transactions"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "on affective computing, 7(2):190â€“202, 2015."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[7]\nFlorian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller. Opensmile: the munich"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "versatile and fast open-source audio feature extractor.\nIn Proceedings of the"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "18th ACM international conference on Multimedia, pages 1459â€“1462, 2010."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[8] Thomas S Ferguson. A bayesian analysis of some nonparametric problems."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "The annals of statistics, pages 209â€“230, 1973."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[9]\nJohn Gideon, Melvin McInnis, and Emily Mower Provost.\nImproving cross-"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "corpus speech emotion recognition with adversarial discriminative domain"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "generalization (addog).\nIEEE Transactions on Affective Computing, 2019."
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "[10] Qin Jin, Chengxin Li, Shizhe Chen, and Huimin Wu.\nSpeech emotion"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "recognition with acoustic and lexical features.\nIn 2015 IEEE international"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": ""
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "conference on acoustics, speech and signal processing (ICASSP), pages 4749â€“"
        },
        {
          "better than DCNN and MTL-CNN with p<0.05. â€ indicates the performance is better than CNN-LSTM with p<0.05.": "4753. IEEE, 2015."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "speech features for emotion detection: a review.\nIn 2015 International",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "invariant adversarial domain adaptation for emotion recognition.\nIn Pro-"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "Conference on Computing Communication Control and Automation, pages",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "ceedings of the 2020 International Conference on Multimodal\nInteraction,"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "661â€“664. IEEE, 2015.",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "pages 481â€“490, 2020."
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "[28]\nLisa Torrey and Jude Shavlik. Transfer learning.\nIn Handbook of research",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "[32] Biqiao Zhang, Emily Mower Provost, and Georg Essl. Cross-corpus acoustic"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "on machine learning applications and trends: algorithms, methods, and tech-",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "emotion recognition with multi-task learning: Seeking common ground"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "niques, pages 242â€“264. IGI global, 2010.",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "while preserving differences.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "[29]\nJia-Ching Wang, Yuan-Shan Lee, Yu-Hao Chin, Ying-Ren Chen, and Wen-",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "10(1):85â€“99, 2017."
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "Chi Hsieh. Hierarchical dirichlet process mixture model for music emotion",
          "[31]": "[33]",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "Jianfeng Zhao, Xia Mao, and Lijiang Chen. Speech emotion recognition"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "recognition.\nIEEE Transactions on Affective Computing, 6(3):261â€“271, 2015.",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "using deep 1d & 2d cnn lstm networks. Biomedical Signal Processing and"
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "[30] Alex Wilf and Emily Mower Provost. Dynamic layer customization for",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": "Control, 47:312â€“323, 2019."
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "noise robust speech emotion recognition in heterogeneous condition train-",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": ""
        },
        {
          "[27] Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of": "ing. arXiv preprint arXiv:2010.11226, 2020.",
          "[31]": "",
          "Yufeng Yin, Baiyu Huang, Yizhen Wu, and Mohammad Soleymani. Speaker-": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics",
      "authors": [
        "Charles E Antoniak"
      ],
      "year": "1974",
      "venue": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics"
    },
    {
      "citation_id": "2",
      "title": "Variational inference for dirichlet process mixtures",
      "authors": [
        "M David",
        "Michael Blei",
        "Jordan"
      ],
      "year": "2006",
      "venue": "Bayesian analysis"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Features and classification models",
      "authors": [
        "Lijiang Chen",
        "Xia Mao",
        "Yuli Xue",
        "Lee Cheng"
      ],
      "year": "2012",
      "venue": "Digital signal processing"
    },
    {
      "citation_id": "5",
      "title": "Predictive modeling of pedestrian motion patterns with bayesian nonparametrics",
      "authors": [
        "Yufan Chen",
        "Miao Liu",
        "Shih-Yuan Liu",
        "Justin Miller",
        "Jonathan How"
      ],
      "year": "2016",
      "venue": "AIAA guidance, navigation, and control conference"
    },
    {
      "citation_id": "6",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "BjÃ¶rn Schuller",
        "Johan Sundberg",
        "Elisabeth AndrÃ©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "A bayesian analysis of some nonparametric problems. The annals of statistics",
      "authors": [
        "Thomas S Ferguson"
      ],
      "year": "1973",
      "venue": "A bayesian analysis of some nonparametric problems. The annals of statistics"
    },
    {
      "citation_id": "9",
      "title": "Improving crosscorpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Qin Jin",
        "Chengxin Li",
        "Shizhe Chen",
        "Huimin Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "authors": [
        "Soheil Khorram",
        "Mimansa Jaiswal",
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2018",
      "venue": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "arxiv": "arXiv:1806.10658"
    },
    {
      "citation_id": "13",
      "title": "Sex differences in emotion: expression, experience, and physiology",
      "authors": [
        "Ann Kring",
        "Albert Gordon"
      ],
      "year": "1998",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "14",
      "title": "Mlt-dnet: Speech emotion recognition using 1d dilated cnn based on multi-learning trick approach",
      "authors": [
        "Soonil Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Temporal attention convolutional network for speech emotion recognition with latent representation",
      "authors": [
        "Jiaxing Liu",
        "Zhilei Liu",
        "Longbiao Wang",
        "Yuan Gao",
        "Lili Guo",
        "Jianwu Dang"
      ],
      "year": "2020",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "16",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "Shuiyang Mao",
        "Dehua Tao",
        "Guangyan Zhang",
        "Tan Ching",
        "Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Cultural similarities and differences in display rules",
      "authors": [
        "David Matsumoto"
      ],
      "year": "1990",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "18",
      "title": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "19",
      "title": "Modeling individual differences using dirichlet processes",
      "authors": [
        "J Daniel",
        "Thomas Navarro",
        "Mark Griffiths",
        "Michael Steyvers",
        "Lee"
      ],
      "year": "2006",
      "venue": "Journal of mathematical Psychology"
    },
    {
      "citation_id": "20",
      "title": "Markov chain sampling methods for dirichlet process mixture models",
      "authors": [
        "M Radford",
        "Neal"
      ],
      "year": "2000",
      "venue": "Journal of computational and graphical statistics"
    },
    {
      "citation_id": "21",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "Anish Nediyanchath",
        "Periyasamy Paramasivam",
        "Promod Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "22",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "23",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "Russell James"
      ],
      "year": "2003",
      "venue": "Psychological review"
    },
    {
      "citation_id": "24",
      "title": "Using multiple databases for training in emotion recognition: To unite or to vote?",
      "authors": [
        "BjÃ¶rn Schuller",
        "Zixing Zhang",
        "Felix Weninger",
        "Gerhard Rigoll"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "25",
      "title": "A constructive definition of dirichlet priors",
      "authors": [
        "Jayaram Sethuraman"
      ],
      "year": "1994",
      "venue": "Statistica sinica"
    },
    {
      "citation_id": "26",
      "title": "From joyous to clinically depressed: Mood detection using spontaneous speech",
      "authors": [
        "Roland Sharifa",
        "Michael Goecke",
        "Julien Wagner",
        "Michael Epps",
        "Gordon Breakspear",
        "Parker"
      ],
      "year": "2012",
      "venue": "Twenty-Fifth International FLAIRS Conference"
    },
    {
      "citation_id": "27",
      "title": "Rode Snehal Sudhakar and Manjare Chandraprabha Anil. Analysis of speech features for emotion detection: a review",
      "year": "2015",
      "venue": "2015 International Conference on Computing Communication Control and Automation"
    },
    {
      "citation_id": "28",
      "title": "Transfer learning",
      "authors": [
        "Lisa Torrey",
        "Jude Shavlik"
      ],
      "year": "2010",
      "venue": "Handbook of research on machine learning applications and trends: algorithms, methods, and techniques"
    },
    {
      "citation_id": "29",
      "title": "Hierarchical dirichlet process mixture model for music emotion recognition",
      "authors": [
        "Jia-Ching Wang",
        "Yuan-Shan Lee",
        "Yu-Hao Chin",
        "Ying-Ren Chen",
        "Wen-Chi Hsieh"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Dynamic layer customization for noise robust speech emotion recognition in heterogeneous condition training",
      "authors": [
        "Alex Wilf",
        "Emily Provost"
      ],
      "year": "2020",
      "venue": "Dynamic layer customization for noise robust speech emotion recognition in heterogeneous condition training",
      "arxiv": "arXiv:2010.11226"
    },
    {
      "citation_id": "31",
      "title": "Speakerinvariant adversarial domain adaptation for emotion recognition",
      "authors": [
        "Yufeng Yin",
        "Baiyu Huang",
        "Yizhen Wu",
        "Mohammad Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "32",
      "title": "Cross-corpus acoustic emotion recognition with multi-task learning: Seeking common ground while preserving differences",
      "authors": [
        "Biqiao Zhang",
        "Emily Provost",
        "Georg Essl"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}