{
  "paper_id": "2203.13072v1",
  "title": "Multitask Emotion Recognition Model With Knowledge Distillation And Task Discriminator",
  "published": "2022-03-24T13:50:48Z",
  "authors": [
    "Euiseok Jeong",
    "Geesung Oh",
    "Sejoon Lim"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Due to the collection of big data and the development of deep learning, research to predict human emotions in the wild is being actively conducted. We designed a multi-task model using ABAW dataset to predict valence-arousal, expression, and action unit through audio data and face images at in real world. We trained model from the incomplete label by applying the knowledge distillation technique. The teacher model was trained as a supervised learning method, and the student model was trained by using the output of the teacher model as a soft label. As a result we achieved 2.40 in Multi Task Learning task validation dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human affective behavior research is an important field in the human computer interaction field, and is actively studied with the development of big data and deep learning technologies. However, for several reasons, it is difficult to apply the effective behavioral study. To address there problems, 3rd Affective Behavior Analysis in-thewild (ABAW) Competition is held in conjunction with IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3rd ABAW includes Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, Action Unit (AU) Detection Challenge, Multi-Task-Learning (MTL) Challenge. 146 teams participated in the 2nd ABAW to improve the performance of the human emotion model in the real world. We participated in the 2nd ABAW valence-arousal (VA) Estimation Challenge and ranked 10th with a average CCC(concordance correlation) of valence and arousal 0.261 with CAPNet  [1]  that predicted the future using only past image data. We proposed a multitasking model that improves performance. Improvement points are as follows. First, audio input stream was added. Audio information is closely related to human emotions. It may reveal human emotions in the form of speech  [2] ,  [3] , or elicitate human emotions in the form of music  [4]  and sound  [5] . Kuhnke et al.  [6]  and Deng et al.  [7]  shows that audio input is effective for recognizing human emotions through the ablation study. Accordingly, audio input for past 10 seconds was added. The audio feature is extracted from a audio input via SoundNet  [8] . Second, knowledge distillation proposed by Hinton et al  [9]  was applied to improve generalization performance. It is a technique that improves generalization performance of a student model by transferring dark knowledge to the student model by utilizing the output of the pre-trained teacher model as a soft label. We trained the teacher model using only the data with ground truths on the task. After training the teacher model, output of the teacher model is used as a soft label to transfer dark knowledge to the student model.\n\nThird, we extracted task-wise features by adding gradient reverse layer and task discriminator. The feature extracted from the feature extractor was made to be task-independent by gradient reversal layer and task discriminator.\n\nAs a result of applying the above methods, we achieved 2.40 performances for the 2022 ABAW validation set, which exceeded baseline performance 0.3 proposed by  [10] .\n\nPrimary contributions of this paper are:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. ABAW Recent years, data-based research on human affective behavior research is growing rapidly, and the ABAW competition is contributing greatly to that foundation. ABAW which has been held by Kollias et al. for 3 years, is a competition that predicts the affect of characters and competes for their prediction performance using video of people appearing.  [10] -  [18] . The first competition was held at 2020 International Conference on Automatic Face and Gesture Recognition (FG), the second at 2021 International Conference on Computer Vision (ICCV), and the third at CVPR. Basically ABAW competition consists of three challenges: 2 dimensional affect classification, 7 categorical affect classification, and 12 facial action unit detection. In the first and second competitions, the top-ranked teams all proposed deep learning-based recognition models, and most of top-ranked models that recognize three challenges simultaneously  [6] ,  [7] ,  [19] -  [23] . From the third competition, MTL challenge for multi-label models that recognize three challenges at the same time is newly established, and the existing three challenges are limited to uni-label models. In addition to the output, the top-ranked teams are classified by the input data. All top-ranked teams use images from video data, but there are teams that use only image data  [19] -  [21] ,  [23] , and teams that use audio data along with image data to improve performance  [6] ,  [7] ,  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Knowledge Distillation",
      "text": "The knowledge distillation was proposed by hinton et al  [9] . The output of the pretrained teacher model is scaled by softmax function with temperature and used as a soft label to train the student model. The student model learns inter-class similarity(dark knowledge) from the soft label of the teacher model and achieves performance similar to the teacher model despite being shallower than the teacher model. Zhang et al.  [24]  also improved performance through a self-distillation technique that configured the structure of the teacher model and the student model equally. Many teams used the knowledge distillation model at the 2021 ABAW and were also on the leaderboard. In particular, Deng et al.  [7]  made it possible to train deeper dark knowledge using the knowledge distillation technique, the ensemble technique, and the generation technique in which a trained student model becomes a teacher model and trains a new student model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Domain Adaptation",
      "text": "Domain adaptation techniques proposed by Ganin et al.  [25]  to improve accuracy in target domains without ground truth through data from source domains with ground truth. Gradient Reverse Layer(GRL), which is multiply -1 to gradient when performing back propagation, and a domain discriminator are added to allow the feature extractor to extract features independent of the domain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Problem Definition",
      "text": "{X,Y } denoted train data. X represents input data and Y represents ground truths. Model function inputs X and outputs Y . For the convenience of notation, it is assumed that the batch size of all tasks is N, but even if the number of tasks' data is different, it can be configured to be divided by the same number of iterations. X consists of X img and X aud . X img represents the facial image and X aud represents the audio data:\n\nn img is the number of input images. H is width and height of the image and C img refers to the number of channel of image. In audio data, sr refers to sample rate of audio data. t aud means audio data time before prediction time. C aud is the number of channel of audio. In this dataset, n hz , frame rate of a video in dataset is 30, sr is 22050 amd C aud is 2. X img contains a image for the past t img seconds from the time of prediction. Of the total t img • n hz images, we extracted n i images with stride s. H is 112 and C img is 3 in this dataset.\n\nThe ground truths Y consists of four types:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Methodologies",
      "text": "A. Architecture We used CNN architecture based on FER model of DRER  [26]  to extract image representation. It aims to predict the valence and arousal of the driver. The FER model is composed of ResNeXt and SENet and is pre-trained with AffectNet  [27] . A detailed description of the FER model can be found in  [26] . Output of FER model from single image is a 512dimensional vector. As mentioned in section III. , Six images are fed into the FER model and (6,512) shaped image representation is extracted. The extracted image representation is fed into the LSTM model to capture the temporal feature. To extract sound representation, we adopted SoundNet proposed by Aytar et al.  [8] . SoundNet composed of a series of one-dimensional convolutions. The audio data for t aud seconds is fed into SoundNet and extracted sound representation from SoundNet is the 1000-dimensional vector.\n\nSoundNet is trained to transfer visual knowledge through well established visual recognition models, leveraging massive amounts of unlabeled videos  [8] . Therefore, it is particularly effective in the cross-model approach by using both video and audio  [28] .\n\nSound representation and image representation are concatenated before fed into feature extractor. The feature extractor consists of a dense module containing linear layer, swish activation function, batch normalization, and dropout layer. Task-wise feature is extracted from feature extractor. Each task has their own task model. The task-wise feature is fed into each task model, and each task model outputs prediction for each task. The task-wise feature is fed into the task discriminator via the gradient reversal layer. The task discriminator outputs an output for which task is. When backpropagation is performed using the loss between the task prediction value and the task label value, the gradient reversal layer multiplies the gradient by -1. The feature extractor is trained in a direction in which the task discriminator cannot distinguish task from feature, and at the same time, it is trained to improve the performance of each task network.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Loss Function",
      "text": "When batch data has a ground truth for the task, we calculate the loss between the output of the model and the ground truth. We refers to this as supervision loss. When batch data does not have a ground truth for the task, we calculate the loss between the output of the teacher model called soft label and the output of the student model. We refers to this as distillation loss.\n\nSupervision loss For valence-arousal estimation task, we used Concordance Correlation Coefficient (CCC) loss. CCC is defined as follows:\n\nwhere, y is ground truth and ŷ is prediction value. and σ and µ is standard deviation and means computed over the batch.\n\nThe supervision loss for valence-arousal estimation task is as follows:\n\nFor expression classification task, we use cross entropy loss. cross entropy defined as follows:\n\nC is the number of classes. The supervision loss for expression classification task is as follows:\n\nFor AU detection task, we use binary cross entropy. Binary cross entropy defined as follows:\n\nThe supervision loss for AU detection task L S AU is defined as follows:\n\nThe supervision loss for MTL task L S MT L is sum of each task loss:\n\nDistillation loss To transfer the dark knowledge of the teacher model to the student model, the loss between the output of the teacher model and the output of the student model is calculated. The distillation loss equation for each task is as follows.\n\nMTL tasks have labels for all tasks, so distillation loss of MTL task are not calculated.\n\nTask classification loss Task classification loss is the cross entropy loss between the output from the Task discriminator and the task label one-hot encoded for VA, EXPR, and AU tasks. Task classification loss is defined as follows:\n\nTrain loss The teacher loss of task i is defined using supervision loss and task classification loss.\n\nStudent loss of task i is defined as follows by combining supervision loss, distillation loss and task classification loss.\n\nWhere, Gamma is the task weight for each task proposed by deng et al  [7] . While training, the number of epochs that validation performance is not improved for each task is counted. The more counts, the greater the weight for task loss to boost training. When the counted number of task i is n i , the weight of that task loss is γ i = e 0.5n i . α is the hyperparameter of the weight between supervision loss and distillation loss for tasks with a ground truth label. β is a hyperparameter representing the weight of distillation losses for tasks without a ground truth label. δ refers to weight of task classification loss",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Train Procedure",
      "text": "The teacher model trains to minimize equation (  12 ) consisting of supervision loss and task classification loss without distillation loss. After the training of the teacher model is completed, the student model trains to minimize the equation (  13 ) using soft label from teacher model. Train procedure of the teacher model and student model is described in algorithm 2.1",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Experiments Setting",
      "text": "We used Adam optimizer, the learning rate was set to 0.0001, and the batch size was 256. We train for 20 epochs and stop training if there is no improvement in validation performance for 5 epochs. We set t aud to 10, t img to 2, and s to 6. In Loss function, α was set to 10, and β was set to 0.9. δ was set to 1 or started with 0 and increased linearly to 1 as train progressed. t was set to 2.5. In feature extractor layer, 0.5 random dropout was applied. δ was set to 0.5 or 1, and started with 0 and increased linearly to 1 as train progressed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Metrics",
      "text": "We used metrics suggested in  [10]  to evaluate model performance. The metric of Multi-Task-Learning is the sum of the metrics of valence-arousal, expression, and action unit challenge. The valence-arousal metric is a concordance correlation coefficient (CCC), and the metric of expression recognition is F1 score across all 8 categories (i.e. macro F1 score). The metric of the action unit detection is the average F1 score across all 12 AUs (i.e. macro F1 score). Although we are participating in the multi task learning challenge, validation was also performed on valence-arousal, expression classification, and action unit detection tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Results",
      "text": "We evaluated the teacher and student model to which the knowledge distillation technique was applied using the ABAW validation set and compared it with the model to which the task discriminator was added. Validation results are described in TABLE  I .\n\nIn all models, it can be seen that the Student model is outperformed than the Teacher model. We can see that the dark knowledge transferred by the Teacher model helped improve generalization performance. Domain discriminator However, validation performance tended to decrease when the Domain discriminator was added. It is possible that feature extractor has already extracted task wise feature in the learning process, and task classification loss has had a negative effect on task performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this paper, we proposed a multi stream, multi task model applying knowledge distillation to improve the generalization performance by training with the incomplete label. We tried to extract task independent features by adding a Gradient reversal layer and a task discriminator. In the future, We will study minimizing input data time without performance decline, and further study how to improve performance through the application of task discriminator.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Model architecture",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": ""
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "as a soft\nlabel. We trained the teacher model using only the"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "data with ground truths on the task. After training the teacher"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "model, output of\nthe teacher model\nis used as a soft\nlabel\nto"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "transfer dark knowledge to the student model."
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "Third, we extracted task-wise features by adding gradient"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "reverse\nlayer\nand task discriminator. The\nfeature\nextracted"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "from the feature extractor was made to be task-independent"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "by gradient reversal layer and task discriminator."
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "As a result of applying the above methods, we achieved"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "2.40 performances for the 2022 ABAW validation set, which"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "exceeded baseline performance 0.3 proposed by [10]."
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "Primary contributions of this paper are:"
        },
        {
          "2Department of Automobile and IT Convergence, Kookmin University, Seoul": "•\nWe\ndesigned\na multi-task model\nto\nsimultaneously"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "trained as a supervised learning method, and the student": "model was trained by using the output of the teacher model",
          "As a result of applying the above methods, we achieved": "2.40 performances for the 2022 ABAW validation set, which"
        },
        {
          "trained as a supervised learning method, and the student": "as a soft label. As a result we achieved 2.40 in Multi Task",
          "As a result of applying the above methods, we achieved": "exceeded baseline performance 0.3 proposed by [10]."
        },
        {
          "trained as a supervised learning method, and the student": "Learning task validation dataset.",
          "As a result of applying the above methods, we achieved": "Primary contributions of this paper are:"
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "•\nWe\ndesigned\na multi-task model\nto\nsimultaneously"
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "perform\nValence-Arousal\nestimation,\nExpression"
        },
        {
          "trained as a supervised learning method, and the student": "I. Introduction",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "Classiﬁcation, and Action Unit Detection through Audio"
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "and image multi input."
        },
        {
          "trained as a supervised learning method, and the student": "Human affective behavior\nresearch is\nan important ﬁeld",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "•\nBy applying the knowledge distillation technique, we"
        },
        {
          "trained as a supervised learning method, and the student": "in\nthe\nhuman\ncomputer\ninteraction\nﬁeld,\nand\nis\nactively",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "improved the generalization performance of\nthe multi-"
        },
        {
          "trained as a supervised learning method, and the student": "studied\nwith\nthe\ndevelopment\nof\nbig\ndata\nand\ndeep",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "task model."
        },
        {
          "trained as a supervised learning method, and the student": "learning\ntechnologies. However,\nfor\nseveral\nreasons,\nit\nis",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "•\nWe extracted task-independent\nfeatures by adding task"
        },
        {
          "trained as a supervised learning method, and the student": "difﬁcult\nto apply the effective behavioral\nstudy. To address",
          "As a result of applying the above methods, we achieved": ""
        },
        {
          "trained as a supervised learning method, and the student": "",
          "As a result of applying the above methods, we achieved": "discriminator and Gradient Reverse Layer."
        },
        {
          "trained as a supervised learning method, and the student": "there\nproblems,\n3rd Affective Behavior Analysis\nin-the-",
          "As a result of applying the above methods, we achieved": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data to improve performance [6], [7], [22].": "B. Knowledge distillation"
        },
        {
          "data to improve performance [6], [7], [22].": "The knowledge distillation was proposed by hinton et al"
        },
        {
          "data to improve performance [6], [7], [22].": "[9]. The\noutput\nof\nthe\npretrained\nteacher model\nis\nscaled"
        },
        {
          "data to improve performance [6], [7], [22].": "by\nsoftmax\nfunction with\ntemperature\nand\nused\nas\na\nsoft"
        },
        {
          "data to improve performance [6], [7], [22].": "label\nto train the\nstudent model. The\nstudent model\nlearns"
        },
        {
          "data to improve performance [6], [7], [22].": "inter-class\nsimilarity(dark\nknowledge)\nfrom the\nsoft\nlabel"
        },
        {
          "data to improve performance [6], [7], [22].": "of\nthe\nteacher model\nand achieves performance\nsimilar\nto"
        },
        {
          "data to improve performance [6], [7], [22].": "the teacher model despite being shallower\nthan the teacher"
        },
        {
          "data to improve performance [6], [7], [22].": "model. Zhang et al. [24] also improved performance through"
        },
        {
          "data to improve performance [6], [7], [22].": "a self-distillation technique that conﬁgured the structure of"
        },
        {
          "data to improve performance [6], [7], [22].": "the teacher model and the student model equally. Many teams"
        },
        {
          "data to improve performance [6], [7], [22].": "used the knowledge distillation model at the 2021 ABAW and"
        },
        {
          "data to improve performance [6], [7], [22].": "were also on the leaderboard.\nIn particular, Deng et al.\n[7]"
        },
        {
          "data to improve performance [6], [7], [22].": "made it possible to train deeper dark knowledge using the"
        },
        {
          "data to improve performance [6], [7], [22].": "knowledge distillation technique, the ensemble technique, and"
        },
        {
          "data to improve performance [6], [7], [22].": "the generation technique\nin which a\ntrained student model"
        },
        {
          "data to improve performance [6], [7], [22].": "becomes a teacher model and trains a new student model."
        },
        {
          "data to improve performance [6], [7], [22].": "C. Domain adaptation"
        },
        {
          "data to improve performance [6], [7], [22].": "Domain adaptation techniques proposed by Ganin et\nal."
        },
        {
          "data to improve performance [6], [7], [22].": "[25]\nto improve accuracy in target domains without ground"
        },
        {
          "data to improve performance [6], [7], [22].": "truth through data from source domains with ground truth."
        },
        {
          "data to improve performance [6], [7], [22].": "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto"
        },
        {
          "data to improve performance [6], [7], [22].": "gradient when performing back propagation, and a domain"
        },
        {
          "data to improve performance [6], [7], [22].": "discriminator are added to allow the feature extractor to extract"
        },
        {
          "data to improve performance [6], [7], [22].": "features independent of the domain."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "gradient when performing back propagation, and a domain"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "discriminator are added to allow the feature extractor to extract"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "features independent of the domain."
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": ""
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "III. Problem deﬁnition"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": ""
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "{X,Y } denoted train data. X represents\ninput data and Y"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "represents ground truths. Model function inputs X and outputs"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "Y . For\nthe\nconvenience of notation,\nit\nis\nassumed that\nthe"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "batch size of all\ntasks is N, but even if\nthe number of\ntasks’"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "data is different,\nit can be conﬁgured to be divided by the"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "same number of iterations. X consists of Ximg and Xaud. Ximg"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "represents the facial image and Xaud represents the audio data:"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "X = {Ximg ∈ RN×nimg×H×H×Cimg, Xaud ∈ RN×sr·taud ×Caud }."
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "nimg is the number of input images. H is width and height of"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "the image and Cimg refers to the number of channel of image."
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "In audio data, sr refers to sample rate of audio data. taud means"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "is the number of\naudio data time before prediction time. Caud"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "channel of audio. In this dataset, nhz, frame rate of a video in"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "dataset is 30, sr is 22050 amd Caud is 2. Ximg contains a image"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "for\nthe past\nseconds from the time of prediction. Of\nthe\ntimg"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "images with stride s. H\nimages, we extracted ni\ntotal timg · nhz"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "is 112 and Cimg is 3 in this dataset."
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "Y\nY =\nThe\nground\ntruths\nconsists\nof\nfour\ntypes:"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "{YVA ∈ RN×2,YEXPR ∈ RN×8,YAU ∈ RN×12,YMT L ∈ RN×22}."
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "represents\na\ncontinuous\nvalence\nand\narousal\nin\nthe\nYVA"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "range\nof\n[-1,\nis\na\none-hot\nencoded\nvector\nfor\n1]. YEXPR"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "eight categories of emotions: Neutral, Anger, Disgust, Fear,"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "Happiness, Sadness, Surprise,\nincludes\n12\nand Other. YAU"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "facial action units labels: AU1, AU2, AU4, AU6, AU7, AU10,"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "AU12, AU15, AU23, AU24, AU25 and AU26. Model function"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "is denoted by\nf .\nIn teacher-student\ntrain algorithm,\nteacher"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "model denoted by\nf tea and soft\nlabel\nfrom teacher model\nin"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "f tea\n(X).\ntask i is"
        },
        {
          "Gradient\nReverse\nLayer(GRL), which\nis multiply\n-1\nto": "i"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "prediction value and the task label value, the gradient reversal": "layer multiplies\nthe gradient by -1. The feature extractor\nis",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "task are not calculated."
        },
        {
          "prediction value and the task label value, the gradient reversal": "trained in a direction in which the task discriminator cannot",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "distinguish\ntask\nfrom feature,\nand\nat\nthe\nsame\ntime,\nit\nis",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "Task classiﬁcation loss\nTask classiﬁcation loss is the cross"
        },
        {
          "prediction value and the task label value, the gradient reversal": "trained to improve the performance of each task network.",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "entropy loss between the output\nfrom the Task discriminator"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "and the task label one-hot encoded for VA, EXPR, and AU"
        },
        {
          "prediction value and the task label value, the gradient reversal": "B. Loss function",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "tasks. Task classiﬁcation loss is deﬁned as follows:"
        },
        {
          "prediction value and the task label value, the gradient reversal": "When\nbatch\ndata\nhas\na\nground\ntruth\nfor\nthe\ntask, we",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "calculate the loss between the output of\nthe model and the",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "(11)\nLT = CE( f task(X),Ytask)"
        },
        {
          "prediction value and the task label value, the gradient reversal": "ground truth. We refers to this as supervision loss. When batch",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "data does not have a ground truth for\nthe task, we calculate",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "i\nTrain\nloss\nThe\nteacher\nloss\nof\ntask\nis\ndeﬁned\nusing"
        },
        {
          "prediction value and the task label value, the gradient reversal": "the loss between the output of\nthe teacher model called soft",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "supervision loss and task classiﬁcation loss."
        },
        {
          "prediction value and the task label value, the gradient reversal": "label and the output of the student model. We refers to this as",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "distillation loss.",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "N∑n\nLosstea\n=\n(LS\n(12)\ni\ni + LT\ni )"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "=1"
        },
        {
          "prediction value and the task label value, the gradient reversal": "Supervision loss\nFor valence-arousal\nestimation task, we",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "used Concordance Correlation Coefﬁcient\n(CCC)\nloss. CCC",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "Student\nloss of\ntask i\nis deﬁned as follows by combining"
        },
        {
          "prediction value and the task label value, the gradient reversal": "is deﬁned as follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "supervision loss, distillation loss and task classiﬁcation loss."
        },
        {
          "prediction value and the task label value, the gradient reversal": "2ρσyσ ˆy",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "CCC(y, ˆy) =\n(1)",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "Lossstu\n=\n(13)\n{γi · (α · LS\nβ · γ j · LD\ni + LD\ni ) + δ · LT"
        },
        {
          "prediction value and the task label value, the gradient reversal": "σ 2\ny + σ 2",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "N∑n\ni }\ni + ∑"
        },
        {
          "prediction value and the task label value, the gradient reversal": "y + (µy − µ ˆy)2",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "i"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "=1\nj(cid:54)=i"
        },
        {
          "prediction value and the task label value, the gradient reversal": "where, y is ground truth and ˆy is prediction value. and σ and",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "Where, Gamma is the task weight\nfor each task proposed"
        },
        {
          "prediction value and the task label value, the gradient reversal": "µ is standard deviation and means computed over the batch.",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "by\ndeng\net\nal\n[7]. While\ntraining,\nthe\nnumber\nof\nepochs"
        },
        {
          "prediction value and the task label value, the gradient reversal": "The supervision loss for valence-arousal estimation task is as",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "that validation performance is not\nimproved for each task is"
        },
        {
          "prediction value and the task label value, the gradient reversal": "follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "counted. The more counts, the greater the weight for task loss"
        },
        {
          "prediction value and the task label value, the gradient reversal": "LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "(2)\nVA (X))\nVA = 1 −CCC(YVA, f tea",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "to boost training. When the counted number of task i is ni, the"
        },
        {
          "prediction value and the task label value, the gradient reversal": "For\nexpression classiﬁcation task, we use\ncross\nentropy",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "weight of that\ntask loss is γi = e0.5ni. α is the hyperparameter"
        },
        {
          "prediction value and the task label value, the gradient reversal": "loss. cross entropy deﬁned as follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "of\nthe weight between supervision loss and distillation loss"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "for\nis a hyperparameter\ntasks with a ground truth label. β"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "representing the weight of distillation losses for tasks without"
        },
        {
          "prediction value and the task label value, the gradient reversal": "C∑c\nCE(y, ˆy) = −\nylog( ˆy)\n(3)",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "task classiﬁcation\na ground truth label. δ refers to weight of"
        },
        {
          "prediction value and the task label value, the gradient reversal": "=1",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "loss"
        },
        {
          "prediction value and the task label value, the gradient reversal": "C is the number of classes. The supervision loss for expression",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "C. Train procedure"
        },
        {
          "prediction value and the task label value, the gradient reversal": "classiﬁcation task is as follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "The\nteacher model\ntrains\nto minimize\nequation\n(12)"
        },
        {
          "prediction value and the task label value, the gradient reversal": "LS\n(4)\nEXPR = CE(YEXPR, f tea\nEXPR(X))",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "consisting\nof\nsupervision\nloss\nand\ntask\nclassiﬁcation\nloss"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "without\ndistillation\nloss. After\nthe\ntraining\nof\nthe\nteacher"
        },
        {
          "prediction value and the task label value, the gradient reversal": "For AU detection task, we use binary cross entropy. Binary",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "model\nis\ncompleted,\nthe\nstudent model\ntrains\nto minimize"
        },
        {
          "prediction value and the task label value, the gradient reversal": "cross entropy deﬁned as follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "the equation (13) using soft\nlabel\nfrom teacher model. Train"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "procedure of the teacher model and student model is described"
        },
        {
          "prediction value and the task label value, the gradient reversal": "BCE(y, ˆy) = [ylog( ˆy) + (1 − y)log(1 − ˆy)]\n(5)",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "in algorithm 2.1"
        },
        {
          "prediction value and the task label value, the gradient reversal": "The supervision loss for AU detection task LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "AU is deﬁned as",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "V\n. Experiments"
        },
        {
          "prediction value and the task label value, the gradient reversal": "follows:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "(6)\nAU (X))\nAU = BCE(YAU , f tea",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "A. Experiments setting"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "We\nused Adam optimizer,\nthe\nlearning\nrate was\nset\nto"
        },
        {
          "prediction value and the task label value, the gradient reversal": "The supervision loss for MTL task LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "MT L is sum of each task",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "0.0001, and the batch size was 256. We train for 20 epochs"
        },
        {
          "prediction value and the task label value, the gradient reversal": "loss:",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "and stop training if\nthere\nis no improvement\nin validation"
        },
        {
          "prediction value and the task label value, the gradient reversal": "LS\nEXPR + LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "(7)\nAU\nVA + LS",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "performance for 5 epochs. We set\nto 10,\nto 2, and s\ntimg\ntaud"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "to 0.9.\nto 6. In Loss function, α was set\nto 10, and β was set"
        },
        {
          "prediction value and the task label value, the gradient reversal": "Distillation\nloss\nTo\ntransfer\nthe\ndark\nknowledge\nof\nthe",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "to 1 or started with 0 and increased linearly to 1 as\nδ was set"
        },
        {
          "prediction value and the task label value, the gradient reversal": "teacher model\nto the\nstudent model,\nthe\nloss between the",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "train progressed. t was set to 2.5. In feature extractor layer, 0.5"
        },
        {
          "prediction value and the task label value, the gradient reversal": "output of the teacher model and the output of the student model",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "random dropout was applied. δ was set to 0.5 or 1, and started"
        },
        {
          "prediction value and the task label value, the gradient reversal": "is calculated. The distillation loss equation for each task is as",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "with 0 and increased linearly to 1 as train progressed."
        },
        {
          "prediction value and the task label value, the gradient reversal": "follows.",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "LD",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "(8)\nVA = 1 −CCC( f stu\nVA (X), f tea\nVA (X))",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "B. Metrics"
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "We\nused metrics\nsuggested\nin\n[10]\nto\nevaluate model"
        },
        {
          "prediction value and the task label value, the gradient reversal": "LD\n(9)\nEXPR = CE( f stu\nEXPR(X), f tea\nEXPR(X))",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "performance. The metric of Multi-Task-Learning is the sum"
        },
        {
          "prediction value and the task label value, the gradient reversal": "LD\nAU = BCE( f stu",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": ""
        },
        {
          "prediction value and the task label value, the gradient reversal": "(10)\nAU (X), f tea\nAU (X))",
          "MTL tasks have labels for all tasks, so distillation loss of MTL": "of\nthe metrics\nof\nvalence-arousal,\nexpression,\nand\naction"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "unit challenge. The valence-arousal metric is a concordance": "correlation coefﬁcient\n(CCC), and the metric of expression"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "recognition is F1 score across all 8 categories (i.e. macro F1"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "score). The metric of the action unit detection is the average"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "F1 score across all 12 AUs (i.e. macro F1 score). Although"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "we\nare\nparticipating\nin\nthe multi\ntask\nlearning\nchallenge,"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "validation was also performed on valence-arousal, expression"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "classiﬁcation, and action unit detection tasks."
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "C. Results"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "We evaluated the teacher and student model\nto which the"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "knowledge distillation technique was applied using the ABAW"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "validation set and compared it with the model\nto which the"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "task discriminator was added. Validation results are described"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "in TABLE I."
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "In all models,\nit\ncan be\nseen that\nthe Student model\nis"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "outperformed than the Teacher model. We can see that the dark"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "knowledge transferred by the Teacher model helped improve"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "generalization performance. Domain discriminator However,"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "validation performance tended to decrease when the Domain"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "discriminator was added.\nIt\nis possible that\nfeature extractor"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "has already extracted task wise feature in the learning process,"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "and task classiﬁcation loss has had a negative effect on task"
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": "performance."
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        },
        {
          "unit challenge. The valence-arousal metric is a concordance": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and further\nstudy how to improve performance through the": "application of task discriminator."
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "REFERENCES"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "[1]\nGeesung\nOh,\nEuiseok\nJeong,\nand\nSejoon\nLim."
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "Causal\naffect\nprediction model\nusing\na\npast\nfacial"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "the\nIEEE/CVF\nimage\nsequence.\nIn Proceedings of"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "International Conference on Computer Vision, pages"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "3550–3556, 2021."
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "[2]\nChien Shing Ooi, Kah Phooi Seng, Li-Minn Ang,"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "and Li Wern Chew.\nA new approach\nof\naudio"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "emotion recognition. Expert systems with applications,"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "41(13):5858–5869, 2014."
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "[3]\nRuhul\nAmin\nKhalil,\nEdward\nJones,"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "Mohammad\nInayatullah\nBabar,\nTariqullah\nJan,"
        },
        {
          "and further\nstudy how to improve performance through the": ""
        },
        {
          "and further\nstudy how to improve performance through the": "Mohammad Haseeb Zafar,\nand Thamer Alhussain."
        },
        {
          "and further\nstudy how to improve performance through the": "Speech\nemotion\nrecognition\nusing\ndeep\nlearning"
        },
        {
          "and further\nstudy how to improve performance through the": "IEEE\ntechniques: A review.\nAccess,\n7:117327–"
        },
        {
          "and further\nstudy how to improve performance through the": "117345, 2019."
        },
        {
          "and further\nstudy how to improve performance through the": "[4]\nAdnan\nMehmood\nBhatti,\nMuhammad\nMajid,"
        },
        {
          "and further\nstudy how to improve performance through the": "Syed Muhammad Anwar,\nand Bilal Khan.\nHuman"
        },
        {
          "and further\nstudy how to improve performance through the": "emotion recognition and analysis in response to audio"
        },
        {
          "and further\nstudy how to improve performance through the": "Computers\nin Human\nmusic\nusing\nbrain\nsignals."
        },
        {
          "and further\nstudy how to improve performance through the": "Behavior, 65:267–275, 2016."
        },
        {
          "and further\nstudy how to improve performance through the": "[5]\nFelix Weninger, Florian Eyben, Björn W Schuller,"
        },
        {
          "and further\nstudy how to improve performance through the": "Marcello Mortillaro,\nand Klaus R Scherer.\nOn the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Teacher\n0.64/0.56/0.51/2.08": "0.63/0.57/0.51/2.40\nStudent",
          "0.58/0.51/0.47/1.76": "0.63/0.58/0.52/2.38",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "0.61/0.53/0.50/2.36\n0.59/0.51/0.51/2.24"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "acoustics of\nemotion in audio: what\nspeech, music,",
          "0.58/0.51/0.47/1.76": "[16]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Dimitrios Kollias and Stefanos Zafeiriou. Expression,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "and sound have in common. Frontiers in psychology,",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "affect,\naction\nunit\nrecognition:\nAff-wild2, multi-"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "4:292, 2013.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "arXiv\npreprint\ntask\nlearning\nand\narcface."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "arXiv:1910.04855, 2019."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Felix Kuhnke, Lars Rumberg,\nand Jörn Ostermann.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Two-stream aural-visual\naffect\nanalysis\nin the wild.",
          "0.58/0.51/0.47/1.76": "[17]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Dimitrios Kollias\nand\nStefanos Zafeiriou.\nAffect"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "2020\n15th\nIEEE\nInternational\nConference\non\nIn",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "analysis\nin-the-wild:\nValence-arousal,\nexpressions,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Automatic Face and Gesture Recognition (FG 2020),",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "arXiv preprint\naction units and a uniﬁed framework."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "pages 600–605. IEEE, 2020.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "arXiv:2103.15792, 2021."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[18]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Stefanos\nZafeiriou,\nDimitrios\nKollias, Mihalis\nA"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Didan Deng, Liang Wu, and Bertram E Shi.\nIterative",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Nicolaou, Athanasios\nPapaioannou, Guoying Zhao,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "distillation for better uncertainty estimates in multitask",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "and Irene Kotsia. Aff-wild: valence and arousal’in-the-"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "emotion recognition.\nIn Proceedings of the IEEE/CVF",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "wild’challenge. In Proceedings of the IEEE conference"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "International Conference on Computer Vision, pages",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "on computer vision and pattern recognition workshops,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "3557–3566, 2021.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "pages 34–41, 2017."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Yusuf Aytar, Carl Vondrick,\nand Antonio Torralba.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[19]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Didan Deng, Zhaokang Chen,\nand Bertram E Shi."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Soundnet:\nLearning\nsound\nrepresentations\nfrom",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Multitask emotion recognition with incomplete labels."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Advances\nin\nneural\ninformation\nunlabeled\nvideo.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "2020\n15th\nIEEE\nInternational\nConference\non\nIn"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "processing systems, 29, 2016.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Automatic Face and Gesture Recognition (FG 2020),"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Geoffrey Hinton, Oriol Vinyals,\nJeff Dean,\net\nal.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "pages 592–599. IEEE, 2020."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv\nDistilling the knowledge in a neural network.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[20]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Sachihiro\nYouoku,\nYuushi\nToyoda,\nTakahisa"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "preprint arXiv:1503.02531, 2(7), 2015.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Yamamoto,\nJunya\nSaito,\nRyosuke\nKawamura,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias. Abaw: Valence-arousal estimation,",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Xiaoyu Mi, and Kentaro Murase.\nA multi-term and"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "expression\nrecognition,\naction\nunit\ndetection\n&",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "multi-task analyzing framework for affective analysis"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv\npreprint\nmulti-task\nlearning\nchallenges.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "in-the-wild. arXiv preprint arXiv:2009.13885, 2020."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv:2202.10659, 2022.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[21]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias,\nIrene Kotsia, Elnar Hajiyev,\nand",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Zhimeng Zhang, and Yu Ding.\nPrior aided streaming"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Stefanos\nZafeiriou.\nAnalysing\naffective\nbehavior",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "network for multi-task affective recognitionat\nthe 2nd"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv\npreprint\nin\nthe\nsecond\nabaw2\ncompetition.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "abaw2 competition. arXiv preprint arXiv:2107.03708,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv:2106.15318, 2021.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "2021."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias, Attila Schulc, Elnar Hajiyev,\nand",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[22]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Su Zhang, Yi Ding, Ziquan Wei,\nand Cuntai Guan."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Stefanos\nZafeiriou.\nAnalysing\naffective\nbehavior",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Continuous\nemotion\nrecognition with\naudio-visual"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "in the ﬁrst\nabaw 2020 competition.\nIn 2020 15th",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "leader-follower\nattentive\nfusion.\nIn Proceedings of"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "IEEE International Conference on Automatic Face and",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "the IEEE/CVF International Conference on Computer"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Gesture Recognition (FG 2020), pages 637–643. IEEE,",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Vision, pages 3567–3574, 2021."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "2020.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "[23]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Sachihiro\nYouoku,\nTakahisa\nYamamoto,\nJunya"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Saito, Akiyoshi Uchida, Xiaoyu Mi, Ziqiang\nShi,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Zafeiriou. Face behavior a la carte: Expressions, affect",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Liu Liu, Zhongling Liu, Osafumi Nakayama,\nand"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv preprint\nand action units in a single network.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": ""
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Kentaro Murase.\nMulti-modal affect analysis using"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv:1910.11111, 2019.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "arXiv"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "standardized data within subjects in the wild."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "preprint arXiv:2107.03009, 2021."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Zafeiriou.\nDistribution matching for heterogeneous",
          "0.58/0.51/0.47/1.76": "[24]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "arXiv\nmulti-task learning:\na\nlarge-scale\nface\nstudy.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Chenglong Bao,\nand Kaisheng Ma.\nBe your own"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "preprint arXiv:2105.03790, 2021.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "teacher:\nImprove\nthe\nperformance\nof\nconvolutional"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Dimitrios Kollias,\nPanagiotis\nTzirakis, Mihalis A",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "neural networks via self distillation.\nIn Proceedings of"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Nicolaou, Athanasios\nPapaioannou, Guoying Zhao,",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "the IEEE/CVF International Conference on Computer"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Björn Schuller,\nIrene Kotsia, and Stefanos Zafeiriou.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Vision, pages 3713–3722, 2019."
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "Deep affect prediction in-the-wild: Aff-wild database",
          "0.58/0.51/0.47/1.76": "[25]",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "Yaroslav Ganin and Victor Lempitsky. Unsupervised"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "and\nchallenge,\ndeep\narchitectures,\nand\nbeyond.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "domain\nadaptation\nby\nbackpropagation.\nIn"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "International Journal of Computer Vision, 127(6):907–",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "International\nconference\non\nmachine\nlearning,"
        },
        {
          "Teacher\n0.64/0.56/0.51/2.08": "929, 2019.",
          "0.58/0.51/0.47/1.76": "",
          "0.59/0.46/0.47/1.81\n0.61/0.41/0.39/1.69": "pages 1180–1189. PMLR, 2015."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Yang, Sungwook Hwang, Sangho Lee,\nand Sejoon"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Lim. Drer: Deep learning–based driver’s real emotion"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "recognizer. Sensors, 21(6):2166, 2021."
        },
        {
          "[26]": "[27]",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Ali Mollahosseini, Behzad Hasani, and Mohammad H"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Mahoor. Affectnet: A database for\nfacial expression,"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "IEEE\nvalence,\nand arousal\ncomputing in the wild."
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Transactions\non Affective Computing,\n10(1):18–31,"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "2017."
        },
        {
          "[26]": "[28]",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Yuma Kajihara,\nShoya Dozono,\nand Nao\nTokui."
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "Imaginary\nsoundscape:\nCross-modal\napproach\nto"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "generate pseudo sound environments.\nIn Proceedings"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "of\nthe Workshop on Machine Learning for Creativity"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "and Design (NIPS 2017), Long Beach, CA, USA, pages"
        },
        {
          "[26]": "",
          "Geesung Oh, Junghwan Ryu, Euiseok Jeong, Ji Hyun": "1–3, 2017."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Causal affect prediction model using a past facial image sequence",
      "authors": [
        "Geesung Oh",
        "Euiseok Jeong",
        "Sejoon Lim"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "A new approach of audio emotion recognition. Expert systems with applications",
      "authors": [
        "Shing Chien",
        "Kah Ooi",
        "Li-Minn Phooi Seng",
        "Li Ang",
        "Chew"
      ],
      "year": "2014",
      "venue": "A new approach of audio emotion recognition. Expert systems with applications"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Human emotion recognition and analysis in response to audio music using brain signals",
      "authors": [
        "Adnan Mehmood Bhatti",
        "Muhammad Majid",
        "Syed Muhammad Anwar",
        "Bilal Khan"
      ],
      "year": "2016",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "5",
      "title": "On the TABLE I Validation performance of teacher, student model. Model Performance",
      "authors": [
        "Felix Weninger",
        "Florian Eyben",
        "W Björn",
        "Marcello Schuller",
        "Klaus Mortillaro",
        "Scherer"
      ],
      "venue": "On the TABLE I Validation performance of teacher, student model. Model Performance"
    },
    {
      "citation_id": "6",
      "title": "acoustics of emotion in audio: what speech, music, and sound have in common",
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "7",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "8",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "Didan Deng",
        "Liang Wu",
        "Bertram Shi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Soundnet: Learning sound representations from unlabeled video",
      "authors": [
        "Yusuf Aytar",
        "Carl Vondrick",
        "Antonio Torralba"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "11",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "12",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Irene Kotsia",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition",
      "arxiv": "arXiv:2106.15318"
    },
    {
      "citation_id": "13",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "14",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "15",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "16",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multitask learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multitask learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "18",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "19",
      "title": "Aff-wild: valence and arousal'in-thewild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "20",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "21",
      "title": "A multi-term and multi-task analyzing framework for affective analysis in-the-wild",
      "authors": [
        "Sachihiro Youoku",
        "Yuushi Toyoda",
        "Takahisa Yamamoto",
        "Junya Saito",
        "Ryosuke Kawamura",
        "Xiaoyu Mi",
        "Kentaro Murase"
      ],
      "year": "2020",
      "venue": "A multi-term and multi-task analyzing framework for affective analysis in-the-wild",
      "arxiv": "arXiv:2009.13885"
    },
    {
      "citation_id": "22",
      "title": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "authors": [
        "Wei Zhang",
        "Zunhu Guo",
        "Keyu Chen",
        "Lincheng Li",
        "Zhimeng Zhang",
        "Yu Ding"
      ],
      "year": "2021",
      "venue": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "23",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Su Zhang",
        "Yi Ding",
        "Ziquan Wei",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal affect analysis using standardized data within subjects in the wild",
      "authors": [
        "Sachihiro Youoku",
        "Takahisa Yamamoto",
        "Junya Saito",
        "Akiyoshi Uchida",
        "Xiaoyu Mi",
        "Ziqiang Shi",
        "Liu Liu",
        "Zhongling Liu",
        "Osafumi Nakayama",
        "Kentaro Murase"
      ],
      "year": "2021",
      "venue": "Multi-modal affect analysis using standardized data within subjects in the wild",
      "arxiv": "arXiv:2107.03009"
    },
    {
      "citation_id": "25",
      "title": "Be your own teacher: Improve the performance of convolutional neural networks via self distillation",
      "authors": [
        "Linfeng Zhang",
        "Jiebo Song",
        "Anni Gao",
        "Jingwei Chen",
        "Chenglong Bao",
        "Kaisheng Ma"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "27",
      "title": "Drer: Deep learning-based driver's real emotion recognizer",
      "authors": [
        "Geesung Oh",
        "Junghwan Ryu",
        "Euiseok Jeong",
        "Hyun Yang",
        "Sungwook Hwang",
        "Sangho Lee",
        "Sejoon Lim"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "28",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Imaginary soundscape: Cross-modal approach to generate pseudo sound environments",
      "authors": [
        "Yuma Kajihara",
        "Shoya Dozono",
        "Nao Tokui"
      ],
      "year": "2017",
      "venue": "Proceedings of the Workshop on Machine Learning for Creativity and Design"
    }
  ]
}