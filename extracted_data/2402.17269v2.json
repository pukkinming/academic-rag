{
  "paper_id": "2402.17269v2",
  "title": "Curriculum Learning Meets Directed Acyclic Graph For Multimodal Emotion Recognition",
  "published": "2024-02-27T07:28:05Z",
  "authors": [
    "Cam-Van Thi Nguyen",
    "Cao-Bach Nguyen",
    "Quang-Thuy Ha",
    "Duc-Trong Le"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Curriculum Learning",
    "Directed Acyclic Graph"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models. We release the code for MultiDAG+CL and experiments: https://github.com/vanntc711/MultiDAG-CL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Online social networks' growing popularity has sparked interest in capturing emotions in conversations. Emotion Recognition in Conversation (ERC) has emerged as a critical task in various domains such as chatbots  (Ghosh et al., 2017) , healthcare  (Li et al., 2019) , and social media analysis  (Polzin and Waibel, 2000) . In the field of ERC, researchs can be broadly categorized into unimodal and multimodal approaches. Unimodal approaches usually focus on using text as the main modality for emotion recognition. Several models have been proposed in the past to tackle unimodal ERC task. DialogueRNN  (Majumder et al., 2019)  introduces a recurrent network to track speaker states and context during the conversation. DialogueGCN  (Ghosal et al., 2019)  utilizes graph structures to combine contextual dependencies.\n\nMultimodal Emotion Recognition in Conversation (Multimodal ERC) classifies emotions in conversation turns using text, audio, and visual cues. By incorporating multiple modalities, it provides a comprehensive representation of emotional expressions, including tone of voice, facial expressions, and body language, resulting in improved accuracy and robustness in emotion recognition compared to traditional unimodal ERC approaches. Several models have been proposed to address the task of multimodal ERC. The MFN  (Zadeh et al., 2018)  synchronizes multimodal sequences using a multiview gated memory. ICON  (Hazarika et al., 2018)  provides conversational features from modalities through multi-hop memories. The bc-LSTM  (Poria et al., 2017)  leverages an utterance-level LSTM to capture multimodal features. MMGCN  (Hu et al., 2021)  uses a graph-based fusion module to cap-ture intra-and inter-modality contextual features. CTNet  (Lian et al., 2021)  utilizes a transformerbased structure to model interactions among multimodal features.  CORECT (Nguyen et al., 2023)  leverages relational temporal GNNs with crossmodality interaction support, effectively capturing conversation-level interactions and utterance-level temporal relations.\n\nA Directed Acyclic Graph (DAG) is a directed graph without any directed cycles, comprising vertices and edges, where each edge is directed from one vertex to another, ensuring no closed loops. Building upon this concept,  Yu et al. (2019)  introduced Directed Acyclic Graph Neural Network (DAG-GNN). Additionally,  Shen et al. (2021)  presented DAG-ERC, a model combining graph-based and recurrence-based neural architectures to capture information flow in long-distance conversations. However, DAG-ERC's focus has been primarily on unimodal text data, with limited exploration in other modalities. Curriculum Learning (CL), inspired by human learning, progressively introduces more complex concepts starting from a simple initial state. It establishes a sequence of curricula where the best curriculum with the simplest examples is used to train the classifier in each learning round  (Bengio et al., 2009; Soviany et al., 2022)   In this paper, we proposes MultiDAG+CL, a multimodal model inspired by DAG-ERC  (Shen et al., 2021) , designed to overcome the limitations of text-based approaches. It integrates multimodal features using DAG-GNN, enabling a comprehensive understanding of emotions in conversations. Leveraging Curriculum Learning, our model, Multi-DAG+CL, addresses emotional shift issues and imbalanced data, significantly enhancing ERC model performance on IEMOCAP and MELD datasets. Notably, we are the first to integrate multimodal ERC models with Curriculum Learning strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "Consider a conversation C having utterances {u 1 , u 2 , . . . , u N } where N is the number of utterances. An utterance is a coherent piece of information conveyed by a single participant p m at a specific moment, where m ≥ 2. The task of Emotion Recognition in Conversation (ERC) is to predict emotion label of each utterance u i with predefined emotion label set E = {y 1 , y 2 , . . . , y r }. Following the multimodal approach, we represent an utterance in terms of three different modalities: audio (a), visual (v), and textual (l). The raw feature representation of utterance",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multidag+Cl Consists Of Two Core Components:",
      "text": "MultiDAG and Curriculum Learning-CL. The Mul-tiDAG component represents the model that combines multimodal features without CL integration. The -CL component is where Curriculum Learning is incorporated to enhance model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Erc With Directed",
      "text": "Acyclic Graph -MultiDAG",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modality Encoder",
      "text": "We use modality-specific encoders to generate context-aware utterance feature encoding. For the textual modality, a bidirectional LSTM network captures sequential textual context information, while a Fully Connected Network is used for the acoustic and visual modalities as follows:\n\n(1) where Enc A , Enc V , Enc L are modality encoder for audio, visual, textual modalities, respectively. These encoders generate the context-aware raw feature encodings h a i , h v i , h l i accordingly. The multimodal feature vector for an utterance u i(mm) corresponding to available modalities is:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multidag Construction",
      "text": "Each utterance in a conversation receives information exclusively from past utterances. This oneway information flow is effectively represented by a Directed Acyclic Graph (DAG), where information moves from predecessors to successors. This characteristic allows the DAG to gather information for a query utterance not only from neighboring utterances but also from more distant ones. Following the multimodal representation input, we initialize the Directed Acyclic Graph Gated Neural Network (DAG-GNN)  (Yu et al., 2019) . The integration of both remote and local information is executed in a manner analogous to the approach undertaken in DAG-ERC by  Shen et al. (2021) . The comprehensive architecture of MultiDAG is visually represented in Figure  1 .\n\nAt each layer l of the MultiDAG, the hidden state of the utterances is continuously computed from the first utterance to the last utterance. For each utterance u i(mm) , the attention weight between u i(mm) and the preceding nodes is calculated by using the hidden state of u i(mm) at layer l -1 to attend to the hidden states of the nodes at layer l:\n\nHere, S denotes the Softmax function; N i(mm) represents the set of preceding nodes leading to u i , W l a is a trainable weight matrix, H l-1 i(mm) is the hidden state of u i(mm) at layer l -1, and || denotes concatenation.\n\nThe attention weight is further utilized in combination with edge relationships to aggregate information.\n\nwhere W l rij ∈ {W l 0 , W l 1 } are trainable parameters. The 0/1 value represents the edge relationship, distinguishing different or same speakers.\n\nThe aggregated information M l i(mm) interacts with the previous layer's hidden state of u i(mm) , H l-1 i(mm) , through a GRU to generate the final hidden state Hl i(mm) at the current layer:\n\nwhere H l-1 i(mm) , M l i(mm) , and Hl i(mm) represent the input, hidden state, and output of the GRU network, respectively. This step is the node information unit. Another GRU serves as the context information unit, modeling the flow of information from the historical context through a layer. In this unit, the roles of H l-1 i and M l i in the GRU are exchanged, where H l-1 i(mm) controls the propagation of M l i(mm) :\n\nThe hidden states of u i from all layers are concatenated together to create final representation:\n\nThis representation is then passed through a Feed-Forward Network to perform emotion prediction.\n\nThe objective function used to train the model is the cross-entropy loss function.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Curriculum Learning -Cl",
      "text": "We design a Difficulty Measure Function (DMF) based on the frequency of emotional shift in conversations, and simultaneously construct a Training Scheduler to implement the training process according to the predefined learning curriculum.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Difficulty Measure Function (Dmf)",
      "text": "When\n\nfor return M * of emotional shift. Here, an emotional shift is defined as occurring when the emotion expressed in two consecutive utterances by the same speaker is different. Specifically, e(u\n\nHere, e(u i ) and e(u k ) is the emotions of two consecutive utterances u i and u k , respectively. The more frequent the emotional shift occur in a conversation, the more it is considered difficult. Therefore, the difficulty of i-th conversation c i is as follows:\n\nwhere N shif t (c i ) and N u (c i ) represent the number of emotional shift in conversation c i and the total number of utterances in c i , respectively. N sp (c i ) is the number of speakers appearing in conversation c i and acts as a smoothing factor. The algorithm for calculating the difficulty of the conversation is fully described in the Algorithm 1.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets And Baselines",
      "text": "We evaluate our approach on the following two ERC datasets: IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2019) . The detailed statistics of the datasets are reported in Table  2 . For the data processing, we use the same split as the work in  (Hu et al., 2021) . We compare our method against several state-of-the-art baselines, including unimodal and multimodal learning approaches. (Due to the space limit, they are brief described in Section 1). The evaluation metrics used are Accuracy (Acc.) and weighted average F1-score (w-F1).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implementation Details",
      "text": "We perform hyperparameter tuning for our proposed model on each dataset using hold-out validation with separate validation sets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Comparision With Baselines",
      "text": "We conducted a comprehensive comparison of our proposed approach with SOTA multimodal ERC methods, and the results are summarized in Table 1. Due to space constraints, we only report Acc. and w-F1 for the MELD dataset. Our approach, MultiDAG+CL, which combines the Mul-tiDAG model with a curriculum learning strategy, achieves SOTA performance on both the IEMO-CAP and MELD datasets. MultiDAG+CL outperforms previous SOTAs by 1.05% (DAG-ERC on IEMOCAP) and 0.34% (DAG-ERC on MELD), respectively. Specifically, our models achieve improvements in individual emotion recognition tasks in most cases, especially for the Sad, Neutral and Angry emotions. In the meantime, we find Happy, Sad, and Angry emotions can be confused with the Neutral emotion in some cases (as shown in Fig.  2 ). Such phenomenon is related to imbalanced class distribution.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Effect Of Modality",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Effect Of Curriculum Learning",
      "text": "The MultiDAG+CL model demonstrates notable performance improvement by incorporating curriculum learning for both the IEMOCAP and MELD datasets. The effectiveness of curriculum learning relies on factors like the difficulty measure design and training strategy, including the number of buckets in the training set. We perform experiments to select the optimal number of buckets in the CL training scheduler. The results shown in the Table  4 , indicate that for the IEMOCAP dataset, the optimal number of buckets is 5, while for the MELD dataset, it is 12. These findings suggest that the CL strategy is effective in improving the performance of the MultiDAG model on both datasets, with the specific number of buckets tailored to each dataset's representations. In summary, our proposed MultiDAG+CL model with curriculum learning, significantly contribute to the achieved results.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Performance For Emotion-Shift",
      "text": "From the confusion matrices of the MultiDAG and MultiDAG+CL models (Figure  2 ), it can be observed that the prediction accuracy for the \"Happy\", \"Neutral\", \"Sad\", and \"Angry\" labels is improved when CL is incorporated into the model. Particularly, the misclassification rate of the \"Neutral\" label as \"Disgust\" decreases significantly from 19.3% in the MultiDAG model to only 12.3% in MultiDAG+CL.\n\nHowever, the prediction accuracy for the \"Disgust\" and \"Happy\" labels decreases.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Excited",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall structure of MultiDAG.",
      "page": 2
    },
    {
      "caption": "Figure 1: At each layer l of the MultiDAG, the hidden state",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Such phenomenon is related to imbalanced",
      "page": 4
    },
    {
      "caption": "Figure 2: ), it can be ob-",
      "page": 5
    },
    {
      "caption": "Figure 2: The confusion matrices on the IEMOCAP.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 3: compares the performance of MultiDAG",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "Happy",
          "MELD": "Acc.\n(%)"
        },
        {
          "Model": "bc-LSTM (Poria et al., 2017)",
          "IEMOCAP": "33.82",
          "MELD": "59.62"
        },
        {
          "Model": "MFN (Zadeh et al., 2018)",
          "IEMOCAP": "48.19",
          "MELD": "60.80"
        },
        {
          "Model": "ICON (Hazarika et al., 2018)",
          "IEMOCAP": "32.80",
          "MELD": "58.20"
        },
        {
          "Model": "DialogueRNN (Majumder et al., 2019)",
          "IEMOCAP": "32.20",
          "MELD": "60.31"
        },
        {
          "Model": "DialogueGCN (Ghosal et al., 2019)",
          "IEMOCAP": "51.57",
          "MELD": "58.62"
        },
        {
          "Model": "DAG-ERC (Shen et al., 2021)",
          "IEMOCAP": "47.59",
          "MELD": "61.04"
        },
        {
          "Model": "MMGCN (Hu et al., 2021)",
          "IEMOCAP": "45.14",
          "MELD": "60.42"
        },
        {
          "Model": "CTNet (Lian et al., 2021)",
          "IEMOCAP": "51.3",
          "MELD": "62.0"
        },
        {
          "Model": "DAG-ERC+HCL (Yang et al., 2022)",
          "IEMOCAP": "-",
          "MELD": "-"
        },
        {
          "Model": "COGMEN (Joshi et al., 2022)",
          "IEMOCAP": "-",
          "MELD": "-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Results of MultiDAG and MultiDAG+CL",
      "data": [
        {
          "IEMOCAP": "Number of buckets\nw-F1",
          "MELD": "Number of buckets\nw-F1"
        },
        {
          "IEMOCAP": "4\n68.05",
          "MELD": "5\n63.94"
        },
        {
          "IEMOCAP": "5\n69.08",
          "MELD": "8\n63.83"
        },
        {
          "IEMOCAP": "7\n68.84",
          "MELD": "10\n63.89"
        },
        {
          "IEMOCAP": "10\n68.38",
          "MELD": "12\n64.00"
        },
        {
          "IEMOCAP": "15\n68.36",
          "MELD": "14\n63.96"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Curriculum learning",
      "authors": [
        "Yoshua Bengio",
        "Jérôme Louradour",
        "Ronan Collobert",
        "Jason Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "4",
      "title": "Affect-LM: A neural language model for customizable affective text generation",
      "authors": [
        "Sayan Ghosh",
        "Mathieu Chollet",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1059"
    },
    {
      "citation_id": "5",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "6",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "7",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "8",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia",
        "Yaohua Bu",
        "Sheng Zhao",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "9",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "11",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "Cam Van",
        "Thi Nguyen",
        "Tuan Mai",
        "Son The",
        "Dang Kieu",
        "Duc-Trong Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.937"
    },
    {
      "citation_id": "12",
      "title": "Emotion-sensitive human-computer interfaces",
      "authors": [
        "S Thomas",
        "Alexander Polzin",
        "Waibel"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "13",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "14",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "15",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "16",
      "title": "Curriculum learning: A survey",
      "authors": [
        "Petru Soviany",
        "Tudor Radu",
        "Paolo Ionescu",
        "Nicu Rota",
        "Sebe"
      ],
      "year": "2022",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "DAG-GNN: DAG structure learning with graph neural networks",
      "authors": [
        "Yue Yu",
        "Jie Chen",
        "Tian Gao",
        "Mo Yu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    }
  ]
}