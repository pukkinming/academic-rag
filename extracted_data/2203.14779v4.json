{
  "paper_id": "2203.14779v4",
  "title": "A Joint Cross-Attention Model For Audio-Visual Fusion In Dimensional Emotion Recognition",
  "published": "2022-03-28T14:09:43Z",
  "authors": [
    "R. Gnana Praveen",
    "Wheidima Carneiro de Melo",
    "Nasib Ullah",
    "Haseeb Aslam",
    "Osama Zeeshan",
    "Théo Denorme",
    "Marco Pedersoli",
    "Alessandro Koerich",
    "Simon Bacon",
    "Patrick Cardinal",
    "Eric Granger"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition has recently gained much attention since it can leverage diverse and complementary relationships over multiple modalities, such as audio, visual, and biosignals. Most state-of-the-art methods for audio-visual (A-V) fusion rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos. Specifically, we propose a joint cross-attention model that relies on the complementary relationships to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. The proposed fusion model efficiently leverages the inter-modal relationships, while reducing the heterogeneity between features. In particular, it computes cross-attention weights based on the correlation between joint feature representations, and that of individual modalities. By deploying a joint A-V feature representation into the crossattention module, the performance of our fusion module improves significantly over the vanilla cross-attention module. Experimental results 1 on the AffWild2 dataset highlight the robustness of our proposed A-V fusion model. It has achieved a concordance correlation coefficient (CCC) of 0.374 (0.663) and 0.363 (0.584) for valence and arousal, respectively, on test set (validation set). This is a significant improvement over the baseline of third challenge of Affective Behavior Analysis in-the-wild (ABAW3) competition, with a CCC of 0.180 (0.310) and 0.170 (0.170).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition (ER) is a challenging problem since the expressions linked to human emotions are extremely diverse in nature across individuals and cultures. It has been extensively researched in various fields such as neuroscience, psychology, cognitive science and computer science, leading to the advancement of a wide range of applications in, e.g., health care (e.g., assessment of anger, fatigue, depression and pain), robotics (human-machine interaction), driver assistance (assessment of a driver's state), etc  [11] . ER can be formulated as the problem of categorical model or dimensional model of emotions. In categorical model, the human emotions has been categorized into six categories -anger, disgust, fear, happy, sad, and surprise  [4] . Subsequently, contempt has been added to these six basic emotions  [25] . The categorical model of ER has been explored extensively in the field of affective computing due to its simplicity and universality. In dimensional model, human emotions can be analyzed on a wide range of emotions on a continuous scale, where the human emotions can be projected onto the dimensions of valence and arousal  [39] . Figure  1  illustrates the use of a two-dimensional space to represent emotional states, where valence and arousal are employed as dimensional axes. Valence reflects the wide range of emotions in the dimension of pleasantness from being negative (sad) to positive (happy), whereas arousal spans the range of intensities from passive (sleepiness) to active (high excitement).\n\nDimensional modelling of emotions is more challenging than categorical case since it is difficult to obtain continuous scale of annotations compared to discrete emotions. Due to the continuous range of emotions, the annotations tend to be noisy and ambiguous. Several databases such as RECOLA  [37] , SEWA  [20] , SEMAINE  [26] , etc have been introduced for the task of dimensional ER. Depending on the video capture conditions, i.e., whether controlled or inthe-wild environments, this task can present different challenges due to factors such as poor illumination, pose variations, and background noise. Recently, Kollias et al.  [16]  introduced Affwild2 database, which is the largest in-the-wild database for dimensional ER. The dataset is also provided with the annotations of other tasks of expression classification and action unit detection. Previously, the data-set has been used for challenges hosted in conjunction with CVPR 2017  [47] , FG 2020  [13]  and ICCV 2021  [19] . Several approaches have been proposed for previous challenges in the framework of multi-task learning  [14, 15, 17, 18] . In continuation with the previous challenges, third competition was held in conjunction with CVPR 2022  [12]  with an exclusive challenge track for valence and arousal estimation.\n\nIn this paper, we investigate the prospect of leveraging the complementary relationship of A and V modalities in videos in a joint cross attentional framework. Facial expressions are one of the most dominant channels through which human emotions can be expressed. It was shown that only one-third of human communication is conveyed through verbal components and two-third of communication occur through non-verbal components  [27] . Voice also serves as a major cue in conveying human emotions as it often carry complementary relationship with the V modality. For instance, when the facial modality is missing due to pose, blur, low illumination, etc., we can still leverage the A modality to estimate the emotional state. Similarly, when we have silent regions in the A modality, we can leverage the rich information in the V modality. In most of the existing approaches, A-V fusion is often achieved by concatenating the A and V features, which may degrade system performance  [43] . Therefore, designing a fusion mechanism based on A and V features which can effectively leverage their complementary relationships is pivotal in improving the performance of a multimodal ER system over uni-modal approaches.\n\nSeveral ER approaches have been proposed for videobased dimensional ER using convolutional neural networks (CNNs) to obtain the deep features, and recurrent neural networks (RNNs) to capture the temporal dynamics  [40, 43] . Deep models have also been widely explored for vocal emotion recognition, typically using spectrograms with 2D-CNNs  [40, 44] , or raw wave forms with 1D-CNNs  [43] . In most of the existing approaches  [42, 43]  for dimensional ER, A-V fusion is performed by concatenating the deep features extracted from individual facial and vocal modalities, and fed to LSTM for predicting valence and arousal. Although LSTM based fusion models the spatio-temporal and intramodal relationships, and can improve system performance, it does not effectively capture the inter-modal relationships across the individual modalities. We therefore investigate the prospect of extracting more contributive features across A and V modalities in order to leverage their complementary temporal relationships.\n\nAttention mechanisms have recently gained much interest in the computer vision and machine learning communities, allowing to extract task relevant features, and thereby improving system performance. Most of the existing attention based approaches for dimensional ER explore the intra-modal relationships  [22] . Although a few approaches attempt to capture the cross-modal relationships using cross-attention based on transformers  [35, 42] , they do not effectively leverage the complementary relationship of A-V modalities. Indeed, their computation of attention weights does not consider the correlation among the A and V features. Recently, Praveen et al.  [36]  proposed crossattentional model for dimensional ER based on AV fusion and showed significant improvement on RECOLA dataset  [37]  over state-of-the-art methods by leveraging the complementary relationships of A and V modalities. In this paper, we introduce a joint modeling of intra-and intermodal relationships into a cross attentional framework. The cross correlation is computed between the joint A-V feature representation, and the features of individual modalities. We have shown that deploying joint representation into the cross attentional module significantly improves the modeling of cross-modal relationships over the vanilla cross attentional model  [36] , while reducing the heterogeneity across the modalities on the challenging in-the-wild Af-fwild2 dataset  [16] .\n\nThe main contributions of the paper are as follows. (1) A joint cross-attentional model is proposed for A-V fusion based on the joint modeling of intra-and inter-modal relationships, which effectively captures the complementary inter-modal as well as intra-modal relationships. Specifically, we use joint A-V feature representations to attend to the other modality (as well as itself) based on the atten-tion weights computed from the cross correlation between the individual features and joint representation.  (2)  The effectiveness of the proposed approach is analyzed through an extensive set of experiments and ablation studies on the challenging in-the-wild Affwild2 dataset.\n\nThe rest of this paper is organized as follows. Section 2 provides a critical analysis of the relevant literature on dimensional ER, and attention models for A-V fusion. Section 3 describes the proposed joint cross-attentional A-V fusion model. Sections 4 and 5 present the experimental methodology for validation, and results obtained with the proposed approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2.1. A-V Fusion Based Emotion Recognition",
      "text": "One of the primitive approaches using DL models for A-V fusion based dimensional ER was proposed by Tzirakis et al.  [43] , where A and V features, obtained from ResNet50 and 1D-CNN respectively, are concatenated and fed to Long short-term memory model (LSTM). Juan et al.  [34]  presented an empirical study of fine-tuning various layers of pretrained CNN models for V modality, and used conventional A features for fusion. Nguyen et al.  [31]  proposed a deep model of two-stream auto-encoders and LSTM to simultaneously learn compact representative features from A and V modalities for dimensional ER. Schonevald et al.  [40]  explored knowledge distillation using teacher-student model for V modality and CNN model for A modality using spectrograms, and fused using RNNs. Deng et al  [2]  proposed iterative self distillation method for modeling the uncertainties in the labels in a multi-task framework. Kuhnke et al.  [21]  proposed two stream A-V network, where V features are extracted from R(2plus1)D model pretrained from action recognition dataset and A features are obtained from Resnet18 model. Wang et al  [44]  further improved their approach  [21]  by introducing teacher-student model in a semi-supervised learning framework. The teacher model is trained on the available labels, which is further used to obtain pseudo labels for unlabeled data. The pseudo labels are finally used to train the student model, which is used for final prediction. Though the above mentioned approaches have shown significant improvement for dimensional ER, they fail to effectively capture the inter-modal relationships and relevant salient features specific to the task. Therefore, we have focused on capturing the comprehensive features in a complimentary fashion using attention mechanisms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attention Models For A-V Fusion:",
      "text": "Attention models for A-V fusion has been widely explored in modeling the intra and inter modal relationships between A-V modalities for various applications such as A-V event localization  [3] , action localization  [23] , emotion recognition  [35] , etc. Zhang et al.  [50]  proposed attentive fusion mechanism, where multi features are obtained from 3D-CNN and 2D-CNN for V modality and 2D-CNN using spectrograms for A modality. The obtained A and V features are further re-weighted using scoring functions based on the relevant information in the individual modalities. Recently, cross-modal attention is found to be promising as effective modeling of inter-modal relationships significantly improves the system performance. Srinivas et al.  [35]  explored transformers with encoder layers, where cross-modal attention is deployed to integrate A and V features for dimensional ER. Tzirakis et al.  [42]  investigated self attention as well as cross-attention fusion based on transformers in order to enable the extracted features of different modalities to attend to each other. Although these approaches have explored cross-modal attention with transformers, they fail to leverage semantic relevance among the A-V features based on cross-correlation. Zhang et al.  [49]  investigated the prospect of improving the fusion performance over individual modalities and proposed leader-follower attentive fusion for dimensional ER. The obtained features are encoded and attention weights are obtained by combining the encoded A and V features. The attention weights are further attended on the V features and concatenated to the original V features for final prediction.\n\nUnlike prior approaches, we advocate for a simple yet efficient joint cross-attentional model based on joint modeling of intra and inter modal relationships between A and V modalities. Cross-attention has been successfully applied in several applications, such as weakly-supervised action localization  [23] , few-shot classification  [9]  and dimensional ER  [6] . In most of these cases, cross-attention has been applied across the individual modalities. Praveen et al.  [36]  have shown significant improvement using cross attention based on cross correlation across the individual features. However, we have explored joint attention between individual and combined AV-features. By deploying the joint AV feature representation, we can effectively capture the intra and inter-modal relationships simultaneously by allowing interactions across the modalities as well as oneself. Recently, joint co-attention has also been explored by Duan et al.  [3]  in a recursive fashion for A-V event localization and found to be promising in obtaining robust multimodal feature representations. In this paper, joint (combined) A-V features are extracted through cross-attention, where the features of each modality attend to themselves, as well as those of the other modality, through cross-correlation of the concatenated A-V features, and features of individual modalities. By effectively leveraging the joint modeling of intra-and inter-modal relationships, the proposed approach can significantly improve system performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Network:",
      "text": "Facial expressions in videos carry rich information pertinent to both appearance and temporal dynamics, which plays a crucial role in understanding the emotions of a person. Therefore, these spatial and temporal cues must be efficiently modeled in order to obtain robust feature representations suitable for ER. In the recent years, deep learning models have been widely explored for analyzing facial expressions in videos. In most of these approaches  [33, 46] , 2D-CNN has been used in conjunction with Recurrent Neural Networks (RNN) to capture the spatial and temporal dynamics respectively. 3D-CNNs have also been widely explored especially for action recognition and found to be promising in simultaneously capturing the spatial and temporal dynamics. Inspired by the performance of 3D-CNNs,  [41]  explored R(2plus1)D network pretrained on the Kinetics-400 action recognition dataset  [21, 44]  and outperformed conventional 2D-CNNs for dimensional ER on Af-fwild2 dataset. Recently, I3D have shown significant improvement for action recognition with fewer number of parameters than that of conventional 3D-CNNs while able to leverage the pretrained weights of 2D-CNN models. However, it fails to capture the long-term temporal dependencies. Temporal Convolutional Networks (TCN) were found to be efficient in capturing the long term temporal dependencies  [49] . Therefore, we have explored TCN in conjunction with I3D in order to leverage both long-and short-term temporal dynamics. We have also explored other visual backbones, such as R(2plus1)D network pretrained on the Kinetics-400 action recognition dataset  [21, 44] , and ResNet CNNs with GRU to obtain the V features and validate our fusion model (see implementation details in Section 4).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Network:",
      "text": "Several low-level descriptors such as prosodic, excitation, MFCC and spectral descriptors have commonly been used as feature representations for A modality in ER  [8, 34] . With the advent of deep models, the performance of speech ER have been significantly improved using 1D-CNNs on raw A signals  [43]  or 2D-CNN models on spectrograms  [40, 44]  over the past few years. Compared to 1D-CNNs, 2D-CNNs using spectrograms have been widely explored in the literature of speech ER, as it was found to carry significant para-lingual information pertaining to the affective state of a person  [24] . Various 2D-CNN architectures such as VGGish  [49]  and Resnet18  [7]  have been used to obtain robust feature representations of A modality for ER. Given the ubiquitous usage of spectrograms for extracting effective feature representations pertinent to affective state of a person, we have also used spectrograms with 2D-CNNs in our framework to validate the proposed fusion model (see implementation details in Section 4).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Cross-Attentional Av-Fusion:",
      "text": "Though A-V fusion can be achieved through unified multimodal training, it was found that simultaneous training of multimodal networks often declines over that of individual modalities  [45] . This can be attributed to a number of factors, such as differences in learning dynamics for A and V modalities  [45] , different noise topologies, with some modality streams containing more or less information for the task at hand, as well as specialised input representations  [30] . Therefore, we have trained DL models for the individual A and V modalities independently in order to extract A and V features, which is fed to the joint crossattentional module for A-V fusion that outputs final valence and arousal prediction.\n\nFor a given video sequence, the V modality carries more relevant information in some video clips, whereas A modality might be more relevant for others. Since multiple modalities convey diverse information for valence and arousal, their complementary relationship needs to be effectively captured. In order to reliably fuse these modalities, we rely on cross-attention based fusion mechanism to efficiently encode the inter-modal information, while preserving the intra-modal characteristics. Though cross-attention has been conventionally applied across the features of individual modalities, we used cross-attention in a joint learning framework. Specifically, our joint A-V feature representation is obtained by concatenating the A and V features to attend to the individual A and V features. By using the joint representation, features of each modality attend to themself, and the other modality, helping to capture the semantic inter-modal relationships across A and V. The heterogeneity among the A and V modalities can also be drastically reduced by using the combined feature representation in the cross-attentional module, which further improves system performance. A block diagram of the proposed model is shown in Figure  2 . A) Training mode: Let X a and X v represents two sets of deep feature vectors extracted for the A and V modalities, in response to a given input video sub-sequence S of fixed size, where\n\nL denotes the number of non overlapping fixed-size clips sampled uniformly from S, d a and d v represents the feature dimension of the A and V representations, x l a and x l v denotes the A and V feature vectors, respectively, for l = 1, 2, ..., L clips.\n\nAs shown in Figure  2 , the joint representation of A-V features, J , is obtained by concatenating the A and V feature vectors: J = [X a ; X v ] ∈ R d×L , where d = d a + d v denotes the feature dimension of concatenated features. This A-V feature representations (J ) of the given video subsequence (S) is now used to attend to unimodal feature rep- resentations X a and X v . The joint correlation matrix C a across the A features X a , and the combined A-V features J are given by:\n\nwhere W ja ∈ R L×L represents learnable weight matrix across the A and joint A-V features, and T denotes transpose operation. Similarly, the joint correlation matrix for V features are given by:\n\nThe joint correlation matrices C a and C v for A and V modalities provide a semantic measure of relevance not only across the modalities but also within the same modality. Higher correlation coefficient of the joint correlation matrices C a and C v shows that the corresponding samples are strongly correlated within the same modality as well as other modality. Therefore, the proposed approach is able to efficiently leverage the complimentary nature of A and V modalities (i.e., inter-modal relationship) as well as intramodal relationships, thereby improving the performance of the system. After computing the joint correlation matrices, the attention weights of A and V modalities are estimated.\n\nSince the dimensions of joint correlation matrices (R da×d ) and the features of corresponding modality (R L×da ) differ, we rely on a different learnable weight matrices corresponding to features of the individual modalities, in order to compute attention weights of the modalities. For the A modality, the joint correlation matrix C a and the corresponding A features X a are combined using the learnable weight matrices W ca and W a respectively to compute the attention weights of A modality, which is given by\n\nwhere W ca ∈ R k×d , W a ∈ R k×L and H a represents the attention maps of the A modality. Similarly, the attention maps (H v ) of V modality are obtained as\n\nwhere\n\nFinally, the attention maps are used to compute the attended features of A and V modalities. These features are obtained as:\n\nwhere W ha ∈ R k×L and W hv ∈ R k×L denote the learnable weight matrices, respectively. The attended A and V features, X att,a and X att,v are further concatenated to obtain the A-V feature representation, which is given by:\n\nFinally, the A-V features are fed to the fully connected layers for the predictions of valence or arousal. The Concordance Correlation Coefficient (ρ c ) has been widely used in the literature to measure the level of agreement between the predictions (x) and ground truth (y) annotations for dimensional ER  [43] . Let µ x and µ y represents the mean of predictions and ground truth, respectively. Similarly, if σ 2\n\nx and σ 2 y denotes the variance of predictions and ground truth, respectively, then ρ c between the predictions and ground truth is:\n\nwhere σ 2 xy denotes the covariance between predictions and ground truth. Although MSE has been widely used as a loss function for regression models, we use L = 1 -ρ c since it is standard and common loss in the dimensional ER literature  [43] . The parameters of our A-V fusion model (W ca , W a , W cv , W v , W ha , and W hv ) are optimized according to this loss. B) Test mode: A continuous video sequence is input to our model during inference. Feature representations x l a and x l v are extracted by A and V backbones for successive input clips and spectrograms, and fed to the A-V fusion model for the prediction of valence and arousal. In addition, the arousal and valence predictions may be produced using multiple diverse A and V backbones that are combined through feature-level fusion, or multiple A-V fusion models that are combined through decision-level fusion (see implementation details in Section 4).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Methodology",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset:",
      "text": "Affwild2 is the largest database in the field of affective computing captured from YouTube, under extreme challenging environments. Though the dataset is provided with annotations for the tasks of expression classification, action unit detection and valence-arousal, we have focused on the problem of estimating valence-arousal in this work. For the track of valence-arousal estimation challenge, there are 567 videos with the annotations of valence and arousal. Sixteen of these video clips display two subjects, both of which have been annotated. The annotations are provided by four experts using a joystick and the final annotations are obtained as the average of the four raters. In total, there are 2, 786, 201 frames with 455 subjects, out of which 277 are male and 178 female. The annotations for valence and arousal are provided continuously in the range of  [-1, 1] . Some of the frames in some videos are not annotated. So we discard those frames. The dataset is split into the training, validation and test sets. The partitioning is done in a subject independent manner, so that every subject's data will present in only one subset. The partitioning produces 341, 71, and 152 train, validation and test videos respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details:",
      "text": "For the V modality, we have used the cropped and aligned images provided by the challenge organizers  [19] . For the missing frames in the V modality, we have considered black frames (i.e., zero pixels). Faces are resized to 224x224 to be fed to the I3D network. The videos are converted to sub-sequences, which is further sampled uniformly to obtain non overlapping fixed-size clips. The subsequence length and the clip length of the videos are considered to be 64 and 8 respectively, obtained by downsampling a sequence of 256 frames by 4. Therefore, we have 8 clips in each sub-sequence, resulting in 1, 96, 265 training samples and 41, 740 validation samples and 92, 941 test samples. I3D model was pre-trained on ImageNet, and inflated to a 3D-CNN using Affwild2 videos of facial expressions. To regularize the network, dropout is used with p = 0.8 on the linear layers. The initial learning rate was set to be 1e -3, and the momentum of 0.8 is used for SGD. Weight decay of 5e -4 is used. Here again, the batch size of the network is set to be 8. Data augmentation is performed on the training data by random cropping, which produces scale invariant model. The number of epochs is set to 50, and early stopping is used to obtain weights of the best model.\n\nFor the A modality, the vocal signal is extracted from the corresponding video, and re-sampled to 44100Hz, which is further processed to extract short vocal segments corresponding to a clip size of 32 frames of the V network. It is ensured that the clips and sub-sequences of the V clips are properly synchronized with that of A clips. The spectrogram is obtained using Discrete Fourier Transform (DFT) of length 1024 for each short clip (corresponding to 32 frames), where the window length is considered to be 20 msec and the hop length to be 10 msec. Following aggregation of short-time spectra, we obtain the spectrogram of 64 x 107 corresponding to each sub-sequence of the V modality. The spectrogram is converted to log-power-spectrum, expressed in dB. Finally, mean and variance normalization is performed on the spectrogram. Now the obtained spectrograms are fed to the Resnet18  [7]  to obtain the A features. Due to the availability of the large number of samples in the Affwild2 dataset, we trained the Resnet18 model from scratch. In order to adapt to the number of channels of the spectrogram, the first convolutional layer in the Resnet18 model is replaced by single channel. The network is trained with an initial learning rate of 0.001 and weights are optimized using Adam optimizer. The batch size is considered to be 64 and early stopping is used to obtain the best model for prediction.\n\nFor the A-V fusion network, the size of the concatenated A-V features J are set to be 1024. In the joint crossattention module, the initial weights of the cross-attention matrix is initialized with Xavier method  [5] , and the weights are updated using Adam optimizer. The initial learning rate is set to be 0.001 and batch size is fixed to be 64. Also, dropout of 0.5 is applied on the attended A-V features and weight decay of 5e -4 is used for all the experiments. Feature-lever (decision-level) fusion is implemented by training a fully connected neural network to provide a weighted fusions of feature representations (decisions values) for arousal and valence predictions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Table  1  presents the results of our ablation study on the validation dataset. The performance of our proposed joint cross-attentional fusion is compared using various A and V backbones and A-V fusion strategies. First, we have implemented I3D  [1]  with simple feature concatenation, where the extracted A and V features are concatenated, and fed to fully connected layers for valence and arousal prediction. Then we have replaced I3D with R3D  [41]  and implemented a similar fusion strategy of feature concatenation. R3D was found to perform slightly better than I3D for arousal while I3D shows superior performance for valence. We have also compared our proposed approach with that of other relevant attention fusion strategies in the literature. We have compared the backbones of I3D with that of leader-follower attention  [49]  and cross-attention  [36] . When compared to vanilla cross attention model, leaderfollower attention was found to perform better.\n\nFinally, in order to validate the generalization capability of the proposed fusion model we have implemented various V backbones of I3D, R3D, Resnet18 with GRU and I3D with TCN. Though the performance of our fusion model slightly varies with different backbones, we can observe that the proposed fusion model is able to achieve superior performance over that of other attention strategies  [36, 49]  especially for valence. Compared to 2D-CNN model (Resnet18 with GRU), 3D-CNNs are found to perform slightly better. I3D shows improvement over valence than arousal with our fusion model compared to R3D. By introducing TCN with I3D, the performance of the proposed fusion model is found to perform even better as it captures better long term temporal cues than vanilla I3D. For all the experiments conducted above, Resnet18 is used as the backbone for the A modality.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "Table  2  shows our comparative results against relevant state-of-the-art A-V fusion models on the Affwild2 validation set submitted for the previous challenges  [13, 19] . Most of the relevant approaches have been implemented with different experimental protocol and training strategies. Therefore, in order to have a fair comparision we have reimplemented these approaches according to our experimental protocol, and analyzed the results on Affwild2 validation set. Similar to our A and V backbones, Kuhnke et al  [21]  also used 3D-CNNs, where R(2plus1)D model is used for V modality and Resnet18 is used for A modality. How-ever, they use additional masks for V modality and annotations of other tasks to refine the annotations of valence and arousal. They further perform simple feature concatenation without any specialized fusion model for the prediction of valence and arousal. So the fusion performance was not significantly improved over the uni-modal performance. Zhang et al  [49]  explored leader follower attention model for fusion and showed minimal improvement of fusion performance over uni-modal performances. Though they have shown significant performance for arousal than valence, it is highly attributed to the V backbone. In our proposed approach, we have shown significant improvement for fusion especially for valence than arousal. Even with vanilla cross attentional fusion  [36] , we have shown that fusion performance for valence has been improved better than  [49]  and  [49] . By deploying joint representation into the cross attententional fusion model, the fusion performance of valence has been significantly improved further. In case of arousal, though the fusion performance is lower than that of  [49]  and  [49] , we can observe that it has been improved better than that of uni-modal V performance. Therefore, the proposed approach is effective in capturing the variations spanning over a wide-range of emotions (valence) than that of the intensities of the emotions (arousal).\n\nWe have further compared our fusion model with that of other valid submissions for the third ABAW challenge  [12]  on the test set as shown in Table  3 . The winner of the challenge  [28]  also uses A-V fusion and showed outstanding performance for both valence and arousal. They have used three external datasets to improve the generalization capability of the training model and features from multiple backbones for both V and A modalities. Flying-Pigs  [48]  uses text modality along with A and V modalities and showed improvement over A-V fusion using leader follower attention strategy. Apart from these, AU-NO  [10]  is the only approach that relies on A-V fusion. They have investigated the performance of attention mechanisms such as self attention and cross attention with that of recurrent networks. They have further used additional loss components of Mean Square Error (MSE) and categorical cross entropy loss along with CCC. PRL  [32]  and HSE-NN  [38]  uses only visual modality, where  [32]  uses ensemble based strategy and  [38]  uses external AffectNet dataset  [29]  for better performance. It is worth mentioning that we don't use any advanced loss components or post processing operations on predictions using cross validation, etc. apart from clipping the predictions to the range of [-1,1]. We also do not use any external data-sets or features from multiple backbones for A and V modalities. The performance of the proposed approach is solely attributed to the efficacy of our fusion model. We can observe that the fusion performance has been significantly improved over the uni-modal performances especially for valence. The proposed fusion model",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, joint cross-attentional is introduced for A-V fusion in video-based dimensional ER, leveraging the intraand inter-modal relationships across A and V features. In particular, the complimentary relationship between A and V features are efficiently captured based on the correlation between the combined A-V features and individual A and V features. By jointly modeling the inter and inter-modal relationships, features of each modality attend to the other modality as well as itself, resulting in robust A and V feature representations. With the proposed model, A and V backbones are first trained individually for facial (V) and vocal (A) modalities. Then, an attention mechanism based on correlation between joint and individual features are applied to obtain the attended A and V features. Finally, the attention weighted features are concatenated, and fed to linear connected layers to predict valence and arousal values. The proposed A-V fusion model is validated experimentally on the challenging Affwild2 video datasets, using different A and V backbones. Results show that the proposed model is a cost-effective approach that can sustaining a high level of performance, and outperform the state-of-the-art.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The valence-arousal space.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the use of a two-dimensional space to",
      "page": 2
    },
    {
      "caption": "Figure 2: A) Training mode: Let Xa and Xv represents two sets",
      "page": 4
    },
    {
      "caption": "Figure 2: , the joint representation of A-V",
      "page": 4
    },
    {
      "caption": "Figure 2: Joint cross-attention model proposed for A-V fusion (training mode).",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kuhnke [21], FGW 2020 – A: Resnet18; V: R(2plus1)D": "Zhang [49], ICCVW 2021 – A: VGGish; V: Resnet50 + TCN",
          "0.351": "-",
          "0.449": "0.405",
          "0.493": "0.457",
          "0.356": "-",
          "0.565": "0.635",
          "0.604": "0.645"
        },
        {
          "Kuhnke [21], FGW 2020 – A: Resnet18; V: R(2plus1)D": "Rajasekhar [36], FG 2021 – A: Resnet18; V: I3D + TCN",
          "0.351": "0.351",
          "0.449": "0.417",
          "0.493": "0.552",
          "0.356": "0.356",
          "0.565": "0.539",
          "0.604": "0.531"
        },
        {
          "Kuhnke [21], FGW 2020 – A: Resnet18; V: R(2plus1)D": "Joint Cross-Attention (Ours) – A: Resnet18; V: I3D + TCN",
          "0.351": "0.351",
          "0.449": "0.417",
          "0.493": "0.663",
          "0.356": "0.356",
          "0.565": "0.539",
          "0.604": "0.584"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Situ-RUCAIM3 [28]": "FlyingPigs [48]",
          "Audio, Visual": "Audio, Visual, Text",
          "0.606": "0.520",
          "0.596": "0.602",
          "0.601": "0.561"
        },
        {
          "Situ-RUCAIM3 [28]": "PRL [32]",
          "Audio, Visual": "Visual",
          "0.606": "0.450",
          "0.596": "0.445",
          "0.601": "0.448"
        },
        {
          "Situ-RUCAIM3 [28]": "HSE-NN [38]",
          "Audio, Visual": "Visual",
          "0.606": "0.417",
          "0.596": "0.454",
          "0.601": "0.436"
        },
        {
          "Situ-RUCAIM3 [28]": "AU-NO [10]",
          "Audio, Visual": "Audio, Visual",
          "0.606": "0.418",
          "0.596": "0.407",
          "0.601": "0.413"
        },
        {
          "Situ-RUCAIM3 [28]": "Joint Cross-Attention (Ours)",
          "Audio, Visual": "Audio, Visual",
          "0.606": "0.374",
          "0.596": "0.363",
          "0.601": "0.369"
        },
        {
          "Situ-RUCAIM3 [28]": "Baseline",
          "Audio, Visual": "Visual",
          "0.606": "0.180",
          "0.596": "0.170",
          "0.601": "0.175"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "Zisserman"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "Didan Deng",
        "Liang Wu",
        "Bertram Shi"
      ],
      "year": "2021",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "3",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "Bin Duan",
        "Hao Tang",
        "Wei Wang",
        "Ziliang Zong",
        "Guowei Yang",
        "Yan Yan"
      ],
      "year": "2021",
      "venue": "WACV"
    },
    {
      "citation_id": "4",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "5",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "Xavier Glorot",
        "Yoshua Bengio"
      ],
      "year": "2010",
      "venue": "In ICAIS"
    },
    {
      "citation_id": "6",
      "title": "Deep weakly supervised domain adaptation for pain localization in videos",
      "authors": [
        "E Gnana Praveen",
        "P Granger",
        "Cardinal"
      ],
      "venue": "Deep weakly supervised domain adaptation for pain localization in videos"
    },
    {
      "citation_id": "7",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "He",
        "S Zhang",
        "Ren",
        "Sun"
      ],
      "year": "2008",
      "venue": "CVPR 2016"
    },
    {
      "citation_id": "8",
      "title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "Lang He",
        "Dongmei Jiang",
        "Le Yang",
        "Ercheng Pei",
        "Peng Wu",
        "Hichem Sahli"
      ],
      "year": "2015",
      "venue": "In AVEC"
    },
    {
      "citation_id": "9",
      "title": "Cross attention network for few-shot classification",
      "authors": [
        "Ruibing Hou",
        "Hong Chang",
        "M Bingpeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "NIPS"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Vincent Karas",
        "Mani Kumar Tellamekala",
        "Adria Mallol-Ragolta",
        "Michel Valstar",
        "Björn Schuller"
      ],
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition and Its Applications",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wróbel"
      ],
      "year": "2014",
      "venue": "Emotion Recognition and Its Applications"
    },
    {
      "citation_id": "12",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "13",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "FG"
    },
    {
      "citation_id": "14",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "15",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "16",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Kollias",
        "M A Tzirakis",
        "A Nicolaou",
        "G Papaioannou",
        "B Zhao",
        "I Schuller",
        "Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "17",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "18",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "19",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "Zafeiriou"
      ],
      "venue": "ICCVw, 2021"
    },
    {
      "citation_id": "20",
      "title": "Sewa db: A rich database for a-v emotion and sentiment research in the wild",
      "authors": [
        "Kossaifi",
        "Y Walecki",
        "J Panagakis",
        "M Shen",
        "F Schmitt",
        "J Ringeval",
        "V Han",
        "Pandit",
        "B Toisoul",
        "K Schuller",
        "E Star",
        "M Hajiyev",
        "Pantic"
      ],
      "year": "2021",
      "venue": "IEEE Trans Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Two-stream auralvisual affect analysis in the wild",
      "authors": [
        "Kuhnke",
        "Rumberg",
        "Ostermann"
      ],
      "year": "2008",
      "venue": "FGW 2020"
    },
    {
      "citation_id": "22",
      "title": "Audio-visual attention networks for emotion recognition",
      "authors": [
        "Jiyoung Lee",
        "Sunok Kim",
        "Seungryong Kim",
        "Kwanghoon Sohn"
      ],
      "year": "2018",
      "venue": "Workshop on Audio-Visual Scene Understanding for Immersive Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Cross-attentional audio-visual fusion for weaklysupervised action localization",
      "authors": [
        "Jun-Tae Lee",
        "Mihir Jain",
        "Hyoungwoo Park",
        "Sungrack Yun"
      ],
      "venue": "Cross-attentional audio-visual fusion for weaklysupervised action localization"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "Xi Ma",
        "Zhiyong Wu",
        "Jia Jia",
        "Mingxing Xu",
        "Helen Meng",
        "Lianhong Cai"
      ],
      "year": "2018",
      "venue": "In INTERSPEECH"
    },
    {
      "citation_id": "25",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "26",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "M Mckeown",
        "R Valstar",
        "M Cowie",
        "M Pantic",
        "Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Nonverbal Communication",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "2002",
      "venue": "Nonverbal Communication"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion estimation for in-the-wild videos",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Yuan Cheng",
        "Meng Wang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "Multimodal emotion estimation for in-the-wild videos"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "Nagrani",
        "Yang",
        "C Arnab",
        "C Schmid",
        "Sun"
      ],
      "year": "2021",
      "venue": "NIPS"
    },
    {
      "citation_id": "31",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "Dung Nguyen",
        "Thanh Duc",
        "Rui Nguyen",
        "Thanh Zeng",
        "Son Nguyen",
        "Thin Tran",
        "S Khac Nguyen",
        "Clinton Sridharan",
        "Fookes"
      ],
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "An ensemble approach for facial expression analysis in video",
      "authors": [
        "Hong-Hai Nguyen",
        "Van-Thong Huynh",
        "Soo-Hyung Kim"
      ],
      "year": "2022",
      "venue": "An ensemble approach for facial expression analysis in video"
    },
    {
      "citation_id": "33",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "D Juan",
        "Patrick Ortega",
        "Alessandro Cardinal",
        "Koerich"
      ],
      "year": "2004",
      "venue": "SMC 2019"
    },
    {
      "citation_id": "35",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "STL 2021"
    },
    {
      "citation_id": "36",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Gnana Praveen",
        "Eric Granger",
        "Patrick Cardinal"
      ],
      "year": "2008",
      "venue": "2021 16th IEEE FG"
    },
    {
      "citation_id": "37",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "FG"
    },
    {
      "citation_id": "38",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices"
    },
    {
      "citation_id": "39",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "40",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "Schoneveld",
        "H Othmani",
        "Abdelkawy"
      ],
      "year": "2008",
      "venue": "Pattern Rec. Letters"
    },
    {
      "citation_id": "41",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "Tzirakis",
        "S Chen",
        "Zafeiriou",
        "Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "43",
      "title": "End-to-end multimodal er using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2006",
      "venue": "IEEE J. of Selected Topics in Signal Proc"
    },
    {
      "citation_id": "44",
      "title": "A multi-task mean teacher for semi-supervised facial affective behavior analysis",
      "authors": [
        "Lingfeng Wang",
        "Shisen Wang",
        "Jin Qi",
        "Kenji Suzuki"
      ],
      "year": "2021",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "45",
      "title": "What makes training multimodal classification networks hard?",
      "authors": [
        "D Wang",
        "M Tran",
        "Feiszli"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "46",
      "title": "Lstm-modeling of continuous emotions in an a-v affect recognition framework",
      "authors": [
        "M Wöllmer",
        "F Kaiser",
        "B Eyben",
        "G Schuller",
        "Rigoll"
      ],
      "year": "2013",
      "venue": "IVC"
    },
    {
      "citation_id": "47",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "48",
      "title": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3",
      "authors": [
        "Su Zhang",
        "Ruyi An",
        "Yi Ding",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3"
    },
    {
      "citation_id": "49",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Y Zhang",
        "Z Ding",
        "C Wei",
        "Guan"
      ],
      "year": "2008",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "50",
      "title": "Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Yuan-Hang Zhang",
        "Rulin Huang",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2020",
      "venue": "IEEE FG"
    }
  ]
}