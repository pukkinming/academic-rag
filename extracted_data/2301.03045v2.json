{
  "paper_id": "2301.03045v2",
  "title": "Cmc Cumulative Match Characteristic",
  "published": "2023-01-08T14:07:01Z",
  "authors": [
    "João Ribeiro Pinto"
  ],
  "keywords": [
    "Aprendizagem Computacional",
    "Atividade",
    "Áudio",
    "Biometria",
    "Electrocardiograma",
    "Emoção",
    "Face",
    "Monitorização de Bem-Estar",
    "Reconhecimento de Padrões",
    "Processamento de Sinal",
    "Veículos Autónomos",
    "Vídeo",
    "Visão Computacional Activity",
    "Audio",
    "Autonomous Vehicles",
    "Biometrics",
    "Computer Vision",
    "Electrocardiogram",
    "Emotion",
    "Face",
    "Machine Learning",
    "Pattern Recognition",
    "Signal Processing",
    "Video",
    "Wellbeing Monitoring 12.2.2 Video-based emotion recognition . . . . . . . . . . . . . . . . . . . . . 12.2.3 Audio-based emotion recognition APCER Attack Presentation Classification Error Rate AR Autoregressive (coefficients) AUC Area Under the Curve BF Bona Fide BPCER Bona fide Presentation Classification Error Rate BPF Bandpass Filter BIOSIG Biometrics Special Interest Group Bi-LSTM Bidirectional Long Short-Term Memory (network) CAM Class Activation Mapping CASIA Chinese Academy of Sciences CC (Pearson's) Correlation Coefficient CC0 Creative Commons \"No Rights Reserved\" License CCC Concordance Correlation Coefficient"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Resumo A perceção através da inteligência artificial está cada vez mais presente nas nossas vidas. Os veículos não são exceção, uma vez que sistemas avançados de assistência ao condutor auxiliam no cumprimento de limites de velocidade, na manutenção dentro das faixas e na prevenção de acidentes. Num futuro próximo, o reconhecimento de padrões terá um paper ainda mais preponderante nos veículos, uma vez que o automóvel autónomo necessitará de meios automáticos para compreender o que acontece ao seu redor (e no seu interior) para agir de forma adequada. Em reconhecimento de padrões, a biometria oferece aplicações promissoras para veículos, do acesso keyless à personalização automática de opções de condução com base no condutor reconhecido. De igual modo, as tecnologias de reconhecimento de bem-estar têm atraído atenção pela possibilidade de reconhecer atividade, emoções, sonolência ou stress em condutores e passageiros. No entanto, estes dois tópicos são diametralmente opostos, uma vez que o reconhecimento de bemestar usa a variabilidade intra-sujeito, enquanto a biometria se baseia na variabilidade inter-sujeito. Apesar das diferenças, a biometria e o reconhecimento de bem-estar poderiam (e deveriam) co-existir. O reconhecimento contínuo de identidade em dados adquiridos de forma impercetível poderiam ser usados para personalizar modelos de reconhecimento de bem-estar e obter melhor desempenho. Estes modelos personalizados poderiam ser a chave para meios mais robustos de monitorizar sonolência e atenção em condutores e evitar acidentes. Num sentido mais amplo, estes poderiam ser aplicados a todos os ocupantes, abrindo o caminho em direção ao reconhecimento eficaz de atividade, emoções, conforto e até episódios de violência em veículos autónomos partilhados. Este doutoramento focou-se em avançar o tópico de perceção em veículos através do estudo de novas metodologias de visão computacional e reconhecimento de padrões para biometria e reconhecimento de bem-estar. O foco principal foi na biometria com electrocardiograma (ECG), um traço reconhecido pelo seu potencial em monitorização impercetível de condutores. Esforços foram dedicados à obtenção de desempenho melhorado em identificação e verificação de identidade em cenários off-the-person, reconhecidos pelo elevado teor de ruído e variabilidade. Aqui, foram propostas soluções deep learning end-to-end e analisados tópicos importantes como o desempenho cross-database e a longo prazo, a importância relativa das ondas através da interpretabilidade, e a conversão entre canais. A biometria com face, um complemento natural ao ECG em cenários impercetíveis, foi também estudada nesta tese. Os desafios em reconhecimento de faces com máscaras e na interpretabilidade em biometria foram abordados com o intuito de avançar para algoritmos mais transparentes, confiáveis e robustos a oclusões significativas. Dentro do tópico de reconhecimento de bem-estar, foram propostas soluções melhoradas para o reconhecimento multimodal de emoções em grupos de pessoas e de atividade/violência dentro de veículos partilhados. Por fim, foram propostos ainda uma forma inovadora de aprender segurança de templates em modelos end-to-end, evitando processos adicionais de encriptação, e um método auto-supervisionado adaptado a dados sequenciais, para garantir segurança de dados e desempenho otimizado.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "11.7",
      "text": "Image Average mean and standard deviation (StD) results for bona fide (BF) and presentation attack (PA) samples in the comparison across the one-attack (OA) and unseen-attack (UA) scenarios. . . . . . . . . . . . . . . . . . . . . .  172 11.8  Comparison of explanations in intraclass one-attack for an example PA sample of type 5 presenting high Iµ value. . . . . . . . . . . . . . . . . . . . . . . . .  173 11.9  Comparison of explanations in intraclass unseen-attack for an example PA sample of type 5 presenting low Iµ value. . . . . . . . . . . . . . . . . . . . . . .  173 11.10  Mean Aµ results for bona fide and presentation attack samples in the one-attack (OA) and unseen-attack (UA, i = 1, ..., 7) scenarios and respective mean value.  174 11.11   Example frames from the in-vehicle dataset, depicting normal activities (top row) and violence between passengers (bottom row). . . . . . . . . . . . . . .  199 13.5  Rank accuracy results for the 21 selected classes from the MMIT database. . .  201 13.6  Cascade results for the 21 selected classes from the MMIT database. . . . . . .  201 13.7  Rank accuracy results for the in-vehicle scenario with 42 classes. . . . . . . .  203 13.8  Cascade results in the in-vehicle scenario with 42 classes. . . . . . . . . . . .  203 13.9  Cascade results in the in-vehicle scenario with 3 classes. . . . . . . . . . . . . 204 The interactions between humans and machines are increasingly mediated by intelligent sensing technologies. Nowadays, the default way to unlock a smartphone is using face or fingerprint biometrics. Highly-populated countries like India and China are building unprecedentedly massive national identity networks relying entirely on biometric characteristics (and dealing with the societal intricacies of such colossal endeavours)  [198; 257; 404] . Sophisticated algorithms monitor our attention levels while we drive  [119; 176] . Meanwhile, the gaming and entertainment industries are investing heavily in using affective computing to continuously personalise and enhance user experience  [24; 97; 259] . Among all of these applications, few have been so revolutionised (and so quickly) as vehicles, and there are plenty of great reasons for that. Whether in personal cars or public transport, people can typically spend up to multiple hours of their day inside vehicles, especially those with long daily commutes or those living/working in large urban centres. Moreover, a considerable fraction of preventable deaths and serious injuries occur in accidents involving vehicles, caused mainly by driver fatigue or the influence of psychotropic substances  [370; 399] .\n\nWhile we wait for fully autonomous vehicles, advanced driver assistance systems (ADAS) based on artificial intelligence help drivers comply with speed limits, stay in their lanes, beware of their blind spots, and avoid accidents  [81; 82; 410] . One of the most interesting applications of this is drowsiness detection, where biometric data such as face video or physiological signals can be used to detect episodes of sleepiness and infer the fatigue levels of the driver  [116; 320; 399] .\n\nIntroduction biometric recognition, intrasubject variability is typically regarded as a nuisance, as it blurs the boundaries between individuals' data and hampers accurate and robust identity recognition  [199] .\n\nIn spite of their differences, biometric recognition and wellbeing monitoring can coexist in a vehicle scenario. Applications of biometric recognition for vehicles are typically focused on access control or the automatic personalisation of driving and environmental conditions (such as seat position, mirror adjustments, or infotainment settings) based on the recognised driver and occupants  [91; 342] . However, in this thesis, we defend that this coexistence could (and should) be intensified, and eventually evolve to a level of symbiotic integration.\n\nDespite the efforts devoted to wellbeing monitoring technologies, one challenge remains unvanquished: the fact that wellbeing patterns are deeply personal. For example, no two individuals experience drowsiness in the exact same way, and similar levels of fatigue can have dramatically different effects on each person. This is also true for the way wellbeing parameters reflect on biometric data. A drowsiness monitoring system can present acceptable accuracy levels for a given set of subjects and, simultaneously, be inadequate at recognising the specific drowsiness patterns of other people  [116; 399] .\n\nBiometric recognition could be the key to unlocking the next generation of wellbeing monitoring technologies. Instead of using generalist models, identity predictions obtained through biometric recognition would enable the selection of specific models for each of the users. These models could also benefit from the influx of new data, continuously learning the subject-specific patterns of wellbeing. Thus, combining biometrics with wellbeing could enable the creation of more accurate and robust solutions for wellbeing monitoring  [116] .\n\nIn the future, fully autonomous driving may put an end to the need for drivers, but not to the need for automatic intelligent sensing technologies inside vehicles  [23; 302; 348] . The same biometric algorithms and wellbeing monitoring systems that once were focused on the driver may easily be adapted to target the occupants. In fact, self-driving vehicles unveil a new scenario for automatic passenger monitoring: since there is no driver, shared autonomous vehicles (such as autonomous taxis) lack an authority figure, responsible for the integrity of the vehicle and the comfort and security of the passengers. The driver could be replaced by pattern recognition solutions to monitor the shared vehicle interior and its passengers.\n\nOverall, the advantages of introducing intelligent sensing technologies in vehicles are plenty.\n\nFrom a narrower subject-centric perspective, integrating the use of inter and intrasubject variability would enable the development of continuously personalisable models for more robust monitoring of wellbeing parameters. From a broader perspective, taking advantage of seamless multimodal data acquisitions for automatic monitoring of the interior and passengers is of utmost importance to ensure security and comfort inside autonomous shared vehicles.",
      "page_start": 26,
      "page_end": 38
    },
    {
      "section_name": "Objectives",
      "text": "This doctoral work focused on advancing in-vehicle sensing technology through the conceptualisation and development of novel computer vision and pattern recognition methodologies. The 1.2 Objectives 5 goal has been to create automatic solutions for in-vehicle monitoring, robustly and efficiently. To achieve this, we targeted two specific scenarios, as follows:\n\n• Personalised wellbeing monitoring systems using biometrics: Here, the objective was to advance biometric recognition technologies to be integrated with wellbeing monitoring methodologies, especially for driver assistance systems. We build upon the ECG biometrics research conducted in  [337] , with additional work on face recognition and other important topics such as biometric security and learning from data streams. With this work, we aimed to pave the way towards a robust multimodal system to recognise the driver using ECG and face information. The automatic identity predictions enable the use of the drivers' data to continuously learn their personal patterns of wellbeing for more accurate monitoring. Within wellbeing, this research work focused on driver drowsiness and emotions, as part of the AUTOMOTIVE project, but it stands to reason that biometrics could be used for personalised monitoring of any wellbeing parameter;\n\n• Occupant monitoring for autonomous shared vehicles: Forecasting the advent of fully autonomous vehicles, this work aimed towards the use of data streams for monitoring occupants.\n\nIntegrated within the Easy Ride project, we targeted the monitoring of emotions among passenger groups, as well as the recognition of activities and violence inside shared autonomous vehicles. Just like the first scenario, this aims towards contributions to more intelligent and robust wellbeing monitoring systems using multimodal data sources, albeit in a less personal subject-centric way, focused instead on passenger groups as a whole. Although activity recognition is a relatively mature research topic, the in-vehicle environment offers very specific challenges, mainly regarding perspective, lighting, and occlusions, which enabled the study of innovative solutions for robust passenger monitoring. Ultimately, the developed occupant wellbeing monitoring solutions could also be personalised, using the biometric recognition of individual passengers as additional information for improved accuracy.\n\nDespite these two scenarios, used to contextualise and motivate the work conducted during this work, we aimed to build solutions that are applicable outside the target applications and beyond the fields of biometrics and wellbeing monitoring. This work also aimed to result in significant advances to the target fields, which are ready for real-life applications and capable to withstand the test of time. As such, throughout the entirety of this doctoral project, there was a constant concern to ensure the proposed methodologies were:\n\n• Multimodal: The diversification of data sources is the key to truly robust models. On the topic of biometric recognition, considering the strengths and shortcomings of the ECG signal as a biometric trait, the face is the best complementary trait for better performance and robustness. Beyond biometric recognition, the fusion of ECG and face results in a larger availability of anatomical and physiological measurements, which enable the more comprehensive monitoring of wellbeing parameters;\n\n• Seamless: Regardless of the possible consequences to accuracy, user comfort should be of utmost importance. Hence, subjects should be as unaware of the acquisition process as possible, to avoid attention or behaviour changes that could impact their comfort or the realism of the collected data. As such, this work focused, as extensively as possible, on seamlessly acquired data, in nearly unconstrained settings. When drivers are the target, and thus the subject is in physical contact with the system nearly continuously (e. g., driving the car), ECG and other physiological signals can be acquired unobtrusively at the steering wheel. Face video can also be easily and inexpensively acquired with cameras. On the other hand, for shared vehicle occupant monitoring, physiological signals have been avoided in favour of less contact-intensive alternatives, namely video and audio;\n\n• Continuous: Continuous biometric methodologies offer unique advantages in usability and effectiveness, both for recognition and wellbeing monitoring. On the other hand, having a continuous stream of biometric data opens up new possibilities for improved accuracy, immersive systems, and error management. As such, beyond novelty and performance, the algorithms developed during this work aimed towards real-time operation, reflecting a constant preference for efficiency and simplicity whenever possible;\n\n• Realistic: Overall, performance results in ECG-based biometrics literature are unrealistic, mainly due to inadequate train-test splits and overly clean signal databases. The same happens in wellbeing monitoring when the problem of subject-independence is highlighted.\n\nThis reveals the deeply flawed nature of typical evaluation frameworks, which should more realistically resemble actual application conditions and ensure reproducible results. To achieve this, this doctoral work included the reformulation of testing procedures, especially for ECG-based biometrics, through the definition of adequate test protocols, the benchmarking of literature methods, and the development of more robust recognition and monitoring algorithms.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Contributions",
      "text": "On the quest for personalised wellbeing monitoring, focused on the objectives detailed in the previous section, this doctoral project comprised several research topics related to both biometric recognition and wellbeing monitoring. This thesis presents the innovative contributions of this work organised throughout four parts.\n\nThe first two of these are focused on biometric recognition, aiming towards the development of personalised wellbeing monitoring solutions. The third part targets the scenario of passenger monitoring inside shared autonomous vehicles, specifically for emotion, activity, and violence recognition. The fourth presents broader contributions applicable to multiple scenarios in biometrics and pattern recognition. These contributions are concisely enumerated below. • the first end-to-end methodology for ECG biometrics, alongside tailored data augmentation strategies for ECG signals and a study on the advantages of integrating typically separate processes inside a single deep architecture (Chapter 4);",
      "page_start": 40,
      "page_end": 41
    },
    {
      "section_name": "Contributions",
      "text": "• an application of triplet loss and transfer learning for ECG-based identity verification, aiming towards higher robustness under realistic evaluation setups using off-the-person databases (Chapter 5);\n\n• a study on long-term performance with multiple state-of-the-art ECG biometric methodologies, including an assessment of the effect of diverse template and model update strategies (Chapter 6);\n\n• a study on the relative importance of ECG waveforms for identification under diverse scenarios, less to more challenging, using explainability tools on our deep learning identification methodology (Chapter 7);\n\n• a methodology for recovering missing ECG leads based on single-lead blindly-segmented input signals, paving the way towards more complete applications (even in clinical scenarios) with less obtrusive signal collection setups (Chapter 8).\n\nIn Part III, Face Biometrics:\n\n• a custom training methodology tailored for face recognition with masks, aiming to promote the use of unmasked parts of the face and close the performance gap relative to typical face recognition (Chapter 10);\n\n• a study using interpretability tools to understand the use of face image information in presentation attack detection (PAD), alongside a discussion on the need for explainability and transparency in biometric recognition (Chapter 11).\n\nIn Part IV, Wellbeing Monitoring:\n\n• an approach for classifying emotion valence in groups of people, based on the late fusion of parallel visual and audio data streams using deep neural networks (Chapter 12);\n\n• a cascade strategy for improved efficiency in continuous audiovisual activity recognition and violence detection inside vehicles (Chapter 13).\n\nIn Part V, Broader Topics on Biometrics and Pattern Recognition:\n\n• the Secure Triplet Loss, a training methodology that promotes template cancelability and unlinkability, alongside identity discrimination, on end-to-end biometric algorithms without the need for separate encryption or hashing processes (Chapter 14);\n\n• a methodology for self-supervised learning based on the triplet loss, taking advantage of the nature of balanced multiclass datasets, especially those composed of sequential data, for more adequate learning of the target tasks (Chapter 15). These research works may be categorised according to the specific contributions by the author of this thesis. The presented work in ECG-based biometric identification (Chapter 4), authentication (Chapter 5), and explainability (Chapter 7), as well as emotion recognition (Chapter 12), activity recognition (Chapter 13), biometric template security (Chapter 14), and self-supervised learning (Chapter 15) comprise the main contributions, resulting completely or mostly from the work performed by the author. The research on long-term performance in ECG biometrics (Chapter 6), ECG interlead conversion (Chapter 8), masked face recognition (Chapter 10), and interpretability for face PAD (Chapter 11) are secondary contributions that resulted from collaborations within the scope of this thesis and benefitted partially from the work of the author. Further details on the specific contributions can be found at the start of each chapter. Other topics have been addressed which are not presented in this thesis, due to the relevance of the respective topics or the relative weight of the author's contributions. They are, however, also mapped in Fig.  1 .1 and listed in the sections below.",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Dissemination",
      "text": "The contributions of the doctoral research to biometrics, wellbeing monitoring, and broader topics described in this thesis have been disseminated as part of twenty-four scientific publications. These are (clustered by type and in reverse chronological order):\n\n1.4 Dissemination Biometrics: Theory, Applications and Systems (Tampa, FL, USA) and at the 2020 International Summer School for Advanced Studies on Biometrics for Secure Authentication (Alghero, Italy).",
      "page_start": 42,
      "page_end": 47
    },
    {
      "section_name": "Collaborations",
      "text": "The doctoral work presented in this thesis included close collaborations with researchers from several institutions within the AUTOMOTIVE, Easy Ride, and Aurora projects. The author also collaborated frequently with VCMI group colleagues within their research work and master dissertations related to diverse biometrics, pattern recognition, and computer vision topics, as described below.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Research Projects",
      "text": "The AUTOMOTIVE project  1  was focused on ushering in the next generation of driver drowsiness monitoring technologies. Led by the VCMI research group at INESC TEC, this project featured the participation of CardioID Technologies, Instituto Superior de Engenharia de Lisboa (ISEL), and Universidade Lusófona de Humanidades e Tecnologias (ULHT). The author of this thesis has participated in the AUTOMOTIVE project from June 2019 to its conclusion in November 2021.\n\nHe mainly contributed to the development of novel algorithms for ECG-based biometric recognition to enable personalised drowsiness models. The results of this project are nicely summed up in  [116] .\n\nThe Easy Ride project 2 , under the motto \"Experience is Everything\", was a large effort led by Bosch Car Multimedia and Universidade do Minho aiming to improve passenger comfort and safety in autonomous shared vehicles. It featured INESC TEC's participation in the SP5 subproject, in close cooperation with Bosch Car Multimedia, which focused on occupant emotional monitoring. The author of this thesis has participated in writing this project's proposal, and then in the research work from its start in February 2020 to its conclusion in December 2021. He has mainly contributed to the design, implementation, and evaluation of efficient multimodal deep learning algorithms using audio and RGB video for in-vehicle emotion, activity, and violence recognition. The work encompassed all stages of research and development from scratch to invehicle deployment and is presented in  [346; 348] .\n\nThe Aurora project inherited Easy Ride's goals of bringing forth the future of shared autonomous vehicles. Specifically, it brought together Bosch Car Multimedia and INESC TEC's Centre for Telecommunications and Multimedia (CTM) and the High-Assurance Software Laboratory (HASLab) to continue SP5's mission of contributing towards accurate and efficient occupant emotional and activity monitoring. The author of this thesis participated in this project since its Introduction beginning in December 2021, having collaborated on the definition of the project's objectives and task planning.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Synergies Within The Research Group",
      "text": "Within the VCMI research group, the author collaborated, in 2018, with Wilson Silva to develop a metric for ordinal classification which takes into account both accuracy and label ranking while retaining robustness to class imbalance  [396] . In 2020, the author collaborated with Sara P. Oliveira, Tiago Gonçalves, and Hélder Oliveira, alongside Rita C. Marques and Maria João Cardoso at Champalimaud Foundation (Lisboa, Portugal), on a weakly-supervised methodology based on multiple instance learning to classify HER2 expression in breast cancer histology slides  [321] .\n\nSince 2020, the author has also been collaborating with Ana F. Sequeira, Wilson Silva, Tiago Gonçalves, and Pedro C. Neto, alongside Arun Ross from Michigan State University (East Lansing, MI, USA) to study biometrics from the perspective of interpretability, rethinking how model accuracy should be measured, and calling for more transparent biometric algorithms  [311; 385; 386] .\n\nIn 2021, the author collaborated with Leonardo G. Capozzi and Ana Rebelo in their work related to person re-identification and scene geolocation for automatic missing person searching  [60; 61] . Also in 2021 and 2022, the author has collaborated with Pedro C. Neto, Mohsen Saffari, and Ana F. Sequeira, alongside Fadi Boutros and Naser Damer at Fraunhofer IGD (Darmstadt, Germany), on novel strategies for masked face recognition to uphold state-of-the-art biometric performance amidst a global pandemic  [50; 308; 309] .",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Organisation Of Scientific Events",
      "text": "This doctoral project and the aforementioned collaborations and synergies motivated the contribution to the organisation of multiple scientific events.\n\nIn regards to biometrics, the main example is that of the 2020 edition of the International Workshop on Biometrics and Forensics (IWBF), organised by INESC TEC and NTNU, where the author of this thesis collaborated as Demo Chair.  He   Aligned with the topic of wellbeing monitoring, and as an extension to the Easy Ride and Aurora projects, the author has also co-organised the In-Vehicle Sensing and Monitorization Workshop (ISM). This first edition of the workshop was hosted at the European Conference on Computer Vision (ECCV) in October 2022.\n\nIn a broader scope, the author of this thesis has also co-organised the special session on Machine Learning in Healthcare Informatics and Medical Biology at the International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB), in 2019 and 2021. He was also a member of the Technical and Programme committee of this conference for its 2021 edition.\n\nSince 2019, the author has also helped organise the VISUM Summer School on computer vision and machine intelligence. In the 2019 and 2020 editions, he was a member of the project team, while in 2021 and 2022 he was part of the main organisation team.",
      "page_start": 48,
      "page_end": 49
    },
    {
      "section_name": "Supervision Of Dissertations And Internships",
      "text": "The author of this thesis has collaborated, as co-supervisor or external supervisor, on the following master dissertations related to his doctoral studies (in reverse chronological order): Introduction 6. Inês Alexandra Teixeira Antunes de  Magalhães (2021) , \"Feel My Heart: Emotion Recognition Using the Electrocardiogram\", Master in Bioengineering, Universidade do Porto -as co-supervisor, alongside Jaime S. Cardoso (supervisor)  [285] ; 5. Arthur Johas  Matta (2020) , \"Open-World Face Recognition\", Master in Informatics and Computing Engineering, Universidade do Porto -as co-supervisor, alongside Jaime S. Cardoso (supervisor)  [291] ; 4. Leonardo Gomes Capozzi (2020), \"Face Recognition For Forensic Applications: Methods for Matching Facial Sketches to Mugshot Pictures\", Master in Informatics and Computing Engineering, Universidade do Porto -as co-supervisor, alongside Ana Rebelo (supervisor)  [62] ;\n\n3. João Manuel Guedes Ferreira (2020), \"Head Pose Estimation for Facial Biometric Recognition Systems\", Master in Informatics and Computing Engineering, Universidade do Porto -as co-supervisor, alongside Ana F. Sequeira (supervisor) and Jaime S. Cardoso (cosupervisor)  [130] ; 2. Carolina Martins Barbosa Rodrigues Afonso (2020), \"Changing Perspectives: Interlead Conversion in Electrocardiographic Signals\", Master in Network and Information Systems Engineering, Universidade do Porto -as co-supervisor, alongside Miguel Coimbra (supervisor)  [5] ;\n\n1. Gabriel Carneiro  Lopes (2019) , \"Don't You Forget About Me: Enhancing Long Term Performance in Electrocardiogram Biometrics\", Master in Bioengineering, Universidade do Porto -as co-supervisor, alongside Jaime S. Cardoso (supervisor)  [270] .\n\nBeyond these aforementioned dissertations, the author of this thesis also collaborated in the supervision of more than twenty students on curricular, extra-curricular, and summer internships related to biometrics, pattern recognition, and computer vision topics.",
      "page_start": 49,
      "page_end": 50
    },
    {
      "section_name": "Awards And Distinctions",
      "text": "Beyond the aforelisted peer-reviewed publications and presentations to the scientific community, the doctoral research work presented in this dissertation has also been the recipient of multiple awards and distinctions. These are listed below:\n\n• The author of this thesis was granted the EAB Max Snijder Award at the European Biometrics Awards 2022 organised by the European Association for Biometrics (EAB). This award recognised the wider perspective and applicability of his work on ECG biometrics, which contributed towards a complete deep learning solution encompassing end-to-end models, learnable template security, and explainability;\n\n1.7 Document Structure\n\n• The initial work on the Secure Triplet Loss  [345] , focused on biometric template cancelability for end-to-end deep models, received the Computers Journal Best Paper Award at the 2020 International Workshop on Biometrics and Forensics (IWBF);\n\n• The work on audiovisual group emotion valence recognition  [346]    Chapter 2",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Fundamental Concepts",
      "text": "Biometric systems are, in several ways, different from other pattern recognition applications. The need for storage of personal data from users and the different modes on which the systems can operate are only some of the special characteristics of biometric systems. When developing one, one should be aware of these specificities to ensure the best performance and robustness.\n\nHence, this chapter presents the fundamental concepts needed to build a biometric system, either for identity recognition or the monitoring of wellbeing parameters. It includes an overview of the general structure and operation of biometric systems, their security vulnerabilities, the different biometric traits (with a special focus on the electrocardiogram and face), and the metrics for thorough performance evaluations. Biometric recognition systems are tools that use hardware and pattern recognition algorithms to compare the identity of a user with that of registered individuals based on their attributes (designated as biometric characteristics or traits). Like traditional identification systems based on keys, cards, or codes, biometric systems are mostly used for access control to restricted places, confidential information, or personal data and belongings  [96] .",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Biometric Systems",
      "text": "A biometric system is typically composed of an acquisition module, a storage module, and a biometric algorithm. The algorithm can, in turn, be divided into three modules: quality assessment, feature extraction, and decision (see Fig.  2 .1)  [46; 201] . These modules are described below:\n\n• Acquisition: The acquisition module is the interface between the system and the subject and is responsible for the measurement of the biometric characteristic. The sensors used in this module should be carefully designed to fit the expected application settings and avoid, as much as possible, the noise and artefacts from environmental interference;  General structure of a biometric recognition system (from  [343] , based on  [46; 201; 351] ).\n\n• Quality Assessment: This module aims to evaluate the quality of the trait measurement and either accept it in its current form, enhance it to reduce noise and variability effects, or discard it if the quality is unacceptably low;\n\n• Feature Extraction: The feature extraction module is focused on the processing of the acquired measurements, using pattern recognition tools, to extract the most meaningful attributes of the biometric trait and thus enable a robust decision. The feature extraction process should be designed to provide attributes that present high intersubject discrimination power and low intrasubject variability;\n\n• Decision: This module uses the output from the feature extraction module and the stored information from registered users to identify the user, or validate or reject their identity claim. To achieve this, it compares the processed traits of the current user and the registered individuals;\n\n• Storage: The storage module is typically composed of a database that stores biometric templates (processed biometric trait measurements) from all individuals registered on the system. For security purposes, it can include template protection measures, such as hashing, to prevent leaks of sensitive personal information.",
      "page_start": 53,
      "page_end": 54
    },
    {
      "section_name": "Wellbeing Monitoring Systems",
      "text": "Wellbeing monitoring systems are very diverse due to the variety of parameters these can monitor, which include emotions, stress, fatigue, and health conditions. However, most of these systems follow a general structure that is very similar to that of most biometric recognition systems.\n\nJust like biometric recognition systems, an acquisition module performs the recording of data, which are checked and processed by the quality assessment module. After this, the feature extraction module extracts meaningful attributes from the trait measurements to enable accurate labelling by the decision module.\n\nMost wellbeing monitoring systems do not require a storage module, as biometric templates from the specific set of enrolled subjects will not be required, unlike in recognition systems. This is an advantage in terms of data security and privacy, which will be discussed later.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "Enrollment",
      "text": "True identity",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Biometric",
      "text": "Trait(s) In identity verification mode, also commonly called authentication, the biometric system will receive an identity claim along with the biometric measurement (the current user will claim to be a specific enrolled individual). Hence, the decision module will only compare the current measurement with the stored data from the claimed identity, performing a one-vs-one comparison, and either accept or reject the claim.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Database",
      "text": "",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Identity Veri Cation",
      "text": "In the identification mode, the biometric system will only receive the biometric measurement.\n\nThus, the decision module performs a one-vs-all comparison between the current biometric measurement and the data stored for each enrolled individual. Ultimately, the system will either assign one of the enrolled identities (corresponding to the strongest comparison) to the current user or reject to identify (if no comparison was strong enough).\n\nAdditionally, biometric systems include the enrollment phase, which comprises the acquisition, processing, and storage of a biometric template of a subject for its registration on the system.",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Acquisition",
      "text": "Processing Decision After this, the system will be able to correctly perform identification or identity verification when used by the subject  [7; 351] .",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Wellbeing Monitoring Systems",
      "text": "As aforementioned, wellbeing monitoring systems like emotion recognition devices rarely require the storage of personal information from the users. Hence, they dismiss enrollment phases and only operate in one mode: inference. In this mode, a trait acquisition is performed by the system, which will output a corresponding label. The output is composed of discrete categories (e. g., sad, happy, or angry, in emotion recognition) or continuous scores (e. g., from very awake to very drowsy, in drowsiness recognition).",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Security And Privacy Concerns",
      "text": "As key protectors of sensitive data, prized possessions, or restricted locations, biometric recognition systems are a prime target for attackers. The literature defines eight attack points (see Fig.  2 .3) that sum up the different ways to unlawfully gain access to a biometric system  [202; 297; 317] . Considering a general structure (that groups the quality assessment and feature extraction into a single processing module) are described below:\n\n• Type 1 -At the acquisition module: Such attacks are commonly called presentation attacks, as they consist of physically forcing the biometric system to grant access to the attacker, either through the use of fake biometric traits (such as fake fingers, voice recordings, or prerecorded face videos) or even through the physical destruction of the system;\n\n• Type 2 -Between the acquisition and the processing modules: In these attacks, called replay attacks, the attackers will target the communication link between the sensor and the processing module, to steal the trait acquisition. They can then bypass the sensor module by injecting the stolen trait measurements directly into the processing module;\n\n• Type 3 -At the processing module: The processing module can be attacked and overridden by another program, controlled by the attacker, that sends the desired features to the decision module upon request;\n\n• Type 4 -Between the processing and decision modules: These attacks are similar to replay attacks. The attacker will target the link between the processing module and the decision module and steal the features sent between them, to be later injected, bypassing the acquisition and processing modules;\n\n• Type 5 -At the decision module: Here, the attackers can replace the decision algorithms so they can generate high matching scores as requested, thus granting them access whenever desired, or to always output negative decisions, amounting to a denial-of-service attack;\n\n• Type 6 -At the storage module: This consists in exploiting database security flaws to add, modify, or delete templates, to ultimately grant access to unauthorised individuals or deny access to enrolled users;\n\n• Type 7 -Between the storage and decision modules: Here, the attackers intercept the communications between the database and the decision module, to steal biometric templates and replay them later;\n\n• Type 8 -Between the decision module and the application: These attacks consist in the manipulation of the data transmitted between the decision module and the application, e. g., to override a rejection decision.\n\nIn ECG-based biometrics, security vulnerabilities are still to be adequately addressed. Despite the pioneer studies of Eberz et al.  [110]  and Karimian et al.  [211] , no efforts have yet been devoted\n\nto better protect such systems. In general, biometric systems offer undeniable advantages when compared with traditional authentication systems, but this would be meaningless if the system introduced new vulnerabilities that paved the way to successful attacks. Hence, it is very important to remember the attack points presented above and their specificities throughout all stages of the development and deployment of a biometric system.\n\nOf all modules, storage is one of the most sensitive, as it stores personal data that could be used to unlawfully access private information and belongings. These intimate data are not specific to a single application and can be used by their legitimate owner as a single credential on several biometric systems (e. g., a user could use fingerprint-based access on two different computers).\n\nThis is an obvious vulnerability as, just like using the same password for several online services, a single security failure can risk the privacy of the user on several applications  [200] .\n\nRegardless of how sophisticated the database is, it can still be accessed or hacked by intruders who exert enough effort. Besides working towards more secure databases, it is paramount to prepare for possible successful attacks and ensure biometric templates cannot be retrieved in those cases  [186; 200] . Hence, regarding biometric template security, it is important to take into account the following factors:\n\n• Non-Invertibility: The processing module should be designed in a way that eases the creation of templates from biometric trait acquisitions. However, the retrieval of a close approximation of the original trait measurement or feature set, from a stored template, should be difficult and sufficiently time-consuming to render the process unfeasible or unattractive for attackers;\n\n• Revokability/Cancelability: Keys can be changed when moving to a new house, and users can easily change e-mail passwords after they have become compromised. However, biometric systems rely on intrinsic personal characteristics, such as facial features or fingerprint minutiae, which are very difficult (or even impossible) to be changed. Hence, biometric systems should include measures that allow the easy invalidation of templates when these have become compromised. Thus, intruders will be denied access when using those credentials, but legitimate users will still be allowed to authenticate using their unchanged biometric trait;\n\n• Unlinkability: For improved performance, a single biometric system can store, separately, more than one template for each user. The data protection scheme should ensure that the comparison between stored templates does not enable an attacker to cluster them by identity. This should also be difficult with templates from the same user in different biometric systems so that attackers are not able to attack multiple systems with a single stolen template  [186; 304] .\n\nBesides these factors, the biometric system should also be able to deal with the characteristic variability of biometric traits and their measurements. Methods like hashing are commonly used for passwords, but such traditional credentials do not present variability. With biometric systems, small variations of the input should be considered normal and acceptable (e. g., with the face, different haircuts or beard styles), and should not influence the final decision. When designing a secure biometric system, it is necessary (although hard) to find an equilibrium between noninvertibility, unlinkability, and the controlled acceptance of natural intrasubject variability.",
      "page_start": 56,
      "page_end": 58
    },
    {
      "section_name": "Biometric Traits",
      "text": "",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "General Overview",
      "text": "Biometric traits are human attributes that include enough personal information to reliably serve as the basis for the recognition and discrimination of individuals  [9; 213] . According to the identity information they carry and the performance they can offer, biometric traits can either be considered hard traits, strong enough to be standalone traits in a reliable biometric system, or soft traits, which need further traits or information to offer acceptable recognition performance (see Table  2 .1).\n\nTraits can be categorised according to their nature, as anatomical, physiological, or behavioural traits. Anatomical traits result from measurements of parts of the human body and include the fingerprints, the face, and the iris. Physiological traits are those that originate from physiological or behaviours, such as their gait (walking cadence), their signature, or their voice  [2; 9] .\n\nThe quality of a biometric trait can be defined, as proposed by Jain et al.  [199] , through seven different aspects:\n\n1. Universality: the trait should be present in all subjects using the system; 2. Uniqueness: the trait should include enough personal information to present differences between all subjects, and thus allow their identification;\n\n3. Permanence: despite the intersubject variability desired (uniqueness), the trait should be sufficiently stable over time (reduced intrasubject variability) to allow the identification through the comparison of measurements in different instances;\n\n4. Measurability: the trait should be easily and comfortably acquired and digitised, and its representation should allow easy processing and measurement;\n\n5. Performance: a system based on such a trait should meet or exceed the recognition accuracy requirements, set by the context in which it will be applied;\n\n6. Acceptability: there should be no foreseeable reservations that could make the subjects unwilling to allow the trait acquisition;\n\n7. Circumvention: the trait should be as hard as possible to mimic or counterfeit, in any way, to prevent spoofing of the biometric system.\n\nAbo-Zahhad et al.  [2]  compared sixteen biometric traits according to their compliance with each of the seven defined qualities. In that comparison, it is possible to verify that the traits with the lowest overall quality are the behavioural ones (gait, keystroke, signature, and voice) and phonocardiogram (heart sounds), with low performance, permanence, and distinctiveness.\n\nLow circumvention and universality are also downsides of behavioural traits. The traits with reported highest overall quality are the DNA, facial thermogram, fingerprint, iris, palm print, and ECG. Overall, the electrocardiogram excels in most factors, with just two 'average' scores (for collectability and acceptability).\n\nTraits like fingerprint, face, signature, iris, and voice have been the most studied. However, these have long seen a quick growth and evolution of spoofing methods (methods of counterfeiting a certain user's trait to unlawfully gain access through the biometric system), which urges researchers to find more robust alternatives  [13; 213] .\n\nThroughout this doctoral work, the focus will be on the electrocardiogram (ECG), an emerging biometric trait that offers unique advantages regarding inherent liveness, anti-spoofing abilities, and wellbeing insight. Its performance and robustness drawbacks shall be mitigated through its fusion with face, a well-established and robust biometric trait. Hence, the next subsections consist of a presentation of both biometric traits, the ways to measure them, and the variability factors that provide them with identity and wellbeing information.",
      "page_start": 58,
      "page_end": 60
    },
    {
      "section_name": "Electrocardiogram",
      "text": "The electrocardiogram (ECG) is a physiological signal generated from the contraction and the recovery of the heart, that has been gaining traction as a biometric trait  [343] . The heart has three main functions: generate blood pressure to keep blood circulating, route venous and arterial blood to the respective parts of the body, and regulate blood supply according to the metabolic demands  [428] . To do this, the heart needs to contract and relax its muscle, the myocardium, through the controlled generation and flow of depolarisation and repolarisation currents  [377; 428] .\n\nThe measurement of such currents using electrodes placed on the body is designated as electrocardiography and results in the electrocardiographic (ECG) signal. In normal conditions, the ECG is a cyclic repetition of five easily recognisable deflections: the P, Q, R, S, and T waves (see Fig.  2 .4). A group of these deflections comprises a single heartbeat and each deflection can be traced back to the phase that originated it  [286; 377; 428] .",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Depolarisation Depolarised Repolarisation",
      "text": "Figure  2 .4: The sequence of depolarisation and depolarisation events in the heart, and their relationship with the different heartbeat waveforms in an ECG signal (from  [343] , based on  [286] ).",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Acquisition",
      "text": "The configurations used for the acquisition of ECG signals for biometric purposes have greatly evolved. From the first ECG-based biometric research works, considerable efforts have been devoted to more usable and comfortable acquisition technologies. This aims to place the ECG as a more attractive alternative to established biometric traits, mitigating the main disadvantage of the ECG, the obtrusive measurement techniques  [343] .\n\nIn early ECG-based biometrics research, recordings from the standard 12-lead or Frank leads were commonly used for the development and evaluation of algorithms  [144; 349; 463] . These are two defined and established configurations of electrodes for standardised and comparable ECG measurement (see Fig.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Holter Acquisition Equipment",
      "text": ".6: Acquisition settings with movement: example of a five-electrode Holter system for ambulatory recordings (from  [343] , electrodes depicted in blue).",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Off-The-Person Configurations",
      "text": "",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Thumb Button Electrodes",
      "text": "",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Index Finger Electrodes",
      "text": "",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Metallic Rod Electrodes",
      "text": "Electrodes mounted on a table",
      "page_start": 63,
      "page_end": 63
    },
    {
      "section_name": "Figure 2.7:",
      "text": "Examples of off-the-person ECG acquisition configurations, using thumb electrodes  [69] , index finger electrodes  [289] , metallic rods grabbed by the subjects  [32; 33; 261; 390] , or electrodes mounted on a table  [394]  (from  [343] , electrodes depicted in blue).\n\nacquisition due to the high number of electrodes and their placement on the chest and legs of the users. Although allowing for longer acquisitions with movement and activity, Holter acquisitions still require the placement of electrodes on the torso. This significantly reduces acquisition acceptability and comfort and damages the ECG strength as a biometric trait.\n\nTo improve acceptability and acquisition comfort, and get closer to biometric systems deployable in real settings, wet electrodes are being replaced by dry metallic electrodes, their number has been reduced to two or three, and their placement has been confined to the upper limbs, especially the on wrists, hands, or fingers (see Fig.  2 .7). These acquisition configurations were designated as off-the-person settings. The first research works in ECG biometrics to use such signals were, to the best of our knowledge, Molina et al.  [298] , who used commercial metallic electrodes strapped to the wrists of the subjects, and Chan et al.  [69] , who acquired ECG signals using dry button electrodes held by the subjects in contact with their thumbs.\n\nSince then, ECG signals have been recorded using metallic rod electrodes  [32; 33; 261; 390] ,\n\nand dry metallic electrodes mounted on plaques  [275]  or attached to the users' fingers  [289] , which offer increased comfort over on-the-person techniques. Nevertheless, off-the-person systems still require the user to hold the electrodes or deliberately place the fingers or palms over them. This prevents us from designating them as unconstrained systems, which puts the ECG at a disadvantage over other biometric traits. Besides this, the use of dry electrodes in farther placements makes the acquisition more vulnerable to interference, thus affecting the quality of the signal  [32; 394] .\n\nRecently, some researchers have tried to improve off-the-person configurations and approach unconstrained settings in ECG biometrics. As for commercial applications, the Nymi Band  [11] , a wearable wristband, acquires the ECG using two metallic electrodes on its inner and outer surfaces. Identity verification is performed when the band is put on and the session remains open until the band is taken off. While a session is open, the Nymi Band broadcasts an identity signal to authenticate the user in other nearby systems. The CardioWheel  [279]  is a steering wheel cover that uses conductive leather for seamless and continuous biometric recognition and health monitoring of drivers, focused on automatic personalisation of driving settings and remote fleet supervision.\n\nThe miBEAT  [471]  is a versatile platform for the simultaneous acquisition of ECG and photoplethysmography (PPG) signals, which can be used for seamlessly integrated signal acquisition in smartphones or tablets. AliveCor provides a set of commercial solutions for easy ECG acquisition in the Kardia 1  lineup, including the KardiaMobile, to be used with typical smartphones, and the KardiaMobile Card, a credit card-sized slim single-lead acquisition device with integrated metallic electrodes.\n\nThese recent efforts have brought ECG biometrics closer to viable and unconstrained applicability. However, these newer technologies still require the users to wear certain products or perform specific actions and need contact with both limbs during acquisition. Besides this, the quality of the acquired signals is typically very low, because of the loose contact with the subject's skin, suffering from wide impedance variations, sensor saturation, and contact loss artefacts.\n\nSome researchers have already started to address these issues. The single-arm acquisition settings studied by Zhang et al.  [484]  and the contactless electrodes developed by Chi et al.  [75]  Figure  2 .9: Variability in off-the-person ECG heartbeats from the same subjects (from  [337] , individual heartbeats superposed after denoising, amplitude normalisation, and outlier removal).\n\nraise new and inspiring possibilities for wearable ECG devices. For applications based solely on heart rate, techniques have been proposed to measure it at a distance, using microwave Doppler sensors  [48; 316; 376] .\n\nThese efforts pave the way for better ECG acquisition technologies. Such systems could consist of seamlessly integrated biometric systems that can acquire ECG signals at short distances from one hand of the user, without requiring contact and thus suffering from signal loss. For wearables, the future could reside in products that can continuously monitor the users' ECG while only contacting with one of their wrists, or when inside their pockets separated from the body by clothes. Despite all efforts devoted so far to ECG biometrics, much work is still needed to reach true applicability in the form of real, comfortable, and easy-to-use ECG-based biometric systems.",
      "page_start": 64,
      "page_end": 65
    },
    {
      "section_name": "Variability",
      "text": "Although the ECG signals present, in normal conditions, the same deflections for all subjects at all times, these are characterised by a high degree of variability (see Fig.  2 .9). Variability in the ECG can be designated as intrasubject, the variations between cycles (heartbeats) in the electrocardiogram of a single subject, or intersubject, the variations between heartbeats of different subjects.\n\nIntrasubject variations on the ECG signal are mainly explored for health monitoring and medical diagnosis  [8; 164; 370] , while intersubject variations are especially useful to discriminate between subjects in biometric recognition. Both intrasubject and intersubject variability can originate from several factors, such as:\n\n• Heart Geometry: Heart size, cardiac muscle thickness, and the overall shape of the heart influence the trajectories of electrical currents throughout the heart, the number of muscle cells that will conduct those currents, and the time to do it across the whole heart. Athletes, because of intensive physical training, commonly have thicker myocardia, which affects the ECG with higher voltages in the QRS complex, and lower basal heart rates  [171; 172; 441] ;\n\n• Individual Attributes: Age, weight, and pregnancy are some individual attributes that can cause shifts in the heart position and orientation. These shifts will change the orientation of the electrical current conduction vectors across the heart, meaning the electrodes will detect the signal from a different perspective, thus altering the ECG waveforms  [379] ;\n\n• Physical Exercise or Meditation: The duration of, and intervals between the different deflections of the heartbeats in an ECG signal, vary with the heart rate. These changes are especially visible on the interval between the QRS complex and the T wave in situations of tachycardia (higher heart rates) or bradycardia (lower heart rates). Changes in the heart rate caused by physical exercise or meditation are reflected on the electrocardiogram  [10] ;\n\n• Cardiac Conditions: Medical conditions of the heart can also interfere with the dynamics of electrical pulse conduction and generate variability. In biometrics, one of the most studied conditions is Arrhythmia, which causes wide variations in the heart rate across time and, as reported by several researchers, can consistently shrink the performance of ECG-based biometric systems  [9; 369; 472] .\n\n• Posture: Postures such as standing or laying down change the position and shape of internal organs. The heart is also affected by this, changing its position in the thorax, and thus its position in reference to the electrode placement, which will cause variations in the collected ECG signal  [379] ;\n\n• Emotions and Fatigue: The sympathetic and parasympathetic systems of the autonomous nervous system work to, respectively, increase or reduce the heart rate. These systems are under the direct influence of psychological states and thus, under stress, fear and other strong emotions, fatigue or drowsiness, the heart rate and the ECG signal can be affected  [10; 370] ;\n\n• Electrode characteristics and placement: The type, the size, and the number of electrodes, whether they are wet or dry, and the positioning on the chest or limbs, can influence the dominance of noise on the signal. The mispositioning of electrodes and reversal of leads are also sources of variability, as they change the perspective of detection of the electrocardiographic signal  [171; 379] . of ECG for a specific application, whether for medical or biometric recognition, it is paramount to consider these factors, the way they can ease or difficult the task at hand, and how to mitigate their negative effects.",
      "page_start": 65,
      "page_end": 66
    },
    {
      "section_name": "Face",
      "text": "The face may be considered the most intuitive of all biometric traits, since humans use facial features as the main clues for the identification of other people. Despite the challenges in its use for biometric recognition, the face offers the unique advantage of being the only trait that can be acquired using sensors at a considerable distance from the user, possibly without their knowledge  [55; 166] . Below, the ways to measure the face trait and the factors that may affect the measurement are described and discussed.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Acquisition",
      "text": "Among widespread and inexpensive cameras and sophisticated thermal sensors, the face trait can be acquired in the following settings  [21; 123] :\n\n• Visible Spectrum: This is the most common setting in face biometrics. Visible light, with a wavelength in  [400, 750]  nm, generally from natural and unconstrained sources in the application environment, is reflected by the face of the user and captured by the sensor. Using the visible spectrum of light has some disadvantages, discussed in subsubsection 2.2.3.2, but it is commonly less expensive than the alternatives and, thus, the most fitting option for widespread applications. Depending on the frequency spectra used, the face trait can be categorised as either anatomical (visible light), or physiological (infrared spectrum). This is because the former mostly captures anatomical features of the face, while the latter is much more dependent on physiological factors that affect blood flow and face heat patterns.\n\nThe face trait measurements can be acquired simultaneously with depth information, resulting in an upgrade of 2D images to 2.5D or 3D. While 2D data only includes colour or greyscale pixel information, 2.5D data combines that with depth information about the face of the subject, which is useful to increase recognition accuracy or avoid attack attempts. 3D data is a further step, where both sources of information are fused to build tridimensional models of the subject's face (see",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Variability",
      "text": "The measurement of the face trait is affected by several intrinsic, environmental, or operational factors (see Fig.  2 .12). These can enhance intersubject or intrasubject variability, and thus make Figure  2 .12: Variability in unconstrained face images of a subject (from  [205] ).\n\nthe recognition task easier or more difficult  [4; 21; 55; 102; 103] . These factors are:\n\n• Illumination and Heat Sources: This factor is most relevant for visible spectrum acquisition settings, as the illumination with visible light will directly affect the light received by the sensor and change the face image. In infrared images, the influence of illumination can also be felt, due to the heat generated by light and heat sources, although generally in a lesser degree than in the visual spectrum;\n\n• Pose Variations: Variations in pitch, roll, and yaw of the subjects head, relative to the sensor, will result in the capture of facial features in different perspectives, which may encumber pattern recognition tasks. Moreover, the relative position of those facial features will also suffer from pose variations. Some authors argue infrared imaging is less susceptible to pose variations than visible spectrum sensors  [135] ;\n\n• Facial Expressions: Like pose variations, facial expressions change the relative positions of the facial features, which may encumber the task at hand. Furthermore, it may create new features (e. g., wrinkles) or hide facial features that would be needed for the task. As with the previous factor, facial expressions were found to have less impact in infrared imaging  [135] ;\n\n• Stress, Emotions, and Illnesses: Stress, emotions like happiness and fear, or illnesses like headaches or tooth infections can influence facial expressions. But more than that, they change the heat patterns on a subject's head, which will influence the face images captured by infrared sensors;\n\n• Age: Growing old is generally accompanied by expression wrinkles, grey hair, among other changes that may affect someone's appearance and make it more difficult for them to be recognised;\n\n• Cosmetics, Tattoos, Disguises, and Accessories: Makeup, face tattoos, disguises, and accessories like rings or piercings can change, hide, or create new facial features. Recognition algorithms may not be prepared to deal with this new information and be induced into errors;\n\n• Occlusions and Surrounding Objects: Like disguises and accessories, objects surrounding the person can sometimes occlude part of the face of the subject, and thus hide certain facial features that could be key for the recognition task at hand;\n\n• Sensor Quality and Stability: The sensor is another important variability source. Using different cameras, with different characteristics, will result in different face images. Furthermore, the movement of the cameras (just like the movement of the subject) can cause blurred images that will disable the retrieval of finer facial features.\n\nLike with the electrocardiogram, it is important to appropriately weigh all these factors when designing a biometric system. Some will contribute more to intersubject variability, and will be desirable for biometric recognition, while others will enhance intrasubject variability, and empower wellbeing monitoring. Considering the task at hand, it is key to design the acquisition and processing stages to focus on the factors that fit the application needs.",
      "page_start": 68,
      "page_end": 69
    },
    {
      "section_name": "Performance Evaluation",
      "text": "",
      "page_start": 69,
      "page_end": 69
    },
    {
      "section_name": "System Design Considerations",
      "text": "When designing, developing, and deploying a biometric system, it is important to be aware of how it will behave in realistic conditions. Hence, designers and developers should consider some central aspects, as described by Bolle et al.  [46] :\n\n• System accuracy: This measures the frequency with which the biometric system makes correct or wrong decisions;\n\n• Computation time: This corresponds to the time required by the system to output a decision, starting from the moment the user initiates contact with the acquisition module. This time depends on the processes of trait acquisition, quality assurance, feature extraction, and decision, and should be as low as possible, to improve usability. This aspect is especially important for continuous recognition systems, where decisions should be quickly output and frequently renewed;\n\n• Exception handling: Because of universality issues and sensor flaws or unavailability, some users may find the system unable to acquire their biometric traits. Also, software errors may occur and render the system incapable of adequately performing its function. These possibilities should be addressed during system design and development, to ensure the application works even without the biometric system;\n\n• System cost: The system cost includes all expenses related to acquisition and processing equipment needed, algorithm development and implementation, routine maintenance, and operational costs. It should be as low as possible, to make the biometric system more affordable;\n\n• Security: Biometric systems decisions can serve as proof of the actions of a certain individual, and this, in some settings, can escalate to serious legal implications. Thus, it is important to minimise the possibility of decision flaws that allow impostors to act under the identity of an authorised person;\n\n• Privacy: Biometric systems require the storage of templates, consisting in discriminating information about each of the enrolled individuals. That information, in the interest of anonymity and security, should be kept as safe as possible, resorting to encryption techniques that allow matching but minimise the possibility of reconstructing the original acquired trait data.\n\nSome conflicts may exist between these aspects. Using more sophisticated approaches (such as deep learning) often leads to higher system accuracy, but at the expense of higher computational complexity. Using passwords or credentials as a fallback in case of an exception weakens system security. Tighter security measures generally lead to more frequent rejection of legitimate users, which translates into reduced accuracy.\n\nUltimately, trying to fully cover all aspects will increase costs. Hence, all these aspects should be carefully analysed and weighted, considering the expected application to get an affordable, efficient, accurate, secure, and usable system. In the next subsection, the most relevant metrics are presented, regarding the specific aspect of system accuracy.",
      "page_start": 70,
      "page_end": 71
    },
    {
      "section_name": "Recognition Accuracy Measurement 2.3.2.1 Identity Verification",
      "text": "As described before, identity verification consists in accepting or rejecting an identity claim made by a user based on their biometric trait measurements. As such, there are four outcomes:\n\n1. The claim is true and the system correctly accepts it;\n\n2. The claim is false and the system correctly rejects it;\n\n3. The claim is true, but the system incorrectly rejects it;\n\n4. The claim is false, but the system incorrectly accepts it.\n\nIn identity verification, the main goal is to minimise the frequency of outcomes three and  From the ROC curve, it is also common to extract two metrics that combine all results into a single performance value, easing performance comparison between algorithms. The Equal Error Rate (EER) is the error that corresponds to the operation point where FAR = FRR and represents an equilibrium between convenience and security (see Fig.  2 .13). The Area Under the Curve (AUC) measures the area under the ROC curve and serves as a measure of overall quality of a biometric system.",
      "page_start": 72,
      "page_end": 72
    },
    {
      "section_name": "Identification",
      "text": "In identification, the system does not receive an identity claim. Hence, it will compare the current biometric measurements with the stored templates to assign one of the enrolled identities to the\n\n.13: Example of a Receiver Operating Characteristic (ROC) curve for an identity verification system (left) and the evolution of False Acceptance and False Rejection rates with the threshold value (right) (from  [343] , adapted from  [419] , example for a similarity-based matching method including the Equal Error Rate point and the Area Under the Curve).\n\nsubject. Additionally, it can reject to identify when the strongest match is, still, not strong enough, which is generally asserted based on a threshold (as in identity verification mode).\n\nThus, in the identification mode, there are five possible outcomes:\n\n1. The subject is enrolled and the system correctly identifies them, granting them access;\n\n2. The subject is enrolled, but the system mistakes their identity for another enrolled subject, granting them access under a wrong identity;\n\n3. The subject is enrolled, but the system fails to identify them with any enrolled subject, rejecting access;\n\n4. The subject is not enrolled and the system correctly rejects access;\n\n5. The subject is not enrolled, but the system erroneously identifies them as one of the enrolled subjects and grants them access.\n\nA good biometric system would maximise the frequency of outcomes 1 and 4 and minimise the frequency of the remaining outcomes. Most metrics for identification mode (see Table  2 .2) are based on these frequencies  [7; 46; 155] . Regarding the situations where the subject is enrolled (designated as legitimate trials), the most common metrics are:\n\n• True-Positive Identification Rate (T PIR or Hit Rate): For a total number of legitimate trials, T PIR corresponds to the fraction of those where one of the system's R strongest predictions corresponds to the true subject identity. As most other metrics, T PIR depends on the defined threshold, the selected number of top ranks R, and the list of enrolled candidates (in identification, each enrolled subject is considered a candidate, and TPIR, as most metrics, varies not only with the size of L but also with the variety and individual characteristics of each subject in L);\n\n• Identification Rate (IDR or Accuracy): IDR corresponds to T PIR when only the single highest ranking prediction is considered (R = 1). It corresponds to the fraction of the legitimate trials where the true identity was the method's strongest prediction above the threshold. In the literature, this is one of the most used metrics;\n\n• Reliability: Reliability corresponds to T PIR with R = N (where N is the number of enrolled subjects), we get the reliability metric. This measures how frequently the true identity satisfies the minimum threshold constraint, regardless of its ranking;\n\n• False-Negative Identification Rate (FNIR or Miss Rate): Represents the fraction of trials where the true identity does not correspond to one of the R strongest predictions above the threshold T ;\n\n• Reject Rate (RR): This metric pertains to the very specific situations where all identity predictions stand below the defined threshold T , and the system has no choice but to reject to identify (RR = 1 -T PIR -FNIR);\n\n• Misidentification Rate (MIDR or, commonly, Misclassification Error): MIDR is the complement of IDR (MIDR = 1 -IDR), equivalent to FNIR with R = 1, and measures the fraction of legitimate trials where the true identity is not the system's top ranking prediction above T .\n\nRegarding situations where the subject is not enrolled in the biometric system (designated as impostor trials), the most common metrics are:\n\n• False Positive Identification Rate (FPIR): It is the fraction of impostor trials where at least one of the system's predictions meets the threshold criterion, and the system thus grants access to the unenrolled subject;\n\n• Selectivity: Similar to FPIR, selectivity counts the average number of predictions above the threshold T across all impostor trials.\n\nAs in identity verification mode, some characteristic curves can be drawn based on these metrics and the defined thresholds, to help evaluate the algorithms more robustly. The first is the Cumulative Match Characteristic (CMC), which plots T PIR with threshold T = 0 against R, varying\n\nThe second is the Receiver Operating Characteristic (ROC) which, in the case of identification, plots Reliability against FPIR, for various threshold values (see Fig.  2 .14). From these plots, we can also extract the AUC and EER metrics.",
      "page_start": 73,
      "page_end": 74
    },
    {
      "section_name": "Time-Based Performance Measurement",
      "text": "There are biometric systems that perform recognition upon request, for example in computers or smartphones, keeping the session open until the user ends it or it reaches an idle time limit. This creates security issues, specifically when the user leaves the device unattended and forgets to close the session.",
      "page_start": 74,
      "page_end": 74
    },
    {
      "section_name": "Auc",
      "text": "Cumulative Match Characteristic Receiver Operating Characteristic R FPIR y t i l i b a i l e R ing Characteristic (ROC) curve for an identification system (from  [337] , including a representation of the Area Under the Curve, AUC).\n\nTo solve this issue, there are continuous (or online) biometric systems, that aim to perform biometric recognition in real-time, acquire traits continuously, and renew decisions as frequently as possible based on the most recent acquisition. With such systems, if the user leaves and is replaced by an attacker, the system would be able to detect this and close the session before any harm could have been done.\n\nBesides accuracy, an important facet of performance evaluation for continuous biometric systems is timeliness. Sim et al.  [400]  have addressed this issue and proposed some time-based metrics for the evaluation of continuous identity verification systems. These can be adapted for both identification and identity verification, as described below:\n\n• Time to Correct Decision (TCD): TCD measures the time the system takes to detect an impostor, and take an appropriate decision, relative to the moment the impostor replaces a legitimate user. For an ideal system, this should be zero, but that is virtually impossible to achieve. Hence, Sim et al.  [400]  state that this window should at least be always lower than W , called Window of Vulnerability (the minimum access time required for the impostor or wrong individual to cause any kind of damage);\n\n• Probability of Time to Correct Decision (PTCD): PTCD measures the probability, for a given system, of TCD being lower than W . The higher this value, the lesser the probability of an impostor having time to cause damage before the system acts;\n\n• Usability: In a normal continuous biometric system, we should expect some decisions to be incorrect. Over t seconds of usage, Usability measures the fraction of t where the legitimate user is deprived of access due to wrong decisions of the system. For any biometric system, this should be as close to zero as possible. From these metrics, the authors also define the Usability-Security curve (USC), a new characteristic curve that plots Usability vs. PTCD for a varying threshold T (see Fig.  2 .15). USC is similar to a ROC curve and, thus, AUC can also be computed, being considered a good metric to evaluate the timeliness of a biometric system.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Wellbeing Monitoring Performance Measurement",
      "text": "For wellbeing monitoring systems, several metrics have been used to measure their performance  [299] . The choices depend on the nature of the ground-truths and system outputs.\n\nFor example, emotion recognition or affective computing algorithms generally focus either on a limited set of discrete emotion categories or on continuous ranges of emotion qualifiers. In the former case, they typically cluster all emotions onto six categories designated as the six basic emotions by Ekman  [112] : happiness, sadness, anger, fear, surprise, and disgust.\n\nFor systems based on categorical labels such as these, the most common metrics are:\n\n• Accuracy: The accuracy is the fraction of test samples that have been correctly classified by the algorithm. It is widely used in wellbeing monitoring applications;\n\n• F1-score: The F 1-score is the harmonic mean of precision p (the fraction of positive predictions that are truly positive) and recall r (the fraction of positive samples that are classified as such), through the expression F 1 = 2(pr)/(p + r). It is commonly used to evaluate performance in binary tasks;\n\n• AUC: The Area Under the ROC Curve, which plots sensitivity (the fraction of positive samples correctly classified) versus the complement of specificity (1-specificity, the fraction of negative samples incorrectly classified as positive) for several thresholds, is also commonly used for binary classification tasks. Figure  2 .16: Illustration of the bidimensional valence-arousal space with example emotion categories (adapted from  [392] , with valence on the horizontal axis and arousal on the vertical axis).\n\nIn the case of continuous labels, emotion recognition systems typically consider a bidimensional space with two main emotion qualifier variables: valence and arousal  [240] . Valence measures the pleasure of the emotion being felt, ranging from negative (unpleasant) to positive (pleasant). Arousal measures the level of activation or intensity of the emotion, ranging from low (passive) to high (active). Fig.  2 .16 illustrates these concepts by offering examples of categorical emotions in the bidimensional valence-arousal space.\n\nFor systems focused on regression tasks such as these, outputting continuous scores, the most common metrics are:\n\n• Root Mean Squared Error (RMSE): RMSE is the root square of the mean of all squared differences between corresponding predictions and ground-truths;\n\n. This is a very common metric for performance evaluation of emotion recognition over time;\n\n• Sign Agreement Metric (SAGR): The sign agreement metric combines the magnitude of the prediction error with a penalisation for sign errors. It can be computed with SAGR =\n\n, where δ is the Kronecker delta function. This metric has been mainly used for valence and arousal prediction, where the concordance of signs between the predictions and the ground-truths can be more important than the magnitude of the scoring errors.\n\nAmong these alternatives, it is important to consider the task at hand when selecting metrics for an adequate performance evaluation that also enables a simple and thorough comparison with the state-of-the-art.",
      "page_start": 76,
      "page_end": 77
    },
    {
      "section_name": "Part Ii",
      "text": "",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "Electrocardiogram Biometrics",
      "text": "Chapter 3\n\nPrior Art in Electrocardiogram Biometrics",
      "page_start": 78,
      "page_end": 78
    },
    {
      "section_name": "Data",
      "text": "Numerous researchers, when working with ECG signals, for biometric recognition purposes or automatic diagnosis of medical cardiac conditions, opt for private acquisitions of data. However, as the needs grow for more complete datasets, with more subjects, including medical conditions, on more sessions, spread across wider time frames, and under different posture and activity conditions, researchers became more aware of the importance of public signal collections  [394] .\n\nMoreover, public ECG databases are needed to enable the comparison and benchmarking of algorithms in challenging conditions, across different publications, without requiring authors to implement algorithms and evaluate them again. Below, we delve into the important aspects behind a well-structured ECG signal collection, we present the most relevant publicly available collections, and we discuss the current needs and future possibilities regarding data in ECG biometrics.",
      "page_start": 81,
      "page_end": 81
    },
    {
      "section_name": "Building A Complete Ecg Data Collection",
      "text": "A well-structured ECG signal collection is key to appropriately guiding the development towards the exploitation of the best possibilities for the system, and accurately predicting its performance upon real-life application. To achieve such a complete collection, a few aspects have to be considered:\n\n• Number of electrodes: Fewer electrodes and leads have been shown to provide more challenging settings for biometrics  [122; 350] ;\n\n• Electrode placement: As shown by  [490] , the use of chest leads is less challenging than limb leads, and the distance of the electrodes to the heart has a significant negative impact on the system's performance;\n\n• Sampling frequency: Sampling causes the loss of fine details that influence the recognition process  [350] . The lower the sampling frequency, the larger the amount of detail that can be lost, and the higher the risk of aliasing of high-frequency noise (such as electromyogram interference);\n\n• Subject posture, activity, and fatigue: Several studies have shown that fatigue, exercise, or different postures have a negative effect on recognition performance if the systems have not been trained accordingly  [332; 350; 445] ;\n\n• Subject health: Some health issues, mainly arrhythmia, can generate intrasubject signal variability that encumbers the recognition process  [95; 96; 369] . Thus, systems should be made robust against this, by including subjects with heart conditions in the datasets used during the development and validation of the methods;\n\n• Number of subjects: The diversity of individuals and their own characteristics may ease or difficult the job of the biometric systems  [104; 468] , and successful state-of-the-art algorithms have been shown to be significantly worse when evaluated on larger datasets  [319] .\n\nThe use of a collection with a large number of subjects ensures the presence of subject diversity, increasing the thoroughness of the performance assessment. As discussed in  [343] ,\n\nthe vast majority of literature in ECG biometrics reports the use of data from less than 100 subjects;\n\n• Acquisition sessions: The ECG signal varies enough to cause recognition errors in most biometric systems, even over a short 24-hour period  [236; 237] . Systems should be prepared with data from several sessions, weeks or months apart, to ensure their robustness  [350; 393] .\n\nAll these factors can have an impact on the performance of an ECG-based biometric system.\n\nIn order to correctly assess the capabilities of such systems, it is of the highest relevance to not only build a database that fits the system's expected application context, but also one that reflects all possibilities mentioned above, in order to study the use of the same biometric system in a wider set of contexts.",
      "page_start": 81,
      "page_end": 82
    },
    {
      "section_name": "Publicly Available Data",
      "text": "Currently, there are several collections, publicly available for ECG biometrics research 1 , which try to cover the aforementioned factors to create a challenging environment for the development of robust biometric systems. Many are stored by Physionet 2 , while others are ceded by their owners upon request. Below, we present and characterise the most relevant of the currently available Table  3 .1: Summary of the technical specificities of the most relevant publicly available ECG collections (from  [343] , OP -off-the-person; NS -number of subjects; Fs -sampling frequency (Hz); L / E -number of leads/electrodes). • DriveDB: Resulting from the Stress Recognition in Automobile Drivers initiative, this database was created with the purpose of monitoring stress in drivers  [168] . Various physiological parameters (electrocardiogram, electromyogram, and skin conductivity) were recorded from 9 subjects over a total of 18 driving sessions, including periods of rest (lower stress levels), highway driving, and city driving (higher stress levels);\n\n• ECG-ID: The ECG-ID is a database entirely focused on biometrics  [281; 307] . 20-second ECG recordings were collected from 90 subjects, and are currently available on Physionet.\n\nFor each subject, the database has between 2 and 20 recordings (a total of 310) collected over six months. The signals were acquired from Lead I using limb-clamp electrodes at the wrists;\n\n• E-HOL 24h Holter: This is an ECG database, focused on biometrics, from the University of Rochester 5  . A total of 203 healthy subjects were recorded using a Holter monitor for 24 hours, with four electrodes placed on the chest, from 3 leads following a pseudo-orthogonal configuration;\n\n• European ST-T: The European ST-T database  [421]  was originally intended for the analysis of ST and T-wave changes. The database is composed of 90 two-hour excerpts of recordings from 79 subjects, from 2 leads, and includes abnormalities with origin in myocardial ischaemia, hypertension, ventricular dyskinesia, and effects of medication;\n\n• FANTASIA: The FANTASIA database  [195] , available on Physionet   [343] ).\n\n• QT: The QT database aims to aid the development of automatic methods of measurement of QT waveforms  [239] . This collection is a compilation of 105 15-minute relevant recording extracts from other public databases;\n\n• UofTDB: The University of Toronto ECG Database  [445]  was specifically created for biometrics and addresses several important criteria for a thorough evaluation of biometric performance. The off-the-person ECG signals were captured using dry electrodes at the thumbs of a total of 1019 subjects. For each subject, the database includes up to six recordings over a period of six months, in various postures: supine, tripod, exercise, sitting, and standing.\n\nWhile many researchers opt to use private acquisitions of data for their studies on ECG biometrics, public datasets have been crucial in allowing the appropriate comparison of results across publications. Nevertheless, if our goal is to increase competitiveness between ECG-based biometrics and more developed traits, we should address some concerns regarding public collections.\n\nCurrently, countries like India, China, and the United States, are starting to invest in nationwide identification systems for their large populations  [198] , which awakens the need for biometric systems that can robustly discriminate between several million enrolled subjects. To keep up with this trend, we need to work towards the creation of public ECG collections with a larger number of subjects. PTB-XL is impressive in this regard, as it includes signals from over eighteen thousand subjects. However, it is still quite limited considering the extent and diversity of data per subject, which are very important aspects for biometrics.\n\nMoreover, researchers can currently choose from small on-the-person datasets that include Figure  3 .2: General structure of an ECG-based recognition system (from  [344] ).\n\nhealth conditions and longer acquisition times (such as the AHA, European ST-T, and the MIT-BIH Arrhythmia databases), or the off-the-person UofTDB collection with short recordings from several healthy subjects. This calls for the creation of a public database with a number of subjects similar or superior to UofTDB, with several longer off-the-person recordings (ideally over one hour), taken over long periods (months to years), during different activities and postures.\n\nECG-BG, used by Ingale et al.  [187] , is composed of off-the-person data from 1119 healthy and unhealthy subjects, but it is still not publicly available.\n\nFinally, it would also be very beneficial to have publicly available collections of signals acquired using recent wearable and seamless technologies, such as the CardioWheel and the Nymi Band. The highly acceptable acquisition settings offered by such products places, undoubtedly, new challenges on signal noise and variability, that would be very useful for the development of robust biometric algorithms.",
      "page_start": 82,
      "page_end": 82
    },
    {
      "section_name": "Related Work",
      "text": "Despite being more recent and less developed than the face or fingerprint, the electrocardiogram (ECG) is quickly growing as a biometric trait, especially due to its inherent liveness and antispoofing capabilities.\n\nA comprehensive review of prior art in ECG-based biometric recognition is available in  [343] ,\n\npublished in the scope of this doctoral work, and succinctly summarised in Table  A .1. In this section, we present a summary of the survey, organised in the four common stages of an ECGbased recognition algorithm: signal denoising, signal preparation, feature extraction, and decision (see Fig.  3 .2). We also offer a discussion on the approaches based on deep learning and the current challenges and possibilities in the topic.",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Signal Denoising",
      "text": "ECG signals are highly susceptible to interference during the acquisition stage  [366] . The amplitude of their waveforms may vary depending on the electrode characteristics and placement but, under ideal conditions (using chest leads in medical settings), the QRS complex only reaches 2 -3 mV, the largest amplitude of the whole cyclic beat  [124] .\n\nThis means that when the electrodes are placed far from the heart, the signal is weaker and the noise is more dominant. This can result in different interference types, such as powerline interference (PLI) from alternating current energy lines, baseline wander from breathing movements, electrode movement from motion, lead reversal due to electrode mispositioning, or pacemaker interference  [124; 403] . The stage of signal denoising is, thus, of utmost importance for an ECG biometric system.\n\nOn the first initiatives in ECG biometrics, using on-the-person acquisitions, the signal-tonoise ratio was higher, and noise sources were mainly limited to powerline interference and baseline wander. Hence, filters, such as bandpass (BPF), lowpass (LPF), highpass (HPF), or notch (NF), were the first and have been the most frequent option, due to their simplicity and lower computational cost. Recently, Choudhary and Manikandan  [78]  proposed the use of the Discrete Cosine Transform (DCT) for simultaneous removal of baseline wander and powerline interference, which proved more successful than bandpass filters, when compared on simulated scenarios. The Discrete Wavelet Transform (DWT) has also been proposed for denoising of on-the-person signals  [80; 124; 373] , as it allows to decompose the signal into several levels, which may be separately processed to eliminate noise in certain frequency ranges.\n\nWhen considering off-the-person approaches, wearables, or seamlessly integrated acquisition settings, it is reasonable to expect a considerable increase in the noise influence, with a lower signal-to-noise ratio. The ability to capture the ECG signal weakens, so the amplitude of the ECG components is smaller when compared with chest leads, and movement artefacts are much more frequent and dominant  [278; 289; 394] .\n\nFor these, filters have also been widely applied  [272; 289] , as well as DWT  [170] . However, the enhanced noise content motivated the proposal of new approaches based on line fitting algorithms, such as fitting of polynomial curves and the Savitzky-Golay algorithm  [374] . Their use or combination with moving average or median filters has been shown more successful than filters or transform denoising  [298; 342] , likely because noise is widely present across the ECG frequency range, and such methods avoid restricting their operation to narrow frequency ranges.\n\nConsidering all this, it is possible to conclude that the trend in signal denoising has been the evolution towards methods that can adapt to increasingly unexpected and dominant noise.\n\nConsidering the efforts devoted to more acceptable and comfortable acquisition settings, with an increasing focus on wearables and seamless settings, it is unreasonable to expect this trend would be reversed in the near future.\n\nWhile filters appear to be a wise option if the noise is confined to known frequency ranges outside the ECG frequency range, for on-the-person signals, transforms (especially DCT) have shown to be good alternatives for denoising without causing distortions  [78] . However, when the noise is widespread and/or its frequency range is unpredictable (such as with off-the-person signals), line fitting algorithms such as the Savitzky-Golay filter may be a better option, as they smooth the signal without making strong assumptions about its noise content.\n\nNevertheless, research must continue to work towards increasingly robust and adaptable denoising methods. Researchers have recently started to use deep learning methodologies (as discussed further on in subsection 3.2.5), that have shown remarkable robustness to noise and variability in several pattern recognition applications  [163; 484] . These, along with a deep study of data augmentation, may result in better alternatives to current and future methods devoted to signal denoising, and should certainly be explored in depth.",
      "page_start": 87,
      "page_end": 89
    },
    {
      "section_name": "Signal Preparation",
      "text": "ECG biometric algorithms frequently resort to the application of several processing operations over the acquired ECG signal, between denoising and feature extraction. These have the main goal to prepare the signal for the feature extraction phase, maximise the performance of the system (by reducing persistent noise and variability), segmenting specific useful parts of the acquired signal, and/or discarding undesirable or prejudicial parts  [191; 278; 342] .\n\nThe noise and variability that may remain after the signal denoising stage, which this stage will aim to attenuate, are generally segment length and alignment inconsistencies, amplitude variations, heart rate variability, movement artefacts, and contact loss or impedance artefacts  [189; 191; 342] .\n\nTo fulfil its objective, this stage generally consists of reference point detection, signal segmentation, amplitude normalisation, time normalisation, and/or outlier detection processes. The most common in the literature methods are fiducial point detection, signal segmentation, and amplitude normalisation. Below, an overview of these processes is presented.\n\n• Fiducial Point Detection: To aid posterior processes, such as signal segmentation, the preparation of the signal for recognition can include a step of detection of heartbeat reference points, designated as fiducials. The majority of the surveyed research works have used this technique, varying in the methods used. On the other hand, some researchers have opted to make their algorithms completely non-fiducial, discarding the processes included in this stage  [8; 170; 349] .\n\nThe Pan-Tompkins algorithm  [328] , based on moving-window integration and adaptive thresholding, has been the most frequent choice for fiducial detection  [272; 303; 327; 390; 446] . Alternatives include the Discrete Wavelet Transform (DWT) (used in  [124; 125; 326; 373] ), the Trahanias algorithm  [436]  (based on morphological operations and adaptive thresholding, used in  [298; 342] ), and the Engelse-Zeelenberg algorithm  [113; 277]  (based on differentiation, negative lobe detection, and adaptive thresholding, used in  [272; 276; 342] ). Pinto et al.  [342]  have applied these methods and found Pan-Tompkins and Engelse-Zeelenberg gave better results for on-the-person signals, while Trahanias performed better in off-the-person settings.\n\n• Signal Segmentation: Signal segmentation is used to limit the signal span for feature extraction, or to set a fixed size to ease template matching when the feature is the signal itself.\n\nIn some cases, the segmentation uses the fiducial point locations and is used to crop QRS complexes and/or other waveforms  [414; 429; 446] . It can also be used to crop the whole heartbeat (or a majority of it), thus being performed at fixed distances before and after detected R-peaks or QRS complexes  [66; 272; 493] . Some research works included signal segmentation using sliding windows, with or without overlap, regardless of the completeness of the heartbeat cycles inside it  [114; 292; 318] .\n\nThe alignment and averaging of various signal segments are closely related to the signal segmentation process. The alignment is generally performed using the R-peaks as a reference after these are located, or it is performed using cross-correlation  [32; 78; 88; 278] . It usually serves as a way to ensure the template and the collected signal are not affected by variability, which distorts the personal information the signal contains, and could threaten the recognition task.\n\n• Amplitude and Time Normalisation: As previously discussed, the electrocardiogram varies over time with several factors. These include differences in acquisition equipment or in the interaction of the subject that may cause differences in signal amplitude and DC offset  [189] ,\n\nor heart rate variability that causes significant changes in the duration of the heartbeats and their waveforms. To mitigate this, some methods include amplitude and time normalisation techniques.\n\nAmplitude normalisation techniques include the min-max technique  [189]  (used in  [122; 250; 369] ), which normalises the signal to the range [0, 1], the zscore method  [318] ,\n\nwhich subtracts the signal and divides it by its standard deviation, or the max-div method  [274; 429] , which divides the signal by the maximum amplitude value (generally, the R-peak amplitude).",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "Feature Extraction",
      "text": "The stage of feature extraction aims to translate the acquired signal into a representation that reduces the effects of remaining noise and intrasubject variability and emphasises differences between subjects. Several feature extraction methods have been proposed for ECG biometrics, which are generally grouped into three types -fiducial, non-fiducial, or hybrid approaches  [288; 292] .\n\n• Fiducial Approaches: Fiducial approaches are those that exclusively use as features measurements of fiducial landmarks of the ECG signal in the time domain. These measurements vary widely throughout the state-of-the-art, including time intervals, amplitude, widths, and angles based on the heartbeat waveforms P, Q, R, S, and T, their onset and offset points  [193; 366; 390; 446; 490] .\n\nNevertheless, these approaches present the significant drawback of requiring the previous localisation of several fiducial points in the ECG heartbeats (see subsection 3.2.2), which proves difficult to satisfy when using off-the-person or seamless signals. Hence, fiducial feature extraction approaches were considerably more frequent in early research works.\n\n• Non-Fiducial Approaches: Non-fiducial approaches are those that use the entirety of the signal (or segments of it), holistically, to extract features related to the waveform morphology  [274; 288; 292] . These approaches include the use of Fourier, Wavelet, or cosine transforms  [32; 288; 289; 318; 342; 368; 429; 472] , autocorrelation coefficients  [8; 10; 170; 349] , cardioid graphs  [188; 414] , generated tridimensional vectorcardiograms (TVCG)  [105] , multiresolution local binary patterns  [272] , and information-theoretical approaches based on Lloyd-Max quantisation  [87; 88]  and Kolmogorov complexity  [54] .\n\nSome methods do not perform feature extraction, alternatively using segmented heartbeats, average ensemble heartbeats, or segments between consecutive R peaks as features  [66; 237; 276; 298; 493] . While more applicable to noisier signals, non-fiducial have still to reach the near-perfect performance reported by earlier works using fiducial features.\n\n• Hybrid Approaches: Hybrid approaches are those that use features from both fiducial and non-fiducial origins. These are rare among the surveyed literature works and include the approach from Palaniappan and Krishnan  [327] , which combined common amplitude, interval and width fiducial features with a non-fiducial QRS complex form factor. Ergin et al.\n\n[114] proposed the fusion of QRS fiducials, with several time-domain, Wavelet transform, and Power Spectral Density (PSD) features. Also, Dar et al.  [96]  opted for the extraction of a total of 46 features from Haar transform and heart-rate-variable R-R intervals.\n\nThrough the analysis of the surveyed research works, it is possible to conclude that fiducial approaches generally contribute more towards a high-performance biometric system, as the use of specific measurements reduces useless information to a minimum, and allows for feature sets with fewer dimensions. However, as noise increases, the relevance of robustness overcomes that of accuracy, and the former can only be offered by non-fiducial methods. The ideal feature extraction method would be one that combines the conditions for high performance offered by fiducial approaches with the robustness to noise and variability offered by non-fiducial approaches, perhaps using deep learning networks and their characteristic robustness to noise and versatile feature extraction capabilities.\n\nExtracted features may, additionally, suffer dimensionality reduction to improve performance  [445] . Although frequently overlooked, dimensionality reduction has a very important goal in biometric systems, as the number of features extracted by biometric algorithms can easily become too high for a time-efficient and reliable recognition process  [133] . Thus, dimensionality reduction aims to select or transform the extracted features, to reduce its number to a more computationally viable number, while keeping the maximum discriminant power to ensure the system's recognition performance  [445] .\n\nDimensionality reduction methods in the surveyed literature range from common methods such as Linear Discriminant Analysis (LDA)  [8; 10; 47; 170; 292]  and its Fisher (FLDA)  [332]  and Heteroscedastic variants (HLDA)  [250] , Principal Component Analysis (PCA)  [170]  and Kernel PCA (KPCA)  [170] , or Greedy Best-First Search (GBFS)  [95; 96] , to rarer methods such as Discrete Cosine Transform (DCT)  [349] , Wilkes' lambda stepwise correlation  [193] , correlation matrices  [42; 43] , or bin selection based on symmetric Kullback-Leibler divergence  [288; 289] .\n\nThe work performed by Plataniotis et al.  [349] , Agrafioti and Hatzinakos  [8] , and Hejazi et al.\n\n[170] provides an adequate platform for the comparison of dimensionality reduction algorithms.\n\nAccording to their findings, LDA offers better performance than unsupervised techniques such as PCA and DCT coefficients, despite its supervised nature that requires knowledge of the subjects prior to the deployment of the biometric system  [292] . More recently, other supervised techniques such as the non-linear KPCA method  [170; 326]  rendered even better results. Hence, research should probably focus on more sophisticated dimensionality reduction methods and deep learning methodologies, which are tunable to provide optimised non-linear dimensionality reduction.",
      "page_start": 91,
      "page_end": 93
    },
    {
      "section_name": "Decision",
      "text": "Based on the representation of the ECG acquisition, obtained through processes of feature extraction and dimensionality reduction, the decision stage aims to accurately attribute one of the enrolled identities to the user, in the case of identification tasks, or to accept or reject an identity claim, for authentication tasks  [9; 46; 155] . In the case of identification, the decision stage usually consists of a classification process while, for authentication, the acceptance or rejection of the identity claim is generally based on a reference threshold T that is applied to the prediction score.\n\nThe decision stage can be based on:\n\n• Classifiers: A classifier can be trained on enrollment templates from a set of subjects, and then be used to discriminate them, to output an accurate decision when needed. Classifiers are more commonly used for identification tasks. • Metric-based Matching: Some methods are based on the comparison between the currently acquired trait and the previously acquired templates, stored in the system database. The comparison is performed based on similarity or dissimilarity metrics. In ECG-Based biometrics, most metric-based matching methods have been based on distance metrics, among which the most popular was the Euclidean distance  [80; 274; 292; 332; 349; 369; 393; 405] .\n\nHowever, since the Euclidean distance is regarded by some as unreliable in high dimensional spaces, some researchers have opted to use the cosine  [393]  or the Mahalanobis distances  [156; 233-235] . Among similarity metrics, literature methods include the correlation coefficient  [8; 69; 124; 125; 373; 389; 390] , normalised cross-correlation (NCC)  [78] , Gaussian log-likelihood  [288; 289; 318] , and Dynamic Time Warping (DTW)  [298; 303; 493] .\n\nIn the literature, it is hard to perform a thorough and fair comparison between the algorithms based on the results reported by the respective authors, as the data used to evaluate such algorithms is commonly not the same, or is used differently. However, it is important to compare algorithms to find the advantages and disadvantages of each and find opportunities for improvement. Hence, to help the comparison of state-of-the-art methods in terms of reported performance, the results of the surveyed publications that have used the six most common data collections (see Fig. SVM and kNN have shown superior performance among traditional classifiers, even in situations with increased noise and variability. Hence, it is safe to assume that these would be wise options for new ECG biometric algorithms. However, there is the need for an equally accurate alternative that would not require re-training with every subject enrollment or update (as SVM does) or the memory-heavy storage of all subjects' templates (as kNN does). Recent studies indicate that Deep Learning models could solve these issues, but researchers will need to dedicate efforts to overcome the challenge of scarce supervised data.",
      "page_start": 93,
      "page_end": 94
    },
    {
      "section_name": "Deep Learning",
      "text": "Deep learning methodologies are quickly revolutionising several fields in pattern recognition, galvanising the machine learning community with outstanding results and unforeseen robustness to input noise and variability in diverse tasks  [163; 164; 242; 440] . It achieved these milestones mainly due to the flexibility and robustness of convolutional layers for feature learning, the selective memory of recurrent layers connected to their previous instances, and the versatility of fully-connected layers  [242; 483] . Their adaptability to scarce data through techniques such as data augmentation, fine-tuning, transfer learning, and weakly supervised learning, just add to their power for pattern recognition applications.\n\nIn the topic of ECG biometrics, the study of deep learning is still a pioneering affair. It has, however, been gathering steam throughout the past four years. Despite a few works which continued focusing on traditional feature extraction and decision models  [37; 252; 358; 433; 450] , the majority of literature methods proposed since 2020 already include some deep learning architecture responsible for the processes of feature extraction, decision, or both.\n\nInitially, Zhang et al.  [483]  proposed a multiscale CNN that receives, in parallel, selected autocorrelation coefficients of approximation and detail Wavelet transform coefficient sets of twosecond ECG segments. Eduardo et al.  [111]  replaced the feature extraction stage using an Autoencoder to learn lower-dimensional representations of segmented heartbeats, which were ultimately fed to a kNN classifier.  However, beyond the approaches proposed within this doctoral work, no truly end-to-end methodologies are present in the literature. Two-dimensional representations are promising, especially considering the possibility of using pretrained deep models, but the transformations themselves are still separately optimised processes that may lose important information and limit achievable performance.\n\nAnother promising category of approaches is temporal networks, such as LSTMs, which have attained interesting results and are a natural match with ECG signals (time series). However, the aforementioned problem is still valid: if separate processes of denoising, preparation, and/or feature extraction are added to the pipeline, the model may be limited in the information received and thus in the performance it can achieve.",
      "page_start": 94,
      "page_end": 95
    },
    {
      "section_name": "Open Challenges And Opportunities",
      "text": "Much of the great potential of deep learning for ECG biometrics is still to be explored. Considering the information presented and discussed throughout this chapter, one can align the main challenges in ECG biometrics with the corresponding trends in ECG acquisition, thus painting a panorama of the history and near future of ECG biometrics (see Fig.  3 .",
      "page_start": 100,
      "page_end": 100
    },
    {
      "section_name": "3).",
      "text": "As illustrated in the aforementioned figure, the use of deep learning is a major research opportunity as we move into the future of ECG biometrics. Although deep learning typically brings significantly increased computational costs to biometric systems, these should be compensated by considerable boosts in performance and robustness due to the flexibility offered by deep models.  Partially linked to data scarcity, another large problem currently plaguing ECG biometrics is the prevalence of unrealistic and mismatching evaluation settings. The diversity of databases and data subsets is noticeable throughout this literature review and makes it impossible to adequately compare different methodologies. The high frequency of random train/test subset splits makes results unrealistic. Moreover, the rarity of long multi-session data acquisitions makes long-term performance a hidden problem waiting to be truly unveiled and solved.",
      "page_start": 101,
      "page_end": 102
    },
    {
      "section_name": "Electrocardiogram Biometrics",
      "text": "Hence, this thesis part focuses on five contributions to these open challenges and opportunities.\n\nSpecifically:\n\n• In Chapter 4, we propose the first true end-to-end methodology for ECG-based identification, complete with a study on the progressive integration of pipeline stages within the deep model and various tailored data augmentation strategies;\n\n• In Chapter 5, we adapt the previous model for identity verification and explore identification vs. triplet loss training for template similarity matching. The proposed methodology is also benchmarked against state-of-the-art approaches on a restructured evaluation setup for more realistic results;\n\n• In Chapter 6, we study the effect of long-term variability on the performance of state-of-theart approaches using a database of day-long Holter acquisitions, along with the application of template/model update strategies;\n\n• In Chapter 7, we study the relative importance of ECG waveforms, with a special focus on the QRS, throughout experimental setups with varying database size and noise/variability using interpretability tools;\n\n• In Chapter 8, we propose an end-to-end methodology for the recovery of the entire set of twelve standard leads requiring as input just one single-lead blindly-segmented ECG signal.",
      "page_start": 103,
      "page_end": 103
    },
    {
      "section_name": "Chapter 4",
      "text": "End-to-End Models and Augmentation Strategies for Identification",
      "page_start": 104,
      "page_end": 104
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted entirely by the author of this thesis, under the supervision of Jaime S. Cardoso and André Lourenço. The results of this work have been disseminated in the form of a chapter in a book and an abstract in national conference proceedings:\n\n• J. R. Pinto, J. S. Cardoso, and A.",
      "page_start": 113,
      "page_end": 113
    },
    {
      "section_name": "Context And Motivation",
      "text": "The state-of-the-art in ECG-based recognition mostly consists of pipeline algorithms, composed of separate stages of denoising, signal preparation, feature extraction, and decision, as discussed in Chapter 3. Even the most recent methods using deep learning techniques still rely on some of these separate processes.\n\nHowever, Convolutional Neural Networks (CNNs) possess the tools to integrate all phases of processing, from acquisition to decision, into a single model. This integration replaces separate, step-by-step tuning with a holistic optimisation process, synergically adapting the model to attain the best performance possible.\n\nFurthermore, the flexibility of convolutional and fully-connected layers makes deep networks able to autonomously learn the most fitted features for the task at hand. Meanwhile, these keep the ability to generalise and be robust against high variability and noise dominance over the signals  [242; 483] . Hence, it could be the key to improving the inferior performance results verified in off-the-person ECG biometrics. This work aimed to study the full extent of the capabilities of CNNs for biometric identification using non-intrusive ECG signal acquisitions. A CNN architecture is proposed for the complete integration of traditional pipeline stages in a single model, for higher accuracy and robustness in off-the-person settings. To obtain further improved performance, unidimensional data augmentation strategies are designed specifically for ECG-based biometrics.",
      "page_start": 103,
      "page_end": 104
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 104,
      "page_end": 104
    },
    {
      "section_name": "Model",
      "text": "The proposed convolutional neural network (see Fig. The feature maps output by the last convolutional layer are concatenated into a single unidimensional vector of features. This serves as input to the fully-connected layer, which weighs and combines the received features at each neuron. The fully-connected layer is composed of N neurons (where N is the number of enrolled subjects), with softmax activations. The neuron that outputs the highest value will correspond to the predicted identity.\n\nBased on a batch of train samples fed to the network, a measure of loss is computed by comparing the output of the network with the true labels of the batch. The weights/parameters that compose the neural network are adjusted to reduce that loss, using an optimiser function. In this work, the optimiser Adam  [219]  was used, with empirically adjusted initial learning rate in [0.01, 0.001], and Sparse Categorical Cross-entropy loss.\n\nTo avoid overfitting, the network used dropout. Dropout will avoid learning overly specific patterns in the training data  [231; 411] . They are placed between two layers and act upon the connections between them, setting the corresponding input to zero. In the proposed method, dropouts are used on the connections between the flattened vector of features and the fully-connected layer, effectively blocking the access of the classifier to a part of the features, and requiring it to become less specific to the training set, and more robust to unexpected variability and noise. Based on the recent work of  [440] , and taking into account the unique characteristics of the ECG signals, seven different types of data augmentation are proposed and explored for 1D convolutional neural networks (see Fig.  4 .2). These types are:",
      "page_start": 104,
      "page_end": 105
    },
    {
      "section_name": "Data Augmentation Strategies",
      "text": "• Baseline Wander: simulates a periodic undulation on the signal, by adding a sinusoidal wave with a frequency near 1 Hz;\n\n• Cropping: takes a contiguous subsegment and resamples it to match the original length. In the case of ECG signals, this technique simulates slower cardiac frequencies;\n\n• Flip: inverts the signal along the time axis, which causes the inversion of the heartbeat waveforms and their relative locations;\n\n• Gaussian Noise: introduces Gaussian noise (with mean zero and standard deviation about ten times lower than the signal amplitude) to cause high-frequency distortions on the signal, similar to movement artefacts and powerline interference noise;\n\n• Magnitude Scaling: rescales the original train sample by multiplying it by a factor inferior or superior (but close) to 1;\n\n• Magnitude Warping: similar to the previous technique, it rescales the signal in a nonuniform fashion, using a sinusoidal wave instead of a fixed factor, so that different parts of the signal will have their amplitude reduced or increased;\n\n• Random Permutations: divides the signal into N contiguous subsegments with similar lengths, and their order is randomly changed. This may cause discontinuities in the heartbeats and their waveforms, simulating sensor faults or abrupt segment terminations.",
      "page_start": 105,
      "page_end": 105
    },
    {
      "section_name": "Experimental Setup",
      "text": "The performance of the proposed convolutional neural network architecture, as previously described, was evaluated on off-the-person ECG recordings of the University of Toronto ECG Database (UofTDB)  [445] . Besides the entire database of 1019 subjects, two subsets were also used, with 25 and 100 subjects, to evaluate the performance in smaller datasets. The datasets were divided with 70% of the data for training and 30% for testing.\n\nThe proposed method suffered slight adaptations to allow the study of the progressive integration of the traditional pipeline stages into the CNN model (see Fig.  4 .3). Thus, besides the proposed end-to-end version that receives raw five-second ECG segments, three other variants were evaluated. The first receives five-second ECG segments denoised using a 1 -30 Hz bandpass filter. The second receives the average of heartbeats detected using the Engelse-Zeelenberg algorithm and normalised to zero mean and unit variance. The third variant receives DCT features extracted from the average heartbeats. The pool size of max-pooling, which was set at 5 for five-second segments as input, was changed to 3 for ensemble heartbeats, or 2 for DCT features.\n\nThe proposed method was compared with a baseline algorithm, adapted from the method proposed by Pinto et al.  [342]  using SVM and kNN for decision, and the state-of-the-art algorithm based on autoencoders proposed by Eduardo et al.  [111] , and the algorithm based on AC/LDA features by Matta et al.  [292] , evaluated in the same conditions.",
      "page_start": 107,
      "page_end": 108
    },
    {
      "section_name": "Results And Discussion",
      "text": "With DCT features as input (see Fig.  4 .4), the performance of the proposed method is similar to that of the baseline algorithm for 25 subjects. However, with the increase of subjects on the dataset (with 100 and 1019 subjects), the proposed algorithm falls behind. This may be caused by the very concise information that the input carries, fitted for typical pipeline algorithms as the baseline but not for deep learning. The results of the evaluation using ensemble heartbeats as input support  this hypothesis (see Fig.  4 .5), as the performance increases and approaches that of the baseline methods, and even surpasses that of the kNN classifier on the two smaller datasets.\n\nIntegrating additional stages into the deep learning model allows us to simplify its structure, and use longer signal segments as inputs (in this case, five seconds). This means an increase in complexity of the input, which can harm the performance of the network, but also an increase in available information and variability, which can allow for a more robust model.\n\nThe results of the use of denoised and raw five-second segments (see Fig. The most promising data augmentation techniques were random permutations (that excelled in both datasets), baseline wander, and flip. These were evaluated in groups to assess if the combina-  show that the sole use of random permutations is the best option, although the combinations also caused an improvement in identification performance.\n\nWe compared the proposed and baseline algorithms with state-of-the-art algorithms. As they were implemented and tested in the same conditions, the algorithms of Eduardo et al.  [111]  and Matta et al.  [292]  can be used for a direct benchmarking (see Fig.  4 .9). The proposed method presents better results than the alternatives and a slightly slower decay with the increase of the number of subjects, denoting better scalability to larger populations. The state-of-the-art algorithms likely suffer from using nearest neighbour classifiers, prone to overfit, as the results of the baseline algorithm with kNN were also consistently worse than with SVM. The method of Eduardo et al.  [111] , despite showing remarkably good results in the denoising of signals during our experiments (using the entire encoder-decoder), falls short in these conditions.\n\nFinally, the results of the proposed and baseline algorithm can be compared with the results reported by the most recent prior artworks (see Table  4 .1). The IDR of the proposed and baseline algorithms may pale in comparison with some results reported in some of the considered prior works, but it is important to consider the evaluation settings. Only Wieclaw et al.  [457]  used an off-the-person database, as opposed to the much cleaner signals of on-the-person databases still used by most researchers. Also, the UofTDB collection enabled the evaluation of the proposed algorithm with a much larger set of subjects than any other identification method.   However, it is important to recall that deep learning both requires and benefits greatly from large datasets where each class is represented by a large number of samples. While, as visible in the results presented here, data augmentation attenuates the prejudicial effects of scarce data, it is difficult to acquire sufficient ECG signals from each subject to compensate for the increased noise and variability in off-the-person settings.\n\nIn the datasets used, each subject was represented, on average, by just 170 five-second ECG segments, which is arguably too few to train a convolutional neural network to robustly discriminate between 1019 individuals. Considering this, with future efforts devoted to adequately dealing with scarce data, deep learning methodologies could see their potential for ECG biometrics be better harnessed and place themselves as clearly better alternatives to traditional pipeline algorithms.",
      "page_start": 109,
      "page_end": 111
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This work proposed a convolutional neural network for biometric identification based on nonintrusive electrocardiogram acquisitions. The proposed method was evaluated for incremental integration of traditional ECG biometric pipeline stages, including a complete substitution by the CNN architecture, that received raw five-second ECG segments and output a decision on the corresponding identity.\n\nBesides this study, seven data augmentation techniques for unidimensional signals were explored and their individual and collective impact on the algorithm's performance was assessed.\n\nThe results on the UofTDB database were compared with those of a baseline algorithm and two promising state-of-the-art methods.\n\nThe results show that the total integration of traditional pipeline processes in the CNN architecture was successful. The proposed CNN with data augmentation and receiving raw fivesecond segments surpassed, in all settings, the baseline and state-of-the-art algorithms in direct benchmarking. Among other recent state-of-the-art methods, considering the diverse dataset characteristics, the proposed method has also shown promise as an accurate and robust biometric identification algorithm.\n\nChapter 5",
      "page_start": 111,
      "page_end": 111
    },
    {
      "section_name": "Triplet Loss And Transfer Learning For Identity Verification Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted entirely by the author of this thesis, under the supervision of Jaime S. Cardoso. The results of this work have been disseminated in the form of an article in international conference proceedings: • J. R. Pinto and J. S.",
      "page_start": 113,
      "page_end": 113
    },
    {
      "section_name": "Context And Motivation",
      "text": "The field of ECG biometrics has been steadily evolving from on-the-person signals to off-theperson acquisition setups. Despite the enhanced usability and comfort, the increased dominance of noise and variability in off-the-person signals places serious hurdles to the real application of ECG biometric systems (more detailed information in Chapter 3).\n\nSome researchers have resorted to deep learning in order to fight off noise and variability and achieve better performance and robustness  [111; 284; 344; 483; 484] . However, these still rely on separate predefined feature transforms and/or noise removal techniques, which are not optimised for the task at hand and therefore limit the achievable performance. In fact, the work presented in Chapter 4 shows that end-to-end deep models offer considerable performance benefits in off-theperson ECG biometric identification, especially when using tailored augmentation techniques.   trained for identification. This aimed to assess whether parameters optimised for identification tasks would offer performance benefits in identity verification.",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Building Upon The Work In",
      "text": "The proposed network and both training methodologies were extensively evaluated on three ECG collections, which include on-the-person and off-the-person signals with varying signal quality, multi-session recordings from several subjects, and the influence of emotions, posture, and exercise. This evaluation included the assessment of the trained model's applicability to other signal collections, through cross-database tests using transfer learning and fine-tuning.",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Model Architecture",
      "text": "The proposed method for ECG biometric identity verification is based on a CNN (see Fig.  5 .1, darker grey). All enrolled users have one or more fixed-length ECG segments (templates) stored in the system, that have been blindly segmented (without requiring any process of reference point detection) from a recording obtained upon enrollment.\n\nWhen a user claims to be an enrolled individual, the model receives and processes, simultaneously, the K stored templates of the claimed identity and 1 current segment of the user. The comparison between the processed current segment and each of the K stored templates allows the model to output a dissimilarity score, which can be used to accept or reject the identity claim.\n\nAfter sample-wise normalisation to zero mean and unit variance, the processing of each input segment or template starts with a succession of convolutional and pooling layers. As visible in Fig.  5 .1, four unidimensional convolutional layers are alternated with three max-pooling layers.\n\nAll have 1 × 5 filters, and the convolution is performed with unit stride and no padding. The first two convolutional layers hold 24 feature maps, while the last two hold 36.\n\nThe second part of the network is composed of a fully-connected layer. The outputs of this fully-connected layer for each stored template (a) and for the current segment (b) are compared using normalised Euclidean distance  [461]  (see Eq. (5.1)), using their variance (Var) so the output lies in [0, 1]. Among the K distances computed, the minimum is output as the final dissimilarity score for identity verification.\n\n.\n\n(5.1)",
      "page_start": 114,
      "page_end": 115
    },
    {
      "section_name": "Model Training",
      "text": "The weights for the identity verification model layers are transferred either from a model trained for identification or from a model trained using triplet loss (see Fig. The training process requires specific structural changes to the model, which are illustrated in Fig.  5 .1 and described below. In all cases, during training, the optimiser used was Adam  [219]  with an initial learning rate of 0.001, β 1 = 0.9, β 2 = 0.999, and no decay. Dropout  [411]  and data augmentation (random permutations, as in  [344] ) were used to prevent overfitting. After training, the weights are transferred to the respective layers on the identity verification model.",
      "page_start": 116,
      "page_end": 116
    },
    {
      "section_name": "Transfer From Identification Network (It-Cnn)",
      "text": "In the case of identification training (IT-CNN), the model is structured to receive 1 input segment and contain one additional fully-connected layer (FC2), using softmax activation, that will output N scores. It is trained for identification with data from N identities (following the work of Pinto et al.  [344] ).\n\nAfter receiving a training segment, considering its true label and the network's output, the sparse categorical cross-entropy loss  [1; 76]  is computed and used during training to ultimately prepare the model to adequately discriminate the subjects.",
      "page_start": 117,
      "page_end": 117
    },
    {
      "section_name": "Triplet Loss Training (Tl-Cnn)",
      "text": "To be trained using triplet loss (TL-CNN), the identity verification model, which has K + 1 inputs and 1 output, is restructured to receive 3 inputs and offer 2 outputs. The three inputs are the reference template, a positive template (whose identity is the same as the reference), and a negative template (of a different identity). The network processes each input and computes the dissimilarities between the reference and the positive template (p) and between the reference and the negative template (n).\n\nUsing adequate triplets of signal segments, the goal is to minimize p and maximize n. Hence, the model is trained using triplet loss  [73] , which can be computed for each triplet of inputs through the function:\n\nwhere α controls the margin to be enforced between the scores of positive and negative pairs (in this work, α = 0.5). This margin eases the choice of an effective threshold for the purpose of identity verification.",
      "page_start": 116,
      "page_end": 116
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this work, one of the main concerns was ensuring the performance results were as realistic as possible. To achieve this, all databases were split between training subjects and testing subjects, to ensure the model can be trained and applied to data from two entirely disjoint sets of subjects.\n\nFurthermore, cross-database tests were performed to ensure the model can generalise to other population samples and acquisition settings. Subject enrollment was limited to realistic durations  (5, 10, 15 , or, at most, 30 seconds of the earliest data from each subject).",
      "page_start": 116,
      "page_end": 116
    },
    {
      "section_name": "Data And Reference Methods",
      "text": "The three selected databases were UofTDB  [445] , CYBHi  [394] , and PTB  [49; 146] . UofTDB Three literature methods were used as reference: the AC/LDA method, proposed by Agrafioti et al.  [10] ; the Autoencoder method, proposed by Eduardo et al.  [111] ; and the DCT method, proposed by Pinto et al.  [342; 344]  (adapted for identity verification, using cosine distance normalised to [0, 1] for matching).",
      "page_start": 117,
      "page_end": 117
    },
    {
      "section_name": "Evaluation Scenarios",
      "text": "The proposed and implemented methods were evaluated across four scenarios, as detailed below, using the Equal Error Rate (EER, see  [343]  for more details). Here, each signal segment used as input for the proposed model was five seconds long (1000 samples at 200 Hz sampling frequency).\n\nIn the single-database scenario, the proposed model was evaluated on UofTDB data, and compared with the aforementioned reference state-of-the-art methods. The last 100 subjects were reserved for training, while the data from the remaining 919 subjects were used for testing. The number of enrollment templates varied between 1, 2, 3, or 6 five-second segments.\n\nThe varying identity set size scenario aimed to study how the performance is affected by the number of subjects used to train the model. Instead of the original 100 subjects, training was performed using the 20, 50, or 150 last subjects of UofTDB, and the remaining 999, 969, or 869 subjects, respectively, were used for testing.\n\nThe cross-database scenario was designed to assess the proposed model's applicability to signals from other databases. The proposed model, previously trained on 100 subjects from UofTDB, was directly tested on data from CYBHi and PTB, without fine-tuning.\n\nAt last, in the fine-tuning scenario, the goal was to assess the performance benefits brought by fine-tuning. As in the cross-database scenario, the proposed model trained on UofTDB data (from 100 subjects) was fine-tuned to CYBHi/PTB data (from 20 subjects). This was compared to the model directly trained, from scratch, on data from CYBHi or PTB (from 20 subjects, following the single-database scenario). With 20 subjects reserved for training, the tests on this scenario were performed for 108 (CYBHi) or 270 (PTB) subjects.",
      "page_start": 117,
      "page_end": 118
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 118,
      "page_end": 118
    },
    {
      "section_name": "Single-Database Scenario",
      "text": "The results obtained in the single-database scenario are presented in Table  5 .1. In all cases, the IT-CNN model, which used weights trained for identification, attained better results than TL-CNN, which was trained using triplet loss. With 30 seconds of user enrollment, IT-CNN achieved 7.86% EER, while TL-CNN offered 9.94% EER in the same circumstances.\n\nWhen considering shorter enrollment recordings (5 s, 10 s, and 15 s), the performance of both proposed methods worsens, but always remained below 14% EER. It is noteworthy that IT-CNN presented a wider advantage over TL-CNN with more enrollment data, which may denote it takes better advantage of the availability of data.\n\nAmong the reference methods, AC/LDA presented the best results in most settings. When compared with these results, both proposed methods offered consistently lower EER. Considering the best reference method for each enrollment duration, IT-CNN attained an EER reduction of around 7 -8%, which can be regarded as a significant improvement over the state-of-the-art.   The statistical significance of the results was assessed, repeating the evaluation on one-hundred random subject data divisions between enrollment and testing (Table  5 .2). Overall, the results were better, as this test is arguably less realistic than the remaining tests performed in this study (a real were significantly different in all cases (the differences are statistically significant at the 1% level), not only from each of the implemented state-of-the-art methods but also between themselves.\n\nAdditionally, the outputs of the network for five-second training segments from different subjects were visualised (see Fig.  5 .2). These are, effectively, the feature vectors used for the identity verification decision. It is possible to observe that, despite the blind segmentation and the noise and variability carried by each five-second segment, the trained network was able to represent each input segment in a way that maximises similarity with other segments from the same subject.\n\nAlthough some variability is still present, it is reduced to a manageable level for the biometric identity verification task, and the differences between the subjects' output patterns are noticeable even through a simple visualisation of the plots.",
      "page_start": 117,
      "page_end": 118
    },
    {
      "section_name": "Varying Identity Set Size Scenario",
      "text": "In the varying identity set size scenario, multiple numbers of UofTDB subjects reserved for training were explored (Fig.",
      "page_start": 119,
      "page_end": 119
    },
    {
      "section_name": "Cross-Database Scenario",
      "text": "In the cross-database scenario, the proposed methodologies were directly applied to CYBHi and PTB data, after training on data from 100 UofTDB subjects (Fig.  5 .4).\n\nWith CYBHi, IT-CNN offered better performance than TL-CNN when using 30 s enrollment (16.30% against 17.56% EER). However, with reduced enrollment duration (5 s), TL-CNN performed better (24.66% against 26.89% EER). This reinforces the idea that TL-CNN is better in scarce data situations, while IT-CNN takes better advantage of the greater availability of data.\n\nWith PTB, IT-CNN was, in all cases, the most successful proposed method (13.83% EER with 5 s enrollment).\n\nAmong the state-of-the-art methods, AC/LDA behaved as in the single-database scenario (see Table  5 .1), offering the worst results when using 5 s enrollment, but sharply improving with more enrollment data, offering the best result with PTB (9.03% EER). DCT presented the best result with CYBHi (15.40% EER), while IT-CNN offered the second-best result (16.30% EER). Both proposed methods were, in general, worse than the state-of-the-art with the PTB database.",
      "page_start": 122,
      "page_end": 122
    },
    {
      "section_name": "Fine-Tuning Scenario",
      "text": "In the fine-tuning scenario, the model was trained with CYBHi/PTB data and compared with the state-of-the-art (Fig.  5 .5) and when trained with UofTDB data and fine-tuned to CYBHi/PTB (Fig.  5 .6).\n\nDirectly trained on CYBHi data, TL-CNN attained 20.04% EER, but it offered 17.56% EER if trained with UofTDB data, and further improving to 15.37% EER if fine-tuning is performed.\n\nTL-CNN was able to attain better performance than IT-CNN in more difficult settings, once again indicating that this method may be better fitted for scarcer data or noisier signals.\n\nOn PTB, TL-CNN did not offer competitive results. For IT-CNN, fine-tuning (9.06% EER with 30 s enrollment) improved the results over the direct application, but it was not enough to significantly improve the results of direct training. Apparently, training with UofTDB data overprepared the network for a degree of noise and variability that is not verified on PTB signals, which ultimately harmed its performance. A hybrid method where, before regular training, the neural network would be encouraged to mimic the behaviour of traditional methods, could be beneficial in cross-database settings.\n\nOverall, the proposed methodologies presented more competitive results on CYBHi than on PTB, likely due to PTB signals' lesser noise and variability. Thus, while the proposed model has shown robustness to noise and variability in off-the-person settings, the state-of-the-art methods are more fitted to cleaner on-the-person signals.",
      "page_start": 123,
      "page_end": 123
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "In this work, an end-to-end model, based on a CNN, was proposed for biometric identity verifica-",
      "page_start": 122,
      "page_end": 122
    },
    {
      "section_name": "Long-Term Performance And Template Update Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted in collaboration with Gabriel C. Lopes, under the supervision of Jaime S. Cardoso. The author of this thesis contributed to this work on the formulation and implementation of the biometric recognition models, the conceptualisation of the template update methodologies, the preparation and conduction of experiments, the discussion of the results, and the writing of the scientific publications. The results of this work have been disseminated as an article in international conference proceedings and an abstract in national conference proceedings: • G. Lopes, J. R. Pinto, and J. S.",
      "page_start": 125,
      "page_end": 125
    },
    {
      "section_name": "Context And Motivation",
      "text": "Modern ECG biometric techniques generally report relatively high identification rates and low verification error, while current off-the-person ECG acquisition techniques contribute towards increased simplicity, usability, and comfort  [342; 343] . Nevertheless, as with most alternatives, the performance decays over time, especially when considering long-term usage  [224] .\n\nThe natural variability of the input biometric data, the effects of ageing, and variations caused by the subject's interaction with the sensor contribute to intrasubject variability  [201] . This causes stored individual templates to quickly lose representativity, resulting in poor recognition performance and placing serious challenges on long-term recognition. Thus, long-term biometrics benefits from the frequent update of stored templates to keep up with the variability and ageing of the users, thus maintaining acceptable performance over time  [134] .",
      "page_start": 125,
      "page_end": 125
    },
    {
      "section_name": "Long-Term Performance And Template Update",
      "text": "Specifically for ECG biometrics, long-term performance and template update remain open challenges. In the most thorough work yet on this topic, Labati et al.  [236]  have studied the performance decay over time on their proposed algorithm for ECG-based authentication, finding that performance decays significantly even over relatively short periods of time. However, the focus of this study was limited to authentication and the algorithm proposed by the authors.\n\nIn this work, we build upon the study of Labati et al.  [236] . We aim to more thoroughly explore the problem of long-term performance decay in ECG biometrics and how to correctly address it.\n\nSpecifically, we extend it to (a) focus on the task of ECG biometric identification, (b) study diverse state-of-the-art biometric methods, and (c) evaluate how different update techniques may be able to improve long-term performance.",
      "page_start": 126,
      "page_end": 126
    },
    {
      "section_name": "Related Work",
      "text": "In the literature, it is difficult to find a strong and widely accepted rule for template update. Most methods are based on heuristics and empirically determined thresholds, which are highly dependent on the data and the application scenario. For example, Komeili et al.  [224] , for authentication, have set the acceptance threshold equal to the point of zero false acceptance rate, thus ensuring updates with only genuine samples.\n\nNevertheless, it is possible to identify some common mechanisms that may vary depending on different factors: these include the choice of the update criterion (based on thresholds or graphs), the update periodicity (online or offline), the selection mechanism, and the template update working mode system (supervised or semi-supervised). The taxonomy of template update (see Fig.  6 .1)\n\ndivides the existing techniques into two categories: supervised and semi-supervised.\n\nSupervised methods are offline methods in which label attribution is given by a supervisor.\n\nThese contain the Clustering subcategory, which includes the MDIST that aims to search for the templates that minimise the distance among all the samples in the database (i.e., the most similar)\n\nand DEND that aims to search for the templates that exhibit large intraclass variations resort to the dendrogram (i.e., the most different)  [282] .\n\nThe second subcategory comprises Editing-based methods, which are independent of the number of templates and give focus on the whole collected training set T . A subset E ∈ T is generated, maintaining the classification performance offered by T . The best subsets were obtained by reviewing the structure of the data (which needs to be done for each subject)  [134; 184] . All the algorithms (based on k-Nearest Neighbours) must be representative of T and can be roughly described as incremental when the E starts empty and grows, or decremental when E starts equal to T and in each iteration some instances are deleted until some criterion is reached  [134] .\n\nSemi-supervised methods merge labelled (in biometrics, these correspond to the initial training samples) and unlabelled (corresponding to the samples available during system operation) data to improve the system's performance. This category comprises the Single Modality (for unimodal biometric systems) and Multiple Modality (for systems using more than one biometric trait) subcategories.   [364] ).\n\n(first-in-first-out), Fixation, Super Template (X composed by N templates x) where new genuine date is always fused to a common single template  [237]  updated online during the execution of continuous verification, Penalised template update method based on the mean of the past ECG's and the actual ECG  [80]  and clock method where the current template is tested against all the others stored in the database  [378] .\n\nGenerally, a new unknown trait measurement is used for template update if its score (returned by the biometric recognition system) is above a set threshold. Hence, the future performance of the system relies heavily on the chosen threshold value  [364] .\n\nThe update threshold is commonly estimated using enrollment templates or training data.\n\nWhen training data are scarce or when using short enrollments, this may lead to some problems.\n\nFirst, important intrasubject variability information may be missed since only the patterns similar to the stored templates are used (and all others are discarded). Second, the effectiveness of the online methods depends on the order of the input data. Third, the methods are vulnerable to large intraclass variations. At last, since the algorithms normally look for the minimal cost (high scores), they may get stuck in local maxima and always only use high-confidence data for updating.\n\nSemi-supervised methods also include Graph approaches. These commonly define a graph where the nodes are either labelled (the identity is known) or unlabelled (unknown identity) data, and the edges (which can have different weights) are the similarity between those samples  [364; 494] . To be considered a graph-based semi-supervised method, it must estimate a function f , approximate the known Y on the labelled nodes, and include two terms to smooth the graph: a loss function and a regulariser. These two terms are what define each approach (as can be seen in Table  6 .1)  [494] , among which the most common in biometrics is min-cut graphs  [364] .\n\nTable  6 .1: Graph-based template update methods and their respective loss and regulariser functions (based on  [494] ).",
      "page_start": 127,
      "page_end": 127
    },
    {
      "section_name": "Method Source Loss Regulariser",
      "text": "Min Cut  [44]  ∑ i∈L\n\nGaussian Random Fields and Harmonic Function  [495]  ∑ i∈L\n\nTikhonov Regularisation  [35]  1\n\nConsidering the topic of template update is still to be adequately addressed in ECG biometrics, this work studies the effect of ECG permanence and variability in long-term identification performance. Furthermore, it aimed to evaluate the effect of template update techniques, on the performance of several state-of-the-art methods.",
      "page_start": 128,
      "page_end": 128
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 128,
      "page_end": 128
    },
    {
      "section_name": "Biometric Identification Methods",
      "text": "To fully and objectively evaluate the effects of ECG variability on the performance of biometric algorithms, a study was conducted on four literature methods:\n\n• Plataniotis et al.  [349]  proposed an ECG biometric recognition method using a non-fiducial approach. Signals are preprocessed using a bandpass filter (0.5 -40 Hz), followed by feature extraction with autocorrelation (AC) and dimensionality reduction using discrete cosine transform (DCT). The fifteen most relevant features were selected, and Euclidean distance was used for classification;\n\n• Tawfik et al.  [429]  used a bandpass filter (1 -40 Hz) in the preprocessing phase. QRS complexes (the most stable part of ECG) were cut from the signal using a 0.35 second window. The average ensemble QRS was computed and features were extracted using the DCT technique (the thirty most relevant features were selected). A multilayer perceptron (MLP) is used for classification;\n\n• Belgacem et al.  [32]  also preprocessed signals with a bandpass filter (1 -40 Hz). The QRS complexes were located and cut from the signal, and the average QRS was computed.\n\nThe feature extraction resorted to Discrete Wavelet Transform (DWT). From all DWT decomposition levels, only the most relevant were selected, and a Random Forest is used for classification. This method was originally proposed for authentication and adapted here for identification;\n\n• Eduardo et al.  [111]  used a Finite Impulse Response (5 -20 Hz) filter for preprocessing.\n\nHeartbeats were cut with a fixed length of  [-200, 400]  ms around each R peak. Outliers were detected and removed using DMEAN (α = 0.5 and β = 1.5, with Euclidean distance).\n\nFor decision, the k-nearest neighbours (kNN) classifier was used with k = 3 and cosine distance.\n\nBeyond these literature methods, this work also explored the deep learning-based methodology presented in Chapter 4 (proposed in Pinto et al.  [344] ). This method uses an end-to-end unidimensional convolutional neural network, that receives five-second blindly-segmented z-score normalised ECG segments to perform biometric identification. The feature extraction part of the model is composed of four convolutional layers, interleaved with three max-pooling layers, with filter/pooling size 1 × 5 and ReLU activation units. For classification, the network uses a single fully-connected layer and softmax activation units.",
      "page_start": 128,
      "page_end": 129
    },
    {
      "section_name": "Template Update Methods",
      "text": "FIFO: First-In-First-Out is the most common strategy and, computationally, is very lightweight.\n\nHere, the database is updated using new samples whose score is above or below a threshold (whether the score represents similarity or dissimilarity, respectively), or between two threshold values (discarding previously stored sample)  [87; 224] . The score of a new sample can either be output by a classifier or be a measure of distance or similarity between that sample and the stored templates  [274] .\n\nIn this work, the training data were used to search for threshold values. Among all training samples, 75% were used to train a model, which was used to obtain scores for the remaining data samples. Comparing the scores with several thresholds, the error at each threshold was analysed (Fig.  6 .2) to find one that simultaneously maximises true positives and minimises false positives.\n\nFixation: This method consists of fixing certain templates, allowing only the remaining stored samples to be updated  [157] . In this work, 25, 50, or 75% of the enrollment templates of the individual are fixed, while the rest of the samples are free to be updated. This ensures some initial, labelled information of the subjects remains on the system over time.\n\nAn adaptation of this technique was explored. Here, n + j × n samples were fixed, where n ∈ [1, 2, 3] is the number of fixed initial templates, and j increases over time. In this work, j ∈ [0, 6] increased by one at each testing moment ( j ∈ [0, 6]), which allowed the system to fix more and more samples over time, thus storing information on the subject's variability over time. In a real system with potentially endless use, the parameters n and j should be carefully chosen to avoid the eventual fixation of the entire template gallery.\n\nFine-Tuning: In this technique, the model is briefly optimised with the samples accepted for update, using the predicted labels. The model retains knowledge of the users' supervised training samples, as it was trained using their enrollment samples, but is slightly adapted to the new personal patterns carried by the new signals. This is explored exclusively for the CNN method  [344] .",
      "page_start": 130,
      "page_end": 130
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 130,
      "page_end": 130
    },
    {
      "section_name": "Dataset",
      "text": "For evaluation, the ECG signals used were from the E-HOL-03-0202-003 database 1 (most commonly designated as E-HOL 24h). This database consists of a study of 202 healthy subjects (only 201 were provided), recorded using three leads at 200 Hz sampling frequency, after an initial resting supine period of 20 minutes. From the available data of 201 subjects, thirteen were discarded due to saturation or unacceptable noise (subjects 1043, 9003, 9005, 9020, 9021, 9022, 9025, 9046, 9061, 9064, 9071, 9082 and 9105), similar to what was done by Labati et al.  [237] . From each of the remaining 188 subjects, only the lead most closely resembling Lead I ECG was selected, to approximate off-the-person settings.",
      "page_start": 130,
      "page_end": 130
    },
    {
      "section_name": "Experiments",
      "text": "Standard sample wise normalisation was performed following Eq. (6.1) for all methods except that of Eduardo et al.  [111] , which required [-1, 1] min-max normalisation, Eq. (6.2), where x 1 THEW. Available on: http://thew-project.org/Database/E-HOL-03-0202-003.html. represents the input signal and x the normalised signal.\n\nIn order to fit the used data, some changes were introduced to the original methods. On the method from Eduardo et al., the cutoff frequencies of the bandpass filter were changed to 1 and 40\n\nHz, to retrieve important information on higher frequencies. Outlier removal was reparametrized with α = 1.  performance is mostly acceptable in the first test point, but performance decays significantly over time and variability changes considerably over the day.\n\nMoreover, a minimum around the 15 th hour occurs independently of the chosen method. Considering that most of the records start between 8-12 am, after 15 hours the subjects must be sleeping. In this perspective, it appears that the ECG is most different from normal when the subject is asleep.\n\nConsidering the previous results, template update was applied to the methods, in an effort to avoid performance decay over time. Fig.  6 .5 presents the results using the FIFO technique, with diverse thresholds.\n\nFor the methods of As for the Fixation technique, the obtained results were more promising (see Fig.  6 .7). This template update technique brought performance improvements for all methods. The fixation technique that offered the best results was j × 3 + 3, improving the baseline identification accuracy, on average, by 10.0%.",
      "page_start": 130,
      "page_end": 131
    },
    {
      "section_name": "Deep Convolutional Network",
      "text": "The results for the implemented end-to-end convolutional neural network are presented in Fig.  6  Likely, the network will require more data with more variability during the first training phase.\n\nIncreasing the training data to thirty minutes or even a few hours per subject would enable the  network to better learn the common variability patterns of the ECG. This should not only increase the initial performance, immediately after enrolment but also reduce the performance decay over time.",
      "page_start": 134,
      "page_end": 134
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This work studied how ECG variability affects the performance of state-of-the-art biometric algorithms, and how template update could mitigate performance decay over time. The results have shown long-term identification performance in ECG biometrics is generally weak, despite the promising results often presented in the literature.\n\nTemplate update techniques proved successful in enhancing the long-term performance of handcrafted state-of-the-art methods, especially when using template fixation techniques. Additionally, with a deep learning algorithm, results are better than traditional methods immediately after enrollment, although it offers slightly worse performance as time progresses.\n\nGenerally, one can conclude that further efforts are needed for the study and development of more advanced techniques. The obtained results in these more realistic settings show that the performance levels commonly reported in the literature would likely not be verified upon real application. Special focus should be devoted to supervised update techniques, so that ECG-based biometric systems can offer reliable performances over long periods.",
      "page_start": 135,
      "page_end": 136
    },
    {
      "section_name": "Chapter 7",
      "text": "Leveraging Explainability to Understand ECG Biometrics",
      "page_start": 138,
      "page_end": 138
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted entirely by the author of this thesis, under the supervision of Jaime S. Cardoso. The results of this work have been disseminated in the form of an article in international conference proceedings and an abstract in national conference proceedings:\n\n• J. R.",
      "page_start": 147,
      "page_end": 147
    },
    {
      "section_name": "Context And Motivation",
      "text": "Throughout the past twenty years, research on biometrics based on the electrocardiogram (ECG) has largely been a success story  [343] .",
      "page_start": 137,
      "page_end": 137
    },
    {
      "section_name": "The Electrocardiogram As A Biometric Trait",
      "text": "As presented in Chapter 2, subsection 2.2.2, the ECG is approximately a cyclical repetition of a set of waveforms (P, Q, R, S, and T) that corresponds to a heartbeat (see Fig. in medical or on-the-person settings (where the subject is at rest, laying down, and signals are acquired using several high-quality gel electrodes), their effects are dominant for realistic off-theperson signals (acquired using fewer dry electrodes on the hands, during common daily activities)  [338; 342; 343] .\n\nWhen compared with the P and T waves, the QRS corresponds to a larger polarisation event over a shorter period. In practice, this makes the QRS more dominant over noise and intrasubject variability than the other ECG waveforms  [342; 343] . Hence, the QRS is considered more stable over time and across variable conditions, which makes it better suited for biometric recognition.\n\nDespite this, it is still unclear how much identity information is carried by the QRS complex compared to the other waveforms, and whether it is enough for an accurate and robust biometric recognition system. Studies on ECG-based biometric identification have shown it is possible to distinguish small sets of individuals in on-the-person settings using only the QRS complex or QRS fiducial amplitude and time measurements  [238; 446] . Nevertheless, this practice is becoming uncommon as research evolves towards realistic off-the-person signals and larger databases.\n\nThis denotes that the sole use of the QRS may not be adequate for off-the-person settings, or the individual information carried by the QRS may not be enough to distinguish individuals in large populations. This work aimed to address these doubts through a study on the role and relevance of the QRS and the other waveforms in ECG-based biometric identification. Interpretability tools are used to assess which parts of the ECG are more relevant to the decisions of an end-to-end identification model  [344] , with on-the-person and off-the-person signals and data subsets with a varying number of identities.",
      "page_start": 138,
      "page_end": 139
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 139,
      "page_end": 139
    },
    {
      "section_name": "Biometric Identification Model",
      "text": "The biometric model for identification followed the architecture proposed by Pinto et al.  [344] ,\n\nwhich has attained state-of-the-art results in off-the-person settings for both identification and, later, identity verification  [338] . The model (see Fig.  7 .2) receives five-second blindly segmented ECG signals and outputs probabilities for each of the N identities considered. Finding the highest probability score allows us to assign the respective identity to the input signal.\n\nThe model consists of an end-to-end 1D convolutional neural network (CNN) with four convolutional layers (with 1 × 5 filters, two layers with 24 followed by two with 36), followed by ReLU activation. Neighbouring convolutional layers are separated by 1 × 5 max-pooling layers.\n\nThe last convolutional layer is followed by two fully-connected layers (100 neurons with ReLU and N neurons with softmax activation).",
      "page_start": 140,
      "page_end": 140
    },
    {
      "section_name": "Interpretability Tools",
      "text": "To capture the dynamics behind the decisions of the biometric model, four interpretability methods are applied to the trained model: Occlusion, Saliency, Gradient SHAP, and DeepLIFT. Oc-ID Conv1D  Occlusion The Occlusion method  [478]  consists in measuring the influence of hiding a portion of the input on the output of the model. When hidden, the more relevant input parts will cause larger changes in the output, and will thus be assigned greater relevance in the explanations offered by this method. This is the simplest method to interpret a model, although the size of occluded regions should be carefully defined to obtain meaningful explanations.\n\nSaliency The Saliency method  [401]  is based on the gradients of a model given a certain input. Through backpropagation, the gradient of target class scores w.r.t. the input is obtained.\n\nA saliency map is then generated by rearranging the class score derivatives, generating saliency maps that assign higher relevance to input regions that correspond to higher gradients. Requiring a single backpropagation pass, this method is a simple and fast way to obtain explanations of model predictions.\n\nGradient SHAP Gradient SHAP  [283]  is an approach based on game theory which considers the explanations of a model's predictions as models themselves. For sophisticated deep learning models, the explanation models are simplified and interpretable approximations of the respective models. SHapley Additive exPlanation (SHAP) values, inspired by game theory's Shapley values, are computed through the gradient of a random point between a baseline and the input with added random noise. The SHAP values denote how much a given part of the input raises the probability for the considered class, and are reportedly better aligned with human intuition and effective in discriminating among output classes.\n\nDeepLIFT DeepLIFT (Deep Learning Important FeaTures)  [391]  performs backpropagation to track the contributions to the output to the responsible parts of the input. Throughout this process, it compares the difference in inputs and outputs considering a reference (or baseline) input, assigning contribution scores to each neuron of the model. It also allows for the study of negative contributions: how much a specific part of the input contributes to lower the probability for the considered class.",
      "page_start": 139,
      "page_end": 140
    },
    {
      "section_name": "Visualisation",
      "text": "Decision explanations obtained using interpretability tools are visualised using the multicoloured line plot feature of Matplotlib  [183] . ECG signals are plotted so that the colour of each signal component represents its relative relevance to the decision. In this case, lighter yellow colours represent less relevant time samples, whereas more relevant samples assume darker purple colours. This way, both the ECG morphology and the relevance of each of its components are easily and intuitively presented.",
      "page_start": 141,
      "page_end": 141
    },
    {
      "section_name": "Experimental Setup",
      "text": "The data used for model training and evaluation have been drawn from the Physikalisch-Technische Bundesanstalt ECG Database (PTB)  [49; 146]  and the University of Toronto ECG Database (UofTDB)  [445] . The PTB database includes on-the-person (high-quality)  12  Five-second segments were blindly extracted (without fiducial detection) from the recordings.\n\nFifty per cent of those segments (per identity) were used during training and the remaining were reserved for testing. This provided more challenging test settings than those commonly found in the literature, but also deliberately avoided the most realistic settings (see  [338] ), for the sake of obtaining meaningful interpretations.\n\nTo simulate gradually increasing identification difficulty within each database, subsets of N identities are considered, with N ∈ {2, 5, 10, 20, 50, 100, 200, 500, 1019}. The identities in each subset are the first N in lexicographical order. Each subset includes all identities that compose smaller subsets, so subjects #1 and #2 are the main focus of analysis since these are present in all subsets. Throughout this paper, T N denotes the subset of UofTDB data from N subjects and P N denotes the subset of PTB data from N identities. As stated in Table  7 .1, P 290 was used instead of P 200 to take advantage of the entire PTB dataset. Model training details can be found online at this project's repository.\n\nPerformance evaluation is based on the True Positive Identification Rate (or accuracy): the fraction of test samples that are correctly assigned to their true identity by the trained model.\n\nInterpretations are examined through the proposed visualisation method.",
      "page_start": 141,
      "page_end": 141
    },
    {
      "section_name": "Results And Discussion",
      "text": "The results of the performance evaluation are presented in Table  7 .1. These results roughly follow the expected patterns considering the use of on-the-person versus off-the-person ECG data. The model is able to attain high true positive identification rates in both databases when the population  1 For PTB, this column corresponds to the entire set of 290 subjects.\n\nis small, but as the set of subjects grows, performance decreases and a wide gap distinguishes the more challenging off-the-person settings from the more controlled on-the-person settings.\n\nAdditionally, one can find some unusual patterns in the performance results. Considering M > N, one would expect identification performance with subset T N to be higher than with subset T M . With UofTDB off-the-person data this is not always verified: e.g., from T 5 to T 10 , performance increases from 97.26% to 98.10%. In these cases, we need to consider that datasets with fewer identities have fewer data and, thus, more unstable results. Alternatively, the identities added to T N to create T M may be easier to discriminate (\"sheep\", according to the concept of biometric menagerie  [104; 468] ) and thus contribute to improving accuracy. However, one should also regard the substantial regularisation needed to avoid overfitting and the instability during training as possible causes for these discrepancies. This is a very important insight into the increased difficulties of using off-the-person data and the need for improved and more robust biometric models.\n\nAnalysing the explanations obtained using the four interpretability tools (examples in Fig.  7 .3 and Fig.  7 .4), a trend is verified from smaller to larger identity subsets, consisting on the deviation from focusing mainly on the QRS complex to the increasing relevance of other parts of the heartbeats. This is also confirmed when combining the explanations of all heartbeats of each person into a single average heartbeat (see Fig.  7 .5 and Fig.  7.6 ).\n\nWith the cleaner medical signals from PTB, the focus is mostly on the QRS complex, but information from other waveforms starts to become more and more relevant as more identities are added. It is noteworthy how, when discriminating PTB subjects #1 and #2 in a two-subject scenario (see Fig.  7 .5), the model still focuses mainly on the QRS, even though subject #2 has a very specific characteristic, the inverted T-wave, that is arguably their most distinctive feature. This denotes how, in these cleaner signals, the QRS complex is so stable that the remaining waveforms, more susceptible to heart rate variability, are largely ignored by the model regardless of any visually obvious intersubject differences they may present.\n\nWith the more realistic off-the-person signals from UofTDB, the QRS retains high importance but the relevance is more evenly spread among the signal waveforms. In the specific case of subject #2 (see Fig.  7 .6), it is evident that the QRS retains the highest importance for the decision, even in T 1019 (the largest subset). This may denote that, even in these more challenging settings, the identification models will still give preference to the QRS over other waveforms if it is sufficiently unique among the considered identities. Nevertheless, in such large sets of identities, the expected behaviour is that of subject #1 (see Fig.      QRS will lead the model to also look to other parts of the signal.\n\nOne interesting aspect is the difference between the results with Occlusion versus the other methods. Occlusion generally grants the QRS complex much more relevance, regardless of the settings. In the state-of-the-art approaches, the QRS complex is not only a source for identity features but also frequently used as an easily detectable reference landmark for the location of other ECG waveforms. This may also be the case in this end-to-end deep model. Although there are challenging contexts where the QRS may not be the main contributor to the decision, it may be essential to the deep model as a reference landmark to locate other waveforms in the signal.\n\nHence, when occluded, it will be the signal component that most impacts the decision, causing the occlusion method to generally consider it the most relevant.",
      "page_start": 141,
      "page_end": 145
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This work aimed to explain how deep models use ECG signals to distinguish people, using interpretability tools. Overall, the obtained results partially confirm the claim that the QRS is the key to ECG-based biometrics. With small populations in on-the-person settings, it can alone be used for reliable recognition. However, as we evolve towards larger populations and off-the-person settings, other components become relevant in discriminating people, as the models require more identity information to overcome the hurdles placed by enhanced intrasubject variability.\n\nHowever, even though relevance is more evenly shared in off-the-person identification in large sets of identities, the QRS is shown as essential by the occlusion method. It appears that, just like several literature methods, the implemented end-to-end model learnt to use the QRS as a landmark for the location of other ECG components in the signal, resulting in large output changes when the QRS is occluded. Hence, despite the literature claims, one should avoid relying too heavily on any single part of the ECG, including the QRS complex, since all waveforms carry identity information that proves increasingly useful in more realistic settings and larger populations. Chapter 8",
      "page_start": 145,
      "page_end": 145
    },
    {
      "section_name": "Interlead Conversion Of Electrocardiographic Signals",
      "text": "Foreword on Author Contributions\n\nThe research work described in this chapter was conducted in collaboration with Sofia C. Beco, under the supervision of Jaime S. Cardoso. The author of this thesis contributed to this work on the formulation, implementation, and improvement of the interlead conversion methodology, the preparation and conduction of the extended experiments, the discussion of the results, and the writing of the scientific publications. The results of this work have been disseminated in the form of an extended journal article and a short paper presented at an international conference: • S. Beco, J. R. Pinto, and J. S.",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Context And Motivation",
      "text": "The electrocardiogram (ECG) is the measurement of electrical potentials that make the heart contract and relax as intended. The morphology of the ECG signal depends on the location of the electrodes used for acquisition: different electrode placement results in different perspectives over the heart  [343] . For medical purposes, the standard configuration acquires the ECG over twelve leads for more information, but it requires ten electrodes placed on the patient's arms, legs, and chest. Using fewer electrodes allows for more comfortable and inexpensive acquisitions, at the expense of certain leads that could be ideal for a more accurate diagnosis of certain conditions.\n\nTo get the best of both worlds, researchers have proposed methods for the automatic interlead conversion of ECG signals  [244; 293; 395; 407; 408] . These transform short ECG segments to mimic other perspectives, using acquired leads to reconstruct any leads that were not recorded.\n\nHowever, these methods still present limited applicability, since they typically require multiple leads as input. Even the most advanced methods  [244; 293] , that only use one input lead, still require the inputs to be single heartbeat segments aligned in time, which makes them dependent on separate processes and, overall, less flexible and robust.\n\nThis chapter presents a study on the feasibility of ECG interlead conversion using short segments from just one limb lead without any kind of temporal alignment (blindly-segmented). With such input, the proposed methodology is trained to reconstruct other leads as faithfully as possible.\n\nThis aims to open up new possibilities for more comfortable ECG acquisition in clinical scenarios or wearable devices without giving up the benefits of multi-lead recordings for medical diagnosis.",
      "page_start": 147,
      "page_end": 148
    },
    {
      "section_name": "Related Work",
      "text": "At the onset of research on interlead conversion, methodologies commonly required several leads as reference for robust lead reconstruction. Zhu et al. [407] was one of the first to use machine learning techniques for interlead conversion. They used a focused time-delay neural network (FTDNN), which is well suited for time series prediction.\n\nHowever, their methodology required seven input leads (all limb leads and V1).\n\nAtoui et al.  [22]  used ensembles of fully-connected neural networks to learn to synthesise V1, V3, V4, V5, and V6 heartbeats from three-lead inputs (I, II, and V2). Schreck and Fishberg  [380]  performed the first study on the synthesis of the entire set of 12 standard leads and scalar 3-lead derived vectorcardiogram from just three measured leads. Their proposed methodology used nonlinear optimisation to construct a universal patient transformation matrix. Hansen et al. 1 Interlead ECG Conversion Github Repository. Available on: https://github.com/jtrpinto/ecg-conversion.\n\n[165] applied linear generic and subject-specific transforms to convert recordings from adhesive patch-type ECG monitors to the standard 12-lead ECG signals. In  [435; 438] , researchers also explored personalised statistically determined linear transforms and went on to achieve improved results.\n\nLee et al.  [243]  proposed methods based on linear regression and artificial neural networks to reconstruct the 12 standard leads from subsets of 35 channels acquired using one single large patch covering the subject's chest. Although accurate, the method is arguably incompatible with scenarios focused on ease of use and patient/user comfort. Similarly, Grande-Fidalgo et al.  [154]  used linear regression and fully-connected networks to reconstruct the entire set of twelve standard leads from a subset of just three input leads. Sohn et al.  [408]  used long short-term memory (LSTM) networks to reconstruct the twelve ECG standard leads from a three-lead patch-type device. Their results show their method was able to correctly retain pathological abnormalities from medical conditions on the reconstructed signals.\n\nThe work of Lee et al.  [244]  was one of the few that studied the synthesis of standard leads using only one reference lead. In their study, chest leads (V1 to V6) were synthesised from lead II using a generative adversarial network (GAN). However, input segments had to be single heartbeats, aligned according to the R-peaks, which decreases the difficulty of the proposed method but also its applicability. Matyschik et al.  [293]  developed patient-specific models to more accurately reconstruct eleven missing ECG signals from a single available lead of the standard 12-lead system. However, the reference lead was either V1, V2, or V3 which, being chest leads, do not enable the usage in less obtrusive setups which would preferentially use limb leads.\n\nIn this work, we explore the more challenging scenario of reconstructing the entire set of twelve standard leads using only one reference lead. Moreover, the reference signals are blindlysegmented (without any kind of temporal alignment) and pertain to one of the limb leads to allow for applications on the least obtrusive setups. Our main goal is to assess whether it is possible to reconstruct the electrocardiogram signal in such challenging scenarios and discuss the next steps towards the use of interlead conversion in less obtrusive clinical setups and wearable devices.",
      "page_start": 148,
      "page_end": 149
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 149,
      "page_end": 149
    },
    {
      "section_name": "General Overview",
      "text": "The proposed methodology for interlead ECG conversion follows the encoder-decoder structure typically used for deep image segmentation. The encoder receives an input signal and processes it to create a compressed representation that retains relevant information for the task at hand. The decoder receives this representation and processes it so that the output matches the ground-truth as closely as possible. Here, the input to the encoder is a short ECG segment of one lead (X) and the ground-truth is the corresponding segment in a different lead (Y). Thus, the encoder is in charge of selecting the information from X that is needed for Y, and the decoder will use that information to reconstruct the corresponding lead Y signal.",
      "page_start": 149,
      "page_end": 149
    },
    {
      "section_name": "Model Architectures",
      "text": "The general encoder-decoder structure allows for diverse specific model architectures. This work focuses on the U-Net model, a fully convolutional architecture that has found many applications related to semantic segmentation and can also be adapted for the task of ECG lead conversion.",
      "page_start": 150,
      "page_end": 150
    },
    {
      "section_name": "U-Net",
      "text": "The U-Net was initially proposed by Ronneberger et al.  [367]  as a tool for biomedical image segmentation. In this work, the implemented architecture (see Fig.  8 .1) receives an input segment of lead X, which initially goes through a chain of three sequential blocks, each with half the signal resolution of the previous block. Each block includes two convolutional layers (each followed by batch normalisation and ReLU activation) and ends with a max-pooling layer.\n\nBetween the encoder and the decoder, two convolutional layers compose the latent space or bottleneck block, which corresponds to the maximum point of information compression. The decoder mirrors the encoder in its structure, with three similar blocks composed of an upsampling layer and two transposed convolutional layers. The last transposed convolutional layer outputs a single-channel signal whose size corresponds to the input segment. The activation function of this last layer is the hyperbolic tangent for an output signal with amplitudes in [-1, 1].\n\nOne aspect of the U-Net which is often cited as the key to its widespread success is the skipconnection. U-Nets typically include skip-connections between corresponding blocks on the encoder and the decoder. This means the feature maps from the encoder blocks are directly routed to the corresponding decoder blocks, allowing the model to propagate context information from multiple resolutions between the encoder and the decoder for higher flexibility.",
      "page_start": 150,
      "page_end": 150
    },
    {
      "section_name": "Convolutional Autoencoder (Ae)",
      "text": "Beyond the aforementioned U-Net architecture, adapted for unidimensional signal inputs, we also explore a convolutional autoencoder (AE, see Fig.  8 .2). Its architecture is very similar to the U-Net, albeit without skip-connections. As a result, the structure is simplified, when compared to the U-Net, and the latent representation sent from the encoder to the decoder is smaller. Experiments with the AE architecture aim to assess if the skip-connections are essential for the task at hand or if the simplified structure could avoid overfitting and bring performance benefits.",
      "page_start": 150,
      "page_end": 150
    },
    {
      "section_name": "Label Refinement Network (Lrn)",
      "text": "The third architecture explored in this work was based on Label Refinement Network (LRN, see Fig.  8 .3) was originally proposed by Islam et al.  [190]  for semantic image segmentation. Its architecture is identical to the aforementioned U-Net. The singularity of the LRN lies in the supervision strategy: while the U-Net only uses the output of the last decoder block in the reconstruction loss, the LRN computes the loss at the outputs of every decoder block. This results in supervision at several resolution levels, leading the decoder to offer a coarse reconstruction right after the first  In this work, we explore both possibilities for 12-lead reconstruction -using one shared encoder connected to all 11 decoders, for all 11 output leads except the one corresponding to the input, or using one individual encoder for each of the 11 decoders. Using individual encoders grants more flexibility to each lead conversion process, as each encoder will be able to learn a unique way to obtain compressed representations and each encoder-decoder pair will work independently from all others. On the other hand, using one shared encoder results in a much lighter and faster algorithm and the added simplicity may contribute to avoiding overfitting.",
      "page_start": 150,
      "page_end": 150
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 150,
      "page_end": 150
    },
    {
      "section_name": "Data",
      "text": "The experiments conducted in this work used mainly the data provided in the PTB Diagnostic ECG Database  [49] , available on Physionet  [146] . The PTB database includes data from 16 channels, including all 12 standard leads, sampled at 1 kHz. It contains a total of 549 records from 290 individuals, with one to five records per subject. Recordings were cropped into segments of 5 s (5000 samples). A second-order Butterworth bandpass filter with cut-off frequencies f c =  [1, 40]  Hz was applied to each segment to remove noise while retaining the most useful ECG information. The amplitudes of the n values of each signal x were then min-max normalised to the interval [-1, 1] following the equation:\n\nThe data from PTB was divided into train and test sets, with approximately 63%, 7% and 30% of the segments, respectively, for a total of 7086, 787, and 3509 ECG segments for each set. For a more thorough and challenging evaluation, subjects are divided between the train/validation and test sets: the latter had recordings from subjects 1 to 50 while the former had recordings from on the quality of the lead conversion results. From the total of 21837 recordings, we selected the 16272 that did not have conflicting superclass annotations. From each recording, the first 5 seconds were cropped, resampled to 1 kHz, and processed as described above for PTB.",
      "page_start": 152,
      "page_end": 153
    },
    {
      "section_name": "Model Training And Evaluation",
      "text": "The models were trained using the l1-loss between the model outputs and the corresponding ground-truth signals as the objective function. The l1 was chosen empirically as it allowed the model to learn most adequately both the overall morphology of the signals and their finer details.\n\nThe Adam optimiser was used with an initial learning rate of 1 × 10 -3 , over a maximum of 500 epochs with batch size 32 (shared encoder) or 16 (individual encoder) and early stopping patience of 50 epochs.\n\nTo compare lead conversions with the corresponding measured ground-truth signals, this work used the following metrics: the average and median Pearson correlation coefficient (r, used in the majority of the related literature), the average root mean square error (RMSE), and the average Structural Similarity Index Measure (SSIM). To compare the selected architectures, the first experiment entailed the one-to-one lead conversion from II to I, two of the most used ECG leads for medical purposes (see Table  8 .1). According to the results, the U-Net performs better than both alternatives AE and LRN. Although the AE achieves the same median r as the U-Net, the average r is lower, meaning that the least successful results are generally worse with the AE than with the U-Net.\n\nThe skip-connections give it the capability to send more information (and at more resolution levels) from the encoder to the decoders, granting it more flexibility and ultimately better performance than the AE. The multi-resolution supervision of the LRN, expected to improve overall performance, appears to excessively draw the model's attention away from the details, which results in worse performance. Following the results of this comparison, subsequent experiments",
      "page_start": 153,
      "page_end": 154
    },
    {
      "section_name": "One-To-All Leads Conversion",
      "text": "Not all leads can be converted equally: the correlation between leads depends on their perspectives of the heart. Table  8 .2 presents an overview of the average correlation between lead II and the remaining eleven standard leads, computed using the PTB, INCART, and PTB-XL test segments.\n\nSpecifically for the PTB data, one can observe that some leads such as aVF or aVR are highly (positively or negatively) correlated with lead II. On the other hand, aVL is almost orthogonal.\n\nHence, one should expect aVL to be much harder to accurately convert from lead II than aVF or aVR, since the former shares much less information with lead II than the latter. This is verified in the results for multi-lead conversion on the PTB database (see Table  8 .3).\n\nConversion from lead II to aVF, aVR, and V6 consistently offer good results, while the conversions to aVL, lead I, or V4 were overall the least successful. This behaviour is also visible in the example of Fig.   better performance with aVR and aVL while struggling to convert from lead I to lead aVF. The same can be observed in Fig.  8 .5: for aVR and aVL, the model is able to correctly capture the target morphology, while the reconstructions of aVF and V3-V6 are largely unsuccessful.\n\nFrom the example result in Fig.  8 .5, one can also identify a shortcoming of the proposed methodology: the occasional offsets between the baseline of the measured and converted signals.\n\nWe suspect this is due to the min-max normalisation of the signals, drawing them into the [-1, 1] amplitude range. Alternatives to this normalisation, such as standard normalisation, should be further investigated.\n\nConsidering the overall results, no lead is perfect for converting all twelve standard leads.\n\nHence, lead II should be chosen as reference input when aVF or V5-V6 are the most important leads for the application at hand. Lead I serves better as a reference when aVR, aVL, or V1-V2 are more important. Otherwise, other leads (such as lead III) should probably be explored.\n\nNevertheless, the results show it is possible to nicely reconstruct several leads using only one input lead without temporal alignment.\n\nUsing either lead as a reference, there is apparently no considerable or consistent difference between using one single shared encoder or using an individual encoder for each target lead. It appears as if the additional flexibility of having multiple encoders is only beneficial up to a point, and the higher complexity ends up opening the door to overfitting and loss of robustness. As such, for this application, one should expect a shared encoder to be the best option, considering its higher simplicity and similar performance.",
      "page_start": 155,
      "page_end": 156
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "For a comparison with the state-of-the-art, we implemented the method recently proposed by Grande-Fidalgo et al.  [154]  as a baseline. This method is based on a simple fully-connected model that receives each signal point's amplitude in three reference leads as inputs and returns the same point's amplitude in all twelve leads. Here, we adapt the methodology so it receives signal point amplitudes from one single lead (leads I or II), to exactly match the evaluation conditions of the proposed method.\n\nUnlike what has been reported in  [154] , the baseline was not successful in learning to retrieve the entire set of leads from just one reference lead. In fact, across all leads, the average test r of this The fact the baseline method reconstructs signals point-by-point, unable to analyse broader local context information, makes it hard to reconstruct the signal without already having data from more than one channel. On the other hand, using convolutional layers allows the proposed method to use broader local information as context to adequately learn to reconstruct signals using only one lead as reference.",
      "page_start": 158,
      "page_end": 158
    },
    {
      "section_name": "Cross-Database Evaluation",
      "text": "The cross-database tests aimed to assess the behaviour of the proposed methodology in more diverse scenarios. Here, the models used were the same as in the previous experiments (trained For PTB-XL (see Table  8 .8 and Table  8 .9), results are, overall, the worst, although some leads (namely V4, V5, and V6), due to higher correlation with the reference leads, are better reconstructed than with the PTB database. In Fig.  8 .8 and Fig.  8 .9, it is possible to observe that, despite occasional baseline offset and prevalent noise, both reference leads enable the approximate reconstruction of most of the set of twelve standard leads.\n\nFor either database, differences in acquisition settings and electrode placement result in inferior performance. The ideal solution is to always make sure the acquisition details of training and inference data match, to ensure optimal performance upon deployment. Nevertheless, the robustness in cross-database scenarios is a relevant issue that merits further research.",
      "page_start": 159,
      "page_end": 161
    },
    {
      "section_name": "Influence Of Medical Conditions",
      "text": "As aforementioned, medical conditions may affect differently the various leads of an ECG signal.\n\nWhile this is the main motivation behind the quest to reconstruct missing leads it may also be one of the main hurdles. If the medical condition is somehow not evident in the input lead, the algorithm could be led to reconstruct the remaining leads incorrectly without the proper information on the respective medical condition.\n\nAs such, we conducted a differential performance evaluation according to the existence and type of diagnosed medical conditions on the signals. To do this, we use the expert clinical annotations on the PTB-XL database and separate the results by the superclass labelling of each test sample. The average r results for each converted lead and each superclass are presented in Table  8 .10 (using lead II as reference) and Table  8 .11 (using lead I as reference).\n\nOverall, no dominant difference could be observed between the results with normal signals and the results with signals with medical conditions. Similarly, no specific medical condition superclass presents considerably different performance results. This is likely due to the presence of medical conditions on the PTB signals originally used for training the model. Thus, although the behaviour of the proposed methodology should be expected to vary slightly in the presence of medical conditions, it should not have a considerable impact on its baseline performance.    In the cross-database scenario, despite the acquisition setup differences, results were promising especially with the INCART database. Finally, the analysis of the influence of medical conditions has shown no considerable effect of pathologies on the performance of the proposed methodology.",
      "page_start": 161,
      "page_end": 166
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "However, a state-of-the-art methodology for automatic diagnosis revealed lower accuracy when using reconstructed signals, a problem that should be addressed in future research.\n\nAlthough the results are promising, further efforts should be devoted towards the improvement of the methodologies for interlead conversion using single-lead blindly-segmented inputs. Namely, the pre-processing and normalisation of the signals, as well as the robustness to diverse acquisition setups, should be the target of further research. Additionally, task-oriented objective functions should be explored to ensure useful signal information is kept and avoid performance losses in subsequent diagnoses.\n\nWith some consolidation, the proposed methodology could be the key to better cardiac health monitoring in wearable devices and less obtrusive clinical scenarios. Taking the example of emergency rooms, if we can retrieve all twelve leads (or the most important among these) from Lead I signals, then patients will only need two electrodes placed on the wrists to have their ECG collected, instead of the full set of 10 electrodes on wrists, ankles, and chest. This is a meaningful step towards higher comfort and usability for patients in clinical settings or users in other scenarios involving the monitoring of ECG signals. Additionally, albeit outside the scope of this work, this methodology could also be applicable to other multi-channel signals where the different channels correspond to different perspectives over the same physiological phenomenon.",
      "page_start": 166,
      "page_end": 166
    },
    {
      "section_name": "Part Iii Face Biometrics",
      "text": "Chapter 9\n\nPrior Art in Face Biometrics",
      "page_start": 167,
      "page_end": 167
    },
    {
      "section_name": "Data",
      "text": "There are several publicly available databases for research purposes, to develop and benchmark face biometric recognition algorithms. Considering the face is one of the most developed biometric traits, the databases available are some of the largest and most complete, thoughtfully structured for deep and adequate evaluation of recognition algorithms. Table  9 .1 compiles some relevant information about the most important databases currently available, which are also described below:\n\n• • CelebA: This database results from a previous one, CelebFaces+, which has been enriched with fiducial and attribute annotations. It includes over two hundred thousand pictures from over ten thousand celebrities, with five fiducial locations, and forty binary attributes per image  [267] ;\n\n• COX Face: The COX Face database was designed to study recognition across still images and videos. Thus, it includes still images from one thousand subjects in a controlled environment with high quality, and surveillance videos from the subjects in unconstrained and low-quality settings  [181] ; • CSIST Lab1: The CSIST database was developed by the Chung-Shan Institute of Science and Technology, with images from volunteers at the Harbin Institute of Technology of Shenzhen. The Lab1 dataset contains ten visible light and ten NIR images from each of fifty subjects  [466] ;\n\n• CSIST Lab2: The CSIST Lab2 dataset is part of the CSIST database, and includes twenty visible light and twenty NIR images from fifty volunteers, with natural lighting and artificial lighting  [466] ;\n\n• collection, which enabled the creation of this large database, with over two million faces in unconstrained conditions  [331] ;\n\n• VGGFace2: The VGGFace database was later extended to create the VGGFace2 database, which includes more than three million unconstrained face images from almost ten thousand identities  [58] ;\n\n• Yale Face: The Yale Face database includes eleven images from each of fifteen subjects.\n\nAlthough not an unconstrained database, it includes annotations on certain expressions and configurations simulated by the subjects, which can be useful in training models for other tasks such as emotion recognition;\n\n• YouTube Faces: This dataset is composed exclusively of faces on YouTube videos. It includes over three thousand videos with over one thousand identities. The videos range from 48 to 6070 frames (average of 181 frames per video)  [460] .\n\nThese databases already cover most bases and offer a good starting point for the study and development of strong biometric algorithms. Nevertheless, it is important to acknowledge the growth of heterogeneous approaches in facial recognition, and the subsequent need for databases of face images acquired in different modalities (e. g., visual spectrum vs. NIR). These databases are still too small and too controlled for the development of robust algorithms. Moreover, it would be useful to have larger databases focused on more specific applications (e. g., face images and videos of car drivers).",
      "page_start": 169,
      "page_end": 170
    },
    {
      "section_name": "Related Work",
      "text": "According to Barnouti et al.  [28] , face recognition can be decomposed into three processes: face detection, feature extraction, and face recognition (see Fig.  9 .1). Below, we delve into the stateof-the-art in each of these processes. Due to the currently common practice of joining feature extraction and face recognition into a single model using deep learning, these two processes are jointly discussed.",
      "page_start": 171,
      "page_end": 171
    },
    {
      "section_name": "Face Detection",
      "text": "Given an image or video stream, the process of face detection has the goal of locating and extracting all human faces visible in the received input. It is an extremely important process not only These presented the advantage of being orientation-invariant, as it would serve to detect a face even if it did not present a frontal pose. However, they fail to consider the great variety of skin colours, both due to natural differences between individuals, and due to diverse illumination conditions.\n\nMore sophisticated algorithms have been proposed, including the method by Sirovich and Kirby  [406]  based on eigenvectors from large face image datasets, the method by Viola and Jones  [442]  which uses cascades of Haar transform filters selected using AdaBoost, or the method by Dalal and Triggs  [92]  which uses histograms of intensity gradients from image regions and their orientation. These commonly offer very fast detection, adequate for real-time systems, but often fail on non-frontal face detection and faces of very diverse scales.\n\nLike most pattern recognition tasks, traditional methods from earlier literature have been recently replaced with deep learning algorithms. These offer more robust and accurate face detections, especially for non-frontal face detection, making better use of very large datasets currently available.\n\nSome of these datasets currently offer a public benchmark for fair and direct comparison with state-of-the-art methods. These include the WIDER face dataset  [469]  and the Face Detection Database (FDDB)  [203] . These benchmarks are currently largely dominated by deep learning approaches.\n\nThe FDDB benchmark is currently dominated by the S 3 FD, the DeepIR, and the RSA algorithms. S 3 FD  [486]  is based on a single deep network specifically fitted to better detect small faces. The DeepIR method  [416]",
      "page_start": 172,
      "page_end": 173
    },
    {
      "section_name": "Feature Extraction And Recognition",
      "text": "Having extracted the detected faces from the input, face-based recognition systems need to extract appropriate features from those faces to accurately decide on their identities. Wang and Deng  [452] , in their survey of deep learning face recognition, have pointed out how the field of face biometrics has moved from traditional machine learning approaches to deep learning (see Fig.  9 .3).\n\nHowever, the best results were only obtained when the development of sophisticated tailored objective functions began (see Fig.  9 .4).\n\nAs such, Wang and Deng  [452]  divide approaches into four categories: holistic learning, local handcrafted, shallow learning, and deep learning methods. Below, the most relevant examples of each category are presented, along with their advantages and shortcomings.\n\nHolistic learning approaches are those that use the whole face image to obtain representations that ease the process of face recognition. The Eigenface method  [406] , described for face detection, is one of these methods, along with the Fisherface method  [34] , which is similar to\n\nEigenface, but uses the Fisher Linear Discriminant Analysis (FLDA, instead of PCA) for dimensionality reduction. Such methods are simple and fast, but lack robustness to several variability factors.\n\nEventually, researchers started to explore methods that extracted features from regions of the face image. These methods mostly used Gabor filters and Local Binary Patterns for feature extraction based on intensity gradients and image edges  [12; 262] . Methods like these and the Elastic Bunch Graph Matching  [459]  were able to improve recognition accuracy, but not to make it high enough for real use.\n\nTo improve accuracy and robustness to pose variations, researchers proposed learning-based methods, that used available data to learn the best features. The first method to use shallow learning was proposed by Cao et al.  [59] , using gradient filtering after facial landmark alignment and clustering methods to learn encodings for better recognition.\n\nBut truly high accuracy and robustness in face recognition were only attained with the rise of deep learning. The first models were convolutional neural networks with conventional architectures, such as DeepFace  [423] , VGG-Face  [331] , or VGG-Face2  [58] . Over time, researchers started to focus on adapting the networks for specific details of face recognition, such as custom loss functions that force increased intersubject separability. This resulted in improved methods such as DeepID  [417] , L2-Softmax  [360] , and ArcFace  [98] .\n\nBoth Facenet and DeepID are among the top five non-commercial methods in the LFW benchmark, with 99.63% and 97.45% accuracy, respectively. However, as discussed by Wang and Deng  [452], the best results yet haven't been achieved with traditional deep learning losses such as softmax or even triplet loss, but with tailored objective functions specifically designed to make the most of available data for the task of face recognition. According to results reported by Wang and Deng  [452] , both L2-softmax and ArcFace offer even better performance in the LFW benchmark, with 99.78% and 99.83% accuracy, respectively. In fact, ArcFace, using the tailored ArcLoss objective function, is still widely recognised as the state-of-the-art approach for face recognition.\n\nThese methods and results show the high potential offered by adapted deep learning networks for face recognition. However, these present the same problem as deep learning approaches for face detection: high computational cost. These models are generally very complex, and researchers should devote efforts to making them more efficient. Furthermore, video-based benchmarks should be used more to evaluate methods also based on their timeliness. Finally, as stated by Arya et al.  [21] , research in visible or infrared spectra may be reaching its limits, and the future may be based on multispectral imaging.\n\nOverall, face biometric recognition is already a thoroughly developed topic, unlike other biometric characteristics (such as the electrocardiogram). Joint efforts dedicated by several international research groups throughout multiple decades have brought this topic to a stage of high maturity that enables real applications. Proof of that is the currently endless variety of applications of face biometrics in our day-to-day routine, from unlocking our phones to border control, including opening bank accounts remotely.",
      "page_start": 174,
      "page_end": 174
    },
    {
      "section_name": "Presentation Attack Detection",
      "text": "Just like any other biometric solution or access control system, face biometric systems are prone to attacks. These systems commonly guard goods or information whose value entices attackers to try to fool it into granting them access. In face biometrics, one of the main ways to do this is through presentation attacks: here, an attacker presents to the sensor fake or altered samples of the biometric trait that contains identity information from an authorised person  [386] .\n\nPresentation attack detection (PAD) algorithms aim to automatically recognise when captured biometric samples have been faked or altered in such a way, preventing a biometric system from granting access to an attacker. They consist of binary classifiers that distinguish between presentation attacks or bona fide samples. Presentation attacks involve presentation attack instruments (PAI) which can be printed photographs, digital screens, paper masks, or even tridimensional silicone masks: each of these types of PAI is called a PAI species (PAISp)  [192] .\n\nEarlier PAD approaches focused on one single PAISp (i.e., trained and tested only with one type of PAI), a scenario that can be designated as one-attack. This can naturally lead to overly optimistic results that may not be verified in real-life applications, since attackers are perpetually working on new and improved PAI species and face biometric systems will expectedly be faced with more than one during application.\n\nAnother",
      "page_start": 176,
      "page_end": 176
    },
    {
      "section_name": "Robustness And Trustworthiness",
      "text": "Faced with the current state of face recognition, some would say face recognition is a solved topic. Advances in deep learning architectures, tailored loss functions, and massive online-sourced databases have enabled the topic of face biometrics to achieve near-perfect performance metrics. This is true even for edge scenarios, on challenging datasets with significant pose, illumination, and environment variability.\n\nHowever, diverse challenges remain to be solved or have surged over the recent years due to developments in face recognition or society in general. Here, we focus on two of the most pressing problems: trustworthiness and robustness. The first relates to the significant opacity of deep learning-based state-of-the-art approaches. The second is linked to the difficulty of current methodologies to recognise faces under occlusions, especially masks. Below, we delve deeper into each of these challenges.",
      "page_start": 177,
      "page_end": 177
    },
    {
      "section_name": "Face Recognition In A Masked Society",
      "text": "The ongoing Covid-19 pandemic has had a meaningful negative impact on face recognition systems  [152] . The widespread (and generally mandated) use of face masks covering the nose, mouth, and chin regions of the face has been reported to significantly degrade the accuracy of existing face recognition solutions  [93; 204; 313; 314] .\n\nBefore the pandemic, research on the robustness to occlusions in face recognition was fairly common. However, it was also rather limited to small occlusions like sunglasses or scarves  [324;  409], which do not typically hide as much information as a face mask (see Fig.  9 .5). One should easily understand how unprepared the existing face recognition solutions were for this new global paradigm.\n\nFigure  9 .5: Example of how a mask can significantly occlude a face and limit the information that can be used by a face recognition algorithm (from  [93] ).\n\nSince the dawn of these challenging circumstances, multiple authors have studied in detail the effects of wearing masks on face recognition  [152] . Two of the most relevant studies were conducted by the National Institute of Standards and Technology (NIST), focusing on pre-Covid-19  [313]  and post-Covid-19 algorithms  [314] . These studies were part of the ongoing Face Recognition Vendor Test (FRVT), an independent and thorough benchmark of face recognition solutions, and the results indicate that competitive algorithms, which fail to authenticate less than 1% of probes in typical circumstances, failed up to 50% more frequently in the presence of synthetic masks.\n\nBeyond these studies, some have studied the effect of real masks in academic and commercial solutions. Damer et al.  [93]  has found that the FMR100 score of the state-of-the-art method SphereFace can increase from 0.065% to 27.35% when evaluated with real masked face images from twenty-four participants. Wearing masks has also been reported to significantly impact verification performance of human operators  [94] , face image quality estimation  [136]  and presentation attack detection performance  [120] .\n\nEarly works on this topic included the automatic detection of face masks in images  [268; 355]  which, however, do not effectively solve the problem. Later, Li et al.  [256]  proposed an attentionbased method to train a model on the periocular face region, avoiding the mask areas and achieving promising results.\n\nMost other solutions have focused on the adaptation of existing pretrained face recognition networks, likely due to the limited amount and diversity of data currently available for masked face recognition. Anwar and Raychowdhury  [19]  studied the benefits of synthesising masks on the training data.  [140]  augmented existing datasets with realistic masked images using a generative adversarial network (GAN) specifically tailored to retain identity information.\n\nMore recently, Boutros et al.  [52]  proposed an efficient solution to be integrated on top of existing face recognition models, which attempts to unmask the embedding produced by the backbone. The unmasking module is based on a neural network trained with a self-restraining triplet loss which prioritises more affected genuine pairs. ticFace loss, which is also used to ensure the resulting embeddings retain identity information.\n\nSimilarly, Li et al.  [248]  used knowledge distillation combined with image-level face completion.\n\nDespite all these recent efforts and meaningful strides, there are still plenty of hurdles to overcome. The rise of masks in our society uncovered the feeble nature of face recognition systems in the face of extensive occlusions, and researchers are organising to face the challenge (e.g., through large competitions in masked face recognition  [50; 100] ). Nevertheless, a definitive solution should only be achievable with strong concerted efforts on improving available databases, adapting existing algorithms, and designing novel training strategies.",
      "page_start": 177,
      "page_end": 179
    },
    {
      "section_name": "Interpretability In Face Biometrics",
      "text": "The growing use of increasingly sophisticated and elusive deep learning models has been sparking the need for strategies to better understand their inner workings. This is also true for several biometric applications. Namely, as face recognition systems permeate further into more critical applications such as border control or law enforcement, trustworthiness and transparency are paramount  [3; 17; 137; 352] .\n\nIn recent years, multiple studies have found that biometric systems (especially those based on face) generally present significant demographic biases which have been able to survive largely unnoticed. For example, lower false match scores among men (when compared to women) have been linked to a significantly larger variety of facial hair styles. Similarly, the variety of hairstyles, face morphology, and makeup styles have also been cited as possible causes behind gendered differences in face recognition accuracy  [14; 15; 356] .\n\nIf a biometric model suffers from bias, traditional metrics or visualisation methods will not be able to detect it or explain that these features played a prominent role in the decision. Having more thorough mechanisms of understanding the behaviour of biometric models, for instance through interpretability, could be the key to unveiling and avoiding such biases and ensuring fair treatment at all times (see Fig.  9 .6).\n\nThe topic of interpretability in biometrics is still in its early stages, although some researchers have already delved into the study of transparency for face biometrics. Some of these focused on attention mechanisms  [6; 206; 387] , which allow models to focus on relevant areas that can be adjusted to avoid certain undesirable behaviours.\n\nOne of the first approaches for interpretable face recognition was proposed by Yin et al.  [473] ,\n\nusing feature and spatial activation diversity losses. These were used to promote, respectively, filter robustness against occlusions and the inclusion of semantic information. Williford et al.\n\n[458] explored a new way to obtain explanations by combining triplets and an inpainting game.\n\nUsing subtree excitation backpropagation and density-based input sampling for the explanation, model interpretability is promoted and saliency maps can be built to support explanations. Liu et al.  [263]  used adversarial training for heterogeneous face recognition, explicitly promoting the introduction of semantic and interpretable information in the model's latent space.\n\nAs for face PAD, most works so far are limited to the simple application of explainability tools (typically GradCAM), or the use of t-distributed stochastic neighbour embedding (t-SNE) as a way to interpret the way features lead to certain decisions  [121; 207; 336; 454; 455; 475] .\n\nBeyond these works, some authors proposed the application of auxiliary supervision techniques to promote interpretability  [265; 266; 470] .\n\nBeyond face recognition and presentation attack detection, Seibold et al.  [382]  studied face morphing attack detection using focused layer-wise relevance propagation (FLRP) as a way to explain decisions to humans. Similarly, Xu et al.  [467]  proposed the use of FLRP to explain decisions of their deepfake detection method. Alongside a method to simulate face aging, Genovese et al.  [141]  proposed the cross-GAN filter similarity index (CGFSI) that can be used to explain the behaviour of face GANs.\n\nDespite these first works on interpretability for face biometrics, there is still a long way to go. The effect of causality on the production of explanations and the possibility for multiple simultaneous explanations are still open topics. Likewise, the production of semantic and textual explanations (beyond simply visual ones) is yet to be addressed. All of these issues remark the relevance of calling for transparency in face biometrics and adopting the new technologies in interpretability for this topic.",
      "page_start": 179,
      "page_end": 179
    },
    {
      "section_name": "Open Challenges And Opportunities",
      "text": "As discussed throughout this chapter, face recognition is an intensively and thoroughly researched topic with praised results and a significant impact in the real world. Through the study of increasingly sophisticated deep learning methodologies and the design of tailored objective functions, face recognition was able to conquer a prominent place in our society with robust and reliable commercial solutions.\n\nNevertheless, it is also clear that some problems remain largely unsolved. Nowadays, those are mainly related to robustness in masked face recognition scenarios, the trustworthiness of sophisticated deep learning-based solutions, and the detection of presentation attacks using unseen species. These are the topics which currently pose the greatest threats to face recognition applications and, thus, the most promising research opportunities.\n\nHence, this thesis part focuses on two contributions to these open challenges and opportunities.\n\nSpecifically:\n\n• In Chapter 10, we propose two methodologies based on triplet and contrastive learning strategies, combined with ArcFace and mean squared error losses to promote similarity between masked and unmasked image embeddings and close the performance gap on masked face recognition;\n\n• In Chapter 11, we study the use of interpretability to better understand the decisions of deep learning models in face PAD, in order to motivate the broader application of interpretability and explainability for more transparent and trustworthy biometrics.",
      "page_start": 180,
      "page_end": 181
    },
    {
      "section_name": "Methodology",
      "text": "Two approaches were explored to close the performance gap between unmasked and masked face recognition. The first one is an adaptation of the triplet loss training strategy  [309] . The second one is a multi-task contrastive learning combination of cross-entropy (CE), ArcFace, and mean squared error (MSE) objective functions  [308] . Both aim to influence the behaviour of the models through the respective loss functions by leading them to steer clear of information that can be occluded by the use of face masks.",
      "page_start": 184,
      "page_end": 184
    },
    {
      "section_name": "Adapted Triplet Loss",
      "text": "The triplet loss  [72]  has been frequently used in general pattern recognition tasks, including the particular case of face recognition models. Instead of instance-based learning, this methodology organises training data into triplets composed of an anchor (x A , which serves as a reference), a positive (x P , which shares the same identity as the anchor), and a negative sample (x N , of a different identity). Each input corresponds to a model embedding representation in the learned output space: y A , y P , and y N , respectively. With these, the triplet loss follows the equation:  The triplet loss will lead to embeddings of the same identity being clustered together in the output space. However, they are still allowed some variability as long as it verifies the α margin to the negative samples. In the scenario of masked face recognition, adding a mask to an image can be enough to result in a large difference in the embedding within the margin allowance. This is, however, undesirable. A method that is truly robust to face masks should be able to completely ignore masks and, ideally, offer the exact same outputs for a face image and the exact same image but with a face mask. Only thus do we have complete certainty that the model is robust to such occlusions.\n\nAs such, we adapt the triplet loss learning strategy to promote such behaviour. Alongside the typical members of a triplet (y A , y P , and y N ), the model also receives a version of the anchor with a synthetically added face mask (y A m ) covering the mouth, chin, and nose according to health and safety guidelines. The mean squared error (MSE) between y A and y A m is added to the aforementioned triplet loss formulation to further promote the similarity between the anchor and the masked anchor:\n\nWith this, we aim to lead the trained model to avoid the regions of the face that are commonly occluded by masks and thus retain performance levels when performing inference on masked face images (see Fig.  10 .2).",
      "page_start": 185,
      "page_end": 185
    },
    {
      "section_name": "Multi-Task Contrastive Learning",
      "text": "Building upon the idea of the adapted triplet loss, we propose a multi-task approach for masked face recognition combining multiple objective functions. The proposed methodology, illustrated in Fig.  10 .3, receives pairs of masked and unmasked images and is composed of two symbiotic parts: one makes the network aware if a face mask exists in the input image, and the other uses mask awareness for higher stability in identity learning among masked and unmasked images.\n\nThe first part consists of the detection of masks in the input image. Embedding features from a common backbone (which can be a pretrained face recognition network) are delivered to a fullyconnected module which performs binary classification (masked or unmasked). This module is optimised by minimising the cross-entropy objective function:\n\n∑ k∈n e y ik ,  (10.3)  where N represents the number of samples in the mini-batch, n is the class number, and y it represents the output of the module for the sample i and target class t. This strategy leads the model to become aware of the presence of a mask in the input image and to include this information in the output embedding.\n\nThe second part focuses on the identity recognition task. For improved results, we take the ArcFace approach  [98] , widely considered the state-of-the-art in face recognition. ArcFace learns identity by minimising the following loss function:\n\nwhere m represents the embedding distance margin, s denotes the scale, and θ it the angle between features (x i ) and weights (W t ) for sample i and target class t. Both m and s are tunable hyperparameters of the loss. ArcFace loss explicitly promotes higher intraclass similarity and diversity among inter-class samples. Also, thanks to the l 2 normalisation of both the weights and the feature vector, the loss becomes equal to the geodesic distance margin penalty in a normalised hypersphere.\n\nThe global loss is the combination of ArcFace and CE losses computed for both masked and unmasked inputs as well as the MSE between the embeddings of each input. As with the afore-described adapted triplet loss approach, the MSE is intended to reinforce the similarity between the latent representations of masked and unmasked inputs throughout the training process, thus further promoting robustness to this kind of face occlusions. The training methodology is applied to a ResNet-50 backbone architecture  [167]  that is used to extract features from masked/unmasked face images. This backbone was trained using crossentropy loss and stochastic gradient descent for 150 thousand epochs with a batch size of 400. The initial learning rate was set as 0.1, decreased by a factor of 10 whenever validation accuracy began to decay. After convergence, the backbone was fine-tuned either with the original triplet loss or the adapted triplet loss, with α empirically set to 0.2. Triplets were randomly generated during training, without using any mining strategy, across a total of 65 thousand epochs.\n\nTraining used synthetic masked face images. The VGGFace2 dataset  [58] , composed of over 3.3 million face images from more than nine thousand identities, was adapted to include masked faces. The NIST Dlib C++ toolkit  [314]  is used to obtain sixty-eight face landmarks which are then used to generate masks appropriately fitted to the mouth/nose region, as detailed in  [218; 314] ,\n\nallowing for some controlled variability regarding mask shape and colour. No face alignment was performed, and input images to the model were shaped 224 × 224 × 3.\n\nThe trained models were evaluated in a biometric verification task in two scenarios: U-M, where the reference is an unmasked image and the probe is a masked image, and M-M, where both the reference and probe are masked face images. The performance is reported through the false non-match rates (FNMR), the FMR100 and FMR10, which are the lowest FNMR for a false match rate (FMR) < 1.0% and < 10.0%, respectively. Additionally, the equal error rate (EER) and the area under the receiver operating characteristic curve (AUC) results are also reported. The genuine mean (GMean) and impostors mean (IMean) scores, which represent the mean distances between the mated and non-mated embedding pairs, were also computed. The proposed adapted triplet loss methodology was evaluated on two distinct datasets: one with synthetic masks (SMFD) and the other with real masked face images (RMFD). A detailed stepwise ablation study is used to understand the behaviour of the models and the impact of the proposed triplet loss adaptation in masked face recognition. The results on real masked data are presented in Table  10 .2, including experiments on the U-M and M-M scenarios. It can be observed that, in general, the performance results are considerably inferior to those on SMFD, likely due to the models being trained with synthetic masks and tested with real masks. However, the proposed adapted triplet loss attained superior performance across all metrics.\n\nIt is noteworthy that M-M results, in general, do not differ considerably or consistently from U-M ones. This could be a result of the optimisation process of the adapted triplet loss, which leads the model to minimise the distance between masked and unmasked face embedding pairs. However, it is expected that some applications, such as border control, could consist of the comparison of masked probes (face images captured in loco) with unmasked references (passport  Alongside the quantitative results presented above, the behaviour of the models trained with the proposed and alternative approaches was also evaluated through explainability. Fig.  10 .4\n\npresents six example images for which the Smooth Grad-CAM++ method  [322]  was used to assess the relevance of the input pixels for the output embedding features. Relevance maps were computed for each embedding feature and then averaged.\n\nIt can be observed that the first method, trained only with cross-entropy loss, was already largely able to ignore mask regions in the figures, even when masks are not present. However, it also commonly uses the region of the chin to construct the output embeddings, which does not happen with the remaining two methods. When comparing the explanations of the triplet loss and the adapted triplet loss, it can be observed that the latter typically considers more information from wider regions of the face, thus capturing more information that could be useful for more robust decisions. Following the example of several literature works  [18; 98; 179; 295] , this research used the MS1MV2 dataset  [98] . The MS1MV2 dataset is based on the MS-Celeb-1M dataset  [158]  and is composed of 5.8 million images from 85 thousand identities. In this work, this dataset has been augmented, offline, with face masks by generating one image with a face mask for each image in the original dataset.\n\nFace masks were synthesised using the MaskTheFace open source tool  [19] , which includes five mask options (N95, KN95, surgical, cloth, and gas masks). For each image in the MS1MV2 dataset, one of the first four mask types was randomly selected for the process of synthesising masked face images, and the MaskTheFace tool took care of correctly reshaping and rotating the mask templates to match the detected face landmarks in each image.\n\nThe described process was similarly followed for the images in the Labeled Faces in the Wild (LFW) dataset  [178] , used for model validation during training (see Fig.  10 .5 for some examples).\n\nAlthough trained and validated on synthetic data, the method was evaluated on the MFR dataset, composed of real masked face images, just as the adapted triplet loss methodology. Also, like with the adapted triplet loss, we explore the evaluation scenarios U-M (with unmasked references and masked probes) and M-M (with masked references and probes).",
      "page_start": 185,
      "page_end": 190
    },
    {
      "section_name": "Results And Discussion",
      "text": "This section presents the results achieved by the multi-task contrastive loss approach. Comparisons focus mainly on the adapted triplet loss methodology, alternative approaches submitted to the MFR competition, and multiple ablation studies.\n\nThe conducted ablation studies explored smaller backbone models, the use of pretrained networks, and diverse image selection approaches for the contrastive learning module. According to the results presented in Table  10 .3, four of the explored variants were capable of outperforming the official MFR competition baseline  [50]  under the U-M scenario. Two of them were capable of outperforming the baseline under the M-M scenario as well. The non-pretrained ResNet-100 backbone model offered the best overall performance, especially when using the original pair selection process where the masked image is the masked version of the unmasked image. Nevertheless, the results with pretrained models reveal that the proposed method can effectively make existing models able to recognise masked faces with reasonable accuracy and reduced training effort.\n\nIn fact, even though accuracy is important, time is also a key factor with such processing-heavy algorithms. Most of the MFR competition submissions using a ResNet-100 backbone required the training of more than 65 million parameters in the backbone model and up to 44 million parameters in the ArcFace layer. Using pretrained weights for the entire backbone (except for the last layer), the number of trainable parameters is reduced by approximately 47.6% while still outperforming the baseline algorithm (see Table  10 .4).\n\nTable  10 .5: Comparison of the FMR100 results of the methods presented in the MFR competition  [50] , the official baseline, the adapted triplet loss, and the multi-task contrastive learning approach.",
      "page_start": 190,
      "page_end": 191
    },
    {
      "section_name": "Fmr100 Method",
      "text": "U-M M-M Baseline [50] 6.009% 5.925 % A1_Simple  [50]  5.538% 5.771% VIPLFACE-M  [50]  5.681% 5.759% MaskedArcFace  [50]  5.687% 5.825% Adapted Triplet Loss  [309]  28.252% 23.507% Multi-Task Contrastive Learning 5.750%",
      "page_start": 192,
      "page_end": 192
    },
    {
      "section_name": "5.509%",
      "text": "In general, one can observe that better results were obtained when the masked image corresponds to the unmasked image (original pair selection), likely as this better allowed to reinforce in the model the behaviour of avoiding information that could be occluded by a mask. As for the U-M and M-M scenarios, performance differences were not relevant nor consistent for any of the models. As such, the proposed method is able to outperform the challenge baseline in both scenarios.\n\nA comparison with other submissions to the MFR 2021 competition is presented in Table 10.5, for the U-M and M-M scenarios. The compared methods (A1_Simple, VIPLFACE-M, and MaskedArcFace) were selected based on the competition results, the chosen loss functions (ArcFace loss), the input and feature vector sizes (112 × 112 × 3 and 512, respectively), and the dataset used (MS1MV2). Additionally, the proposed method is also compared to the challenge baseline and the adapted triplet loss method  [309] .\n\nAs presented in Table  10 .5, it is possible to see that, despite presenting very similar performance between the U-M and M-M scenarios, the proposed multi-task contrastive learning methodology is only able to outperform the selected competition submissions on the M-M scenario. However, the difference between the methods' performances is relatively small and the contrastive approach was able to consistently outperform both the official baseline and the adapted triplet loss methodology in both scenarios.\n\nIn the U-M scenario, a possible explanation for the advantage of A1_Simple is that it uses a larger backbone model than ours, with roughly 34% more parameters. As for VIPLFACE-M, the proponents used a different (improved) mask synthesis technique, which could be responsible for the performance benefit. MaskedArcFace appears to also apply synthetic masks to the test set.\n\nAs for the M-M scenario, the proposed contrastive solution was able to outperform all alternative methods by a margin of at least 0.250% FMR100.\n\nThanks to the multi-task nature of the proposed methodology, it is possible to leverage the second embedding to perform simultaneous mask detection. Fig.  10 .6 shows the ROC curve results of the proposed model on the mask detection task, evaluated on the augmented LFW dataset.\n\nThe model achieved a perfect face detection score, which could be exaggerated by the relative Overall, these results showcase the capabilities of the proposed methodology, not only versus the official MFR 2021 competition baseline and the submitted algorithms, but also against the proposed adapted triplet loss. Moreover, the ability to achieve competitive results with largely frozen pretrained backbones enables obtaining multi-task architectures with up to 80% fewer trainable parameters.",
      "page_start": 192,
      "page_end": 193
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "The work presented in this chapter addressed the challenge of masked face recognition, made relevant by the worldwide face mask mandates born out of the recent Covid-19 pandemic. Despite the extensive work on the robustness to occlusions in face biometrics, the presence of a mask hiding the nose, chin, and mouth has resulted in significant performance decay.\n\nHere, two approaches were presented to mitigate this issue. The first is an adaptation of the triplet loss by combining it with the mean squared error (MSE) loss, which will minimise the distance between embeddings from masked and unmasked versions of a face image. The second is a multi-task contrastive learning approach that aims to make the model aware of the presence of a mask and uses it alongside the ArcFace and MSE losses to achieve a more robust recognition model. Both approaches aim to lead a biometric model into offering the same templates for a face image, whether or not it contains a mask, effectively leading the model to avoid information from the face regions that could be occluded by a mask.\n\nResults show that the combination of different losses and the promotion of embedding similarity between masked and unmasked versions of a face image are successful at improving masked face recognition performance. Experiments with an explainability tool also show the models are indeed led to avoid regions that could be occluded by masks and to better use information from visible face regions.\n\nHowever, the second proposed approach, based on a multi-task contrastive learning strategy, is the one which offered the best results. By combining the ArcFace loss (widely praised in face recognition literature) with the MSE loss (which showed promise in the adapted triplet loss results)\n\nand a strategy to make the network aware of the presence of masks, it was able to outperform the official baseline and the best algorithm submissions in the MFR 2021 challenge.\n\nAlthough the results are promising, further efforts should be devoted to the improvement of the contrastive learning approach, the preprocessing stage of adding synthetic masks to test data, and the study of larger and more accurate backbone architectures. The topic of masked face recognition would also benefit heavily from larger and improved databases, with real masks of diverse types worn by larger sets of individuals.",
      "page_start": 193,
      "page_end": 194
    },
    {
      "section_name": "Chapter 11",
      "text": "Interpretability for Face Biometrics",
      "page_start": 195,
      "page_end": 195
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted in collaboration with Ana F. Sequeira, Wilson Silva, and Tiago Gonçalves, under the supervision of Jaime S. Cardoso. The author of this thesis contributed to this work on the formulation and implementation of the face presentation attack detection framework, the preparation and conduction of experiments, the discussion of the results, and the writing of the scientific publications. The results of this work have been disseminated as a journal article, an article in international conference proceedings, and an abstract in national conference proceedings:\n\n• A. F. Sequeira, T. Gonçalves, W. Silva, J. R. Pinto, and J. S.",
      "page_start": 195,
      "page_end": 335
    },
    {
      "section_name": "Context And Motivation",
      "text": "Like plenty of other artificial intelligence fields, biometrics has been witnessing a steadily increasing dominance of deep learning-based approaches. This is largely due to the availability of unprecedentedly large datasets and major computational gains offered by powerful graphics processing units (GPU)  [212; 242] . As discussed in Chapter 9, the largest downside of such a trend is the lack of transparency of the resulting algorithms, which has been increasingly criticised by the research community  [107; 174; 372] .\n\nBeyond simple decision accuracy-based metrics, there are several other aspects of a model that may be useful (and even to understand during its development and evaluation to avoid undesirable future consequences. One great example of this was described by Lapuschkin et al.  [241] , who observed that for the detection of the class \"horse\" by a deep neural network, the model assigned relevance to the bottom left corner of the images. A careful inspection revealed the presence of a copyright tag in that location, meaning the model was relying on this tag for the decision instead of using meaningful image information.\n\nFor the example of a face presentation attack detection (PAD) model, it may be important for a model to verify certain properties that may not be immediately obvious. First, the model should look for the same information in a given sample whether or not that sample has been seen during training. Second, a presentation attack sample should be processed similarly by a model whether or not it was trained to detect that specific attack. Third, the behaviour of the model should be coherent (similar) for different samples with the same predicted label. At last, the model's choice of information within a sample should be meaningful (as in, a human would likely look to the same regions to provide the same decision).\n\nThese ideal behaviours may not be entirely objective and consensual. However, they are aspects of deep learning models that cannot be measured by traditional metrics. The ability to peek deeper into the inner workings of biometric models is the true advantage behind integrating interpretability in their development and evaluation.\n\nThe exploratory study presented in this chapter focused on the face PAD task to illustrate how interpretability could be integrated with biometrics to reach the aforementioned goal. As such, this work did not aim to push forward the state-of-the-art in face PAD, but to push forward the almost nonexistent field of the explainability analysis in biometrics. An end-to-end CNN was implemented to perform face PAD, and Grad-CAM was used to explain its decisions, delving deeper into the behaviour of the model and assessing whether or not it verifies the desirable properties discussed above.",
      "page_start": 336,
      "page_end": 336
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 336,
      "page_end": 336
    },
    {
      "section_name": "Implemented Face Pad Network",
      "text": "A PAD method receives a biometric trait measurement as input and returns a prediction of whether that measurement belongs to a live individual (referred to as a bona fide sample) or a spoof attempt to intrude the system (in this case referred to as a presentation attack).\n\nIn this work, the model used for PAD is an end-to-end convolutional neural network (CNN).\n\nAs an end-to-end CNN, the model was granted the flexibility to freely learn the most appropriate features for the task at hand. This provides the most interesting context for interpretability studies focused on gaining insight into the inner workings of a classifier. A relatively simple architecture was chosen (see Fig. dense layers are composed of 100, 100, and 2 neurons, respectively. All convolutional and fullyconnected layers are followed by rectified linear unit (ReLU) activations, except for the last dense layer, which is followed by softmax activation.",
      "page_start": 196,
      "page_end": 196
    },
    {
      "section_name": "Interpretability Method",
      "text": "The Gradient-weighted Class Activation Mapping (Grad-CAM)  [383]  method is inspired by the Class Activation Mapping (CAM)  [492] . CAM was introduced for the identification of discriminative regions in CNNs without fully-connected layers and restricted to the architectures that perform global average pooling over convolutional maps immediately before prediction. Grad-CAM is a generalisation of CAM in the sense that it was designed to be used with any type of CNN architecture.\n\nGrad-CAM consists of the combination of feature maps using the gradient signal. The gradient information flows to the last convolutional layer of the model, thus assigning different importance values to each neuron according to a particular decision of interest. This interpretability tool enables the generation of explanations for any layer of the network. Additionally, it is possible to obtain explanations per class, allowing the analysis of the model predictions at a class-level.",
      "page_start": 197,
      "page_end": 197
    },
    {
      "section_name": "Experimental Setup 11.3.1 Data",
      "text": "This work used bona fide and presentation attack images extracted from the ROSE-Youtu Face Liveness Detection Dataset  [249] . This dataset is composed of 3497 videos of twenty subjects, including attack videos of seven different PAI species (see Table  11 .1). From each video, frames were extracted every five seconds and faces were detected on each frame using an MTCNN  [482] .\n\nFace regions were cropped, resized to 224 × 224, and normalised to [0, 1]. The samples from subjects {2, 3, 4, 5, 6} were reserved for testing, while the data from the remaining fifteen subjects were used for training and validation.",
      "page_start": 198,
      "page_end": 198
    },
    {
      "section_name": "Implementation Details",
      "text": "The PAD end-to-end model was trained on the extracted ROSE Youtu data using the Adam optimiser, with an initial learning rate of 10 -4 for a maximum of 150 epochs with batch size 8. Early stopping was used, monitoring the validation loss, with patience of 20 epochs. For regularisation, dropout (0.5) was used between each pair of consecutive dense layers. Horizontal flips, rotations with a range of 20 degrees, and width and height shifts with a range of 0.2 were used for data augmentation.\n\nExplainability experiments were performed using the Grad-CAM  [383]  implementation provided by the Keras Visualisation Toolkit  [227]  for Python, which is a library that enables visualising and debugging a trained Keras model. It supports the visualisation of class activation maps, saliency maps, and activation maximisation. In the obtained maps, each pixel is assigned a relevance value that corresponds to a specific colour from blue (less relevant to the decision) to yellow (more relevant).",
      "page_start": 198,
      "page_end": 198
    },
    {
      "section_name": "Experimental Scenarios And Evaluation",
      "text": "The explored scenarios follow those discussed in Chapter 9:\n\n• In one-attack, the PAD model is trained and tested with bona fide samples and only one type of attack/PAD species (PAISp). Therefore, the only type of attack shown to the network during the test phase was already seen in the training step. The expression One-Attack#i, used throughout the remainder of this chapter, denotes the respective model was trained and tested with bona fide samples and presentation attack samples of type i; The presentation attack samples belong to a specific type of attack. Considering the one-attack evaluation scenario, an attack sample of type #i can only be tested for the respective One-Attack#i scenario. It is not meaningful to test this sample with the models of One-Attack# j (with j = i)\n\nbecause then this would be an Unseen-Attack#i scenario.\n\nAs such, let I = {I 1 ,...,I n } and E xi = {E xi 1 ,...,E xi n } be a set of images and the respective set of explanations. For each image I k (for k = 1, ..., n) there is a corresponding explanation E xi k . Note that x = o or x = u whether the explanation refers to a classification result within the one-attack or unseen-attack scenario, respectively, and i = 1, ..., 7 is the type of attack that defines the model used for testing. Thus, each explanation is obtained with a specific model determined by the evaluation framework and the attack used in testing.",
      "page_start": 198,
      "page_end": 199
    },
    {
      "section_name": "Semantic Representation Of Explanations",
      "text": "One of the objectives of this work was to measure the variability of the explanations for both bona fide and presentation attack samples in the two different evaluation scenarios. To do this, one needs a suitable representation of the produced explanations, so that it becomes possible to quantify how much two explanations differ from each other. In this work, this comparison is performed in a semantic context. An illustrative example of how the approach used to perform a quantitative comparison between explanations is depicted in Fig.  11 .2.\n\nAs stated before, the explanations are generated using Grad-CAM, which highlights the image regions that maximise the predicted class. Since Grad-CAM typically produces blobby and coarse explanations that fail to preserve finer details, it was decided to multiply the saliency maps by the respective images. However, this space is still not ideal for image comparison, as it would be highly impacted by the spatial location of important features.\n\nTo overcome this issue, and inspired by what is being done in image retrieval  [173; 398]  and concept-based interpretability  [145]  to find similar images, the learned features computed by a pre-trained CNN were used as the space to measure the distance between two explanations. This follows the finding of Zhang et al.  [485]  that the Euclidean distance in the activation space of final layers is an effective similarity metric.\n\nSince this work focused on face images, FaceNet  [381] , a face-specific network pretrained on the VGGFace2 dataset  [58] , was used for the extraction of the deep features. This CNN was trained using a triplet loss, aiming to optimise the embedding space and ensure that the FaceNet could learn a function that correctly maps the face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.\n\nTo take advantage of these FaceNet properties and achieve meaningful mappings of the Grad-CAM explanations, we start by multiplying the original images by their respective Grad-CAM explanations. We then input the resulting image into FaceNet, and extract the features generated in the penultimate layer. All the Euclidean distances reported in this paper are computed in this semantic space.",
      "page_start": 199,
      "page_end": 200
    },
    {
      "section_name": "Comparison Of Explanations Across Different Scenarios",
      "text": "One could hypothesise that, for a robust PAD model, explanations for the same sample should be similar whether or not the model is trained to detect that specific attack. Verifying this property means we are in the presence of a PAD algorithm with thorough generalisation capabilities. In this section, we describe a study that addresses the assessment of this property.\n\nFor bona fide images, Fig.  11 .3 illustrates the process to compare the explanations, having an image I k , the evaluation framework x (either one-attack or unseen-Attack), and fixing as reference the explanation obtained by the model for the PAISp #i in both frameworks.\n\nAs for presentation attack images, Fig.  11 .4 illustrates the process to compare explanations, having a presentation attack image I k of type Attack #i. In this case, the comparison is made by fixing as reference the explanation of the result of the classification in the one-attack framework obtained by the model for the Attack #i. Thus, the evaluation framework is x = o. As mentioned before, this is done in order to have a more stable benchmark for comparison, since in the oneattack scenario the model is trained and tested with the same type of attack.\n\nSo, for each image I k (either bona fide or presentation attack), evaluation framework x, and using the model regarding attack #i, a set of six total values {d x j k : j ∈ {1, ..., 7} \\ i} is obtained. It results of the comparison between the explanation E xi k (always E oi k in the presentation attack case) and the explanation E x j k (for j ∈ {1, ..., 7} \\ i). The distance measurements between explanations {d x j k } provide a quantitative measure of the variability of the explanations produced by the different models when processing the same image.\n\nAveraging these values will provide the dxi k values for comparison, given by:\n\nFor sake of clarity, Table  11 .2 presents these distance values and their correspondence to the images and scenarios. The values dxi k obtained for each image I k are unique for a presentation attack sample and multiple (one for each i ∈ {1, ..., 7}) for a bona fide sample. They can be used for other quantitative interpretability endeavours, including to obtain image average and attack average distances:\n\n• The Image Average (Iµ) provides a quantitative measure of the variability, across all models, of the explanations produced by each model under the evaluation framework defined by xi (for x = o or x = u and i = 1, ..., 7) regarding one image I k . The Iµ for the bona fide is given for a bona fide sample:\n\nfor an attack sample (type #i):\n\n• The Attack Average (Aµ) provides a quantitative measure of the variability, across all samples, of the explanations produced by the model under one evaluation scenario defined by xi (for x = o or x = u and i = 1, ..., 7). Consider the values dxi k as defined in Table  11 .2 and being n ad m the number of bona fide and attack samples, respectively. The Aµ for the bona fide is given by Eq. (11.4) and for the presentation attacks is given by Eq.  (11.5) .\n\nfor a bona fide sample:\n\nfor an attack sample (type #i):",
      "page_start": 200,
      "page_end": 201
    },
    {
      "section_name": "Interclass Comparison In The Unseen-Attack Scenario",
      "text": "This study investigates the interclass comparison between explanations obtained using the models in the unseen-attack framework. In other words, it investigated the variability of the explanations between bona fide and presentation attack samples. To achieve the desired goal, the explanations obtained from the classification of each image, with the different models trained in the Unseen-Attack scenario, are compared in a pairwise manner. This comparison is performed for all images within each class.\n\nBy using the unseen-attack models it is possible to test the robustness of the models to the variability in the attacks present in the training and testing steps. Recall that a model resulting from unseen-attack#i is trained with attacks j for j ∈ {1, ..., 7} \\ i.\n\nFor a bona fide sample I k , the process to obtain the comparison of all explanations is illustrated in Fig.  11 .5. It shows one example of how to obtain the pairwise distances D k given by: D k = {d jh k : Eq. (\n\nfor each bona fide image I k , k = 1, ..., n.\n\nAs for a presentation attack image I k (of type #i), Fig.  11 .6 shows the process to obtain the comparison between the explanations obtained for the models Unseen-Attack# j for j ∈ {1, ..., 7} \\ i. The figure shows one example, regarding image I k of type Attack#i, on how to obtain the\n\n, with j, h ∈ {1, ..., 7} \\ i} and j = h}. The values in D k are averaged and dk is obtained for image I k . A global value is obtained averaging all these values, dPA , as given by Eq. (11.7\n\nfor each presentation attack image I k , k = 1, ..., m, of type #i.",
      "page_start": 202,
      "page_end": 203
    },
    {
      "section_name": "Intraclass Comparison Across Different Samples",
      "text": "One other property that a robust PAD solution should verify is that explanations should be similar for different samples with the same label. In other words, the explanations should reveal that the model looks for the same regions of the face even when the images are very different, since the ground-truth label is the same. This means the algorithm is coherent in its decisions and has effectively pinpointed the features that allow for accurate and stable detection of presentation attacks.\n\nIt is important to investigate the coherence of explanations for the bona fide samples, in order to understand if the considered PAD solution knows indeed what is a \"real face\". However, it is also important to understand how much the explanations vary for presentation attack samples, so we can assess if our algorithm is indeed capable of generalising well to the multiple possible attack species.\n\nThis analysis can be performed by measuring how much the explanations for the models' decisions are affected by variations in the types of presentation attacks known in the learning phase. In this work, this comparison is done by comparing features extracted from the explanations (using the FaceNet model described above) and not in a pixel-to-pixel manner, making it possible to compare the explanations obtained for different samples.",
      "page_start": 203,
      "page_end": 204
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 204,
      "page_end": 204
    },
    {
      "section_name": "Performance Of The Face Pad Algorithm",
      "text": "The main focus of this work was interpreting the decisions of a face PAD model to illustrate and motivate the application of interpretability to biometrics. Achieving improved face PAD results versus the state-of-the-art was not a priority. Nevertheless, one should not aim to interpret a model that lacks PAD abilities by design, as this will impair and bias the drawn conclusions. As such, the performance results of the implemented model in the one-attack and unseen-attack scenarios are presented in Table  11 .3.\n\nThe results were generally inferior in the unseen-attack scenario when compared to the oneattack scenario, as expected given the considerably higher difficulty of the former scenario and the insight found in face PAD literature  [384] . One reason for such results may be the large variability between the different PAI species found in the ROSE Youtu dataset. Hence, to obtain a truly robust PAD algorithm, the test set should always be composed of PAI species which have not been seen by the algorithm during training  [132] .\n\nAttack #7, an upper face mask with eyes cropped out, is a good example of the implemented model's difficulty to generalise. Although the results are acceptable in the one-attack scenario, the error rates are considerably higher in the unseen-attack scenario. Some exceptions can also be found, such as Attack #1 (full face printed photo), for which both EER and APCER are actually lower in the unseen-attack scenario vs. one-attack. For this specific PAI species, the model is probably learning most of the needed features from various other paper-based attack types in the unseen-attack scenario, taking advantage of the greater availability of data despite the absence of Attack #1 samples in the training.   The Iµ values also show higher variability in one-attack, according to the σ (Iµ) results. This suggests that the models in the unseen-attack scenario are better able to generalise in the recognition of bona fide samples, since they see a wider variety of attacks during training. For the PA samples, the variability results also suggest the models are more robust when a wider diversity of attacks is available during training. show an example of a PA sample presenting a higher µ(Iµ) in the one-attack scenario when compared to unseen-attack. These results further support the idea that a model trained with more than one PAI species is able to learn better patterns and become a more robust model with better generalisation capabilities.",
      "page_start": 204,
      "page_end": 205
    },
    {
      "section_name": "Attack Average (Aµ)",
      "text": "The mean and standard deviation of the attack average results (respectively, µ(Aµ) and σ (Aµ))\n\nacross the one-attack and unseen-attack scenarios for bona fide and presentation attack samples are presented in Fig.  11 .10.\n\nSimilar to the Iµ results presented before, the Aµ results show higher variability in the oneattack scenario. This suggests once again that a training setup that integrates more than one PAI species may be more successful at promoting the learning of more coherent features for the bona fide class. The same is also verified for presentation attack samples, as the mean distance in the unseen-attack scenario is inferior to that in one-attack.\n\nSince each PAI species contains intrinsic specificities, one could hypothesise that grouping multiple of them into a single class \"presentation attack\" would confuse the model and result in It is interesting to observe the variability for presentation attack samples across the different attacks in the unseen-attack scenario: although, in general, the values are inferior to those in the one-attack scenario, some specific attacks present much lower results than others. This may denote that differences (and similarities) between PAI species seen in training and evaluation lead to models that are much more sensitive to unseen PAI species in the testing phase.\n\nIn particular, in the presentation attack graphs in Fig.  11 .10, Unseen-Attack#5, consisting of paper masks with eyes and mouth cut off, presents the lowest µ(Aµ). This may result from the fact that, in this scenario, the model has seen multiple print-based PAI species (like complete photos, complete paper masks, and half paper masks) which help the model prepare for PAI species #5.\n\nThe Unseen-Attack#7 (consisting of upper-half face paper masks) presents a higher result since these samples combine skin and paper in the facial area and may be more difficult to learn from the PAI species seen during training. Nevertheless, despite the observed variability, in both types of samples there are certain regions of the images that are consistently used by all models to make their decisions. This may denote that, despite the variety of training conditions and the resulting noise found in the explanations, the model is generally able to pinpoint some regions of the face that correspond to the real underlying label information. This idea is verified when the pairwise distance is above the median values (see Naturally, this rationale based on subjective visual evaluations is limited, but this is a result of the current unavailability of a ground-truth for what is a good or meaningful explanation. These are still muddy unexplored grounds that require further research into the combination of interpretability methods and human expert knowledge in the field of biometrics. One can observe that the one-attack scenario (both for bona fide and presentation attack samples) leads to overall higher variability in explanations. This confirms the idea, brought up previously in this section, that a greater variety of PAI species in the training phase will result in models that are more robust and consistent in the information they use for their decisions.",
      "page_start": 205,
      "page_end": 211
    },
    {
      "section_name": "Intraclass Comparison Across Different Samples",
      "text": "",
      "page_start": 211,
      "page_end": 211
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This study consisted of an analysis of the explanations produced for a face PAD model explored in different scenarios and settings. Both the standard one-attack and the more challenging unseenattack scenarios were considered in this exploratory study. Additionally, intraclass and interclass experiments were also conducted to evaluate certain desirable properties of a robust and effective PAD solution.\n\nFocusing on the intraclass comparison of explanations' variability, it was possible to verify that the one-attack framework led to an increased mean distance value for both bona fide and presentation attack samples. This denotes that the presence of more attacks during training has a positive effect on the generalisation capabilities of the models, despite one-attack performance metrics being generally better than those in unseen-attack.\n\nAs for the interclass comparison of explanations, it was found that both classes exhibit similar levels of variability. Further analysis of bona fide and presentation attack samples has shown that, despite the wide variability in explanations, the models were generally able to pinpoint important regions of the face images that correspond to the true underlying labels.\n\nOverall, this exploratory study illustrates the deeper level of insight that can be obtained when biometric studies are combined with interpretability. If one is to overcome the elusive nature of It is clear that there is still much to do. Regardless of the biometric task at hand, it may be very difficult to find a consensual definition of a \"good\" explanation, or what behaviours an ideal model should exhibit. According to Phillips and Przybocki  [335] , an explanation must describe how the system came to its conclusion and an \"accurate\" explanation (whatever that may mean)\n\ndoes not imply that a system provided the correct answer.\n\nAs such, the major open challenge is for the biometrics community to evolve from established evaluation metrics based on decision accuracy to novel explanation-based performance metrics.\n\nThis work took one step forward in this direction. However, improving objectivity by combining subjective but knowledgeable opinions from several experts is essential to consolidate interpretability and thus enable more meaningful performance analysis on biometrics. Additionally, further efforts should be devoted to investigating the integration of the explanations on the training itself as a regularisation method to guide models through the learning of more meaningful features.",
      "page_start": 211,
      "page_end": 212
    },
    {
      "section_name": "Part Iv",
      "text": "",
      "page_start": 212,
      "page_end": 212
    },
    {
      "section_name": "Wellbeing Monitoring",
      "text": "Chapter 12",
      "page_start": 215,
      "page_end": 215
    },
    {
      "section_name": "Emotion Valence Classification In The Wild",
      "text": "",
      "page_start": 215,
      "page_end": 215
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted within the Easy Ride project in collaboration with Tiago Gonçalves, Carolina Pinto, and Luís Sanhudo, under the supervision of Jaime S. Cardoso, Pedro Carvalho, Joaquim Fonseca, and Filipe Gonçalves. The author of this thesis contributed to this work on the conceptualisation and implementation of the video module, the evaluation of the unimodal and multimodal algorithms, the preparation of submissions to the EmotiW 2020 AV Group sub-challenge, and the preparation of the scientific publication.\n\nThe results of this work were disseminated in the form of an article in international conference proceedings: • J. R. Pinto, T. Gonçalves, C. Pinto, L. Sanhudo, J. Fonseca, F. Gonçalves, P. Carvalho, and J. S. Cardoso, \"Audiovisual Classification of Group Emotion Valence Using Activity Recognition Networks,\" in Fourth IEEE International Conference on Image Processing, Applications and Systems (IPAS 2020), Dec. 2020.  [346]  This work was awarded the Best Session Paper Award at the aforementioned international conference.",
      "page_start": 215,
      "page_end": 215
    },
    {
      "section_name": "Context And Motivation",
      "text": "Emotion recognition is a fast-growing research topic, due to its potential for enhanced humancomputer interfaces and automatic services that immediately respond to the emotions of the user or client  [131] . Horror videogames that adapt the gameplay and sound effects based on the player's fear, as well as autonomous vehicles that adapt the travel experience based on the occupants' emotions, are only two of the endless innovations attainable through emotion recognition  [294] .\n\nState-of-the-art methods for emotion recognition are mainly based on facial expressions, and important hurdles have been overcome in this field  [131; 294] . Group-level emotion is a fairly uncharted research topic that extends the analysis to the emotional state displayed by a group of people as a whole  [425] . While there are several challenges in individual emotion recognition, approaches for group emotion recognition also need to deal with the variety of emotions, their valence, and arousal levels, that can differ among members of the same group. This topic was the focus of the EmotiW 2020  [101]  sub-challenge that motivated this work. The scarce data and the difficulty in obtaining annotations is the reason why few have addressed this topic  [159; 415; 456] , and why current approaches still offer low accuracy levels.\n\nThe task of group emotion recognition shares some similarities with the recognition of human activity based on the video. Unlike the former, the latter boasts several large and thoroughly labelled datasets, such as the Kinetics  [65]  or the ActivityNet  [169] , even when restricting to data focused on groups rather than on individuals. These larger sets of available data have allowed for the development of very robust and high-performing algorithms, such as the I3D  [65] , the SlowFast networks  [127] , or the stagNet  [354] .\n\nWhile methods based on visual information compose most of the literature, some works discuss the advantages of including additional sources of information, especially audio  [85; 214; 258; 453] . Specifically, it has been shown that using audio complements some of the flaws of video-based recognition  [214] , despite offering subpar accuracy results when in a unimodal recognition system. These results have confirmed the advantages of combining audio information with a strong method for video-based recognition. This work explores the novel application of inflated convolutional neural networks (CNN) to classify emotion valence at the group level in videos. The network uses weights pretrained for activity recognition, to take advantage of the greater availability of data to boost performance on our target task. We also study the use of audio for improved performance, through scorelevel fusion, with a Bi-LSTM network receiving spectral features. Throughout the experiments, we assess the performance of the proposed method for multimodal and unimodal classification, analyse its behaviour in different scenarios, and compare it directly with the EmotiW 2020 subchallenge official baseline.",
      "page_start": 215,
      "page_end": 216
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 216,
      "page_end": 216
    },
    {
      "section_name": "General Overview",
      "text": "The proposed algorithm is composed of three modules: a video-based emotion recognition model, an audio-based emotion recognition model, and a multimodal fusion module (see Fig.  12 .1). Video and audio-based emotion recognition modules are trained independently, while the fusion module, based on a multiclass SVM receives the softmax scores provided by the other two. Thus, the proposed method consists of a pipeline that relies on late audio-video fusion, at the score level, using a multi-class SVM emotion recognition classifier.",
      "page_start": 216,
      "page_end": 216
    },
    {
      "section_name": "Video-Based Emotion Recognition",
      "text": "The video-based emotion recognition module is based on an inflated bidimensional (2D) convolutional neural network (CNN), similar to I3D  [65] , the state-of-the-art in activity recognition. The model is an end-to-end network: it receives frames extracted from a video, ordered and concatenated over a time dimension, and returns class probabilities for that video. The architecture of the network follows the structure of a ResNet-50 (see Fig.  12 .2), proposed by He et al.  [167] , whose name stands for residual networks. The shortcut connections that perform identity mapping on each residual learning block enable the stable training of models with more convolutional layers, resulting in deeper representations of the input data.\n\nThe inflated ResNet-50 consists of a bidimensional ResNet-50 model where the convolutional filters and layers have been converted into 3D. This allows them to process several frames simultaneously as a single input. Downsampling operation before the first block of each type enables learning multi-resolution features. This model has been pretrained 1  to discriminate between 339 activity classes on the Multi-Moments In Time database  [300] . To offer probability outputs for each of the three group-level emotion valence classes, the last fully-connected layer of this network is replaced by a three-neuron fully-connected layer, followed by softmax activation, trained on the EmotiW 2020 sub-challenge train dataset.",
      "page_start": 216,
      "page_end": 216
    },
    {
      "section_name": "Audio-Based Emotion Recognition",
      "text": "The audio-based recognition module (see Fig.  12 .1) is composed of two main processes: feature extraction on sliding windows, and a Bi-LSTM recognition model. Audio features were extracted using pyAudioAnalysis  2  which contains an off-the-shelf feature set with 34 available features, including signal zero-crossing rate, signal energy, entropy of energy, spectral centroid, spectral spread, spectral entropy, spectral flux, spectral roll-off, mel-frequency cepstral coefficients (MFCC), chroma vector, and chroma deviation. All these features are extracted over sliding windows of 25 milliseconds with a time step of 10 milliseconds.\n\nThe features are received by a Bi-LSTM model with local attention that returns the class probabilities for the respective audio (see Fig.  12 .3), adapted from  [296] . Its weighted-pooling strategy enables the focus on the specific sound parts which contain strong emotional characteristics, controlled by an attention function trained simultaneously with the Bi-LSTM model.  12.3 Experimental Setup",
      "page_start": 217,
      "page_end": 217
    },
    {
      "section_name": "Data",
      "text": "All the experiments were conducted on an adapted version of the \"Video-level Group AFfect\" (VGAF) dataset  [388]  for the EmotiW 2020 AV Group-level sub-challenge  [101] . The VGAF is a video-based database that contains labels for emotion and cohesion. The data was collected from the YouTube platform and consists of videos under the creative commons license (CC0) and present keywords that correspond to the range of emotions and cohesion.\n\nSince the number of individuals per video is variable, and the groups on each video can also present a varying number of persons over time, the videos have been divided so that each video clip has always the same number of persons per frame. Each VGAF clip was manually labelled by different annotators for emotion and cohesion and every annotator was informed of the basic concepts of emotion and cohesion. Only videos with mutual consensus were kept in the final database. The labels for group emotion are related to emotion valence (i.e., positive, neutral, and negative) whereas the group cohesion labels are in the range [0 -3], being 0 the state of very low cohesion (dominance over the group members) and 3 the state of very high cohesion.\n\nFor the EmotiW 2020 AV Group-Level Emotion sub-challenge, the task was the classification of group emotion. The VGAF dataset videos were divided into five-second videos: 2661 for the train, 766 for the validation, and 756 for the test. Each video (except those in the test set) is accompanied by a discrete group emotion valence ground-truth label. In the training dataset, there are 802 positive videos, 923 neutral videos, and 936 negative videos. In the validation set, there are 302 positive videos, 280 neutral videos, and 184 negative videos.",
      "page_start": 219,
      "page_end": 220
    },
    {
      "section_name": "Baseline Algorithm",
      "text": "The baseline algorithm is the audiovisual group-level emotion recognition sub-challenge baseline  [388]  of the EmotiW 2020 Grand Challenge. This method is composed of two streams, for audio and video data processing, fused at the feature level. The video stream is a pretrained Inception V3 network that separately processes frames extracted from a video. The extracted features are combined using a long short-term memory (LSTM) network. The audio stream is composed of a fully-connected network that receives OpenSMILE  [117]  features extracted from the audio. The outputs from the video and audio streams are concatenated and used by a fully-connected layer to offer two outputs: the probabilities for the three emotion valence classes and an emotion cohesion value on the [0 -3] range.",
      "page_start": 220,
      "page_end": 220
    },
    {
      "section_name": "Preprocessing",
      "text": "The videos on the EmotiW 2020 AV Group-level Emotion sub-challenge were subject to preprocessing before being used by the proposed method. For each five-second video, ten frames were extracted, thus resulting in 2 frames per second. This is an adaptation from the original model pretrained on the Multi-Moments In Time database (MMIT), which worked at 5 frames per second. We found that reducing the frame rate did not harm performance and sped up the recognition process. Before being used on the audio-based recognition module, the audio was extracted from each file by converting them (originally in the MP4 format) to audio files (in the WAV format).\n\nRegarding audio, we noticed that after feature extraction some of the generated features could assume non-number values. For training purposes, we removed these samples and trained only with valid ones. For inference during validation/test, we replaced non-number values with zero.",
      "page_start": 220,
      "page_end": 220
    },
    {
      "section_name": "Training",
      "text": "The audio-based recognition module was trained from scratch  3  . The weights were randomly initialised, and the model was trained over a maximum of 200 epochs, with a batch size of 128, categorical-cross-entropy as the loss function, and using the Adam optimiser with an initial learning rate of 10 -2 . To prevent overfitting, we used dropout and early-stopping with a patience value of 15 epochs.\n\nThe video-based recognition module, pretrained on the MMIT database, was adapted to output probabilities for each of the three valence classes in group emotion recognition. This was achieved by replacing the last fully-connected layer with a new one, with three neurons.\n\nSince this layer needs to be trained, all weights of the network have been frozen (except those of this layer). The network was briefly fine-tuned until convergence over a maximum of 250 epochs, with batch size 32, using the Adam optimiser with an initial learning rate of 10 -5 .\n\nWhen training the audio-based module and the video-based module the hyperparameters have been selected empirically, to maximise performance in the validation set. The hyperparameters for the fusion module (e.g., the regularisation parameter \"C\", the kernel, or the polynomial degree of the kernel) were found through a grid search. Once the optimal hyperparameters were found, the train and validation set were combined to make a \"full train\" set, and thus take full advantage of all available labelled data for better performance in the test set.",
      "page_start": 220,
      "page_end": 221
    },
    {
      "section_name": "Experiments",
      "text": "In this work, the aim is not only to assess the proposed method's performance for group-level emotion recognition but also to examine its behaviour in several conditions.\n\nWe use the accuracy metric and confusion matrices to examine the overall performance of the method and also analyse its class-wise accuracy. The performances of the multimodal method and its audio and video-based modules, separately, are evaluated in both the validation set (with available ground-truth labels) and the test set (accuracy values delivered by the EmotiW 2020 sub-challenge organisation upon request). The performance is compared with the official subchallenge baseline, following the results reported in  [388] .\n\nThe performance is also evaluated according to the number of people in the video. Since the number of people in each video is not included, we use the MTCNN method  [482]  on each frame of each video, and infer the group size based on the average number of detected faces: a group with less than five detected faces are considered small (total of 502 videos on the validation set), otherwise, it is considered a large group (264 videos on the validation set). With this, we aim to evaluate the difficulties associated with recognising emotion in large groups, where cohesion is likely to be generally lower.",
      "page_start": 221,
      "page_end": 221
    },
    {
      "section_name": "Results And Discussion",
      "text": "The performance results of the proposed method, and the comparison with the official subchallenge baseline, on the validation and test sets, is presented, respectively, in Table  12 .1 and Table  12 .2.\n\nOn the validation set, the performance offered by the proposed multimodal method is superior to the baseline. The accuracy attained by the video-only approach is close to that offered by the multimodal method, over 62%. This is evidence of the advantages of using pretrained networks (in this case, transferred from the task of human activity recognition). The audio-only approach offers considerably lower performance (47%) than the audio-only baseline (50%), which indicates the use of OpenSMILE features and fully-connected networks may be better fitted for group emotion recognition based on audio.\n\nFrom the validation to the test set (Table  12 .2), the official video-only baseline suffers a sharp performance decay (from 52% to 42% accuracy), which is also felt with the proposed video-only approach (albeit not as dramatic, from 62% to 59% accuracy). Fusing with audio on a multimodal approach reduced that decrease in the case of the official baseline (from 50% to 48% accuracy), and even reversed it in the case of the proposed method (from 62% to 66% accuracy). This confirms the idea present in the literature that, while audio alone is not suitable for recognition, it offers additional information that is essential for the robustness and accuracy of the method. Analysing the class-wise accuracies and the confusion matrices (Table  12 .3, Table  12 .4, and Table  12 .5), one can notice that video is, overall, the best modality to recognise emotions. The advantages of using video rely mainly on the \"extreme\" classes, positive and negative, which denote visual information is more advantageous to recognise strong group emotions. The proposed audio-only approach attains very poor accuracy in the positive class.\n\nSince the positive class is the minority class in the training dataset, the results of the audio-only approach may partially be explained by this slight class imbalance. However, as mentioned before, the video-only approach does not verify this, which is fortunate when combining both approaches into the multimodal proposed method. Using both modalities slightly decreases the accuracy of positive and negative videos, when compared with the video-only approach, but takes advantage of the audio information to considerably improve accuracy on neutral videos and achieve overall better performance.\n\nAt last, the results of the group size study are presented in Table  12 .6. In both audio and video-only approaches, as well as the multimodal method, the recognition performance is higher in smaller groups. The performance values should serve as a rough reference, since the process of face detection may present errors, and the number of faces may not accurately describe the number of people in the video's group (which may include occluded faces or people facing the opposite direction of the camera).\n\nNevertheless, the performance differences are considerable and show expected behaviour: it should be harder for larger groups to consistently show the same emotion than smaller groups.\n\nHence, emotion cohesion should be higher, on average, for smaller groups, and thus the certainty of the algorithms when recognising the emotion valence. This could perhaps be addressed using hierarchical methodologies (from individual-level to group-level) as used in current group activity recognition approaches (discussed in the related work section).\n\nThrough an analysis of some videos where the proposed model failed, a pattern emerged.\n\nWhile there are certain videos where the error was evident, there are several examples where it is very difficult to notice that an apparently neutral scene displays, in fact, a positive or negative group emotion. Some examples are shown in Fig.  12 .4. On the top left, is a short video of a calm conversation on a TV show that is labelled as positive. On the top right, is a negative emotion video that the proposed method classified as positive, since the video only covers the moment before the boy being bullied started crying. On the bottom left, a conference presentation that the method classified as neutral since the positive ground-truth emotion could only be verified by facial expressions. On the bottom right, a conversation is deemed neutral by the proposed method, where only a closer inspection of the audio shows that it is, in fact, part of a protest or a similar confrontation.\n\nAlthough the information about the underlying ground-truth labels is indeed present in these example videos, it is hidden in contextual clues, expressions in small faces, or the content of conversations. Exploring ways to integrate these aspects into the recognition of group-level emotion could be the way to avoid the most common mistakes of the proposed method, and ultimately achieve better overall performance and robustness.",
      "page_start": 221,
      "page_end": 224
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "In this work we proposed a novel method for the automatic recognition of group emotion that uses a late-fusion multimodal approach, combining scores from both video and audio-based emotion recognition models that are used to feed a multiclass SVM that returns a final class probability. This method showed significant improvement against the baseline, confirming that the use of acquired knowledge from activity recognition is useful for group-emotion recognition and that the joint utilisation of audio and video benefits the learning of the model.\n\nOn the other hand, taking into account the maximum accuracy value, we believe that there is still room for further improvements. Further efforts should be devoted to the study of the links between the tasks of activity recognition and emotion recognition, especially at the group level.\n\nApproaches for abnormal behaviour recognition through video anomaly detection, such as  [23] , could offer meaningful improvements in the automatic distinction between positive and negative emotions. Also, this method could benefit from OpenSMILE features in the audio-based emotion recognition module, the development of multimodal approaches that are based on early-fusion (e.g., input or intermediate-layer levels), and the design of a \"fully\" end-to-end network that receives both video and audio as input and learns the relevant features for the classification task (e.g., through regularisation methods such as loss functions with different terms and weights).",
      "page_start": 224,
      "page_end": 225
    },
    {
      "section_name": "Chapter 13",
      "text": "",
      "page_start": 225,
      "page_end": 225
    },
    {
      "section_name": "Activity And Violence Recognition In Shared Vehicles",
      "text": "",
      "page_start": 225,
      "page_end": 225
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted in collaboration with Carolina Pinto, Afonso Sousa, and Leonardo Capozzi, under the supervision of Jaime S. Cardoso and Pedro Carvalho. The author of this thesis contributed to this work on the reformulation and implementation of the audio module, the conceptualisation and implementation of the cascade strategy, the preparation of data and the experimental setup, the evaluation of the parallel and cascade algorithms, the discussion of the results, and the preparation of the scientific publication. The results of this work were disseminated in the form of an article in international conference proceedings: • J. R.",
      "page_start": 227,
      "page_end": 227
    },
    {
      "section_name": "Context And Motivation",
      "text": "Human action or activity recognition is a vibrant and challenging research topic. Being able to recognise actions automatically is game-changing and often crucial for several industries, including the scenario of shared autonomous vehicles. Without a driver responsible for the vehicle's and occupant's security and integrity, it falls upon automatic recognition systems to monitor passenger well-being and actions, and eventually recognise harmful behaviours or even violence  [23] . However, the wide range of possible actions that can be portrayed, the variability in the way different individuals portray the same actions, the heterogeneity of sensors and the type of information captured and the influence of external factors still pose significant hurdles to this task. Despite all the above-mentioned challenges, the topic of action recognition has thrived by following a very recognisable recipe for success. As in plenty of other pattern recognition tasks, the state-of-the-art gradually evolved towards larger and more sophisticated models based on deep learning methodologies  [65; 127; 354] . These have achieved increasingly higher accuracy thanks to a growing number of massive databases typically using public video data gathered through online sourcing, such as Kinetics  [65] , Multi-Moments in Time (MMIT)  [300] , or ActivityNet  [169] .\n\nThis also means most research in action recognition is based on visual information (images or video). This is the case of the I3D  [65] , the methodology currently deemed the state-of-the-art in this topic. In fact, I3D goes further beyond simple visual spatial information by adopting a two-stream approach, including optical flow for temporal action encoding. Other approaches have explored recurrent networks for the same purpose  [175; 226; 330] , but have seldom managed to reach the accuracy level offered by the I3D method.\n\nDespite the meaningful strides brought by such sophisticated methods and large databases, some limitations can be observed. On the one hand, the general nature of the data sourced to train and evaluate the state-of-the-art models lead to overly general results that may not be verified in more specific scenarios, such as in-vehicle passenger monitoring. On the other hand, hefty models based on visual information and optical flow (such as I3D) may offer very high accuracy, but their complexity does not allow for real-time applications in inexpensive limited hardware, such as embedded devices.\n\nThis work proposes a set of changes to the state-of-the-art I3D method to bring it closer to real applicability in edge computing scenarios: in this case, we focus on action recognition and violence detection in shared autonomous vehicles. First, inspired by  [346] , the current work discards the time-consuming optical flow component of I3D and introduces a lightweight model for action recognition with audio. Despite being less frequently used than video, audio is considered one of the most promising options for a multimodal system for action recognition  [85; 214; 258] . This way, we obtain a simpler methodology that can use both video and audio modalities for a greater variety of information. Then, as each modality is likely to contribute differently to the recognition of each action, we propose a cascade strategy based on confidence score thresholding. This strategy allows a simplification of the multimodal pipeline by using only one (primary) modality as often as possible; the two modalities are used together only when the primary one is not enough for sufficiently confident predictions. Hence, it is possible to attain significant time and computing energy savings without overlooking classification accuracy.\n\nThis chapter is organised as follows: beyond this introduction, a description of the proposed multimodal methodology and cascade strategy is presented in section 13.2; the experimental setup is detailed in section 13.3; section 13.4 presents and discusses the obtained results; and the conclusions drawn from this work are presented in section 13.5.",
      "page_start": 227,
      "page_end": 228
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 228,
      "page_end": 228
    },
    {
      "section_name": "Multimodal Pipeline",
      "text": "The baseline consists of a multimodal pipeline for activity recognition based on an audio-visual module previously proposed for group emotion recognition  [346] . The pipeline is composed of three submodules (as illustrated in Fig. the audio submodule, which processes sound data; and the fusion submodule, which combines individual decisions from the previous two submodules into joint multimodal classifications. The specific structures of each of these submodules are described below.",
      "page_start": 228,
      "page_end": 229
    },
    {
      "section_name": "Visual Submodule",
      "text": "As in  [346] , the visual submodule is based on an inflated ResNet-50  [167]  using pretrained weights for the Multi-Moments in Time (MMIT) activity recognition database  [300] . Using an inflated ResNet-50 ensures optimal performance by following the successful example of the state-of-theart I3D method  [65] . Using model weights pretrained on the large MMIT database allows us to transfer deeper and more general knowledge to our narrower task of activity recognition inside vehicles.\n\nThe inflated ResNet-50 model (see Fig.  13 .2) is composed of seventeen residual blocks, each including three 3D convolutional layers with 64 to 2048 filters, batch normalisation and ReLU activation. Downsampling at each block allows the model to capture important information at different levels of resolution. After an average pooling layer, the last fully-connected layer, followed by a softmax activation function, offers probability scores for each of the N considered activity labels.",
      "page_start": 230,
      "page_end": 230
    },
    {
      "section_name": "Audio Submodule",
      "text": "The audio submodule consists of a simple network based on a bi-directional long short-term memory (LSTM) model. These are known for their ability to encode temporal information, important for audio-related topics, and have been previously successful for tasks such as group emotion valence recognition  [346]  or speech-based sentiment analysis  [296] . Unlike the visual submodule, which largely follows the method proposed in  [346]  to approach the state-of-the-art performance of I3D, the audio submodule was reformulated. In the aforementioned work, the audio Bi-LSTM model received a set of cepstral, frequency, and energy handcrafted features extracted from each signal window. Moreover, it included multiple convolutional layers with 512 filters each and an attention mechanism after the LSTM layer. In this work, we design a streamlined and faster audio submodule.\n\nThe simplified and lighter Bi-LSTM model (see Fig.  13 .3), with less trainable parameters, receives a raw audio signal divided into 100 ms windows with 50 ms overlap, without any preceding process of feature extraction. Each window is processed by three convolutional layers (with 16, 32, and 64 1 × 5 filters, respectively, stride 1, and padding 2), each followed by ReLU activation and max-pooling (with pooling size 5). A Bi-LSTM layer receives features from the convolutional part for each window, and its output for the last window is sent to a fully-connected layer for classification (with N neurons, one for each activity class, followed by softmax activation).\n\nIn section 13.4, we analyse the advantages of using the proposed audio submodule vs. the one in  [346] .",
      "page_start": 229,
      "page_end": 230
    },
    {
      "section_name": "Fusion Submodule",
      "text": "The aforedescribed visual and audio submodules output their respective sets of class probability predictions for a given video. To combine the two separate sets of predictions for each task into a single audio-visual multimodal classification, the fusion submodule is used. The fusion submodule is composed of a simple support vector machine (SVM) classifier. This classifier receives the probability score sets from the two previous submodules concatenated as a single unidimensional feature vector. The SVM model is trained to use these probability sets to output a joint class prediction for the respective video.",
      "page_start": 230,
      "page_end": 231
    },
    {
      "section_name": "Cascade Strategy",
      "text": "In the multimodal pipeline described above, all submodules are used for each instance (video) that needs to be classified. This means that regardless of the difficulty of a given video or the activity portrayed, both visual and sound data are always processed, resulting in two sets of unimodal class predictions which are then combined into a set of multimodal class probabilities.\n\nGiven the considerable complexity of both the residual-network-based visual submodule and the Bi-LSTM-based audio submodule, this multimodal pipeline is arguably too heavy for the target application. This is especially true considering, as observed in  [346] , that different classes may benefit much more from one of the modalities and thus not need the other one. Hence, we design a cascade strategy to explore the possibility of using just one of the modalities and \"turning off\" the remaining two submodules as often as possible. This aimed to achieve improved processing times and energy usage, offering an alternative or complement to model compression strategies.\n\nIn the proposed cascade strategy, one of the data modalities (visual or audio) is selected as the \"primary\" modality and, as such, the corresponding submodule is always used to offer a starting prediction. The probability score offered for the predicted class is considered a \"confidence score\": a measure of how confident the primary submodule is in the prediction it provided. If the confidence score is above a specific confidence threshold T ∈ [0, 1], the remaining modules remain unused, and the primary submodule predictions are considered final. However, if the aforementioned condition is not verified, the secondary submodule is called to offer additional information for more confident predictions, which are then combined into a single multimodal prediction (just like the original multimodal pipeline).\n\nThe performance benefits of such a strategy are intimately related to the defined confidence threshold. If T is too high, most of the instances will use both data sources, thus retaining the accuracy offered by the parallel pipeline but reaping very few benefits related to complexity or processing time. Conversely, if T is too low, most instances will be classified using only the primary submodule, which may result in heavily impacted accuracy, despite the complexity benefits of the simplified pipeline. Section 13.4 includes thorough experimental results on the impact of the confidence score on the accuracy and processing requirements of the pipeline for activity classification.\n\n13.3 Experimental Setup",
      "page_start": 231,
      "page_end": 232
    },
    {
      "section_name": "Databases",
      "text": "For generic scenarios, this work used the Multi-Moments in Time (MMIT) database  [300] , made available by the creators upon request. The MMIT database includes a total of 1 035 862 videos, split between a training set (1 025 862 videos) and a validation set (10 000 videos). These correspond to a total of 339 classes, describing the main activity verified in each video. From those classes, only those related to the target scenario of in-vehicle passenger monitoring were included.\n\nThis resulted in a subset of twenty-one classes: fighting/attacking, punching, pushing, sitting, sleeping, coughing, singing, speaking, discussing, pulling, slapping, hugging, kissing, reading, telephoning, studying, socialising, resting, celebrating, laughing, and eating. Train and test divisions use the official predefined MMIT dataset splits.\n\nFor the in-vehicle scenario, a private dataset was used. The dataset includes a total of 490 videos of the back seat of a car occupied by one or two passengers (see example frames in Fig.  13 .4). Videos are acquired using a fish-eye camera to capture most of the interior of the car and microphones to acquire sound data. Each video includes annotations for forty-two action classes: \"entering\", \"leaving\", \"buckle on/off\", \"turning head\", \"lay down\", \"sleeping\", \"stretching\", \"changing seats\", \"changing clothes\", \"reading\", \"use mobile phone\", \"making a call\", \"posing\", \"waving hand\", \"drinking\", \"eating\", \"singing\", \"pick up item\", \"come closer\", \"handshaking\", \"talking\", \"dancing\", \"finger-pointing\", \"leaning forward\", \"tickling\", \"hugging\", \"kissing\", \"elbowing\", \"provocate\", \"pushing\", \"protecting oneself\", \"stealing\", \"screaming\", \"pulling\", \"arguing\", \"grabbing\", \"touching (sexual harassment)\", \"slapping\", \"punching\", \"strangling\", \"fighting\", and \"threatening with weapon\". Videos are randomly drawn into the train dataset (70%) or the test dataset (30%).",
      "page_start": 232,
      "page_end": 233
    },
    {
      "section_name": "Data Preprocessing",
      "text": "A total of 10 frames, evenly spaced, was extracted from each video in the MMIT selected data subset (each with about 5 sec). These frames were concatenated over a third dimension following their temporal order to serve as input to the visual submodule. Two seconds of audio were extracted from each video and normalised to 16 kHz sampling frequency to serve as input to the audio submodule.\n\nFor the in-vehicle dataset, each video can have multiple labels (the passengers portray different actions over the course of each acquisition). As such, each video period labelled as one of the 42 classes, is divided into two-second long individual samples. From each of these, 8 frames are",
      "page_start": 233,
      "page_end": 233
    },
    {
      "section_name": "Model Training",
      "text": "The inflated ResNet-50 model used on the visual submodule uses the official pretrained weights from the MMIT database. Given that it was pretrained on the same database used in the laboratory experiments, this work took full advantage of this by setting most of the parameters of the network as non-trainable. The only parameters that were trained are those of the fully-connected layers which correspond to the classification of the selected twenty-one categories. This layer was optimised for a maximum of 250 epochs according to categorical cross-entropy loss, with batch size 32, using the Adam optimiser with an initial learning rate of 10 -4 . For regularisation, dropout with a probability of 0.5 is used before the fully-connected layer. For the in-vehicle scenario, the training process is identical to the one described above. However, since the nature of the invehicle video data is substantially different from MMIT, the pretrained weights are also trained (not frozen) alongside the fully-connected layer for classification.\n\nFor both the generic and in-vehicle scenarios, the audio submodule was trained for a maximum of 200 epochs with batch size 64 and early-stopping patience of 25 epochs. The optimisation was performed using Adam with an initial learning rate of 10 -4 and cross-entropy loss.",
      "page_start": 234,
      "page_end": 234
    },
    {
      "section_name": "Results",
      "text": "After training the methodologies previously described, including the proposed cascade strategy, an overview of the obtained accuracy results is presented in Table  13 .",
      "page_start": 234,
      "page_end": 234
    },
    {
      "section_name": "Generic Scenarios",
      "text": "For the laboratory experiments using the selected data from the MMIT database, the full parallel multimodal pipeline explored in this work offered 55.12% accuracy. However, when considering the proposed cascade strategy based on confidence score thresholds, it was possible to achieve an improved accuracy score of 55.30% (see Fig.  13 .5). Beyond this relatively small accuracy improvement, the largest benefit of the proposed cascade algorithm is related to processing time.\n\nAs presented in Fig.  13 .6, the best accuracy of 55.30% is achieved with an audio-first cascade with a confidence score threshold T = 0.5. This means it is possible to avoid the visual and fusion submodules for approximately 51% of all instances without performance losses.\n\nAn analysis of size, number of parameters, and average run time per instance for each submodule (see Table  13 .2) shows that the visual model is the heftiest among the three submodules.\n\nHence, being able to bypass it on more than half of the instances translates into significant time savings: while the full multimodal pipeline takes, on average, 85.9 ms to predict an instance's  Considering the average run times presented in the previous subsection, this means the cascade is able to offer activity predictions in 28.4 ms, on average, while offering considerably higher accuracy than the full multimodal pipeline (which would take 85.9 ms).\n\nFor the three-class violence recognition task, the results follow the same trend, albeit with higher accuracy scores for all submodules and fusion strategies. The proposed cascade strategy with audio as the primary modality was able to attain 68.88% accuracy, considerably better than the 66.61% offered by the full multimodal pipeline. This accuracy corresponds to T = 0.8, which enabled avoiding the visual submodule for 35% of all instances (see Fig.  13 .9). While this value is lower than those reported for the previous experiments, it still translates into average time savings of 27.2 ms per instance (58.7 ms for the cascade vs. 85.9 ms for the full pipeline), accompanied by a considerable improvement in accuracy.",
      "page_start": 235,
      "page_end": 236
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This work explored a different strategy for the recognition of human activities, focusing on the scenario of autonomous shared vehicles. In addition to the inherent difficulties of automatically recognising human actions using audio-visual data, this specific scenario poses specific constraints regarding available hardware and energy consumption.\n\nInspired by state-of-the-art multimodal approaches, the main contributions are two-fold: a lighter-weight deep-learning based audio processing submodule; and a cascade processing pipeline. The proposed audio processing module demonstrated state-of-the-art performance while presenting lesser memory requirements and computational demands. With the submodules implemented, different configurations were tested for the cascade strategy to assess which one provides the best performance, taking into account two critical axes: accuracy and computational performance. Results show that by using audio as the first processing block, it was possible to obtain an accuracy score higher than the state-of-the-art, along with a significant reduction in processing/inference time.\n\nThe obtained results are interesting and reveal a high potential for further improvement. Modifications to the individual processing submodules could contribute to even higher accuracies while further reducing computational weight. The latter may benefit from a combination with model compression and acceleration techniques, such as quantisation, avoiding likely losses in accuracy due to compression.\n\nThe proposed strategy demonstrated benefits from cascading the processing modules. Other early modules could bring other benefits by filtering out incoming audio-visual data, without relevant content (e. g., without people present or without movement/sound).",
      "page_start": 237,
      "page_end": 238
    },
    {
      "section_name": "Part V Broader Topics On Biometrics And Pattern Recognition",
      "text": "Chapter 14\n\nLearning Template Security on End-to-End Biometric Models",
      "page_start": 239,
      "page_end": 239
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted entirely by the author of this thesis, under the supervision of Jaime S. Cardoso and Miguel V. Correia. The results of this work have been disseminated in the form of a journal article, an article in international conference proceedings, and an abstract in national conference proceedings:\n\n• J. R. Pinto, M. V. Correia, and J. S. Cardoso, \"Secure Triplet Loss: Achieving Cancelability and Non-Linkability in End-to-End Deep Biometrics,\" IEEE Transactions on Biometrics, Behavior, and Identity Science, 3",
      "page_start": 241,
      "page_end": 241
    },
    {
      "section_name": "Context And Motivation",
      "text": "It is easy to change our keys or passwords when a traditional authentication system is compromised, but it is very hard to change our compromised biometric characteristics. This is the reason why it is paramount that biometric templates are kept secure  [186; 304] . This is not easily achievable since, unlike password-based systems, biometric comparison is not binary and must also account for the natural intrasubject biometric variability  [200; 304] . While several methods have been proposed to protect biometric templates, most require specific feature extraction or additional processes based on salting, biohashing, or cryptographic protection  [200; 304] . Even those proposed for deep learning biometric methods  [329; 424]  are integrated into end-to-end models, thus creating hurdles that often limit the achievable performance.\n\nHence, this chapter presents the Secure Triplet Loss, a reformulation of the well-known triplet loss that enables training end-to-end deep learning models to obtain secure biometric templates.\n\nBinding keys with the input within the model and considering key divergences in the objective function enables learning template cancelability. A component based on Kullback-Leibler divergence or distance statistics measures and actively promotes unlinkability. Thus, the proposed method aims to allow taking full advantage of the capabilities of end-to-end deep networks while still ensuring the security of the stored biometric data.\n\nA detailed presentation of the Secure Triplet Loss can be found throughout the next sections.\n\nThis training methodology was thoroughly evaluated for the task of identity verification, considering the template security properties of cancelability, unlinkability, non-invertibility, and secrecy leakage. It was thoroughly evaluated for both ECG and face, to confirm its solidity and flexibility, using the off-the-person University of Toronto ECG database (UofTDB)  [445]  and the unconstrained YouTube Faces  [460]  database.\n\nMoreover, it was studied in two scenarios: (a) training a model \"from scratch\" (initialised with random parameters), or (b) adapting an existing end-to-end biometric model to make it secure (taking advantage of pretrained weights and fine-tuning with the proposed method). The Secure Triplet Loss was also compared with the original triplet loss and competitive state-of-theart approaches based on Bloom Filters  [149]  and Homomorphic Encryption  [109] . The code used in this work is available online 1 .",
      "page_start": 241,
      "page_end": 242
    },
    {
      "section_name": "Related Work",
      "text": "Beyond accounting for natural biometric characteristic variability, biometric data protection methods need to verify template cancelability, non-invertibility, and unlinkability. Cancelability (or revokability) means the templates can be easily and effectively rendered useless if they become compromised, generally through the change of a personal key that is bound with the template  [353; 427] . Non-invertibility requires the transformation from biometric samples to templates to be as close to irreversible as possible. Thus, if the template is compromised, the original biometric sample cannot be reliably recovered or approximated  [200; 304] . Finally, template unlinkability means it is difficult to assess if compromised templates from different biometric systems belong to the same identity  [149] .\n\nOne of the first template protection methods was the fuzzy commitment scheme proposed by Juels and Wattenberg  [208] , using cryptography and error-correcting codes for template cancelability. Later, Teoh et al.  [430]  proposed BioHashing, an adaptation of the hashing process commonly applied to passwords to deal with fingerprint variability. A similar approach has been proposed by Sutcu et al.  [418] .\n\nMore recently, Rathgeb et al.  [362, 363]  proposed the Bloom filter approach for alignmentfree template cancelability and irreversibility. This approach was later adapted by Gomez-Barrero 1 Secure Triplet Loss Github repository. Available on: https://github.com/jtrpinto/SecureTL. et al.  [149, 151]  to ensure template unlinkability, and by Drozdowski et al.  [108]  for higher computational efficiency. Raja et al.  [357]  proposed a highly efficient method using neighbourhoodpreserving manifolds and hashing for biometric template protection in smartphones.\n\nAmong cryptography-based methods, homomorphic encryption (HE) approaches are particularly promising as HE allows arithmetic operations on the encrypted domain  [45] . This allows the biometric comparison to be fully conducted on the encrypted domain, ensuring data security  [109] .\n\nFully HE approaches, that allow for unlimited operations in the encrypted domain, most notably include Gentry's  [142] , Brakerski's  [53] , and Fan-Vercauteren's  [118]  schemes. HE has been successfully applied for biometric template protection in face  [45; 109; 223] , signature  [148] , and even multibiometric recognition  [150] . However, with HE the protection of templates remains the responsibility of a separate process that should, ideally, be harmoniously integrated within the feature extraction algorithm.\n\nUsing deep learning, Pandey et al.  [329]  proposed a template protection scheme that receives features from a convolutional neural network (CNN), quantises them, and applies hashing to obtain exact comparison despite the variability. Later, Talreja et al.  [424]  used forward error control (FEC) decoding and hashing to protect biometric features extracted by deep neural networks.\n\nWhile these are applied to deep learning, they still require separate protection and comparison schemes. Hence, they are inadequate for recent state-of-the-art biometric recognition methods, which largely rely on end-to-end deep learning models for significantly improved performance.\n\nConsidering this, this work proposes the Secure Triplet Loss, a reformulation of the triplet loss  [72]  that promotes cancelability and unlinkability in end-to-end biometric models. More importantly, it aims to achieve this while avoiding additional protection processes and decreases in performance relative to the original triplet loss.",
      "page_start": 242,
      "page_end": 243
    },
    {
      "section_name": "The Secure Triplet Loss 14.3.1 Original Triplet Loss",
      "text": "The triplet loss  [72]  has been widely used in deep learning to train networks to accurately determine whether or not two samples belong to the same class  [73; 74; 338] . During training, such networks receive three inputs (a triplet), in parallel: one is the anchor (x A , the reference with identity i A ), the second is the positive sample (x P , with identity i P = i A ), and the third is the negative sample (x N , with identity i N = i A ). In biometrics, triplets are groups of three biometric samples (images or signals): the anchor and positive inputs correspond to the same individual, unlike the negative input.\n\nFor each input, the network will output a representation: e. g., for the anchor, y A = f (x A ). The three representations are then compared using a measure of distance or dissimilarity d(y 1 , y 2 ), and the network is optimised through the minimisation of the triplet loss function:\n\nFigure  14 .1: Comparison between the model training schemes of the original triplet loss and the proposed Secure Triplet Loss method  [345] .\n\nwhich leads representations of the same class to be more similar than those of different classes, minimising d(y A , y P ) and maximising d(y A , y N ). The loss also aims to enforce a minimum margin α > 0 between the two distances. This is a generally successful strategy when training neural networks for biometric verification (assessing if the identities of a biometric template and a biometric query match). However, it does not address the important issue of security in biometrics, especially the topics of cancelability and unlinkability.",
      "page_start": 244,
      "page_end": 244
    },
    {
      "section_name": "Learning Cancelability",
      "text": "In its initial formulation, proposed in  [345] , the Secure Triplet Loss modifies the original triplet loss to make the final sample representations cancelable (as illustrated in Fig.  14 .1). Besides the triplet inputs (x A , x P , and x N ), the network also receives two different keys (k 1 , k 2 ) that are bound with the inputs by the network itself.\n\nUnlike the original triplet loss, x P and x N are processed by the network twice. First, they are combined with k 1 and then with k 2 . The anchor x A is only bound with k 1 . Thus, five representations are obtained: The objective is to minimise d SP , when both the identities and the keys match, and maximise the remaining three distances (see Fig.  14 .2). Hence, the loss is computed through:\n\nwhere d n results from the combination of all three distances to be maximised. Here, we consider d n = min({d SN , d DP , d DN }), with the three distances to be maximised being considered equally relevant. This results in:\n\nAs with triplet loss, α enforces a margin between positive and negative distances. In this case, the loss involves four distances, since it also takes into account whether or not the keys match. By minimising the loss in Eq. (  14 .3), the network learns to deal with the intrasubject and intersubject variability of the biometric characteristic. More importantly, it learns to recognise when the keys do not match, even if the identity is the same. Hence, if the stored templates become compromised, they can easily be invalidated through a key change. However, as reported in  [345] , l ST L fails to promote unlinkability.",
      "page_start": 245,
      "page_end": 245
    },
    {
      "section_name": "Promoting Unlinkability",
      "text": "Unlinkability can be achieved by combining the original formulation of the Secure Triplet Loss, l ST L , with a component that quantifies linkability in the representations output by the network during training, l L . Thus, the proposed reformulation of the Secure Triplet Loss, as presented in  [347] , follows the equation: This should lead the model to offer embeddings that result in similar distance scores when the keys do not match, regardless of whether or not the identities match, thus avoiding template linkability.\n\nThroughout the remainder of this chapter, for brevity, the formulation of the Secure Triplet Loss with this statistics-based linkability module is designated as SecureTL w/SL.",
      "page_start": 245,
      "page_end": 246
    },
    {
      "section_name": "Experimental Setup",
      "text": "The proposed methodology for learning secure biometric models was explored for the two biometric characteristics addressed within this doctoral work: the ECG and the face. This section presents the details of the models, the data, and the conducted experiments.\n\nFor either characteristic, keys have been randomly generated for each triplet, consisting of unidimensional arrays with 100 binary values. Each key is processed after generation to verify unit l2 norm. For SecureTL w/KLD and SecureTL w/SL, the parameter γ that controls the balance between the Secure Triplet Loss and the linkability component was set to 0.9: this value has overall been able to offer good template unlinkability without considerably harming the validation performance and cancelability.  , which is presented in higher detail in  [420] ).\n\nData from the last 100 identities were used for training, while the data from the remaining 919 subjects have been reserved for testing. From these 919, one has been discarded for only having a total of 30 seconds of data. Triplets have been generated by selecting an anchor from the first 30 s of data from a subject and positive and negative samples from the remaining data of the same or another identity, respectively. From the 100 training identities, 100 000 triplets have been generated, with 20% being used for validation. A total of 10 000 triplets have been generated for testing. Each of the three samples in a triplet is a blindly-segmented five-second raw ECG sample, normalised to zero mean and unit variance.",
      "page_start": 246,
      "page_end": 247
    },
    {
      "section_name": "Model",
      "text": "The model for ECG identity verification (see Fig.  14 .3) is adapted from the end-to-end architectures proposed in  [338; 344]  and described in Chapter 4 and Chapter 5. The model is composed of four unidimensional convolutional layers (with 16, 16, 32, and 32 filters, respectively, with size 1 × 5, unit stride, and zero padding), each followed by ReLU activation and max-pooling (with 1 × 3 kernels and stride 3). The model ends with two fully-connected layers, each with 100 units and followed by ReLU activation.\n\nOnce trained, this model receives a 5 second long raw ECG segment (1000 samples long at 200 Hz sampling frequency) and outputs an embedding or template that can be compared to a reference through the Euclidean distance (during training) or through the normalised Euclidean distance  [461]  (with the trained model, to obtain dissimilarity scores in the [0, 1] range). In the case of Secure Triplet Loss, the feature vector s(x) (the flattened feature maps from the last max-pooling layer) is concatenated with the key array k, and both are bound together by the fully-connected layers to make the final secure template f (x, k).\n\nThe model was trained using the Adam To fine-tune and evaluate the model, images from the YouTube Faces database  [460]  were used.\n\nThis database is composed of frames from 3425 YouTube videos, depicting a total of 1595 subjects (up to six videos of each subject). Each video corresponds to between 48 and 6070 frames.\n\nThis work used the aligned images provided on the database, which resulted from face detection, cropping, and alignment.\n\nEach face image has been reduced to 70% height and width and resized to 160 × 160 to match the input dimensions of the model. Ten random triplets have been generated for each of the first 500 subjects on the database for a total of 5000 training triplets, of which 1000 have been used for validation. Ten random triplets have also been generated from each of the remaining identities, reserved for testing, resulting in a total of 10 950 test triplets. Whenever possible, the anchor and positive samples corresponded to different videos of the same identity.",
      "page_start": 248,
      "page_end": 248
    },
    {
      "section_name": "Model",
      "text": "The model for face identity verification (see Fig.  14 .3) is based on the Inception-ResNet  [420] .\n\nThis network has been pretrained 2  for identification on the VGGFace2 dataset  [58]  and offered an accuracy of 99.63% on the Labelled Faces in the Wild (LFW) dataset and 95.12% on the YouTube Faces database  [381] . The original fully-connected layer has been replaced with two new fullyconnected layers, each with 100 units and followed by ReLU activation. For the Secure Triplet Loss, the first of these layers receives the feature vector s(x) from the first part of the model, concatenated with the key k. The second outputs the template y(x, k).\n\nAll layers on the model have been frozen, to take advantage of the pretrained parameters. The exceptions are the last convolutional block and the fully-connected layers that come, respectively, before and after the average pooling operation. The last convolutional block is fine-tuned to allow for small adjustments during training, while the fully-connected layers are newly created and thus require training. The model was trained for a maximum of 250 epochs at batch size 32, with early stopping based on validation EER with a patience of 25 epochs. As with the ECG model, the Adam optimiser was used with an initial learning rate of0.0001 and l2 regularisation with λ = 0.001.",
      "page_start": 249,
      "page_end": 249
    },
    {
      "section_name": "Evaluation Frameworks And Metrics",
      "text": "The experiments have been designed to quantify the performance of the models trained with the original and Secure Triplet Loss formulations, not only considering verification accuracy but also biometric security.",
      "page_start": 248,
      "page_end": 248
    },
    {
      "section_name": "Verification Performance",
      "text": "The verification performance is quantified through the measurement of false match rates (FMR) and false non-match rates (FNMR) over the range of possible decision thresholds (for these models, t ∈ [0, 1]). These values are presented in FMR vs. FNMR plots and detection error tradeoff (DET) curves and used to compute the equal error rate (EER), corresponding to the error where FMR V = FNMR, and the FNMR@FMR = 0.01%.",
      "page_start": 249,
      "page_end": 249
    },
    {
      "section_name": "Cancelability",
      "text": "Avoiding additional processes such as biohashing or template encryption, the proposed Secure\n\nTriplet Loss integrates cancelability into the single output of the system, the template y(x, k), and is reflected in the distance measure d between two templates. Although the proposed loss is designed to promote cancelability, this property may not necessarily be achieved.\n\nHence, the experiments with the Secure Triplet Loss include the measurement of cancelability error. The plots of false match vs. false non-match rates over the dissimilarity/distance scores include both the false match rate based on identity (when identities don't match, denoted as FMR V )\n\nand the false match rate based on cancelability (when keys don't match, denoted as FMR C ). The false non-match rate (FNMR) values are the same for identity and cancelability since they refer to situations when both identity and keys match. The value of cancelability false accept rate at the operation point that corresponds to the verification EER, FMR C @EER, is also computed.",
      "page_start": 249,
      "page_end": 249
    },
    {
      "section_name": "Unlinkability",
      "text": "The template unlinkability analysis followed the method described by Gomez-Barrero et al.  [149] .\n\nThe test samples were paired into mated (different biometric samples from the same identity with different keys) and non-mated instances (different identities and keys). These have been used to compute p(d|H m ) and p(d|H nm ): the probability density functions of the distance score d given the instances are, respectively, mated (hypothesis H m ) or non-mated (hypothesis H nm ). From the likelihood ratio LR\n\nwhich allows to compute the D sys ↔ linkability metric with\n\nThe D sys ↔ is considered the main metric to quantify template linkability. A biometric system verifying perfect template unlinkability, which is highly desirable, will assume D sys ↔ = 0. A biometric system creating entirely linkable templates will verify D sys ↔ = 1.\n\nwhere X is the input biometric, Y is the output of the model, H(X) denotes the entropy of X, H(X|Y ) denotes the conditional entropy of X given Y , and I(X;Y ) denotes the mutual information between X and Y . The privacy leakage rate, in the range [0, 1], should be as close to 1 as possible:\n\nobtaining information on X should be impossible even when one has all knowledge of Y . The secrecy leakage measures the mutual information between the stored template Y and the key K, through the expression I(Y ; K). The keys are public, unlike the templates, so they should reveal as little information as possible on the templates. Hence, the secrecy leakage should be close to zero.\n\nThese require the computation of some information theoretical measures, such as entropy and mutual information. This is very difficult in biometrics, due to the high dimensionality of the inputs and the feature sets, as well as their variability. In this work, entropy and mutual information were estimated using a Python implementation 3 of the methods proposed in  [228]  and in  [229] ,\n\nrespectively, for continuous multivariate data. These methods, based on nearest neighbour statistics, were shown to be more accurate than the alternatives  [106] . Since the processing cost of such estimations grows exponentially with the size of the dataset, a subset of 1000 test anchors has been used for this test.",
      "page_start": 249,
      "page_end": 250
    },
    {
      "section_name": "Results And Discussion",
      "text": "A general overview of the results obtained is presented in Table  14 .1 and Table  14 .2, respectively for ECG and face identity verification. The following subsections discuss the results on verification performance, cancelability, and unlinkability, and the comparison with state-of-the-art alternatives.\n\n3 Paul Brodersen's Entropy Estimators. Available on: https://github.com/paulbrodersen/entropy_estimators. Nevertheless, the model trained with any of the proposed loss formulations still offers considerably better performance than the state-of-the-art methods. The best state-of-the-art method evaluated in the same conditions (in  [338] ) offered 21.82% EER vs. 13.58% attained by SecureTL w/KLD and 13.33% achieved by SecureTL w/SL. This denotes that the proposed method, while presenting a small performance gap with the linkability loss module, still retains most of the performance advantages associated with deep end-to-end models.\n\nFor face identity verification, the performance results are presented in Table  14 . In harmony with the results on ECG, the model trained with the Secure Triplet Loss without    a linkability component offered a small improvement in verification performance (13.61% EER).\n\nLikewise, the addition of a linkability-measuring term to the loss leads to a 2% increase in EER. This confirms the aforementioned belief that the separate linkability loss term is affecting performance and improvements could be achieved by integrating it into the Secure Triplet Loss in a more cohesive way.\n\nOverall, the verification performance results denote that it is possible to adequately train or fine-tune an end-to-end model with the proposed loss formulations. With either biometric characteristic, the performance difference between using KLD and distance statistics is not appreciable, which denotes these formulations may each be fitted for specific settings or used interchangeably.",
      "page_start": 250,
      "page_end": 251
    },
    {
      "section_name": "Cancelability Evaluation",
      "text": "As aforementioned, by integrating identity verification and template cancelability into a single comparison score, template cancelability is not necessarily ensured. Hence, the results of false match rates based on cancelability (FMR C ) are presented, in Fig.  14 .6 and Fig.  14 .7, alongside the false match rates based on verification (FMR V ) and the common false non-match rates (FNMR).\n\nIn all cases, the FMR C is lower than FMR V at and around the EER operation point. In most cases, FMR C at this point is very small and is lower than or equal to FMR V for all operation points, which is highly desirable. As presented in Table  14 .1 and Table  14 .2, cancelability error is significantly lower in the ECG models. As shown by the results, SecureTL w/KLD and w/SL appear to be better at promoting cancelability than the original secure loss formulation, denoting that the linkability loss term could have a positive effect on cancelability.\n\nConsidering these results and the increased difficulty experienced while fine-tuning the face models, one can conclude that the proposed Secure Triplet Loss is likely better fitted for training models from scratch than to adapting previously trained models to become secure. Nevertheless, the cancelability results, especially with the SecureTL w/KLD and SecureTL w/SL, are encouraging in either case.",
      "page_start": 252,
      "page_end": 252
    },
    {
      "section_name": "Unlinkability Evaluation",
      "text": "The results of the linkability analysis following the framework established in  [149]  are presented in Fig.  14 .8. In both cases, the original formulation of the Secure Triplet Loss presents relatively high D sys ↔ (0.288 for ECG and 0.399 for face). However, the result with ECG is better than the equivalent reported earlier in  [345]  (0.67). This results from the fact that linkability was not promoted by this loss during model training: hence, the model may achieve adequate unlinkability, but that would be accidental.\n\nIn the case of the proposed SecureTL w/KLD and SecureTL w/SL, linkability is actively promoted during training through the loss. The effects of this loss reformulation are clear: the probability density functions of mated and non-mated are more superposed, which indicates that it would be more difficult, as desired, to distinguish identities in pairs of templates where the keys do not match.",
      "page_start": 255,
      "page_end": 255
    },
    {
      "section_name": "With Ecg, D Sys",
      "text": "↔ assumes the values 0.005 for SecureTL w/KLD and 0.004 for SecureTL w/SL. With face, it assumes 0.132 for SecureTL w/KLD and 0.070 for SecureTL w/SL. All of these can be considered semi to fully-unlinkable. Just as with cancelability, the proposed method seems more adequate for training models from scratch than for fine-tuning existing biometric models.\n\nAdditionally, using KLD appears to offer some advantages in linkability for ECG verification, but that should be weighted with the increased instability this alternative has shown during training, relative to SecureTL w/SL, especially in face verification.",
      "page_start": 256,
      "page_end": 256
    },
    {
      "section_name": "Non-Invertibility And Secrecy Leakage",
      "text": "Regarding other security metrics, the privacy leakage rate was estimated as 1 for the model trained with any of the losses. This indicates that it is highly difficult for an attacker to recover the original biometric measurements x based on compromised templates y output by the model. This could be a result of using end-to-end deep learning models: recent research indicates that optimised deep models compress the inputs retaining only the information needed for the task  [434] . This means perfect non-invertibility can be achieved without carefully handcrafted feature extraction algorithms.\n\nSimilarly, all losses led the model to offer a perfect secrecy leakage rate of 0, which denotes that the public keys used to make the templates cancelable reveal no information on them. These results on non-invertibility and secrecy leakage do not show a superiority of the proposed loss formulations over the original triplet loss but emphasise the meaningful advantages of using endto-end deep learning models for secure biometrics.",
      "page_start": 256,
      "page_end": 257
    },
    {
      "section_name": "Comparison With State-Of-The-Art Approaches",
      "text": "The proposed method was compared with two state-of-the-art approaches: Bloom Filters (BF) and Homomorphic Encryption (HE), as described in  [149]  and  [109] , respectively. To provide a fair and direct comparison between the template protection schemes, the features given to those methods were those output by the triplet loss baseline model.\n\nThe results are presented in Table  14 .1, Table  14 .2, Fig.  14 .4, and Fig.  14 .5. Both with face and ECG, the proposed method outperformed BF in EER, cancelability, and linkability. HE offered the best linkability results, at the cost of poor cancelability. Additionally, HE took significantly longer for biometric comparison than any of the alternatives, which may grant it limited real applicability.\n\nAlthough the error results are relatively high, the Secure Triplet Loss is competitive vs. the state-of-the-art alternatives, especially on cancelability and linkability. Moreover, improved results are expected when the Secure Triplet Loss is used on more accurate biometric models.  method is not only able to fulfil this purpose, but also to adapt pretrained biometric models to offer secure templates, with competitive performance results.",
      "page_start": 257,
      "page_end": 258
    },
    {
      "section_name": "Effects Of Varying Γ",
      "text": "However, there is still room for improvement. Further efforts should be devoted to designing ways to better integrate linkability in the Secure Triplet Loss, in order to avoid performance decreases. A scheme where linkability would be measured triplet-by-triplet (instead of batch-bybatch), similarly to cancelability, should lead to improved performance using the Secure Triplet Loss. This would also enable the formulation of triplet mining approaches for the proposed method. Nevertheless, the Secure Triplet Loss is, overall, a suitable and flexible general scheme for template protection in end-to-end deep biometrics.",
      "page_start": 258,
      "page_end": 258
    },
    {
      "section_name": "Chapter 15",
      "text": "Self-Supervised Learning with Sequential Data",
      "page_start": 260,
      "page_end": 260
    },
    {
      "section_name": "Foreword On Author Contributions",
      "text": "The research work described in this chapter was conducted entirely by the author of this thesis, under the supervision of Jaime S. Cardoso. The results of this work have been disseminated in the form of an article in international conference proceedings: • J. R. Pinto and J. S. Cardoso, \"Self-Learning with Stochastic Triplet Loss,\" in International Joint Conference on Neural Networks (IJCNN 2020), Jul. 2020.  [340]  15",
      "page_start": 103,
      "page_end": 103
    },
    {
      "section_name": ".1 Context And Motivation",
      "text": "In recent years, deep learning algorithms have offered improved performance over handcrafted methodologies in several pattern recognition tasks. These commonly take advantage of convolutional layers, which enable the autonomous learning of the most relevant features for the task at hand, and use fully-connected layers for more intricate decision boundaries  [242] . However, these improvements come with an important drawback: the need for labelled data.\n\nMost tasks where such performance breakthroughs have been achieved are those where researchers have plenty of labelled data at their disposal. The ImageNet dataset enabled the training of deeper models for better performance in the detection and recognition of objects. Similarly, datasets such as VGGFace  [331]  and VGGFace2  [58]  helped in the development of improved models for biometric recognition based on face images.\n\nHowever, for some pattern recognition problems, supervised data is scarce. In most of these cases, even though available data is plenty, the annotation process is cumbersome and/or expensive. This is very frequent in automatic medical image diagnosis tasks, where several imaging exams are usually available but lack specific annotations which typically would need to be offered by experts.\n\nAnother exemplary application is video surveillance. Given the current ubiquity of surveillance cameras, the availability of data is not a problem. However, the annotation of individuals on the recordings is a long and expensive endeavour. This limits the performance one can attain in these tasks since deeper models will be harder to train.\n\nYet another key application is continuous biometrics with electrocardiogram (ECG) signals  [343] . Deep learning has offered improved performance through increased robustness to signal noise and variability  [344] . However, in scenarios with off-the-person signals (acquired during normal activity using few dry electrodes on the fingers and palms), the performance still fails to match the use of cleaner on-the-person signals (acquired on medical-grade settings from subjects at rest, using several wet electrodes on the chest and limbs)  [284; 338; 483] . This is verified because current off-the-person signal datasets are too few and too small to train larger and deeper models.\n\nSelf-learning (SL) has emerged as a promising approach to learn from unlabelled data. Contrarily to unsupervised learning, SL uses contextual or prior information to automatically define labels and tasks, which then support the learning. Generally, self-learning methods are focused on specific applications, including visual representation learning  [128; 147] , action classification  [129] , or human motion capture  [439] , thus including details that restrict their use to those specific tasks.\n\nOn the other hand, several self-learning methods use simple low-level tasks for the training, such as ranking samples  [254]  or recovering masked parts of an image  [437] , that do not necessarily guarantee that the learned parameters can be useful for high-level tasks. For example, features that prove adequate for the approximate reconstruction of biometric samples may not be good enough to discriminate their identities. Hence, there is currently a need for a more general and capable self-learning methodology.\n\nIn this work, we propose a novel formulation of the triplet loss  [72]  for self-supervised learning with unlabelled data, unrestricted to specific problems. The triplet loss is particularly suited for this task since it does not require absolute labels for the samples. As samples are combined into triplets, only their relative label information is required. The proposed formulation finds its key application in sequential data scenarios, where mild assumptions about the data acquisition process enable the adoption of a stochastic adaptation of the triplet loss to trigger and sustain the learning.\n\nIn the experimental work, the proposed methodology is successfully applied to off-the-person ECG-based biometric identity verification tasks, using signals from the University of Toronto ECG Database (UofTDB)  [445] , and to unconstrained face identity verification, using the YouTube Faces dataset  [460] . Specific stress experiments were conducted on the ECG-based identity verification task to evaluate the behaviour of the proposed methodology in strain conditions.",
      "page_start": 259,
      "page_end": 260
    },
    {
      "section_name": "The Stochastic Triplet Loss",
      "text": "The triplet loss  [72]  uses triplets of data samples to train a network to accurately assess if two samples belong to the same class. Each triplet is composed by an anchor x a with identity i a and two other samples, one positive (x p ) and one negative (x n ), where i p = i a = i n . The three samples are processed, in parallel, by the same network, which returns a learned representation of each\n\nDuring training, the goal to decrease the triplet loss will lead the model to adjust its weights to obtain a final representation which brings samples of the same class closer together (reducing d + ), and samples of different classes further apart (increasing d -). Here, the margin parameter α will contribute to enforce a minimum distance margin between the samples of different classes.\n\nIn the standard triplet loss, it is certain that x p is sampled from the same class as x a and x n is sampled from a class different from the class of x a . This can be rewritten as P(I i a (i p ) = 1) = 1 and P(I i a (i n ) = 0) = 1, where I A (x) is the indicator function.\n\nWith unlabelled samples, there is an uncertainty associated with the generation of the triplets, arising from the possibility of errors during the selection of the positive and negative samples.\n\nWe generalise the previous assumption by modelling I i a (i p ) as a random variable following a\n\nBernoulli distribution with parameter β , i. e., P(I i a (i p ) = 1) = β . Similarly, I i a (i n ) is assumed to follow a Bernoulli distribution with parameter γ, i. e., P(I i a (i n ) = 0) = γ.\n\nAssuming the independence of I i a (i p ) and I i a (i n ), and conditioned on the true identity of the observations in the triplet, the triplet loss follows a multinoulli distribution, with:\n\nOn average, the middle terms in Eq. (15.2) do not contribute to the learning. The last term in the equation negatively impacts the learning process. In practice, one would need more data/time to learn under the noisy sampling of the triplets. The parameters β and γ guide the training of the model through the triplet loss, and their values depend on the specificities of the task and the data.\n\nIn ideal conditions, these should be as close as possible to 1 to approximate the original triplet loss in supervised settings. Lower values would work against the purpose of the triplet loss and diminish its training effectiveness (or, equivalently, increase the difficulty of the training).\n\nThe proposed self-learning methodology can be used to train models with unsupervised data.\n\nDuring triplet generation, after the selection of an anchor sample, one can randomly draw one sample from the dataset to serve as the negative sample. Assuming a balanced dataset with C classes will give γ = 1 -1/C. If C is large, the probability of errors in negative sample selection p(i a = i n ) will be very low (e. g., 0.1 for C = 10 or 0.01 for C = 100), and so will be their impact on the training process. More importantly, in practice, prior knowledge allows us to adopt a sampling strategy with a much higher probability of success.\n\nThe positive sample can be obtained through the transformation of the anchor according to x p = f (x a ). The transformation f should be carefully defined in order to change the anchor according to an expected range of intraclass variability but without degrading the underlying label information carried by the sample. The probability β will depend on the degree to which f complies with this need. Similarly to the negative sample selection, prior knowledge of the data may be useful to maximise the probability of success. For example, when dealing with sequential data, choosing a positive sample closer in time to the anchor will increase the probability of both samples sharing the same label. However, the anchor and the positive sample will likely be more similar, which will restrict the model's robustness to intraclass variability. Hence, one should find a trade-off between ensuring intraclass variability and maximising the probability of success in positive sample generation.\n\nAn approximation of the expected value of the loss in Eq. (15.2) can be computed under some simplified conditions. Assuming a setting with two classes C 1 and C 2 , with a probability density functions p 1 (x) and p 2 (x), respectively. If x a is sampled from either of the distributions, it results in p a (x a ) = π p 1 (x a ) + (1π)p 2 (x a ), with 0 ≤ π ≤ 1. Setting x = [x a x p x n ] with a probability density function p(x) = p(x a , x p , x n ) assumed to be equal to\n\nthe triplet loss between x a , x p , x n can be described using the Euclidean distance function as\n\nwhere r(x) is the learned representation of x.\n\nIn the presence of the assumed noise model in the sampling process of the triplets (x a , x b , x c ), the probability density function becomes\n\nWith this, the triplet loss becomes:\n\nwith:\n\nand:\n\nNoting that the expected value of the gradient of the loss L (x a , x p , x n ) is zero under h 1 and h 2 (since x p and x n are sampled from the same distribution and the loss is symmetric), the impact of those two cases in a gradient-based learning scheme is small. The total loss is then:\n\nunder the p(x) probability density function, where y = r(x).\n\nFinally, this loss can be compacted to:\n\n.\n\n(15.10)\n\nIn section 15.3, example applications of this methodology are presented for the tasks of electrocardiogram-based biometric identity verification and face identity verification.",
      "page_start": 261,
      "page_end": 261
    },
    {
      "section_name": "Application Scenarios",
      "text": "The proposed method can be used to train models relying solely on unsupervised data. On classification tasks, the negative sample can be generated through the random selection of a sample in the dataset. Assuming a large number of balanced classes, errors in negative sample selection should be rare. For the positive samples, the function f (x) that generates them based on an anchor can be a data augmentation procedure. This should be carefully adjusted to cover the expected intraclass noise and variability while retaining the information pertaining to the underlying image label, which can be difficult.\n\nAlternatively, when training with sequential data, the triplet generation can forgo the data augmentation procedures. In these situations, depending on the acquisition context or protocol, the temporal distance or proximity between data can be used to infer the identity of the subjects.\n\nA sample that is very close in time to the anchor can safely be used as x p . Similarly, a sample that is sufficiently distant in time to the anchor can be assumed to belong to a different user, and thus used as x n . Some knowledge of the domain and the acquisition settings can be used to adjust the distance between x a , x p , and x n to maximise β and γ.\n\nBoth aforementioned alternatives (entirely unsupervised or using sequential data) were explored for the applications described below, through the experiments described in section 15.4.",
      "page_start": 263,
      "page_end": 264
    },
    {
      "section_name": "Ecg Identity Verification",
      "text": "Deep learning models have previously shown improved robustness to off-the-person noise and variability in electrocardiogram-based biometrics  [338; 344] . However, to train such models and match the performances reported for cleaner on-the-person signals, one would need large databases of off-the-person acquisitions, which are currently unavailable  [343] . In such circumstances, a pretrained network would often be the natural option in computer vision tasks. However, these too are currently nonexistent for unidimensional physiological signals such as the electrocardiogram.\n\nThe integration of ECG sensors in everyday objects, e. g. using the CardioWheel steering wheel cover  [279]  for shared vehicles or similar solutions for shared bicycles or scooters, enables the continuous acquisition of data from several subjects over long periods. This large amount of collected data could be used to train deeper and more sophisticated models. However, this data is commonly unlabelled, as the identity of the users at the moment of acquisition cannot be easily verified.\n\nThe proposed methodology for self-learning can be applied to train models for ECG-based identity verification using such data. As aforementioned, perturbations based on data augmentation procedures can be applied to the anchor to generate a positive sample. Thus, the four most successful data augmentation procedures proposed by Pinto et al.  [344]  were implemented. For each triplet, one of these was randomly selected to generate a positive sample from the anchor:\n\n• Cropping: a smaller contiguous segment is taken from the anchor sample and resampled to match the anchor's length, to simulate slower heart rates;\n\n• Baseline Wander: a periodic undulation, with a frequency near 1 Hz, is added to the anchor segment to simulate breathing movement artefacts;\n\n• Gaussian Noise: Gaussian noise is added to the anchor signal, simulating high-frequency distortions similar to the electromyogram (EMG) and powerline interference;",
      "page_start": 264,
      "page_end": 264
    },
    {
      "section_name": "Face Identity Verification",
      "text": "More face data are available now than ever before, especially from surveillance feeds or public videos shared on online social media platforms. However, as with ECG-based biometrics, the labelling of faces in acquired datasets is a tedious and lengthy task. Some researchers have taken advantage of online videos to build large datasets for face recognition, such as the YouTube Faces dataset from Wolf et al.  [460] . However, these datasets are limited by the number of annotations available.\n\nThe proposed self-learning method can be used to train models for face verification without labelled data. In this case, common image data augmentation based on rotations, width and height shifts, and horizontal flips were used as the transformation function f (x) that generates a positive sample x p based on an anchor x a .\n\nHaving short videos, a random detected face from the same recording as the anchor can serve as a positive sample, while a negative sample can be drawn from a different recording. With some knowledge of the recordings, we minimise the probability of errors in positive and negative The data used to train and evaluate the model is from the University of Toronto ECG Database (UofTDB)  [445] . This database includes data from 1019 subjects, acquired at 200 Hz using dry metallic button electrodes, held by the subjects in contact with one finger of each hand. Each recording is 2 -5 minutes long, and each subject has recordings for up to five different postures (supine, tripod, exercise, standing, and sitting) on up to six sessions over a period of six months.\n\nThe data was divided for model training and evaluation as done by Pinto et al.  [338] . The last 100 subjects (from subject 921 to subject 1020) were reserved for model training. The data from the remaining 918 subjects were used for evaluation. One subject (8) was discarded for having too few data. From the 918 subjects reserved for evaluation, the first 30 seconds of the first recording were used for enrollment, while the remaining data were used for testing. This aimed to mimic a realistic context with scarce supervised data as expected in real ECG-based biometric applications.",
      "page_start": 265,
      "page_end": 266
    },
    {
      "section_name": "Face Data",
      "text": "For face identity verification, data from the YouTube Faces database  [460]  were used. This database contains frames from 3425 videos of 1595 subjects, sourced from YouTube. Each video is 48 to 6070 frames long, and there are up to six videos of each subject. This work used the aligned images provided on the database, which resulted from face detection, cropping, and alignment.\n\nThe first 150 subjects (in alphabetical order) were used to build the dataset used in this work:\n\nthe first 100 subjects were reserved for training and validation, while the data from the remaining subjects were used for testing. Triplets were generated using this data subset, after resizing the images to 224 × 224, as detailed below in the experiments' description.",
      "page_start": 267,
      "page_end": 267
    },
    {
      "section_name": "Models",
      "text": "The self-supervised training method proposed in this work was explored for ECG-based identity verification using an adapted version of the end-to-end network proposed by Pinto et al.  [338]  (see Fig.  15 .1). The model receives two z-score normalised five-second raw ECG segments (a stored template and a query sample) and returns a measure of dissimilarity related to their identity.\n\nThe network is composed of four convolutional layers followed by a dense layer. A maxpooling layer (pooling size 1 × 5) follows each of the first three convolutional layers. Both models were trained using the Adam optimiser, with an initial learning rate of 0.0001. As in  [338] , the Euclidean distance was used as distance measure d during training, while for identity verification this was replaced by the normalised Euclidean distance for scores in [0, 1]. The triplet loss margin was set as α = 1.0. A maximum of 200 epochs was given, with batches of 12 triplets, along with early stopping with a patience of 10 epochs. Dropout was used before each dense layer, with rates of 0.5 and 0.2 for the ECG and the face models, respectively. L2 regularisation (λ = 0.01) was used for the convolutional layers in both models.",
      "page_start": 266,
      "page_end": 267
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "The evaluation metrics used are the False Acceptance Rate (FAR), the False Rejection Rate (FRR), the Equal Error Rate (EER), and the Receiver-Operating Characteristic (ROC) curve  [343] . The",
      "page_start": 267,
      "page_end": 267
    },
    {
      "section_name": "Ecg Identity Verification",
      "text": "The baseline results correspond to the identity verification model trained with supervised data. The equal error rate of 12.56% represents a small improvement over the corresponding result reported in  [338] . Considering the evaluation data and conditions were the same, this method also offered significantly better results than the state-of-the-art methods implemented and tested in  [338]",
      "page_start": 268,
      "page_end": 268
    },
    {
      "section_name": "Face Identity Verification",
      "text": "As with the ECG-based identity verification task, the model trained with supervised data was used as a baseline for comparison of results in face identity verification. The performance offered by the baseline was 18.45% EER. This is considerably higher than the state-of-the-art, which is explained by the relative simplicity of the implemented model and the relatively small dataset used. Nevertheless, the goal of this work was not to overcome or match the state-of-the-art in face verification but to illustrate how the proposed self-learning methodology can be applied to face biometrics with small performance losses relative to a supervised baseline in similar conditions. The results with the proposed methodology are illustrated in Fig.  15 .4. When using entirely unsupervised data, the proposed method offered 22.81% EER, a 4.36% increase relative to the use of supervised data. With recording-based triplet generation, the model offered 19.77% EER, a 1.32% increase. These small performance losses when forgoing labels during training show that the model can learn without supervision using the stochastic triplet loss, as verified above for ECG biometrics. Besides these two applications, one should expect the proposed self-learning methodology to be successfully applied to similar problems.",
      "page_start": 269,
      "page_end": 270
    },
    {
      "section_name": "Stress Experiments",
      "text": "As discussed in subsection 15.4.4, the success of the unsupervised triplet generation technique depends on the number of identities (classes) in the database. Hence, an experiment was performed on ECG identity verification to simulate the variation of the number of identities on the dataset, inducing errors in the negative sample selection with the respective probability. The results (see Fig.  15.5)  show that, although the performance worsens with fewer subjects, the errors have a very small effect for datasets with more than 20 subjects. In fact, with 50 subjects or more, the performance results stabilised around 20% EER. Hence, a dataset with 50 classes should be enough to adequately apply this method with better performance than handcrafted state-of-the-art approaches. decrease is relatively small unless the probabilities of error are over 50%. This means that some knowledge of the typical usage times and patterns during data acquisition would be enough to adjust the process of positive and negative sample selection and ensure the best results.\n\nResults could be further improved using more enrollment data (see Fig. sidered. ECG identity verification performance with the proposed unsupervised and recordingbased training approaches reached 14.55% and 9.89%, respectively, when using thirty-second enrollments.",
      "page_start": 270,
      "page_end": 270
    },
    {
      "section_name": "Summary And Conclusions",
      "text": "This work proposed a novel formulation of the triplet loss for self-supervised learning with unlabelled data. This method considers the uncertainty associated with the triplet generation in unsupervised settings and maximises the probability of success using prior knowledge.\n\nIn harmony with the goals of this thesis, the proposed methodology was applied to the task of ECG-based biometric identity verification, using transformations based on data augmentation or the temporal proximity between samples to generate valid triplets. The method offered better performance than handcrafted state-of-the-art methods, especially when using temporal proximity between samples, with performance results similar to supervised training. This pattern was also confirmed on the task of unconstrained face identity verification. Training with entirely unsupervised data using the proposed triplet loss formulation resulted in just a small performance loss when compared with the use of supervised data. When generating triplets based on video streams, this loss was considerably smaller.\n\nIt should be noted that the proposed method can be influenced by imbalanced classes and errors in the unsupervised triplet generation. However, the results of the stress experiments show its robustness is sufficient to avoid considerable impacts on performance in most cases. Thus, this method would, according to the presented results, be a viable training option in multiclass classification problems where only unlabelled data are available, especially with sequential data. the different parts of the ECG signal, especially the QRS complex, when distinguishing identities in diverse scenarios;\n\n• A multimodal approach for audiovisual recognition of emotion valence in groups of individuals. The proposed approach, based on adapted state-of-the-art sound and video recognition modules, presented promising results in regards to both accuracy and efficiency in the recognition of emotional states of groups of people;\n\n• A cascade strategy to streamline multimodal audiovisual activity recognition in timesensitive scenarios. When evaluated for multiclass activity recognition and violence detection, including inside real vehicles, the proposed approach was able to offer improved accuracy with up to 66% reduction in runtime;\n\n• The first approach to achieve template security in end-to-end biometric models. Aiming to combine the benefits of end-to-end deep learning with the security of template cancelability and unlinkability, the proposed Secure Triplet Loss was able to outperform the state-of-theart alternatives and offer minimal performance gaps without requiring separate encryption processes or protection schemes. At the time of writing this thesis, the resulting journal article and international conference article had been cited in the literature fifteen times;\n\n• An adaptation of the triplet loss to allow for fully unsupervised learning. Taking advantage of multiclass balance and the natural structure of sequential data, the proposed approach was able to considerably close the gap to supervised performance levels, even in the more challenging evaluation scenarios.\n\nThe secondary contributions of this doctoral work, resulting from collaborations within the scope of this thesis which benefitted partially from the work of the author, were:\n\n• The first study on long-term performance evolution of multiple state-of-the-art ECG biometric methodologies. This study highlighted the problem of intrasubject variability in ECG biometrics, even over relatively short time periods, and proposed multiple template/model update strategies to mitigate its negative effects on identification performance;\n\n• An approach for recovering the full set of twelve standard ECG leads relying only on short single-lead blindly-segmented recordings. The proposed approach has offered promising results in a considerably more challenging evaluation scenario than those found in the literature, and paves the way towards robust and efficient methods for retrieving missing leads in more comfortable acquisition setups;\n\n• Two novel strategies to reduce the performance gap in masked face recognition. Among the proposed approaches, the one based on multi-task contrastive learning outperformed the alternative methods and illustrated the benefits of promoting the similarity between latent masked and unmasked face image representations. At the time of writing this thesis, the resulting three international conference articles had been cited in the literature over fifty times;\n\n• A pioneering study on interpretability for face biometrics, through the presentation attack detection task. This exploratory experiment illustrated how interpretability tools could be used to achieve a deeper understanding of the behaviour of biometric models and motivated their use in the next generation of biometric evaluation strategies.\n\nThrough these contributions, this doctoral project touched on several important topics related to each individual research area and the thesis theme as a whole. In ECG biometrics, the work addressed the topics of end-to-end models, transfer learning, data augmentation, long-term performance, template update, interpretability, and interlead conversion. In face biometrics, the topics of masked face recognition, presentation attack detection, and interpretability were covered. In wellbeing monitoring, this work focused on multimodal fusion, group emotion recognition, violence detection, in-vehicle scenarios, and optimisation/efficiency. Additionally, the broader topics of biometric template security and self-supervised learning were also addressed.\n\nConsidering the achievements of this work, one can conclude that, although the ideal of truly personalised wellbeing monitoring is yet to be achieved, meaningful and valuable strides have been successfully taken to reach it. As such, a strong framework is now built to support future work towards the central goal of tightly integrating biometric recognition and wellbeing monitoring in a multimodal, seamless, continuous, and realistic way. This conclusion is supported by the reception of this research within the scientific community. The work described in this thesis has directly resulted in twenty-four scientific publications, including five articles in peer-reviewed journals and eleven articles in the proceedings of international conferences. This increases to thirty-eight total publications if one also considers other minor contributions to other topics in biometrics, computer vision, and pattern recognition, which have not been addressed in this document. These had been welcomed by the scientific community with over three hundred citations by the time this thesis was written.",
      "page_start": 272,
      "page_end": 277
    },
    {
      "section_name": "Future Work Considerations",
      "text": "Despite the results achieved in the doctoral work described throughout this thesis, plenty is yet to be done to achieve the full symbiotic integration of biometric recognition and wellbeing monitoring. The successful integration of these two tasks in real scenarios is a challenging endeavour in itself. Nevertheless, plenty of opportunities are yet to be explored in the topics of ECG biometrics, face biometrics, and wellbeing monitoring focused on this doctoral work, which would be essential to achieve our major objective.\n\nWhen considering the current state of ECG biometrics, it is hard to disagree with the notion that data is the main problem to be tackled. Many would argue that face biometrics is more evolved than ECG biometrics because the ECG is more deeply affected by noise and variability. But this is misleading, since the face suffers (and heavily so in truly unconstrained scenarios) from most of the same factors that affect the ECG, including emotions, exercise, drowsiness, and medical conditions. Even the loss of information due to heavy noise or sensor contact losses in off-the-person ECG is analogous to common occlusions that can deeply limit the available information in unconstrained face images. Face biometrics is more developed thanks to the unprecedented magnitude of data available to train increasingly sophisticated and robust models. The number of subjects, the unconstrained nature of the data, the diversity of scenarios, the variety of acquisition sessions, and the comprehensiveness of current benchmark data in ECG biometrics pale in comparison to what can be found for face biometrics.\n\nAs such, researchers should dedicate special efforts to creating larger (and better) datasets for ECG biometrics. Increased number of identities, longer recordings, more sessions across wider periods of time, and more realistic off-the-person scenarios are only some of the aspects that need to be verified by new datasets. Since the ECG could likely serve society better as part of multimodal solutions, new datasets could focus on the simultaneous (and continuous) acquisition of other traits alongside the ECG, especially face video. While such datasets remain unavailable, it would be interesting to find new ways to mitigate the effects of data scarcity. This includes the development of sophisticated pretrained models that could be fine-tuned to multiple tasks, the study of learnable 1D to 2D transformations to take advantage of image pretrained models, and the development of tailored solutions for unsupervised and self-supervised learning. At last, it is also important to standardise the way performance is evaluated in ECG biometrics, through the definition of complete and realistic benchmark datasets to assess and compare the performance of state-of-the-art methods.\n\nConversely, in face biometrics, data is plenty and models are considerably more accurate.\n\nHowever, as we could witness from the results of this doctoral research, there is still much to do regarding the robustness of face recognition in realistic scenarios. The advent of face masks revealed the feeble nature of current state-of-the-art approaches to unexpectedly drastic scenarios and reignited the old topic of occlusions in face biometrics. In fact, after over two years of heavy and dedicated research in masked face recognition, there is still a considerable performance gap vs. unmasked scenarios. Masks may soon leave our society, but the possibility for such severe occlusions to be witnessed once again (in the shape of masks or any other object) is enough to warrant further research. As such, the successful example of multi-task contrastive learning should be followed with the development of more sophisticated multi-objective schemes for learning to ignore masked face regions. Moreover, the creation of larger datasets of real masked face images and videos is also a key step towards closing the performance gap in masked face recognition.\n\nAdditionally, given that face biometrics is a much more developed topic than ECG biometrics, interpretability is also a much greater opportunity in this topic. Face recognition solutions have permeated countless aspects of our society, including deeply sensitive applications such as border control or urban surveillance. This situation starts to raise doubts about the trustworthiness of such algorithms. Interpretability is essential to offer the deeper level of understanding needed to avoid biases, lead models away from undesirable behaviours, and ultimately make the general public more comfortable with the use of such technologies. As such, it is important that more attention is devoted to the topic of interpretability in biometrics (especially face biometrics), through the redefinition of evaluation standards and the development of approaches with interpretability and explainability as one of the main goals (and not just an afterthought).\n\nOn the topic of wellbeing monitoring, two major problems are evident from the results of the work conducted during this doctoral project. The first is performance in specific scenarios such as in-vehicle monitoring. Current models present promising results in general purpose scenarios, for which data has become plenty through online sourcing, but are still relatively weak for recognition in specific scenarios. These, naturally, should not be overlooked and researchers should dedicate efforts to better take advantage of generalistic models for specific scenarios, e.g., through more sophisticated methods for transfer learning and domain adaptation. The second problem is subject dependency. This is a well-documented problem in the literature and, while not as evident in the group-level emotion recognition work presented in this thesis, has been encountered and discussed frequently in emotion and drowsiness recognition works linked to this doctoral work and the AU-TOMOTIVE project. It is troubling how performance can be affected when wellbeing monitoring models are applied to data from individuals they have never seen before. If wellbeing monitoring is to be applied in real scenarios, this problem should be tackled head-on through dedicated robustness studies and the reformulation of evaluation setups to ensure realistic subject-independent results.",
      "page_start": 279,
      "page_end": 279
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: 2: Reproduced with permission of Taylor and Francis Group",
      "page": 13
    },
    {
      "caption": "Figure 1: 1 presents them and illustrates their interconnec-",
      "page": 40
    },
    {
      "caption": "Figure 1: 1: Schema of the topics covered during this doctoral project, their interconnections, and",
      "page": 41
    },
    {
      "caption": "Figure 2: 1) [46; 201]. These modules are described",
      "page": 53
    },
    {
      "caption": "Figure 2: 1: General structure of a biometric recognition system (from [343], based on [46; 201;",
      "page": 54
    },
    {
      "caption": "Figure 2: 2: Schematics of the operation of a biometric recognition system in identiﬁcation and",
      "page": 55
    },
    {
      "caption": "Figure 2: 3: Attack points on a biometric system (based on [138; 361]).",
      "page": 56
    },
    {
      "caption": "Figure 2: 3) that sum up the different ways to unlawfully gain access to a biometric sys-",
      "page": 56
    },
    {
      "caption": "Figure 2: 4). A group of these deﬂections comprises a single heartbeat and each deﬂection can be",
      "page": 60
    },
    {
      "caption": "Figure 2: 4: The sequence of depolarisation and depolarisation events in the heart, and their rela-",
      "page": 61
    },
    {
      "caption": "Figure 2: 5), widely used for the diagnosis of cardiac disorders. Authors frequently",
      "page": 61
    },
    {
      "caption": "Figure 2: 6), which are pre-",
      "page": 61
    },
    {
      "caption": "Figure 2: 5: Medical acquisition settings: electrode placement and leads on the standard 12-lead",
      "page": 62
    },
    {
      "caption": "Figure 2: 6: Acquisition settings with movement: example of a ﬁve-electrode Holter system for",
      "page": 62
    },
    {
      "caption": "Figure 2: 7: Examples of off-the-person ECG acquisition conﬁgurations, using thumb electrodes",
      "page": 63
    },
    {
      "caption": "Figure 2: 7). These acquisition conﬁgurations were designated",
      "page": 63
    },
    {
      "caption": "Figure 2: 8). In research, the ﬁrst example of this highly acceptable acquisition",
      "page": 63
    },
    {
      "caption": "Figure 2: 8: Wearable and seamless acquisition: examples of surveyed conﬁgurations (from [343],",
      "page": 64
    },
    {
      "caption": "Figure 2: 9: Variability in off-the-person ECG heartbeats from the same subjects (from [337],",
      "page": 65
    },
    {
      "caption": "Figure 2: 9). Variability in the ECG",
      "page": 65
    },
    {
      "caption": "Figure 2: 10: Comparison of face images acquired on the (a) visible light, (b) short-wave infrared,",
      "page": 67
    },
    {
      "caption": "Figure 2: 11: Examples of tridimensional face models (from the Bosphorus 3D Face Data-",
      "page": 68
    },
    {
      "caption": "Figure 2: 11) [55; 77].",
      "page": 68
    },
    {
      "caption": "Figure 2: 12). These can enhance intersubject or intrasubject variability, and thus make",
      "page": 68
    },
    {
      "caption": "Figure 2: 12: Variability in unconstrained face images of a subject (from [205]).",
      "page": 69
    },
    {
      "caption": "Figure 2: 13). The Area Under the Curve",
      "page": 72
    },
    {
      "caption": "Figure 2: 13: Example of a Receiver Operating Characteristic (ROC) curve for an identity veri-",
      "page": 73
    },
    {
      "caption": "Figure 2: 14). From these",
      "page": 74
    },
    {
      "caption": "Figure 2: 14: Examples of a Cumulative Match Characteristic (CMC) curve, and a Receiver Operat-",
      "page": 75
    },
    {
      "caption": "Figure 2: 15: Example of a Usability-Security characteristic curve (from [337], adapted from Sim",
      "page": 76
    },
    {
      "caption": "Figure 2: 15). USC is",
      "page": 76
    },
    {
      "caption": "Figure 2: 16: Illustration of the bidimensional valence-arousal space with example emotion cate-",
      "page": 77
    },
    {
      "caption": "Figure 2: 16 illustrates these concepts by offering examples of categorical",
      "page": 77
    },
    {
      "caption": "Figure 3: 1 for the number of publications that have used them), and Table 3.1",
      "page": 84
    },
    {
      "caption": "Figure 3: 1: Currently available ECG collections and the number of surveyed publications that have",
      "page": 86
    },
    {
      "caption": "Figure 3: 2: General structure of an ECG-based recognition system (from [344]).",
      "page": 87
    },
    {
      "caption": "Figure 3: 2). We also offer a discussion on the approaches based on deep learning and the current",
      "page": 87
    },
    {
      "caption": "Figure 3: 3: A panorama of ECG biometrics across time: the past, present, and future trends in",
      "page": 101
    },
    {
      "caption": "Figure 4: 1: Architecture of the proposed CNN model for ECG-based identiﬁcation (the number of",
      "page": 104
    },
    {
      "caption": "Figure 4: 1) integrates all common pipeline stages",
      "page": 104
    },
    {
      "caption": "Figure 4: 2). These types are:",
      "page": 105
    },
    {
      "caption": "Figure 4: 2: Illustration of the effects of the different data augmentation techniques on an exam-",
      "page": 106
    },
    {
      "caption": "Figure 4: 3: Illustration of the progressive phases of integration of the traditional pipeline stages",
      "page": 107
    },
    {
      "caption": "Figure 4: 3). Thus, besides the",
      "page": 107
    },
    {
      "caption": "Figure 4: 4), the performance of the proposed method is similar to",
      "page": 107
    },
    {
      "caption": "Figure 4: 4: Results of the proposed and baseline algorithms, when using DCT features as input.",
      "page": 108
    },
    {
      "caption": "Figure 4: 5), as the performance increases and approaches that of the baseline",
      "page": 108
    },
    {
      "caption": "Figure 4: 6) illustrate the",
      "page": 108
    },
    {
      "caption": "Figure 4: 7). Most of the",
      "page": 108
    },
    {
      "caption": "Figure 4: 5: Results of the proposed and baseline algorithms, when using ensemble heartbeats as",
      "page": 108
    },
    {
      "caption": "Figure 4: 6: Results of the proposed and baseline algorithms, when using ﬁve-second ECG seg-",
      "page": 109
    },
    {
      "caption": "Figure 4: 9). The proposed method",
      "page": 109
    },
    {
      "caption": "Figure 4: 7: Results of the proposed algorithm receiving raw ﬁve-second segments, with each",
      "page": 109
    },
    {
      "caption": "Figure 4: 8: Results of the proposed algorithm, receiving raw ﬁve-second segments as input, with",
      "page": 110
    },
    {
      "caption": "Figure 4: 9: Direct benchmarking between the proposed architecture with the best baseline algo-",
      "page": 110
    },
    {
      "caption": "Figure 5: 1: Schemes of the proposed identity veriﬁcation model, including the weight transfer",
      "page": 114
    },
    {
      "caption": "Figure 5: 1, four unidimensional convolutional layers are alternated with three max-pooling layers.",
      "page": 115
    },
    {
      "caption": "Figure 5: 1). The training methodol-",
      "page": 115
    },
    {
      "caption": "Figure 5: 1 and described below. In all cases, during training, the optimiser used was Adam [219]",
      "page": 115
    },
    {
      "caption": "Figure 5: 2: Network outputs for all training samples of ﬁve example subjects from the UofTDB",
      "page": 118
    },
    {
      "caption": "Figure 5: 2). These are, effectively, the feature vectors used for the identity",
      "page": 119
    },
    {
      "caption": "Figure 5: 3). In all cases, an increase in the number of training subjects resulted",
      "page": 119
    },
    {
      "caption": "Figure 5: 3: Varying identity set size scenario: EER evolution with number of subjects reserved for",
      "page": 120
    },
    {
      "caption": "Figure 5: 4: Cross-database scenario: EER for the proposed methods IT-CNN and TL-CNN when",
      "page": 120
    },
    {
      "caption": "Figure 5: 5: Fine-tuning scenario: EER results for the proposed methods IT-CNN and TL-CNN",
      "page": 121
    },
    {
      "caption": "Figure 5: 6: Fine-tuning scenario: EER results for the proposed methods when (DT) trained, from",
      "page": 121
    },
    {
      "caption": "Figure 5: 5) and when trained with UofTDB data and ﬁne-tuned to CYBHi/PTB",
      "page": 122
    },
    {
      "caption": "Figure 6: 2) to ﬁnd one that simultaneously maximises true positives and minimises false positives.",
      "page": 129
    },
    {
      "caption": "Figure 6: 2: Illustration of the search for the ideal threshold (the values were chosen near the",
      "page": 130
    },
    {
      "caption": "Figure 6: 3: Schema illustrating the use of each E-HOL record for training and testing (in orange -",
      "page": 131
    },
    {
      "caption": "Figure 6: 3): one immediately after enrollment, another after one hour,",
      "page": 131
    },
    {
      "caption": "Figure 6: 4a), although permanence was not veriﬁed.",
      "page": 131
    },
    {
      "caption": "Figure 6: 4b. It was found that the",
      "page": 131
    },
    {
      "caption": "Figure 6: 4: Identiﬁcation performance over time corresponding to (a) the Labati et al. method, and",
      "page": 132
    },
    {
      "caption": "Figure 6: 5 presents the results using the FIFO technique, with",
      "page": 132
    },
    {
      "caption": "Figure 6: 6). With kNN, the template update",
      "page": 132
    },
    {
      "caption": "Figure 6: 5: Comparison of the FIFO method applied with different thresholds to different identi-",
      "page": 133
    },
    {
      "caption": "Figure 6: 6: Results using FIFO update with different thresholds.",
      "page": 133
    },
    {
      "caption": "Figure 6: 7: Results using Fixation update (the corresponding value represents the number of sam-",
      "page": 134
    },
    {
      "caption": "Figure 6: 9). Although the results with kNN are slightly worse than those of the",
      "page": 134
    },
    {
      "caption": "Figure 6: 8: Performance results over time for the CNN model with ﬁne-tuning-based model up-",
      "page": 135
    },
    {
      "caption": "Figure 6: 9: Performance results over time for the CNN model adapted with kNN decision and",
      "page": 135
    },
    {
      "caption": "Figure 7: 1: Illustration of the ECG waveforms on a sample PTB signal segment.",
      "page": 138
    },
    {
      "caption": "Figure 7: 1) [286; 343]. Each of",
      "page": 138
    },
    {
      "caption": "Figure 7: 2) receives ﬁve-second blindly segmented",
      "page": 139
    },
    {
      "caption": "Figure 7: 2: Architecture of the biometric identiﬁcation model.",
      "page": 140
    },
    {
      "caption": "Figure 7: 4), a trend is veriﬁed from smaller to larger identity subsets, consisting on the deviation",
      "page": 142
    },
    {
      "caption": "Figure 7: 5 and Fig. 7.6).",
      "page": 142
    },
    {
      "caption": "Figure 7: 5), the model still focuses mainly on the QRS, even though subject #2 has a very speciﬁc",
      "page": 142
    },
    {
      "caption": "Figure 7: 6), it is evident that the QRS retains the highest importance for the decision, even",
      "page": 142
    },
    {
      "caption": "Figure 7: 6), since the limited identity information carried by the",
      "page": 142
    },
    {
      "caption": "Figure 7: 3: Explanations over an example ﬁve-second ECG segment from PTB (in each subplot,",
      "page": 143
    },
    {
      "caption": "Figure 7: 4: Explanations over an example ﬁve-second ECG segment from UofTDB (in each sub-",
      "page": 143
    },
    {
      "caption": "Figure 7: 5: Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets",
      "page": 144
    },
    {
      "caption": "Figure 7: 6: Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets of",
      "page": 144
    },
    {
      "caption": "Figure 8: 1) receives an input segment",
      "page": 150
    },
    {
      "caption": "Figure 8: 2). Its architecture is very similar to the U-",
      "page": 150
    },
    {
      "caption": "Figure 8: 3) was originally proposed by Islam et al. [190] for semantic image segmentation. Its archi-",
      "page": 150
    },
    {
      "caption": "Figure 8: 1: Schema of the U-Net architecture.",
      "page": 151
    },
    {
      "caption": "Figure 8: 2: Schema of the convolutional autoencoder (AE) architecture.",
      "page": 151
    },
    {
      "caption": "Figure 8: 3: Schema of the architecture based on label reﬁnement networks (LRN).",
      "page": 152
    },
    {
      "caption": "Figure 8: 42 where the model is unable to capture the ﬁner details of the signals in lead aVL and",
      "page": 155
    },
    {
      "caption": "Figure 8: 4: Example result of lead II to all conversion on the PTB test dataset (each row depicts",
      "page": 156
    },
    {
      "caption": "Figure 8: 5: Example result of lead I to all conversion on the PTB test dataset (each row depicts",
      "page": 157
    },
    {
      "caption": "Figure 8: 5: for aVR and aVL, the model is able to correctly capture the",
      "page": 158
    },
    {
      "caption": "Figure 8: 5, one can also identify a shortcoming of the proposed",
      "page": 158
    },
    {
      "caption": "Figure 8: 6 and Figure 8.7 that both reference leads can offer good",
      "page": 160
    },
    {
      "caption": "Figure 8: 8 and Fig. 8.9, it is possible to observe that,",
      "page": 161
    },
    {
      "caption": "Figure 8: 6: Example cross-database result of lead II to all conversion on INCART (each row",
      "page": 162
    },
    {
      "caption": "Figure 8: 7: Example cross-database result of lead I to all conversion on INCART (each row depicts",
      "page": 163
    },
    {
      "caption": "Figure 8: 8: Example cross-database result of lead II to all conversion on PTB-XL (each row",
      "page": 164
    },
    {
      "caption": "Figure 8: 9: Example cross-database result of lead I to all conversion on PTB-XL (each row depicts",
      "page": 165
    },
    {
      "caption": "Figure 9: 1: Stages of a biometric recognition algorithm based on face images (based on [28]).",
      "page": 172
    },
    {
      "caption": "Figure 9: 1). Below, we delve into the state-",
      "page": 172
    },
    {
      "caption": "Figure 9: 2: Examples of face detection in unconstrained settings (images from the FDDB data-",
      "page": 173
    },
    {
      "caption": "Figure 9: 3: Evolution of face recognition approaches, from holistic to deep learning (from [452]).",
      "page": 175
    },
    {
      "caption": "Figure 9: 4: Recent history of face recognition, from deep learning to tailored objective functions (from [452]).",
      "page": 175
    },
    {
      "caption": "Figure 9: 5). One should",
      "page": 177
    },
    {
      "caption": "Figure 9: 5: Example of how a mask can signiﬁcantly occlude a face and limit the information that",
      "page": 178
    },
    {
      "caption": "Figure 9: 6: Illustration of how interpretability/explainability can be used to understand and im-",
      "page": 179
    },
    {
      "caption": "Figure 10: 1: Expected effect of the original triplet loss on the output embedding space.",
      "page": 184
    },
    {
      "caption": "Figure 10: 2: Expected effect of the proposed adapted triplet loss on the output embedding space.",
      "page": 185
    },
    {
      "caption": "Figure 10: 3, receives pairs of masked and unmasked images and is composed of two symbiotic",
      "page": 185
    },
    {
      "caption": "Figure 10: 3: Schema of the proposed multi-task contrastive learning approach.",
      "page": 186
    },
    {
      "caption": "Figure 10: 4: Explanations obtained for each trained model with the Smooth Grad-CAM++ ex-",
      "page": 189
    },
    {
      "caption": "Figure 10: 5: Example of masked face images generated to validate the multi-task contrastive",
      "page": 190
    },
    {
      "caption": "Figure 10: 5 for some examples).",
      "page": 190
    },
    {
      "caption": "Figure 10: 6 shows the ROC curve results",
      "page": 192
    },
    {
      "caption": "Figure 10: 6: Receiver operating characteristic (ROC) curve for mask detection on the LFW dataset",
      "page": 193
    },
    {
      "caption": "Figure 11: 1), as the emphasis of this work was to study the interpretability of the",
      "page": 196
    },
    {
      "caption": "Figure 11: 1: Architecture of the PAD end-to-end deep model used in this work.",
      "page": 197
    },
    {
      "caption": "Figure 11: 2: Example of the approach used to quantify the difference between two explanations.",
      "page": 200
    },
    {
      "caption": "Figure 11: 3 illustrates the process to compare the explanations, having an",
      "page": 200
    },
    {
      "caption": "Figure 11: 4 illustrates the process to compare explanations,",
      "page": 200
    },
    {
      "caption": "Figure 11: 3: Comparison of explanations for a bona ﬁde sample Ik, on the evaluation scenario x,",
      "page": 201
    },
    {
      "caption": "Figure 11: 4: Comparison of explanations for a presentation attack sample Ik, on the evaluation",
      "page": 201
    },
    {
      "caption": "Figure 11: 5. It shows one example of how to obtain the pairwise distances Dk given by: Dk = {d jh",
      "page": 202
    },
    {
      "caption": "Figure 11: 5: Pairwise comparison of explanations produced by the models in unseen-attack sce-",
      "page": 203
    },
    {
      "caption": "Figure 11: 6: Pairwise comparison of explanations produced by the models in unseen-attack sce-",
      "page": 203
    },
    {
      "caption": "Figure 11: 6 shows the process to obtain the",
      "page": 203
    },
    {
      "caption": "Figure 11: 7 presents the mean results (µ(Iµ)) and respective standard deviation (σ(Iµ)) of the",
      "page": 205
    },
    {
      "caption": "Figure 11: 8 and Fig. 11.9 show an example of a PA sample presenting a higher µ(Iµ) in the",
      "page": 205
    },
    {
      "caption": "Figure 11: 7: Image Average mean and standard deviation (StD) results for bona ﬁde (BF) and",
      "page": 206
    },
    {
      "caption": "Figure 11: 10, Unseen-Attack#5, consisting of",
      "page": 206
    },
    {
      "caption": "Figure 11: 8: Comparison of explanations in intraclass one-attack for an example PA sample of",
      "page": 207
    },
    {
      "caption": "Figure 11: 9: Comparison of explanations in intraclass unseen-attack for an example PA sample of",
      "page": 207
    },
    {
      "caption": "Figure 11: 10: Mean Aµ results for bona ﬁde and presentation attack samples in the one-attack",
      "page": 208
    },
    {
      "caption": "Figure 11: 11: Explanations for an example bona ﬁde samples with pairwise distance close to the",
      "page": 209
    },
    {
      "caption": "Figure 11: 12: Explanations for an example presentation attack sample of type #2 with pairwise",
      "page": 209
    },
    {
      "caption": "Figure 11: 13: Explanations for an example bona ﬁde sample with pairwise distance above the",
      "page": 210
    },
    {
      "caption": "Figure 11: 11 and Fig. 11.12 depict",
      "page": 210
    },
    {
      "caption": "Figure 11: 11, and the Unseen-Attacks#3,#4 in comparison to #1,#6) in Fig. 11.12.",
      "page": 210
    },
    {
      "caption": "Figure 11: 13 and Fig. 11.14).",
      "page": 210
    },
    {
      "caption": "Figure 11: 14: Explanations for an example presentation attack sample of type #7 with pairwise",
      "page": 211
    },
    {
      "caption": "Figure 11: 15: Bona ﬁde and presentation attack intraclass comparison mean and standard deviation",
      "page": 212
    },
    {
      "caption": "Figure 12: .1). Video",
      "page": 216
    },
    {
      "caption": "Figure 12: 2), proposed by He et al. [167], whose",
      "page": 216
    },
    {
      "caption": "Figure 12: 1: Illustration of the structure of the proposed method for audiovisual group emotion",
      "page": 217
    },
    {
      "caption": "Figure 12: 1) is composed of two main processes: fea-",
      "page": 217
    },
    {
      "caption": "Figure 12: 3), adapted from [296]. Its weighted-pooling strategy",
      "page": 217
    },
    {
      "caption": "Figure 12: 2: Structure of the video-based group emotion recognition module, based on an inﬂated",
      "page": 218
    },
    {
      "caption": "Figure 12: 3: Structure of the audio-based group emotion recognition module, based on a Bi-LSTM",
      "page": 219
    },
    {
      "caption": "Figure 12: 4. On the top left, is a short video of a calm conversation",
      "page": 223
    },
    {
      "caption": "Figure 12: 4: Some examples of validation set videos where the model offered unsuccessful pre-",
      "page": 224
    },
    {
      "caption": "Figure 13: 1): the visual submodule, which processes visual data;",
      "page": 228
    },
    {
      "caption": "Figure 13: 1: Diagram of the full multimodal pipeline for activity recognition.",
      "page": 229
    },
    {
      "caption": "Figure 13: 2) is composed of seventeen residual blocks, each",
      "page": 229
    },
    {
      "caption": "Figure 13: 2: Diagram of the visual submodule (more details on the ResNet-50 and the residual",
      "page": 230
    },
    {
      "caption": "Figure 13: 3), with less trainable parameters, re-",
      "page": 230
    },
    {
      "caption": "Figure 13: 3: Diagram of the audio submodule.",
      "page": 231
    },
    {
      "caption": "Figure 13: 4). Videos are acquired using a ﬁsh-eye camera to capture most of the interior of the",
      "page": 232
    },
    {
      "caption": "Figure 13: 4: Example frames from the in-vehicle dataset, depicting normal activities (top row)",
      "page": 233
    },
    {
      "caption": "Figure 13: 5: Rank accuracy results for the 21 selected classes from the MMIT database.",
      "page": 235
    },
    {
      "caption": "Figure 13: 6: Cascade results for the 21 selected classes from the MMIT database (overall classi-",
      "page": 235
    },
    {
      "caption": "Figure 13: 5). Beyond this relatively small accuracy",
      "page": 235
    },
    {
      "caption": "Figure 13: 6, the best accuracy of 55.30% is achieved with an audio-ﬁrst cascade",
      "page": 235
    },
    {
      "caption": "Figure 13: 7). With a conﬁdence score threshold of T = 0.3, this cascade strategy",
      "page": 236
    },
    {
      "caption": "Figure 13: 9). While this value is",
      "page": 236
    },
    {
      "caption": "Figure 13: 7: Rank accuracy results for the in-vehicle scenario with 42 classes.",
      "page": 237
    },
    {
      "caption": "Figure 13: 8: Cascade results in the in-vehicle scenario with 42 classes (overall classiﬁcation ac-",
      "page": 237
    },
    {
      "caption": "Figure 13: 9: Cascade results in the in-vehicle scenario with 3 classes (overall classiﬁcation ac-",
      "page": 238
    },
    {
      "caption": "Figure 14: 1: Comparison between the model training schemes of the original triplet loss and the",
      "page": 244
    },
    {
      "caption": "Figure 14: 1). Besides the",
      "page": 244
    },
    {
      "caption": "Figure 14: 2). Hence, the loss is computed through:",
      "page": 244
    },
    {
      "caption": "Figure 14: 2: Illustration of the expected results when training with the proposed Secure Triplet",
      "page": 245
    },
    {
      "caption": "Figure 14: 3: Architecture of the models used for ECG and face identity veriﬁcation (x denotes",
      "page": 247
    },
    {
      "caption": "Figure 14: 3) is adapted from the end-to-end architec-",
      "page": 247
    },
    {
      "caption": "Figure 14: 3) is based on the Inception-ResNet [420].",
      "page": 248
    },
    {
      "caption": "Figure 14: 4, the ﬁrst formulation of the Secure Triplet Loss (SecureTL), without consider-",
      "page": 251
    },
    {
      "caption": "Figure 14: 5. The model trained with the triplet loss attained 13.99% EER, which",
      "page": 251
    },
    {
      "caption": "Figure 14: 4: Detection Error Tradeoff (DET) curves for the ECG identity veriﬁcation model when",
      "page": 252
    },
    {
      "caption": "Figure 14: 5: Detection Error Tradeoff (DET) curves for the face identity veriﬁcation model when",
      "page": 252
    },
    {
      "caption": "Figure 14: 6: False match rate (FMR) and false non-match rate (FNMR) curves w.r.t. the dis-",
      "page": 253
    },
    {
      "caption": "Figure 14: 7: False match rate (FMR) and false non-match rate (FNMR) curves w.r.t. the dis-",
      "page": 254
    },
    {
      "caption": "Figure 14: 6 and Fig. 14.7, alongside the",
      "page": 255
    },
    {
      "caption": "Figure 14: 8. In both cases, the original formulation of the Secure Triplet Loss presents relatively high",
      "page": 255
    },
    {
      "caption": "Figure 14: 8: Template linkability analysis for the ECG and face identity veriﬁcation models (fol-",
      "page": 256
    },
    {
      "caption": "Figure 14: 4, and Fig. 14.5. Both with face and",
      "page": 257
    },
    {
      "caption": "Figure 14: 9 presents the EER and Dsys",
      "page": 257
    },
    {
      "caption": "Figure 14: 9: Results with the proposed loss when varying the γ parameter.",
      "page": 258
    },
    {
      "caption": "Figure 15: 1: Architecture of the ECG identity veriﬁcation model that was trained with the pro-",
      "page": 265
    },
    {
      "caption": "Figure 15: 2: Architecture of the face identity veriﬁcation model that was trained with the proposed",
      "page": 265
    },
    {
      "caption": "Figure 15: 1). The model receives two z-score normalised ﬁve-second raw ECG segments (a stored",
      "page": 266
    },
    {
      "caption": "Figure 15: 2), which receives two 224 × 224 RGB face images, normalised to [0,1] intensities, and",
      "page": 266
    },
    {
      "caption": "Figure 15: 3, in comparison with the baseline results. The Equal Error Rate values",
      "page": 268
    },
    {
      "caption": "Figure 15: 3: Comparison of the Receiver-Operating Characteristic curves on ECG identity veriﬁ-",
      "page": 269
    },
    {
      "caption": "Figure 15: 4: Comparison of the Receiver-Operating Characteristic curves on face veriﬁcation for",
      "page": 269
    },
    {
      "caption": "Figure 15: 5: Receiver-Operating Characteristic curve for negative selection error based on the",
      "page": 270
    },
    {
      "caption": "Figure 15: 4. When using entirely",
      "page": 270
    },
    {
      "caption": "Figure 15: 5) show that, although the performance worsens with fewer subjects, the errors have a",
      "page": 270
    },
    {
      "caption": "Figure 15: 6 and Fig. 15.7). However, that",
      "page": 270
    },
    {
      "caption": "Figure 15: 6: Receiver-Operating Characteristic curves for varying positive sample selection error",
      "page": 271
    },
    {
      "caption": "Figure 15: 7: Receiver-Operating Characteristic curves for varying negative sample selection error",
      "page": 271
    },
    {
      "caption": "Figure 15: 8). As studied by",
      "page": 271
    },
    {
      "caption": "Figure 15: 8: Equal Error Rate (EER) results when using more enrollment data from each subject.",
      "page": 272
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0": "6"
        },
        {
          "0": "12"
        },
        {
          "0": "0"
        },
        {
          "0": "22"
        },
        {
          "0": "5"
        },
        {
          "0": "3\n4\n2"
        },
        {
          "0": "28"
        },
        {
          "0": "26"
        },
        {
          "0": "0\n0\n2"
        },
        {
          "0": "12"
        }
      ],
      "page": 86
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "25 subj.\n100 subj.\n99.2\n97.3": "",
          "98.7": "",
          "98.2": "",
          "97.7": ""
        }
      ],
      "page": 110
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FNMR\nFMR": "EER"
        }
      ],
      "page": 253
    }
  ],
  "citations": [
    {
      "citation_id": "6",
      "title": "Interclass comparison in the unseen-attack scenario",
      "venue": "Interclass comparison in the unseen-attack scenario"
    },
    {
      "citation_id": "7",
      "title": "Explanations over an example five-second ECG segment from UofTDB",
      "venue": "Explanations over an example five-second ECG segment from UofTDB"
    },
    {
      "citation_id": "8",
      "title": "Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets of",
      "venue": "Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets of"
    },
    {
      "citation_id": "9",
      "title": "Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets of the UofTDB database",
      "venue": "Average explanations over heartbeat waveforms of subjects #1 and #2 on the subsets of the UofTDB database"
    },
    {
      "citation_id": "12",
      "title": "3 Schema of the architecture based on label refinement networks (LRN)",
      "venue": "3 Schema of the architecture based on label refinement networks (LRN)"
    },
    {
      "citation_id": "13",
      "title": "Example result of lead II to all conversion on the",
      "venue": "Example result of lead II to all conversion on the"
    },
    {
      "citation_id": "14",
      "title": "Example result of lead I to all conversion on the",
      "venue": "Example result of lead I to all conversion on the"
    },
    {
      "citation_id": "15",
      "title": "Example cross-database result of lead II to all conversion on INCART",
      "venue": "Example cross-database result of lead II to all conversion on INCART"
    },
    {
      "citation_id": "16",
      "title": "Example cross-database result of lead I to all conversion on INCART",
      "venue": "Example cross-database result of lead I to all conversion on INCART"
    },
    {
      "citation_id": "17",
      "title": "Example cross-database result of lead II to all conversion on PTB-XL",
      "venue": "Example cross-database result of lead II to all conversion on PTB-XL"
    },
    {
      "citation_id": "18",
      "title": "Example cross-database result of lead I to all conversion on PTB-XL",
      "venue": "Example cross-database result of lead I to all conversion on PTB-XL"
    },
    {
      "citation_id": "19",
      "title": "Stages of a biometric recognition algorithm based on face images",
      "venue": "Stages of a biometric recognition algorithm based on face images"
    },
    {
      "citation_id": "20",
      "title": "Examples of face detection in unconstrained settings",
      "venue": "Examples of face detection in unconstrained settings"
    },
    {
      "citation_id": "21",
      "title": "Evolution of face recognition approaches, from holistic to deep learning",
      "venue": "Evolution of face recognition approaches, from holistic to deep learning"
    },
    {
      "citation_id": "22",
      "title": "Recent history of face recognition, from deep learning to tailored objective functions",
      "venue": "Recent history of face recognition, from deep learning to tailored objective functions"
    },
    {
      "citation_id": "23",
      "title": "Example of how a mask can significantly occlude a face and limit the information that can be used by a face recognition algorithm",
      "venue": "Example of how a mask can significantly occlude a face and limit the information that can be used by a face recognition algorithm"
    },
    {
      "citation_id": "24",
      "title": "Illustration of how interpretability/explainability can be used to understand and improve a biometric model",
      "venue": "Illustration of how interpretability/explainability can be used to understand and improve a biometric model"
    },
    {
      "citation_id": "25",
      "title": "10.2 Expected effect of the proposed adapted triplet loss on the output embedding space",
      "venue": "10.2 Expected effect of the proposed adapted triplet loss on the output embedding space"
    },
    {
      "citation_id": "26",
      "title": "10.6 Receiver operating characteristic (ROC) curve for mask detection on the LFW dataset with simulated masks",
      "venue": "10.6 Receiver operating characteristic (ROC) curve for mask detection on the LFW dataset with simulated masks"
    },
    {
      "citation_id": "27",
      "title": "11.5 Pairwise comparison of explanations produced by the models in unseen-attack scenarios, for a bona fide sample I k",
      "venue": "11.5 Pairwise comparison of explanations produced by the models in unseen-attack scenarios, for a bona fide sample I k"
    },
    {
      "citation_id": "28",
      "title": "Electrocardiogram Lead Conversion from Single-Lead Blindly-Segmented Signals",
      "authors": [
        "S Beco",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "29",
      "title": "AUTOMOTIVE: A case study on AUTOmatic multiMOdal drowsiness detecTIon for smart VEhicles",
      "authors": [
        "T Esteves",
        "J Pinto",
        "P Ferreira",
        "P Costa",
        "L Rodrigues",
        "I Antunes",
        "G Lopes",
        "P Gamito",
        "A Abrantes",
        "P Jorge",
        "A Lourenço",
        "A Sequeira",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "An Exploratory Study of Interpretability for Face Presentation Attack Detection",
      "authors": [
        "A Sequeira",
        "T Gonçalves",
        "W Silva",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "31",
      "title": "Secure Triplet Loss: Achieving Cancelability and Non-Linkability in End-to-End Deep Biometrics",
      "authors": [
        "J Pinto",
        "M Correia",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "32",
      "title": "Evolution, Current Challenges, and Future Possibilities in ECG Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "Streamlining Action Recognition in Autonomous Shared Vehicles with an Audiovisual Cascade Strategy",
      "authors": [
        "J Pinto",
        "P Carvalho",
        "C Pinto",
        "A Sousa",
        "L Capozzi",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "17th International Conference on Computer Vision Theory and Applications (VISAPP)"
    },
    {
      "citation_id": "34",
      "title": "Audiovisual Classification of Group Emotion Valence Using Activity Recognition Networks",
      "authors": [
        "J Pinto",
        "T Gonçalves",
        "C Pinto",
        "L Sanhudo",
        "J Fonseca",
        "F Gonçalves",
        "P Carvalho",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "Fourth IEEE International Conference on Image Processing, Applications and Systems"
    },
    {
      "citation_id": "35",
      "title": "Explaining ECG Biometrics: Is It All In The QRS?",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG 2020)"
    },
    {
      "citation_id": "36",
      "title": "Self-Learning with Stochastic Triplet Loss",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "International Joint Conference on Neural Networks (IJCNN 2020)"
    },
    {
      "citation_id": "37",
      "title": "Secure Triplet Loss for End-to-End Deep Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "M Correia"
      ],
      "year": "2020",
      "venue": "th International Workshop on Biometrics and Forensics (IWBF 2020)"
    },
    {
      "citation_id": "38",
      "title": "Interpretable Biometrics: Should We Rethink How Presentation Attack Detection is Evaluated?",
      "authors": [
        "A Sequeira",
        "W Silva",
        "J Pinto",
        "T Gonçalves",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "\" in 8th International Workshop on Biometrics and Forensics"
    },
    {
      "citation_id": "39",
      "title": "An End-to-End Convolutional Neural Network for ECG-Based Biometric Authentication",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "10th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS 2019)"
    },
    {
      "citation_id": "40",
      "title": "Don't You Forget About Me: A Study on Long-Term Performance in ECG Biometrics",
      "authors": [
        "G Lopes",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "IbPRIA 2019: 9th Iberian Conference on Pattern Recognition and Image Analysis"
    },
    {
      "citation_id": "41",
      "title": "Interlead Conversion of Single-Lead Blindly-Segmented Electrocardiogram Signals",
      "authors": [
        "S Beco",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "17th International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB 2021)"
    },
    {
      "citation_id": "42",
      "title": "Deep Neural Networks for Biometric Identification Based on Non-Intrusive ECG Acquisitions",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço"
      ],
      "year": "2019",
      "venue": "The Biometric Computing: Recognition and Registration"
    },
    {
      "citation_id": "43",
      "title": "ECG Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "Encyclopedia of Cryptography, Security and Privacy"
    },
    {
      "citation_id": "44",
      "title": "27th Portuguese Conference on Pattern Recognition (RECPAD 2021)",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "27th Portuguese Conference on Pattern Recognition (RECPAD 2021)"
    },
    {
      "citation_id": "45",
      "title": "Achieving Cancellability in End-to-End Deep Biometrics with the Secure Triplet Loss",
      "authors": [
        "J Pinto",
        "M Correia",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "26th Portuguese Conference on Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Explainable Artificial Intelligence for Face Presentation Attack Detection",
      "authors": [
        "W Silva",
        "J Pinto",
        "T Gonçalves",
        "A Sequeira",
        "Jaime Cardoso"
      ],
      "year": "2020",
      "venue": "26th Portuguese Conference on Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Long-Term Performance of a Convolutional Neural Network for ECG-Based Biometrics",
      "authors": [
        "G Lopes",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2019",
      "venue": "25th Portuguese Conference on Pattern Recognition (RECPAD 2019)"
    },
    {
      "citation_id": "48",
      "title": "Improving ECG-Based Biometric Identification Using End-to-End Convolutional Networks",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço"
      ],
      "year": "2018",
      "venue": "24th Portuguese Conference on Pattern Recognition (RECPAD 2018)"
    },
    {
      "citation_id": "49",
      "title": "Beyond the aforementioned publications, the author has contributed to fourteen other scientific publications related to diverse pattern recognition and computer vision research topics not covered in this thesis",
      "venue": "Beyond the aforementioned publications, the author has contributed to fourteen other scientific publications related to diverse pattern recognition and computer vision research topics not covered in this thesis"
    },
    {
      "citation_id": "50",
      "title": "Explainable Biometrics in the Age of Deep Learning",
      "authors": [
        "P Neto",
        "T Gonçalves",
        "J Pinto",
        "W Silva",
        "A Sequeira",
        "A Ross",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "51",
      "title": "Toward Vehicle Occupant-Invariant Models for Activity Characterization",
      "authors": [
        "L Capozzi",
        "V Barbosa",
        "C Pinto",
        "J Pinto",
        "A Pereira",
        "P Carvalho",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "52",
      "title": "Beyond Masks: On the Generalization of Masked Face Recognition Models to Occluded Face Recognition",
      "authors": [
        "P Neto",
        "J Pinto",
        "F Boutros",
        "N Damer",
        "A Sequeira",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "53",
      "title": "Weakly-Supervised Classification of HER2 Expression in Breast Cancer Haematoxylin and Eosin Stained Slides",
      "authors": [
        "S Oliveira",
        "J Pinto",
        "T Gonçalves",
        "R Marques",
        "M Cardoso",
        "H Oliveira",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "54",
      "title": "IJCB OCFR 2022: Competition on Occluded Face Recognition From Synthetically Generated Structure-Aware Occlusions",
      "authors": [
        "P Neto",
        "F Boutros",
        "J Pinto",
        "N Damer",
        "A Sequeira",
        "J Cardoso",
        "M Bengherabi",
        "A Bousnat",
        "S Boucheta",
        "N Hebbadj",
        "B Yahya-Zoubir",
        "M Erakın",
        "U Demir",
        "H Ekenel",
        "P Vidal",
        "D Menotti"
      ],
      "year": "2022",
      "venue": "International Joint Conference on Biometrics (IJCB 2022)"
    },
    {
      "citation_id": "55",
      "title": "Impact of visual noise in activity recognition using deep neural networks -an experimental approach",
      "authors": [
        "L Capozzi",
        "P Carvalho",
        "A Sousa",
        "C Pinto",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "2nd International Conference on Pattern Recognition and Machine Learning (PRML 2021)"
    },
    {
      "citation_id": "56",
      "title": "End-to-End Deep Sketchto-Photo Matching Enforcing Realistic Photo Generation",
      "authors": [
        "L Capozzi",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "25th Iberoamerican Congress on Pattern Recognition (CIARP'21)"
    },
    {
      "citation_id": "57",
      "title": "Optimizing Person Re-Identification using Generated Attention Masks",
      "authors": [
        "L Capozzi",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "25th Iberoamerican Congress on Pattern Recognition (CIARP'21)"
    },
    {
      "citation_id": "58",
      "title": "Mixture-Based Open World Face Recognition",
      "authors": [
        "A Matta",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "9th World Conference on Information Systems and Technologies (World-CIST'21)"
    },
    {
      "citation_id": "59",
      "title": "A Uniform Performance Index for Ordinal Classification with Imbalanced Classes",
      "authors": [
        "W Silva",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2018",
      "venue": "International Joint Conference on Neural Networks (IJCNN 2018)"
    },
    {
      "citation_id": "60",
      "title": "Sketch-to-Photo Matching Enforcing Realistic Rendering Generation",
      "authors": [
        "L Capozzi",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "27th Portuguese Conference on Pattern Recognition (RECPAD 2021)"
    },
    {
      "citation_id": "61",
      "title": "IHC Classification in Breast Cancer H&E Slides with a Weakly-Supervised Approach",
      "authors": [
        "S Oliveira",
        "J Pinto",
        "T Gonçalves",
        "H Oliveira",
        "Jaime Cardoso"
      ],
      "year": "2020",
      "venue": "26th Portuguese Conference on Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Face Anti Spoofing: Handcrafted and Learned Features for Face Liveness Detection",
      "authors": [
        "P Costa",
        "P Silva",
        "J Pinto",
        "A Sequeira",
        "A Rebelo"
      ],
      "year": "2019",
      "venue": "25th Portuguese Conference on Pattern Recognition (RECPAD 2019)"
    },
    {
      "citation_id": "63",
      "title": "Fine Segmentation of Head and Torso Using Label Refinement Networks",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "25th Portuguese Conference on Pattern Recognition (RECPAD 2019)"
    },
    {
      "citation_id": "64",
      "title": "The research conducted during these doctoral studies has also been partially presented to the scientific community at the Doctoral Consortium of the",
      "year": "2019",
      "venue": "Inside Out: Fusing ECG and Face Information to Recognise Emotions"
    },
    {
      "citation_id": "65",
      "title": "Going 2D: Exploring Learnable Bidimensional Approaches for ECG Biometrics",
      "authors": [
        "Guilherme Augusto",
        "Tiritan Barbosa"
      ],
      "year": "2022",
      "venue": "Master in Bioengineering, Universidade do Porto -as co-supervisor"
    },
    {
      "citation_id": "66",
      "title": "Universidade do Porto -as external institution supervisor, alongside Jaime S. Cardoso (supervisor) and Ana F. Sequeira (cosupervisor)",
      "authors": [
        "Pedro Duarte Da Cunha",
        "Nunes Lopes"
      ],
      "year": "2022",
      "venue": "Master in Bioengineering"
    },
    {
      "citation_id": "67",
      "title": "Single-Wrist Electrocardiogram Acquisition Application in Biometrics",
      "authors": [
        "Erfan Omidvar"
      ],
      "year": "2022",
      "venue": "Master in Biomedical Engineering, Universidade do Porto -as second co-supervisor, alongside Miguel V. Correia (supervisor) and Duarte Dias"
    },
    {
      "citation_id": "68",
      "title": "Universidade do Porto -as external institution supervisor, alongside Jaime S. Cardoso (supervisor) and Pedro Carvalho (co-supervisor)",
      "authors": [
        "Vítor Hugo",
        "Pereira Barbosa"
      ],
      "year": "2022",
      "venue": "Master in Informatics and Computing Engineering"
    },
    {
      "citation_id": "69",
      "title": "-as external institution supervisor, alongside Ricardo Vigário (supervisor), André Lourenço (cosupervisor)",
      "authors": [
        "Telma Sofia",
        "Caldeira Esteves"
      ],
      "year": "2021",
      "venue": "-as external institution supervisor, alongside Ricardo Vigário (supervisor), André Lourenço (cosupervisor)"
    },
    {
      "citation_id": "70",
      "title": "Make My Heartbeat: Generation and Interlead Conversion of ECG Signals",
      "authors": [
        "Sofia Cardoso"
      ],
      "year": "2021",
      "venue": "Master in Bioengineering"
    },
    {
      "citation_id": "71",
      "title": "Figure 4.2: Illustration of the effects of the different data augmentation techniques on an example five-second ECG segment (for easier visualisation, the original segment was denoised with a bandpass filter 1-30 Hz and had its amplitude z-score normalised)",
      "venue": "Figure 4.2: Illustration of the effects of the different data augmentation techniques on an example five-second ECG segment (for easier visualisation, the original segment was denoised with a bandpass filter 1-30 Hz and had its amplitude z-score normalised)"
    },
    {
      "citation_id": "72",
      "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "authors": [
        "M Abadi",
        "A Agarwal",
        "P Barham",
        "E Brevdo",
        "Z Chen",
        "C Citro",
        "G Corrado",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "I Goodfellow",
        "A Harp",
        "G Irving",
        "M Isard",
        "Y Jia",
        "R Jozefowicz",
        "L Kaiser",
        "M Kudlur",
        "J Levenberg",
        "D Mané",
        "R Monga",
        "S Moore",
        "D Murray",
        "C Olah",
        "M Schuster",
        "J Shlens",
        "B Steiner",
        "I Sutskever",
        "K Talwar",
        "P Tucker",
        "V Vanhoucke",
        "V Vasudevan",
        "F Viégas",
        "O Vinyals",
        "P Warden",
        "M Wattenberg",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"
    },
    {
      "citation_id": "73",
      "title": "Biometric authentication based on PCG and ECG signals: present status and future directions. Signal, Image and Video Processing",
      "authors": [
        "M Abo-Zahhad",
        "S Ahmed",
        "S Abbas"
      ],
      "year": "2014",
      "venue": "Biometric authentication based on PCG and ECG signals: present status and future directions. Signal, Image and Video Processing",
      "doi": "10.1007/s11760-013-0593-4"
    },
    {
      "citation_id": "74",
      "title": "How to Do It Right: A Framework for Biometrics Supported Border Control",
      "authors": [
        "M Abomhara",
        "S Yayilgan",
        "A Nymoen",
        "M Shalaginova",
        "Z Székely",
        "O Elezaj"
      ],
      "year": "2019",
      "venue": "E-Democracy -Safeguarding Democracy and Human Rights in the Digital Age",
      "doi": "10.1007/978-3-030-37545-4_7"
    },
    {
      "citation_id": "75",
      "title": "A Review on Different Face Recognition Techniques",
      "authors": [
        "A Admane",
        "A Sheikh",
        "S Paunikar",
        "S Jawade",
        "S Wadbude",
        "M Sawarkar"
      ],
      "year": "2019",
      "venue": "International Journal of Scientific Research in Computer Science, Engineering and Information Technology",
      "doi": "10.32628/CSEIT195159"
    },
    {
      "citation_id": "76",
      "title": "Changing Perspectives: Interlead Conversion in Electrocardiographic Signals",
      "authors": [
        "C Afonso"
      ],
      "year": "2020",
      "venue": "Changing Perspectives: Interlead Conversion in Electrocardiographic Signals"
    },
    {
      "citation_id": "77",
      "title": "Attention Aware Wavelet-based Detection of Morphed Face Images",
      "authors": [
        "P Aghdaie",
        "B Chaudhary",
        "S Soleymani",
        "J Dawson",
        "N Nasrabadi"
      ],
      "year": "2021",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)",
      "doi": "10.1109/IJCB52358.2021.9484398"
    },
    {
      "citation_id": "78",
      "title": "ECG in Biometric Recognition: Time Dependency and Application Challenges",
      "authors": [
        "F Agrafioti"
      ],
      "year": "2011",
      "venue": "ECG in Biometric Recognition: Time Dependency and Application Challenges"
    },
    {
      "citation_id": "79",
      "title": "ECG Based Recognition Using Second Order Statistics",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos"
      ],
      "year": "2008",
      "venue": "Annual Communication Networks and Services Research Conference (CNSR)",
      "doi": "10.1109/CNSR.2008.38"
    },
    {
      "citation_id": "80",
      "title": "Heart Biometrics: Theory, Methods and Applications",
      "authors": [
        "F Agrafioti",
        "J Gao",
        "D Hatzinakos"
      ],
      "year": "2011",
      "venue": "Biometrics",
      "doi": "10.5772/18113"
    },
    {
      "citation_id": "81",
      "title": "Secure Telemedicine: Biometrics for Remote and Continuous Patient Verification",
      "authors": [
        "F Agrafioti",
        "F Bui",
        "D Hatzinakos"
      ],
      "year": "2012",
      "venue": "Journal of Computer Networks and Communications",
      "doi": "10.1155/2012/924791"
    },
    {
      "citation_id": "82",
      "title": "Preauthorized wearable biometric device, system and method for use thereof",
      "authors": [
        "F Agrafioti",
        "K Martin",
        "S Oung"
      ],
      "year": "2016",
      "venue": "Preauthorized wearable biometric device, system and method for use thereof"
    },
    {
      "citation_id": "83",
      "title": "Face description with local binary patterns: Application to face recognition",
      "authors": [
        "T Ahonen",
        "A Hadid",
        "M Pietikainen"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2006.244"
    },
    {
      "citation_id": "84",
      "title": "Heart-Based Biometrics and Possible Use of Heart Rate Variability in Biometric Recognition Systems",
      "authors": [
        "N Akhter",
        "S Tharewal",
        "V Kale",
        "A Bhalerao",
        "K Kale"
      ],
      "year": "2016",
      "venue": "Advanced Computing and Systems for Security",
      "doi": "10.1007/978-81-322-2650-5_2"
    },
    {
      "citation_id": "85",
      "title": "Analysis of Gender Inequality In Face Recognition Accuracy",
      "authors": [
        "V Albiero",
        "K Krishnapriya",
        "K Vangara",
        "K Zhang",
        "M King",
        "K Bowyer"
      ],
      "year": "2020",
      "venue": "IEEE Winter Applications of Computer Vision Workshops (WACVW)",
      "doi": "10.1109/WACVW50321.2020.9096947"
    },
    {
      "citation_id": "86",
      "title": "Gendered differences in face recognition accuracy explained by hairstyles, makeup, and facial morphology",
      "authors": [
        "V Albiero",
        "K Zhang",
        "M King",
        "K Bowyer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2021.3135750"
    },
    {
      "citation_id": "87",
      "title": "Single Heartbeat ECG Biometric Recognition using Convolutional Neural Network",
      "authors": [
        "D Alduwaile",
        "M Islam"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Science and Engineering (ICOASE)",
      "doi": "10.1109/ICOASE51841.2020.9436592"
    },
    {
      "citation_id": "88",
      "title": "BioFoV -an open platform for forensic video analysis and biometric data extraction",
      "authors": [
        "M Almeida",
        "P Correia",
        "P Larsen"
      ],
      "year": "2016",
      "venue": "International Conference on Biometrics and Forensics (IWBF)",
      "doi": "10.1109/IWBF.2016.7449693"
    },
    {
      "citation_id": "89",
      "title": "Partial FC: Training 10 Million Identities on a Single Machine",
      "authors": [
        "X An",
        "X Zhu",
        "Y Gao",
        "Y Xiao",
        "Y Zhao",
        "Z Feng",
        "L Wu",
        "B Qin",
        "M Zhang",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW54120.2021.00166"
    },
    {
      "citation_id": "90",
      "title": "Masked face recognition for secure authentication. arXiv",
      "authors": [
        "A Anwar",
        "A Raychowdhury"
      ],
      "year": "2020",
      "venue": "Masked face recognition for secure authentication. arXiv"
    },
    {
      "citation_id": "91",
      "title": "An anomaly detection approach to face spoofing detection: A new formulation and evaluation protocol",
      "authors": [
        "S Arashloo",
        "J Kittler",
        "W Christmas"
      ],
      "year": "2017",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2017.2729161"
    },
    {
      "citation_id": "92",
      "title": "Future of Face Recognition: A Review",
      "authors": [
        "S Arya",
        "N Pratap",
        "K Bhatia"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2015.08.076"
    },
    {
      "citation_id": "93",
      "title": "A Novel Neural-Network Model for Deriving Standard 12-Lead ECGs From Serial Three-Lead ECGs: Application to Self-Care",
      "authors": [
        "H Atoui",
        "J Fayn",
        "P Rubel"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine",
      "doi": "10.1109/TITB.2010.2047754"
    },
    {
      "citation_id": "94",
      "title": "Automotive Interior Sensing -Towards a Synergetic Approach between Anomaly Detection and Action Recognition Strategies",
      "authors": [
        "P Augusto",
        "J Cardoso",
        "J Fonseca"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Image Processing, Applications and Systems (IPAS)",
      "doi": "10.1109/IPAS50080.2020.9334942"
    },
    {
      "citation_id": "95",
      "title": "Personalised Gaming",
      "authors": [
        "S Bakkes",
        "C Tan",
        "Y Pisan"
      ],
      "year": "2013",
      "venue": "The Journal of Creative Technologies"
    },
    {
      "citation_id": "96",
      "title": "UMDFaces: An annotated face dataset for training deep networks",
      "authors": [
        "A Bansal",
        "A Nanduri",
        "C Castillo",
        "R Ranjan",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)",
      "doi": "10.1109/BTAS.2017.8272731"
    },
    {
      "citation_id": "97",
      "title": "Going 2D: Exploring Learnable Bidimensional Approaches for ECG Biometrics. Master's thesis",
      "authors": [
        "G Barbosa"
      ],
      "year": "2022",
      "venue": "Going 2D: Exploring Learnable Bidimensional Approaches for ECG Biometrics. Master's thesis"
    },
    {
      "citation_id": "98",
      "title": "Robust occupant action classification in shared autonomous vehicles",
      "authors": [
        "V Barbosa"
      ],
      "year": "2022",
      "venue": "Robust occupant action classification in shared autonomous vehicles"
    },
    {
      "citation_id": "99",
      "title": "Face Recognition: A Literature Review",
      "authors": [
        "N Barnouti",
        "S Al-Dabbagh",
        "W Matti"
      ],
      "year": "2016",
      "venue": "International Journal of Applied Information Systems",
      "doi": "10.5120/ijais2016451597"
    },
    {
      "citation_id": "100",
      "title": "Make My Heartbeat: Generation and Interlead Conversion of ECG Signals. Master's thesis",
      "authors": [
        "S Beco"
      ],
      "year": "2021",
      "venue": "Make My Heartbeat: Generation and Interlead Conversion of ECG Signals. Master's thesis"
    },
    {
      "citation_id": "101",
      "title": "Interlead Conversion of Single-Lead Blindly-Segmented Electrocardiogram Signals",
      "authors": [
        "S Beco",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB 2021)"
    },
    {
      "citation_id": "102",
      "title": "Electrocardiogram Lead Conversion from Single-Lead Blindly-Segmented Signals",
      "authors": [
        "S Beco",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "BMC Medical Informatics and Decision Making",
      "doi": "10.1186/s12911-022-02063-6"
    },
    {
      "citation_id": "103",
      "title": "ECG based human authentication using wavelets and random forests",
      "authors": [
        "N Belgacem",
        "A Nait-Ali",
        "R Fournier",
        "F Bereksi-Reguig"
      ],
      "year": "2012",
      "venue": "International Journal on Cryptography and Information Security",
      "doi": "10.5121/ijcis.2012.2201"
    },
    {
      "citation_id": "104",
      "title": "ECG Based Human Identification Using Random Forests",
      "authors": [
        "N Belgacem",
        "A Nait-Ali",
        "R Fournier",
        "F Bereksi-Reguig"
      ],
      "year": "2013",
      "venue": "International Conference on E-Technologies and Business on the Web (EBW)"
    },
    {
      "citation_id": "105",
      "title": "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection",
      "authors": [
        "P Belhumeur",
        "J Hespanha",
        "D Kriegman"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/34.598228"
    },
    {
      "citation_id": "106",
      "title": "Regularization and Semi-supervised Learning on Large Graphs",
      "authors": [
        "M Belkin",
        "I Matveeva",
        "P Niyogi"
      ],
      "venue": "Learning Theory"
    },
    {
      "citation_id": "107",
      "title": "",
      "authors": [
        "Heidelberg Springer Berlin"
      ],
      "year": "2004",
      "venue": "",
      "doi": "10.1007/978-3-540-27819-1_43"
    },
    {
      "citation_id": "108",
      "title": "ECG Biometrics Using Deep Learning and Relative Score Threshold Classification",
      "authors": [
        "D Belo",
        "N Bento",
        "H Silva",
        "A Fred",
        "H Gamboa"
      ],
      "year": "2020",
      "venue": "ECG Biometrics Using Deep Learning and Relative Score Threshold Classification",
      "doi": "10.3390/s20154078"
    },
    {
      "citation_id": "109",
      "title": "ECG based biometric identification using one-dimensional local difference pattern",
      "authors": [
        "M Benouis",
        "L Mostefai",
        "N Costen",
        "M Regouid"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2020.102226"
    },
    {
      "citation_id": "110",
      "title": "ECG Biometrics Using Spectrograms and Deep Neural Networks",
      "authors": [
        "N Bento",
        "D Belo",
        "H Gamboa"
      ],
      "year": "2020",
      "venue": "International Journal of Machine Learning and Computing",
      "doi": "10.18178/ijmlc.2020.10.2.929"
    },
    {
      "citation_id": "111",
      "title": "The challenge of face recognition from digital point-and-shoot cameras",
      "authors": [
        "J Beveridge",
        "P Phillips",
        "D Bolme",
        "B Draper",
        "G Givens",
        "Y Lui",
        "M Teli",
        "H Zhang",
        "W Scruggs",
        "K Bowyer",
        "P Flynn",
        "S Cheng"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS.2013.6712704"
    },
    {
      "citation_id": "112",
      "title": "Recent Advances in Face Presentation Attack Detection",
      "authors": [
        "S Bhattacharjee",
        "A Mohammadi",
        "A Anjos",
        "S Marcel"
      ],
      "year": "2019",
      "venue": "Recent Advances in Face Presentation Attack Detection",
      "doi": "10.1007/978-3-319-92627-8_10"
    },
    {
      "citation_id": "113",
      "title": "Thermal Infrared Face Recognition -A Biometric Identification Technique for Robust Security system",
      "authors": [
        "M Bhowmik",
        "K Saha",
        "S Majumder",
        "G Majumder",
        "A Saha",
        "A Sarma",
        "D Bhattacharjee",
        "D Basu",
        "M Nasipuri"
      ],
      "year": "2011",
      "venue": "Reviews, Refinements and New Ideas in Face Recognition",
      "doi": "10.5772/18986"
    },
    {
      "citation_id": "114",
      "title": "ECG analysis: a new approach in human identification",
      "authors": [
        "L Biel",
        "O Pettersson",
        "L Philipson",
        "P Wide"
      ],
      "year": "1999",
      "venue": "IEEE Instrumentation and Measurement Technology Conference (IMTC)",
      "doi": "10.1109/IMTC.1999.776813"
    },
    {
      "citation_id": "115",
      "title": "ECG analysis: a new approach in human identification",
      "authors": [
        "L Biel",
        "O Pettersson",
        "L Philipson",
        "P Wide"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "doi": "10.1109/19.930458"
    },
    {
      "citation_id": "116",
      "title": "Learning from Labeled and Unlabeled Data using Graph Mincuts",
      "authors": [
        "A Blum",
        "S Chawla"
      ],
      "year": "2001",
      "venue": "International Conference on Machine Learning (ICML)",
      "doi": "10.5555/645530.757779"
    },
    {
      "citation_id": "117",
      "title": "Secure face matching using fully homomorphic encryption",
      "authors": [
        "V Boddeti"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS)",
      "doi": "10.1109/BTAS.2018.8698601"
    },
    {
      "citation_id": "118",
      "title": "Guide to Biometrics",
      "authors": [
        "R Bolle",
        "J Connell",
        "S Pankanti",
        "N Ratha",
        "A Senior"
      ],
      "year": "2004",
      "venue": "Guide to Biometrics",
      "doi": "10.1007/978-1-4757-4036-3"
    },
    {
      "citation_id": "119",
      "title": "ECG personal identification in subspaces using radial basis neural networks",
      "authors": [
        "O Boumbarov",
        "Y Velchev",
        "S Sokolov"
      ],
      "year": "2009",
      "venue": "IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)",
      "doi": "10.1109/IDAACS.2009.5342942"
    },
    {
      "citation_id": "120",
      "title": "Monitoring of a driver's heart rate using a microwave sensor and template-matching algorithm",
      "authors": [
        "S Bounyong",
        "M Yoshioka",
        "J Ozawa"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Consumer Electronics (ICCE)",
      "doi": "10.1109/ICCE.2017.7889222"
    },
    {
      "citation_id": "121",
      "title": "Nutzung der EKG-Signaldatenbank CAR-DIODAT der PTB über das Internet",
      "authors": [
        "R Bousseljot",
        "D Kreiseler",
        "A Schnabel"
      ],
      "year": "1995",
      "venue": "Biomedizinische Technik",
      "doi": "10.1515/bmte.1995.40.s1.317"
    },
    {
      "citation_id": "122",
      "title": "International Joint Conference on Biometrics (IJCB 2021)",
      "authors": [
        "F Boutros",
        "N Damer",
        "J Kolf",
        "K Raja",
        "F Kirchbuchner",
        "R Ramachandra",
        "A Kuijper",
        "P Fang",
        "C Zhang",
        "F Wang",
        "D Montero",
        "N Aginako",
        "B Sierra",
        "M Nieto",
        "M Erakin",
        "U Demir",
        "H Kemal",
        "A Ekenel",
        "K Kataoka",
        "S Ichikawa",
        "J Kubo",
        "M Zhang",
        "D He",
        "S Han",
        "K Shan",
        "V Grm",
        "S Štruc",
        "N Seneviratne",
        "S Kasthuriarachchi",
        "P Rasnayaka",
        "A Neto",
        "J Sequeira",
        "M Pinto",
        "J Saffari",
        "Cardoso"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Biometrics (IJCB 2021)",
      "doi": "10.1109/IJCB52358.2021.9484337"
    },
    {
      "citation_id": "123",
      "title": "ElasticFace: Elastic Margin Loss for Deep Face Recognition",
      "authors": [
        "F Boutros",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "124",
      "title": "Self-restrained triplet loss for accurate masked face recognition",
      "authors": [
        "F Boutros",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2022",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2021.108473"
    },
    {
      "citation_id": "125",
      "title": "Fully Homomorphic Encryption from Ring-LWE and Security for Key Dependent Messages",
      "authors": [
        "Z Brakerski",
        "V Vaikuntanathan"
      ],
      "year": "2011",
      "venue": "Advances in Cryptology: Annual Cryptology Conference (CRYPTO)",
      "doi": "10.1007/978-3-642-22792-9_29"
    },
    {
      "citation_id": "126",
      "title": "ECG biometric identification: A compression based approach",
      "authors": [
        "S Brás",
        "A Pinho"
      ],
      "year": "2015",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "doi": "10.1109/EMBC.2015.7319719"
    },
    {
      "citation_id": "127",
      "title": "Biometrics systems and technologies: A survey",
      "authors": [
        "I Buciu",
        "A Gacsadi"
      ],
      "year": "2016",
      "venue": "International Journal of Computers Communications & Control",
      "doi": "10.15837/ijccc.2016.3.2556"
    },
    {
      "citation_id": "128",
      "title": "Ensemble Deep Learning Models for ECGbased Biometrics",
      "authors": [
        "Y.-H Byeon",
        "S.-B Pan",
        "K.-C Kwak"
      ],
      "year": "2020",
      "venue": "Cybernetics & Informatics (K&I)",
      "doi": "10.1109/KI48306.2020.9039871"
    },
    {
      "citation_id": "129",
      "title": "Real-time electrocardiogram streams for continuous authentication",
      "authors": [
        "C Camara",
        "P Peris-Lopez",
        "L Gonzalez-Manzano",
        "J Tapiador"
      ],
      "year": "2018",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2017.07.032"
    },
    {
      "citation_id": "130",
      "title": "VGGFace2: A Dataset for Recognising Faces across Pose and Age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition (FG)",
      "doi": "10.1109/FG.2018.00020"
    },
    {
      "citation_id": "131",
      "title": "Face recognition with learning-based descriptor",
      "authors": [
        "Z Cao",
        "Q Yin",
        "X Tang",
        "J Sun"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2010.5539992"
    },
    {
      "citation_id": "132",
      "title": "End-to-end deep sketch-tophoto matching enforcing realistic photo generation",
      "authors": [
        "L Capozzi",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "Iberoamerican Congress on Pattern Recognition (CIARP)",
      "doi": "10.1007/978-3-030-93420-0_42"
    },
    {
      "citation_id": "133",
      "title": "Optimizing person re-identification using generated attention masks",
      "authors": [
        "L Capozzi",
        "J Pinto",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "Iberoamerican Congress on Pattern Recognition (CIARP)",
      "doi": "10.1007/978-3-030-93420-0_24"
    },
    {
      "citation_id": "134",
      "title": "Face Recognition For Forensic Applications: Methods for Matching Facial Sketches to Mugshot Pictures. Master's thesis",
      "authors": [
        "L Capozzi"
      ],
      "year": "2020",
      "venue": "Face Recognition For Forensic Applications: Methods for Matching Facial Sketches to Mugshot Pictures. Master's thesis"
    },
    {
      "citation_id": "135",
      "title": "Impact of visual noise in activity recognition using deep neural networks -an experimental approach",
      "authors": [
        "L Capozzi",
        "P Carvalho",
        "A Sousa",
        "C Pinto",
        "J Pinto",
        "J Cardoso"
      ],
      "venue": "International Conference on Pattern Recognition and Machine Learning (PRML)",
      "doi": "10.1109/PRML52754.2021.9520734"
    },
    {
      "citation_id": "136",
      "title": "Toward vehicle occupant-invariant models for activity characterization",
      "authors": [
        "L Capozzi",
        "V Barbosa",
        "C Pinto",
        "J Pinto",
        "A Pereira",
        "P Carvalho",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2022.3210973"
    },
    {
      "citation_id": "137",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.502"
    },
    {
      "citation_id": "138",
      "title": "Evaluating Template Uniqueness in ECG Biometrics",
      "authors": [
        "C Carreiras",
        "A Lourenço",
        "H Silva",
        "A Fred",
        "R Ferreira"
      ],
      "year": "2016",
      "venue": "International Conference on Informatics in Control, Automation and Robotics (ICINCO)",
      "doi": "10.1007/978-3-319-26453-0_7"
    },
    {
      "citation_id": "139",
      "title": "Machine Learning Interpretability: A Survey on Methods and Metrics. Electronics",
      "authors": [
        "D Carvalho",
        "E Pereira",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "Machine Learning Interpretability: A Survey on Methods and Metrics. Electronics",
      "doi": "10.3390/electronics8080832"
    },
    {
      "citation_id": "140",
      "title": "Compression-Based Classification of ECG Using First-Order Derivatives",
      "authors": [
        "J Carvalho",
        "S Brás",
        "A Pinho"
      ],
      "year": "2019",
      "venue": "EAI International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)",
      "doi": "10.1007/978-3-030-16447-8_3"
    },
    {
      "citation_id": "141",
      "title": "Wavelet Distance Measure for Person Identification Using Electrocardiograms",
      "authors": [
        "A Chan",
        "M Hamdy",
        "A Badre",
        "V Badee"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "doi": "10.1109/TIM.2007.909996"
    },
    {
      "citation_id": "142",
      "title": "Semi-supervised learning",
      "authors": [
        "O Chapelle",
        "B Schölkopf",
        "A Zien"
      ],
      "year": "2006",
      "venue": "Semi-supervised learning"
    },
    {
      "citation_id": "143",
      "title": "Return of the devil in the details: Delving deep into convolutional nets",
      "authors": [
        "K Chatfield",
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "144",
      "title": "Large scale online learning of image similarity through ranking",
      "authors": [
        "G Chechik",
        "V Sharma",
        "U Shalit",
        "S Bengio"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "145",
      "title": "Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-identification",
      "authors": [
        "W Chen",
        "X Chen",
        "J Zhang",
        "K Huang"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.145"
    },
    {
      "citation_id": "146",
      "title": "Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function",
      "authors": [
        "D Cheng",
        "Y Gong",
        "S Zhou",
        "J Wang",
        "N Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.149"
    },
    {
      "citation_id": "147",
      "title": "Dry-Contact and Noncontact Biopotential Electrodes: Methodological Review",
      "authors": [
        "Y Chi",
        "T Jung",
        "G Cauwenberghs"
      ],
      "year": "2010",
      "venue": "IEEE Reviews in Biomedical Engineering",
      "doi": "10.1109/RBME.2010.2084078"
    },
    {
      "citation_id": "148",
      "title": "",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "149",
      "title": "Feature fusions for 2.5D face recognition in Random Maxout Extreme Learning Machine",
      "authors": [
        "L Chong",
        "T Ong",
        "A Teoh"
      ],
      "year": "2019",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2018.11.024"
    },
    {
      "citation_id": "150",
      "title": "A novel unified framework for noise-robust ECGbased biometric authentication",
      "authors": [
        "T Choudhary",
        "M Manikandan"
      ],
      "year": "2015",
      "venue": "International Conference on Signal Processing and Integrated Networks (SPIN)",
      "doi": "10.1109/SPIN.2015.7095379"
    },
    {
      "citation_id": "151",
      "title": "ECG Authentication Method Based on Parallel Multi-Scale One-Dimensional Residual Network With Center and Margin Loss",
      "authors": [
        "Y Chu",
        "H Shen",
        "K Huang"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2912519"
    },
    {
      "citation_id": "152",
      "title": "Single pulse ECG-based small scale user authentication using guided filtering",
      "authors": [
        "S Chun"
      ],
      "year": "2016",
      "venue": "International Conference on Biometrics (ICB)",
      "doi": "10.1109/ICB.2016.7550065"
    },
    {
      "citation_id": "153",
      "title": "Effectiveness of forward collision warning and autonomous emergency braking systems in reducing front-to-rear crash rates",
      "authors": [
        "J Cicchino"
      ],
      "year": "2017",
      "venue": "Accident Analysis & Prevention",
      "doi": "10.1016/j.aap.2016.11.009"
    },
    {
      "citation_id": "154",
      "title": "Effects of blind spot monitoring systems on police-reported lane-change crashes",
      "authors": [
        "J Cicchino"
      ],
      "year": "2018",
      "venue": "Traffic Injury Prevention",
      "doi": "10.1080/15389588.2018.1476973"
    },
    {
      "citation_id": "155",
      "title": "Off-the-person ECG Biometrics Using Convolutional Neural Networks",
      "authors": [
        "I Ciocoiu",
        "N Cleju"
      ],
      "year": "2019",
      "venue": "International Symposium on Signals, Circuits and Systems (ISSCS)",
      "doi": "10.1109/ISSCS.2019.8801783"
    },
    {
      "citation_id": "156",
      "title": "Off-Person ECG Biometrics Using Spatial Representations and Convolutional Neural Networks",
      "authors": [
        "I Ciocoiu",
        "N Cleju"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3042547"
    },
    {
      "citation_id": "157",
      "title": "Deep Learning for Classroom Activity Detection from Audio",
      "authors": [
        "R Cosbey",
        "A Wusterbarth",
        "B Hutchinson"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2019.8683365"
    },
    {
      "citation_id": "158",
      "title": "One-Lead ECG-based Personal Identification Using Ziv-Merhav Cross Parsing",
      "authors": [
        "D Coutinho",
        "A Fred",
        "M Figueiredo"
      ],
      "year": "2010",
      "venue": "International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR.2010.940"
    },
    {
      "citation_id": "159",
      "title": "ECG-based continuous authentication system using adaptive string matching",
      "authors": [
        "D Coutinho",
        "A Fred",
        "M Figueiredo"
      ],
      "year": "2011",
      "venue": "International Conference on Bio-Inspired Systems and Signal Processing (BIOSIGNALS)",
      "doi": "10.5220/0003292003540359"
    },
    {
      "citation_id": "160",
      "title": "Novel fiducial and nonfiducial approaches to electrocardiogram-based biometric systems",
      "authors": [
        "D Coutinho",
        "H Silva",
        "H Gamboa",
        "A Fred",
        "M Figueiredo"
      ],
      "year": "2013",
      "venue": "IET Biometrics",
      "doi": "10.1049/iet-bmt.2012.0055"
    },
    {
      "citation_id": "161",
      "title": "Vision for man-machine interaction",
      "authors": [
        "J Crowley"
      ],
      "year": "1997",
      "venue": "Robotics and Autonomous Systems",
      "doi": "10.1016/S0921-8890(96)00061-9"
    },
    {
      "citation_id": "162",
      "title": "Learning Augmentation Strategies From Data",
      "authors": [
        "E Cubuk",
        "B Zoph",
        "D Mane",
        "V Vasudevan",
        "Q Le",
        "Autoaugment"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.00020"
    },
    {
      "citation_id": "163",
      "title": "Using Multimodal Biometrics to Secure Vehicles",
      "authors": [
        "K Daimi",
        "N Hazzazi",
        "M Saed",
        "K Daimi",
        "H Arabnia",
        "L Deligiannidis",
        "M.-S Hwang",
        "F Tinetti"
      ],
      "year": "2021",
      "venue": "Advances in Security, Networks, and Internet of Things",
      "doi": "10.1007/978-3-030-71017-0_41"
    },
    {
      "citation_id": "164",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2005.177"
    },
    {
      "citation_id": "165",
      "title": "The Effect of Wearing a Mask on Face Recognition Performance: An Exploratory Study",
      "authors": [
        "N Damer",
        "J Grebe",
        "C Chen",
        "F Boutros",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2020",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG), volume P-306 of LNI"
    },
    {
      "citation_id": "166",
      "title": "Masked face recognition: Human versus machine",
      "authors": [
        "N Damer",
        "F Boutros",
        "M Süßmilch",
        "M Fang",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "venue": "IET Biometrics",
      "doi": "10.1049/bme2.12077"
    },
    {
      "citation_id": "167",
      "title": "ECG Based Biometric Identification for Population with Normal and Cardiac Anomalies Using Hybrid HRV and DWT Features",
      "authors": [
        "M Dar",
        "M Akram",
        "A Shaukat",
        "M Khan"
      ],
      "year": "2015",
      "venue": "International Conference on IT Convergence and Security (ICITCS)",
      "doi": "10.1109/ICITCS.2015.7292977"
    },
    {
      "citation_id": "168",
      "title": "ECG biometric identification for general population using multiresolution analysis of DWT based features",
      "authors": [
        "M Dar",
        "M Akram",
        "A Usman",
        "S Khan"
      ],
      "year": "2015",
      "venue": "International Conference on Information Security and Cyber Forensics (InfoSec)",
      "doi": "10.1109/InfoSec.2015.7435498"
    },
    {
      "citation_id": "169",
      "title": "Emotion in Games, chapter 21",
      "authors": [
        "C De Melo",
        "A Paiva",
        "J Gratch"
      ],
      "year": "2014",
      "venue": "Emotion in Games, chapter 21",
      "doi": "10.1002/9781118796443.ch21"
    },
    {
      "citation_id": "170",
      "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.00482"
    },
    {
      "citation_id": "171",
      "title": "RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00525"
    },
    {
      "citation_id": "172",
      "title": "Masked Face Recognition Challenge: The InsightFace Track Report",
      "authors": [
        "J Deng",
        "J Guo",
        "X An",
        "Z Zhu",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW54120.2021.00165"
    },
    {
      "citation_id": "173",
      "title": "EmotiW 2020: Driver Gaze, Group Emotion, Student Engagement and Physiological Signal based Challenges",
      "authors": [
        "A Dhall",
        "G Sharma",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3382507.3417973"
    },
    {
      "citation_id": "174",
      "title": "Robust Face Recognition via Multimodal Deep Face Representation",
      "authors": [
        "C Ding",
        "D Tao"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2015.2477042"
    },
    {
      "citation_id": "175",
      "title": "Trunk-Branch Ensemble Convolutional Neural Networks for Video-Based Face Recognition",
      "authors": [
        "C Ding",
        "D Tao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2017.2700390"
    },
    {
      "citation_id": "176",
      "title": "Sheep, Goats, Lambs and Wolves: A Statistical Analysis of Speaker Performance in the NIST 1998 Speaker Recognition Evaluation",
      "authors": [
        "G Doddington",
        "W Liggett",
        "A Martin",
        "M Przybocki",
        "D Reynolds"
      ],
      "year": "1998",
      "venue": "Sheep, Goats, Lambs and Wolves: A Statistical Analysis of Speaker Performance in the NIST 1998 Speaker Recognition Evaluation"
    },
    {
      "citation_id": "177",
      "title": "ECG-based identity recognition via deterministic learning",
      "authors": [
        "X Dong",
        "W Si",
        "W Huang"
      ],
      "year": "2018",
      "venue": "Biotechnology & Biotechnological Equipment",
      "doi": "10.1080/13102818.2018.1428500"
    },
    {
      "citation_id": "178",
      "title": "A comparison of multivariate mutual information estimators for feature selection",
      "authors": [
        "G Doquire",
        "M Verleysen"
      ],
      "year": "2012",
      "venue": "International Conference on Pattern Recognition Applications and Methods (ICPRAM)",
      "doi": "10.5220/0003726101760185"
    },
    {
      "citation_id": "179",
      "title": "Towards a rigorous science of interpretable machine learning. arXiv",
      "authors": [
        "F Doshi-Velez",
        "B Kim"
      ],
      "year": "2017",
      "venue": "Towards a rigorous science of interpretable machine learning. arXiv"
    },
    {
      "citation_id": "180",
      "title": "Privacy-Preserving Indexing of Iris-Codes with Cancelable Bloom Filter-based Search Structures",
      "authors": [
        "P Drozdowski",
        "S Garg",
        "C Rathgeb",
        "M Gomez-Barrero",
        "D Chang",
        "C Busch"
      ],
      "year": "2018",
      "venue": "European Signal Processing Conference (EUSIPCO)",
      "doi": "10.23919/EUSIPCO.2018.8553053"
    },
    {
      "citation_id": "181",
      "title": "On the Application of Homomorphic Encryption to Face Identification",
      "authors": [
        "P Drozdowski",
        "N Buchmann",
        "C Rathgeb",
        "M Margraf",
        "C Busch"
      ],
      "year": "2019",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "182",
      "title": "Broken Hearted: How To Attack ECG Biometrics",
      "authors": [
        "S Eberz",
        "N Paoletti",
        "M Roeschlin",
        "M Kwiatkowska",
        "I Martinovic"
      ],
      "year": "2017",
      "venue": "Network and Distributed System Security Symposium (NDSS)",
      "doi": "10.14722/ndss.2017.23408"
    },
    {
      "citation_id": "183",
      "title": "ECG-based Biometrics using a Deep Autoencoder for Feature Learning: An Empirical Study on Transferability",
      "authors": [
        "A Eduardo",
        "H Aidos",
        "A Fred"
      ],
      "year": "2017",
      "venue": "International Conference on Pattern Recognition Applications and Methods (ICPRAM)",
      "doi": "10.5220/0006195404630470"
    },
    {
      "citation_id": "184",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "185",
      "title": "A single scan algorithm for QRS-detection and feature extraction",
      "authors": [
        "W Engelse",
        "C Zeelenberg"
      ],
      "year": "1979",
      "venue": "Computers in Cardiology"
    },
    {
      "citation_id": "186",
      "title": "ECG based biometric authentication using ensemble of features",
      "authors": [
        "S Ergin",
        "A Uysal",
        "E Gunal",
        "S Gunal",
        "M Gulmezoglu"
      ],
      "year": "2014",
      "venue": "Iberian Conference on Information Systems and Technologies (CISTI)",
      "doi": "10.1109/CISTI.2014.6877089"
    },
    {
      "citation_id": "187",
      "title": "Sleepy Drivers: Drowsiness Monitoring Using ECG and Face Video",
      "authors": [
        "T Esteves"
      ],
      "year": "2021",
      "venue": "Sleepy Drivers: Drowsiness Monitoring Using ECG and Face Video"
    },
    {
      "citation_id": "188",
      "title": "AUTOMOTIVE: A case study on AUTOmatic multiMOdal drowsiness de-tecTIon for smart VEhicles",
      "authors": [
        "T Esteves",
        "J Pinto",
        "P Ferreira",
        "P Costa",
        "L Rodrigues",
        "I Antunes",
        "G Lopes",
        "P Gamito",
        "A Abrantes",
        "P Jorge",
        "A Lourenço",
        "A Sequeira",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3128016"
    },
    {
      "citation_id": "189",
      "title": "Real-time Speech and Music Classification by Large Audio Feature Space Extraction",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "Real-time Speech and Music Classification by Large Audio Feature Space Extraction",
      "doi": "10.1007/978-3-319-27299-3"
    },
    {
      "citation_id": "190",
      "title": "Somewhat practical fully homomorphic encryption. IACR Cryptology ePrint Archive",
      "authors": [
        "J Fan",
        "F Vercauteren"
      ],
      "year": "2012",
      "venue": "Somewhat practical fully homomorphic encryption. IACR Cryptology ePrint Archive"
    },
    {
      "citation_id": "191",
      "title": "DADA: Driver Attention Prediction in Driving Accident Scenarios",
      "authors": [
        "J Fang",
        "D Yan",
        "J Qiao",
        "J Xue",
        "H Yu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": "10.1109/TITS.2020.3044678"
    },
    {
      "citation_id": "192",
      "title": "Real Masks and Spoof Faces: On the Masked Face Presentation Attack Detection",
      "authors": [
        "M Fang",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2022",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2021.108398"
    },
    {
      "citation_id": "193",
      "title": "Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection",
      "authors": [
        "M Fang",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV51458.2022.00120"
    },
    {
      "citation_id": "194",
      "title": "Human identification by quantifying similarity and dissimilarity in electrocardiogram phase space",
      "authors": [
        "S.-C Fang",
        "H.-L Chan"
      ],
      "year": "2009",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2008.11.020"
    },
    {
      "citation_id": "195",
      "title": "Near infrared face recognition: A literature survey",
      "authors": [
        "S Farokhi",
        "J Flusser",
        "U Sheikh"
      ],
      "year": "2016",
      "venue": "Computer Science Review",
      "doi": "10.1016/j.cosrev.2016.05.003"
    },
    {
      "citation_id": "196",
      "title": "A new ECG feature extractor for biometric recognition",
      "authors": [
        "S Fatemian",
        "D Hatzinakos"
      ],
      "year": "2009",
      "venue": "International Conference on Digital Signal Processing (ICDSP)",
      "doi": "10.1109/ICDSP.2009.5201143"
    },
    {
      "citation_id": "197",
      "title": "HeartID: Cardiac biometric recognition",
      "authors": [
        "S Fatemian",
        "F Agrafioti",
        "D Hatzinakos"
      ],
      "year": "2010",
      "venue": "IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS.2010.5634493"
    },
    {
      "citation_id": "198",
      "title": "Combining multiple one-class classifiers for anomaly based face spoofing attack detection",
      "authors": [
        "S Fatemifar",
        "M Awais",
        "S Arashloo",
        "J Kittler"
      ],
      "year": "2019",
      "venue": "International Conference on Biometrics (ICB)",
      "doi": "10.1109/ICB45273.2019.8987326"
    },
    {
      "citation_id": "199",
      "title": "SlowFast Networks for Video Recognition",
      "authors": [
        "C Feichtenhofer",
        "H Fan",
        "J Malik",
        "K He"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00630"
    },
    {
      "citation_id": "200",
      "title": "Self-Supervised Representation Learning by Rotation Feature Decoupling",
      "authors": [
        "Z Feng",
        "C Xu",
        "D Tao"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.01061"
    },
    {
      "citation_id": "201",
      "title": "Self-Supervised Video Representation Learning With Odd-One-Out Networks",
      "authors": [
        "B Fernando",
        "H Bilen",
        "E Gavves",
        "S Gould"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.607"
    },
    {
      "citation_id": "202",
      "title": "Head Pose Estimation for Facial Biometric Recognition Systems",
      "authors": [
        "J Ferreira"
      ],
      "year": "2020",
      "venue": "Head Pose Estimation for Facial Biometric Recognition Systems"
    },
    {
      "citation_id": "203",
      "title": "Physiological Inspired Deep Neural Networks for Emotion Recognition",
      "authors": [
        "P Ferreira",
        "F Marques",
        "J Cardoso",
        "A Rebelo"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2870063"
    },
    {
      "citation_id": "204",
      "title": "Adversarial learning for a robust iris presentation attack detection method against unseen attack presentations",
      "authors": [
        "P Ferreira",
        "A Sequeira",
        "D Pernes",
        "A Rebelo",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "205",
      "title": "Individual identification via electrocardiogram analysis",
      "authors": [
        "A Fratini",
        "M Sansone",
        "P Bifulco",
        "M Cesarelli"
      ],
      "year": "2015",
      "venue": "BioMedical Engineering OnLine",
      "doi": "10.1186/s12938-015-0072-y"
    },
    {
      "citation_id": "206",
      "title": "Template Editing and Replacement: Novel Methods for Biometric Template Selection and Update",
      "authors": [
        "B Freni"
      ],
      "year": "2010",
      "venue": "Template Editing and Replacement: Novel Methods for Biometric Template Selection and Update"
    },
    {
      "citation_id": "207",
      "title": "Seeing People in the Dark: Face Recognition in Infrared Images",
      "authors": [
        "G Friedrich",
        "Y Yeshurun"
      ],
      "year": "2002",
      "venue": "International Workshop on Biologically Motivated Computer Vision (BMCV)",
      "doi": "10.1007/3-540-36181-2_35"
    },
    {
      "citation_id": "208",
      "title": "The effect of wearing a face mask on face image quality",
      "authors": [
        "B Fu",
        "F Kirchbuchner",
        "N Damer"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG52635.2021.9667088"
    },
    {
      "citation_id": "209",
      "title": "Biometrics and Policing: A Protocol for Multichannel Sensor Data Collection and Exploratory Analysis of Contextualized Psychophysiological Response During Law Enforcement Operations",
      "authors": [
        "R Furberg",
        "T Taniguchi",
        "B Aagaard",
        "A Ortiz",
        "M Hegarty-Craver",
        "K Gilchrist",
        "T Ridenour"
      ],
      "year": "2017",
      "venue": "JMIR Research Protocols",
      "doi": "10.2196/resprot.7499"
    },
    {
      "citation_id": "210",
      "title": "Vulnerabilities in Biometric Systems: Attacks and Recent Advances in Liveness Detection",
      "authors": [
        "J Galbally",
        "J Fierrez",
        "J Ortega-Garcia"
      ],
      "year": "2007",
      "venue": "Spanish Workshop on Biometrics (SWB 2007)"
    },
    {
      "citation_id": "211",
      "title": "A comparison of heartbeat detectors for the seismocardiogram",
      "authors": [
        "M García-González",
        "A Argelagós-Palau",
        "M Fernández-Chimeno",
        "J Ramos-Castro"
      ],
      "year": "2013",
      "venue": "Computing in Cardiology (CinC)"
    },
    {
      "citation_id": "212",
      "title": "Masked face recognition with generative data augmentation and domain constrained ranking",
      "authors": [
        "M Geng",
        "P Peng",
        "Y Huang",
        "Y Tian"
      ],
      "year": "2020",
      "venue": "28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413723"
    },
    {
      "citation_id": "213",
      "title": "Towards explainable face aging with generative adversarial networks",
      "authors": [
        "A Genovese",
        "V Piuri",
        "F Scotti"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Image Processing (ICIP)",
      "doi": "10.1109/ICIP.2019.8803616"
    },
    {
      "citation_id": "214",
      "title": "A Fully Homomorphic Encryption Scheme",
      "authors": [
        "C Gentry"
      ],
      "year": "2009",
      "venue": "A Fully Homomorphic Encryption Scheme"
    },
    {
      "citation_id": "215",
      "title": "Learning one class representations for face presentation attack detection using multi-channel convolutional neural networks",
      "authors": [
        "A George",
        "S Marcel"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2020.3013214"
    },
    {
      "citation_id": "216",
      "title": "Reliable features for an ECG-based biometric system",
      "authors": [
        "N Ghofrani",
        "R Bostani"
      ],
      "year": "2010",
      "venue": "Iranian Conference of Biomedical Engineering (ICBME)",
      "doi": "10.1109/ICBME.2010.5704918"
    },
    {
      "citation_id": "217",
      "title": "Towards automatic concept-based explanations",
      "authors": [
        "A Ghorbani",
        "J Wexler",
        "J Zou",
        "B Kim"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.5555/3454287.3455119"
    },
    {
      "citation_id": "218",
      "title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation",
      "authors": [
        "A Goldberger",
        "L Amaral",
        "L Glass",
        "J Hausdorff",
        "P Ivanov",
        "R Mark",
        "J Mietus",
        "G Moody",
        "C.-K Peng",
        "H Stanley"
      ],
      "year": "2000",
      "venue": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation",
      "doi": "10.1161/01.CIR.101.23.e215"
    },
    {
      "citation_id": "219",
      "title": "Self-Supervised Learning of Visual Features Through Embedding Images Into Text Topic Spaces",
      "authors": [
        "L Gomez",
        "Y Patel",
        "M Rusinol",
        "D Karatzas",
        "C Jawahar"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.218"
    },
    {
      "citation_id": "220",
      "title": "Variable-length template protection based on homomorphic encryption with application to signature biometrics",
      "authors": [
        "M Gomez-Barrero",
        "J Fierrez",
        "J Galbally"
      ],
      "year": "2016",
      "venue": "International Workshop on Biometrics and Forensics (IWBF)",
      "doi": "10.1109/IWBF.2016.7449672"
    },
    {
      "citation_id": "221",
      "title": "Unlinkable and irreversible biometric template protection based on bloom filters",
      "authors": [
        "M Gomez-Barrero",
        "C Rathgeb",
        "J Galbally",
        "C Busch",
        "J Fierrez"
      ],
      "year": "2016",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2016.06.046"
    },
    {
      "citation_id": "222",
      "title": "Multi-biometric template protection based on homomorphic encryption",
      "authors": [
        "M Gomez-Barrero",
        "E Maiorana",
        "J Galbally",
        "P Campisi",
        "J Fierrez"
      ],
      "year": "2017",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2017.01.024"
    },
    {
      "citation_id": "223",
      "title": "Multibiometric template protection based on bloom filters",
      "authors": [
        "M Gomez-Barrero",
        "C Rathgeb",
        "G Li",
        "R Ramachandra",
        "J Galbally",
        "C Busch"
      ],
      "year": "2018",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2017.10.003"
    },
    {
      "citation_id": "224",
      "title": "Biometrics in the era of COVID-19: challenges and opportunities. arXiv, 2021",
      "authors": [
        "M Gomez-Barrero",
        "P Drozdowski",
        "C Rathgeb",
        "J Patino",
        "M Todisco",
        "A Nautsch",
        "N Damer",
        "J Priesnitz",
        "N Evans",
        "C Busch"
      ],
      "venue": "Biometrics in the era of COVID-19: challenges and opportunities. arXiv, 2021"
    },
    {
      "citation_id": "225",
      "title": "Automated border control: Problem formalization",
      "authors": [
        "D Gorodnichy",
        "S Yanushkevich",
        "V Shmerko"
      ],
      "year": "2014",
      "venue": "IEEE Symposium on Computational Intelligence in Biometrics and Identity Management (CIBIM)",
      "doi": "10.1109/CIBIM.2014.7015452"
    },
    {
      "citation_id": "226",
      "title": "Lead Reconstruction Using Artificial Neural Networks for Ambulatory ECG Acquisition",
      "authors": [
        "A Grande-Fidalgo",
        "J Calpe",
        "M Redón",
        "C Millán-Navarro",
        "E Soria-Olivas"
      ],
      "year": "2021",
      "venue": "Sensors",
      "doi": "10.3390/s21165542"
    },
    {
      "citation_id": "227",
      "title": "Report on the Evaluation of 2D Still-Image Face Recognition Algorithms",
      "authors": [
        "P Grother",
        "G Quinn",
        "P Phillips"
      ],
      "year": "2010",
      "venue": "Report on the Evaluation of 2D Still-Image Face Recognition Algorithms",
      "doi": "10.6028/NIST.IR.7709"
    },
    {
      "citation_id": "228",
      "title": "Continuous authentication by electrocardiogram data",
      "authors": [
        "M Guennoun",
        "N Abbad",
        "J Talom",
        "S Rahman",
        "K El-Khatib"
      ],
      "year": "2009",
      "venue": "IEEE Toronto International Conference on Science and Technology for Humanity (TIC-STH)",
      "doi": "10.1109/TIC-STH.2009.5444466"
    },
    {
      "citation_id": "229",
      "title": "Score optimization and template updating in a biometric technique for authentication in mobiles based on gestures",
      "authors": [
        "J Guerra-Casanova",
        "C Sánchez-Ávila",
        "A Sierra",
        "G Del Pozo"
      ],
      "year": "2011",
      "venue": "Journal of Systems and Software",
      "doi": "10.1016/j.jss.2011.05.059"
    },
    {
      "citation_id": "230",
      "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision",
      "doi": "10.1007/978-3-319-46487-9_6"
    },
    {
      "citation_id": "231",
      "title": "An Attention Model for Group-Level Emotion Recognition",
      "authors": [
        "A Gupta",
        "D Agrawal",
        "H Chauhan",
        "J Dolz",
        "M Pedersoli"
      ],
      "year": "2018",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3242969.3264985"
    },
    {
      "citation_id": "232",
      "title": "Biometric identification using fingertip electrocardiogram signals. Signal, Image and Video Processing",
      "authors": [
        "G Guven",
        "H Gürkan",
        "U Guz"
      ],
      "year": "2018",
      "venue": "Biometric identification using fingertip electrocardiogram signals. Signal, Image and Video Processing",
      "doi": "10.1007/s11760-018-1238-4"
    },
    {
      "citation_id": "233",
      "title": "A novel two-dimensional ECG feature extraction and classification algorithm based on convolution neural network for human authentication",
      "authors": [
        "M Hammad",
        "S Zhang",
        "K Wang"
      ],
      "year": "2019",
      "venue": "Future Generation Computer Systems",
      "doi": "10.1016/j.future.2019.06.008"
    },
    {
      "citation_id": "234",
      "title": "ResNet-Attention model for human authentication using ECG signals",
      "authors": [
        "M Hammad",
        "P Pławiak",
        "K Wang",
        "U Acharya"
      ],
      "year": "2020",
      "venue": "Expert Systems",
      "doi": "10.1111/exsy.12547"
    },
    {
      "citation_id": "235",
      "title": "Deep Speech: Scaling up end-to-end speech recognition. arXiv",
      "authors": [
        "A Hannun",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "G Diamos",
        "E Elsen",
        "R Prenger",
        "S Satheesh",
        "S Sengupta",
        "A Coates",
        "A Ng"
      ],
      "year": "2014",
      "venue": "Deep Speech: Scaling up end-to-end speech recognition. arXiv"
    },
    {
      "citation_id": "236",
      "title": "Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network",
      "authors": [
        "A Hannun",
        "P Rajpurkar",
        "M Haghpanahi",
        "G Tison",
        "C Bourn",
        "M Turakhia",
        "A Ng"
      ],
      "year": "2019",
      "venue": "Nature Medicine",
      "doi": "10.1038/s41591-018-0268-3"
    },
    {
      "citation_id": "237",
      "title": "Comparing twelve-lead electrocardiography with close-to-heart patch based electrocardiography",
      "authors": [
        "I Hansen",
        "K Hoppe",
        "A Gjerde",
        "J Kanters",
        "H Sorensen"
      ],
      "year": "2015",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "doi": "10.1109/EMBC.2015.7318366"
    },
    {
      "citation_id": "238",
      "title": "Face recognition: challenges, achievements and future directions",
      "authors": [
        "M Hassaballah",
        "S Aly"
      ],
      "year": "2015",
      "venue": "IET Computer Vision",
      "doi": "10.1049/iet-cvi.2014.0084"
    },
    {
      "citation_id": "239",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "240",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems",
      "doi": "10.1109/TITS.2005.848368"
    },
    {
      "citation_id": "241",
      "title": "ActivityNet: A large-scale video benchmark for human activity understanding",
      "authors": [
        "F Heilbron",
        "V Escorcia",
        "B Ghanem",
        "J Niebles"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2015.7298698"
    },
    {
      "citation_id": "242",
      "title": "ECG biometric authentication based on non-fiducial approach using kernel methods",
      "authors": [
        "M Hejazi",
        "S Al-Haddad",
        "Y Singh",
        "S Hashim",
        "A Aziz"
      ],
      "year": "2016",
      "venue": "Digital Signal Processing",
      "doi": "10.1016/j.dsp.2016.02.008"
    },
    {
      "citation_id": "243",
      "title": "Geometrical aspects of the interindividual variability of multilead ECG recordings",
      "authors": [
        "R Hoekema",
        "G Uijen",
        "A Van Oosterom"
      ],
      "year": "1999",
      "venue": "Computers in Cardiology 1999",
      "doi": "10.1109/CIC.1999.826017"
    },
    {
      "citation_id": "244",
      "title": "Geometrical aspects of the interindividual variability of multilead ECG recordings",
      "authors": [
        "R Hoekema",
        "G Uijen",
        "A Van Oosterom"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/10.918594"
    },
    {
      "citation_id": "245",
      "title": "Mapping visual features to semantic profiles for retrieval in medical imaging",
      "authors": [
        "J Hofmanninger",
        "G Langs"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2015.7298643"
    },
    {
      "citation_id": "246",
      "title": "Causability and explainabilty of artificial intelligence in medicine",
      "authors": [
        "A Holzinger",
        "G Langs",
        "H Denk",
        "K Zatloukal",
        "H Müller"
      ],
      "year": "1312",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
      "doi": "10.1002/widm.1312"
    },
    {
      "citation_id": "247",
      "title": "Early action prediction by soft regression",
      "authors": [
        "J Hu",
        "W Zheng",
        "L Ma",
        "G Wang",
        "J Lai",
        "J Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2018.2863279"
    },
    {
      "citation_id": "248",
      "title": "Data-Driven Estimation of Driver Attention Using Calibration-Free Eye Gaze and Scene Features",
      "authors": [
        "Z Hu",
        "C Lv",
        "P Hang",
        "C Huang",
        "Y Xing"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Electronics",
      "doi": "10.1109/TIE.2021.3057033"
    },
    {
      "citation_id": "249",
      "title": "Labeled Faces in the Wild: Updates and New Reporting Procedures",
      "authors": [
        "G Huang",
        "E Learned-Miller"
      ],
      "year": "2014",
      "venue": "Labeled Faces in the Wild: Updates and New Reporting Procedures"
    },
    {
      "citation_id": "250",
      "title": "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments",
      "authors": [
        "G Huang",
        "M Ramesh",
        "T Berg",
        "E Learned-Miller"
      ],
      "year": "2007",
      "venue": "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments"
    },
    {
      "citation_id": "251",
      "title": "CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition",
      "authors": [
        "Y Huang",
        "Y Wang",
        "Y Tai",
        "X Liu",
        "P Shen",
        "S Li",
        "J Li",
        "F Huang"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00594"
    },
    {
      "citation_id": "252",
      "title": "Robust multi-feature collective nonnegative matrix factorization for ECG biometrics",
      "authors": [
        "Y Huang",
        "G Yang",
        "K Wang",
        "H Liu",
        "Y Yin"
      ],
      "year": "2022",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2021.108376"
    },
    {
      "citation_id": "253",
      "title": "A Benchmark and Comparative Study of Video-Based Face Recognition on COX Face Database",
      "authors": [
        "Z Huang",
        "S Shan",
        "R Wang",
        "H Zhang",
        "S Lao",
        "A Kuerban",
        "X Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2015.2493448"
    },
    {
      "citation_id": "254",
      "title": "Mask-invariant face recognition through template-level knowledge distillation",
      "authors": [
        "M Huber",
        "F Boutros",
        "F Kirchbuchner",
        "N Damer"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG52635.2021.9667081"
    },
    {
      "citation_id": "255",
      "title": "Matplotlib: A 2D Graphics Environment",
      "authors": [
        "J Hunter"
      ],
      "year": "2007",
      "venue": "Computing in Science & Engineering",
      "doi": "10.1109/MCSE.2007.55"
    },
    {
      "citation_id": "256",
      "title": "Proceedings of the Joint IAPR International Workshop, SSPR & SPRStructural, Syntactic, and Statistical Pattern Recognition",
      "authors": [
        "D Hutchison",
        "G Anagnostopoulos"
      ],
      "year": "2008",
      "venue": "Proceedings of the Joint IAPR International Workshop, SSPR & SPRStructural, Syntactic, and Statistical Pattern Recognition",
      "doi": "10.1007/978-3-540-89689-0"
    },
    {
      "citation_id": "257",
      "title": "EDITH : ECG Biometrics Aided by Deep Learning for Reliable Individual Authentication",
      "authors": [
        "N Ibtehaz",
        "M Chowdhury",
        "A Khandakar",
        "S Kiranyaz",
        "M Rahman",
        "A Tahir",
        "Y Qiblawey",
        "T Rahman"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "doi": "10.1109/TETCI.2021.3131374"
    },
    {
      "citation_id": "258",
      "title": "Biometric security from an information-theoretical perspective",
      "authors": [
        "T Ignatenko",
        "F Willems"
      ],
      "year": "2012",
      "venue": "Foundations and Trends in Communications and Information Theory",
      "doi": "10.1561/0100000051"
    },
    {
      "citation_id": "259",
      "title": "ECG Biometric Authentication: A Comparative Analysis",
      "authors": [
        "M Ingale",
        "R Cordeiro",
        "S Thentu",
        "Y Park",
        "N Karimian"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3004464"
    },
    {
      "citation_id": "260",
      "title": "A comparative analysis of QRS and Cardioid Graph Based ECG Biometric Recognition in different Physiological conditions",
      "authors": [
        "F.-T.-Z Iqbal",
        "K Sidek",
        "N Noah",
        "T Gunawan"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)",
      "doi": "10.1109/ICSIMA.2014.7047431"
    },
    {
      "citation_id": "261",
      "title": "eigenPulse: Robust human identification from cardiovascular function",
      "authors": [
        "J Irvine",
        "S Israel",
        "W Scruggs",
        "W Worek"
      ],
      "year": "2008",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2008.04.015"
    },
    {
      "citation_id": "262",
      "title": "Label Refinement Network for Coarse-to-Fine Semantic Segmentation. arXiv",
      "authors": [
        "M Islam",
        "S Naha",
        "M Rochan",
        "N Bruce",
        "Y Wang"
      ],
      "year": "2017",
      "venue": "Label Refinement Network for Coarse-to-Fine Semantic Segmentation. arXiv"
    },
    {
      "citation_id": "263",
      "title": "Biometric template extraction from a heartbeat signal captured from fingers",
      "authors": [
        "M Islam",
        "N Alajlan"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-016-3694-6"
    },
    {
      "citation_id": "264",
      "title": "Information Technology -Biometrics -Presentation attack detection Part 3: Testing and Reporting",
      "year": "2017",
      "venue": "International Organization for Standardization"
    },
    {
      "citation_id": "265",
      "title": "ECG to identify individuals",
      "authors": [
        "S Israel",
        "J Irvine",
        "A Cheng",
        "M Wiederhold",
        "B Wiederhold"
      ],
      "year": "2005",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2004.05.014"
    },
    {
      "citation_id": "266",
      "title": "An ECG-based Authentication System Using Siamese Neural Networks",
      "authors": [
        "L Ivanciu",
        "I.-A Ivanciu",
        "P Farago",
        "M Roman",
        "S Hintea"
      ],
      "year": "2021",
      "venue": "Journal of Medical and Biological Engineering",
      "doi": "10.1007/s40846-021-00637-9"
    },
    {
      "citation_id": "267",
      "title": "Age-related alterations in the fractal scaling of cardiac interbeat interval dynamics",
      "authors": [
        "N Iyengar",
        "C Peng",
        "R Morin",
        "A Goldberger",
        "L Lipsitz"
      ],
      "year": "1996",
      "venue": "R1078-R1084",
      "doi": "10.1152/ajpregu.1996.271.4.R1078"
    },
    {
      "citation_id": "268",
      "title": "Long-term ST database: A reference for the development and evaluation of automated ischaemia detectors and for the study of the dynamics of myocardial ischaemia",
      "authors": [
        "F Jager",
        "A Taddei",
        "G Moody",
        "M Emdin",
        "G Antolic",
        "R Dorn",
        "A Smrdel",
        "C Marchesi",
        "R Mark"
      ],
      "year": "2003",
      "venue": "Medical & Biological Engineering & Computing",
      "doi": "10.1007/BF02344885"
    },
    {
      "citation_id": "269",
      "title": "ECG based biometric human identification using chaotic encryption",
      "authors": [
        "M Jahiruzzaman",
        "A Hossain"
      ],
      "year": "2015",
      "venue": "International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)",
      "doi": "10.1109/ICEEICT.2015.7307417"
    },
    {
      "citation_id": "270",
      "title": "Biometric Recognition: An Overview",
      "authors": [
        "A Jain",
        "A Kumar"
      ],
      "year": "2012",
      "venue": "Second Generation Biometrics: The Ethical, Legal and Social Context",
      "doi": "10.1007/978-94-007-3892-8_3"
    },
    {
      "citation_id": "271",
      "title": "Biometrics: Personal Identification in Networked Society",
      "authors": [
        "A Jain",
        "R Bolle",
        "S Pankanti"
      ],
      "year": "1999",
      "venue": "Biometrics: Personal Identification in Networked Society",
      "doi": "10.1007/b117227"
    },
    {
      "citation_id": "272",
      "title": "Biometric template security",
      "authors": [
        "A Jain",
        "K Nandakumar",
        "A Nagar"
      ],
      "year": "2008",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "doi": "10.1155/2008/579416"
    },
    {
      "citation_id": "273",
      "title": "Introduction to Biometrics",
      "authors": [
        "A Jain",
        "A Ross",
        "K Nandakumar"
      ],
      "venue": "Introduction to Biometrics",
      "doi": "10.1007/978-0-387-77326-1"
    },
    {
      "citation_id": "274",
      "title": "Attacks on Biometric Systems: An Overview",
      "authors": [
        "R Jain",
        "C Kant"
      ],
      "year": "1975",
      "venue": "International Journal of Advances in Scientific Research",
      "doi": "10.7439/ijasr.v1i7.1975"
    },
    {
      "citation_id": "275",
      "title": "FDDB: A Benchmark for Face Detection in Unconstrained Settings",
      "authors": [
        "V Jain",
        "E Learned-Miller"
      ],
      "year": "2010",
      "venue": "FDDB: A Benchmark for Face Detection in Unconstrained Settings"
    },
    {
      "citation_id": "276",
      "title": "An empirical study of the impact of masks on face recognition",
      "authors": [
        "G Jeevan",
        "G Zacharias",
        "M Nair",
        "J Rajan"
      ],
      "year": "2022",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2021.108308"
    },
    {
      "citation_id": "277",
      "title": "Variability in photos of the same face",
      "authors": [
        "R Jenkins",
        "D White",
        "X Montfort",
        "A Burton"
      ],
      "year": "2011",
      "venue": "Cognition",
      "doi": "10.1016/j.cognition.2011.08.001"
    },
    {
      "citation_id": "278",
      "title": "Explainable Face Recognition Based on Accurate Facial Compositions",
      "authors": [
        "H Jiang",
        "D Zeng"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCVW54120.2021.00175"
    },
    {
      "citation_id": "279",
      "title": "Face De-spoofing: Anti-spoofing via Noise Modeling",
      "authors": [
        "A Jourabloo",
        "Y Liu",
        "X Liu"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-030-01261-8_18"
    },
    {
      "citation_id": "280",
      "title": "A Fuzzy Commitment Scheme",
      "authors": [
        "A Juels",
        "M Wattenberg"
      ],
      "year": "1999",
      "venue": "ACM Conference on Computer and Communications Security",
      "doi": "10.1145/319709.319714"
    },
    {
      "citation_id": "281",
      "title": "An LSTM-Based Model for Person Identification Using ECG Signal",
      "authors": [
        "D Jyotishi",
        "S Dandapat"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Letters",
      "doi": "10.1109/LSENS.2020.3012653"
    },
    {
      "citation_id": "282",
      "title": "Highly Reliable Key Generation From Electrocardiogram (ECG)",
      "authors": [
        "N Karimian",
        "Z Guo",
        "M Tehranipoor",
        "D Forte"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2016.2607020"
    },
    {
      "citation_id": "283",
      "title": "On the vulnerability of ECG verification to online presentation attacks",
      "authors": [
        "N Karimian",
        "D Woodard",
        "D Forte"
      ],
      "year": "2017",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)",
      "doi": "10.1109/BTAS.2017.8272692"
    },
    {
      "citation_id": "284",
      "title": "Large-Scale Video Classification with Convolutional Neural Networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2014.223"
    },
    {
      "citation_id": "285",
      "title": "A Review on Biometric Recognition",
      "authors": [
        "G Kaur",
        "G Singh",
        "V Kumar"
      ],
      "year": "2014",
      "venue": "International Journal of Bio-Science and Bio-Technology",
      "doi": "10.14257/ijbsbt.2014.6.4.07"
    },
    {
      "citation_id": "286",
      "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition",
      "authors": [
        "E Kazakos",
        "A Nagrani",
        "A Zisserman",
        "D Damen"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00559"
    },
    {
      "citation_id": "287",
      "title": "ECG Identification For Personal Authentication Using LSTM-Based Deep Recurrent Neural Networks",
      "authors": [
        "B.-H Kim",
        "J.-Y Pyun"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20113069"
    },
    {
      "citation_id": "288",
      "title": "Physiology-based augmented deep neural network frameworks for ECG biometrics with short ECG pulses considering varying heart rates",
      "authors": [
        "H Kim",
        "T Phan",
        "W Hong",
        "S Chun"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2022.02.014"
    },
    {
      "citation_id": "289",
      "title": "Study on a Biometric Authentication Model based on ECG using a Fuzzy Neural Network",
      "authors": [
        "H Kim",
        "J Lim"
      ],
      "year": "2018",
      "venue": "IOP Conference Series: Materials Science and Engineering",
      "doi": "10.1088/1757-899X/317/1/012030"
    },
    {
      "citation_id": "290",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "291",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "292",
      "title": "Finding skin in color images",
      "authors": [
        "R Kjeldsen",
        "J Kender"
      ],
      "year": "1996",
      "venue": "International Conference on Automatic Face and Gesture Recognition (AFGR)",
      "doi": "10.1109/AFGR.1996.557283"
    },
    {
      "citation_id": "293",
      "title": "",
      "authors": [
        "N Kokhlikyan",
        "V Miglani",
        "M Martin",
        "E Wang",
        "J Reynolds",
        "A Melnikov",
        "N Lunova",
        "O Reblitz-Richardson"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "294",
      "title": "Local learning regularization networks for localized regression",
      "authors": [
        "Y Kokkinos",
        "K Margaritis"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-016-2569-0"
    },
    {
      "citation_id": "295",
      "title": "Efficiency Analysis of Post-quantum-secure Face Template Protection Schemes based on Homomorphic Encryption",
      "authors": [
        "J Kolberg",
        "P Drozdowski",
        "M Gomez-Barrero",
        "C Rathgeb",
        "C Busch"
      ],
      "year": "2020",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "296",
      "title": "Liveness Detection and Automatic Template Updating Using Fusion of ECG and Fingerprint",
      "authors": [
        "M Komeili",
        "N Armanfard",
        "D Hatzinakos"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2018.2804890"
    },
    {
      "citation_id": "297",
      "title": "Feature Selection for Nonstationary Data: Application to Human Recognition Using Medical Biometrics",
      "authors": [
        "M Komeili",
        "W Louis",
        "N Armanfard",
        "D Hatzinakos"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/TCYB.2017.2702059"
    },
    {
      "citation_id": "298",
      "title": "Deep Sequential Context Networks for Action Prediction",
      "authors": [
        "Y Kong",
        "Z Tao",
        "Y Fu"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.390"
    },
    {
      "citation_id": "299",
      "title": "Kotikalapudi and contributors. keras-vis",
      "year": "2017",
      "venue": "GitHub repository"
    },
    {
      "citation_id": "300",
      "title": "Sample estimate of the entropy of a random vector",
      "authors": [
        "L Kozachenko",
        "N Leonenko"
      ],
      "year": "1987",
      "venue": "Problemy Peredachi Informatsii"
    },
    {
      "citation_id": "301",
      "title": "Estimating mutual information",
      "authors": [
        "A Kraskov",
        "H Stögbauer",
        "P Grassberger"
      ],
      "year": "2004",
      "venue": "Physical Review E",
      "doi": "10.1103/PhysRevE.69.066138"
    },
    {
      "citation_id": "302",
      "title": "Automatisierte EKG-Auswertung mit Hilfe der EKG-Signaldatenbank CARDIODAT der PTB",
      "authors": [
        "D Kreiseler",
        "R Bousseljot"
      ],
      "year": "1995",
      "venue": "Automatisierte EKG-Auswertung mit Hilfe der EKG-Signaldatenbank CARDIODAT der PTB",
      "doi": "10.1515/bmte.1995.40.s1.319"
    },
    {
      "citation_id": "303",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.5555/2999134.2999257"
    },
    {
      "citation_id": "304",
      "title": "Face detection techniques: a review",
      "authors": [
        "A Kumar",
        "A Kaur",
        "M Kumar"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence Review",
      "doi": "10.1007/s10462-018-9650-2"
    },
    {
      "citation_id": "305",
      "title": "Development of an ECG identification system",
      "authors": [
        "M Kyoso",
        "A Uchiyama"
      ],
      "year": "2001",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/IEMBS.2001.1019645"
    },
    {
      "citation_id": "306",
      "title": "Development of ECG Identification System. Japanese Journal of",
      "authors": [
        "M Kyoso",
        "A Uchiyama"
      ],
      "year": "2001",
      "venue": "Medical Electronics and Biological Engineering",
      "doi": "10.11239/jsmbe1963.39.Supplement_559"
    },
    {
      "citation_id": "307",
      "title": "Development of ECG Identification System",
      "authors": [
        "M Kyoso",
        "K Ohishi",
        "A Uchiyama"
      ],
      "year": "2000",
      "venue": "Japanese Journal of Medical Electronics and Biological Engineering",
      "doi": "10.11239/jsmbe1963.38.Supplement_388"
    },
    {
      "citation_id": "308",
      "title": "ECG biometric recognition: Permanence analysis of QRS signals for 24 hours continuous authentication",
      "authors": [
        "R Labati",
        "R Sassi",
        "F Scotti"
      ],
      "year": "2013",
      "venue": "IEEE International Workshop on Information Forensics and Security (WIFS)",
      "doi": "10.1109/WIFS.2013.6707790"
    },
    {
      "citation_id": "309",
      "title": "Adaptive ECG biometric recognition: a study on re-enrollment methods for QRS signals",
      "authors": [
        "R Labati",
        "V Piuri",
        "R Sassi",
        "F Scotti",
        "G Sforza"
      ],
      "year": "2014",
      "venue": "IEEE Symposium on Computational Intelligence in Biometrics and Identity Management (CIBIM)",
      "doi": "10.1109/CIBIM.2014.7015440"
    },
    {
      "citation_id": "310",
      "title": "Deep-ECG: Convolutional Neural Networks for ECG biometric recognition",
      "authors": [
        "R Labati",
        "E Muñoz",
        "V Piuri",
        "R Sassi",
        "F Scotti"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2018.03.028"
    },
    {
      "citation_id": "311",
      "title": "A Database for Evaluation of Algorithms for Measurement of QT and Other Waveform Intervals in the ECG",
      "authors": [
        "P Laguna",
        "R Mark",
        "A Goldberger",
        "G Moody"
      ],
      "year": "1997",
      "venue": "Computers in Cardiology (CinC)",
      "doi": "10.1109/CIC.1997.648140"
    },
    {
      "citation_id": "312",
      "title": "The Emotion Probe: Studies of Motivation and Attention",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "American Psychologist",
      "doi": "10.1037/0003-066X.50.5.372"
    },
    {
      "citation_id": "313",
      "title": "Analyzing Classifiers: Fisher Vectors and Deep Neural Networks",
      "authors": [
        "S Lapuschkin",
        "A Binder",
        "G Montavon",
        "K.-R Muller",
        "W Samek"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.318"
    },
    {
      "citation_id": "314",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature",
      "doi": "10.1038/nature14539"
    },
    {
      "citation_id": "315",
      "title": "Reconstruction of 12-lead ECG Using a Single-patch Device",
      "authors": [
        "H Lee",
        "D Lee",
        "H Kwon",
        "D Kim",
        "K Park"
      ],
      "year": "2017",
      "venue": "Methods of Information in Medicine",
      "doi": "10.3414/ME16-01-0067"
    },
    {
      "citation_id": "316",
      "title": "Synthesis of Electrocardiogram V-Lead Signals From Limb-Lead Measurement Using R-Peak Aligned Generative Adversarial Network",
      "authors": [
        "J.-E Lee",
        "K.-T Oh",
        "B Kim",
        "S Yoo"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2019.2936583"
    },
    {
      "citation_id": "317",
      "title": "ECG-Based Biometrics Using a Deep Network Based on Independent Component Analysis",
      "authors": [
        "J.-N Lee",
        "K.-C Kwak"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2022.3147807"
    },
    {
      "citation_id": "318",
      "title": "Individual Biometric Identification Using Multi-Cycle Electrocardiographic Waveform Patterns",
      "authors": [
        "W Lee",
        "S Kim",
        "D Kim"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18041005"
    },
    {
      "citation_id": "319",
      "title": "Heartbeats in the Wild: A Field Study Exploring ECG Biometrics in Everyday Life",
      "authors": [
        "F Lehmann",
        "D Buschek"
      ],
      "year": "2020",
      "venue": "Conference on Human Factors in Computing Systems (CHI)",
      "doi": "10.1145/3313831.3376536"
    },
    {
      "citation_id": "320",
      "title": "Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation",
      "authors": [
        "C Li",
        "S Ge",
        "D Zhang",
        "J Li"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimedia (MM)",
      "doi": "10.1145/3394171.3413960"
    },
    {
      "citation_id": "321",
      "title": "Unsupervised domain adaptation for face anti-spoofing",
      "authors": [
        "H Li",
        "W Li",
        "H Cao",
        "S Wang",
        "F Huang",
        "A Kot"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2018.2801312"
    },
    {
      "citation_id": "322",
      "title": "Robust ECG Biometrics by Fusing Temporal and Cepstral Information",
      "authors": [
        "M Li",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR.2010.330"
    },
    {
      "citation_id": "323",
      "title": "ET-UMAP integration feature for ECG biometrics using Stacking",
      "authors": [
        "M Li",
        "Y Si",
        "W Yang",
        "Y Yu"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2021.103159"
    },
    {
      "citation_id": "324",
      "title": "Robust ECG biometrics using GNMF and sparse representation",
      "authors": [
        "R Li",
        "G Yang",
        "K Wang",
        "Y Huang",
        "F Yuan",
        "Y Yin"
      ],
      "year": "2020",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2019.11.005"
    },
    {
      "citation_id": "325",
      "title": "The CASIA NIR-VIS 2.0 Face Database",
      "authors": [
        "S Li",
        "D Yi",
        "Z Lei",
        "S Liao"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW.2013.59"
    },
    {
      "citation_id": "326",
      "title": "Joint-task self-supervised learning for temporal correspondence",
      "authors": [
        "X Li",
        "S Liu",
        "S Mello",
        "X Wang",
        "J Kautz",
        "M.-H Yang"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.5555/3454287.3454316"
    },
    {
      "citation_id": "327",
      "title": "Toward improving ECG biometric identification using cascaded convolutional neural networks",
      "authors": [
        "Y Li",
        "Y Pang",
        "K Wang",
        "X Li"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.01.019"
    },
    {
      "citation_id": "328",
      "title": "Cropping and attention based approach for masked face recognition",
      "authors": [
        "Y Li",
        "K Guo",
        "Y Lu",
        "L Liu"
      ],
      "year": "2021",
      "venue": "Applied Intelligence",
      "doi": "10.1007/s10489-020-02100-9"
    },
    {
      "citation_id": "329",
      "title": "Resistance to facial recognition payment in China: The influence of privacy-related factors",
      "authors": [
        "Y Liu",
        "W Yan",
        "B Hu"
      ],
      "year": "2021",
      "venue": "Telecommunications Policy",
      "doi": "10.1016/j.telpol.2021.102155"
    },
    {
      "citation_id": "330",
      "title": "Audio-Based Activities of Daily Living (ADL) Recognition with Large-Scale Acoustic Embeddings from Online Videos",
      "authors": [
        "D Liang",
        "E Thomaz"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)",
      "doi": "10.1145/3314404"
    },
    {
      "citation_id": "331",
      "title": "Emotion Recognition Using Eye-Tracking: Taxonomy, Review and Current Challenges",
      "authors": [
        "J Lim",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20082384"
    },
    {
      "citation_id": "332",
      "title": "Fast AutoAugment",
      "authors": [
        "S Lim",
        "I Kim",
        "T Kim",
        "C Kim",
        "S Kim"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.5555/3454287.3454885"
    },
    {
      "citation_id": "333",
      "title": "Individual identification based on chaotic electrocardiogram signals during muscular exercise",
      "authors": [
        "S Lin",
        "C Chen",
        "C Lin",
        "W Yang",
        "C Chiang"
      ],
      "year": "2014",
      "venue": "IET Biometrics",
      "doi": "10.1049/iet-bmt.2013.0014"
    },
    {
      "citation_id": "334",
      "title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition",
      "authors": [
        "C Liu",
        "H Wechsler"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2002.999679"
    },
    {
      "citation_id": "335",
      "title": "Heterogeneous Face Interpretable Disentangled Representation for Joint Face Recognition and Synthesis",
      "authors": [
        "D Liu",
        "X Gao",
        "C Peng",
        "N Wang",
        "J Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": "10.1109/TNNLS.2021.3071119"
    },
    {
      "citation_id": "336",
      "title": "Recurrent Scale Approximation for Object Detection in CNN",
      "authors": [
        "Y Liu",
        "H Li",
        "J Yan",
        "F Wei",
        "X Wang",
        "X Tang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.69"
    },
    {
      "citation_id": "337",
      "title": "Learning deep models for face anti-spoofing: Binary or auxiliary supervision",
      "authors": [
        "Y Liu",
        "A Jourabloo",
        "X Liu"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2018.00048"
    },
    {
      "citation_id": "338",
      "title": "On disentangling spoof trace for generic face antispoofing",
      "authors": [
        "Y Liu",
        "J Stehouwer",
        "X Liu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-030-58523-5_24"
    },
    {
      "citation_id": "339",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.425"
    },
    {
      "citation_id": "340",
      "title": "A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic",
      "authors": [
        "M Loey",
        "G Manogaran",
        "M Taha",
        "N Khalifa"
      ],
      "year": "2021",
      "venue": "Measurement",
      "doi": "10.1016/j.measurement.2020.108288"
    },
    {
      "citation_id": "341",
      "title": "Don't You Forget About Me: A Study on Long-Term Performance in ECG Biometrics",
      "authors": [
        "G Lopes",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA)",
      "doi": "10.1007/978-3-030-31321-0_4"
    },
    {
      "citation_id": "342",
      "title": "Don't You Forget About Me: Enhancing Long Term Performance in Electrocardiogram",
      "authors": [
        "G Lopes"
      ],
      "year": "2019",
      "venue": "Biometrics. Master's thesis"
    },
    {
      "citation_id": "343",
      "title": "Deep Neural Networks for Face-based Emotion Recognition",
      "authors": [
        "P Lopes"
      ],
      "year": "2022",
      "venue": "Deep Neural Networks for Face-based Emotion Recognition"
    },
    {
      "citation_id": "344",
      "title": "Continuous Authentication Using One-Dimensional Multi-Resolution Local Binary Patterns (1DMRLBP) in ECG Biometrics",
      "authors": [
        "W Louis",
        "M Komeili",
        "D Hatzinakos"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2016.2599270"
    },
    {
      "citation_id": "345",
      "title": "Outlier detection in non-intrusive ecg biometric system",
      "authors": [
        "A Lourenço",
        "H Silva",
        "C Carreiras",
        "A Fred"
      ],
      "year": "2013",
      "venue": "International Conference on Image Analysis and Recognition (ICIAR)",
      "doi": "10.1007/978-3-642-39094-4_6"
    },
    {
      "citation_id": "346",
      "title": "Unveiling the Biometric Potential of Finger-based ECG Signals",
      "authors": [
        "A Lourenço",
        "H Silva",
        "A Fred"
      ],
      "year": "2011",
      "venue": "Computational Intelligence and Neuroscience",
      "doi": "10.1155/2011/720971"
    },
    {
      "citation_id": "347",
      "title": "Towards a Finger Based ECG Biometric System",
      "authors": [
        "A Lourenço",
        "H Silva",
        "D Santos",
        "A Fred"
      ],
      "year": "2011",
      "venue": "International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS)",
      "doi": "10.5220/0003286803480353"
    },
    {
      "citation_id": "348",
      "title": "ECG-based biometrics: A real time classification approach",
      "authors": [
        "A Lourenço",
        "H Silva",
        "A Fred"
      ],
      "year": "2012",
      "venue": "IEEE International Workshop on Machine Learning for Signal Processing",
      "doi": "10.1109/MLSP.2012.6349735"
    },
    {
      "citation_id": "349",
      "title": "Real Time Electrocardiogram Segmentation For Finger Based ECG Biometrics",
      "authors": [
        "A Lourenço",
        "H Silva",
        "R Lourenço",
        "P Leite",
        "A Fred"
      ],
      "year": "2012",
      "venue": "International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS)",
      "doi": "10.5220/0003777300490054"
    },
    {
      "citation_id": "350",
      "title": "ECG biometrics: A template selection approach",
      "authors": [
        "A Lourenço",
        "C Carreiras",
        "H Silva",
        "A Fred"
      ],
      "year": "2014",
      "venue": "IEEE International Symposium on Medical Measurements and Applications (MeMeA)",
      "doi": "10.1109/MeMeA.2014.6860081"
    },
    {
      "citation_id": "351",
      "title": "CardioWheel: ECG Biometrics on the Steering Wheel",
      "authors": [
        "A Lourenço",
        "A Alves",
        "C Carreiras",
        "R Duarte",
        "A Fred"
      ],
      "year": "2015",
      "venue": "European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)",
      "doi": "10.1007/978-3-319-23461-8_27"
    },
    {
      "citation_id": "352",
      "title": "Mobile Biometrics in Financial Services: A Five Factor Framework",
      "authors": [
        "G Lovisotto",
        "R Malik",
        "I Sluganovic",
        "M Roeschlin",
        "P Trueman",
        "I Martinovic"
      ],
      "year": "2017",
      "venue": "Mobile Biometrics in Financial Services: A Five Factor Framework"
    },
    {
      "citation_id": "353",
      "title": "Biometric human identification based on electrocardiogram. Master's thesis, Faculty of Computing Technologies and Informatics, Electrotechnical University \"LETI",
      "authors": [
        "T Lugovaya"
      ],
      "year": "2005",
      "venue": "Biometric human identification based on electrocardiogram. Master's thesis, Faculty of Computing Technologies and Informatics, Electrotechnical University \"LETI"
    },
    {
      "citation_id": "354",
      "title": "A clustering method for automatic biometric template selection",
      "authors": [
        "A Lumini",
        "L Nanni"
      ],
      "year": "2006",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2005.11.004"
    },
    {
      "citation_id": "355",
      "title": "A Unified Approach to Interpreting Model Predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing Systems (NeurIPS)",
      "doi": "10.5555/3295222.3295230"
    },
    {
      "citation_id": "356",
      "title": "Learning deep off-the-person heart biometrics representations",
      "authors": [
        "E Luz",
        "G Moreira",
        "L Oliveira",
        "W Schwartz",
        "D Menotti"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2017.2784362"
    },
    {
      "citation_id": "357",
      "title": "Feel My Heart: Emotion Recognition Using the Electrocardiogram. Master's thesis",
      "authors": [
        "I Magalhães"
      ],
      "year": "2021",
      "venue": "Feel My Heart: Emotion Recognition Using the Electrocardiogram. Master's thesis"
    },
    {
      "citation_id": "358",
      "title": "Human Anatomy & Physiology",
      "authors": [
        "E Marieb",
        "K Hoehn"
      ],
      "year": "2013",
      "venue": "Human Anatomy & Physiology"
    },
    {
      "citation_id": "359",
      "title": "An annotated ECG database for evaluating arrhythmia detectors",
      "authors": [
        "R Mark",
        "P Schluter",
        "G Moody",
        "P Devlin",
        "D Chernoff"
      ],
      "year": "1982",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "360",
      "title": "Biometric recognition system using low bandwidth ECG signals",
      "authors": [
        "A Matos",
        "A Lourenço",
        "J Nascimento"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on e-Health Networking, Applications and Services (Healthcom)",
      "doi": "10.1109/HealthCom.2013.6720731"
    },
    {
      "citation_id": "361",
      "title": "Embedded System for Individual Recognition Based on ECG",
      "authors": [
        "A Matos",
        "A Lourenço",
        "J Nascimento"
      ],
      "year": "2014",
      "venue": "Biometrics. Procedia Technology",
      "doi": "10.1016/j.protcy.2014.10.236"
    },
    {
      "citation_id": "362",
      "title": "Mixture-Based Open World Face Recognition",
      "authors": [
        "A Matta",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "World Conference on Information Systems and Technologies (WorldCIST)",
      "doi": "10.1007/978-3-030-72660-7_62"
    },
    {
      "citation_id": "363",
      "title": "Open-World Face Recognition",
      "authors": [
        "A Matta"
      ],
      "year": "2020",
      "venue": "Portugal"
    },
    {
      "citation_id": "364",
      "title": "Real-time continuous identification system using ECG signals",
      "authors": [
        "R Matta",
        "J Lau",
        "F Agrafioti",
        "D Hatzinakos"
      ],
      "year": "2011",
      "venue": "Canadian Conference on Electrical and Computer Engineering (CCECE)",
      "doi": "10.1109/CCECE.2011.6030676"
    },
    {
      "citation_id": "365",
      "title": "Feasibility of ECG Reconstruction from Minimal Lead Sets Using Convolutional Neural Networks",
      "authors": [
        "M Matyschik",
        "H Mauranen",
        "P Bonizzi",
        "J Karel"
      ],
      "year": "2020",
      "venue": "Computing in Cardiology",
      "doi": "10.22489/CinC.2020.164"
    },
    {
      "citation_id": "366",
      "title": "Facial Emotion Recognition: A Survey and Real-World User Experiences in Mixed Reality",
      "authors": [
        "D Mehta",
        "M Siddiqui",
        "A Javaid"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18020416"
    },
    {
      "citation_id": "367",
      "title": "MagFace: A Universal Representation for Face Recognition and Quality Assessment",
      "authors": [
        "Q Meng",
        "S Zhao",
        "Z Huang",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR46437.2021.01400"
    },
    {
      "citation_id": "368",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2017.7952552"
    },
    {
      "citation_id": "369",
      "title": "Walk the Walk: Attacking Gait Biometrics by Imitation",
      "authors": [
        "B Mjaaland",
        "P Bours",
        "D Gligoroski"
      ],
      "year": "2010",
      "venue": "International Conference on Information Security (ISC)",
      "doi": "10.1007/978-3-642-18178-8_31"
    },
    {
      "citation_id": "370",
      "title": "Morphological synthesis of ECG signals for person authentication",
      "authors": [
        "G Molina",
        "F Bruekers",
        "C Presura",
        "M Damstra",
        "M Van Der Veen"
      ],
      "year": "2007",
      "venue": "European Signal Processing Conference"
    },
    {
      "citation_id": "371",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "372",
      "title": "Multi-moments in time: Learning and interpreting models for multi-action video understanding",
      "authors": [
        "M Monfort",
        "K Ramakrishnan",
        "A Andonian",
        "B Mcnamara",
        "A Lascelles",
        "B Pan",
        "Q Fan",
        "D Gutfreund",
        "R Feris",
        "A Oliva"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3126682"
    },
    {
      "citation_id": "373",
      "title": "The MIT-BIH Arrhythmia Database on CD-ROM and software for use with it",
      "authors": [
        "G Moody",
        "R Mark"
      ],
      "year": "1990",
      "venue": "Computers in Cardiology (CinC)",
      "doi": "10.1109/CIC.1990.144205"
    },
    {
      "citation_id": "374",
      "title": "Intelligent In-Vehicle Interaction Technologies. Advanced Intelligent Systems",
      "authors": [
        "P Murali",
        "M Kaboli",
        "R Dahiya"
      ],
      "year": "2022",
      "venue": "Intelligent In-Vehicle Interaction Technologies. Advanced Intelligent Systems",
      "doi": "10.1002/aisy.202100122"
    },
    {
      "citation_id": "375",
      "title": "Human Electrocardiogram for Biometrics Using DTW and FLDA",
      "authors": [
        "S Jayaraman"
      ],
      "year": "2010",
      "venue": "International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR.2010.935"
    },
    {
      "citation_id": "376",
      "title": "Biometric template protection: Bridging the performance gap between theory and practice",
      "authors": [
        "K Nandakumar",
        "A Jain"
      ],
      "year": "2015",
      "venue": "IEEE Signal Processing Magazine",
      "doi": "10.1109/MSP.2015.2427849"
    },
    {
      "citation_id": "377",
      "title": "Level Playing Field for Million Scale Face Recognition",
      "authors": [
        "A Nech",
        "I Kemelmacher-Shlizerman"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.363"
    },
    {
      "citation_id": "378",
      "title": "Reconstruction of the 12-lead electrocardiogram from reduced lead sets",
      "authors": [
        "S Nelwan",
        "J Kors",
        "S Meij",
        "J Van Bemmel",
        "M Simoons"
      ],
      "year": "2004",
      "venue": "Journal of Electrocardiology",
      "doi": "10.1016/j.jelectrocard.2003.10.004"
    },
    {
      "citation_id": "379",
      "title": "Biometric human identification based on electrocardiogram",
      "authors": [
        "A Nemirko",
        "T Lugovaya"
      ],
      "year": "2005",
      "venue": "Russian Conference on Mathematical Methods of Pattern Recognition"
    },
    {
      "citation_id": "380",
      "title": "Focus-Face: Multi-task Contrastive Learning for Masked Face Recognition",
      "authors": [
        "P Neto",
        "F Boutros",
        "J Pinto",
        "N Damer",
        "A Sequeira",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG52635.2021.9666792"
    },
    {
      "citation_id": "381",
      "title": "My Eyes Are Up Here: Promoting Focus on Uncovered Regions in Masked Face Recognition",
      "authors": [
        "P Neto",
        "F Boutros",
        "J Pinto",
        "M Saffari",
        "N Damer",
        "A Sequeira",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG)",
      "doi": "10.1109/BIOSIG52210.2021.9548320"
    },
    {
      "citation_id": "382",
      "title": "IJCB OCFR 2022: Competition on Occluded Face Recognition From Synthetically Generated Structure-Aware Occlusions",
      "authors": [
        "P Neto",
        "F Boutros",
        "J Pinto",
        "N Damer",
        "A Sequeira",
        "J Cardoso",
        "M Bengherabi",
        "A Bousnat",
        "S Boucheta",
        "N Hebbadj",
        "B Yahya-Zoubir",
        "M Erakın",
        "U Demir",
        "H Ekenel",
        "P De Queiroz",
        "D Vidal",
        "Menotti"
      ],
      "year": "2022",
      "venue": "International Joint Conference on Biometrics (IJCB)"
    },
    {
      "citation_id": "383",
      "title": "Explainable Biometrics in the Age of Deep Learning",
      "authors": [
        "P Neto",
        "T Gonçalves",
        "J Pinto",
        "W Silva",
        "A Sequeira",
        "A Ross",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "Explainable Biometrics in the Age of Deep Learning"
    },
    {
      "citation_id": "384",
      "title": "Beyond Masks: On the Generalization of Masked Face Recognition Models to Occluded Face Recognition",
      "authors": [
        "P Neto",
        "J Pinto",
        "F Boutros",
        "N Damer",
        "A Sequeira",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2022.3199014"
    },
    {
      "citation_id": "385",
      "title": "Ongoing face recognition vendor test (FRVT) Part 6A: Face recognition accuracy with masks using pre-COVID-19 algorithms",
      "authors": [
        "M Ngan",
        "P Grother",
        "K Hanaoka"
      ],
      "year": "2020",
      "venue": "NIST Interagency/Internal Report (NISTIR)",
      "doi": "10.6028/NIST.IR.8311"
    },
    {
      "citation_id": "386",
      "title": "Ongoing Face Recognition Vendor Test (FRVT) Part 6B: Face recognition accuracy with face masks using post-COVID-19 algorithms",
      "authors": [
        "M Ngan",
        "P Grother",
        "K Hanaoka"
      ],
      "year": "2020",
      "venue": "NIST Interagency/Internal Report (NISTIR)",
      "doi": "10.6028/NIST.IR.8331"
    },
    {
      "citation_id": "387",
      "title": "On Effectiveness of Anomaly Detection Approaches against Unseen Presentation Attacks in Face Anti-spoofing",
      "authors": [
        "O Nikisins",
        "A Mohammadi",
        "A Anjos",
        "S Marcel"
      ],
      "year": "2018",
      "venue": "International Conference on Biometrics (ICB)",
      "doi": "10.1109/ICB2018.2018.00022"
    },
    {
      "citation_id": "388",
      "title": "Microwave doppler radar for heartbeat detection vs electrocardiogram",
      "authors": [
        "D Obeid",
        "G Zaharia",
        "S Sadek",
        "G Zein"
      ],
      "year": "2012",
      "venue": "Microwave and Optical Technology Letters",
      "doi": "10.1002/mop.27152"
    },
    {
      "citation_id": "389",
      "title": "How to attack biometric systems in your spare time",
      "authors": [
        "A Obied"
      ],
      "year": "2006",
      "venue": "How to attack biometric systems in your spare time"
    },
    {
      "citation_id": "390",
      "title": "ECG biometrics: A robust short-time frequency analysis",
      "authors": [
        "I Odinaka",
        "P Lai",
        "A Kaplan",
        "J O'sullivan",
        "E Sirevaag",
        "S Kristjansson",
        "A Sheffield",
        "J Rohrbaugh"
      ],
      "year": "2010",
      "venue": "IEEE International Workshop on Information Forensics and Security (WIFS)",
      "doi": "10.1109/WIFS.2010.5711466"
    },
    {
      "citation_id": "391",
      "title": "ECG Biometric Recognition: A Comparative Analysis",
      "authors": [
        "I Odinaka",
        "P Lai",
        "A Kaplan",
        "J O'sullivan",
        "E Sirevaag",
        "J Rohrbaugh"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2012.2215324"
    },
    {
      "citation_id": "392",
      "title": "Driver drowsiness detection: a comparison between intrusive and non-intrusive signal acquisition methods",
      "authors": [
        "L Oliveira",
        "J Cardoso",
        "A Lourenço",
        "C Ahlström"
      ],
      "year": "2018",
      "venue": "European Workshop on Visual Information Processing (EUVIP)",
      "doi": "10.1109/EUVIP.2018.8611704"
    },
    {
      "citation_id": "393",
      "title": "Weakly-Supervised Classification of HER2 Expression in Breast Cancer Haematoxylin and Eosin Stained Slides",
      "authors": [
        "S Oliveira",
        "J Pinto",
        "T Gonçalves",
        "R Marques",
        "M Cardoso",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10144728"
    },
    {
      "citation_id": "394",
      "title": "Smooth Grad-CAM++: An enhanced inference level visualization technique for deep convolutional neural network models. arXiv",
      "authors": [
        "D Omeiza",
        "S Speakman",
        "C Cintas",
        "K Weldermariam"
      ],
      "year": "2019",
      "venue": "Smooth Grad-CAM++: An enhanced inference level visualization technique for deep convolutional neural network models. arXiv"
    },
    {
      "citation_id": "395",
      "title": "Single-Wrist Electrocardiogram Acquisition Application in Biometrics. Master's thesis",
      "authors": [
        "E Omidvar"
      ],
      "year": "2022",
      "venue": "Single-Wrist Electrocardiogram Acquisition Application in Biometrics. Master's thesis"
    },
    {
      "citation_id": "396",
      "title": "Grid Loss: Detecting Occluded Faces",
      "authors": [
        "M Opitz",
        "G Waltner",
        "G Poier",
        "H Possegger",
        "H Bischof"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-319-46487-9_24"
    },
    {
      "citation_id": "397",
      "title": "Beat-ID: Towards a computationally low-cost single heartbeat biometric identity check system based on electrocardiogram wave morphology",
      "authors": [
        "J Paiva",
        "D Dias",
        "J Cunha"
      ],
      "year": "2017",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0180942"
    },
    {
      "citation_id": "398",
      "title": "ECG Biometric Recognition",
      "authors": [
        "A Pal",
        "Y Singh"
      ],
      "year": "2018",
      "venue": "International Conference on Mathematics and Computing (ICMC)",
      "doi": "10.1007/978-981-13-0023-3_7"
    },
    {
      "citation_id": "399",
      "title": "Identifying individuals using ECG beats",
      "authors": [
        "R Palaniappan",
        "S Krishnan"
      ],
      "year": "2004",
      "venue": "International Conference on Signal Processing and Communications (SPCOM)",
      "doi": "10.1109/SPCOM.2004.1458524"
    },
    {
      "citation_id": "400",
      "title": "A Real-Time QRS Detection Algorithm",
      "authors": [
        "J Pan",
        "W Tompkins"
      ],
      "year": "1985",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.1985.325532"
    },
    {
      "citation_id": "401",
      "title": "Deep Secure Encoding for Face Template Protection",
      "authors": [
        "R Pandey",
        "Y Zhou",
        "B Kota",
        "V Govindaraju"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW.2016.17"
    },
    {
      "citation_id": "402",
      "title": "DBDNet: Learning Bi-directional Dynamics for Early Action Prediction",
      "authors": [
        "G Pang",
        "X Wang",
        "J.-F Hu",
        "Q Zhang",
        "W.-S Zheng"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)",
      "doi": "10.24963/ijcai.2019/126"
    },
    {
      "citation_id": "403",
      "title": "Deep Face Recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference (BMVC)",
      "doi": "10.5244/C.29.41"
    },
    {
      "citation_id": "404",
      "title": "Robustness study of ECG biometric identification in heart rate variability conditions",
      "authors": [
        "S Pathoumvanh",
        "S Airphaiboon",
        "K Hamamoto"
      ],
      "year": "2014",
      "venue": "IEEJ Transactions on Electrical and Electronic Engineering",
      "doi": "10.1002/tee.21970"
    },
    {
      "citation_id": "405",
      "title": "Learning Deep Features for One-Class Classification",
      "authors": [
        "P Perera",
        "V Patel"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2019.2917862"
    },
    {
      "citation_id": "406",
      "title": "Deep Anomaly Detection for Generalized Face Anti-Spoofing",
      "authors": [
        "D Perez-Cabo",
        "D Jimenez-Cabello",
        "A Costa-Pazo",
        "R Lopez-Sastre"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW.2019.00201"
    },
    {
      "citation_id": "407",
      "title": "Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms. arXiv",
      "authors": [
        "P Phillips",
        "M Przybocki"
      ],
      "year": "2020",
      "venue": "Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms. arXiv"
    },
    {
      "citation_id": "408",
      "title": "Leveraging shape, reflectance and albedo from shading for face presentation attack detection",
      "authors": [
        "A Pinto",
        "S Goldenstein",
        "A Ferreira",
        "T Carvalho",
        "H Pedrini",
        "A Rocha"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2020.2988168"
    },
    {
      "citation_id": "409",
      "title": "Continuous Biometric Identification on the Steering Wheel",
      "authors": [
        "J Pinto"
      ],
      "year": "2017",
      "venue": "Continuous Biometric Identification on the Steering Wheel"
    },
    {
      "citation_id": "410",
      "title": "An End-to-End Convolutional Neural Network for ECG-Based Biometric Authentication",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "International Conference on Biometrics Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS46853.2019.9185990"
    },
    {
      "citation_id": "411",
      "title": "Explaining ECG Biometrics: Is It All In The QRS",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "International Conference of the Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "412",
      "title": "Self-Learning with Stochastic Triplet Loss",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN48605.2020.9206799"
    },
    {
      "citation_id": "413",
      "title": "ECG Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "Encyclopedia of Cryptography, Security and Privacy",
      "doi": "10.1007/978-3-642-27739-9_1517-1"
    },
    {
      "citation_id": "414",
      "title": "Towards a Continuous Biometric System Based on ECG Signals Acquired on the Steering Wheel",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço",
        "C Carreiras"
      ],
      "year": "2017",
      "venue": "Sensors",
      "doi": "10.3390/s17102228"
    },
    {
      "citation_id": "415",
      "title": "Evolution, Current Challenges, and Future Possibilities in ECG Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2849870"
    },
    {
      "citation_id": "416",
      "title": "Deep Neural Networks For Biometric Identification Based On Non-Intrusive ECG Acquisitions",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "A Lourenço"
      ],
      "year": "2019",
      "venue": "The Biometric Computing: Recognition and Registration",
      "doi": "10.1201/9781351013437-11"
    },
    {
      "citation_id": "417",
      "title": "Secure Triplet Loss for End-to-End Deep Biometrics",
      "authors": [
        "J Pinto",
        "J Cardoso",
        "M Correia"
      ],
      "year": "2020",
      "venue": "International Workshop on Biometrics and Forensics (IWBF)",
      "doi": "10.1109/IWBF49977.2020.9107958"
    },
    {
      "citation_id": "418",
      "title": "Audiovisual Classification of Group Emotion Valence Using Activity Recognition Networks",
      "authors": [
        "J Pinto",
        "T Gonçalves",
        "C Pinto",
        "L Sanhudo",
        "J Fonseca",
        "F Gonçalves",
        "P Carvalho",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Image Processing, Applications and Systems (IPAS)",
      "doi": "10.1109/IPAS50080.2020.9334943"
    },
    {
      "citation_id": "419",
      "title": "Secure Triplet Loss: Achieving Cancelability and Non-Linkability in End-to-End Deep Biometrics",
      "authors": [
        "J Pinto",
        "M Correia",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science",
      "doi": "10.1109/TBIOM.2020.3046620"
    },
    {
      "citation_id": "420",
      "title": "Streamlining Action Recognition in Autonomous Shared Vehicles with an Audiovisual Cascade Strategy",
      "authors": [
        "J Pinto",
        "P Carvalho",
        "C Pinto",
        "A Sousa",
        "L Capozzi",
        "J Cardoso"
      ],
      "year": "2022",
      "venue": "International Conference on Computer Vision Theory and Applications (VISAPP)",
      "doi": "10.5220/0010838900003124"
    },
    {
      "citation_id": "421",
      "title": "ECG Biometric Recognition Without Fiducial Detection",
      "authors": [
        "K Plataniotis",
        "D Hatzinakos",
        "J Lee"
      ],
      "year": "2006",
      "venue": "Biometrics Symposium: Special Session on Research at the Biometric Consortium Conference",
      "doi": "10.1109/BCC.2006.4341628"
    },
    {
      "citation_id": "422",
      "title": "ECG biometric analysis in different physiological recording conditions",
      "authors": [
        "F Porée",
        "G Kervio",
        "G Carrault"
      ],
      "year": "2016",
      "venue": "Signal, Image and Video Processing",
      "doi": "10.1007/s11760-014-0737-1"
    },
    {
      "citation_id": "423",
      "title": "Biometric recognition: security and privacy concerns",
      "authors": [
        "S Prabhakar",
        "S Pankanti",
        "A Jain"
      ],
      "year": "2003",
      "venue": "IEEE Security & Privacy",
      "doi": "10.1109/MSECP.2003.1193209"
    },
    {
      "citation_id": "424",
      "title": "Vehicle And License Authentication Using Finger Print",
      "authors": [
        "S Prema",
        "M Deen",
        "M Krishna",
        "S Praveen"
      ],
      "year": "2019",
      "venue": "International Conference on Advanced Computing & Communication Systems (ICACCS)",
      "doi": "10.1109/ICACCS.2019.8728402"
    },
    {
      "citation_id": "425",
      "title": "Can cancellable biometrics preserve privacy? Biometric Technology Today",
      "authors": [
        "P Punithavathi",
        "G Subbiah"
      ],
      "year": "2017",
      "venue": "Can cancellable biometrics preserve privacy? Biometric Technology Today",
      "doi": "10.1016/S0969-4765(17)30138-8"
    },
    {
      "citation_id": "426",
      "title": "stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition",
      "authors": [
        "M Qi",
        "Y Wang",
        "J Qin",
        "A Li",
        "J Luo",
        "L Van Gool"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2019.2894161"
    },
    {
      "citation_id": "427",
      "title": "Identifying facemask-wearing condition using image super-resolution with classification network to prevent COVID-19",
      "authors": [
        "B Qin",
        "D Li"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20185236"
    },
    {
      "citation_id": "428",
      "title": "Does Face Recognition Error Echo Gender Classification Error?",
      "authors": [
        "Y Qiu",
        "V Albiero",
        "M King",
        "K Bowyer"
      ],
      "year": "2021",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)",
      "doi": "10.1109/IJCB52358.2021.9484346"
    },
    {
      "citation_id": "429",
      "title": "Biometric template protection on smartphones using the manifold-structure preserving feature representation",
      "authors": [
        "K Raja",
        "R Raghavendra",
        "M Stokkenes",
        "C Busch"
      ],
      "year": "2019",
      "venue": "Selfie Biometrics: Advances and Challenges",
      "doi": "10.1007/978-3-030-26972-2_15"
    },
    {
      "citation_id": "430",
      "title": "Shallow Neural Network for Biometrics from the ECG-WATCH",
      "authors": [
        "V Randazzo",
        "G Cirrincione",
        "E Pasero"
      ],
      "year": "2020",
      "venue": "International Conference on Intelligent Computing Theories and Application (ICIC)",
      "doi": "10.1007/978-3-030-60799-9_22"
    },
    {
      "citation_id": "431",
      "title": "Permanence of ECG Biometric: Experiments Using Convolutional Neural Networks",
      "authors": [
        "A Ranjan"
      ],
      "year": "2019",
      "venue": "International Conference on Biometrics (ICB)",
      "doi": "10.1109/ICB45273.2019.8987383"
    },
    {
      "citation_id": "432",
      "title": "L2-constrained Softmax Loss for Discriminative Face Verification. arXiv",
      "authors": [
        "R Ranjan",
        "C Castillo",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "L2-constrained Softmax Loss for Discriminative Face Verification. arXiv"
    },
    {
      "citation_id": "433",
      "title": "Enhancing security and privacy in biometricsbased authentication systems",
      "authors": [
        "N Ratha",
        "J Connell",
        "R Bolle"
      ],
      "year": "2001",
      "venue": "IBM Systems Journal",
      "doi": "10.1147/sj.403.0614"
    },
    {
      "citation_id": "434",
      "title": "Alignment-free cancelable iris biometric templates based on adaptive bloom filters",
      "authors": [
        "C Rathgeb",
        "F Breitinger",
        "C Busch"
      ],
      "year": "2013",
      "venue": "International Conference on Biometrics (ICB)",
      "doi": "10.1109/ICB.2013.6612976"
    },
    {
      "citation_id": "435",
      "title": "Towards cancelable multi-biometrics based on bloom filters: a case study on feature level fusion of face and iris",
      "authors": [
        "C Rathgeb",
        "M Gomez-Barrero",
        "C Busch",
        "J Galbally",
        "J Fierrez"
      ],
      "year": "2015",
      "venue": "International Workshop on Biometrics and Forensics (IWBF)",
      "doi": "10.1109/IWBF.2015.7110225"
    },
    {
      "citation_id": "436",
      "title": "Adaptive Biometric System based on Template Update Procedures",
      "authors": [
        "A Rattani"
      ],
      "year": "2010",
      "venue": "Adaptive Biometric System based on Template Update Procedures"
    },
    {
      "citation_id": "437",
      "title": "Open set fingerprint spoof detection across novel fabrication materials",
      "authors": [
        "A Rattani",
        "W Scheirer",
        "A Ross"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2015.2464772"
    },
    {
      "citation_id": "438",
      "title": "ECG biometric recognition using SVM-based approach",
      "authors": [
        "D Rezgui",
        "Z Lachiri"
      ],
      "year": "2016",
      "venue": "IEEJ Transactions on Electrical and Electronic Engineering",
      "doi": "10.1002/tee.22241"
    },
    {
      "citation_id": "439",
      "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)",
      "doi": "10.1007/978-3-319-24574-4_28"
    },
    {
      "citation_id": "440",
      "title": "Human Identification System Based ECG Signal",
      "authors": [
        "S Saechia",
        "J Koseeyaporn",
        "P Wardkein"
      ],
      "year": "2005",
      "venue": "IEEE Region 10 Conference (TENCON)",
      "doi": "10.1109/TENCON.2005.300986"
    },
    {
      "citation_id": "441",
      "title": "Electrocardiogram (ECG) Biometric Authentication Using Pulse Active Ratio (PAR)",
      "authors": [
        "S Safie",
        "J Soraghan",
        "L Petropoulakis"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2011.2162408"
    },
    {
      "citation_id": "442",
      "title": "Detecting driver drowsiness based on sensors: A review",
      "authors": [
        "A Sahayadhas",
        "K Sundaraj",
        "M Murugappan"
      ],
      "year": "2012",
      "venue": "Sensors",
      "doi": "10.3390/s121216937"
    },
    {
      "citation_id": "443",
      "title": "ECG-based biometrics using recurrent neural networks",
      "authors": [
        "R Salloum",
        "C Kuo"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2017.7952519"
    },
    {
      "citation_id": "444",
      "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv",
      "authors": [
        "W Samek",
        "T Wiegand",
        "K.-R Müller"
      ],
      "year": "2017",
      "venue": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv"
    },
    {
      "citation_id": "445",
      "title": "Identification of Individuals using Electrocardiogram",
      "authors": [
        "P Sasikala",
        "R Wahidabanu"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Science and Network Security"
    },
    {
      "citation_id": "446",
      "title": "Smoothing and differentiation of data by simplified least squares procedures",
      "authors": [
        "A Savitzky",
        "M Golay"
      ],
      "year": "1964",
      "venue": "Analytical Chemistry",
      "doi": "10.1021/ac60214a047"
    },
    {
      "citation_id": "447",
      "title": "Bosphorus Database for 3D Face Analysis",
      "authors": [
        "N Savran",
        "H Dibeklioglu",
        "O Çeliktutan",
        "B Gökberk",
        "B Sankur",
        "L Akarun"
      ],
      "year": "2008",
      "venue": "European Workshop on Biometrics and Identity Management (BIOID)",
      "doi": "10.1007/978-3-540-89991-4_6"
    },
    {
      "citation_id": "448",
      "title": "Non Contact Heart Monitoring",
      "authors": [
        "L Scalise"
      ],
      "year": "2012",
      "venue": "Advances in Electrocardiograms",
      "doi": "10.5772/22937"
    },
    {
      "citation_id": "449",
      "title": "Essentials of Anatomy and Physiology",
      "authors": [
        "V Scanlon",
        "T Sanders"
      ],
      "year": "2007",
      "venue": "Essentials of Anatomy and Physiology"
    },
    {
      "citation_id": "450",
      "title": "Automatic Template Update Strategies for Biometrics",
      "authors": [
        "T Scheidat",
        "A Makrushin",
        "C Vielhauer"
      ],
      "year": "2007",
      "venue": "Germany"
    },
    {
      "citation_id": "451",
      "title": "Intra-individual Variability of the Electrocardiogram: Assessment and exploitation in computerized ECG analysis",
      "authors": [
        "R Schijvenaars"
      ],
      "year": "2000",
      "venue": "Intra-individual Variability of the Electrocardiogram: Assessment and exploitation in computerized ECG analysis"
    },
    {
      "citation_id": "452",
      "title": "Derivation of the 12-lead electrocardiogram and 3-lead vectorcardiogram",
      "authors": [
        "D Schreck",
        "R Fishberg"
      ],
      "year": "2013",
      "venue": "The American Journal of Emergency Medicine",
      "doi": "10.1016/j.ajem.2013.04.037"
    },
    {
      "citation_id": "453",
      "title": "FaceNet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2015.7298682"
    },
    {
      "citation_id": "454",
      "title": "Focused LRP: Explainable AI for Face Morphing Attack Detection",
      "authors": [
        "C Seibold",
        "A Hilsmann",
        "P Eisert"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "doi": "10.1109/WACVW52041.2021.00014"
    },
    {
      "citation_id": "455",
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.74"
    },
    {
      "citation_id": "456",
      "title": "A realistic evaluation of iris presentation attack detection",
      "authors": [
        "A Sequeira",
        "S Thavalengal",
        "J Ferryman",
        "P Corcoran",
        "J Cardoso"
      ],
      "year": "2016",
      "venue": "International Conference on Telecommunications and Signal Processing (TSP)",
      "doi": "10.1109/TSP.2016.7760965"
    },
    {
      "citation_id": "457",
      "title": "Interpretable Biometrics: Should We Rethink How Presentation Attack Detection is Evaluated?",
      "authors": [
        "A Sequeira",
        "W Silva",
        "J Pinto",
        "T Gonçalves",
        "J Cardoso"
      ],
      "year": "2020",
      "venue": "International Workshop on Biometrics and Forensics (IWBF)",
      "doi": "10.1109/IWBF49977.2020.9107949"
    },
    {
      "citation_id": "458",
      "title": "An Exploratory Study of Interpretability for Face Presentation Attack Detection",
      "authors": [
        "A Sequeira",
        "T Gonçalves",
        "W Silva",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2021",
      "venue": "IET Biometrics",
      "doi": "10.1049/bme2.12045"
    },
    {
      "citation_id": "459",
      "title": "Regularized fine-grained meta face anti-spoofing",
      "authors": [
        "R Shao",
        "X Lan",
        "P Yuen"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "460",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "G Sharma",
        "S Ghosh",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)",
      "doi": "10.1109/ACIIW.2019.8925231"
    },
    {
      "citation_id": "461",
      "title": "One-lead ECG for identity verification",
      "authors": [
        "T.-W Shen",
        "W Tompkins",
        "Y Hu"
      ],
      "year": "2002",
      "venue": "Joint Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society EMBS/BMES Conference",
      "doi": "10.1109/IEMBS.2002.1134388"
    },
    {
      "citation_id": "462",
      "title": "Implementation of a one-lead ECG human identification system on a normal population",
      "authors": [
        "T.-W Shen",
        "W Tompkins",
        "Y Hu"
      ],
      "year": "2011",
      "venue": "Journal of Engineering and Computer Innovations",
      "doi": "10.5897/JECI.9000009"
    },
    {
      "citation_id": "463",
      "title": "Learning Important Features through Propagating Activation Differences",
      "authors": [
        "A Shrikumar",
        "P Greenside",
        "A Kundaje"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)",
      "doi": "10.5555/3305890.3306006"
    },
    {
      "citation_id": "464",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18072074"
    },
    {
      "citation_id": "465",
      "title": "Finger ECG signal for user authentication: Usability and performance",
      "authors": [
        "H Silva",
        "A Fred",
        "A Lourenço",
        "A Jain"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS.2013.6712689"
    },
    {
      "citation_id": "466",
      "title": "Check Your Biosignals Here: A new dataset for off-the-person ECG biometrics",
      "authors": [
        "H Silva",
        "A Lourenço",
        "A Fred",
        "N Raposo",
        "M Aires-De-Sousa"
      ],
      "year": "2014",
      "venue": "Computer Methods and Programs in Biomedicine",
      "doi": "10.1016/j.cmpb.2013.11.017"
    },
    {
      "citation_id": "467",
      "title": "Comparison of spatial temporal representations of the vectorcardiogram using digital image processing",
      "authors": [
        "I Silva",
        "J Barbosa",
        "R De Sousa",
        "I De Souza",
        "R De Aguiar Hortegal",
        "C Regis"
      ],
      "year": "2020",
      "venue": "Journal of Electrocardiology",
      "doi": "10.1016/j.jelectrocard.2020.02.013"
    },
    {
      "citation_id": "468",
      "title": "A uniform performance index for ordinal classification with imbalanced classes",
      "authors": [
        "W Silva",
        "J Pinto",
        "J Cardoso"
      ],
      "year": "2018",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN.2018.8489327"
    },
    {
      "citation_id": "469",
      "title": "How to produce complementary explanations using an ensemble model",
      "authors": [
        "W Silva",
        "K Fernandes",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN.2019.8852409"
    },
    {
      "citation_id": "470",
      "title": "Interpretability-guided content-based medical image retrieval",
      "authors": [
        "W Silva",
        "A Poellinger",
        "J Cardoso",
        "M Reyes"
      ],
      "year": "2020",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)",
      "doi": "10.1007/978-3-030-59710-8_30"
    },
    {
      "citation_id": "471",
      "title": "Importance of subjectdependent classification and imbalanced distributions in driver sleepiness detection in realistic conditions",
      "authors": [
        "C Silveira",
        "J Cardoso",
        "A Lourenço",
        "C Ahlström"
      ],
      "year": "2019",
      "venue": "IET Intelligent Transport Systems",
      "doi": "10.1049/iet-its.2018.5284"
    },
    {
      "citation_id": "472",
      "title": "Continuous Verification Using Multimodal Biometrics",
      "authors": [
        "T Sim",
        "S Zhang",
        "R Janakiraman",
        "S Kumar"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2007.1010"
    },
    {
      "citation_id": "473",
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": [
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "International Conference on Learning Representations Workshops (ICLR)"
    },
    {
      "citation_id": "474",
      "title": "Linear Manifold Regularization for Large Scale Semi-supervised Learning",
      "authors": [
        "V Sindhwani",
        "P Niyogi",
        "M Belkin",
        "S Keerthi"
      ],
      "year": "2005",
      "venue": "ICML Workshop on Learning with Partially Classified Training Data"
    },
    {
      "citation_id": "475",
      "title": "Various Approaches to Minimise Noises in ECG Signal: A Survey",
      "authors": [
        "B Singh",
        "P Singh",
        "S Budhiraja"
      ],
      "year": "2015",
      "venue": "International Conference on Advanced Computing Communication Technologies (ACCT)",
      "doi": "10.1109/ACCT.2015.87"
    },
    {
      "citation_id": "476",
      "title": "Aadhaar and data privacy: biometric identification and anxieties of recognition in India",
      "authors": [
        "P Singh"
      ],
      "year": "2021",
      "venue": "Information, Communication & Society",
      "doi": "10.1080/1369118X.2019.1668459"
    },
    {
      "citation_id": "477",
      "title": "Evaluation of Electrocardiogram for Biometric Authentication",
      "authors": [
        "Y Singh",
        "S Singh"
      ],
      "year": "2012",
      "venue": "Journal of Information Security",
      "doi": "10.4236/jis.2012.31005"
    },
    {
      "citation_id": "478",
      "title": "Low-dimensional procedure for the characterization of human faces",
      "authors": [
        "L Sirovich",
        "M Kirby"
      ],
      "year": "1987",
      "venue": "Journal of the Optical Society of America A",
      "doi": "10.1364/JOSAA.4.000519"
    },
    {
      "citation_id": "479",
      "title": "The reconstruction of a 12-lead electrocardiogram from a reduced lead set using a focus time-delay neural network",
      "authors": [
        "G Smith",
        "D Van Den Heever",
        "W Swart"
      ],
      "year": "2021",
      "venue": "Acta Cardiologica Sinica",
      "doi": "10.6515/ACS.202101_37(1).20200712A"
    },
    {
      "citation_id": "480",
      "title": "Reconstruction of 12-Lead Electrocardiogram from a Three-Lead Patch-Type Device Using a LSTM Network",
      "authors": [
        "J Sohn",
        "S Yang",
        "J Lee",
        "Y Ku",
        "H Kim"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20113278"
    },
    {
      "citation_id": "481",
      "title": "Occlusion robust face recognition based on mask learning with pairwise differential siamese network",
      "authors": [
        "L Song",
        "D Gong",
        "Z Li",
        "C Liu",
        "W Liu"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00086"
    },
    {
      "citation_id": "482",
      "title": "Aging: Older Adults' Driving Behavior Using Longitudinal and Lateral Warning Systems",
      "authors": [
        "D Souders",
        "N Charness",
        "N Roque",
        "H Pham"
      ],
      "year": "2020",
      "venue": "Human Factors",
      "doi": "10.1177/0018720819864510"
    },
    {
      "citation_id": "483",
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "1929",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "484",
      "title": "PlexNet: A fast and robust ECG biometric system for human recognition",
      "authors": [
        "R Srivastva",
        "A Singh",
        "Y Singh"
      ],
      "year": "2021",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2021.01.001"
    },
    {
      "citation_id": "485",
      "title": "ECG-Based Authentication",
      "authors": [
        "F Sufi",
        "I Khalil",
        "J Hu"
      ],
      "year": "2010",
      "venue": "Handbook of Information and Communication Security",
      "doi": "10.1007/978-3-642-04117-4_17"
    },
    {
      "citation_id": "486",
      "title": "Cardioids-based faster authentication and diagnosis of remote cardiovascular patients. Security and Communication Networks",
      "authors": [
        "F Sufi",
        "I Khalil",
        "I Habib"
      ],
      "year": "2011",
      "venue": "Cardioids-based faster authentication and diagnosis of remote cardiovascular patients. Security and Communication Networks",
      "doi": "10.1002/sec.262"
    },
    {
      "citation_id": "487",
      "title": "LSTM for Dynamic Emotion and Group Emotion Recognition in the Wild",
      "authors": [
        "B Sun",
        "Q Wei",
        "L Li",
        "Q Xu",
        "J He",
        "L Yu"
      ],
      "year": "2016",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/2993148.2997640"
    },
    {
      "citation_id": "488",
      "title": "Face detection using deep learning: An improved faster RCNN approach",
      "authors": [
        "X Sun",
        "P Wu",
        "S Hoi"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2018.03.030"
    },
    {
      "citation_id": "489",
      "title": "Deep learning face representation from predicting 10,000 classes",
      "authors": [
        "Y Sun",
        "X Wang",
        "X Tang"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2014.244"
    },
    {
      "citation_id": "490",
      "title": "A secure biometric authentication scheme based on robust hashing",
      "authors": [
        "Y Sutcu",
        "H Sencar",
        "N Memon"
      ],
      "year": "2005",
      "venue": "Workshop on Multimedia and Security",
      "doi": "10.1145/1073170.1073191"
    },
    {
      "citation_id": "491",
      "title": "Technical Document About FAR, FRR and EER",
      "year": "2004",
      "venue": "Technical Document About FAR, FRR and EER"
    },
    {
      "citation_id": "492",
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence (AAAI)",
      "doi": "10.5555/3298023.3298188"
    },
    {
      "citation_id": "493",
      "title": "The European ST-T Database: standard for evaluating systems for the analysis of ST-T changes in ambulatory electrocardiography",
      "authors": [
        "A Taddei",
        "G Distante",
        "M Emdin",
        "P Pisani",
        "G Moody",
        "C Zeelenberg",
        "C Marchesi"
      ],
      "year": "1992",
      "venue": "European Heart Journal",
      "doi": "10.1093/oxfordjournals.eurheartj.a060332"
    },
    {
      "citation_id": "494",
      "title": "Scalable NPairLoss-Based Deep-ECG for ECG Verification",
      "authors": [
        "Y.-S Tai",
        "Y.-T Chen",
        "-Y Wu"
      ],
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence Applications and Innovations (AIAI)",
      "doi": "10.1007/978-3-030-79150-6_5"
    },
    {
      "citation_id": "495",
      "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
      "authors": [
        "Y Taigman",
        "M Yang",
        "M Ranzato",
        "L Wolf"
      ],
      "year": "2014",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2014.220"
    },
    {
      "citation_id": "496",
      "title": "Multibiometric secure system based on deep learning",
      "authors": [
        "V Talreja",
        "M Valenti",
        "N Nasrabadi"
      ],
      "year": "2017",
      "venue": "IEEE Global Conference on Signal and Information Processing (GlobalSIP)",
      "doi": "10.1109/GlobalSIP.2017.8308652"
    },
    {
      "citation_id": "497",
      "title": "Group Emotion Recognition with Individual Facial Emotion CNNs and Global Image Based CNNs",
      "authors": [
        "L Tan",
        "K Zhang",
        "K Wang",
        "X Zeng",
        "X Peng",
        "Y Qiao"
      ],
      "year": "2017",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3136755.3143008"
    },
    {
      "citation_id": "498",
      "title": "Toward Improving Electrocardiogram (ECG) Biometric Verification using Mobile Sensors: A Two-Stage Classifier Approach",
      "authors": [
        "R Tan",
        "M Perkowski"
      ],
      "year": "2017",
      "venue": "Sensors",
      "doi": "10.3390/s17020410"
    },
    {
      "citation_id": "499",
      "title": "Robust cancellable biometrics scheme based on neural networks",
      "authors": [
        "M Tarek",
        "O Ouda",
        "T Hamza"
      ],
      "year": "2016",
      "venue": "IET Biometrics",
      "doi": "10.1049/iet-bmt.2015.0045"
    },
    {
      "citation_id": "500",
      "title": "Seeley's Principles of Anatomy and Physiology",
      "authors": [
        "P Tate"
      ],
      "year": "2009",
      "venue": "Seeley's Principles of Anatomy and Physiology"
    },
    {
      "citation_id": "501",
      "title": "Human identification using time normalized QT signal and the QRS complex of the ECG",
      "authors": [
        "M Tawfik",
        "H Selim",
        "T Kamal"
      ],
      "year": "2010",
      "venue": "International Symposium on Communication Systems Networks and Digital Signal Processing (CSNDSP)",
      "doi": "10.1109/CSNDSP16145.2010.5580317"
    },
    {
      "citation_id": "502",
      "title": "Biohashing: two factor authentication featuring fingerprint data and tokenised random number",
      "authors": [
        "A Teoh",
        "D Ngo",
        "A Goh"
      ],
      "year": "2004",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2004.04.011"
    },
    {
      "citation_id": "503",
      "title": "ECG biometric using 2D Deep Convolutional Neural Network",
      "authors": [
        "S Thentu",
        "R Cordeiro",
        "Y Park",
        "N Karimian"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Consumer Electronics (ICCE)",
      "doi": "10.1109/ICCE50685.2021.9427616"
    },
    {
      "citation_id": "504",
      "title": "BioECG: Improving ECG Biometrics with Deep Learning and Enhanced Datasets. Applied Sciences",
      "authors": [
        "P Tirado-Martin",
        "R Sanchez-Reillo"
      ],
      "year": "2021",
      "venue": "BioECG: Improving ECG Biometrics with Deep Learning and Enhanced Datasets. Applied Sciences",
      "doi": "10.3390/app11135880"
    },
    {
      "citation_id": "505",
      "title": "QRS Differentiation to Improve ECG Biometrics under Different Physical Scenarios Using Multilayer Perceptron",
      "authors": [
        "P Tirado-Martin",
        "J Liu-Jimenez",
        "J Sanchez-Casanova",
        "R Sanchez-Reillo"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10196896"
    },
    {
      "citation_id": "506",
      "title": "Deep learning and the information bottleneck principle",
      "authors": [
        "N Tishby",
        "N Zaslavsky"
      ],
      "year": "2015",
      "venue": "IEEE Information Theory Workshop (ITW)",
      "doi": "10.1109/ITW.2015.7133169"
    },
    {
      "citation_id": "507",
      "title": "Estimating the universal positions of wireless body electrodes for measuring cardiac electrical activity",
      "authors": [
        "I Tomašić",
        "S Frljak",
        "R Trobec"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2013.2276291"
    },
    {
      "citation_id": "508",
      "title": "An approach to QRS complex detection using mathematical morphology",
      "authors": [
        "P Trahanias"
      ],
      "year": "1993",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/10.212060"
    },
    {
      "citation_id": "509",
      "title": "Self-supervised pretraining for image embedding. arXiv",
      "authors": [
        "T Trinh",
        "M Luong",
        "Q Le",
        "Selfie"
      ],
      "year": "2019",
      "venue": "Self-supervised pretraining for image embedding. arXiv"
    },
    {
      "citation_id": "510",
      "title": "Synthesis of the 12-lead electrocardiogram from differential leads",
      "authors": [
        "R Trobec",
        "I Tomašić"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Information Technology in Biomedicine",
      "doi": "10.1109/TITB.2011.2159236"
    },
    {
      "citation_id": "511",
      "title": "Self-supervised Learning of Motion Capture",
      "authors": [
        "H.-Y Tung",
        "H.-W Tung",
        "E Yumer",
        "K Fragkiadaki"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing Systems (NIPS)",
      "doi": "10.5555/3295222.3295276"
    },
    {
      "citation_id": "512",
      "title": "Data Augmentation of Wearable Sensor Data for Parkinson's Disease Monitoring using Convolutional Neural Networks",
      "authors": [
        "T Um",
        "F Pfister",
        "D Pischler",
        "S Endo",
        "M Lang",
        "S Hirche",
        "U Fietzek",
        "D Kulić"
      ],
      "year": "2017",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3136755.3136817"
    },
    {
      "citation_id": "513",
      "title": "Geometrical factors affecting the interindividual variability of the ECG and the VCG",
      "authors": [
        "A Van Oosterom",
        "R Hoekema",
        "G Uijen"
      ],
      "year": "2000",
      "venue": "Journal of Electrocardiology",
      "doi": "10.1054/jelc.2000.20356"
    },
    {
      "citation_id": "514",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2001.990517"
    },
    {
      "citation_id": "515",
      "title": "PTB-XL, a large publicly available electrocardiography dataset (version 1.0.1), Physionet",
      "authors": [
        "P Wagner",
        "N Strodthoff",
        "R Bousseljot",
        "W Samek",
        "T Schaeffter"
      ],
      "year": "2020",
      "venue": "PTB-XL, a large publicly available electrocardiography dataset (version 1.0.1), Physionet",
      "doi": "10.13026/x4td-x982"
    },
    {
      "citation_id": "516",
      "title": "PTB-XL, a large publicly available electrocardiography dataset. Scientific Data",
      "authors": [
        "P Wagner",
        "N Strodthoff",
        "R.-D Bousseljot",
        "D Kreiseler",
        "F Lunze",
        "W Samek",
        "T Schaeffter"
      ],
      "year": "2020",
      "venue": "PTB-XL, a large publicly available electrocardiography dataset. Scientific Data",
      "doi": "10.1038/s41597-020-0495-6"
    },
    {
      "citation_id": "517",
      "title": "On Evaluating ECG Biometric Systems: Session-Dependence and Body Posture",
      "authors": [
        "S Wahabi",
        "S Pouryayevali",
        "S Hari",
        "D Hatzinakos"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Information Forensics and Security",
      "doi": "10.1109/TIFS.2014.2360430"
    },
    {
      "citation_id": "518",
      "title": "Electrocardiogram Identification: Use a Simple Set of Features in QRS Complex to Identify Individuals",
      "authors": [
        "T Waili",
        "R Nor",
        "A Rahman",
        "K Sidek",
        "A Ibrahim"
      ],
      "year": "2016",
      "venue": "International Conference on Computing and Information Technology (IC2IT)",
      "doi": "10.1007/978-3-319-40415-8_14"
    },
    {
      "citation_id": "519",
      "title": "A Hasty Approach to ECG Person Identification",
      "authors": [
        "T Waili",
        "R Nor",
        "H Yaacob",
        "K Sidek",
        "A Rahman"
      ],
      "year": "2016",
      "venue": "International Conference on Computer and Communication Engineering (ICCCE)",
      "doi": "10.1109/ICCCE.2016.65"
    },
    {
      "citation_id": "520",
      "title": "The Devil of Face Recognition Is in the Noise",
      "authors": [
        "F Wang",
        "L Chen",
        "C Li",
        "S Huang",
        "Y Chen",
        "C Qian",
        "C Loy"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-030-01240-3_47"
    },
    {
      "citation_id": "521",
      "title": "Human Identification From ECG Signals Via Sparse Representation of Local Segments",
      "authors": [
        "J Wang",
        "M She",
        "S Nahavandi",
        "A Kouzani"
      ],
      "year": "2013",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2013.2267593"
    },
    {
      "citation_id": "522",
      "title": "Multi-scale differential feature for ECG biometrics with collective matrix factorization",
      "authors": [
        "K Wang",
        "G Yang",
        "Y Huang",
        "Y Yin"
      ],
      "year": "2020",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2020.107211"
    },
    {
      "citation_id": "523",
      "title": "STERLING: Towards Effective ECG Biometric Recognition",
      "authors": [
        "K Wang",
        "G Yang",
        "L Yang",
        "Y Huang",
        "Y Yin"
      ],
      "year": "2021",
      "venue": "IEEE International Joint Conference on Biometrics (IJCB)",
      "doi": "10.1109/IJCB52358.2021.9484360"
    },
    {
      "citation_id": "524",
      "title": "Deep face recognition: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.10.081"
    },
    {
      "citation_id": "525",
      "title": "Privacy-Aware Environmental Sound Classification for Indoor Human Activity Recognition",
      "authors": [
        "W Wang",
        "F Seraj",
        "N Meratnia",
        "P Havinga"
      ],
      "year": "2019",
      "venue": "ACM International Conference on Pervasive Technologies Related to Assistive Environments (PETRA)",
      "doi": "10.1145/3316782.3321521"
    },
    {
      "citation_id": "526",
      "title": "Disentangled Representation with Dual-stage Feature Learning for Face Anti-spoofing",
      "authors": [
        "Y.-C Wang",
        "C.-Y Wang",
        "S.-H Lai"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV51458.2022.00130"
    },
    {
      "citation_id": "527",
      "title": "Deep spatial gradient and temporal depth learning for face anti-spoofing",
      "authors": [
        "Z Wang",
        "Z Yu",
        "C Zhao",
        "X Zhu",
        "Y Qin",
        "Q Zhou",
        "F Zhou",
        "Z Lei"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00509"
    },
    {
      "citation_id": "528",
      "title": "A new deep-learning framework for group emotion recognition",
      "authors": [
        "Q Wei",
        "Y Zhao",
        "Q Xu",
        "L Li",
        "J He",
        "L Yu",
        "B Sun"
      ],
      "year": "2017",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3136755.3143014"
    },
    {
      "citation_id": "529",
      "title": "Biometrie identification from raw ECG signal using deep learning techniques",
      "authors": [
        "L Wieclaw",
        "Y Khoma",
        "P Fałat",
        "D Sabodashko",
        "V Herasymenko"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)",
      "doi": "10.1109/IDAACS.2017.8095063"
    },
    {
      "citation_id": "530",
      "title": "Explainable face recognition",
      "authors": [
        "J Williford",
        "B May",
        "J Byrne"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-030-58621-8_15"
    },
    {
      "citation_id": "531",
      "title": "Face recognition by elastic bunch graph matching",
      "authors": [
        "L Wiskott",
        "J.-M Fellous",
        "N Kruger",
        "C Der Malsburg"
      ],
      "year": "1997",
      "venue": "International Conference on Image Processing (ICIP)",
      "doi": "10.1109/ICIP.1997.647401"
    },
    {
      "citation_id": "532",
      "title": "Face recognition in unconstrained videos with matched background similarity",
      "authors": [
        "L Wolf",
        "T Hassner",
        "I Maoz"
      ],
      "year": "2011",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2011.5995566"
    },
    {
      "citation_id": "533",
      "title": "Wolfram Language and System Documentation Center",
      "year": "2010",
      "venue": "Wolfram Language and System Documentation Center"
    },
    {
      "citation_id": "534",
      "title": "International Conference on Pattern Recognition (ICPR)",
      "authors": [
        "B Wu",
        "G Yang",
        "L Yang",
        "Y Yin"
      ],
      "year": "2018",
      "venue": "International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR.2018.8545285"
    },
    {
      "citation_id": "535",
      "title": "Verification of humans using the electrocardiogram",
      "authors": [
        "G Wübbeler",
        "M Stavridis",
        "D Kreiseler",
        "R.-D Bousseljot",
        "C Elster"
      ],
      "year": "2007",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2007.01.014"
    },
    {
      "citation_id": "536",
      "title": "Inside Out: Fusing ECG and Face Information to Recognise Emotions",
      "authors": [
        "M Xavier"
      ],
      "year": "2022",
      "venue": "Inside Out: Fusing ECG and Face Information to Recognise Emotions"
    },
    {
      "citation_id": "537",
      "title": "Unknown presentation attack detection with face RGB images",
      "authors": [
        "F Xiong",
        "W Abdalmageed"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS.2018.8698574"
    },
    {
      "citation_id": "538",
      "title": "Bimodal biometrics based on a representation and recognition approach",
      "authors": [
        "Y Xu",
        "A Zhong",
        "J Yang",
        "D Zhang"
      ],
      "year": "2011",
      "venue": "Optical Engineering",
      "doi": "10.1117/1.3554740"
    },
    {
      "citation_id": "539",
      "title": "Supervised contrastive learning for generalizable and explainable deepfakes detection",
      "authors": [
        "Y Xu",
        "K Raja",
        "M Pedersen"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
      "doi": "10.1109/WACVW54805.2022.00044"
    },
    {
      "citation_id": "540",
      "title": "The Biometric Menagerie. IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "authors": [
        "N Yager",
        "T Dunstone"
      ],
      "year": "2010",
      "venue": "The Biometric Menagerie. IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2008.291"
    },
    {
      "citation_id": "541",
      "title": "WIDER FACE: A Face Detection Benchmark",
      "authors": [
        "S Yang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.596"
    },
    {
      "citation_id": "542",
      "title": "Face Anti-Spoofing: Model Matters, so Does Data",
      "authors": [
        "X Yang",
        "W Luo",
        "L Bao",
        "Y Gao",
        "D Gong",
        "S Zheng",
        "Z Li",
        "W Liu"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2019.00362"
    },
    {
      "citation_id": "543",
      "title": "miBEAT Based Continuous and Robust Biometric Identification System for On-the-Go Applications",
      "authors": [
        "J Yathav",
        "A Bailur",
        "A Goyal"
      ],
      "year": "2017",
      "venue": "International Conference on Communication and Networks (ComNet)",
      "doi": "10.1007/978-981-10-2750-5_28"
    },
    {
      "citation_id": "544",
      "title": "Investigation of human identification using two-lead Electrocardiogram (ECG) signals",
      "authors": [
        "C Ye",
        "M Coimbra",
        "B Kumar"
      ],
      "year": "2010",
      "venue": "IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS.2010.5634478"
    },
    {
      "citation_id": "545",
      "title": "Towards interpretable face recognition",
      "authors": [
        "B Yin",
        "L Tran",
        "H Li",
        "X Shen",
        "X Liu"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00944"
    },
    {
      "citation_id": "546",
      "title": "Evaluation of synthesized electrocardiogram on additional leads based on clinical data",
      "authors": [
        "Y Yoshida",
        "X Zhu",
        "W Chen",
        "D Wei"
      ],
      "year": "2012",
      "venue": "IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems (VECIMS)",
      "doi": "10.1109/VECIMS.2012.6273186"
    },
    {
      "citation_id": "547",
      "title": "Searching central difference convolutional networks for face anti-spoofing",
      "authors": [
        "Z Yu",
        "C Zhao",
        "Z Wang",
        "Y Qin",
        "Z Su",
        "X Li",
        "F Zhou",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00534"
    },
    {
      "citation_id": "548",
      "title": "A survey on face detection in the wild: Past, present and future",
      "authors": [
        "S Zafeiriou",
        "C Zhang",
        "Z Zhang"
      ],
      "year": "2015",
      "venue": "Computer Vision and Image Understanding",
      "doi": "10.1016/j.cviu.2015.03.015"
    },
    {
      "citation_id": "549",
      "title": "ECG based authentication for e-healthcare systems: Towards a secured ECG features transmission",
      "authors": [
        "E Zaghouani",
        "A Benzina",
        "R Attia"
      ],
      "year": "2017",
      "venue": "International Wireless Communications and Mobile Computing Conference (IWCMC)",
      "doi": "10.1109/IWCMC.2017.7986553"
    },
    {
      "citation_id": "550",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "M Zeiler",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision (ECCV)",
      "doi": "10.1007/978-3-319-10590-1_53"
    },
    {
      "citation_id": "551",
      "title": "Graph-based semi-supervised learning with multiple labels",
      "authors": [
        "Z Zha",
        "T Mei",
        "J Wang",
        "Z Wang",
        "X Hua"
      ],
      "year": "2009",
      "venue": "Journal of Visual Communication and Image Representation",
      "doi": "10.1016/j.jvcir.2008.11.009"
    },
    {
      "citation_id": "552",
      "title": "Directional binary code with application to PolyU near-infrared face database",
      "authors": [
        "B Zhang",
        "L Zhang",
        "D Zhang",
        "L Shen"
      ],
      "year": "2010",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2010.07.006"
    },
    {
      "citation_id": "553",
      "title": "Accurate face detection for high performance",
      "authors": [
        "F Zhang",
        "X Fan",
        "G Ai",
        "J Song",
        "Y Qin",
        "J Wu"
      ],
      "year": "2019",
      "venue": "arXiv"
    },
    {
      "citation_id": "554",
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2016.2603342"
    },
    {
      "citation_id": "555",
      "title": "HeartID: A Multiresolution Convolutional Neural Network for ECG-Based Biometric Human Identification in Smart Health Applications",
      "authors": [
        "Q Zhang",
        "D Zhou",
        "X Zeng"
      ],
      "year": "2017",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2017.2707460"
    },
    {
      "citation_id": "556",
      "title": "Pulseprint: Single-arm-ECG biometric human identification using deep learning",
      "authors": [
        "Q Zhang",
        "D Zhou",
        "X Zeng"
      ],
      "year": "2017",
      "venue": "IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON)",
      "doi": "10.1109/UEMCON.2017.8249111"
    },
    {
      "citation_id": "557",
      "title": "The unreasonable effectiveness of deep features as a perceptual metric",
      "authors": [
        "R Zhang",
        "P Isola",
        "A Efros",
        "E Shechtman",
        "O Wang"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2018.00068"
    },
    {
      "citation_id": "558",
      "title": "S3FD: Single Shot Scale-Invariant Face Detector",
      "authors": [
        "S Zhang",
        "X Zhu",
        "Z Lei",
        "H Shi",
        "X Wang",
        "S Li"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.30"
    },
    {
      "citation_id": "559",
      "title": "ECG-based personal recognition using a convolutional neural network",
      "authors": [
        "Y Zhang",
        "Z Xiao",
        "Z Guo",
        "Z Wang"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2019.07.009"
    },
    {
      "citation_id": "560",
      "title": "ECG Biometrics Method Based on Convolutional Neural Network and Transfer Learning",
      "authors": [
        "Y Zhang",
        "Z Zhao",
        "C Guo",
        "J Huang",
        "K Xu"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning and Cybernetics (ICMLC)",
      "doi": "10.1109/ICMLC48188.2019.8949218"
    },
    {
      "citation_id": "561",
      "title": "Heart biometrics based on ECG signal by sparse coding and bidirectional long short-term memory",
      "authors": [
        "Y Zhang",
        "Z Zhao",
        "Y Deng",
        "X Zhang",
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-020-09608-9"
    },
    {
      "citation_id": "562",
      "title": "A New ECG Identification Method Using Bayes' Theorem",
      "authors": [
        "Z Zhang",
        "D Wei"
      ],
      "year": "2006",
      "venue": "IEEE Region 10 Conference (TENCON)",
      "doi": "10.1109/TENCON.2006.344146"
    },
    {
      "citation_id": "563",
      "title": "A Human ECG Identification System Based on Ensemble Empirical Mode Decomposition",
      "authors": [
        "Z Zhao",
        "L Yang",
        "D Chen",
        "Y Luo"
      ],
      "year": "2013",
      "venue": "Sensors",
      "doi": "10.3390/s130506832"
    },
    {
      "citation_id": "564",
      "title": "Learning Deep Features for Discriminative Localization",
      "authors": [
        "B Zhou",
        "A Khosla",
        "A Lapedriza",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.319"
    },
    {
      "citation_id": "565",
      "title": "A method of ECG template extraction for biometrics applications",
      "authors": [
        "X Zhou",
        "Y Lu",
        "M Chen",
        "S Bao",
        "F Miao"
      ],
      "year": "2014",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/EMBC.2014.6943663"
    },
    {
      "citation_id": "566",
      "title": "Semi-Supervised Learning Literature Survey",
      "authors": [
        "X Zhu"
      ],
      "year": "2006",
      "venue": "Semi-Supervised Learning Literature Survey"
    },
    {
      "citation_id": "567",
      "title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions",
      "authors": [
        "X Zhu",
        "Z Ghahramani",
        "J Lafferty"
      ],
      "year": "2003",
      "venue": "International Conference on Machine Learning (ICML)",
      "doi": "10.5555/3041838.3041953"
    },
    {
      "citation_id": "568",
      "title": "Conversion of the ambulatory ECG to the standard 12-lead ECG: a preliminary study",
      "authors": [
        "X Zhu",
        "K Yoshida",
        "W Yamanobe",
        "Y Yamamoto",
        "W Chen",
        "D Wei"
      ],
      "year": "2003",
      "venue": "IEEE EMBS Asian-Pacific Conference on Biomedical Engineering",
      "doi": "10.1109/APBME.2003.1302577"
    }
  ]
}