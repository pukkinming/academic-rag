{
  "paper_id": "2210.08713v2",
  "title": "Supervised Prototypical Contrastive Learning For Emotion Recognition In Conversation",
  "published": "2022-10-17T03:08:23Z",
  "authors": [
    "Xiaohui Song",
    "Longtao Huang",
    "Hui Xue",
    "Songlin Hu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically depending on contexts or speakers. In this paper, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical Network, the SPCL targets at solving the imbalanced classification problem through contrastive learning and does not require a large batch size. Meanwhile, we design a difficulty measure function based on the distance between classes and introduce curriculum learning to alleviate the impact of extreme samples. We achieve state-of-the-art results on three widely used benchmarks. Further, we conduct analytical experiments to demonstrate the effectiveness of our proposed SPCL and curriculum learning strategy. We release the code at https://github.com/caskcsg/SPCL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the development of online social networks, capturing emotions during conversations has gained increasing attention in both academia and industry  (Li et al., 2020; Shen et al., 2021; Wang et al., 2020; Ghosal et al., 2020; Song et al., 2022; Zhu et al., 2021) . Emotion recognition in conversation (ERC) is critical in many scenarios, such as chatbots, healthcare applications, mining opinions on social media, and so on  (Poria et al., 2019b) . The ERC task aims to identify different emotions at each turn within a conversation based on the transcript. A conversation often contains several speakers and runs several turns; thus, emotions can vary drastically during the conversation. Compared  to traditional text classification tasks, figuring out emotions needs not only one turn of textual utterance but also contextual information. An example of ERC is illustrated in Figure  1 .\n\nContrastive learning applied to self-supervised representation learning has seen a resurgence in recent years.  Khosla et al. (2020)  extended the self-supervised batch contrastive approach to the fully-supervised setting and show outperformance over cross-entropy loss in several benchmarks. Although CoG-BART  (Li et al., 2021)  has demonstrated the effectiveness of supervised contrastive learning (SCL) in the ERC task, there are still two issues worth to solve when building an ERC model with SCL: (1) As illustrated in Figure  2 , existing ERC datasets are often class-imbalanced and samples may not be able to meet appropriate positive/negative samples within a mini-batch. (2) Existing ERC datasets are usually collected in a multi-modal manner. There are some conversations whose textual information is insufficient to distinguish emotions. Training a textual ERC model with those extreme samples may lead to performance degradation.\n\nFor the first issue, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss, which integrates Prototypical Network  (Snell et al., 2017)  and supervised contrastive learning. SPCL maintains a representation queue for each category. At each training step, SPCL samples a certain number of representations from these queues as the support set and calculates a temporary prototype vector for each emotion category. These prototype vectors are used as samples of the corresponding class to compute the loss. SPCL ensures that each sample has at least one positive sample of the same category and negative samples of all other categories within a mini-batch. Experiments show that SPCL can work well in class-imbalanced scenarios and is less sensitive to the training batch size.\n\nTo alleviate the performance degradation caused by extreme samples, we combine curriculum learning  (Bengio et al., 2009)  with contrastive learning. We design a distance-based difficulty measure function. By sorting the training data via this function, we can schedule the training data in an easy-tohard fashion. Experimental results demonstrate the effectiveness of our proposed curriculum learning strategy. Finally, we utilize SimCSE  (Gao et al., 2021) , a pretrained language model trained with contrastive learning as our backbone model. Combining our proposed SPCL loss and curriculum learning strategy, we reach state-of-the-art results on three widely used benchmarks. In summary, our contributions can be concluded as follows:\n\n• We propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task, which can perform supervised contrastive learning efficiently on classimbalanced data and has no need for large batch size.\n\n• To the best of our knowledge, we are the first to combine supervised contrastive learning and curriculum learning for the ERC task.\n\n• We achieve state-of-the-art results on three widely used benchmarks. Experimental results further demonstrate the effectiveness of our proposed SPCL loss and curriculum learning strategy.  2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Most existing approaches focus on context modeling. They can be divided into sequence-based, graph-based, and knowledge-enhanced methods.\n\nSequence-based methods consider contextual information as utterance sequences. ICON  (Hazarika et al., 2018)  and HiGRU  (Jiao et al., 2019)  both use the gated recurrent unit to capture the context information. DialogRNN  (Majumder et al., 2019 ) is a recurrence-based method that models dialog dynamics with several RNNs. Dia-logueCRN  (Hu et al., 2021)  introduces multi-turn reasoning modules to model the ERC task from a cognitive perspective. CoMPM(Lee and Lee, 2021) models the context and speaker's memory via pretrained language models. For those graph-based methods, DialogGCN  (Hu et al., 2021)  and RGAT  (Ishiwatari et al., 2020)  build a graph upon the utterances nodes. ConGCN  (Zhang et al., 2019)  trades both speakers and utterances as nodes and builds a single graph upon the whole ERC dataset. DAG-ERC  (Shen et al., 2021)  uses a directed acyclic graph (DAG) to model the intrinsic structure within a conversation. Knowledgeenhanced methods  (Zhong et al., 2019; Zhu et al., 2021; Ghosal et al., 2020; Zhang et al., 2020)  usu-ally utilize external knowledge from ATIMOC  (Sap et al., 2019)  or ConceptNet  (Liu and Singh, 2004) . Besides individual models, several frameworks have also been proposed.  Yang et al. (2021)  developed an ERC-oriented hybrid curriculum learning framework and  Bao et al. (2022)  proposed a speaker-guided encoder-decoder framework, formulating the modeling of speaker interactions as a flexible component.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Learning",
      "text": "In the field of natural language processing, Sim-CSE  (Gao et al., 2021)  is a state-of-the-art contrastive learning framework for generating sentence embeddings, it can learn from unlabeled sentences or annotated pairs from natural language inference datasets.  Khosla et al. (2020)  extend the self-supervised batch contrastive approach to the fully-supervised setting to make full use of label information.  Yeh et al. (2021)  let the contrastive learning get rid of the dependence on large batch size. CoG-BART  (Li et al., 2021)  adapts supervised contrastive learning to make different emotions mutually exclusive to identify similar emotions better.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Definition",
      "text": "Given a collection of all speakers S, an emotion label set E and a conversation C, our goal is to identify speaker's emotion label at each conversation turn. A conversation is denoted as\n\nwhere s i ∈ S is the speaker and u i is the utterance of i-th turn.\n\nIn this paper, we focus on the real-time settings of ERC, in which model can only take previous turns\n\nas input to predict the emotion label y t of t-th turn.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context Modeling",
      "text": "We build a prompt-based context encoder upon SimCSE  (Gao et al., 2021)\n\nKim and Vossen (2021) indicated that it is difficult for the pretrained language model to distinguish the \"context\" (i.e.,\n\n) and target turn (i.e., s t , u t ). Inspired by prompt learning  (Liu et al., 2021) , we construct a prompt for the t-th turn as follows.\n\nThe full input of the encoder is C t ⊕ P t , where ⊕ is the concatenation operation. In order to let the encoder realize that the prompt contains the target sentence, for the training pair of t-th turn X t t = {C t ⊕ P t , y t }, we construct an additional training pair\n\nwhere l is the number of tokens in C t ⊕ P k , and d is the dimension of a token embedding. Then we use the embeddings of the special token <mask> from H k t as a representation of y k -th emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Supervised Prototypical Contrastive",
      "text": "Learning for ERC Supervised Contrastive Learning Supervised contrastive learning  (Khosla et al., 2020)  treats all examples with the same label in the batch as positive examples. A batch of N emotion representations generated via context encoder is denoted as\n\nThe vanilla supervised contrastive learning computes the loss L sup i for z i as follows,\n\nHere, G(z i , z j ) is a score function that can be dot production, cosine similarity, etc. In our work, we use the cosine similarity for G(•). τ ∈ R + is a scalar temperature parameter. A(i) ≡ I\\{z i } contains all representations in I except z i , and P (i) is the set of positive samples that have the same label with z i in a batch.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Prototypical Contrastive Learning",
      "text": "The object function L sup introduces contrastive learning into supervised learning scenarios but suffers from class-imbalanced problem. Due to the limitations of batch size, samples from the majority class (e.g., neutral emotion) of the dataset may see insufficient negative samples within a batch. At the same time, it is hard for samples from the minority class to meet positive samples.\n\nTo solve this issue, we design a supervised prototypical contrastive learning (SPCL) loss function, which introduces prototype vectors of each category into the L sup loss. First, we maintain a fixed-size representation queue for each emotion category. A representation queue for i-th emotion with size M is denoted as\n\nthen detach the gradient of ẑi and push it into Q i . Second, to calculate the prototype vector for i-th category, we randomly select K samples from Q i as the support set S K , then take the mean of support set as the prototype vector T i .\n\nWe can get at most C K M different prototypes through sampling even if the representation queue is not updated.\n\nAfter obtaining the prototype vectors, we treat each of them as an example of the corresponding category, so the sum of negative scores of z i can be calculated as follows,\n\nwhere y i is the emotion label of i-th sample. Simultaneously, the sum of positive scores of z i is also computed with the corresponding prototype vector.\n\nBased on Eq.(  10 ) and Eq.(  11 ), the SPCL loss can be formulated as follows,\n\nThe total SPCL loss of a batch is as follows,\n\nIn summary, by introducing the prototype vectors, the SPCL loss ensures that there are at least one positive pair and |E| -1 negative pairs for each sample in a batch.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Curriculum Learning",
      "text": "Existing ERC datasets are usually collected in a multi-modal fashion. When building a text-only ERC model, some utterances are not informative enough to judge the emotions. Training the model with these extreme samples will lead to performance degradation. In this paper, we try to use curriculum learning to alleviate this issue.\n\nDifficulty Measure Function To combine with contrastive learning, we propose a difficulty measure function based on the distance between classes. Let the total size of training set D train as L, the emotion representation of i-th data sample as z i , and the label of i-th data sample as y i . Before each training epoch, we first compute z i for all samples, then the center of k-th emotion is computed as follows,\n\nThe difficulty of i-th sample DIF(i) is calculate as follows,\n\ndis function here is cosine distance. This function has the following two properties:\n\n• The closer the sample is to the category center, the lower the difficulty.\n\n• For two samples with the same distance from the center within the category, the further away from the center of other categories, the lower the difficulty.\n\nCurriculum Strategy After sorting the entire training set, instead of directly splitting the training set, we design a sampling-based approach to construct a series of subsets ranging from easy to hard. Let R as the number of training epochs, to train the model at k-th epoch, we first generate a arithmetic progression a with a length of L, where a 1 = 1 -k/R and a L = k/R. Then we initialize a Bernoulli distribution with a and draw a binary random array R B from it. We use B to draw a subset D sub-k from training set for the current epoch, where D sub-k ≡ {x i ∈ D train |R B i = 1}. Obviously, D sub-0 mainly consists of easy samples and D sub-R mainly consists of hard samples. Compared to splitting the training set sequentially, the sampling-based approach provides a smoother difficulty variation for the model. The curriculum strategy is illustrated in line2 -line9 of Algorithm 1. We conduct a qualitative analysis of our curriculum learning strategy in Section 5.5.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training And Evaluation",
      "text": "Training The overall procedure of our proposed approach is illustrated in Algorithm 1. We first generate emotion representations for all samples in training set, then use them to compute difficulty for each sample. After sorting the training set based on difficulty, we sample a subset S K and train the context encoder on S K through the SPCL loss.\n\nEvaluation Since we computed the center of each class C when calculating SPCL loss, we can directly obtain the prediction through matching the centers as follows,\n\ncompute DIF(i), i = 1..L (Eq.15) 5:\n\ncompute prototype T j , j ∈ 1..|E| (Eq.9) 15: compute SPCL loss (Eq.10-Eq.13) 16: optimize(M ) 17:\n\nend for 18: end for 19: return M * , C j , j ∈ 1..|E| where p ic m indicates the probability that i-th sample belongs to category c, and the subscript m means p ic m is calculated through matching.\n\nFor comparison, we train an additional linear layer to predict the labels using cross-entropy loss,\n\nwhere W ∈ R dim×|E| is a trainable parameter. The gradient of z i is detached so the model is only optimized via contrastive learning loss.\n\nIn this paper, we use p i m to predict labels when SPCL is the loss function and use p i l for other cases.\n\n4 Experimental Settings",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "The code framework and initial weight of Sim-CSE come from Huggingface's Transformers(Wolf Models IEMOCAP MELD EmoryNLP COSMIC  (Ghosal et al., 2020)  65.28 65.21 38.11 DialogueCRN  (Hu et al., 2021)  66.46 63.42 38.91 DAG-ERC  (Shen et al., 2021)  68.03 63.65 39.02 TODKAT  (Zhu et al., 2021)  61.33 65.47 38.69 Cog-BART  (Li et al., 2021)  66.18 64.81 39.04 TUCORE-GCN_RoBERTa  (Lee and Choi, 2021)  -65.36 39.24 SGED + DAG-ERC  (Bao et al., 2022)  68   et al., 2020) . We use the AdamW optimizer and cosine learning rate schedule strategy. When constructing training samples, we restrict their length to less than 256. We search the hyper-parameters on the develop set. For all experiments in this paper, we keep the best checkpoint on the develop set, then report the results on the test set using the kept checkpoint. All experiments are conducted on Nvidia V100 GPU.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiments on three widely used benchmarks: MELD  (Poria et al., 2019a), EmoryNLP(Zahiri and Choi, 2018)  and IEMO-CAP  (Busso et al., 2008) .\n\nMELD This dataset has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear.\n\nEmoryNLP This dataset comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox's feeling wheel  (Willcox, 1982) , i.e., Sad, Mad, Scared, Powerful, Peaceful, Joyful, and a default emotion of Neutral.\n\nIEMOCAP This dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (Angry, Excited, Fear, Sad, Surprised, Frustrated, Happy, Disappointed and Neutral). The dataset is recorded across 5 sessions with 5 pairs of speakers.\n\nThe Statistics of these datasets are listed in Table  2 . No.dials stands for the number of dialogues while No.uttrs stands for the total number of utterances in the dataset. No.CLS is the number of different emotions in the dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Metrics",
      "text": "From Figure  2  we can see class-imbalance in all three benchmarks, so we use weighted-F1 score as the metric for all experiments in this paper.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Main Results",
      "text": "We compare our proposed approach with stateof-the-art text-based ERC methods, and the results are presented in Table  1 . We can see that combining our proposed SPCL and curriculum learning strategy, we achieves state-of-the-art results on three benchmarks, which outperform previous SOTAs by 0.28%(CoMPM on IEMOCAP), 0.73%(CoMPM on MELD) and 0.7%(SGED + DAG-ERC on EmoryNLP).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the individual effects of SPCL and CL, we conducted a series of ablation experiments, and the results are shown in Table  3 . The first line in Table  3  shows the performances of our proposed prompt-based context encoder trained with crossentropy loss, which is our baseline model. We notice that curriculum learning didn't help a lot with cross-entropy loss. We believe that it is because we use the cosine distance in the difficulty measure function DIF. However, it is unreasonable to compute cosine distance directly on representations optimized via cross-entropy loss. The SupCon loss performs better than crossentropy loss on MELD and EmoryNLP datasets but slightly worse than on the IEMOCAP dataset. Combining the three results, we can see no significant performance gap between SupCon and crossentropy losses. But the combination of curriculum learning and SupCon(SupCon+CL) shows consistent superiority since SupCon uses cosine similarity as the score function. For the representations generated by SupCon, the cosine distance between representations of the same category will be closer, and the distance between representations of different categories will be distant. Therefore, the difficulty measure function of CL can be more faithful, resulting in better performance.\n\nThe SPCL loss outperforms SupCon and crossentropy losses on all three datasets. Meanwhile, it also has consistent performance improvements in combination with curriculum learning.\n\nTo summary, both our proposed SPCL and curriculum learning strategy contribute significantly to the results.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Using Spcl On Class Imbalanced Data",
      "text": "To demonstrate the superiority of SPCL on imbalanced data, we construct an imbalanced subset from MELD training set, as illustrated in Figure  4 . We sample  1024, 128, 64, 32, 32, 32, and 32  samples of neutral, joy, surprise, anger, sadness, disgust, and fear, respectively. We train the model using these two loss functions on the imbalanced training set. Since a small batch size will aggravate the impact of class-imbalance on contrastive learning, we conducted experiments with batch size of 4, 8, 16, and 32, respectively. The results on MELD test set are shown in Table  4 . We notice that using SPCL outperforms using SupCon in all four sets of experiments. As the batch size decreases from 32 to 4, the weighted-F1 score of SupCon loss drops 6.95% while SPCL drops 4.1%. We can conclude that in the class-imbalance scenarios: 1) both SupCon and SPCL need a larger batch size to reach satisfied performances; 2) introducing the prototypical network into contrastive learning can alleviate the impact of class-imbalance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Using Spcl With Small Batch Size",
      "text": "Contrastive learning approaches usually need a large batch size to ensure more positive/negative pairs within a batch, which leads high compu-tational cost. In the Section 5.3, we find that both SupCon and SPCL relay on large batch sizes. We conjecture that SPCL's dependence on batch size may be because we sample too few data samples for some categories(i.e., 32 for anger, sadness,disgust, and fear) to compute reasonable prototypes at the beginning of the training.\n\nIn order to further investigate the effect of batch sizes on SPCL, we apply SPCL to a more general scenario. From Figure  2     To conduct a qualitative analysis of our proposed curriculum learning strategy, we generate a toy dataset that contains three classes and visualize it in Figure  5(A) .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Qualitative Analysis Of Curriculum Learning",
      "text": "As illustrated in Figure  5(B-C ), the difficulty measure function DIF ranks the samples in a reasonable way. The easiest 20% samples(in green) are distributed in the center of their respective categories, while the hardest 20% samples(in red) are mainly on the boundaries between classes. In practice, we found that directly sorting the data with DIF cannot obtain satisfactory results. The model will overfit on simple samples in the early stage of training and produce large losses in the later stage, so we design the sampling-based curriculum learning strategy described in Section 3.4 to provide a smoother difficulty variation for the model. We control the difficulty of training subsets based on the sampling probability of samples with different difficulties. As illustrated in Figure  6 , the arithmetic progression a we used to sample downs from 1 to 0 at the first epoch, the sampling results are shown in Figure  6 (A), we can find that the most selected samples(in green) are around the centers while a few samples are away from the centers. When running to the last epoch, a grows from 0 to 1, so hard samples are in the majority of the subset, as shown in Figure  6(B) .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel loss function called Supervised Prototypical Contrastive Learning (SPCL) loss for the emotion recognition in conversation task. Combining with Prototypical Network, the SPCL loss outperforms the traditional supervised contrastive learning loss. It also works well on class-imbalanced data and is less sensitive to the training batch size, which reduces the requirement of computing resource. To further exploit the power of contrastive learning on ERC tasks, we design a distance-based difficulty measure function and introduce curriculum learning to alleviate the impact of extreme samples. We conduct experiments on three widely used benchmarks: IEMO-CAP, MELD, and EmoryNLP. Results show that our approach achieves state-of-the-art performance on all three datasets.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "This work has three limitations: 1) We introduce too many hyperparameters, which requires additional computing resources to search. 2) Our proposed difficulty measure function can not be combined with most existing ERC methods since it requires the emotion representations produced by the ERC model to be distance-aware. 3) We used multiple random sampling, resulting in unstable performance. The results in this paper are averaged with multiple seeds. In practice, we found that the results generated by different seeds may have significant variance.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of emotion recognition in conver-",
      "page": 1
    },
    {
      "caption": "Figure 1: Contrastive learning applied to self-supervised",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotion distributions of the three datasets.",
      "page": 2
    },
    {
      "caption": "Figure 3: The architecture of our prompt-based context",
      "page": 3
    },
    {
      "caption": "Figure 2: we can see class-imbalance in all",
      "page": 6
    },
    {
      "caption": "Figure 4: Emotion distribution of the extreme class-",
      "page": 7
    },
    {
      "caption": "Figure 4: We sample 1024, 128, 64, 32, 32, 32, and 32",
      "page": 7
    },
    {
      "caption": "Figure 2: we can see the IEMOCAP",
      "page": 8
    },
    {
      "caption": "Figure 5: Visualizations of how the difﬁculty measure",
      "page": 8
    },
    {
      "caption": "Figure 5: (B-C), the difﬁculty",
      "page": 8
    },
    {
      "caption": "Figure 6: The sampling-based curriculum strategy.",
      "page": 8
    },
    {
      "caption": "Figure 6: (A), we can ﬁnd that the most",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: We can see that",
      "data": [
        {
          "Models": "COSMIC(Ghosal et al., 2020)",
          "IEMOCAP": "65.28",
          "MELD": "65.21",
          "EmoryNLP": "38.11"
        },
        {
          "Models": "DialogueCRN (Hu et al., 2021)",
          "IEMOCAP": "66.46",
          "MELD": "63.42",
          "EmoryNLP": "38.91"
        },
        {
          "Models": "DAG-ERC (Shen et al., 2021)",
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "EmoryNLP": "39.02"
        },
        {
          "Models": "TODKAT (Zhu et al., 2021)",
          "IEMOCAP": "61.33",
          "MELD": "65.47",
          "EmoryNLP": "38.69"
        },
        {
          "Models": "Cog-BART (Li et al., 2021)",
          "IEMOCAP": "66.18",
          "MELD": "64.81",
          "EmoryNLP": "39.04"
        },
        {
          "Models": "TUCORE-GCN_RoBERTa(Lee and Choi, 2021)",
          "IEMOCAP": "-",
          "MELD": "65.36",
          "EmoryNLP": "39.24"
        },
        {
          "Models": "SGED + DAG-ERC(Bao et al., 2022)",
          "IEMOCAP": "68.53",
          "MELD": "65.46",
          "EmoryNLP": "40.24"
        },
        {
          "Models": "EmotonFlow-Large (Song et al., 2022)",
          "IEMOCAP": "-",
          "MELD": "66.50",
          "EmoryNLP": "-"
        },
        {
          "Models": "CoMPM (Lee and Lee, 2021)",
          "IEMOCAP": "69.46",
          "MELD": "66.52",
          "EmoryNLP": "38.93"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: We can see that",
      "data": [
        {
          "MELD\nIEMOCAP\nEmoryNLP": "1,432\n151\n827\n1,038\n100\n659\n114\n20\n89\n280\n31\n79"
        },
        {
          "MELD\nIEMOCAP\nEmoryNLP": "13,708\n7,333\n9,489\n9,989\n4,810\n7,551\n1,109\n1,000\n954\n2,610\n1,523\n984"
        },
        {
          "MELD\nIEMOCAP\nEmoryNLP": "7\n6\n7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Results of ablation study. Here, CE means",
      "data": [
        {
          "IEMOCAP": "68.35\n67.40",
          "MELD": "65.33\n65.63",
          "EmoryNLP": "38.72\n39.00"
        },
        {
          "IEMOCAP": "68.13\n68.64",
          "MELD": "65.67\n66.15",
          "EmoryNLP": "39.20\n39.49"
        },
        {
          "IEMOCAP": "69.03\n69.74",
          "MELD": "66.56\n67.25",
          "EmoryNLP": "40.14\n40.94"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speaker-guided encoderdecoder framework for emotion recognition in conversation",
      "authors": [
        "Yinan Bao",
        "Qianwen Ma",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "2",
      "title": "Curriculum learning",
      "authors": [
        "Yoshua Bengio",
        "Jérôme Louradour",
        "Ronan Collobert",
        "Jason Weston"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "SimCSE: Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "6",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "7",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "9",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "10",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "12",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2021",
      "venue": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "arxiv": "arXiv:2108.11626"
    },
    {
      "citation_id": "14",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "arxiv": "arXiv:2003.01478"
    },
    {
      "citation_id": "15",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2021",
      "venue": "Contrast and generation make bart a good dialogue emotion recognizer",
      "arxiv": "arXiv:2112.11202"
    },
    {
      "citation_id": "16",
      "title": "Conceptnet-a practical commonsense reasoning tool-kit",
      "authors": [
        "Hugo Liu",
        "Push Singh"
      ],
      "year": "2004",
      "venue": "BT technology journal"
    },
    {
      "citation_id": "17",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2021",
      "venue": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "arxiv": "arXiv:2107.13586"
    },
    {
      "citation_id": "18",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "authors": [
        "Maarten Sap",
        "Le Ronan",
        "Emily Bras",
        "Chandra Allaway",
        "Nicholas Bhagavatula",
        "Hannah Lourie",
        "Brendan Rashkin",
        "Noah Roof",
        "Yejin Smith",
        "Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "23",
      "title": "Prototypical networks for few-shot learning. Advances in neural information processing systems",
      "authors": [
        "Jake Snell",
        "Kevin Swersky",
        "Richard Zemel"
      ],
      "year": "2017",
      "venue": "Prototypical networks for few-shot learning. Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "26",
      "title": "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy",
      "authors": [
        "Gloria Willcox"
      ],
      "year": "1982",
      "venue": "Transactional Analysis Journal"
    },
    {
      "citation_id": "27",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "28",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2021",
      "venue": "Hybrid curriculum learning for emotion recognition in conversation",
      "arxiv": "arXiv:2112.11718"
    },
    {
      "citation_id": "29",
      "title": "Decoupled contrastive learning",
      "authors": [
        "Chun-Hsiao Yeh",
        "Cheng-Yao Hong",
        "Yen-Chi Hsu",
        "Tyng-Luh Liu",
        "Yubei Chen",
        "Yann Lecun"
      ],
      "year": "2021",
      "venue": "Decoupled contrastive learning",
      "arxiv": "arXiv:2110.06848"
    },
    {
      "citation_id": "30",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "31",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "32",
      "title": "Knowledge aware emotion recognition in textual conversations via multi-task incremental transformer",
      "authors": [
        "Duzhen Zhang",
        "Xiuyi Chen",
        "Shuang Xu",
        "Bo Xu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "34",
      "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
      "arxiv": "arXiv:2106.01071"
    }
  ]
}