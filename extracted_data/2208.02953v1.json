{
  "paper_id": "2208.02953v1",
  "title": "A Novel Enhanced Convolution Neural Network With Extreme Learning Machine: Facial Emotional Recognition In Psychology Practices",
  "published": "2022-08-05T02:21:34Z",
  "authors": [
    "Nitesh Banskota",
    "Abeer Alsadoon",
    "P. W. C. Prasad",
    "Ahmed Dawoud",
    "Tarik A. Rashid",
    "Omar Hisham Alsadoon"
  ],
  "keywords": [
    "Convolution Neural Network",
    "Stochastic Gradient Descent",
    "Log-likelihood Estimator",
    "Optical Flow Estimation",
    "Extreme Learning Machine",
    "Cross-Entropy Loss"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial emotional recognition is one of the essential tools used by recognition psychology to diagnose patients. Face and facial emotional recognition are areas where machine learning is excelling. Facial Emotion Recognition in an unconstrained environment is an open challenge for digital image processing due to different environments, such as lighting conditions, pose variation, yaw motion, and occlusions. Deep learning approaches have shown significant improvements in image recognition. However, accuracy and time still need improvements. This research aims to improve facial emotion recognition accuracy during the training session and reduce processing time using a modified Convolution Neural Network Enhanced with Extreme Learning Machine (CNNEELM). The proposed system consists of an optical flow estimation technique that detects the motion of change in facial expression and extracts peak images from video frames for image pre-processing. The system entails (CNNEELM) improving the accuracy in image registration during the training session. Furthermore, the system recognizes six facial emotionshappy, sad, disgust, fear, surprise, and neutral with the proposed CNNEELM model. The study shows that the overall facial emotion recognition accuracy is improved by 2% than the state of art solutions with a modified Stochastic Gradient Descent (SGD) technique. With the Extreme Learning Machine (ELM) classifier, the processing time is brought down to 65ms from 113ms, which can smoothly classify each frame from a video clip at 20fps. With the pre-trained InceptionV3 model, the proposed CNNEELM model is trained with JAFFE, CK+, and FER2013 expression datasets. The simulation results show significant improvements in accuracy and processing time, making the model suitable for the video analysis process. Besides, the study solves the issue of the large processing time required to process the facial images.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotions convey a large amount of unspeakable information that could be an essential tool used by psychology prpractitionersuring clinical diagnosis  [1, 2] . Human facial expressions are considered as a leading carrier to convey human emotion in communications. A study on non-verbal communication discloses that about 55% of human emotions are conveyed through facial expressions  [1] . Human facial expression provides an important clue to understand and analyze human emotion and their behaviors. In recent times, several research in emotion analysis have been carried out extensively, and different methods have been deployed to recognize facial expressions for various applications such as driver monitoring, customer service, digital entertainment, and emotion robots  [2] [3] [4] .\n\nIn traditional Facial Emotion Recognition (FER) approaches, first, the face and its components are detected, then, the spatial and temporal features are extracted by handcrafted methods, and finally classified with a pre-trained facial expression classifier. Even though these methods of implementation occur instantaneously, the recognition accuracy is low due to the local variation (pose, illumination, alignment, and occlusion) of facial components that affects the facial expression feature's vectors. Commonly in the traditional approach, machine learning techniques such as Completed Local Binary Patterns (CLBP), Local Ternary Patterns (LTP), Local Phase Quantization (LPQ), Rotation Invariant Co-occurrence (RICo), and Rotated Local Binary Pattern Image (RLBPI) are used for feature's extraction. However, Support Vector Machine (SVM), AdaBoost, and random forest are used for classification.The traditional approaches require comparatively low memory and computational power; however, Nitesh Banskota, Abeer Alsadoon, P.W.C. Prasad, Ahmed Dawoud, Tarik A. Rashid, Omar Hisham Alsadoon (2022). A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices. Multimedia Tools and Applications. DOI : 10.1007/s11042-022-13567-8 2 they require a large set of samples for training and various image pre-processing techniques. In recent years, traditional neural networking techniques are being replaced with a deep learning model, which has transformed the conventional way of machine learning. The multi-layer structure that forms a filter collection in the deep learning model differs from traditional machine learning  [5] . This technological model provides an advantage over conventional image processing frameworks by creating a complex network for solving image preprocessing, feature extraction and classification  [6] . Multiple deep network layers, commonly known as Convolution Neural Network (CNN) layers, act as a feature extractor that is independent of any classification task  [7] . Thus, features are learned directly by observing the input image  [8] . Though Convolution neural networking requires a large training dataset and high-performance computational power (GPUs and CPUs), its ability to automatically extracts features from images made it robust and effective.\n\nDeep neural networks have been utilized extensively for facial emotion recognition-significant improvements achieved by using CNN and Deep belief networks DBN  [10] . The recent studies carried on facial emotion recognition using various models of CNNs have achieved high accuracy of over 95% on emotion recognition using commonly available datasets like CK+, BU-3DEF and LFW datasets  [10] . However, in these studies, the processing time required for computation has been ignored. The lowest processing time observed in the current state of the art solution is 135ms  [10] . The contribution of this paper can be summarized as below,\n\n• Proposes a model called Convolutional Neural network Enhanced with Extreme Learning Machine (CNNEELM) classifier. • Our approach has proven improvements in the accuracy of ≈ 97% and reduced the processing time by half. • The proposed model recognizes the facial expression from an image or a video clip in the unconstrained environment at high accuracy and low processing time for tele-psychological practices in real-time.\n\nConvolution Neural Network enhanced with Extreme Learning Machine (CNNEELM) model aims to extract salient facial patches with saliency detection algorithms. These patches minimized image aberration like illumination, low image resolution, occlusion, etc., with Extreme Learning Machine (ELM) classifier for faster image classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "Facial Expression Recognition is a crucial human ability to determine how one person's expression conveyed the message of their emotion to another person. Research on developing such a computerized system similar to human perception are being carried out extensively  [11] . However, in most of the studies, successful results are obtained under a constrained environment. In the last few years, numerous algorithms have been developed by researchers and have been suggested for FER. In these methodologies, the commonalities in most of the approaches are facial component detection, feature extraction, and expression classification. Several researchers have noticed that adopting the concept of Deep Learning with a convolution neural network model has better recognition and classification rate than the traditional machine learning techniques  [2] [3] [4] . Many researchers have proposed their own models to improve the overall accuracy of state-of-art systems so that a highly reliable system can be introduced to recognize facial emotion from human expression. In the below sections, a literature review of various authors is observed based on several stages, which are: pre-processing, feature extraction, and classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Processing",
      "text": "Wu et al.  [12]  examined how to customize the generic model when label information is not available in testing samples that causing a degradation in performance for emotion recognition system in terms of accuracy. In order to solve this problem, they used a feature mapping technique called Weighted Center Regression Adaptive Feature Mapping (W-CR-AFM) that adapts to the feature of new samples, which do not have labeled information, and correcting some misclassified samples; With their technique, they improve the recognition accuracy by about 3.01%, 0.49%, and 5 3 facial image that eliminating the image distortions. The average accuracy of 94.09% on the multi-view BU-3DEF dataset, 99.02% on CK+ and JAFFE frontal facial datasets, and 60.9% on the LFW dataset were achieved. However, the prediction time of the method is more than 0.05 sec per frame for real-time video analysis, which is must be reduced for better efficiency. This method can be enhanced by using optical flow estimation to improve the quality of a saliency map so that only the required area of the facial image is made ready for system training. Li et al.  [13]  investigated the challenges that have to be faced while optimally combining the facial images of 2D and 3D face information for expression prediction to solve the challenging problems of illumination and pose variations. They proposed a solution to this problem by introducing learning-based feature-level fusion between SVM and Deep CNN. Therefore, the weights of 2D and 3D facial representations can be combined at the optimal size for multi-modal FER. Their solution is achieved the overall average recognition accuracy of 78.9% using two 3D face datasets which are BU-3DFE and Bosphorus. On the other hand, fusing the features between 2D and 3D faces can generate redundant features, which increases the processing time. This model can be enhanced by initializing the system with pre-trained CNN model and modifying the loss function for optimal training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "Zhang et al.  [4]   The recognition accuracy can be refined more with pre-processing of peak frames where only salient features of the face are detected and used to train the system. However, while processing only the peak frame, the frame with a micro-expression can be missed.\n\nPeng et al.  [16]  investigated that emotions from facial expressions can be misleading as someone may try to deceive others by showing opposite facial expressions. In solution to this problem, they used dual temporal scale CNN for Micro-Expression Recognition, which analyses frames in a video clip for unusual emotion expression detected via a quick change in expression between frames. Average classification accuracy of 66.67% was achieved by this system using spontaneous micro-expression databases (CASME, I/II). However, the processing time of the system is increased, and feature points could be in the wrong position if the testing facial images have colour variation. Furthermore, the fusion of this method with any other method is only going to increase the processing time. Hence, additional techniques do not offer a further area of improvement. et al.  [17]  evaluated the need for accurate recognition of micro-expression for lie detection without being in physical contact with an individual. To address this situation, they researched on the micro-expression recognition system with small sample size frame by transferring the Long-term Convolutional Neural Network (TLCNN) which improved the recognition accuracy of state of art by 7% using 560 micro-expression video clips. TLCNN uses Deep CNN to extract features from each frame of micro-expression video clips, then feeds them to Long Short-Term Memory (LSTM) which learn the temporal sequence information of micro-expression. The reduction of mini-batch size during the training can refine the system with improved training accuracy. However, no further improvement in accuracy could be achieved. Ahmed et al.  [18]  investigated the impact of constraints like pose dissimilarity, age, lighting conditions and occlusions that minimizes the accuracy for facial emotion recognition system in a wild situation. They offer a solution to this problem by implementing an incremental active learning framework with CNN on wild facial expression with pre-trained VGG16 model. Besides, the system used new data labeling using the total active learning method to reduce the network training time. The system achieved an overall accuracy of Nitesh Banskota, Abeer Alsadoon, P.W.C. Prasad, Ahmed Dawoud, Tarik A. Rashid, Omar Hisham Alsadoon (2022). A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices. Multimedia Tools and Applications. DOI : 10.1007/s11042-022-13567-8 4 72.9% using the Intelligent Technology Lab (ITLab) dataset. However, the system fails to identify the microexpression. Hence, the system can be enhanced by adding additional convolution layers for generalized feature extraction in VGG model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification",
      "text": "Ruiz-Garcia et al.  [19]  examined that the socially assistive robots in the modern world that require high-level image processing with an accurate result at low processing time. In order to address this problem, they combined Deep CNN as a feature extractor with SVM as a classifier to classify the human in seven basic human emotions for improved human-robot interaction. With their system implementation, they could achieve 96.26% accurate results on the Karolinska Directed Emotional Faces (KDFE) dataset. However, the accuracy dropped down to 68.78% when used in the real world. This performance issue can be resolved by balancing the histogram of an input image and by using a better camera for capturing images. Kaya et al.  [20]  examined the large variance in wild Facial Emotion Recognition that occurred due to uneven illumination, cluttered background as well as a sudden change in facial expression for multimodal audio-visual recognition resulting in audio lag. In order to improve processing time, they used Extreme Learning Machine (ELM) fused with Partial Least Square regression (well known for fast learning capabilities) as a classifier. Their method has resulted in a decrease in processing time by 42.5%.\n\nMoreover, the obtained accuracy of the proposed method is 98.47% on CK+ dataset and72.46% on MMI dataset respectively. This method can be refined more by combining this work with the system proposed by Jain et al.  [14]  for highly accurate facial expression recognition. Liu et al.  [10]  investigated how actual word application's factors such as head pose variation, occlusion, and poor image quality can drop the recognition accuracy of the FER system and how the system requires very large training data to train the model. To address these problems, they fused Random Forest with Conditional CNN as a feature extractor and used their state-of-art NCFS function to improve the learning capability of the system. Conditional CoNERF is devised to enhance decision trees with the capacity of representation learning from transferred convolutional neural networks and to model facial expressions of different perspectives with conditional probabilistic learning. Their system achieved an accuracy of 98.8% on CK+ dataset and 84.48% on JAFFE datasets. The number of training samples is highly reduced with NCFS function without compromising the recognition accuracy. The processing time of this system is around 113ms. The enhancement of this system could be the utilization of Extreme Learning Machine (ELM) classifier to classify the extracted features which reduce the processing time due to less number of hidden layer in this classifier. Though they have achieved high accuracy for still images, this model does not process real-time videos.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The State Of Art",
      "text": "In this section, the features of the current state-of-art system (highlighted inside the broken blue line in Fig.  1 ) and limitations (highlighted inside the broken red line in Fig.  1 ) are explained. Liu et al. proposed a novel conditional (CoNERF) for Facial Emotion Recognition in an unconstrained environment  [10] . Their proposed method classified facial expressions (fear, happy, sad, anger, surprise, and disgust) at a multi-angle with conditional probabilistic learning technique (Gaussian Mixture Model GMM).their method provides an average classification accuracy of 98.9% and 89.7% on BU-3DEF and LFW datasets, respectively. Moreover, the average processing time for performing a pose-aligned Facial Expression Recognition is about 113 ms. The recognition accuracy for images from the wild is about 15% lower than the dataset images used for testing. The motivation behind selecting  [10]    Deep Salient Feature Representation: This stage starts from detecting the face and extracting the salient deep features from saliency-guided facial patches with pre-trained CNN model. The saliency detection algorithm has been adopted to sample salient face patch that distinguishes the salient region in the face depending on the image signature which contains information about the foreground of the image and also shows the probability of that region in the image. The image signature is computed by DCT in the algorithm, underlying the usefulness of the descriptor for detecting salient image regions. The salient region includes the eyes, nose, mouth, and cheeks. The similarity between facial patches has been measured to sample more distinct facial patches. The pre-trained VGG model is retrained with all the facial patches from LWF and YTF datasets to obtain deep high-level features. These features will be later matched with the image obtained from the camera (webcam) to detect the facial expression at real-time. However, in this model, the salient features are extracted directly from the input images without preprocessing stage. Raw images can contain unwanted features that may cause an error in salience detection algorithm results. The image has to be purified using PCA analysis before feeding to the pre-trained CNN model. With image resizing to 48x48 for dimension reduction (a pre-processing technique), the computational time is also reduced.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Pose-Aligned Facial Expression Recognition Stage:",
      "text": "In this stage, saliency-guided facial patches are used to extract the deep salient features that were detected in the previous stage under different head poses. Liu et al.  [10]  utilized Neurally Connected Split Function (NCSF) to improve the learning capability of decision tree nodes with the splitting child nodes by deep learning representation with CoNERF method. Each decision node is split into the leaf node unless each leaf contains the label information for one facial expression. Stochastic Gradient Descent (SGD) approach has been employed with the loss function minimize the risk of false feature representation in leaf nodes. The risk is minimized by adapting a random-set of feature samples from different leaf nodes. These nodes are grouped as a mini-batch frame of samples, that is used to train the VGG model. Each mini-batch of images contains seven facial expressions where the total facial expression has five sets, for a total of 35 images per batch. The SoftMax classifier, which is a generalization to multiple classes, is used in the output layer for the classification of facial expressions. The performance of the system can be improved with the replacement of this classifier with a fast Extensive Learning Machine (ELM) classifier that has fewer numbers of hidden layers. This classification layer has been implemented to bring down the processing time by Kaya et al.  [20] .  Here,\n\nis a sigmoid function.\n\nx = input variable that represents NCFS function  (5)  To address the Multiview face-positions, convolution CoNERF is trained to estimate head poses in nine yaw axis -90o to +90o at 15o difference. The extracted features are nine probabilistic models for head posed are combined by adopting a Gaussian Mixture model, which is shown in equation(4)  [10] ,",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Proposed System",
      "text": "After analyzing several existing methods for Facial Emotion Recognition with Deep Learning Technique, we evaluate the merit and demerits of each method. Accuracy, processing time, image illumination, feature extraction and head-pose alignment  [4]    [10] [11] [12] [13] [14] [15] [16]  were the main issues to be considered. From the collected list of reviews, we selected the method proposed by Liu et al.  [10]  as the best solution since it extracts deep salient features from saliency-guided facial patches and uses a probabilistic learning method based on (GMM) for facial expression recognition to achieve an accurate outcome. This method represents and manipulates uncertainty from input data and prediction's Log Loss function. The sigmoid activation function adapted by Liu et al.  [10]  is best suited for predicting facial emotions for six different emotion classes due to the obtained accuracy and the processing time comparing to other systems. Moreover, some modifications in the classification stage can be implemented on this state of art solution with the help of selected second-best solution proposed by Kaya et al.  [20]  using (ELM) classifier instead of SoftMax classifier. See fig.  2  Kaya et al.  [20]  implemented Linear and Radial Basis Function (RBF) kernels of ELM classifier to enhance the learning capabilities of the system. ELM was initially proposed for generalized single-hidden-layer feedforward neural networks and overcome the local minima, learning rate, stopping criteria and learning epochs that exist in gradient-based methods such as back-propagation (BP). ELMs are widely used due to some significant advantages such as learning speed, ease of implementation and minimal human intervention. The potential for large-scale learning and artificial intelligence is preserved. The main steps of ELM include the random projection of a hidden layer with random input weight's algorithm. Since the number of layers is only two in ELM classifier, the processing time is highly reduced because the required learning times for two layers are less than multiple layers. This method has a negligible impact on the accuracy of the system because CNN is used for feature extraction, which is the main advantage of CNN  [19] . This approach has the capability to bring down the processing time of 113ms of the state of art method to around 65ms, which is suitable for expression detection in video analysis.\n\nAdditionally, an entire Optical Flow Estimator and pre-processing of image blocks are proposed in the feature representation stage to prepare clear images without noise and fewer pixels for the feature extraction stage. It controls the video frame flow rate to obtain an image for further processing and utilizes a Principal Component Analysis (PCA) technique in fig.  3  for false-positive face detection by measuring the mean reconstruction error per image, after projecting the images to the PCA space and back. The frames with a high reconstruction error are discarded, as these are probably poorly detected or poorly aligned images. These techniques have been used in the second-best solution by Kaya et al.  [20] .\n\nThe proposed system consists of three major stages (Fig.  2 ), (a). Pre-processing, (b). Feature Extraction, and (c) Classification. The stages in state of art solutions are modified as these three stages with some additional steps as shown in fig.  3  Preprocessing: In this stage, the images are obtained from video clips. These images are sequenced, and their flow rate is controlled with the optical flow estimator so that the peak frame from the video clip is made ready for pre-processing. The image thus obtained is converted into a grayscale image. PCA analysis which measures the mean reconstruction error per image, after projecting the images to the PCA space and back, is done to detect the false-positive faces so that those false images with a high reconstruction error, as these are probably poorly detected or poorly aligned can be discarded  [20] . The image is then scaled to 48x48 to get a unified features vector size in the feature extraction stage. As the state of art solution reviewed above, the images were not pre-processed, and the features are extracted directly from the obtained images. In this proposed paper, the images are purified with PCA so that the transfer learning process in the proposed CNNEELM model could be established efficiently.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Feature Extraction:",
      "text": "The salient region in the face (eyes, nose, cheeks and mouth) is detected and nine salient facial patches are created. These patches highlight sparse salient regions based on image signature and contain features that are to be extracted in the next stage. These patches are fed to pre-trained CNN, which represents the features in some numerical value. In the proposed system, Inception v3 is a widely-used image recognition model that is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, concerts, dropouts, and fully connected layers. InceptionV3 model has been used instead of VGG face pre-trained model because of its easy availability and less complexity in convolution layer alignments. Simultaneously, with the peak frame detected by the optical flow estimator, the video attention model is employed to leverage the spatial-temporal expression features in the frame sequence to improve efficiency  [10] . From this frame, the feature is extracted with Fisher Vector (FV) encoding technique proposed by kaya et al.  [20] , which is not a deep learning technique. FV provides a supra-frame encoding of the local descriptors, quantifying the gradient of the parameters of the background model with respect to the data.\n\nThe state of art solution did not employ the video attention model; thus, the fusion of deep learned network and machine learning was not important. The proposed system fuses these two methods to improve the accuracy in feature representation.\n\nClassification: For classification, the Softmax classifier of CNN, which use the 6-dimensional expression probabilities for prediction, is replaced with the ELM classifier layers for better predictive analysis. The advantage of ELM classifier is that it can classify the features very quickly into different classes. Proposed by Huang et al.  [21] , ELM classifier is a feedforward neural network with single hidden layer feedforward networks (SLFNs) nods. Sic it used only SLFNs nods the ELM classifier is very popular, which boosts up the training speed and reduces overfitting and prediction parameters to a new plane to determine linear regression. . In the proposed system, ELM classifier is used instead of SoftMax classifier used by Liu et al.  [10]  in order to get the best possible results.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Proposed Equation:",
      "text": "The stochastic gradient descent SGD technique is employed with the loss function in equation (  1 ) with the logloss term as equation (  2 )  [10] . SGD can converge faster than batch training of extracted features because it performs updates more frequently. Mini-batch samples of features for training are major characteristics of SGD. This can be faster than training on single data points because it can take advantage of vectored operations to process the entire mini-batch at once. The performance of SGD can be improved with the replacement of Log loss function  [10] .\n\nThe log value of the probability function gives the negative result which is neutralized by the negative sign to make the calculation easier. However, equation (  2 ) does not smoothen the stochastic gradient descent curve from equation (  5 ), which may result in some predictions being unconsidered within the curve.\n\nLog Loss calculates the uncertainty of the prediction depending on how much it varies from the original input. However, in our proposed system, the log-loss term is entirely replaced with the maximum likelihood function in order to avoid the problem of overfitting. So, the function could give the maximum likeliness of prediction to fall under the curve. The modifying log-likelihood estimator(LI(Y, ; P) ) function is shown in equation (  7 )  [22] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Li(Y, ",
      "text": "Our Modified Stochastic gradient descent for maximum likelihood estimator function for loss is presented in equation (  8 )\n\nWhere, LI = Likelihood estimator n = total number of samples p = facial expression probability x = input variable\n\nIn the above equation (  8 ), it can be observed that the subtraction of variables inside summation in equation (  5 ) is replaced with the multiplication sign so that value of the new function can be plotted in the logarithmic curve, which makes the proposed prediction model more accurate. The negative sign is introduced at the beginning of the right-hand side of equation (  8 ) to neutralize the negative parameterization of the log probability function. Additionally, the batch size is doubled, resulting in minimization of processing time during the training session.\n\nThe activation function named sigmoid function in equation (  4 ) in the state of art solution is also modified by us as equation  (9) .\n\nHence, the modified NCSF function will be presented as equation  (10) .\n\nThe exponential curve by sigmoid function is \"S\" shaped curve whose shape can be made flatter by multiplying the input parameter in the right-hand side of equation(  9 ) by half to obtain less varied output for rapid change in input. Towards the end of the sigmoid curve in equation (  4 ), there exists a vanishing point at which the change in the input does not cause the output to change. In order to push back the vanishing point further away, the proposed curve is made slightly flatter so that feature representation is done at a slower rate to improve accuracy in feature representation and a classification stage. This modification causes a negligible increase in processing time. We Nitesh Banskota, Abeer Alsadoon, P.W.C. Prasad, Ahmed Dawoud, Tarik A. Rashid, Omar Hisham Alsadoon (2022). A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices. Multimedia Tools and Applications. DOI : 10.1007/s11042-022-13567-8\n\npropose to take this modified function as our reference function to make a prediction for feature representation for six different classes.\n\nWe propose the modification of two equations (  1 ), (  4 ) from the state of art solution proposed by Liu et al.  [10] . First, the risk in feature extraction is high due to cross-entropy loss in Stochastic Gradient Decent approach. With the help of the proposed equation (  8 ), the likelihood of feature prediction is improved at each iteration, which minimizes the risk factor. Along with this, the sample size is doubled so that the system can be trained at a quicker rate. Second, the shape of the activation function is altered and has been made flatter with equation (  9 ). The enhanced activation sigmoid curve gives less varied output for rapid change in input. This results in a slower rate of learning at the training stage so that the system can achieve more accurate results during facial expression detection.\n\nExtreme Learning Machine(ELM) can easily classify the one-dimensional vectors which are obtained from the feature map of Convolution Neural Network fused. The facial expression features extracted from CNN is used to create a facial map used for gathering all the obtained features. This map is utilized by ELM to compute the hidden layer weight to get the best possible one with random input from the fully connected layer of CNN. Since ELM has only one hidden layer in its structure, the classification process is swift. Additionally, ELM maintains the feature generalizing ability at the reduced computational cost of classification under the proper selection of activation function (in our case, sigmoid function). Thus, this classifier is very suitable for emotion detection in video clips as it is twice as fast as CNN classifiers  [23] . The literature review in this paper shows that none of the authors has considered fusing CNN with ELM classifier to classify facial emotion. The proposed solution reduces the processing time without affecting the accuracy of feature extraction by CNN. However, the proposed CNNEELM model for facial emotion recognition is illustrated in Table  2 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation Of The Model",
      "text": "Our evaluation was implemented in Python 3.7.1 programming language with OpenCV and Tensorflow libraries. Three datasets CK+  [24]  JAFFE  [25]  and FER2013 was used to train and validate the model. 30,709 images in total were used to train the system. CK+ dataset contains eight emotional expression images from 123 subjects. There is a total of 593 image sequences, and most of the images are grey. The JAFFE dataset contains 6 peak emotions with 1 neutral expression class. Total of 10 Japanese models' facial expression is captured consisting of 213 static images. Fig.  7  and Fig.  8  shows the number of images in each category for CK+ and JAFFE dataset. Five images from for six different classes (Happy, sad, surprise, disgust, fear and neutral) from CK+ has been randomly taken as a sample to validate the proposed model. The images of men and women of different ages are considered. The resolution of images is different for each dataset. For CK+ dataset the image resolution is 720x480 or 460x480; for JAFEE, it is 256x256 and for FER2013, it is 48x48 due to the video quality. The images are categorized into six different folders -happy, sad, surprise, disgust, neutral and fear with Python.\n\nIn CK+ dataset, the image sequence number was matched with their respective labels in label.txt file and were categorized with python code. The JAFFE dataset contained images, and their filename had emotion labels on them. The filename was split to get the category and images were classified. In the case of FER2013 dataset, the raw images were represented in the CSV file. There is total of 35,887 grayscaled images. The file contained three columnsemotion (0-5), image data and set(validating set and training set). The CSV file was converted to NumPy(.npy) file in Python which could be used directly to train the model. The images from all the datasets Nitesh Banskota, Abeer Alsadoon, P.W.C. Prasad, Ahmed Dawoud, Tarik A. Rashid, Omar Hisham Alsadoon (2022). A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices. Multimedia Tools and Applications. DOI : 10.1007/s11042-022-13567-8 were pre-processed with PCA analysis technique where all faces are scaled at 48x48 images and converted to grayscale.\n\nFor video analysis, an image sequence from CK+ dataset, at 23fps, showing different facial emotions were fed into a system to determine the processing time of the system. The video length is only 10 seconds. In order to calculate the processing time, the time at which frame in input to the system is subtracted from output is computed. Some of the frames that are irrelevant to the experiment has been ignored.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Result",
      "text": "In order to understand the influence of accuracy and processing time, we conducted an experiment with deeply extracted features from CNN and ELM for classification and compared the results with the state of art solution.\n\nFrom Table  3 , it can be observed that in most of the cases, the accuracy is better than the results obtained from state of art. Additionally, the processing time in the proposed solution is almost half of the state of art processing times. Here, we have been able to bring the processing time from around 113 ms to 60ms on average. This results in better system response to video data.\n\nAll the samples for experiments are taken from all three datasets, with each category containing five images from CK+ database. With the help of graphs and tables, the proposed system is compared with the state of art. The results of the experiments performed on few samples for facial expression classification are listed in Table  3 .\n\nFor feature extraction, during the training session, the bottleneck files are created in text format with an index number so that a dictionary of training images could be created. In Python, a graph is created that returns the graph object. The graph holds the trained network and other various tensors. The pre-trained InceptionV3 network is trained with the bottleneck files. The network is updated with new files that plot a new prediction graph on output graph file in .pb format. This graph is used later during image recognition to classify the facial image.\n\nIn testing samples in Table  3 , there are cases where the change in accuracy is significantly high in some cases and similar in others. Considering sample 3.4 from Table  3  for sad face class, the original image is a little blurred due to the image noise which affected the accuracy in recognition in the state of art method. Our proposed model preprocessed the image with PCA analysis, which discards the images with high reconstruction error, improving the lighting intensities of the pixel, hence improving the accuracy. See Figures  (4) (5) (6) (7) (8) (9) (10) (11) (12) (13)  Fig.  6 , 7, and 8 show the average accuracy of facial expression classification in all image categories (happy, sad, disgusted, fear, neutral, surprise and angry) in CK+, FER2013 and JAFFE datasets, respectively. The graph shows that the accuracies for each emotion category are in the range of 85% to the high end of 90% for the various selected emotions. In all three datasets, the accuracy for 'disgust' and 'fear' is less as compared to other categories because of the similarity in extracted facial features. 10% of the images selected randomly from each dataset were separated before training the model. These images are used for testing the accuracy and processing time of the proposed model. The average of accuracy and processing time for each sample is computed to obtain the overall accuracy and processing time in all the datasets. Fig.  9  shows that the overall accuracy has been improved in the proposed solution.  shows that as the system is trained with facial images, the accuracy is low for both the training set and validation set in the beginning but at the end, the validation accuracy is above 95% and is consistent. As the number of epochs increases the cross-entropy for training is decreasing and has reached less than 0.1, whereas, for validation, the cross-entropy is around 0.3.\n\nAll these values are calculated in the TensorFlow library. While developing the system, the tensors are attached with summary modules, which log all the necessary parameters required to generate a graph during and after the training session. The summary log includes curves for training and validation accuracy during training sessions (fig.  11 ), cross-entropy loss (fig.  12 ) and standard deviation, mean for bias values and weights (Fig.  13 ). These curves are plotted in TensorBoard, which comes pre-installed in TensorFlow library. The smoothness of the curve is made high so that the change in the graph is distinctively visible.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Discussion",
      "text": "The comparison between state of the art and the proposed system shows the overall improvement inaccuracy in terms of image registration using the proposed CNNEELM model during the training session of the detected features. Likewise, the image-processing time of classification has been substantially decreased (as shown in table  3 ) during classification within fast ELM classifier hence increasing the system performance. The proposed system with enhanced equation (  8 ) improves the overall accuracy during image registration by roughly 2%. The difference between training accuracy and validation accuracy is minimal as shown in Fig.  11 , which means that no overfitting has occurred in the classification stage. During classification, the image frames are classified at around 20 fps for better recognition results, which show that the system can be adopted emotion recognition in video clips as well. These parameters are simulated in TensorFlow library by implementing both state of art and proposed systems. We compute the system recognition accuracy as in Fig.  9by  calculating the mean of loglikelihood estimation in the activation layer of the convolution neural network.\n\nThe main feature of the proposed system is the influence on the accuracy during image recognition as shown in Table  3 . The minimization of the risk factor of overfitting due to the adoption of the maximum-likelihood estimator in SGD provides an increase in accuracy by 2% during feature recognition than the state of art system. Zhao et al.  [22]  implemented a similar estimator to a study of bone marrow transplantation for patients with acute leukemia. ELM classifier has proven to be more advantageous in terms of processing time than traditional SoftMax classifier since it reduced the processing time by half than state of the art.\n\nDuring the pre-processing stage, both state of art and the proposed method use saliency detection technique in a similar fashion. The patch size of extracted features in the proposed system is 12x12 resolution, which is slightly bigger than state-of-the-art so that no features are ignored during facial patch sampling.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) are explained.   Liu et al. proposed a novel",
      "page": 4
    },
    {
      "caption": "Figure 1: and Table 1.",
      "page": 4
    },
    {
      "caption": "Figure 1: Flow diagram of State of the Art System (Liu et al., [10]). The dotted blue box represents the good feature, and the red box represents",
      "page": 5
    },
    {
      "caption": "Figure 2: ), (a). Pre-processing, (b). Feature Extraction, and (c)",
      "page": 7
    },
    {
      "caption": "Figure 2: Block Diagram of Proposed Solution. Green represents the additional proposed steps to state of art solution",
      "page": 8
    },
    {
      "caption": "Figure 3: The salient patch sampling in a face",
      "page": 11
    },
    {
      "caption": "Figure 4: total number of images for CK+ and JAFFE dataset for the",
      "page": 12
    },
    {
      "caption": "Figure 5: Total number of images for FER3013 dataset for the training",
      "page": 12
    },
    {
      "caption": "Figure 6: Accuracy of Emotion Classification in CK+ dataset with",
      "page": 12
    },
    {
      "caption": "Figure 7: Accuracy of Emotion Classification in FER2013 dataset",
      "page": 12
    },
    {
      "caption": "Figure 8: Accuracy of Emotion Classification JAFFE dataset with",
      "page": 13
    },
    {
      "caption": "Figure 6: -8: Comparison of classification accuracy between state of the art and proposed solution in (fig 6) CK+ dataset, (fig 7) FER2013",
      "page": 13
    },
    {
      "caption": "Figure 8: ) JAFFE dataset for each emotion – happy, sad, disgusted, fear, neutral, surprise and angry.",
      "page": 13
    },
    {
      "caption": "Figure 9: Overall average accuracy of the state of art method and",
      "page": 13
    },
    {
      "caption": "Figure 10: Overall processing time(in ms) for  state of art method and",
      "page": 13
    },
    {
      "caption": "Figure 11: Graph plot for 100 epochs showing Training accuracy (red) Vs. Validation accuracy (blue)",
      "page": 13
    },
    {
      "caption": "Figure 12: Cross Entropy loss during training session for 100 epochs for Training set (red) and Validation set (blue)",
      "page": 13
    },
    {
      "caption": "Figure 13: Graph plot for Bias and weight summaries during a training session for 100 epochs for training (red) and validation(blue) set (a)",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Algorithm for facial emotion recognition in state-of-art",
      "data": [
        {
          "Algorithm: Conditional convolution neural network enhanced random forest for facial expression recognition \nInput: CK+, JAFFE, multi-view BU-3DEF and LFW datasets \nOutput: Improved Facial Emotion Recognition model": "BEGIN \n1. \nDeep salient feature extraction from facial patches \n2. \nDetect of head pose 0 \n3. \nSplitting the nodes by NCSF \n4. \nCreate leaves nodes \n5. \nObtain the probability of the leave nodes \n6. \nClassify image  feature with CoNERF model \nEND"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Table 2: Algorithm of proposed CNNEELM model for facial emotion recognition",
      "data": [
        {
          "Algorithm: Proposed CNNEELM model for facial emotion recognition \nInput: Image containing facial image at real-time, CK+ dataset, JAFFE dataset and FER2013 dataset \nOutput: Facial Emotion Recognition": "START \n1. \nFeed the system with video frames \n2. \nDetect peak frame \ni. \nIf peak frame, advance to step 3 \nii. \nElse, go to step 2 \n3. \nExtract Features \ni. \nExtract salient features with a Saliency detection algorithm \nii. \nObtain spatial-temporal expression \n4. \nCheck whether the features are from the same peak frame, \ni. \nIf no, discard features and go to step 2 \n5. \nCombine features with CNN \n6. \nELM classification \n7. \nObtain Facial Emotion recognition \nEND"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Accuracy and Processing Time result for CK+ dataset",
      "data": [
        {
          "Emotion \nNo.": "",
          "Original image": "",
          "State of the art": "Accuracy \nin terms of \nProcessed \nProcessing \ncross-\nimage \nTime(ms) \nentropy \nloss (%)",
          "Proposed solution": "Accuracy in \nterms of \nProcessed \nProcessing \ncross-\nImage \nTime(ms \nentropy loss \n(%)"
        },
        {
          "Emotion \nNo.": "1. \nSample Images for Neutral Face",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "1.1",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "1.2",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "1.3",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "1.4",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "1.5",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "2. \nSample images for Happy Face",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        },
        {
          "Emotion \nNo.": "2.1 \n99.87 \n112 \n98.03 \n63",
          "Original image": "",
          "State of the art": "",
          "Proposed solution": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2": "2.3",
          "93.41": "92.76",
          "105": "113",
          "97.54": "93.65",
          "65": "67"
        },
        {
          "2.2": "2.4",
          "93.41": "96.47",
          "105": "115",
          "97.54": "96.78",
          "65": "65"
        },
        {
          "2.2": "2.5",
          "93.41": "98.79",
          "105": "132",
          "97.54": "98.56",
          "65": "63"
        },
        {
          "2.2": "3. \nSample Images for Sad Face",
          "93.41": "",
          "105": "",
          "97.54": "",
          "65": ""
        },
        {
          "2.2": "3.1",
          "93.41": "94.55",
          "105": "120",
          "97.54": "95.95",
          "65": "61"
        },
        {
          "2.2": "3.2",
          "93.41": "95.07",
          "105": "98",
          "97.54": "92.44",
          "65": "71"
        },
        {
          "2.2": "3.3",
          "93.41": "97.80",
          "105": "99",
          "97.54": "92.31",
          "65": "70"
        },
        {
          "2.2": "3.4",
          "93.41": "83.54",
          "105": "98",
          "97.54": "99.50",
          "65": "68"
        },
        {
          "2.2": "3.5",
          "93.41": "88.46",
          "105": "111",
          "97.54": "93.33",
          "65": "59"
        },
        {
          "2.2": "4. \nSample images for Angry Face",
          "93.41": "",
          "105": "",
          "97.54": "",
          "65": ""
        },
        {
          "2.2": "4.1",
          "93.41": "93.86",
          "105": "105",
          "97.54": "94.65",
          "65": "58"
        },
        {
          "2.2": "4.2",
          "93.41": "87.36",
          "105": "106",
          "97.54": "91.65",
          "65": "59"
        },
        {
          "2.2": "4.3",
          "93.41": "97.65",
          "105": "112",
          "97.54": "98.76",
          "65": "64"
        },
        {
          "2.2": "4.4",
          "93.41": "89.47",
          "105": "97",
          "97.54": "86.88",
          "65": "67"
        },
        {
          "2.2": "",
          "93.41": "",
          "105": "",
          "97.54": "",
          "65": ""
        },
        {
          "2.2": "5.1",
          "93.41": "98.96",
          "105": "107",
          "97.54": "95.76",
          "65": "66"
        },
        {
          "2.2": "5.2",
          "93.41": "97.34",
          "105": "103",
          "97.54": "98.68",
          "65": "61"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 3: ) during classification within fast ELM classifier hence increasing the system performance. The proposed system",
      "data": [
        {
          "5.3": "5.4",
          "93.75": "96.37",
          "108": "105",
          "92.76": "97.33",
          "60": "65"
        },
        {
          "5.3": "5.5",
          "93.75": "92.21",
          "108": "104",
          "92.76": "95.78",
          "60": "67"
        },
        {
          "5.3": "6. \nSample images for Disgust Face",
          "93.75": "",
          "108": "",
          "92.76": "",
          "60": ""
        },
        {
          "5.3": "6.1",
          "93.75": "85.15",
          "108": "108",
          "92.76": "91.43",
          "60": "63"
        },
        {
          "5.3": "6.2",
          "93.75": "82.36",
          "108": "113",
          "92.76": "86.30",
          "60": "70"
        },
        {
          "5.3": "6.3",
          "93.75": "78.27",
          "108": "112",
          "92.76": "88.64",
          "60": "72"
        },
        {
          "5.3": "6.4",
          "93.75": "80.54",
          "108": "119",
          "92.76": "90.43",
          "60": "77"
        },
        {
          "5.3": "6.5",
          "93.75": "88.67",
          "108": "116",
          "92.76": "91.76",
          "60": "71"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices",
      "authors": [
        "Nitesh Banskota",
        "P Abeer Alsadoon",
        "Ahmed Prasad",
        "Tarik Dawoud",
        "Omar Rashid",
        "Hisham Alsadoon"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-022-13567-8"
    },
    {
      "citation_id": "2",
      "title": "A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices",
      "authors": [
        "Nitesh Banskota",
        "P Abeer Alsadoon",
        "Ahmed Prasad",
        "Tarik Dawoud",
        "Omar Rashid",
        "Hisham Alsadoon"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-022-13567-8Reference"
    },
    {
      "citation_id": "3",
      "title": "Nonverbal communication",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2007",
      "venue": "Nonverbal communication"
    },
    {
      "citation_id": "4",
      "title": "Personality driven task allocation for emotional robot team",
      "authors": [
        "B Fang",
        "Q Zhang",
        "H Wang"
      ],
      "year": "2017",
      "venue": "International Journal of Machine Learning and Cybernetics"
    },
    {
      "citation_id": "5",
      "title": "A regularized ensemble framework of deep learning for cancer detection from multi-class, imbalanced training data",
      "authors": [
        "X Yuan",
        "L Xie",
        "M Abouelenien"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "A Deep Neural Network-Driven Feature Learning Method for Multi-view Facial Expression Recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "J Yan",
        "K Yan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "A Fast Learning Algorithm for Deep Belief Nets",
      "authors": [
        "G Hinton",
        "S Osindero",
        "Y Teh"
      ],
      "year": "2006",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "8",
      "title": "Deep learning in neural networks: An overview",
      "authors": [
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Pap smear image classification using convolutional neural network",
      "authors": [
        "K Bora",
        "M Chowdhury",
        "L Mahanta",
        "M Kundu",
        "A Das"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "10",
      "title": "HEp-2 Cell Classification Using K-Support Spatial Pooling in Deep CNNs. Deep Learning and Data Labeling for",
      "authors": [
        "X Han",
        "J Lei",
        "Y Chen"
      ],
      "year": "2016",
      "venue": "Medical Applications Lecture Notes in Computer Science"
    },
    {
      "citation_id": "11",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey",
      "arxiv": "arXiv:1804.08348"
    },
    {
      "citation_id": "12",
      "title": "Conditional convolution neural network enhanced random forest for facial expression recognition",
      "authors": [
        "Y Liu",
        "X Yuan",
        "X Gong",
        "Z Xie",
        "F Fang",
        "Z Luo"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Face Recognition: A Survey",
      "authors": [
        "M Sharif",
        "F Naz",
        "M Yasmin",
        "M Shahid",
        "A Rehman"
      ],
      "year": "2017",
      "venue": "Journal of Engineering Science and Technology Review",
      "doi": "10.25103/jestr.102.20"
    },
    {
      "citation_id": "14",
      "title": "Adaptive Feature Mapping for Customizing Deep Learning Based Facial Expression Recognition Model",
      "authors": [
        "B Wu",
        "C Lin"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Multimodal 2D 3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network",
      "authors": [
        "H Li",
        "J Sun",
        "Z Xu",
        "L Chen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Hybrid deep neural network for the emotion recognition",
      "authors": [
        "N Jain",
        "S Kumar",
        "A Kumar",
        "P Shamsolmoali",
        "M Zareapoor"
      ],
      "year": "2018",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "17",
      "title": "Non-Frontal Facial Expression Recognition Using a Depth-Patch Based Deep Neural Network",
      "authors": [
        "N Yao",
        "H Chen",
        "Q Guo",
        "H Wang"
      ],
      "year": "2017",
      "venue": "Journal of Computer Science and Technology"
    },
    {
      "citation_id": "18",
      "title": "Dual Temporal Scale Convolutional Neural Network for Micro-Expression Recognition",
      "authors": [
        "M Peng",
        "C Wang",
        "T Chen",
        "G Liu",
        "X Fu"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "19",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S Wang",
        "B Li",
        "Y Liu",
        "W Yan",
        "X Ou",
        "X Huang",
        "X Fu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "20",
      "title": "Wild facial expression recognition based on incremental active learning",
      "authors": [
        "M Ahmed",
        "K Woo",
        "K Hyeon",
        "M Bashar",
        "P Rhee"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "21",
      "title": "A hybrid deep learning neural appoach emotion recognition from facial expression for sociallu assistive robots",
      "authors": [
        "A Ruiz-Garcia",
        "M Elshaw",
        "A Altahhan",
        "V Palade"
      ],
      "year": "2018",
      "venue": "Neural Computing and Application"
    },
    {
      "citation_id": "22",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "H Kaya",
        "F Gürpınar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "23",
      "title": "Universal Approximation Using Incremental Constructive Feedforward Networks With Random Hidden Nodes",
      "authors": [
        "G Huang",
        "L Chen",
        "C Siew"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Neural Networks",
      "doi": "10.1109/tnn.2006.875977"
    },
    {
      "citation_id": "24",
      "title": "Sieve maximum likelihood estimation for a general class of accelerated hazards models with bundled parameters",
      "authors": [
        "X Zhao",
        "W Yuanshan",
        "Y Guosheng"
      ],
      "year": "2017",
      "venue": "Bernoulli",
      "doi": "10.3150/16-BEJ850"
    },
    {
      "citation_id": "25",
      "title": "A novel enhanced convolution neural network with extreme learning machine: facial emotional recognition in psychology practices",
      "authors": [
        "Nitesh Banskota",
        "P Abeer Alsadoon",
        "Ahmed Prasad",
        "Tarik Dawoud",
        "Omar Rashid",
        "Hisham Alsadoon"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-022-13567-8"
    },
    {
      "citation_id": "26",
      "title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part I)",
      "authors": [
        "X Liu",
        "S Lin",
        "J Fang",
        "Z Xu"
      ],
      "year": "2015",
      "venue": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part I)",
      "doi": "10.1109/tnnls.2014.2335212"
    },
    {
      "citation_id": "27",
      "title": "The Extended Cohn-Kanade Dataset (CK ): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/cvprw.2010.5543262"
    },
    {
      "citation_id": "28",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/afgr.1998.670949"
    }
  ]
}