{
  "paper_id": "2202.10763v1",
  "title": "A Review Of Affective Generation Models",
  "published": "2022-02-22T09:32:11Z",
  "authors": [
    "Guangtao Nie",
    "Yibing Zhan"
  ],
  "keywords": [
    "Affective Computing",
    "Affective generation",
    "Synthesis of affective behavior"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing is an emerging interdisciplinary field where computational systems are developed to analyze, recognize, and influence the affective states of a human. It can generally be divided into two subproblems: affective recognition and affective generation. Affective recognition has been extensively reviewed multiple times in the past decade. Affective generation, however, lacks a critical review. Therefore, we propose to provide a comprehensive review of affective generation models, as models are most commonly leveraged to affect others' emotional states. Affective computing has gained momentum in various fields and applications, thanks to the leap of machine learning, especially deep learning since 2015. With critical models introduced, this work is believed to benefit future research on affective generation. We concluded this work with a brief discussion on existing challenges.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "ffective computing is an emerging interdisciplinary field where computational systems are developed to analyze, recognize, and influence affective states of a human. Affective computing (AC) involves multiple research disciplines, including psychology, neuroscience, cognition, computer science, and electrical engineering. AC can be generally divided into two subproblems: affective detection/recognition and affective generation  [1] , which are related to input signal processing and output expression generation of an intelligent agent, respectively.\n\nWith the popularization of mobile smartphone and wearable devices and the increase of data storage and processing capacity, AC in these years has gained momentum in various fields and applications, such as health, education, marketing, and advertising. Thanks to the recent leap of development in machine learning, especially the development of deep learning since 2015, publications on AC have soared. During the past period, affective detection/recognition have been reviewed several times. Nonetheless, there is a lack of literature review on affective generation. Consequently, we aim to deliver a comprehensive review of affective generation models since 2015.\n\nAffective state is a notion that originated in psychology. It was first acknowledged in science in the 19th century  [2] ,  [3] ,  [4] . At first, emotion was considered irrelevant with rigorous scientific research and even as an obstacle for rational decision making and reasoning. Damasio refuted the above bias towards emotion by acknowledging its positive role in rational thought: the combined functionality of of emotion and body contributed to rational human thinking  [5] . In this work, affective state and emotional state are used interchangeably as in most literature  [1] ,  [6] ,  [7] .\n\nAC, in contrast, has not been proposed and drew public attention until 1990s, with remarkable works from Picard and Scherer  [1] ,  [8] . It was first described as computing that related to, arose from, or influenced emotions  [1] . Several works have tried to categorize the research topics under AC, but there has not been any consensus yet  [9] : Tao and Tan divided AC into three general topics: affective understanding, affective generation, and application  [10] . Carberry and de Rosis divided AC into four areas, namely, the analysis and characterization of affective states, automatic recognition of affective state, adaption of response to user's affective state, and expression and exihibition of affective state  [11] . In this work, we adapted the category proposed by Tao and Tan, with affective understanding renamed as affective detection/recognition. We suspect the capability of current technology to really understand emotions as a human does, and we do not include applications in our work, since applications are not the key of this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "There have been several works of literature review/overview/summary on AC, but few explicitly concerned the affective generation:\n\nBack in 2005, Tao and Tan reviewed AC development, with focuses on emotional speech processing, facial expression analysis and synthesis, body gesture and movement analysis, multi-channel information processing, and affect understanding and cognition  [10] . However, they did not provide a comprehensive review on affective text/speech and movement synthesis.\n\nCalvo and Mello  [6]  delivered a survey on affect detection on the first issue of IEEE Transactions on AC back in 2010. Emotion theories, affect detection methods, and input signals ranging from facial expressions, voice, body language and posture, physiology, brain imaging, text, and their multi-channel fusion were reviewed and discussed. The survey  [6]  was a landmark in AC, even though it did not cover affective generation.\n\nPublished in 2017, Poria et al.  [12]  reviewed affect recognition from the perspective of multimodal information fusion. They reviewed multimodal affect analysis and recognition framework based on the input of audio, visual, and text information. Public datasets and off-the-shelf APIs re-lated to affect recognition were also summarized. Nonetheless, they only focused on affect recognition, and they did not include physiology information as input.\n\nThe latest related work came from Arya et al.  [13] . They surveyed three critical questions related to AC: the contribution from interrelated domains to AC, prominent applications of AC, and research challenges and the pertinent issues. Their survey was conducted through the perspective of mutil-disciplinary contribution such as physiology, psychology, computer science, sociology, mathematics, linguistics, and their fusion. However, they did not mention state-of-the-art technologies applied, neither did they review affective generation techniques.\n\nBesides the preceding general-purpose review of AC, back in 2013  [14]  and 2019  [15] , expressive body movement generation for robots were surveyed. Other works reviewed: 1) AC in specific applications such as education  [16] ,  [17] , medicine  [18] ,  [19] , workplace  [20] , human machine interaction  [21] , and video gaming  [22] ; 2) AC on certain media including mobile devices  [23] ,  [24] ,  [25]  and multimedia  [26] ; 3) AC with certain signal input or influence factors, like EEG  [27] ,  [28] ,  [29] , color and temperature  [30] ,  [31] ,  [21] , gender and age  [32] ; 4) and others  [33] ,  [34] .\n\nIn this work, we aimed to deliver a general-purpose literature review on affective generation models. Specifically, modalities of affective generation of text, audio, facial expression, and body movement are covered.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Criteria For Inclusion In The Literature Review",
      "text": "Extensive literature reviews on affective generation ever since 2015 are covered in this work, as this is the period when deep learning  [35]  booms and benefits AC significantly. Moreover, only models related to affective generation are covered, topics such as hardwares, sensors, ethics, human computer interaction, and human robot interaction will not be discussed.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Structure",
      "text": "The remainder of this paper is organized as follows: the background of emotion theories and models are introduced in Section 2. Affective generation models in different areas are summarized in Section 3. Discussions on challenges and future works are delivered in Section 4. Finally, this work is concluded in Section 5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Theories",
      "text": "First of all, there is no consensus on the definition of emotion up till now  [36] ,  [37] . The history of emotion theories is long and diverged  [23] . Here we introduce three types of emotion theories, namely, psychology, cognition, and neurobiology.\n\nSeveral theories were developed for the psychology of emotion, namely, introspective, behavioral, and psychoanalytic  [4] . The introspective theory focuses on the feeling states and relating visceral physiology to feelings  [4] ,  [38] ,  [39] . The behavioral theory can date back to Darwin, who claimed that all men, including some animals, express emotions through similar behaviors  [2] . The psychoanalytic theory was developed by Freud and Breuer, who believed that emotion could only be inferred through human verbal reports, expressive behavior, or dreams  [40] .\n\nThere is also cognition of emotion  [41] . In a cognitive appraisal theory, it is hypothesized that emotion coordinates quasi-autonomous processes in the nervous system. It helps to maintain and accomplish transitions between plans and in systems with multiple goals  [42] .\n\nBesides psychology and cognition, neurobiological theories were also proposed. In facial-feedback theory, it is believed that skeletal muscle feedback from facial expression plays a causal role in regulating emotional experience and behavior  [43] . Damasio hypothesized that brain responses constitute the (body expression of) emotion and emotion feeling can be regarded as a consequence of neurobiological expression  [44] . Izard, nonetheless, proposed that emotion is a phase, rather than a consequence of neurobiological activity or body expression of emotion  [45] ,  [46] .\n\nFor more detailed discussions of emotion theories, readers can refer to  [1] ,  [45] ,  [47] ,  [48] ,  [49] ,  [50] , as this is not the main focus of this work.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Emotion Models",
      "text": "Numerous models have been proposed to model the relationships between different emotions. These models can be approximately classified into two categories: dimensional and discrete emotion approaches  [51] .\n\nDimensional models are proposed based on the hypothesis that the core effect of emotion is inherently continuous.\n\nA dimensional model provides important information such as valence and arousal of the stimuli  [45] ,  [52] . In dimensional models, emotions are projected onto several dimensions  [48] ,  [53] , including valence, arousal, and dominance. Valence is usually used interchangeably with pleasure, with negative as unpleasant and positive as pleasant. Arousal is sometimes in lieu of excitement or activation, which represents how strong emotion is. Dominance stands for to what extent a participant is in control of the emotional state.\n\nSome dimensional models, such as PAD and SAM, involve three dimensions. Pleasure-arousal-dominance (PAD) model is a three-dimensional emotion model  [54] . The three axes formed a triangular coordinate system, where an emotional state can be projected. The Self-Assessment Manikin (SAM) model was proposed for self-evaluation on valence, arousal, and dominance  [55] . There are five manikins in each of three rows, representing valence from positive to negative, arousal from high to low, and dominance from minimum control to maximum control, respectively. In the first row, a manikin with a sad face represents negative valence, while one with a smiling face represents positive valence. In the second row, a manikin with large excitement represents high arousal, and one with small excitement stands for low arousal. In the third row, small manikin indicates minimum control, whereas large manikin denotes maximum control.\n\nThe majority of dimensional models typically focuses on valence and arousal: FEELTRACE  [56]  is a dimensional model where the most intense emotions define a circle. The horizontal axis ranges from very negative to very positive while the vertical axis ranges from very passive to very positive. It is designed for observers to describe perceived emotional state by moving a pointer to the appropriate point in the model. Circumplex model is a two-dimensional plane where valence and arousal/activation are two dimensions  [57] . Valence is the horizontal axis, which ranges from unpleasant (negative) to pleasant (positive), while arousal/activation is the vertical axis ranging from deactivation (negative) to activation (positive). The two axes cross at the neural state.\n\nThe vector model can be considered as a rotation of Circumplex model, 90 degrees counterclockwise  [58] . The positive and negative affect schedule (PANAS) model provides 10-item scales for both positive affect and negative affect  [59] . It can be used with both long-term and shortterm labelling of emotions. The Activation-Deactivation Adjective Check List (AD ACL) is a two-dimensional selfrating test constructed and extensively validated for rapid assessments of momentary activation or arousal states  [60] . Energetic arousal and tense arousal are the two core dimensions.\n\nThe aforementioned dimensional models emphasize the conscious experience or phenomenology of affect. In contrast, there are also hypotheses of the existence of a discrete emotion or pattern of interaction emotion in the conscious brain  [61] ,  [62] ,  [63] . Discrete emotion models provide several basic discrete emotion, on which complex emotions are believed to form  [51] . Differed by the number of basic emotions, there have been numerous types of discrete emotion models  [64] :\n\nPaul Ekman proposed a basic emotion model, of which the six emotions were believed to be culturally universal  [65] ,  [66] : anger, disgust, fear, happiness, sadness, and surprise  [67] . Plutchik proposed eight primary emotions: fear, anger, joy, sadness, acceptance, disgust, expectancy, and surprise. Arranged in a circle in the form of \"emotion solid\", primary emotions can form more complicated emotions in the third dimension based on intensity. Nonetheless, the plausibility of the \"emotion solid\" assumption was questionable. In contrast, Oatley and Johnson-Laird made no assumption and treated five basic emotions (happiness, anger, anxiety, sadness, and disgust) as separate categories. Complex emotions can also be constructed based on these five emotions. For example, \"Differential Emotions Theory\" was proposed by Izard, which consisted of 10 basic emotions (interest, enjoyment, surprise, sadness, anger, disgust, contempt, fear, shame, and shyness)  [68] . In this model, each state had respective substate, unique organization, motivational features, facial expression, and evolutionary basis  [51] . Profile of Mood States (POMS) is another discrete emotion model that consists of 65 items for evaluating six basic moods: anger, confusion, depression, fatigue, tension, and vigor  [69] .\n\nThe discrete emotion models have drawn criticism for their varying number of basic emotions in different models  [64] . Some of the basic emotions adopted in the aforementioned models, such as interest, surprise, acceptance, and expectancy, were even being questioned for their qualification as emotion  [51] .\n\nBesides dimensional and discrete emotion approaches, hybrid models are developed based on the combination of dimensional and discrete emotion approaches: Smith and Ellsworth proposed eight cognitive appraisal dimensions to differentiate emotional experience  [70] . In their study, subjects were asked to recall past experience with each of 15 emotions and rate them along 6 dimensions, pleasantness, anticipated effort, certainty, attentional activity, selfother responsibility/control, and situational control. In addition, Diener et al. proposed a hybrid model with structural equation modelling: six basic emotions (love, joy, anger, fear, shame, sadness) were combined with negativepositive valence dimension  [51] ,  [71] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Generation Models",
      "text": "Affective generation can be described as generating messages to influence the emotional states of users. In Picard's description, affective generation application is a computational system that expresses what a human would perceive as an emotion  [6] .\n\nIn affective detection/recognition for an intelligent agent, the inputs include textual, audio, visual, bodily, facial, and physiological modalities. In correspondence, the outputs consist of all modalities except physiology. In this section, affective generation is reviewed in five categories: affective text generation, affective audio generation, affective facial expression generation, and affective movement generation.\n\nGeneration techniques are applied either for virtual agents, or embodied agents such as robots and embodied conversational agents.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affective Text Generation",
      "text": "We combined affective text generation and affective dialogue generation, as dialogue generation can be viewed as back and forth text generation, with text as input  [72] . There are two types of text generation algorithms, namely, text-to-text generation and data-to-text generation  [72] .\n\nIt is widely acknowledged that the following six subproblems constitute natural language generation: content determination, text structuring, sentence aggregation, lexicalization, referring expression generation, and linguistic realization  [72] ,  [73] . The effort of generating affective text has been predominantly put on lexicalization  [74] ,  [75] ,  [76] , where models were proposed to add affect intensity onto generated text/dialogue without losing grammatical correctness.\n\nGenerating emotional language is a key step towards building empathetic natural language processing agents  [77] . Having the capability to generate and express affect during human-machine interaction is one of the major milestones in machine learning development  [78] . Studies have shown that addressing affect in dialogue systems can enhance user satisfaction  [79] , leading to positive perception  [80]  and fewer breakdowns  [81]  in dialogues.\n\nAgents endowed with affective text-based dialogue generation system are generating interest both in academy  [82]  and the industry. Similar to the advancement of machine learning, affective text generation has also gone through the evolution from hand engineered features  [83] ,  [84]  to automated feature extraction  [74] ,  [76] ,  [85] ,  [86] ,  [87] ,  [88] ,  [89] .\n\nMost works on affective dialogue generation are based on enconder-decoder architectures, such as Sequence to Sequence (Seq2Seq)  [90]  text generation model. Encoderdecoder architectures are frequently used for machine translation and free form question answering.\n\nSong et al. proposed an emotional dialogue system (EmoDS) that can generate meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework  [91] . The EmoDS consisted of encoder-decoder architecture with lexicon-based attention mechanism. An emotion classifier provided global guidance on the emotional response generation by increasing the intensity of emotional expression. Zhong et al. proposed an end-to-end affect-rich open-domain neural conversational model, Affect-rich Seq2Seq (AR-S2S), which produced affective responses appropriate in syntax and semantics  [78] . The proposed AR-S2S extended Seq2Seq model with VAD (valence, arousal, dominance) embedding and affective attention. Emotional Chatting Machine (ECM)  [86]  was an affective conversational model based on Seq2Seq model. It modeled high-level abstraction of emotion expressions by embedding emotion categories. The change of implicit emotion state was captured by an internal memory while external memory helped generate explicit emotional words. In Mojitalk  [77] , authors applied several state-ofthe-art neural models, such as Seq2Seq, Conditional Variational Autoencoder (CVAE), and Reinforced CVAE, to learn a generation system that was capable of responding with an arbitrary emotion. They also trained a large-scale emoji classifier and validated its accuracy on the generated responses.\n\nIn models including ECM, users need to input the desired emotion  [77] ,  [86] ,  [91] ,  [92] ,  [93] ,  [94] ,  [95] , whereas for the model proposed by Asghar  [74] , they modeled emotion with affective word embedding as input. Asghar et al.  [74]  proposed an affective text generation model based on Seq2Seq. They used cognitive embedding of words by affective dictionary. Moreover, affective objective was added onto the cross-entropy loss function, and affectively diverse beam search algorithm was adopted to inject affective diversity.\n\nBesides the aforementioned works, there are other works investigating special topics or conditional problems: Valitutti and Veale investigated the way of inducing ironic effects in automated tweets  [96] . They studied two types of irony: The first type is adjectives with opposite polarity and contrastive comparison, used to produce ironic incongruity. The second type is irony markers, including scare quotes and hashtag #irony. Valence, irony, surprise, humor, and retweetability of the automated tweets were investigated in their experiments. Ghosh et al. proposed Affect-Long Short Term Memory (Affect-LM) to generate affective conversational text based on context words, affective category, and affective strength  [85] . They explicitly used two parameters e and β, both manually adjustable and automatically inferrable , to represent the affective category and strength, respectively.\n\nAffective dialogue generation can be divided into two subproblems based on the application scenarios: one is virtual agent, such as Siri, which delivers information through voice; the other is embodied agent, such as embodied conversational agent, which conveys both verbal and non-verbal information  [97] .\n\nAffective image captioning can be viewed as a problem under the category of data-to-text generation, with image as input: Yang et al.  [98]  proposed a method to generate captions with distinctiveness and attractiveness, which consisted of a content module and a linguistic module. The content module mapped the image into deep representations, where emotion was classified. The linguistic model took these deep representations as input, then embedded the training results into latent space, where frequency vector was clustered as output. The outputs from both content and linguistic module were integrated before inputting into a LSTM module for affective caption generation. Affective Guiding and Selective Attention Mechanism (AG-SAM)  [99]  was also proposed for affective image captioning. Deep representations were first extracted from input images by CNN, followed by paralleled attention gate and affective vector. The attention gate decided the amount of visual information fed into a LSTM module and the affective vector was trained based on deep representations to reflect emotional response, which was then input into the LSTM module. Finally, the LSTM module worked to generate affective captions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affective Audio Generation",
      "text": "Affective text/dialogue generation should be combined with affective pronunciations, as the utterance of the synthesized speech is another important issue that can influence the affective performance of an agent  [100] .\n\nBoth affective speech synthesis and affective music synthesis are covered in this subsection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Speech Synthesis",
      "text": "There are two types of affective speech synthesis models: one is affective Text-to-Speech, where affective audio spectrogram is synthesized directly from text; the other is adding emotional component onto neutral speech. All related works considered improving the naturalness of synthesized affective speech, which was mostly validated through subjective ratings. Different objective measurements were also included in some works, depending on the concrete problem background.\n\nIt has been more and more prevalent for Text-to-Speech models synthesizing natural and affect rich speech, instead of machine-like sound. Typically, the affective Text-to-Speech synthesis can be divided into two subproblems, namely, detection of affect in text  [101] , and generation of speech in line with the affective message  [100] .\n\nThere are several works on affective speech synthesis based on the modification of Tacotron  [102] , an end-to-end speech synthesizer based on the Seq2Seq model with attention paradigm, which took texts as input and output audio spectrogram. The generated speech can then be fed into Griffin-Lim reconstruction algorithm  [103]  to synthesize speech clips: Lee et al. introduced context vector and residual connection at RNN to address the problem of exposure bias and irregularity of attention alignment  [104] . Li et al.  [105]  proposed a controllable emotion speech synthesis approach based on emotion embedding space learned from references. In order to deliver the emotion more accurately and expressively with strength control, they modify the Prosody-Tacotron structure with two emotion classifiers to enhance the discriminative emotion ability of the emotion embedding and the predicted mel-spectrum.\n\nOther works on Text-to-Speech demonstrated the importance of features and the superiority of models: Tahon et al.  [100]  proposed double adaption to improve the quality and expressivity of the synthesized sentences. Double adaption refers to adapting voice and emotional pronunciation. Their work exhibited the importance of prosodic features. An et al.  [102]  investigated the performance of two lstm-rnn based models, one as emotion dependent model, the other as an unified approach with emotion as code. The two models both outperform traditional HMM based models  [103]  in terms of synthesized utterance naturalness.\n\nChoi et al.  [104]  considered the scenario of multispeaker emotional speech synthesis, they used emotion code and mel-frequency spectrogram as emotion identity. Trainable speaker representation and speaker code were used for speaker variation. Objective measures such as mel cepstral distortion, band aperiodicity distortion, fundamental frequency distortion in the root mean squared error, and voice/unvoiced error rate, along with subjective measures on naturalness, speaker similarity, and emotion similarity, were provided as validation for their proposed method's superiority.\n\nHuang et al.  [105]  presented a work of emotional 3D talking head synthesis. A temporal restricted Boltzmann machine was adopted for emotion transfer between neutral faces and emotional ones while an LSTM-RNN was used for emotional voice conversion from neutral one. Four discrete emotions were realized, and human raters were invited to classify the synthesized results.\n\nIn  [106] ,  [107] , it was revealed that duration and timing were two critical issues in the manual synchronization of gesture with speech for conversational agents.\n\nAdding emotional components onto neutral speech has been repeatedly investigated using statistical methods, which are called statistical parametric speech synthesis (SPSS): In  [108] , emotion additive model (EAM) was introduced to represent the difference between emotional and neutral voices. EAMs can be linearly combined to create certain emotional speech. Eigenvoice was also constructed for better synthesis in target emotion speech. In  [109] , Xue et al. studied the model adaption models in this problem: retraining model with emotion-specific data, augmenting model input using emotion specific codes, and using emotion depedent output layers with shared hidden layers. An et al.  [110]  proposed to combine a LSTM with a mixture density network for multimodal regression and variance prediction. KL-divergence was also introduced to maximize the distance between the distributions of emotional speech and neutural speech. Inoue et al.  [111]  also investigated this problem with several models: the parallel model, the serial model, and the auxiliary input model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affective Music Synthesis",
      "text": "It is suggested that music generation approaches can be divided into two categories: transformational and generative algorithms  [115] . Affective music synthesis models all fall within the latter category, which needs to create the musical structures, as opposed to acting on prepared structure  [116] . Some models take visual information as input, some composite music for computer games related to dynamic environments, and others take advantage of human physiology or communication.\n\nSergio and Lee  [112] ,  [113]  designed RNNs and Neuro-Fuzzy Networks to generate music that aimed to convey similar emotions as the input image, by trained with the image-music pairs extracted from video excerpts. Viewers were invited to rate the input images as either positive or negative emotions. The Neuro-Fuzzy Network was adopted to classify the binary emotion of input images, with viewers' ratings as labels in the training stage. Two RNNs were designed for music generation, following the prediction of Neuro-Fuzzy Network, one for positive emotion and the other for negative emotion. They later developed Scene2Wav  [114] , which was dedicated for the same problem, with improved neural network architecture and synthesis results.\n\nScirea et al.  [115]  proposed MetaCompose for affective music composition in dynamic environments, such as computer games. MetaCompose consisted of a graph traversal-based chord sequence generator, a search-based melody generator, a pattern-based accompaniment generator. In particular, they investigated the problem of diverse solutions with multi-objective optimization. Viewers were recruited to rate the synthesized music and its expressed valence.\n\nOther affective music composition works include autonomous agent-assisted affective music generation  [116] , where users were supposed to communicate with an autonomous agent about their emotion before it composed music with the desired emotion preference. The agent was programmed with the basic compositional rule of tonal music. Affective music generation based on brain-computer interfaces, such as electroencephalogram (EEG), was also investigated, where emotion features were extracted from EEG signal, which then directed the composition of emotional music  [117] . The problem of long-term coherence when generating affective music was investigated by MorpheuS  [118] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Affective Facial Expression Generation",
      "text": "Based on the characteristics of input/output images, facial expression generation can be categorized into virtual avatar animation and photorealistic face synthesis  [119] . Depending on whether the input and the output shared the same identity or not, facial expression generation could be categorized into facial expression editing (FEE) and facial expression transfer (FET). In FEE, the facial expression is changed on a given portrait, whereas in FET, the model works to transfer facial expression between different iden-tities. Based on the adopted models, facial image expression syntheses can be categorized into traditional graphicbased models and emerging generative, data-driven approaches. All methods either rely on the extraction of features from geometric  [120] , appearance  [121] , RGBD space  [122] ,  [123] , or crossfading, wrapping/morphing existing faces  [119] .\n\nThe following work is an example of FEE adopting a graphic-based model: Kollias et al.  [124]  proposed a model that took as input an arbitrary face with neutral expression and synthesized a new expression on it with either a target emotion category or emotion valence and arousal. The model fit a 3D morphable model on an input image, then deformed the reconstructed face and added the input affect, finally blended the new face with the given affect into the original image.\n\nBy contrast, more and more works adopted data-driven approaches, mostly resorting to deep neural networks, such as generative adversarial networks (GANs)  [125] , conditional GANs (cGANs)  [126] , and conditional variational autoencoders (cVAEs)  [127] , which required large scale training data instead of paired samples, as opposited to some of the graphic methods, to properly disambiguate identity information  [124] .\n\nZhou proposed a conditional difference adversarial autoencoder (CDAAE) for photorealistic FET  [119] . The CDAAE learned to generate an unseen person's facial expression with a discrete target emotion or facial action unit. The CDAAE worked partially due to the newly added feed-forward path connecting low-level features at the encoder with the corresponding level at the decoder. In  [128] , ExprGen was presented to take as input images of human faces and generate the character rig parameters accordingly for virtual avatar animation. ExprGen was a multistage deep learning system using the latent variables of human and character recognition convolutional nets to control 3D animated character rig. The multi-stage deep learning system consisted of 3D-CNN and Character Multi-Layer Perceptron. The multi-stage deep learning system was trained using five publicly available labeled facial expression datasets.\n\nInstead of using discrete emotion labels, continuous labels/dimensions were also adopted in some works: Ding et al. proposed Expression GAN (ExprGAN)  [129]  for photorealistic FEE with controllable expression intensity, which enabled the expression intensity to be continuously adjusted from low to high. Tang et al.  [130]  tried finegrained expression manipulation with expression-guided GAN (EGGAN), which could synthesize photorealistic images with continuous intermediate expressions based on continuous emotion labels and structured latent codes. Pham et al.  [131]  presented generative adversarial talking head (GATH), a deep neural network that enabled facial expression synthesis of a given portrait with continuous action unit coefficients. Their model directly manipulated image pixels to generate various expressions on an unseen face, while maintaining features such as facial geometry, skin color, hair style, and surrounding background. In  [132] , GANimation was introduced as a novel GAN that could control the magnitude of activation of each action unit and several combinations of them.\n\nThere are also works trying to combine the strengths of graphic methods and data-driven approaches:\n\nA Geometry-Guided Generative Adversarial Network (G2-GAN) was designed for continuously adjustable and identity preservable facial expression synthesis (FEE)  [133] , where facial geometry was adopted to guide the generation of facial texture synthesis with a certain expression. In this model, paired submodels were jointly trained for expression removal and expression synthesis, which formed a cycle between neutral expression and any other expression. Similarly, a Geometry-Contrastive Generative Adversarial Network (GC-GAN)  [134]  was proposed for transferring continuous emotions across different subjects. The embedded geometry is injected into the latent space of GC-GAN as continuous conditions to guide the generation of facial expressions effectively. Yeh et al.  [135]  presented an automatic approach of FEE, such as from smiling to neutral. Their approach combined flow-based face manipulation with the generative capability of Variational Autoencoder (VAE), which learned to encode the flows among different expressions in a latent space. Their results demonstrated higher perception quality than previous methods adopting VAEs.\n\nMore than developing affective facial expression generation models on images, Thies et al.  [136]  further tried face reenactment, which did facial expression synthesis in videos: Their model animated the facial expressions of a target video by a source actor and re-render the manipulated output video in a photorealistic fashion. Technically, the model first dealt with facial identity recovery by non-rigid model-based bundling. The model then tracked facial expressions of both target and source videos by a dense photometric consistency measure. The video reenactment was finally achieved by deformation transfer between source and target  [136] . They later added other functionalities, extending the modelling of facial expressions onto head, eye, and kinematic torso  [137] . ReenactGAN  [138]  was engineered to transfer facial movements and expressions from an arbitrary person's monocular video input to a target person's video. It mapped the source face to a boundary latent space, adapted source face's boundary to the target's boundary, and generated the reenacted target face. Ma and Deng  [139]  proposed a real-time end-to-end facial reenactment system, without the need for any driving source. It could generate desired photorealistic facial expressions on top of input RGB video, with an unpaired learning framework developed to learn the mapping between any two facial expressions in the facial blendshape space. Otberdout et al.  [140]  proposed a facial reenactment model, exploiting the face geometry by modelling the facial landmarks motion on a hypersphere. A GAN was adopted to generate facial landmark motion in the hypersphere to synthesize various facial expressions.\n\nBeyond facial expression synthesis on images and video reenactment, there were also works on affective image-tovideo translation, a FEE problem: Conditional MultiMode Network (CMM-Net)  [141]  was devised for generating various facial expression videos with distinctive characteristics, given a neutral face image and a discrete emotion label.\n\nThe input face image and emotion label were used to generate landmark sequences, which were used to guide the translation from the neutral image into facial expression video. Potamias et al.  [142]  advanced the image-to-video translation problem from 2D to 3D: they took as input 3D meshes instead of 2D images and, therefore, their generative model output 4D facial expression video. Their experimental results demonstrated the preservation of identity and high-quality expression synthesis. Similarly, Mo-tion3DGAN  [143]  exploited a set of 3D landmarks to generate expressive 4D faces by modeling the temporal dynamics of facial expressions using a manifold-valued GAN, with a neutral face as input.\n\nOther works include FET-FEE fusion and talking-head video generation with affect rich facial expression:\n\nIn 2019, Ali and Hughes  [144]  introduced Transfer-Editing and Recognition Generative Adversarial Network (TER-GAN), a model to realize three functions: facial expression transfer, editing, and recognition. When doing FET in TER-GAN, two encoders were adopted to encode identity and expression information from the source and target input image, respectively. When doing FEE in TER-GAN, two portraits with the same identity but different facial expressions were input to the two encoders. The generated images, either in FEE or FET, were more blurry, compared with ones generated by ExprGAN. Zeng et al. proposed an end-to-end expression-tailored generative adversarial network (ET-GAN) to generate talking face videos of identity with enriched facial expression  [145] . Expressional video of that identity, instead of identity image and audio, was the model input.\n\nThe aforementioned works were concerned with affective facial expression generation on the screen, Benson et al.  [146] ,  [147] , however, developed a Facial Expression Time Petri Net (FETPN) model to display facial expression on an embodied robotic face. They treated the expression of an affective state as a time-constrained behavior of facial physiognomy and regarded the facial physiognomic features as components of a concurrent system. Discrete facial expression was realized through the movement of critical facial points on latex-made mask (with wires attached to apply pressure and pull/ release the facial mask segments). Huang et al.  [148]  proposed a facial motion imitation method to transfer facial geometric characteristics from humans to robots.\n\nAffective speech-driven facial animation has also been investigated several times: Sadiq and Erzin  [149]  investigated this problem by domain adaption: affective and neutral speech representations were first mapped to a common latent space with smaller bias, then the domain adaption augmented affective representation for each discrete emotion state. An emotion-dependent deep audio-to-visual model was then trained on a public dataset for affective facial expression generation. Both objective and subjective measurements justified the model's performance. Karras et al.  [150]  developed low latency, audio-driven facial animation models. The end-to-end neural net learned a mapping from input audio to facial coordinates and a latent code that could be used as an intuitive controller of the emotional state of synthesized face. Their model consisted of three modules: formant analysis network, articulation network, and output network. Their model could yield reasonable results for inputs of other speakers, even being trained only on audios from a single speaker. Pham et al.  [151]  proposed training a LSTM network on a large audio-visual data corpus for real-time facial animation, with audio stream as input. The proposed network could estimate head rotation and facial action unit activations from the speaker's speech. In  [152] , both acoustic features and phoneme label features were utilized to generate natural looking, speaker-independent lip animations synchronized with affective speech.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Affective Movement Generation",
      "text": "Body movement, along with facial expression, are both forms of non-verbal communication. Body movement can be characterized by three dimensions, namely, function, execution, and expression  [153] . Affective body movement generation focuses on the aspect of expression, which reflects and influences the affective qualities, such as believability and engagement that the movement is conveying  [153] ,  [154] . Body movement generation can be realized through either physics-based approaches, in which parameters were adopted to control specific actions or datadriven machine learning approaches that relied on human actors performing actions as the demo. Most affective movement generation works were concerned with body movement and head movement, with few works focusing on hand gesture synthesis  [155] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Affective Movement Generation Of Virtual Agent",
      "text": "Most works concerning affective movement generation for virtual agents adopted physics-based approaches:\n\nIn  [156] , a general parameterized behavior model was adopted to integrate affect expression with functional behaviors. Models were parameterized by both spatial extent and motion dynamics. The model was applied to coverbal gestures with a NAO robot in order to express mood in storytelling scenarios.\n\nBurton et al.  [157]  proposed to imbue a given trajectory of robot movement with expressive content, which was sampled from a database of movements with expressive qualities. This method worked to find emotionally similar movements from the database, based on Laban movement analysis  [158] .\n\nCarreno-Medrano et al.  [159] ,  [160]  tried to represent affective bodily movement through a low-dimensional parameterization based on the spatio-temporal trajectories of some basic joints in the human body. It was assumed that the low-dimensional parameterization encodes the affective state that could also be mapped to whole-body motions.\n\nXia et al.  [161]  proposed a real-time style translation model that automatically transformed unlabeled, heterogeneous motion data into new styles. The styles of the output animation could be blended by blending the parameters of distinctive styles. For example, a \"neutral\" walk can be translated into an \"angry\"-\"strutting\" walk by linearly interpolating two distinctive output animation styles: \"angry\" and \"strutting\".\n\nIn  [162] , an inverse kinematics (IK) reconstruction model along with a statistical motion resampling scheme were proposed to synthesize high-dimensional full-body movements from low-dimensional end-effector trajectories. An inverse kinematic controller was defined for each limb movement while the resampling scheme was for endeffector and pelvis trajectories generation, with the constraint of preserving underlying emotional states.\n\nThere are plenty of works on data-driven approaches for body movement generation, but only a few considering the impact of affect states  [163] ,  [164] . Alemi et al.  [153]  presented an interactive animated agent model with controllable affective movements. A Factored, Conditional Restricted Boltzman Machine (FCRBM)  [165]  was adopted to control the valence and arousal of walking movements, with a corpus of recorded actor-performed affective walking. Two actors were recruited to perform 9 different expressive combinations of valence (negative, neutral, positive) and arousal (low, neutral, high).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Affective Robotic Movement Generation",
      "text": "Using NAO and Keepon  [166] , Knight and Simmons showed that simple robots could convey complex emotion through motion and varying robot task motions was sufficient to communicate a variety of expressive states  [167] .\n\nIn  [168] , in order to let robots expressing dominance, authors developed a parameter-based model for head tilt and body expansiveness. The model was applied to a collection of behaviors, which were evaluated by human observers. Some works used learning from demonstration or imitation learning to let robots learn from human gestures to express emotions  [169] : Suguitan et al.  [170]  investigated robot movement generation through learning from demonstration of non-professional actors, the help of CycleGANs  [171] . The CycleGAN consisted of a forward cycle and a backward cycle. In the forward cycle, an encoder-decoder module was adopted to generate a robot movement with human movement as input. In the backward cycle, the generated robot movement was passed into another encoder-decoder module for human movement reconstruction.\n\nAffective body movement generation does not only exist for virtual agents and embodied humanoid robots. Non-humanoid robots can also express emotion  [175] :\n\nCauchard et al.  [172]  defined a range of personality traits and emotional attributes that could be encoded in drones through their flight paths and speed. Their results indicated that adding an emotional component was part of the key to success in drones' acceptability. Jørgensen tried expressive movement generation in a soft robot  [173] . Rincon et al. proposed an adaptive fuzzy mechanism to adjust the perceived PAD (pleasure, arousal, and dominance) according to the environmental temperature, humidity, luminosity, and human proximity. The PAD values were then used to change the motion trajectory of a robot arm to express different emotional states. The motion trajectory was commanded by the Robust Generalized Predictive Controllers (RGPC) using convex optimization by Youla parametrization. Sial et al.  [174]  introduced a non-verbal and non-facial method for a \"mechanoid robot\" to express emotions through gestures. Their results indicated that the motion parameters of robots were linked with the change of emotions. How emotions could be expressed in swarms of miniature mobile robots were investigated in  [175] .\n\nClaret et al.  [176]  studied the problem of a robot executing a primary task and simultaneously conveying emotions using body motions, which was defined as a lower priority task. They explored the possibility of using kinematic redundancy of a robot to convey emotions. Happiness and sadness were shown as very well delivered, and calm was moderately conveyed, while fear was not well delivered.\n\nLöffler et al.  [177]  investigated using output modalities of color, motion, and sound for expressing joy, sadness, fear, and anger in an appearance-constrained social robot. The best expressions for each modality and emotion were selected and systematically combined.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Challenges",
      "text": "Challenges and potential contributions of existing affective generation models are discussed in this section.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Contextualization",
      "text": "Context, including both cultural context and social context, is a critical problem in delivering affective messages  [178] .\n\nIt has been shown the importance of context in understanding the meaning of words  [179]  and coherence of sentences  [180] . However, among all the reviewed affective text generation works, only Affect-LM  [85]  considered the context words, although Affect-LM only considered words within the range of a sentence. Among the reviewed works on affective speech synthesis, the work in  [181]  adopted filters to extract contextual information. So far, we have not found any works considering context in affective facial expression and affective movement generation.\n\nThe future works on affective generation models are suggested to take into account the contextual information, which should not be limited within the range of words, sentences, but also the social attitude and cultural preference. Social attitude in dialogues can permeate through the whole interaction process, rather than occur at a certain moment  [49] . Moreover, better contextualization can provide more information on emotion boundaries. Existing works assume emotions are either instantaneous or over time, which is not necessarily valid  [6] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Offline Generation Vs. Online Generation",
      "text": "The exisiting works were all offline generation models that were trained on a large corpus of data simultaneously. Offline learning is known for suffering from the restriction of large memory and slow model updates. In the real-world human machine interaction scenarios, it is expected that plenty of sensors will be deployed to collect a large amount of real-time and personalized data. The data processing speed will influence how fast and efficient an agent can respond to the sudden changes in the environment.\n\nAn online learning model treats input data as a running stream and makes small incremental updates to the model. Online learning models do not need large capacity memory to store input data for training, and it can better fit the latest trends of patterns in the data stream as the influence of past data may be gradually discounted. Affective generative models that update online will likely provide more adaptive and individualized solutions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodality",
      "text": "In affevtive recognition works, multimodal information are fused to promote the recognition capability as different modalities can reinforce or complement each other. Whereas in multimodal affective generation, different modalities are expected to be generated in a consistent and coherent manner. For example, if well organized, the generated affective text, music and video can provide a comprehensive and immersive environment as a better delivery, as ooposed to that of single modality.\n\nWhile there have been extensive works on multimodal affective recognition, limited has been dedicated for multimodal affective generation. The coordination and synchronization among different modalities have been shown to effectively improve the delivery quality of the generated content as well as the user satisfaction  [106] . Future works are encouraged to generate an immersive environment where multiple modalities complement each other to affect users' emotional states.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Affective Generation For Special Groups",
      "text": "Numerous technological systems, including affective recognition ones, have been specifically designed for the welfare and rehabilitation of special groups  [182] ,  [183] ,  [184] . In contrast, there is a lack of such consideration in affective generation works.\n\nWhen designing models to achieve optimal rehabilitation purposes, affective generation should target not only general audiences and users, but also special groups, such as the elderly group with dementia  [185] , children with developmental disorders like Autism Spectrum Disorder  [186] , and people with depression  [187]  and anxiety  [188] , who need taking special care on emotional support.\n\nFor example, individuals with ASD are characterized with the difficulties of perceiving other's emotional cues. As a consequence, affective content generated by an automated agent designed for typically developing individuals may have unwanted or unpredictable emotional influences on individuals with ASD.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Combination Of Affective Generation And Affective Recognition",
      "text": "Affective recognition and affective generation have been studied separately in most scenarios. However, an intelligent agent should be endowed with the capability of both input signal processing and analysis (affective recognition) and generating appropriate messages as a response (affective generation). Heretofore, we have not found any work combining affective generation with affective recognition when designing an intelligent affective agent.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "We reviewed affective generation models since 2015, the year when deep learning drew public attention. Affective generation is a technique that aims to generate messages to influence the emotional states of users. Topics including affective generation of text, audio, facial expression, and movement are reviewed. Challenges and potential improvements of existing models are discussed at last. We hope this work can pave the way for future works on developing novel affective generation models.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Guangtao Nie, Yibing Zhan": "Abstract—Affective computing is an emerging interdisciplinary field where computational systems are developed to analyze,"
        },
        {
          "Guangtao Nie, Yibing Zhan": "recognize, and influence the affective states of a human. It can generally be divided into two subproblems: affective recognition"
        },
        {
          "Guangtao Nie, Yibing Zhan": "and affective generation. Affective recognition has been extensively reviewed multiple times in the past decade. Affective"
        },
        {
          "Guangtao Nie, Yibing Zhan": "generation, however, lacks a critical review. Therefore, we propose to provide a comprehensive review of affective generation"
        },
        {
          "Guangtao Nie, Yibing Zhan": "models, as models are most commonly leveraged to affect others’ emotional states. Affective computing has gained momentum"
        },
        {
          "Guangtao Nie, Yibing Zhan": "in various fields and applications, thanks to the leap of machine learning, especially deep learning since 2015. With critical"
        },
        {
          "Guangtao Nie, Yibing Zhan": "models introduced, this work is believed to benefit future research on affective generation. We concluded this work with a brief"
        },
        {
          "Guangtao Nie, Yibing Zhan": "discussion on existing challenges."
        },
        {
          "Guangtao Nie, Yibing Zhan": "Index Terms—Affective Computing, Affective generation, Synthesis of affective behavior"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "——————————   ◆   ——————————": "1 \nINTRODUCTION"
        },
        {
          "——————————   ◆   ——————————": "ffective  computing  is  an  emerging  interdisciplinary"
        },
        {
          "——————————   ◆   ——————————": "field where computational systems are developed to \nA"
        },
        {
          "——————————   ◆   ——————————": "analyze, recognize, and influence affective states of a hu-"
        },
        {
          "——————————   ◆   ——————————": "man. Affective computing (AC) involves multiple research"
        },
        {
          "——————————   ◆   ——————————": "disciplines, including psychology, neuroscience, cognition,"
        },
        {
          "——————————   ◆   ——————————": "computer  science,  and  electrical  engineering.  AC  can  be"
        },
        {
          "——————————   ◆   ——————————": "generally  divided  into  two  subproblems:  affective  detec-"
        },
        {
          "——————————   ◆   ——————————": "tion/recognition and affective generation [1], which are re-"
        },
        {
          "——————————   ◆   ——————————": "lated  to  input  signal  processing  and  output  expression"
        },
        {
          "——————————   ◆   ——————————": "generation of an intelligent agent, respectively."
        },
        {
          "——————————   ◆   ——————————": "With  the  popularization  of  mobile  smartphone  and"
        },
        {
          "——————————   ◆   ——————————": "wearable devices and the increase of data storage and pro-"
        },
        {
          "——————————   ◆   ——————————": "cessing capacity, AC in these years has gained momentum"
        },
        {
          "——————————   ◆   ——————————": "in  various  fields  and  applications,  such  as  health,  educa-"
        },
        {
          "——————————   ◆   ——————————": "tion, marketing, and advertising. Thanks to the recent leap"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "of development in machine learning, especially the devel-"
        },
        {
          "——————————   ◆   ——————————": "opment  of  deep  learning  since  2015,  publications  on  AC"
        },
        {
          "——————————   ◆   ——————————": "have  soared.  During \nthe  past  period,  affective  detec-"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "tion/recognition have been reviewed several times. None-"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "theless, there is a lack of literature review on affective gen-"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "eration. Consequently, we aim to deliver a comprehensive"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "review of affective generation models since 2015."
        },
        {
          "——————————   ◆   ——————————": "Affective state is a notion that originated in psychology."
        },
        {
          "——————————   ◆   ——————————": "It was first acknowledged in science in the 19th century [2],"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "[3],  [4].  At  first,  emotion  was  considered  irrelevant  with"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "rigorous scientific research and even as an obstacle for ra-"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "tional  decision  making  and  reasoning.  Damasio    refuted"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "the above bias towards emotion by acknowledging its pos-"
        },
        {
          "——————————   ◆   ——————————": "itive role in rational thought: the combined functionality of"
        },
        {
          "——————————   ◆   ——————————": "of emotion and body contributed to rational human think-"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "ing [5]. In this work, affective state and emotional state are"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "used interchangeably as in most literature [1], [6], [7]."
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "AC, in contrast, has not been proposed and drew public"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "attention until 1990s, with remarkable works from Picard"
        },
        {
          "——————————   ◆   ——————————": "and Scherer [1], [8]. It was first described as computing that"
        },
        {
          "——————————   ◆   ——————————": "related  to, arose from, or influenced emotions [1]. Several"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "————————————————"
        },
        {
          "——————————   ◆   ——————————": ""
        },
        {
          "——————————   ◆   ——————————": "•  G.N. is with JD.com, China. E-mail: nieguangtao1@jd.com."
        },
        {
          "——————————   ◆   ——————————": "•  Y.Z. is with with JD Explore Academy, China. Email: zhanyibing@jd.com."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "lated  to  affect  recognition  were  also  summarized.  None-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "emotions  through  similar  behaviors  [2].  The  psychoana-"
        },
        {
          "2": "theless, they only focused on affect recognition, and they",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "lytic theory was developed by Freud and Breuer, who be-"
        },
        {
          "2": "did not include physiology information as input.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "lieved that emotion could only be inferred through human"
        },
        {
          "2": "The latest related work came from Arya et al. [13]. They",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "verbal reports, expressive behavior, or dreams [40]."
        },
        {
          "2": "surveyed three critical questions related to AC: the contri-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "There  is  also  cognition  of  emotion  [41].  In  a  cognitive"
        },
        {
          "2": "bution from interrelated domains to AC, prominent appli-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "appraisal  theory,  it  is  hypothesized  that  emotion  coordi-"
        },
        {
          "2": "cations  of  AC,  and  research  challenges  and  the  pertinent",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "nates quasi-autonomous processes in the nervous system."
        },
        {
          "2": "issues. Their survey was conducted through the perspec-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "It  helps  to  maintain  and  accomplish  transitions  between"
        },
        {
          "2": "tive of mutil-disciplinary contribution such as physiology,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "plans and in systems with multiple goals [42]."
        },
        {
          "2": "psychology,  computer  science,  sociology,  mathematics,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Besides psychology and cognition, neurobiological the-"
        },
        {
          "2": "linguistics, and their fusion. However, they did not men-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ories were also proposed. In facial-feedback theory, it is be-"
        },
        {
          "2": "tion state-of-the-art technologies applied, neither did they",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "lieved that skeletal muscle feedback from facial expression"
        },
        {
          "2": "review affective generation techniques.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "plays a causal role in regulating emotional experience and"
        },
        {
          "2": "Besides  the  preceding  general-purpose  review  of  AC,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "behavior [43]. Damasio hypothesized that brain responses"
        },
        {
          "2": "back in 2013 [14] and 2019 [15], expressive body movement",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "constitute  the  (body  expression  of)  emotion  and  emotion"
        },
        {
          "2": "generation  for  robots  were  surveyed.  Other  works  re-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "feeling can be regarded as a consequence of neurobiologi-"
        },
        {
          "2": "viewed:  1)  AC  in  specific  applications  such  as  education",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "cal expression [44]. Izard, nonetheless, proposed that emo-"
        },
        {
          "2": "[16], [17], medicine [18], [19], workplace [20], human ma-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tion is a phase, rather than a consequence of neurobiologi-"
        },
        {
          "2": "chine interaction [21], and video gaming [22]; 2) AC on cer-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "cal activity or body expression of emotion [45], [46]."
        },
        {
          "2": "tain  media  including  mobile  devices  [23],  [24],  [25]  and",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "For more detailed discussions of emotion theories, read-"
        },
        {
          "2": "multimedia [26]; 3) AC with certain signal input or influ-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ers can refer to [1], [45], [47], [48], [49], [50], as this is not"
        },
        {
          "2": "ence factors, like EEG [27], [28], [29], color and temperature",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the main focus of this work."
        },
        {
          "2": "[30], [31], [21], gender and age [32]; 4) and others [33], [34].",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2.2 Emotion models"
        },
        {
          "2": "In this work, we aimed to deliver a general-purpose lit-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Numerous models have been proposed to model the rela-"
        },
        {
          "2": "erature review on affective generation models. Specifically,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tionships between different emotions. These models can be"
        },
        {
          "2": "modalities of affective generation of text, audio, facial ex-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "pression, and body movement are covered.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "approximately classified into two categories: dimensional"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and discrete emotion approaches [51]."
        },
        {
          "2": "1.2 Criteria for inclusion in the literature review",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Dimensional models are proposed based on the hypoth-"
        },
        {
          "2": "Extensive  literature  reviews  on  affective  generation  ever",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "esis that the core effect of emotion is inherently continuous."
        },
        {
          "2": "since  2015  are  covered  in  this  work,  as  this  is  the  period",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "A  dimensional  model  provides \nimportant \ninformation"
        },
        {
          "2": "when  deep  learning  [35]  booms  and  benefits  AC  signifi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "such as valence and arousal of the stimuli [45], [52]. In di-"
        },
        {
          "2": "cantly. Moreover, only models related to affective genera-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "mensional models, emotions are projected onto several di-"
        },
        {
          "2": "tion are covered, topics such as hardwares, sensors, ethics,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "mensions [48], [53], including valence, arousal, and domi-"
        },
        {
          "2": "human computer interaction, and human robot interaction",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "nance. Valence is usually used interchangeably with pleas-"
        },
        {
          "2": "will not be discussed.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ure, with negative as unpleasant and positive as pleasant."
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Arousal  is  sometimes  in  lieu  of  excitement  or  activation,"
        },
        {
          "2": "1.3 Structure",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "which \nrepresents  how  strong  emotion \nis.  Dominance"
        },
        {
          "2": "The  remainder  of  this  paper  is  organized  as  follows:  the",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "stands for to what extent a participant is in control of the"
        },
        {
          "2": "background  of  emotion  theories  and  models  are  intro-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "emotional state."
        },
        {
          "2": "duced in Section 2. Affective generation models in differ-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Some dimensional models, such as PAD and SAM, in-"
        },
        {
          "2": "ent areas are summarized in Section 3. Discussions on chal-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "volve \nthree \ndimensions. \nPleasure-arousal-dominance"
        },
        {
          "2": "lenges and future works are delivered in Section 4. Finally,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "(PAD)  model  is  a  three-dimensional  emotion  model  [54]."
        },
        {
          "2": "this work is concluded in Section 5.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The  three  axes  formed  a  triangular  coordinate  system,"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "where an emotional state can be projected. The Self-Assess-"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ment Manikin (SAM) model was proposed for self-evalua-"
        },
        {
          "2": "2  EMOTION",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tion  on  valence,  arousal,  and  dominance  [55].  There  are"
        },
        {
          "2": "2.1 Emotion theories",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "five manikins in each of three rows, representing valence"
        },
        {
          "2": "First of all, there is no consensus on the definition of emo-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "from  positive  to  negative,  arousal  from  high  to  low,  and"
        },
        {
          "2": "tion up till now [36], [37]. The history of emotion theories",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "dominance  from  minimum  control  to  maximum  control,"
        },
        {
          "2": "is long and diverged [23]. Here we introduce three types of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "respectively. In the first row, a manikin with a sad face rep-"
        },
        {
          "2": "emotion theories, namely, psychology, cognition, and neu-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "resents negative valence, while one with a smiling face rep-"
        },
        {
          "2": "robiology.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "resents positive valence. In the second row, a manikin with"
        },
        {
          "2": "Several theories were developed for the psychology of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "large  excitement  represents  high  arousal,  and  one  with"
        },
        {
          "2": "emotion, namely, introspective, behavioral, and psychoan-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "small excitement stands for low arousal. In the third row,"
        },
        {
          "2": "alytic  [4].  The  introspective  theory  focuses  on  the  feeling",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "small manikin indicates minimum control, whereas large"
        },
        {
          "2": "states and relating visceral physiology to feelings [4], [38],",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "manikin denotes maximum control."
        },
        {
          "2": "[39]. The behavioral theory can date back to Darwin, who",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The  majority  of  dimensional  models  typically  focuses"
        },
        {
          "2": "claimed  that  all  men,  including  some  animals,  express",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "on valence and arousal:"
        },
        {
          "2": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "FEELTRACE  [56]  is  a  dimensional  model  where  the"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "most intense emotions define a circle. The horizontal axis",
          "3": "Besides dimensional and discrete emotion approaches,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ranges from very negative to very positive while the verti-",
          "3": "hybrid models are developed based on the combination of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cal axis ranges from very passive to very positive. It is de-",
          "3": "dimensional and discrete emotion approaches: Smith and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "signed for observers to describe perceived emotional state",
          "3": "Ellsworth proposed eight cognitive appraisal dimensions"
        },
        {
          "AUTHOR ET AL.:  TITLE": "by moving a pointer to the appropriate point in the model.",
          "3": "to  differentiate  emotional  experience  [70].  In  their  study,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Circumplex model is a two-dimensional plane where va-",
          "3": "subjects were asked to recall past experience with each of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "lence and arousal/activation are two dimensions [57]. Va-",
          "3": "15 emotions and rate them along 6 dimensions, pleasant-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "lence is the horizontal axis, which ranges from unpleasant",
          "3": "ness, anticipated effort, certainty, attentional activity, self-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "(negative) to pleasant (positive), while arousal/activation",
          "3": "other responsibility/control, and situational control. In ad-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "is the vertical axis ranging from deactivation (negative) to",
          "3": "dition, Diener et al. proposed a hybrid model with struc-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "activation (positive). The two axes cross at the neural state.",
          "3": "tural equation modelling: six basic emotions (love, joy, an-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "The  vector  model  can  be  considered  as  a  rotation  of  Cir-",
          "3": "ger,  fear,  shame,  sadness)  were  combined  with  negative-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cumplex  model,  90  degrees  counterclockwise  [58].  The",
          "3": "positive valence dimension [51], [71]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "positive and negative affect schedule (PANAS) model pro-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "vides  10-item  scales  for  both  positive  affect  and  negative",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "3  AFFECTIVE GENERATION MODELS"
        },
        {
          "AUTHOR ET AL.:  TITLE": "affect [59]. It can be used with both long-term and short-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "term  labelling  of  emotions.  The  Activation-Deactivation",
          "3": "Affective generation can be  described as generating mes-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Adjective Check List (AD ACL) is a two-dimensional self-",
          "3": "sages to influence the emotional states of users. In Picard’s"
        },
        {
          "AUTHOR ET AL.:  TITLE": "rating test constructed and extensively validated for rapid",
          "3": "description, affective generation application is a computa-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "assessments of momentary activation or arousal states [60].",
          "3": "tional system that expresses what a human would perceive"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Energetic  arousal  and  tense  arousal  are  the  two  core  di-",
          "3": "as an emotion [6]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "mensions.",
          "3": "In  affective  detection/recognition \nfor  an \nintelligent"
        },
        {
          "AUTHOR ET AL.:  TITLE": "The  aforementioned  dimensional  models  emphasize",
          "3": "agent, the inputs include textual, audio, visual, bodily, fa-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "the  conscious  experience  or  phenomenology  of  affect.  In",
          "3": "cial, and physiological modalities. In correspondence, the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "contrast, there are also hypotheses of the existence of a dis-",
          "3": "outputs consist of all modalities except physiology. In this"
        },
        {
          "AUTHOR ET AL.:  TITLE": "crete emotion or pattern of interaction emotion in the con-",
          "3": "section, affective generation is reviewed in five categories:"
        },
        {
          "AUTHOR ET AL.:  TITLE": "scious brain [61], [62], [63]. Discrete emotion models pro-",
          "3": "affective text generation, affective audio generation, affec-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "vide  several  basic  discrete  emotion,  on  which  complex",
          "3": "tive facial expression generation, and affective movement"
        },
        {
          "AUTHOR ET AL.:  TITLE": "emotions are believed to form [51]. Differed by the number",
          "3": "generation."
        },
        {
          "AUTHOR ET AL.:  TITLE": "of basic emotions, there have been numerous types of dis-",
          "3": "Generation  techniques  are  applied  either  for  virtual"
        },
        {
          "AUTHOR ET AL.:  TITLE": "crete emotion models [64]:",
          "3": "agents, or embodied agents such as robots and embodied"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Paul Ekman proposed a basic emotion model, of which",
          "3": "conversational agents."
        },
        {
          "AUTHOR ET AL.:  TITLE": "the six emotions were believed to be  culturally universal",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "3.1 Affective text generation"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[65], [66]: anger, disgust, fear, happiness, sadness, and sur-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "We  combined  affective  text  generation  and  affective  dia-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "prise [67]. Plutchik proposed eight primary emotions: fear,",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "logue generation, as dialogue generation can be viewed as"
        },
        {
          "AUTHOR ET AL.:  TITLE": "anger,  joy,  sadness,  acceptance,  disgust,  expectancy,  and",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "back  and  forth  text  generation,  with  text  as  input  [72]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "surprise.  Arranged  in  a  circle  in  the  form  of  “emotion",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "There are two types of text generation algorithms, namely,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "solid”, primary emotions can form more complicated emo-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "text-to-text generation and data-to-text generation [72]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "tions in the third dimension based on intensity. Nonethe-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "It  is  widely  acknowledged  that  the  following  six  sub-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "less,  the  plausibility  of  the  “emotion  solid”  assumption",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "problems  constitute  natural  language  generation:  content"
        },
        {
          "AUTHOR ET AL.:  TITLE": "was  questionable.  In  contrast,  Oatley  and  Johnson-Laird",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "determination, text structuring, sentence aggregation, lexicali-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "made no assumption and treated five basic emotions (hap-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "zation,  referring  expression  generation,  and  linguistic  realiza-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "piness,  anger,  anxiety,  sadness,  and  disgust)  as  separate",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "tion  [72],  [73].  The  effort  of  generating  affective  text  has"
        },
        {
          "AUTHOR ET AL.:  TITLE": "categories.  Complex  emotions  can  also  be  constructed",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "been  predominantly  put  on  lexicalization  [74],  [75],  [76],"
        },
        {
          "AUTHOR ET AL.:  TITLE": "based  on  these  five  emotions.  For  example,  “Differential",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "where models were proposed to add affect intensity onto"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Emotions Theory” was proposed by Izard, which consisted",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "generated text/dialogue without losing grammatical cor-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "of 10 basic emotions (interest, enjoyment, surprise, sadness,",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "rectness."
        },
        {
          "AUTHOR ET AL.:  TITLE": "anger, disgust, contempt, fear, shame, and shyness) [68]. In",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "Generating  emotional  language  is  a  key  step  towards"
        },
        {
          "AUTHOR ET AL.:  TITLE": "this model, each state had respective substate, unique or-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "building  empathetic  natural  language  processing  agents"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ganization,  motivational  features,  facial  expression,  and",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "[77]. Having the capability to generate and express affect"
        },
        {
          "AUTHOR ET AL.:  TITLE": "evolutionary basis [51]. Profile of Mood States (POMS) is",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "during  human-machine  interaction  is  one  of  the  major"
        },
        {
          "AUTHOR ET AL.:  TITLE": "another  discrete  emotion  model  that  consists  of  65  items",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "milestones in machine learning development [78]. Studies"
        },
        {
          "AUTHOR ET AL.:  TITLE": "for evaluating six basic moods: anger, confusion, depres-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "have shown that addressing affect in dialogue systems can"
        },
        {
          "AUTHOR ET AL.:  TITLE": "sion, fatigue, tension, and vigor [69].",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "enhance user satisfaction [79], leading to positive percep-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "The discrete emotion models have drawn criticism for",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "tion [80] and fewer breakdowns [81] in dialogues."
        },
        {
          "AUTHOR ET AL.:  TITLE": "their varying number of basic emotions in different models",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "Agents  endowed  with  affective \ntext-based  dialogue"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[64]. Some of the basic emotions adopted in the aforemen-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "generation system are generating interest both in academy"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tioned models, such as interest, surprise, acceptance, and",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "[82] and the industry. Similar to the advancement of ma-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "expectancy, were even being questioned for  their qualifi-",
          "3": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "3": "chine \nlearning,  affective  text  generation  has  also  gone"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cation as emotion [51].",
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "through the evolution from hand engineered features [83],",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and strength, respectively."
        },
        {
          "4": "[84]  to  automated  feature  extraction  [74],  [76],  [85],  [86],",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Affective dialogue generation can be divided into two"
        },
        {
          "4": "[87], [88], [89].",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "subproblems based on the application scenarios: one is vir-"
        },
        {
          "4": "Most works on affective dialogue generation are based",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tual  agent, \nsuch  as  Siri,  which  delivers \ninformation"
        },
        {
          "4": "on  enconder-decoder  architectures,  such  as  Sequence  to",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "through  voice;  the  other  is  embodied  agent,  such  as  em-"
        },
        {
          "4": "Sequence  (Seq2Seq)  [90]  text  generation  model.  Encoder-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "bodied  conversational  agent,  which  conveys  both  verbal"
        },
        {
          "4": "decoder  architectures  are  frequently  used  for  machine",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and non-verbal information [97]."
        },
        {
          "4": "translation and free form question answering.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Affective image captioning can be viewed as a problem"
        },
        {
          "4": "Song  et  al.  proposed  an  emotional  dialogue  system",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "under the category of data-to-text generation, with image"
        },
        {
          "4": "(EmoDS)  that  can  generate  meaningful  responses  with  a",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "as  input:  Yang  et  al.  [98]  proposed  a  method  to  generate"
        },
        {
          "4": "coherent structure for a post, and meanwhile express the",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "captions  with  distinctiveness  and  attractiveness,  which"
        },
        {
          "4": "desired  emotion  explicitly  or  implicitly  within  a  unified",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "consisted of a content module and a linguistic module. The"
        },
        {
          "4": "framework [91]. The EmoDS consisted of encoder-decoder",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "content module mapped the image into deep representa-"
        },
        {
          "4": "architecture  with  lexicon-based  attention  mechanism.  An",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tions, where emotion was classified. The linguistic model"
        },
        {
          "4": "emotion  classifier  provided  global  guidance  on  the  emo-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "took these deep representations as input, then embedded"
        },
        {
          "4": "tional  response  generation  by  increasing  the  intensity  of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the training results into latent space, where frequency vec-"
        },
        {
          "4": "emotional expression. Zhong et al. proposed an end-to-end",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tor was clustered as output. The outputs from both content"
        },
        {
          "4": "affect-rich open-domain neural conversational model, Af-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and  linguistic  module  were  integrated  before  inputting"
        },
        {
          "4": "fect-rich  Seq2Seq  (AR-S2S),  which  produced  affective  re-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "into a LSTM module for affective caption generation. Af-"
        },
        {
          "4": "sponses appropriate in syntax and semantics [78]. The pro-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "fective Guiding and Selective Attention Mechanism (AG-"
        },
        {
          "4": "posed  AR-S2S  extended  Seq2Seq  model  with  VAD  (va-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "SAM) [99] was also proposed for affective image caption-"
        },
        {
          "4": "lence, arousal, dominance) embedding and affective atten-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ing. Deep representations were first extracted from input"
        },
        {
          "4": "tion. Emotional Chatting Machine (ECM) [86] was an affec-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "images by CNN, followed by paralleled attention gate and"
        },
        {
          "4": "tive  conversational  model  based  on  Seq2Seq  model.  It",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "affective vector. The attention gate decided the amount of"
        },
        {
          "4": "modeled high-level abstraction of emotion expressions by",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "visual information fed into a LSTM module and the affec-"
        },
        {
          "4": "embedding  emotion  categories.  The  change  of \nimplicit",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tive  vector  was  trained based  on  deep  representations  to"
        },
        {
          "4": "emotion state was captured by an internal memory while",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "reflect emotional response, which was then input into the"
        },
        {
          "4": "external  memory  helped  generate \nexplicit \nemotional",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "LSTM module. Finally, the LSTM module worked to gen-"
        },
        {
          "4": "words.  In  Mojitalk  [77],  authors  applied  several  state-of-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "erate affective captions."
        },
        {
          "4": "the-art neural models, such as Seq2Seq, Conditional Vari-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "3.2 Affective audio generation"
        },
        {
          "4": "ational  Autoencoder  (CVAE),  and  Reinforced  CVAE,  to",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Affective  text/dialogue  generation  should  be  combined"
        },
        {
          "4": "learn a generation system that was capable of responding",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "with an arbitrary emotion. They also trained a large-scale",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "with affective pronunciations, as the utterance of the syn-"
        },
        {
          "4": "emoji classifier and validated its accuracy on the generated",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "thesized speech is another important issue that can influ-"
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ence the affective performance of an agent [100]."
        },
        {
          "4": "responses.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Both affective speech synthesis and affective music syn-"
        },
        {
          "4": "In models including ECM, users need to input the de-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "thesis are covered in this subsection."
        },
        {
          "4": "sired emotion [77], [86], [91], [92], [93], [94], [95], whereas",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "for the model proposed by Asghar [74], they modeled emo-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "3.2.1 Affective speech synthesis"
        },
        {
          "4": "tion with affective word embedding as input. Asghar et al.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "There are two types of affective speech synthesis models:"
        },
        {
          "4": "[74] proposed an affective text generation model based on",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "one is affective Text-to-Speech, where affective audio spec-"
        },
        {
          "4": "Seq2Seq. They used cognitive embedding of words by af-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "trogram is synthesized directly from text; the other is add-"
        },
        {
          "4": "fective dictionary. Moreover, affective objective was added",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ing emotional component onto neutral speech. All related"
        },
        {
          "4": "onto  the  cross-entropy  loss  function,  and  affectively  di-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "works  considered  improving  the  naturalness  of  synthe-"
        },
        {
          "4": "verse beam search algorithm was adopted to inject affec-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "sized \naffective \nspeech,  which  was  mostly  validated"
        },
        {
          "4": "tive diversity.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "through  subjective  ratings.  Different  objective  measure-"
        },
        {
          "4": "Besides \nthe  aforementioned  works, \nthere  are  other",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ments  were  also  included  in  some  works,  depending  on"
        },
        {
          "4": "works investigating special topics or conditional problems:",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the concrete problem background."
        },
        {
          "4": "Valitutti and Veale investigated the way of inducing ironic",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "It has been more and more prevalent for Text-to-Speech"
        },
        {
          "4": "effects in automated tweets [96]. They studied two types of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "models synthesizing natural and affect rich speech, instead"
        },
        {
          "4": "irony:  The  first  type  is  adjectives  with  opposite  polarity",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of  machine-like  sound.  Typically,  the  affective  Text-to-"
        },
        {
          "4": "and contrastive comparison, used to produce ironic incon-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Speech  synthesis  can  be  divided  into  two  subproblems,"
        },
        {
          "4": "gruity. The second type is irony markers, including scare",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "namely, detection of affect in text [101], and generation of"
        },
        {
          "4": "quotes and hashtag #irony. Valence, irony, surprise, humor,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "speech in line with the affective message [100]."
        },
        {
          "4": "and  retweetability  of  the  automated  tweets  were  investi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "There  are  several  works  on  affective  speech  synthesis"
        },
        {
          "4": "gated in their experiments. Ghosh et al. proposed Affect-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "based on the modification of Tacotron [102], an end-to-end"
        },
        {
          "4": "Long  Short  Term  Memory  (Affect-LM)  to  generate  affec-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "speech synthesizer based on the Seq2Seq model with atten-"
        },
        {
          "4": "tive conversational text based on context words, affective",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tion paradigm, which took texts as input and output audio"
        },
        {
          "4": "category, and affective strength [85]. They explicitly used",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "spectrogram.  The  generated  speech  can  then  be  fed  into"
        },
        {
          "4": "two parameters e and β, both manually adjustable and au-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "4": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Griffin-Lim  reconstruction  algorithm  [103]  to  synthesize"
        },
        {
          "4": "tomatically inferrable , to represent the affective category",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "speech clips: Lee et al. introduced context vector and resid-",
          "5": "the serial model, and the auxiliary input model."
        },
        {
          "AUTHOR ET AL.:  TITLE": "ual connection at RNN to address the problem of exposure",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "3.2.2 Affective music synthesis"
        },
        {
          "AUTHOR ET AL.:  TITLE": "bias and irregularity of attention alignment [104]. Li et al.",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "[105] proposed a controllable emotion speech synthesis ap-",
          "5": "It is suggested that music generation approaches can be di-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "vided into two categories: transformational and generative"
        },
        {
          "AUTHOR ET AL.:  TITLE": "proach based on emotion embedding space learned from",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "algorithms [115]. Affective music synthesis models all fall"
        },
        {
          "AUTHOR ET AL.:  TITLE": "references. In order to deliver the emotion more accurately",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "within the latter category, which needs to create the musi-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "and  expressively  with  strength  control,  they  modify  the",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "cal structures, as opposed to acting on prepared structure"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Prosody-Tacotron structure with two emotion classifiers to",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "enhance the discriminative emotion ability of the emotion",
          "5": "[116]. Some models take visual information as input, some"
        },
        {
          "AUTHOR ET AL.:  TITLE": "embedding and the predicted mel-spectrum.",
          "5": "composite music for computer games related to dynamic"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "environments, and others take advantage of human phys-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Other  works  on  Text-to-Speech  demonstrated  the  im-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "iology or communication."
        },
        {
          "AUTHOR ET AL.:  TITLE": "portance of features and the superiority of models: Tahon",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "Sergio and Lee [112], [113] designed RNNs and Neuro-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "et al. [100] proposed double adaption to improve the qual-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "Fuzzy  Networks  to  generate  music  that aimed  to  convey"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ity and expressivity of the synthesized sentences. Double",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "adaption refers to adapting voice and emotional pronunci-",
          "5": "similar  emotions  as  the  input  image,  by trained  with  the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ation. Their work exhibited the importance of prosodic fea-",
          "5": "image-music pairs extracted from video excerpts. Viewers"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "were invited to rate the input images as either positive or"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tures. An et al. [102] investigated the performance of two",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "negative \nemotions. \nThe  Neuro-Fuzzy  Network  was"
        },
        {
          "AUTHOR ET AL.:  TITLE": "lstm-rnn based models, one as emotion dependent model,",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "adopted  to  classify  the  binary  emotion  of  input  images,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "the other as an unified approach with emotion as code. The",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "with viewers’ ratings as labels in  the training  stage. Two"
        },
        {
          "AUTHOR ET AL.:  TITLE": "two models both outperform traditional HMM based mod-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "els [103] in terms of synthesized utterance naturalness.",
          "5": "RNNs were designed for music generation, following the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Choi  et  al. \n[104]  considered \nthe  scenario  of  multi-",
          "5": "prediction of Neuro-Fuzzy Network, one for positive emo-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "tion and the other for negative emotion. They later devel-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "speaker  emotional  speech  synthesis,  they  used  emotion",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "oped Scene2Wav [114], which was dedicated for the same"
        },
        {
          "AUTHOR ET AL.:  TITLE": "code and mel-frequency spectrogram as emotion identity.",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "problem, with improved neural network architecture and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Trainable  speaker  representation  and  speaker  code  were",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "synthesis results."
        },
        {
          "AUTHOR ET AL.:  TITLE": "used for speaker variation. Objective measures such as mel",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "cepstral  distortion,  band  aperiodicity  distortion,  funda-",
          "5": "Scirea et al. [115] proposed MetaCompose for affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "mental frequency distortion in the root mean squared error,",
          "5": "music  composition \nin  dynamic  environments,  such  as"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "computer games. MetaCompose consisted of a graph tra-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "and  voice/unvoiced  error \nrate,  along  with  subjective",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "versal-based  chord  sequence  generator,  a  search-based"
        },
        {
          "AUTHOR ET AL.:  TITLE": "measures on naturalness, speaker similarity, and emotion",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "melody generator, a pattern-based accompaniment gener-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "similarity, were provided as validation for their proposed",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "ator. In particular, they investigated the problem of diverse"
        },
        {
          "AUTHOR ET AL.:  TITLE": "method’s superiority.",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "Huang  et  al.  [105]  presented  a  work  of  emotional  3D",
          "5": "solutions with multi-objective optimization. Viewers were"
        },
        {
          "AUTHOR ET AL.:  TITLE": "talking  head  synthesis.  A  temporal  restricted  Boltzmann",
          "5": "recruited to rate the synthesized music and its expressed"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "valence."
        },
        {
          "AUTHOR ET AL.:  TITLE": "machine  was  adopted  for  emotion  transfer  between  neu-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "Other  affective  music  composition  works  include  au-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tral  faces  and  emotional  ones  while  an  LSTM-RNN  was",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "tonomous agent-assisted affective music generation [116],"
        },
        {
          "AUTHOR ET AL.:  TITLE": "used  for  emotional  voice  conversion  from  neutral  one.",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "where users were  supposed to communicate with an au-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Four  discrete  emotions  were  realized,  and  human  raters",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "were invited to classify the synthesized results.",
          "5": "tonomous  agent  about  their  emotion  before  it  composed"
        },
        {
          "AUTHOR ET AL.:  TITLE": "In [106], [107], it was revealed that duration and timing",
          "5": "music with the desired emotion preference. The agent was"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "programmed  with  the  basic  compositional  rule  of  tonal"
        },
        {
          "AUTHOR ET AL.:  TITLE": "were two critical issues in the manual synchronization of",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "music.  Affective  music  generation  based  on  brain-com-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "gesture with speech for conversational agents.",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "puter interfaces, such as electroencephalogram (EEG), was"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Adding emotional components onto neutral speech has",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "also investigated,  where emotion features were extracted"
        },
        {
          "AUTHOR ET AL.:  TITLE": "been  repeatedly \ninvestigated  using  statistical  methods,",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "which  are  called  statistical  parametric  speech  synthesis",
          "5": "from EEG signal, which then directed the composition of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "(SPSS): In [108], emotion additive model (EAM) was intro-",
          "5": "emotional  music  [117].  The  problem  of  long-term  coher-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "ence when generating affective music was investigated by"
        },
        {
          "AUTHOR ET AL.:  TITLE": "duced to represent the difference between emotional and",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "MorpheuS [118]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "neutral  voices.  EAMs  can  be  linearly  combined  to  create",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "certain emotional speech. Eigenvoice was also constructed",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "3.3 Affective facial expression generation"
        },
        {
          "AUTHOR ET AL.:  TITLE": "for better synthesis in target emotion speech. In [109], Xue",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "Based on the characteristics of input/output images, facial"
        },
        {
          "AUTHOR ET AL.:  TITLE": "et al. studied the model adaption models in this problem:",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "expression generation can be categorized into virtual ava-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "retraining model with emotion-specific data, augmenting",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "tar  animation and  photorealistic  face  synthesis  [119].  De-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "model input using emotion specific codes, and using emo-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "pending on whether the input and the output shared the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tion depedent output layers with shared hidden layers. An",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "same identity or not, facial expression generation could be"
        },
        {
          "AUTHOR ET AL.:  TITLE": "et  al.  [110]  proposed  to  combine  a  LSTM  with  a  mixture",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "categorized into facial expression editing (FEE) and facial"
        },
        {
          "AUTHOR ET AL.:  TITLE": "density  network  for  multimodal  regression  and  variance",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "expression transfer (FET).  In FEE, the facial expression is"
        },
        {
          "AUTHOR ET AL.:  TITLE": "prediction. KL-divergence was also introduced to maxim-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "changed  on  a  given  portrait,  whereas  in  FET,  the  model"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ize  the  distance  between  the  distributions  of  emotional",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "5": "works to transfer facial expression between different iden-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "speech and neutural speech. Inoue et al. [111] also investi-",
          "5": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "gated this problem with several models: the parallel model,",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "tities. Based on the adopted  models, facial image expres-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "unit and several combinations of them."
        },
        {
          "6": "sion syntheses can be categorized into traditional graphic-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "There are also works trying to combine the strengths of"
        },
        {
          "6": "based  models  and  emerging  generative,  data-driven  ap-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "graphic methods and data-driven approaches:"
        },
        {
          "6": "proaches. All methods either rely on the extraction of fea-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "A  Geometry-Guided  Generative  Adversarial  Network"
        },
        {
          "6": "tures from geometric [120], appearance [121], RGBD space",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "(G2-GAN) was designed for continuously adjustable and"
        },
        {
          "6": "[122],  [123],  or  crossfading,  wrapping/morphing  existing",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "identity preservable facial expression synthesis (FEE) [133],"
        },
        {
          "6": "faces [119].",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "where  facial  geometry  was  adopted  to  guide  the  genera-"
        },
        {
          "6": "The  following  work  is  an  example  of  FEE  adopting  a",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tion of facial texture synthesis with a certain expression. In"
        },
        {
          "6": "graphic-based model: Kollias et al. [124] proposed a model",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "this model, paired submodels were jointly trained for ex-"
        },
        {
          "6": "that took as input an arbitrary face with neutral expression",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pression removal and expression synthesis, which formed"
        },
        {
          "6": "and synthesized a new expression on it with either a target",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "a cycle between neutral expression and any other expres-"
        },
        {
          "6": "emotion  category  or  emotion  valence  and  arousal.  The",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "sion.  Similarly,  a  Geometry-Contrastive  Generative  Ad-"
        },
        {
          "6": "model fit a 3D morphable model on an input image, then",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "versarial  Network \n(GC-GAN) \n[134]  was  proposed \nfor"
        },
        {
          "6": "deformed the reconstructed  face and added the input af-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "transferring continuous emotions across different subjects."
        },
        {
          "6": "fect, finally blended the new face with the given affect into",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The embedded geometry is injected into the latent space of"
        },
        {
          "6": "the original image.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "GC-GAN as continuous conditions to guide the generation"
        },
        {
          "6": "By contrast, more and more works adopted data-driven",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of facial expressions effectively. Yeh et al. [135] presented"
        },
        {
          "6": "approaches,  mostly  resorting  to  deep  neural  networks,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "an  automatic  approach  of  FEE,  such  as  from  smiling  to"
        },
        {
          "6": "such  as  generative  adversarial  networks  (GANs)  [125],",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "neutral. Their approach combined flow-based face manip-"
        },
        {
          "6": "conditional  GANs  (cGANs)  [126],  and  conditional  varia-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ulation with the generative capability of Variational Auto-"
        },
        {
          "6": "tional  autoencoders  (cVAEs)  [127],  which  required  large",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "encoder (VAE), which learned to encode the flows among"
        },
        {
          "6": "scale training data instead of paired samples, as opposited",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "different  expressions \nin  a \nlatent \nspace.  Their \nresults"
        },
        {
          "6": "to some of the graphic methods, to properly disambiguate",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "demonstrated  higher  perception  quality \nthan  previous"
        },
        {
          "6": "identity information [124].",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "methods adopting VAEs."
        },
        {
          "6": "Zhou proposed a conditional difference adversarial au-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "More than developing affective facial expression gener-"
        },
        {
          "6": "toencoder \n(CDAAE) \nfor  photorealistic  FET \n[119].  The",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ation models on images, Thies et al. [136] further tried face"
        },
        {
          "6": "CDAAE learned to generate an unseen person’s facial ex-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "reenactment, which did facial expression synthesis in vid-"
        },
        {
          "6": "pression with a discrete target emotion or facial action unit.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "eos: Their model animated the facial expressions of a target"
        },
        {
          "6": "The  CDAAE  worked  partially  due  to  the  newly  added",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "video by a source actor and re-render the manipulated out-"
        },
        {
          "6": "feed-forward path connecting low-level features at the en-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "put  video \nin  a  photorealistic  fashion.  Technically, \nthe"
        },
        {
          "6": "coder with the corresponding level at the decoder. In [128],",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "model first dealt with facial identity recovery by non-rigid"
        },
        {
          "6": "ExprGen was presented to take as input images of human",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "model-based bundling. The model then tracked facial ex-"
        },
        {
          "6": "faces  and  generate  the  character  rig  parameters  accord-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pressions of both target and source videos by a dense pho-"
        },
        {
          "6": "ingly for virtual avatar animation. ExprGen was a multi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tometric consistency measure. The video reenactment was"
        },
        {
          "6": "stage deep learning system using the latent variables of hu-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "finally  achieved  by  deformation  transfer  between  source"
        },
        {
          "6": "man and character recognition convolutional nets to  con-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and target [136]. They later added other functionalities, ex-"
        },
        {
          "6": "trol 3D animated character rig. The multi-stage deep learn-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tending the modelling of facial expressions onto head, eye,"
        },
        {
          "6": "ing  system  consisted  of  3D-CNN  and  Character  Multi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and  kinematic  torso  [137].  ReenactGAN  [138]  was  engi-"
        },
        {
          "6": "Layer  Perceptron.  The  multi-stage  deep  learning  system",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "neered to transfer facial movements and expressions from"
        },
        {
          "6": "was trained using five publicly available labeled facial ex-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "an  arbitrary  person’s  monocular  video  input  to  a  target"
        },
        {
          "6": "pression datasets.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "person’s  video.  It  mapped  the  source  face  to  a  boundary"
        },
        {
          "6": "Instead of using discrete emotion labels, continuous la-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "latent space, adapted source face’s boundary to the target’s"
        },
        {
          "6": "bels/dimensions were also adopted in some works:  Ding",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "boundary, and generated the reenacted target face. Ma and"
        },
        {
          "6": "et al. proposed Expression GAN (ExprGAN) [129] for pho-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Deng [139] proposed a real-time end-to-end facial reenact-"
        },
        {
          "6": "torealistic  FEE  with \ncontrollable  expression \nintensity,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ment system, without the need  for any driving source. It"
        },
        {
          "6": "which enabled the expression intensity to be continuously",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "could generate desired photorealistic facial expressions on"
        },
        {
          "6": "adjusted  from  low  to  high.  Tang  et  al.  [130]  tried  fine-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "top of input RGB video, with an unpaired learning frame-"
        },
        {
          "6": "grained expression manipulation with expression-guided",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "work developed to learn the mapping between any two fa-"
        },
        {
          "6": "GAN (EGGAN), which could synthesize photorealistic im-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "cial expressions in the facial blendshape space. Otberdout"
        },
        {
          "6": "ages  with  continuous  intermediate  expressions  based  on",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "et al. [140] proposed a facial reenactment model, exploiting"
        },
        {
          "6": "continuous  emotion  labels  and  structured  latent  codes.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the face geometry by modelling the facial landmarks mo-"
        },
        {
          "6": "Pham et al. [131] presented generative adversarial talking",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tion  on  a  hypersphere.  A  GAN  was  adopted  to  generate"
        },
        {
          "6": "head  (GATH),  a  deep  neural  network  that  enabled  facial",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "facial  landmark  motion  in  the  hypersphere  to  synthesize"
        },
        {
          "6": "expression  synthesis  of  a  given  portrait  with  continuous",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "various facial expressions."
        },
        {
          "6": "action unit coefficients. Their model directly manipulated",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Beyond facial expression synthesis on images and video"
        },
        {
          "6": "image pixels to generate various expressions on an unseen",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "reenactment, there were also works on affective image-to-"
        },
        {
          "6": "face,  while  maintaining  features  such  as  facial  geometry,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "video translation, a FEE problem: Conditional MultiMode"
        },
        {
          "6": "skin  color,  hair  style,  and  surrounding  background.  In",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Network (CMM-Net) [141] was devised for generating var-"
        },
        {
          "6": "[132],  GANimation  was  introduced  as  a  novel  GAN  that",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ious  facial  expression  videos  with  distinctive  characteris-"
        },
        {
          "6": "could  control  the  magnitude  of  activation  of  each  action",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tics, given a neutral face image and a discrete emotion label."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "The input face image and emotion label were used to gen-",
          "7": "used as an intuitive controller of the emotional state of syn-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "erate landmark sequences, which were used to guide the",
          "7": "thesized face. Their model consisted of three modules: for-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "translation  from  the  neutral  image  into  facial  expression",
          "7": "mant  analysis  network,  articulation  network,  and  output"
        },
        {
          "AUTHOR ET AL.:  TITLE": "video.  Potamias  et  al.  [142] advanced  the  image-to-video",
          "7": "network. Their model could yield reasonable results for in-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "translation problem from 2D to 3D: they took as input 3D",
          "7": "puts of other speakers, even being trained only on audios"
        },
        {
          "AUTHOR ET AL.:  TITLE": "meshes instead of 2D images and, therefore, their genera-",
          "7": "from a single speaker. Pham et al. [151] proposed training"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tive model output 4D facial expression video. Their exper-",
          "7": "a  LSTM  network  on  a  large  audio-visual  data  corpus  for"
        },
        {
          "AUTHOR ET AL.:  TITLE": "imental results demonstrated the preservation of identity",
          "7": "real-time facial animation, with audio stream as input. The"
        },
        {
          "AUTHOR ET AL.:  TITLE": "and  high-quality  expression \nsynthesis.  Similarly,  Mo-",
          "7": "proposed network could estimate head rotation and facial"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tion3DGAN [143] exploited a set of 3D landmarks to gen-",
          "7": "action unit activations from the speaker’s speech. In [152],"
        },
        {
          "AUTHOR ET AL.:  TITLE": "erate  expressive  4D  faces  by  modeling  the  temporal  dy-",
          "7": "both  acoustic  features  and  phoneme  label  features  were"
        },
        {
          "AUTHOR ET AL.:  TITLE": "namics of facial expressions using a manifold-valued GAN,",
          "7": "utilized to generate natural looking, speaker-independent"
        },
        {
          "AUTHOR ET AL.:  TITLE": "with a neutral face as input.",
          "7": "lip animations synchronized with affective speech."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Other works include FET-FEE fusion and talking-head",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "3.4 Affective movement generation"
        },
        {
          "AUTHOR ET AL.:  TITLE": "video generation with affect rich facial expression:",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "In 2019, Ali and Hughes [144] introduced Transfer-Ed-",
          "7": "Body  movement,  along  with  facial  expression,  are  both"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "forms of non-verbal communication. Body movement can"
        },
        {
          "AUTHOR ET AL.:  TITLE": "iting  and  Recognition  Generative  Adversarial  Network",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "be  characterized  by  three  dimensions,  namely,  function,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "(TER-GAN),  a  model to  realize  three  functions:  facial ex-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "execution, and expression [153]. Affective body movement"
        },
        {
          "AUTHOR ET AL.:  TITLE": "pression  transfer,  editing,  and  recognition.  When  doing",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "generation focuses on the aspect of expression, which re-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "FET in TER-GAN, two encoders were adopted to encode",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "identity  and  expression  information  from  the  source  and",
          "7": "flects and influences the affective qualities, such as believ-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "target input image, respectively. When doing FEE in TER-",
          "7": "ability  and  engagement  that  the  movement  is  conveying"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "[153],  [154].  Body  movement  generation  can  be  realized"
        },
        {
          "AUTHOR ET AL.:  TITLE": "GAN, two portraits with the same identity but different fa-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "through either physics-based approaches, in which param-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cial expressions were input to the two encoders. The gen-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "eters  were  adopted  to  control  specific  actions  or  data-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "erated  images,  either  in  FEE  or  FET,  were  more  blurry,",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "driven machine learning approaches that relied on human"
        },
        {
          "AUTHOR ET AL.:  TITLE": "compared  with  ones  generated  by  ExprGAN.  Zeng  et  al.",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "proposed an end-to-end expression-tailored generative ad-",
          "7": "actors  performing  actions  as  the  demo.  Most  affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "versarial network (ET-GAN) to generate talking face vid-",
          "7": "movement  generation  works  were  concerned  with  body"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "movement and head movement, with few works focusing"
        },
        {
          "AUTHOR ET AL.:  TITLE": "eos  of  identity  with  enriched  facial  expression  [145].  Ex-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "on hand gesture synthesis [155]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "pressional video of that identity, instead of identity image",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "and audio, was the model input.",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "3.4.1 Affective movement generation of virtual"
        },
        {
          "AUTHOR ET AL.:  TITLE": "The aforementioned works were concerned with affec-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "agent"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tive facial expression generation on the screen, Benson et",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "Most works concerning affective movement generation for"
        },
        {
          "AUTHOR ET AL.:  TITLE": "al.  [146],  [147],  however,  developed  a  Facial  Expression",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "virtual agents adopted physics-based approaches:"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Time Petri Net (FETPN) model to display facial expression",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "In  [156], a  general  parameterized  behavior  model  was"
        },
        {
          "AUTHOR ET AL.:  TITLE": "on an embodied robotic face. They treated the expression",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "adopted to integrate affect expression with functional be-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "of an affective state as a time-constrained behavior of facial",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "haviors. Models were parameterized by both spatial extent"
        },
        {
          "AUTHOR ET AL.:  TITLE": "physiognomy  and  regarded  the  facial  physiognomic  fea-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "and motion dynamics. The model was applied to coverbal"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tures as components of a concurrent system. Discrete facial",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "gestures  with  a  NAO  robot  in  order  to  express  mood  in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "expression was realized through the movement of critical",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "storytelling scenarios."
        },
        {
          "AUTHOR ET AL.:  TITLE": "facial points on latex-made  mask (with wires attached to",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "Burton et al. [157] proposed to imbue a given trajectory"
        },
        {
          "AUTHOR ET AL.:  TITLE": "apply pressure and pull/ release the facial mask segments).",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "of  robot  movement  with  expressive  content,  which  was"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Huang  et  al.  [148]  proposed  a  facial  motion \nimitation",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "sampled  from  a  database  of  movements  with  expressive"
        },
        {
          "AUTHOR ET AL.:  TITLE": "method  to  transfer  facial  geometric  characteristics  from",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "qualities. This method worked to find emotionally similar"
        },
        {
          "AUTHOR ET AL.:  TITLE": "humans to robots.",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "movements from the database, based on Laban movement"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Affective speech-driven facial animation has also been",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "analysis [158]."
        },
        {
          "AUTHOR ET AL.:  TITLE": "investigated several times:",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "Carreno-Medrano et al. [159], [160] tried to represent af-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Sadiq and Erzin [149] investigated this problem by do-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "fective  bodily  movement  through  a  low-dimensional  pa-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "main  adaption:  affective  and  neutral  speech  representa-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "rameterization based on the spatio-temporal trajectories of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tions  were  first  mapped  to  a  common  latent  space  with",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "some basic joints in the human body. It was assumed that"
        },
        {
          "AUTHOR ET AL.:  TITLE": "smaller bias, then the domain adaption  augmented affec-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "the  low-dimensional  parameterization  encodes  the  affec-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tive  representation  for  each  discrete  emotion  state.  An",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "tive  state  that  could  also  be  mapped  to  whole-body  mo-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "emotion-dependent deep audio-to-visual model was then",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "tions."
        },
        {
          "AUTHOR ET AL.:  TITLE": "trained  on  a  public  dataset  for  affective  facial  expression",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "Xia  et  al.  [161]  proposed  a  real-time  style  translation"
        },
        {
          "AUTHOR ET AL.:  TITLE": "generation.  Both  objective  and  subjective  measurements",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "model  that  automatically  transformed  unlabeled,  hetero-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "justified the model’s performance. Karras et al. [150] devel-",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "geneous motion data into new styles. The styles of the out-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "oped  low  latency,  audio-driven  facial  animation  models.",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "put animation could be blended by blending the parame-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "The end-to-end neural net learned a mapping from input",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "ters of distinctive styles. For example, a “neutral” walk can"
        },
        {
          "AUTHOR ET AL.:  TITLE": "audio to facial coordinates and a latent code that could be",
          "7": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "7": "be translated into an “angry”-“strutting” walk by linearly"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "interpolating two distinctive output animation styles: “an-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "parametrization.  Sial  et  al.  [174]  introduced  a  non-verbal"
        },
        {
          "8": "gry” and “strutting”.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and non-facial method for a “mechanoid robot” to express"
        },
        {
          "8": "In \n[162],  an \ninverse  kinematics \n(IK) \nreconstruction",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "emotions through gestures. Their results indicated that the"
        },
        {
          "8": "model along with a statistical motion resampling scheme",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "motion parameters of robots were linked with the change"
        },
        {
          "8": "were  proposed  to  synthesize  high-dimensional  full-body",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of emotions. How emotions could be expressed in swarms"
        },
        {
          "8": "movements  from  low-dimensional  end-effector  trajecto-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of miniature mobile robots were investigated in [175]."
        },
        {
          "8": "ries. An inverse kinematic controller was defined for each",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Claret et al. [176] studied the problem of a robot execut-"
        },
        {
          "8": "limb movement while the resampling scheme was for end-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ing  a  primary  task  and  simultaneously  conveying  emo-"
        },
        {
          "8": "effector  and  pelvis  trajectories  generation,  with  the  con-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tions  using  body  motions,  which  was  defined  as  a  lower"
        },
        {
          "8": "straint of preserving underlying emotional states.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "priority task. They explored the possibility of using kine-"
        },
        {
          "8": "There  are  plenty  of  works  on  data-driven  approaches",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "matic redundancy of a robot to convey emotions. Happi-"
        },
        {
          "8": "for body movement generation, but only a few considering",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ness and sadness were shown as very well delivered, and"
        },
        {
          "8": "the impact of affect states [163], [164]. Alemi et al. [153] pre-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "calm  was  moderately  conveyed,  while  fear  was  not  well"
        },
        {
          "8": "sented an interactive animated agent model with control-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "delivered."
        },
        {
          "8": "lable  affective  movements.  A  Factored,  Conditional  Re-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Löffler et al. [177] investigated using output modalities"
        },
        {
          "8": "stricted Boltzman Machine (FCRBM) [165] was adopted to",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of  color,  motion,  and  sound  for  expressing  joy,  sadness,"
        },
        {
          "8": "control  the  valence  and  arousal  of  walking  movements,",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "fear, and anger in an appearance-constrained social robot."
        },
        {
          "8": "with a corpus of recorded actor-performed affective walk-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The best expressions for each modality and emotion were"
        },
        {
          "8": "ing. Two actors were recruited to perform 9 different ex-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "selected and systematically combined."
        },
        {
          "8": "pressive combinations of valence (negative, neutral, posi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "tive) and arousal (low, neutral, high).",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "4  CHALLENGES"
        },
        {
          "8": "3.4.2 Affective robotic movement generation",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Challenges and potential contributions of existing affective"
        },
        {
          "8": "Using  NAO  and  Keepon \n[166],  Knight  and  Simmons",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "generation models are discussed in this section."
        },
        {
          "8": "showed that simple robots could convey complex emotion",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "4.1  Contextualization"
        },
        {
          "8": "through motion and varying robot task motions was suffi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "cient to communicate a variety of expressive states [167].",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Context, including both cultural context and social context,"
        },
        {
          "8": "In [168], in order to let robots expressing dominance, au-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "is a critical problem in delivering affective messages [178]."
        },
        {
          "8": "thors developed a parameter-based model for head tilt and",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "It has been shown the importance of context in under-"
        },
        {
          "8": "body expansiveness. The model was applied to a collection",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "standing the meaning of words [179] and coherence of sen-"
        },
        {
          "8": "of behaviors, which were evaluated by human observers.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "tences  [180].  However,  among  all  the  reviewed  affective"
        },
        {
          "8": "Some works used learning from demonstration or imi-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "text generation works, only Affect-LM [85] considered the"
        },
        {
          "8": "tation learning to let robots learn from human gestures to",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "context words, although Affect-LM only considered words"
        },
        {
          "8": "express  emotions  [169]:  Suguitan  et  al.  [170]  investigated",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "within the range of a sentence. Among the reviewed works"
        },
        {
          "8": "robot  movement \ngeneration \nthrough \nlearning \nfrom",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "on affective speech synthesis, the work in [181] adopted fil-"
        },
        {
          "8": "demonstration of non-professional actors, with the help of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ters to extract contextual information. So far, we have not"
        },
        {
          "8": "CycleGANs [171]. The CycleGAN consisted  of  a forward",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "found any works considering context in affective facial ex-"
        },
        {
          "8": "cycle  and  a  backward  cycle.  In  the  forward  cycle,  an  en-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pression and affective movement generation."
        },
        {
          "8": "coder-decoder  module  was  adopted  to  generate  a  robot",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The  future  works  on  affective  generation  models  are"
        },
        {
          "8": "movement  with  human  movement  as  input.  In the  back-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "suggested to take into account the contextual information,"
        },
        {
          "8": "ward  cycle,  the  generated  robot  movement  was  passed",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "which  should  not  be  limited  within  the  range  of  words,"
        },
        {
          "8": "into  another  encoder-decoder  module  for  human  move-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "sentences, but also the social attitude and cultural prefer-"
        },
        {
          "8": "ment reconstruction.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "ence. Social attitude in dialogues can permeate through the"
        },
        {
          "8": "Affective body movement generation does not only ex-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "whole  interaction  process,  rather  than  occur  at  a  certain"
        },
        {
          "8": "ist  for  virtual  agents  and  embodied  humanoid  robots.",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "moment [49]. Moreover, better contextualization can pro-"
        },
        {
          "8": "Non-humanoid robots can also express emotion [175]:",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "vide  more  information  on  emotion  boundaries.  Existing"
        },
        {
          "8": "Cauchard  et  al.  [172]  defined  a  range  of  personality",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "works  assume  emotions  are  either  instantaneous  or  over"
        },
        {
          "8": "traits  and  emotional  attributes  that  could  be  encoded  in",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "time, which is not necessarily valid [6]."
        },
        {
          "8": "drones through their flight paths and speed. Their results",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "4.2  Offline generation vs. online generation"
        },
        {
          "8": "indicated that adding an emotional component was part of",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "The exisiting works were all offline generation models that"
        },
        {
          "8": "the key to success in drones’ acceptability. Jørgensen tried",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "were trained on a large corpus of data simultaneously. Of-"
        },
        {
          "8": "expressive movement generation in a soft robot [173]. Rin-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "fline learning is known for suffering from the restriction of"
        },
        {
          "8": "con et al. proposed an adaptive fuzzy mechanism to adjust",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "large memory and slow model updates. In the real-world"
        },
        {
          "8": "the perceived PAD (pleasure, arousal, and dominance) ac-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "human  machine  interaction  scenarios,  it  is  expected  that"
        },
        {
          "8": "cording  to  the  environmental  temperature,  humidity,  lu-",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "plenty of sensors will be deployed to collect a large amount"
        },
        {
          "8": "minosity,  and  human  proximity.  The  PAD  values  were",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "of  real-time  and  personalized  data.  The  data  processing"
        },
        {
          "8": "then used to change the motion trajectory of a robot arm to",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "speed will influence how fast and efficient an agent can re-"
        },
        {
          "8": "express  different  emotional  states.  The  motion  trajectory",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "spond to the sudden changes in the environment."
        },
        {
          "8": "was  commanded  by  the  Robust  Generalized  Predictive",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "An online learning model treats input data as a running"
        },
        {
          "8": "Controllers  (RGPC)  using  convex  optimization  by  Youla",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "8": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "stream and makes small incremental updates to the model."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "Online \nlearning  models  do  not  need \nlarge \ncapacity",
          "9": "year when deep learning drew public attention. Affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "memory to store input data for training, and it can better",
          "9": "generation is a technique that aims to generate messages to"
        },
        {
          "AUTHOR ET AL.:  TITLE": "fit the latest trends of patterns in the data stream as the in-",
          "9": "influence the emotional states of users. Topics including af-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "fluence of past data may be gradually discounted. Affec-",
          "9": "fective  generation  of  text,  audio,  facial  expression,  and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tive generative models that update online will likely pro-",
          "9": "movement  are  reviewed.  Challenges  and  potential  im-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "vide more adaptive and individualized solutions.",
          "9": "provements  of  existing  models  are  discussed  at  last.  We"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "hope this work can pave the way for future works on de-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "4.3  Multimodality",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "veloping novel affective generation models."
        },
        {
          "AUTHOR ET AL.:  TITLE": "In  affevtive  recognition  works,  multimodal  information",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "are fused to promote the recognition capability as different",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "ACKNOWLEDGMENT"
        },
        {
          "AUTHOR ET AL.:  TITLE": "modalities \ncan \nreinforce  or \ncomplement \neach  other.",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "This work is supported partially by the National Natural"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Whereas in multimodal affective generation, different mo-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "Science Foundation of China, No.: 62002090."
        },
        {
          "AUTHOR ET AL.:  TITLE": "dalities are expected to be generated in a consistent and co-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "herent manner. For example, if well organized, the gener-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "REFERENCES"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ated affective text, music and video can provide a compre-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "hensive and immersive  environment as a better delivery,",
          "9": "[1] \nR. W. Picard, Affective computing. MIT press, 2000."
        },
        {
          "AUTHOR ET AL.:  TITLE": "as ooposed to that of single modality.",
          "9": "[2] \nC. Darwin, The expression of the emotions in man and animals."
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "University of Chicago press, 2015."
        },
        {
          "AUTHOR ET AL.:  TITLE": "While there have been extensive works on multimodal",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[3] \nW. James, What is an Emotion? Simon and Schuster, 2013."
        },
        {
          "AUTHOR ET AL.:  TITLE": "affective recognition, limited effort has been dedicated for",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[4] \nR. Plutchik, “What is an emotion?,” J. Psychol., vol. 61, no. 2,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "multimodal  affective  generation.  The  coordination  and",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "pp. 295–303, 1965."
        },
        {
          "AUTHOR ET AL.:  TITLE": "synchronization  among  different  modalities  have  been",
          "9": "[5] \nA. R. Damasion, Descartes’ error: emotion, reason and the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "shown  to  effectively  improve  the  delivery  quality  of  the",
          "9": "human brain. Vintage, 1994."
        },
        {
          "AUTHOR ET AL.:  TITLE": "generated content as well as the user satisfaction [106]. Fu-",
          "9": "[6] \nR. A. Calvo and S. D’Mello, “Affect detection: An"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "interdisciplinary review of models, methods, and their"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ture works are encouraged to generate an immersive envi-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "applications,” IEEE Trans. Affect. Comput., vol. 1, no. 1, pp."
        },
        {
          "AUTHOR ET AL.:  TITLE": "ronment  where  multiple  modalities  complement  each",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "18–37, 2010."
        },
        {
          "AUTHOR ET AL.:  TITLE": "other to affect users' emotional states.",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[7] \nR. W. Picard, “No. 321,«Affective computing».” MIT Media"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "Laboratory Perceptual Computing Section Technical"
        },
        {
          "AUTHOR ET AL.:  TITLE": "4.4  Affective generation for special groups",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "Report, 1997."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Numerous \ntechnological \nsystems, \nincluding \naffective",
          "9": "[8] \nK. R. Scherer, “Studying the emotion-antecedent appraisal"
        },
        {
          "AUTHOR ET AL.:  TITLE": "recognition  ones,  have  been  specifically  designed  for  the",
          "9": "process: An expert system approach,” Cogn. Emot., vol. 7,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "welfare  and  rehabilitation  of  special  groups  [182],  [183],",
          "9": "no. 3–4, pp. 325–355, 1993."
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[9] \nJ. D. Schwark, “Toward a taxonomy of affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[184].  In  contrast, there  is  a lack  of  such  consideration  in",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "computing,” Int. J. Hum. Comput. Interact., vol. 31, no. 11,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "affective generation works.",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "pp. 761–768, 2015."
        },
        {
          "AUTHOR ET AL.:  TITLE": "When designing models to achieve optimal rehabilita-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[10] \nJ. Tao and T. Tieniu, “Affective Computing: A Review."
        },
        {
          "AUTHOR ET AL.:  TITLE": "tion purposes, affective generation should target not only",
          "9": "Affective Computing and Intelligent Interaction. LNCS"
        },
        {
          "AUTHOR ET AL.:  TITLE": "general audiences and users, but also special groups, such",
          "9": "3784,” Springer, vol. 981, p. 995, 2005."
        },
        {
          "AUTHOR ET AL.:  TITLE": "as the elderly group with dementia [185], children with de-",
          "9": "[11] \nS. Carberry and F. de Rosis, “Introduction to special Issue"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "on ‘Affective modeling and adaptation,’” User Model. User-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "velopmental  disorders \nlike  Autism  Spectrum  Disorder",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "adapt. Interact., vol. 18, no. 1–2, pp. 1–9, 2008."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[186], and people with depression [187] and anxiety [188],",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[12] \nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review"
        },
        {
          "AUTHOR ET AL.:  TITLE": "who need taking special care on emotional support.",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "of affective computing: From unimodal analysis to"
        },
        {
          "AUTHOR ET AL.:  TITLE": "For example, individuals with ASD are characterized",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "multimodal fusion,” Inf. Fusion, vol. 37, pp. 98–125, 2017."
        },
        {
          "AUTHOR ET AL.:  TITLE": "with the difficulties of perceiving other’s emotional  cues.",
          "9": "[13] \nR. Arya, J. Singh, and A. Kumar, “A survey of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "As a consequence, affective content generated by an auto-",
          "9": "multidisciplinary domains contributing to affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "mated agent designed for typically developing individuals",
          "9": "computing,” Comput. Sci. Rev., vol. 40, p. 100399, 2021."
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[14] \nM. Karg, A.-A. Samadani, R. Gorbet, K. Kühnlenz, J. Hoey,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "may  have  unwanted  or  unpredictable  emotional  influ-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "and D. Kulić, “Body movements for affective expression: A"
        },
        {
          "AUTHOR ET AL.:  TITLE": "ences on individuals with ASD.",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "survey of automatic recognition and generation,” IEEE"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "Trans. Affect. Comput., vol. 4, no. 4, pp. 341–359, 2013."
        },
        {
          "AUTHOR ET AL.:  TITLE": "4.5  Combination of affective generation and",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[15] \nG. Venture and D. Kulić, “Robot expressive motions: a"
        },
        {
          "AUTHOR ET AL.:  TITLE": "affective recognition",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "survey of generation and evaluation methods,” ACM Trans."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Affective  recognition  and  affective  generation  have  been",
          "9": "Human-Robot Interact., vol. 8, no. 4, pp. 1–17, 2019."
        },
        {
          "AUTHOR ET AL.:  TITLE": "studied separately in most scenarios. However, an intelli-",
          "9": "[16] \nE. Yadegaridehkordi, N. F. B. M. Noor, M. N. Bin Ayub, H."
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "B. Affal, and N. B. Hussin, “Affective computing in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "gent agent should be endowed with the capability of both",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "education: A systematic review and future research,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "input signal processing and analysis (affective recognition)",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "Comput. Educ., vol. 142, p. 103649, 2019."
        },
        {
          "AUTHOR ET AL.:  TITLE": "and generating appropriate messages as a response (affec-",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[17] \nC. Wu, Y. Huang, and J. Hwang, “Review of affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "tive generation). Heretofore, we have not found any work",
          "9": ""
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "computing in education/learning: Trends and challenges,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "combining affective generation with affective recognition",
          "9": "Br. J. Educ. Technol., vol. 47, no. 6, pp. 1304–1323, 2016."
        },
        {
          "AUTHOR ET AL.:  TITLE": "when designing an intelligent affective agent.",
          "9": "[18] \nA. Luneski, P. D. Bamidis, and M. Hitoglou-Antoniadou,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "“Affective computing and medical informatics: state of the"
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "art in emotion-aware medical applications,” Stud. Health"
        },
        {
          "AUTHOR ET AL.:  TITLE": "5  CONCLUSIONS",
          "9": "Technol. Inform., vol. 136, p. 517, 2008."
        },
        {
          "AUTHOR ET AL.:  TITLE": "",
          "9": "[19] \nA. Luneski, E. Konstantinidis, and P. Bamidis, “Affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "We  reviewed  affective  generation  models  since  2015,  the",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pp. 940–943."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "S. Freud and P. M. Bonaparte, The origins of psychoanalysis,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "vol. 216. Imago London, 1954."
        },
        {
          "10": "[20]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "D. Kulic and E. A. Croft, “Affective state estimation for"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "human–robot interaction,” IEEE Trans. Robot., vol. 23, no. 5,"
        },
        {
          "10": "[21]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pp. 991–1000, 2007."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "K. Oatley and P. N. Johnson-Laird, “Towards a cognitive"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "theory of emotions,” Cogn. Emot., vol. 1, no. 1, pp. 29–50,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "1987."
        },
        {
          "10": "[22]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. Buck, “Nonverbal behavior and the theory of emotion:"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the facial feedback hypothesis.,” J. Pers. Soc. Psychol., vol."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "38, no. 5, p. 811, 1980."
        },
        {
          "10": "[23]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "A. R. Damasio, The feeling of what happens: Body and emotion"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "in the making of consciousness. Houghton Mifflin Harcourt,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "1999."
        },
        {
          "10": "[24]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "C. E. Izard, “Emotion theory and research: Highlights,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "unanswered questions, and emerging issues,” Annu. Rev."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Psychol., vol. 60, pp. 1–25, 2009."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "S. K. Langer, Mind: An essay on human feeling, vol. 2. JHU"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Press, 1967."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. A. Calvo, S. D’Mello, J. M. Gratch, and A. Kappas, The"
        },
        {
          "10": "[25]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Oxford handbook of affective computing. Oxford University"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Press, USA, 2015."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "T. Dalgleish and M. Power, Handbook of cognition and"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "emotion. John Wiley & Sons, 2000."
        },
        {
          "10": "[26]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "K. R. Scherer, “What are emotions? And how can they be"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "measured?,” Soc. Sci. Inf., vol. 44, no. 4, pp. 695–729, 2005."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. W. Picard, “Affective computing: challenges,” Int. J."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Hum. Comput. Stud., vol. 59, no. 1–2, pp. 55–64, 2003."
        },
        {
          "10": "[27]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "M. J. Power, “The structure of emotion: An empirical"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "comparison of six models,” Cogn. Emot., vol. 20, no. 5, pp."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "694–713, 2006."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. A. Russell, “Core affect and the psychological"
        },
        {
          "10": "[28]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "construction of emotion.,” Psychol. Rev., vol. 110, no. 1, p."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "145, 2003."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "D. Watson and L. A. Clark, “Affects separable and"
        },
        {
          "10": "[29]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "inseparable: on the hierarchical arrangement of the negative"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "affects.,” J. Pers. Soc. Psychol., vol. 62, no. 3, p. 489, 1992."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "A. Mehrabian, “Pleasure-arousal-dominance: A general"
        },
        {
          "10": "[30]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "framework for describing and measuring individual"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "differences in temperament,” Curr. Psychol., vol. 14, no. 4,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pp. 261–292, 1996."
        },
        {
          "10": "[31]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "M. M. Bradley and P. J. Lang, “Measuring emotion: the self-"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "assessment manikin and the semantic differential,” J. Behav."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Ther. Exp. Psychiatry, vol. 25, no. 1, pp. 49–59, 1994."
        },
        {
          "10": "[32]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. Cowie, E. Douglas-Cowie, S. Savvidou*, E. McMahon, M."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Sawey, and M. Schröder, “‘FEELTRACE’: An instrument"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "for recording perceived emotion in real time,” 2000."
        },
        {
          "10": "[33]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. Posner, J. A. Russell, and B. S. Peterson, “The circumplex"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "model of affect: An integrative approach to affective"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "neuroscience, cognitive development, and"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "psychopathology,” Dev. Psychopathol., vol. 17, no. 3, p. 715,"
        },
        {
          "10": "[34]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2005."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "M. M. Bradley, M. K. Greenwald, M. C. Petry, and P. J."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Lang, “Remembering pictures: pleasure and arousal in"
        },
        {
          "10": "[35]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "memory.,” J. Exp. Psychol. Learn. Mem. Cogn., vol. 18, no. 2,"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "p. 379, 1992."
        },
        {
          "10": "[36]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "D. Watson, L. A. Clark, and A. Tellegen, “Development and"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "validation of brief measures of positive and negative affect:"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "the PANAS scales.,” J. Pers. Soc. Psychol., vol. 54, no. 6, p."
        },
        {
          "10": "[37]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "1063, 1988."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. E. Thayer, “Activation-Deactivation Adjective Check"
        },
        {
          "10": "[38]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "List: Current Overview and Structural Analysis,” Psychol."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Rep., vol. 58, no. 2, pp. 607–614, Apr. 1986, doi:"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "10.2466/pr0.1986.58.2.607."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "C. E. Izard, Human emotions. Springer Science & Business"
        },
        {
          "10": "[39]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Media, 2013."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "C. E. Izard, “Basic emotions, natural kinds, emotion"
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "schemas, and a new paradigm,” Perspect. Psychol. Sci., vol."
        },
        {
          "10": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2, no. 3, pp. 260–280, 2007."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "[63] \nC. Izard, “Levels of emotion and levels of consciousness,”",
          "11": "arXiv1704.06851, 2017."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Behav. Brain Sci., vol. 30, no. 1, p. 96, 2007.",
          "11": "H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[64] \nJ. A. Russell, “Is there universal recognition of emotion",
          "11": "“Emotional chatting machine: Emotional conversation"
        },
        {
          "AUTHOR ET AL.:  TITLE": "from facial expression? A review of the cross-cultural",
          "11": "generation with internal and external memory,” in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "studies.,” Psychol. Bull., vol. 115, no. 1, p. 102, 1994.",
          "11": "Proceedings of the AAAI Conference on Artificial Intelligence,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[65] \nP. Ekman et al., “Universals and cultural differences in the",
          "11": "2018, vol. 32, no. 1."
        },
        {
          "AUTHOR ET AL.:  TITLE": "judgments of facial expressions of emotion.,” J. Pers. Soc.",
          "11": "C. Huang, A. Trabelsi, X. Qin, N. Farruque, L. Mou, and O."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Psychol., vol. 53, no. 4, p. 712, 1987.",
          "11": "R. Zaiane, “Seq2Emo: A Sequence to Multi-Label Emotion"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[66] \nP. Ekman, “Universal facial expressions of emotions,” Calif.",
          "11": "Classification Model,” in Proceedings of the 2021 Conference of"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Ment. Heal. Res. Dig., vol. 8, no. 4, pp. 151–158, 1970.",
          "11": "the North American Chapter of the Association for"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[67] \nP. Ekman, “An argument for basic emotions,” Cogn. Emot.,",
          "11": "Computational Linguistics: Human Language Technologies,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "vol. 6, no. 3–4, pp. 169–200, 1992.",
          "11": "2021, pp. 4717–4724."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[68] \nC. E. Izard, “The face of emotion.,” 1971.",
          "11": "Y. Liu, J. Du, X. Li, and R. Xu, “Generating Empathetic"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[69] \nD. M. McNair, M. Lorr, and L. F. Droppleman, EdITS",
          "11": "Responses by Injecting Anticipated Emotion,” in ICASSP"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Manual for the Profile of Mood States (POMS). Educational",
          "11": "2021-2021 IEEE International Conference on Acoustics, Speech"
        },
        {
          "AUTHOR ET AL.:  TITLE": "and industrial testing service, 1992.",
          "11": "and Signal Processing (ICASSP), 2021, pp. 7403–7407."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[70] \nC. A. Smith and P. C. Ellsworth, “Patterns of cognitive",
          "11": "Y.-P. Ruan and Z. Ling, “Emotion-Regularized Conditional"
        },
        {
          "AUTHOR ET AL.:  TITLE": "appraisal in emotion.,” J. Pers. Soc. Psychol., vol. 48, no. 4, p.",
          "11": "Variational Autoencoder for Emotional Response"
        },
        {
          "AUTHOR ET AL.:  TITLE": "813, 1985.",
          "11": "Generation,” IEEE Trans. Affect. Comput., 2021."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[71] \nE. Diener, H. Smith, and F. Fujita, “The personality",
          "11": "I. Sutskever, O. Vinyals, and Q. V Le, “Sequence to"
        },
        {
          "AUTHOR ET AL.:  TITLE": "structure of affect.,” J. Pers. Soc. Psychol., vol. 69, no. 1, p.",
          "11": "sequence learning with neural networks,” in Advances in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "130, 1995.",
          "11": "neural information processing systems, 2014, pp. 3104–3112."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[72] \nA. Gatt and E. Krahmer, “Survey of the state of the art in",
          "11": "Z. Song, X. Zheng, L. Liu, M. Xu, and X.-J. Huang,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "natural language generation: Core tasks, applications and",
          "11": "“Generating responses with a specific emotion in dialog,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "evaluation,” J. Artif. Intell. Res., vol. 61, pp. 65–170, 2018.",
          "11": "in Proceedings of the 57th Annual Meeting of the Association for"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[73] \nE. Reiter and R. Dale, “Building applied natural language",
          "11": "Computational Linguistics, 2019, pp. 3685–3695."
        },
        {
          "AUTHOR ET AL.:  TITLE": "generation systems,” Nat. Lang. Eng., vol. 3, no. 1, pp. 57–87,",
          "11": "K. Wang and X. Wan, “SentiGAN: Generating Sentimental"
        },
        {
          "AUTHOR ET AL.:  TITLE": "1997.",
          "11": "Texts via Mixture Adversarial Networks.,” in IJCAI, 2018,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[74] \nN. Asghar, P. Poupart, J. Hoey, X. Jiang, and L. Mou,",
          "11": "pp. 4446–4452."
        },
        {
          "AUTHOR ET AL.:  TITLE": "“Affective neural response generation,” in European",
          "11": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Conference on Information Retrieval, 2018, pp. 154–166.",
          "11": "“Toward controlled generation of text,” in International"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[75] \nA. B. Warriner, V. Kuperman, and M. Brysbaert, “Norms of",
          "11": "Conference on Machine Learning, 2017, pp. 1587–1596."
        },
        {
          "AUTHOR ET AL.:  TITLE": "valence, arousal, and dominance for 13,915 English",
          "11": "C. Huang, O. R. Zaiane, A. Trabelsi, and N. Dziri,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "lemmas,” Behav. Res. Methods, vol. 45, no. 4, pp. 1191–1207,",
          "11": "“Automatic dialogue generation with expressed emotions,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "2013.",
          "11": "in Proceedings of the 2018 Conference of the North American"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[76] \nD. Varshney, A. Ekbal, and P. Bhattacharyya, “Modelling",
          "11": "Chapter of the Association for Computational Linguistics:"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Context Emotions using Multi-task Learning for Emotion",
          "11": "Human Language Technologies, Volume 2 (Short Papers), 2018,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Controlled Dialog Generation,” in Proceedings of the 16th",
          "11": "pp. 49–54."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Conference of the European Chapter of the Association for",
          "11": "L. Logeswaran, H. Lee, and S. Bengio, “Content preserving"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Computational Linguistics: Main Volume, 2021, pp. 2919–2931.",
          "11": "text generation with attribute controls,” Adv. Neural Inf."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[77] \nX. Zhou and W. Y. Wang, “Mojitalk: Generating emotional",
          "11": "Process. Syst., vol. 31, 2018."
        },
        {
          "AUTHOR ET AL.:  TITLE": "responses at scale,” arXiv Prepr. arXiv1711.04090, 2017.",
          "11": "A. Valitutti and T. Veale, “Inducing an ironic effect in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[78] \nP. Zhong, D. Wang, and C. Miao, “An affect-rich neural",
          "11": "automated tweets,” in 2015 International Conference on"
        },
        {
          "AUTHOR ET AL.:  TITLE": "conversational model with biased attention and weighted",
          "11": "Affective Computing and Intelligent Interaction (ACII), 2015,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cross-entropy loss,” in Proceedings of the AAAI Conference on",
          "11": "pp. 153–159, doi: 10.1109/ACII.2015.7344565."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Artificial Intelligence, 2019, vol. 33, no. 01, pp. 7492–7500.",
          "11": "M. Chindamo, J. Allwood, and E. Ahlsén, “Some"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[79] \nH. Prendinger, J. Mori, and M. Ishizuka, “Using human",
          "11": "suggestions for the study of stance in communication,” in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "physiology to evaluate subtle expressivity of a virtual",
          "11": "2012 International Conference on Privacy, Security, Risk and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "quizmaster in a mathematical game,” Int. J. Hum. Comput.",
          "11": "Trust and 2012 International Confernece on Social Computing,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Stud., vol. 62, no. 2, pp. 231–245, 2005.",
          "11": "2012, pp. 617–622."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[80] \nH. Prendinger and M. Ishizuka, “THE EMPATHIC",
          "11": "J. Yang, Y. Sun, J. Liang, B. Ren, and S.-H. Lai, “Image"
        },
        {
          "AUTHOR ET AL.:  TITLE": "COMPANION: A CHARACTER-BASED INTERFACE",
          "11": "captioning by incorporating affective concepts learned from"
        },
        {
          "AUTHOR ET AL.:  TITLE": "THAT ADDRESSES USERS’AFFECTIVE STATES,” Appl.",
          "11": "both visual and textual components,” Neurocomputing, vol."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Artif. Intell., vol. 19, no. 3–4, pp. 267–285, 2005.",
          "11": "328, pp. 56–68, 2019."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[81] \nB. Martinovski and D. Traum, “Breakdown in human-",
          "11": "A. Wang, H. Hu, and L. Yang, “Image captioning with"
        },
        {
          "AUTHOR ET AL.:  TITLE": "machine interaction: the error is the clue,” in Proceedings of",
          "11": "affective guiding and selective attention,” ACM Trans."
        },
        {
          "AUTHOR ET AL.:  TITLE": "the ISCA tutorial and research workshop on Error handling in",
          "11": "Multimed. Comput. Commun. Appl., vol. 14, no. 3, pp. 1–15,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "dialogue systems, 2003, pp. 11–16.",
          "11": "2018."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[82] \nA. Malhotra, L. Yu, T. Schröder, and J. Hoey, “An",
          "11": "M. Tahon, G. Lecorvé, and D. Lolive, “Can we Generate"
        },
        {
          "AUTHOR ET AL.:  TITLE": "exploratory study into the use of an emotionally aware",
          "11": "Emotional Pronunciations for Expressive Speech"
        },
        {
          "AUTHOR ET AL.:  TITLE": "cognitive assistant,” 2015.",
          "11": "Synthesis?,” IEEE Trans. Affect. Comput., vol. 11, no. 4, pp."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[83] \nJ. Pittermann, A. Pittermann, and W. Minker, “Emotion",
          "11": "684–695, 2018."
        },
        {
          "AUTHOR ET AL.:  TITLE": "recognition and adaptation in spoken dialogue systems,”",
          "11": "Y. Gao and W. Zhu, “Detecting affective states from text"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Int. J. Speech Technol., vol. 13, no. 1, pp. 49–60, 2010.",
          "11": "based on a multi-component emotion model,” Comput."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[84] \nZ. Callejas, D. Griol, and R. López-Cózar, “Predicting user",
          "11": "Speech Lang., vol. 36, pp. 42–57, 2016."
        },
        {
          "AUTHOR ET AL.:  TITLE": "mental states in spoken dialogue systems,” EURASIP J.",
          "11": "S. An, Z. Ling, and L. Dai, “Emotional statistical parametric"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Adv. Signal Process., vol. 2011, no. 1, pp. 1–21, 2011.",
          "11": "speech synthesis using LSTM-RNNs,” in 2017 Asia-Pacific"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[85] \nS. Ghosh, M. Chollet, E. Laksana, L.-P. Morency, and S.",
          "11": "Signal and Information Processing Association Annual Summit"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Scherer, “Affect-lm: A neural language model for",
          "11": "and Conference (APSIPA ASC), 2017, pp. 1613–1616."
        },
        {
          "AUTHOR ET AL.:  TITLE": "customizable affective text generation,” arXiv Prepr.",
          "11": "J. Lorenzo-Trueba, R. Barra-Chicote, R. San-Segundo, J."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "autoencoder,” in 2017 seventh international conference on"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "affective computing and intelligent interaction (ACII), 2017, pp."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "370–376."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Q. Zhang, Z. Liu, G. Quo, D. Terzopoulos, and H.-Y. Shum,"
        },
        {
          "12": "[104]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "“Geometry-driven photorealistic facial expression"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "synthesis,” IEEE Trans. Vis. Comput. Graph., vol. 12, no. 1,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "pp. 48–60, 2005."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "W. Xie, L. Shen, and J. Jiang, “A novel transient wrinkle"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "detection algorithm and its application for expression"
        },
        {
          "12": "[105]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "synthesis,” IEEE Trans. Multimed., vol. 19, no. 2, pp. 279–"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "292, 2016."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. Thies, M. Zollhöfer, M. Nießner, L. Valgaerts, M."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Stamminger, and C. Theobalt, “Real-time expression"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "transfer for facial reenactment.,” ACM Trans. Graph., vol. 34,"
        },
        {
          "12": "[106]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "no. 6, pp. 181–183, 2015."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. B. Queiroz, A. Braun, and S. R. Musse, “A framework for"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "generic facial expression transfer,” Entertain. Comput., vol."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "18, pp. 125–141, 2017."
        },
        {
          "12": "[107]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "“Deep neural network augmentation: Generating faces for"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "affect analysis,” arXiv Prepr. arXiv1811.05027, 2018."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "I. Goodfellow et al., “Generative adversarial nets,” Adv."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Neural Inf. Process. Syst., vol. 27, 2014."
        },
        {
          "12": "[108]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "M. Mirza and S. Osindero, “Conditional generative"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "adversarial nets,” arXiv Prepr. arXiv1411.1784, 2014."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "K. Sohn, H. Lee, and X. Yan, “Learning structured output"
        },
        {
          "12": "[109]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "representation using deep conditional generative models,”"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Adv. Neural Inf. Process. Syst., vol. 28, pp. 3483–3491, 2015."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "D. Aneja, B. Chaudhuri, A. Colburn, G. Faigin, L. Shapiro,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "and B. Mones, “Learning to generate 3D stylized character"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "expressions from humans,” in 2018 IEEE Winter Conference"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "on Applications of Computer Vision (WACV), 2018, pp. 160–"
        },
        {
          "12": "[110]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "169."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "H. Ding, K. Sricharan, and R. Chellappa, “Exprgan: Facial"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "expression editing with controllable expression intensity,”"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "in Proceedings of the AAAI Conference on Artificial Intelligence,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2018, vol. 32, no. 1."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. Tang, Z. Shao, and L. Ma, “EGGAN: Learning Latent"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Space for Fine-Grained Expression Manipulation,” IEEE"
        },
        {
          "12": "[111]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Multimed., 2021."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "H. X. Pham, Y. Wang, and V. Pavlovic, “Generative"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "adversarial talking head: Bringing portraits to life with a"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "weakly supervised neural network,” arXiv Prepr."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "arXiv1803.07716, 2018."
        },
        {
          "12": "[112]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and F."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Moreno-Noguer, “Ganimation: Anatomically-aware facial"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "animation from a single image,” in Proceedings of the"
        },
        {
          "12": "[113]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "European conference on computer vision (ECCV), 2018, pp."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "818–833."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "L. Song, Z. Lu, R. He, Z. Sun, and T. Tan, “Geometry"
        },
        {
          "12": "[114]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "guided adversarial facial expression synthesis,” in"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Proceedings of the 26th ACM international conference on"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Multimedia, 2018, pp. 627–635."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "F. Qiao, N. Yao, Z. Jiao, Z. Li, H. Chen, and H. Wang,"
        },
        {
          "12": "[115]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "“Geometry-contrastive gan for facial expression transfer,”"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "arXiv Prepr. arXiv1802.01822, 2018."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "R. Yeh, Z. Liu, D. B. Goldman, and A. Agarwala, “Semantic"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "facial expression editing using autoencoded flow,” arXiv"
        },
        {
          "12": "[116]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Prepr. arXiv1611.09961, 2016."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and M."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Nießner, “Face2face: Real-time face capture and"
        },
        {
          "12": "[117]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "reenactment of rgb videos,” in Proceedings of the IEEE"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "conference on computer vision and pattern recognition, 2016, pp."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2387–2395."
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "J. Thies, M. Zollhöfer, C. Theobalt, M. Stamminger, and M."
        },
        {
          "12": "[118]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "Nießner, “Headon: Real-time reenactment of human"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "portrait videos,” ACM Trans. Graph., vol. 37, no. 4, pp. 1–13,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "2018."
        },
        {
          "12": "[119]",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "W. Wu, Y. Zhang, C. Li, C. Qian, and C. C. Loy,"
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": ""
        },
        {
          "12": "",
          "IEEE TRANSACTIONS ON JOURNAL NAME,  MANUSCRIPT ID": "“Reenactgan: Learning to reenact faces via boundary"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR ET AL.:  TITLE": "transfer,” in Proceedings of the European conference on",
          "13": "International Conference on Multimedia and Expo (ICME),"
        },
        {
          "AUTHOR ET AL.:  TITLE": "computer vision (ECCV), 2018, pp. 603–619.",
          "13": "2015, pp. 1–6."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[139] \nL. Ma and Z. Deng, “Real‐Time Facial Expression",
          "13": "J. Xu, J. Broekens, K. Hindriks, and M. A. Neerincx, “Effects"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Transformation for Monocular RGB Video,” in Computer",
          "13": "of a robotic storyteller’s moody gestures on storytelling"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Graphics Forum, 2019, vol. 38, no. 1, pp. 470–481.",
          "13": "perception,” in 2015 International Conference on Affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[140] \nN. Otberdout, M. Daoudi, A. Kacem, L. Ballihi, and S.",
          "13": "Computing and Intelligent Interaction (ACII), 2015, pp. 449–"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Berretti, “Dynamic facial expression generation on hilbert",
          "13": "455."
        },
        {
          "AUTHOR ET AL.:  TITLE": "hypersphere with conditional wasserstein generative",
          "13": "S. J. Burton, A.-A. Samadani, R. Gorbet, and D. Kulić,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "adversarial nets,” IEEE Trans. Pattern Anal. Mach. Intell.,",
          "13": "“Laban movement analysis and affective movement"
        },
        {
          "AUTHOR ET AL.:  TITLE": "2020.",
          "13": "generation for robots and other near-living creatures,” in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[141] \nW. Wang, X. Alameda-Pineda, D. Xu, P. Fua, E. Ricci, and",
          "13": "Dance notations and robot motion, Springer, 2016, pp. 25–48."
        },
        {
          "AUTHOR ET AL.:  TITLE": "N. Sebe, “Every smile is unique: Landmark-guided diverse",
          "13": "J. Newlove, Laban for actors and dancers: putting Laban’s"
        },
        {
          "AUTHOR ET AL.:  TITLE": "smile generation,” in Proceedings of the IEEE Conference on",
          "13": "movement theory into practice: a step-by-step guide. Theatre"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Computer Vision and Pattern Recognition, 2018, pp. 7083–",
          "13": "Arts Books, 1993."
        },
        {
          "AUTHOR ET AL.:  TITLE": "7092.",
          "13": "P. Carreno-Medrano, S. Gibet, and P.-F. Marteau, “End-"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[142] \nR. A. Potamias, J. Zheng, S. Ploumpis, G. Bouritsas, E.",
          "13": "effectors trajectories: An efficient low-dimensional"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Ververas, and S. Zafeiriou, “Learning to Generate",
          "13": "characterization of affective-expressive body motions,” in"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Customized Dynamic 3D Facial Expressions,” in European",
          "13": "2015 International Conference on Affective Computing and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Conference on Computer Vision, 2020, pp. 278–294.",
          "13": "Intelligent Interaction (ACII), 2015, pp. 435–441."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[143] \nN. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Del",
          "13": "P. Carreno-Medrano, S. Gibet, and P.-F. Marteau, “From"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Bimbo, “3D to 4D Facial Expressions Generation Guided by",
          "13": "expressive end-effector trajectories to expressive bodily"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Landmarks,” arXiv Prepr. arXiv2105.07463, 2021.",
          "13": "motions,” in Proceedings of the 29th International Conference"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[144] \nK. Ali and C. E. Hughes, “All-in-one: Facial expression",
          "13": "on Computer Animation and Social Agents, 2016, pp. 157–163."
        },
        {
          "AUTHOR ET AL.:  TITLE": "transfer, editing and recognition using a single network,”",
          "13": "S. Xia, C. Wang, J. Chai, and J. Hodgins, “Realtime style"
        },
        {
          "AUTHOR ET AL.:  TITLE": "arXiv Prepr. arXiv1911.07050, 2019.",
          "13": "transfer for unlabeled heterogeneous human motion,” ACM"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[145] \nD. Zeng, H. Liu, H. Lin, and S. Ge, “Talking Face",
          "13": "Trans. Graph., vol. 34, no. 4, pp. 1–10, 2015."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Generation with Expression-Tailored Generative",
          "13": "P. Carreno-Medrano, S. Gibet, and P.-F. Marteau,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Adversarial Network,” in Proceedings of the 28th ACM",
          "13": "“Perceptual validation for the generation of expressive"
        },
        {
          "AUTHOR ET AL.:  TITLE": "International Conference on Multimedia, 2020, pp. 1716–1724.",
          "13": "movements from end-effector trajectories,” ACM Trans."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[146] \nD. Benson, M. M. Khan, T. Tan, and T. Hargreaves,",
          "13": "Interact. Intell. Syst., vol. 8, no. 3, pp. 1–26, 2018."
        },
        {
          "AUTHOR ET AL.:  TITLE": "“Modeling and verification of facial expression display",
          "13": "D. Holden, J. Saito, and T. Komura, “A deep learning"
        },
        {
          "AUTHOR ET AL.:  TITLE": "mechanism for developing a sociable robot face,” in 2016",
          "13": "framework for character motion synthesis and editing,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "International Conference on Advanced Robotics and",
          "13": "ACM Trans. Graph., vol. 35, no. 4, pp. 1–11, 2016."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Mechatronics (ICARM), 2016, pp. 76–81.",
          "13": "D. Holden, T. Komura, and J. Saito, “Phase-functioned"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[147] \nT. Hargreaves, M. M. Khan, D. Benson, and T. Tan,",
          "13": "neural networks for character control,” ACM Trans. Graph.,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "“Closed-loop Petri Net model for implementing an",
          "13": "vol. 36, no. 4, pp. 1–13, 2017."
        },
        {
          "AUTHOR ET AL.:  TITLE": "affective-state expressive robotic face,” in 2016 IEEE",
          "13": "G. W. Taylor and G. E. Hinton, “Factored conditional"
        },
        {
          "AUTHOR ET AL.:  TITLE": "International Conference on Advanced Intelligent Mechatronics",
          "13": "restricted Boltzmann machines for modeling motion style,”"
        },
        {
          "AUTHOR ET AL.:  TITLE": "(AIM), 2016, pp. 463–467.",
          "13": "in Proceedings of the 26th annual international conference on"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[148] \nZ. Huang, F. Ren, and Y. Bao, “Human-like facial",
          "13": "machine learning, 2009, pp. 1025–1032."
        },
        {
          "AUTHOR ET AL.:  TITLE": "expression imitation for humanoid robot based on",
          "13": "H. Kozima, M. P. Michalowski, and C. Nakagawa,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "recurrent neural network,” in 2016 International Conference",
          "13": "“Keepon,” Int. J. Soc. Robot., vol. 1, no. 1, pp. 3–18, 2009."
        },
        {
          "AUTHOR ET AL.:  TITLE": "on Advanced Robotics and Mechatronics (ICARM), 2016, pp.",
          "13": "H. Knight and R. Simmons, “Laban head-motions convey"
        },
        {
          "AUTHOR ET AL.:  TITLE": "306–311.",
          "13": "robot state: A call for robot body language,” in 2016 IEEE"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[149] \nR. Sadiq and E. Erzin, “Emotion Dependent Domain",
          "13": "international conference on robotics and automation (ICRA),"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Adaptation for Speech Driven Affective Facial Feature",
          "13": "2016, pp. 2881–2888."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Synthesis,” IEEE Trans. Affect. Comput., 2020.",
          "13": "R. Peters, J. Broekens, K. Li, and M. A. Neerincx, “Robots"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[150] \nT. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen,",
          "13": "expressing dominance: Effects of behaviours and"
        },
        {
          "AUTHOR ET AL.:  TITLE": "“Audio-driven facial animation by joint end-to-end",
          "13": "modulation,” in 2019 8th International Conference on Affective"
        },
        {
          "AUTHOR ET AL.:  TITLE": "learning of pose and emotion,” ACM Trans. Graph., vol. 36,",
          "13": "Computing and Intelligent Interaction (ACII), 2019, pp. 1–7."
        },
        {
          "AUTHOR ET AL.:  TITLE": "no. 4, pp. 1–12, 2017.",
          "13": "T. Tsujimoto, Y. Takahashi, S. Takeuchi, and Y. Maeda,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[151] \nH. X. Pham, S. Cheung, and V. Pavlovic, “Speech-driven 3D",
          "13": "“RNN with Russell’s circumplex model for emotion"
        },
        {
          "AUTHOR ET AL.:  TITLE": "facial animation with implicit emotional awareness: a deep",
          "13": "estimation and emotional gesture generation,” in 2016 IEEE"
        },
        {
          "AUTHOR ET AL.:  TITLE": "learning approach,” in Proceedings of the IEEE Conference on",
          "13": "Congress on Evolutionary Computation (CEC), 2016, pp. 1427–"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Computer Vision and Pattern Recognition Workshops, 2017, pp.",
          "13": "1431."
        },
        {
          "AUTHOR ET AL.:  TITLE": "80–88.",
          "13": "M. Suguitan, M. Bretan, and G. Hoffman, “Affective robot"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[152] \nS. Asadiabadi, R. Sadiq, and E. Erzin, “Multimodal speech",
          "13": "movement generation using cyclegans,” in 2019 14th"
        },
        {
          "AUTHOR ET AL.:  TITLE": "driven facial shape animation using deep neural networks,”",
          "13": "ACM/IEEE International Conference on Human-Robot"
        },
        {
          "AUTHOR ET AL.:  TITLE": "in 2018 Asia-Pacific Signal and Information Processing",
          "13": "Interaction (HRI), 2019, pp. 534–535."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Association Annual Summit and Conference (APSIPA ASC),",
          "13": "J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired"
        },
        {
          "AUTHOR ET AL.:  TITLE": "2018, pp. 1508–1512.",
          "13": "image-to-image translation using cycle-consistent"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[153] \nO. Alemi, W. Li, and P. Pasquier, “Affect-expressive",
          "13": "adversarial networks,” in Proceedings of the IEEE"
        },
        {
          "AUTHOR ET AL.:  TITLE": "movement generation with factored conditional restricted",
          "13": "international conference on computer vision, 2017, pp. 2223–"
        },
        {
          "AUTHOR ET AL.:  TITLE": "boltzmann machines,” in 2015 International Conference on",
          "13": "2232."
        },
        {
          "AUTHOR ET AL.:  TITLE": "Affective Computing and Intelligent Interaction (ACII), 2015,",
          "13": "J. R. Cauchard, K. Y. Zhai, M. Spadafora, and J. A. Landay,"
        },
        {
          "AUTHOR ET AL.:  TITLE": "pp. 442–448.",
          "13": "“Emotion encoding in human-drone interaction,” in 2016"
        },
        {
          "AUTHOR ET AL.:  TITLE": "[154] \nJ. Bates, “The role of emotion in believable agents,”",
          "13": "11th ACM/IEEE International Conference on Human-Robot"
        },
        {
          "AUTHOR ET AL.:  TITLE": "Commun. ACM, vol. 37, no. 7, pp. 122–125, 1994.",
          "13": "Interaction (HRI), 2016, pp. 263–270."
        },
        {
          "AUTHOR ET AL.:  TITLE": "[155] \nE. Bozkurt, E. Erzin, and Y. Yemez, “Affect-expressive hand",
          "13": "J. Jørgensen, “Leveraging morphological computation for"
        },
        {
          "AUTHOR ET AL.:  TITLE": "gestures synthesis and animation,” in 2015 IEEE",
          "13": "expressive movement generation in a soft robotic artwork,”"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[174]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[175]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[176]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[177]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[178]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[179]"
        },
        {
          "14": ""
        },
        {
          "14": "[180]"
        },
        {
          "14": ""
        },
        {
          "14": "[181]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[182]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[183]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[184]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[185]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[186]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[187]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": "[188]"
        },
        {
          "14": ""
        },
        {
          "14": ""
        },
        {
          "14": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "year": "2015",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "3",
      "title": "What is an Emotion? Simon and Schuster",
      "authors": [
        "W James"
      ],
      "year": "2013",
      "venue": "What is an Emotion? Simon and Schuster"
    },
    {
      "citation_id": "4",
      "title": "What is an emotion?",
      "authors": [
        "R Plutchik"
      ],
      "year": "1965",
      "venue": "J. Psychol"
    },
    {
      "citation_id": "5",
      "title": "Damasion, Descartes' error: emotion, reason and the human brain",
      "year": "1994",
      "venue": "Damasion, Descartes' error: emotion, reason and the human brain"
    },
    {
      "citation_id": "6",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "MIT Media Laboratory Perceptual Computing Section Technical Report"
    },
    {
      "citation_id": "8",
      "title": "Studying the emotion-antecedent appraisal process: An expert system approach",
      "authors": [
        "K Scherer"
      ],
      "year": "1993",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "9",
      "title": "Toward a taxonomy of affective computing",
      "authors": [
        "J Schwark"
      ],
      "year": "2015",
      "venue": "Int. J. Hum. Comput. Interact"
    },
    {
      "citation_id": "10",
      "title": "Affective Computing: A Review. Affective Computing and Intelligent Interaction",
      "authors": [
        "J Tao",
        "T Tieniu"
      ],
      "year": "2005",
      "venue": "Affective Computing: A Review. Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "11",
      "title": "Introduction to special Issue on 'Affective modeling and adaptation",
      "authors": [
        "S Carberry",
        "F De Rosis"
      ],
      "year": "2008",
      "venue": "User Model. Useradapt. Interact"
    },
    {
      "citation_id": "12",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "13",
      "title": "A survey of multidisciplinary domains contributing to affective computing",
      "authors": [
        "R Arya",
        "J Singh",
        "A Kumar"
      ],
      "year": "2021",
      "venue": "Comput. Sci. Rev"
    },
    {
      "citation_id": "14",
      "title": "Body movements for affective expression: A survey of automatic recognition and generation",
      "authors": [
        "M Karg",
        "A.-A Samadani",
        "R Gorbet",
        "K Kühnlenz",
        "J Hoey",
        "D Kulić"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "15",
      "title": "Robot expressive motions: a survey of generation and evaluation methods",
      "authors": [
        "G Venture",
        "D Kulić"
      ],
      "year": "2019",
      "venue": "ACM Trans. Human-Robot Interact"
    },
    {
      "citation_id": "16",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Bin Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Comput. Educ"
    },
    {
      "citation_id": "17",
      "title": "Review of affective computing in education/learning: Trends and challenges",
      "authors": [
        "C Wu",
        "Y Huang",
        "J Hwang"
      ],
      "year": "2016",
      "venue": "Br. J. Educ. Technol"
    },
    {
      "citation_id": "18",
      "title": "Affective computing and medical informatics: state of the art in emotion-aware medical applications",
      "authors": [
        "A Luneski",
        "P Bamidis",
        "M Hitoglou-Antoniadou"
      ],
      "year": "2008",
      "venue": "Stud. Health Technol. Inform"
    },
    {
      "citation_id": "19",
      "title": "Affective medicine: a review of affective computing efforts in medical informatics",
      "authors": [
        "A Luneski",
        "E Konstantinidis",
        "P Bamidis"
      ],
      "year": "2010",
      "venue": "Methods Inf. Med"
    },
    {
      "citation_id": "20",
      "title": "Affective computing in the modern workplace",
      "authors": [
        "S Richardson"
      ],
      "year": "2020",
      "venue": "Bus. Inf. Rev"
    },
    {
      "citation_id": "21",
      "title": "Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: a review",
      "authors": [
        "C Filippini",
        "D Perpetuini",
        "D Cardone",
        "A Chiarelli",
        "A Merla"
      ],
      "year": "2020",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "22",
      "title": "Adaptation in affective video games: A literature review",
      "authors": [
        "B Bontchev"
      ],
      "year": "2016",
      "venue": "Cybern. Inf. Technol"
    },
    {
      "citation_id": "23",
      "title": "A survey on mobile affective computing",
      "authors": [
        "E Politou",
        "E Alepis",
        "C Patsakis"
      ],
      "year": "2017",
      "venue": "Comput. Sci. Rev"
    },
    {
      "citation_id": "24",
      "title": "A study of the state of the art of Affective Computing in Ambient Intelligence environments",
      "authors": [
        "I Cearreta",
        "J López",
        "K De Ipiña",
        "N Garay",
        "C Hernández",
        "M Graña"
      ],
      "year": "2007",
      "venue": "Interacció n 2007 (VIII Congreso de Interacció n Persona-Ordenador"
    },
    {
      "citation_id": "25",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Marí N-Morales"
      ],
      "year": "2018",
      "venue": "Sci. Rep"
    },
    {
      "citation_id": "26",
      "title": "Affective computing for large-scale heterogeneous multimedia data: A survey",
      "authors": [
        "S Zhao",
        "S Wang",
        "M Soleymani",
        "D Joshi",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "ACM Trans. Multimed. Comput. Commun. Appl"
    },
    {
      "citation_id": "27",
      "title": "A review on the computational methods for emotional state estimation from the human EEG",
      "authors": [
        "M.-K Kim",
        "M Kim",
        "E Oh",
        "S.-P Kim"
      ],
      "year": "2013",
      "venue": "Comput. Math. Methods Med"
    },
    {
      "citation_id": "28",
      "title": "A Literature Review of EEG-Based Affective Computing in Marketing",
      "authors": [
        "G Pei",
        "T Li"
      ],
      "year": "2021",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "29",
      "title": "Ten challenges for EEG-based affective computing",
      "authors": [
        "X Hu",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2019",
      "venue": "Brain Sci. Adv"
    },
    {
      "citation_id": "30",
      "title": "A review on the role of color and light in affective computing",
      "authors": [
        "M Sokolova",
        "A Fernández-Caballero"
      ],
      "year": "2015",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "31",
      "title": "Physiological and affective computing through thermal imaging: A survey",
      "authors": [
        "Y Cho",
        "N Bianchi-Berthouze"
      ],
      "year": "2019",
      "venue": "Physiological and affective computing through thermal imaging: A survey"
    },
    {
      "citation_id": "32",
      "title": "Affective computing and the impact of gender and age",
      "authors": [
        "S Rukavina",
        "S Gruss",
        "H Hoffmann",
        "J.-W Tan",
        "S Walter",
        "H Traue"
      ],
      "year": "2016",
      "venue": "PLoS One"
    },
    {
      "citation_id": "33",
      "title": "Stimulus-response compatibility and affective computing: A review",
      "authors": [
        "P Lemmens",
        "A De Haan",
        "G Van Galen",
        "R Meulenbroek"
      ],
      "year": "2007",
      "venue": "Theor. Issues Ergon. Sci"
    },
    {
      "citation_id": "34",
      "title": "Effects of dynamic aspects of facial expressions: A review",
      "authors": [
        "E Krumhuber",
        "A Kappas",
        "A Manstead"
      ],
      "year": "2013",
      "venue": "Emot. Rev"
    },
    {
      "citation_id": "35",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "36",
      "title": "Experts' definitions of emotion and their ratings of its components and characteristics",
      "authors": [
        "C Izard"
      ],
      "year": "2006",
      "venue": "Unpubl. manuscript, Univ. Delaware"
    },
    {
      "citation_id": "37",
      "title": "Damasio's error?",
      "authors": [
        "J Panksepp"
      ],
      "year": "2003",
      "venue": "Conscious. Emot"
    },
    {
      "citation_id": "38",
      "title": "Valence-arousal evaluation using physiological signals in an emotion recall paradigm",
      "authors": [
        "G Chanel",
        "K Ansari-Asl",
        "T Pun"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "39",
      "title": "From physiological signals to emotions: Implementing and comparing selected methods for feature extraction and classification",
      "authors": [
        "J Wagner",
        "J Kim",
        "E André"
      ],
      "year": "2005",
      "venue": "2005 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "40",
      "title": "The origins of psychoanalysis",
      "authors": [
        "S Freud",
        "P Bonaparte"
      ],
      "year": "1954",
      "venue": "The origins of psychoanalysis"
    },
    {
      "citation_id": "41",
      "title": "Affective state estimation for human-robot interaction",
      "authors": [
        "D Kulic",
        "E Croft"
      ],
      "year": "2007",
      "venue": "IEEE Trans. Robot"
    },
    {
      "citation_id": "42",
      "title": "Towards a cognitive theory of emotions",
      "authors": [
        "K Oatley",
        "P Johnson-Laird"
      ],
      "year": "1987",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "43",
      "title": "Nonverbal behavior and the theory of emotion: the facial feedback hypothesis",
      "authors": [
        "R Buck"
      ],
      "year": "1980",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "44",
      "title": "The feeling of what happens: Body and emotion in the making of consciousness",
      "authors": [
        "A Damasio"
      ],
      "year": "1999",
      "venue": "The feeling of what happens: Body and emotion in the making of consciousness"
    },
    {
      "citation_id": "45",
      "title": "Emotion theory and research: Highlights, unanswered questions, and emerging issues",
      "authors": [
        "C Izard"
      ],
      "year": "2009",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "46",
      "title": "Mind: An essay on human feeling",
      "authors": [
        "S Langer"
      ],
      "year": "1967",
      "venue": "Mind: An essay on human feeling"
    },
    {
      "citation_id": "47",
      "title": "The Oxford handbook of affective computing",
      "authors": [
        "R Calvo",
        "S D'mello",
        "J Gratch",
        "A Kappas"
      ],
      "year": "2015",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "48",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "T Dalgleish",
        "M Power"
      ],
      "year": "2000",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "49",
      "title": "What are emotions? And how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Soc. Sci. Inf"
    },
    {
      "citation_id": "50",
      "title": "Affective computing: challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "Int. J. Hum. Comput. Stud"
    },
    {
      "citation_id": "51",
      "title": "The structure of emotion: An empirical comparison of six models",
      "authors": [
        "M Power"
      ],
      "year": "2006",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "52",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "2003",
      "venue": "Psychol. Rev"
    },
    {
      "citation_id": "53",
      "title": "Affects separable and inseparable: on the hierarchical arrangement of the negative affects",
      "authors": [
        "D Watson",
        "L Clark"
      ],
      "year": "1992",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "54",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Curr. Psychol"
    },
    {
      "citation_id": "55",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "J. Behav. Ther. Exp. Psychiatry"
    },
    {
      "citation_id": "56",
      "title": "FEELTRACE': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "FEELTRACE': An instrument for recording perceived emotion in real time"
    },
    {
      "citation_id": "57",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Dev. Psychopathol"
    },
    {
      "citation_id": "58",
      "title": "Remembering pictures: pleasure and arousal in memory",
      "authors": [
        "M Bradley",
        "M Greenwald",
        "M Petry",
        "P Lang"
      ],
      "year": "1992",
      "venue": "J. Exp. Psychol. Learn. Mem. Cogn"
    },
    {
      "citation_id": "59",
      "title": "Development and validation of brief measures of positive and negative affect: the PANAS scales",
      "authors": [
        "D Watson",
        "L Clark",
        "A Tellegen"
      ],
      "year": "1988",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "60",
      "title": "Activation-Deactivation Adjective Check List: Current Overview and Structural Analysis",
      "authors": [
        "R Thayer"
      ],
      "year": "1986",
      "venue": "Psychol. Rep",
      "doi": "10.2466/pr0.1986.58.2.607"
    },
    {
      "citation_id": "61",
      "title": "Human emotions",
      "authors": [
        "C Izard"
      ],
      "year": "2013",
      "venue": "Human emotions"
    },
    {
      "citation_id": "62",
      "title": "Basic emotions, natural kinds, emotion schemas, and a new paradigm",
      "authors": [
        "C Izard"
      ],
      "year": "2007",
      "venue": "Perspect. Psychol. Sci"
    },
    {
      "citation_id": "63",
      "title": "Levels of emotion and levels of consciousness",
      "authors": [
        "C Izard"
      ],
      "year": "2007",
      "venue": "Behav. Brain Sci"
    },
    {
      "citation_id": "64",
      "title": "Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies",
      "authors": [
        "J Russell"
      ],
      "year": "1994",
      "venue": "Psychol. Bull"
    },
    {
      "citation_id": "65",
      "title": "Universals and cultural differences in the judgments of facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1987",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "66",
      "title": "Universal facial expressions of emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1970",
      "venue": "Calif. Ment. Heal. Res. Dig"
    },
    {
      "citation_id": "67",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "68",
      "title": "The face of emotion",
      "authors": [
        "C Izard"
      ],
      "year": "1971",
      "venue": "The face of emotion"
    },
    {
      "citation_id": "69",
      "title": "EdITS Manual for the Profile of Mood States (POMS). Educational and industrial testing service",
      "authors": [
        "D Mcnair",
        "M Lorr",
        "L Droppleman"
      ],
      "year": "1992",
      "venue": "EdITS Manual for the Profile of Mood States (POMS). Educational and industrial testing service"
    },
    {
      "citation_id": "70",
      "title": "Patterns of cognitive appraisal in emotion",
      "authors": [
        "C Smith",
        "P Ellsworth"
      ],
      "year": "1985",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "71",
      "title": "The personality structure of affect",
      "authors": [
        "E Diener",
        "H Smith",
        "F Fujita"
      ],
      "year": "1995",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "72",
      "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
      "authors": [
        "A Gatt",
        "E Krahmer"
      ],
      "year": "2018",
      "venue": "J. Artif. Intell. Res"
    },
    {
      "citation_id": "73",
      "title": "Building applied natural language generation systems",
      "authors": [
        "E Reiter",
        "R Dale"
      ],
      "year": "1997",
      "venue": "Nat. Lang. Eng"
    },
    {
      "citation_id": "74",
      "title": "Affective neural response generation",
      "authors": [
        "N Asghar",
        "P Poupart",
        "J Hoey",
        "X Jiang",
        "L Mou"
      ],
      "year": "2018",
      "venue": "European Conference on Information Retrieval"
    },
    {
      "citation_id": "75",
      "title": "Norms of valence, arousal, and dominance for 13,915 English lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behav. Res. Methods"
    },
    {
      "citation_id": "76",
      "title": "Modelling Context Emotions using Multi-task Learning for Emotion Controlled Dialog Generation",
      "authors": [
        "D Varshney",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter"
    },
    {
      "citation_id": "77",
      "title": "Mojitalk: Generating emotional responses at scale",
      "authors": [
        "X Zhou",
        "W Wang"
      ],
      "year": "2017",
      "venue": "Mojitalk: Generating emotional responses at scale"
    },
    {
      "citation_id": "78",
      "title": "An affect-rich neural conversational model with biased attention and weighted cross-entropy loss",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "79",
      "title": "Using human physiology to evaluate subtle expressivity of a virtual quizmaster in a mathematical game",
      "authors": [
        "H Prendinger",
        "J Mori",
        "M Ishizuka"
      ],
      "year": "2005",
      "venue": "Int. J. Hum. Comput. Stud"
    },
    {
      "citation_id": "80",
      "title": "THE EMPATHIC COMPANION: A CHARACTER-BASED INTERFACE THAT ADDRESSES USERS'AFFECTIVE STATES",
      "authors": [
        "H Prendinger",
        "M Ishizuka"
      ],
      "year": "2005",
      "venue": "Appl. Artif. Intell"
    },
    {
      "citation_id": "81",
      "title": "Breakdown in humanmachine interaction: the error is the clue",
      "authors": [
        "B Martinovski",
        "D Traum"
      ],
      "year": "2003",
      "venue": "Proceedings of the ISCA tutorial and research workshop on Error handling in dialogue systems"
    },
    {
      "citation_id": "82",
      "title": "An exploratory study into the use of an emotionally aware cognitive assistant",
      "authors": [
        "A Malhotra",
        "L Yu",
        "T Schröder",
        "J Hoey"
      ],
      "year": "2015",
      "venue": "An exploratory study into the use of an emotionally aware cognitive assistant"
    },
    {
      "citation_id": "83",
      "title": "Emotion recognition and adaptation in spoken dialogue systems",
      "authors": [
        "J Pittermann",
        "A Pittermann",
        "W Minker"
      ],
      "year": "2010",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "84",
      "title": "Predicting user mental states in spoken dialogue systems",
      "authors": [
        "Z Callejas",
        "D Griol",
        "R López-Cózar"
      ],
      "year": "2011",
      "venue": "EURASIP J. Adv. Signal Process"
    },
    {
      "citation_id": "85",
      "title": "Affect-lm: A neural language model for customizable affective text generation",
      "authors": [
        "S Ghosh",
        "M Chollet",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "Affect-lm: A neural language model for customizable affective text generation"
    },
    {
      "citation_id": "86",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "87",
      "title": "Seq2Emo: A Sequence to Multi-Label Emotion Classification Model",
      "authors": [
        "C Huang",
        "A Trabelsi",
        "X Qin",
        "N Farruque",
        "L Mou",
        "O Zaiane"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "88",
      "title": "Generating Empathetic Responses by Injecting Anticipated Emotion",
      "authors": [
        "Y Liu",
        "J Du",
        "X Li",
        "R Xu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "89",
      "title": "Emotion-Regularized Conditional Variational Autoencoder for Emotional Response Generation",
      "authors": [
        "Y.-P Ruan",
        "Z Ling"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "90",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "91",
      "title": "Generating responses with a specific emotion in dialog",
      "authors": [
        "Z Song",
        "X Zheng",
        "L Liu",
        "M Xu",
        "X.-J Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "92",
      "title": "SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks",
      "authors": [
        "K Wang",
        "X Wan"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "93",
      "title": "Toward controlled generation of text",
      "authors": [
        "Z Hu",
        "Z Yang",
        "X Liang",
        "R Salakhutdinov",
        "E Xing"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "94",
      "title": "Automatic dialogue generation with expressed emotions",
      "authors": [
        "C Huang",
        "O Zaiane",
        "A Trabelsi",
        "N Dziri"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "95",
      "title": "Content preserving text generation with attribute controls",
      "authors": [
        "L Logeswaran",
        "H Lee",
        "S Bengio"
      ],
      "year": "2018",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "96",
      "title": "Inducing an ironic effect in automated tweets",
      "authors": [
        "A Valitutti",
        "T Veale"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2015.7344565"
    },
    {
      "citation_id": "97",
      "title": "Some suggestions for the study of stance in communication",
      "authors": [
        "M Chindamo",
        "J Allwood",
        "E Ahlsén"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing"
    },
    {
      "citation_id": "98",
      "title": "Image captioning by incorporating affective concepts learned from both visual and textual components",
      "authors": [
        "J Yang",
        "Y Sun",
        "J Liang",
        "B Ren",
        "S.-H Lai"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "99",
      "title": "Image captioning with affective guiding and selective attention",
      "authors": [
        "A Wang",
        "H Hu",
        "L Yang"
      ],
      "year": "2018",
      "venue": "ACM Trans. Multimed. Comput. Commun. Appl"
    },
    {
      "citation_id": "100",
      "title": "Can we Generate Emotional Pronunciations for Expressive Speech Synthesis?",
      "authors": [
        "M Tahon",
        "G Lecorvé",
        "D Lolive"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "101",
      "title": "Detecting affective states from text based on a multi-component emotion model",
      "authors": [
        "Y Gao",
        "W Zhu"
      ],
      "year": "2016",
      "venue": "Comput. Speech Lang"
    },
    {
      "citation_id": "102",
      "title": "Emotional statistical parametric speech synthesis using LSTM-RNNs",
      "authors": [
        "S An",
        "Z Ling",
        "L Dai"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "103",
      "title": "Emotion transplantation through adaptation in HMM-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "R Barra-Chicote",
        "R San-Segundo",
        "J Ferreiros",
        "J Yamagishi",
        "J Montero"
      ],
      "year": "2015",
      "venue": "Comput. Speech Lang"
    },
    {
      "citation_id": "104",
      "title": "Multi-speaker emotional acoustic modeling for cnn-based speech synthesis",
      "authors": [
        "H Choi",
        "S Park",
        "J Park",
        "M Hahn"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "105",
      "title": "Visual Speech Emotion Conversion using Deep Learning for 3D Talking Head",
      "authors": [
        "D.-Y Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "106",
      "title": "Accurate Synchronization of Gesture and Speech for Conversational Agents using Motion Graphs",
      "authors": [
        "J Xu",
        "Y Nagai",
        "S Takayama",
        "S Sakazawa"
      ],
      "year": "2014",
      "venue": "Accurate Synchronization of Gesture and Speech for Conversational Agents using Motion Graphs"
    },
    {
      "citation_id": "107",
      "title": "Developing Embodied Agents for Education Applications with Accurate Synchronization of Gesture and Speech",
      "authors": [
        "J Xu",
        "Y Nagai",
        "S Takayama",
        "S Sakazawa"
      ],
      "year": "2015",
      "venue": "Transactions on Computational Collective Intelligence XX"
    },
    {
      "citation_id": "108",
      "title": "Emotional transplant in statistical speech synthesis based on emotion additive model",
      "authors": [
        "Y Ohtani",
        "Y Nasu",
        "M Morita",
        "M Akamine"
      ],
      "year": "2015",
      "venue": "Emotional transplant in statistical speech synthesis based on emotion additive model"
    },
    {
      "citation_id": "109",
      "title": "A comparison of expressive speech synthesis approaches based on neural network",
      "authors": [
        "L Xue",
        "X Zhu",
        "X An",
        "L Xie"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "110",
      "title": "A Kullback-Leibler Divergence Based Recurrent Mixture Density Network for Acoustic Modeling in Emotional Statistical Parametric Speech Synthesis",
      "authors": [
        "X An",
        "Y Zhang",
        "B Liu",
        "L Xue",
        "L Xie"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "111",
      "title": "An investigation to transplant emotional expressions in DNNbased TTS synthesis",
      "authors": [
        "K Inoue",
        "S Hara",
        "M Abe",
        "N Hojo",
        "Y Ijima"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "112",
      "title": "Audio generation from scene considering its emotion aspect",
      "authors": [
        "G Sergio",
        "M Lee"
      ],
      "year": "2016",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "113",
      "title": "Emotional video to audio transformation using deep recurrent neural networks and a neuro-fuzzy system",
      "authors": [
        "G Sergio",
        "M Lee"
      ],
      "year": "2020",
      "venue": "Math. Probl. Eng"
    },
    {
      "citation_id": "114",
      "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
      "authors": [
        "G Sergio",
        "M Lee"
      ],
      "year": "2021",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "115",
      "title": "Affective evolutionary music composition with MetaCompose",
      "authors": [
        "M Scirea",
        "J Togelius",
        "P Eklund",
        "S Risi"
      ],
      "year": "2017",
      "venue": "Genet. Program. Evolvable Mach"
    },
    {
      "citation_id": "116",
      "title": "Collaborating with an autonomous agent to generate affective music",
      "authors": [
        "F Morreale",
        "A Angeli"
      ],
      "year": "2016",
      "venue": "Comput. Entertain"
    },
    {
      "citation_id": "117",
      "title": "Listen to your Mind's (He) Art: A System for Affective Music Generation via Brain-Computer Interface",
      "authors": [
        "M Tiraboschi",
        "F Avanzini",
        "G Boccignone"
      ],
      "year": "2021",
      "venue": "Sound and Music Computing Conference"
    },
    {
      "citation_id": "118",
      "title": "MorpheuS: generating structured music with constrained patterns and tension",
      "authors": [
        "D Herremans",
        "E Chew"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "119",
      "title": "Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder",
      "authors": [
        "Y Zhou",
        "B Shi"
      ],
      "year": "2017",
      "venue": "2017 seventh international conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "120",
      "title": "Geometry-driven photorealistic facial expression synthesis",
      "authors": [
        "Q Zhang",
        "Z Liu",
        "G Quo",
        "D Terzopoulos",
        "H.-Y Shum"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Vis. Comput. Graph"
    },
    {
      "citation_id": "121",
      "title": "A novel transient wrinkle detection algorithm and its application for expression synthesis",
      "authors": [
        "W Xie",
        "L Shen",
        "J Jiang"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "122",
      "title": "Real-time expression transfer for facial reenactment",
      "authors": [
        "J Thies",
        "M Zollhöfer",
        "M Nießner",
        "L Valgaerts",
        "M Stamminger",
        "C Theobalt"
      ],
      "year": "2015",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "123",
      "title": "A framework for generic facial expression transfer",
      "authors": [
        "R Queiroz",
        "A Braun",
        "S Musse"
      ],
      "year": "2017",
      "venue": "Entertain. Comput"
    },
    {
      "citation_id": "124",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Deep neural network augmentation: Generating faces for affect analysis"
    },
    {
      "citation_id": "125",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2014",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "126",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets"
    },
    {
      "citation_id": "127",
      "title": "Learning structured output representation using deep conditional generative models",
      "authors": [
        "K Sohn",
        "H Lee",
        "X Yan"
      ],
      "year": "2015",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "128",
      "title": "Learning to generate 3D stylized character expressions from humans",
      "authors": [
        "D Aneja",
        "B Chaudhuri",
        "A Colburn",
        "G Faigin",
        "L Shapiro",
        "B Mones"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "129",
      "title": "Exprgan: Facial expression editing with controllable expression intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "130",
      "title": "EGGAN: Learning Latent Space for Fine-Grained Expression Manipulation",
      "authors": [
        "J Tang",
        "Z Shao",
        "L Ma"
      ],
      "year": "2021",
      "venue": "EGGAN: Learning Latent Space for Fine-Grained Expression Manipulation"
    },
    {
      "citation_id": "131",
      "title": "Generative adversarial talking head: Bringing portraits to life with a weakly supervised neural network",
      "authors": [
        "H Pham",
        "Y Wang",
        "V Pavlovic"
      ],
      "year": "2018",
      "venue": "Generative adversarial talking head: Bringing portraits to life with a weakly supervised neural network"
    },
    {
      "citation_id": "132",
      "title": "Ganimation: Anatomically-aware facial animation from a single image",
      "authors": [
        "A Pumarola",
        "A Agudo",
        "A Martinez",
        "A Sanfeliu",
        "F Moreno-Noguer"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "133",
      "title": "Geometry guided adversarial facial expression synthesis",
      "authors": [
        "L Song",
        "Z Lu",
        "R He",
        "Z Sun",
        "T Tan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "134",
      "title": "Geometry-contrastive gan for facial expression transfer",
      "authors": [
        "F Qiao",
        "N Yao",
        "Z Jiao",
        "Z Li",
        "H Chen",
        "H Wang"
      ],
      "year": "2018",
      "venue": "Geometry-contrastive gan for facial expression transfer"
    },
    {
      "citation_id": "135",
      "title": "Semantic facial expression editing using autoencoded flow",
      "authors": [
        "R Yeh",
        "Z Liu",
        "D Goldman",
        "A Agarwala"
      ],
      "year": "2016",
      "venue": "Semantic facial expression editing using autoencoded flow"
    },
    {
      "citation_id": "136",
      "title": "Face2face: Real-time face capture and reenactment of rgb videos",
      "authors": [
        "J Thies",
        "M Zollhofer",
        "M Stamminger",
        "C Theobalt",
        "M Nießner"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "137",
      "title": "Headon: Real-time reenactment of human portrait videos",
      "authors": [
        "J Thies",
        "M Zollhöfer",
        "C Theobalt",
        "M Stamminger",
        "M Nießner"
      ],
      "year": "2018",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "138",
      "title": "Reenactgan: Learning to reenact faces via boundary transfer",
      "authors": [
        "W Wu",
        "Y Zhang",
        "C Li",
        "C Qian",
        "C Loy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "139",
      "title": "Real-Time Facial Expression Transformation for Monocular RGB Video",
      "authors": [
        "L Ma",
        "Z Deng"
      ],
      "year": "2019",
      "venue": "Computer Graphics Forum"
    },
    {
      "citation_id": "140",
      "title": "Dynamic facial expression generation on hilbert hypersphere with conditional wasserstein generative adversarial nets",
      "authors": [
        "N Otberdout",
        "M Daoudi",
        "A Kacem",
        "L Ballihi",
        "S Berretti"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "141",
      "title": "Every smile is unique: Landmark-guided diverse smile generation",
      "authors": [
        "W Wang",
        "X Alameda-Pineda",
        "D Xu",
        "P Fua",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "142",
      "title": "Learning to Generate Customized Dynamic 3D Facial Expressions",
      "authors": [
        "R Potamias",
        "J Zheng",
        "S Ploumpis",
        "G Bouritsas",
        "E Ververas",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "143",
      "title": "3D to 4D Facial Expressions Generation Guided by Landmarks",
      "authors": [
        "N Otberdout",
        "C Ferrari",
        "M Daoudi",
        "S Berretti",
        "A Del Bimbo"
      ],
      "year": "2021",
      "venue": "3D to 4D Facial Expressions Generation Guided by Landmarks"
    },
    {
      "citation_id": "144",
      "title": "All-in-one: Facial expression transfer, editing and recognition using a single network",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2019",
      "venue": "All-in-one: Facial expression transfer, editing and recognition using a single network"
    },
    {
      "citation_id": "145",
      "title": "Talking Face Generation with Expression-Tailored Generative Adversarial Network",
      "authors": [
        "D Zeng",
        "H Liu",
        "H Lin",
        "S Ge"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "146",
      "title": "Modeling and verification of facial expression display mechanism for developing a sociable robot face",
      "authors": [
        "D Benson",
        "M Khan",
        "T Tan",
        "T Hargreaves"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Advanced Robotics and Mechatronics"
    },
    {
      "citation_id": "147",
      "title": "Closed-loop Petri Net model for implementing an affective-state expressive robotic face",
      "authors": [
        "T Hargreaves",
        "M Khan",
        "D Benson",
        "T Tan"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)"
    },
    {
      "citation_id": "148",
      "title": "Human-like facial expression imitation for humanoid robot based on recurrent neural network",
      "authors": [
        "Z Huang",
        "F Ren",
        "Y Bao"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Advanced Robotics and Mechatronics"
    },
    {
      "citation_id": "149",
      "title": "Emotion Dependent Domain Adaptation for Speech Driven Affective Facial Feature Synthesis",
      "authors": [
        "R Sadiq",
        "E Erzin"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "150",
      "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "A Herva",
        "J Lehtinen"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "151",
      "title": "Speech-driven 3D facial animation with implicit emotional awareness: a deep learning approach",
      "authors": [
        "H Pham",
        "S Cheung",
        "V Pavlovic"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "152",
      "title": "Multimodal speech driven facial shape animation using deep neural networks",
      "authors": [
        "S Asadiabadi",
        "R Sadiq",
        "E Erzin"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "153",
      "title": "Affect-expressive movement generation with factored conditional restricted boltzmann machines",
      "authors": [
        "O Alemi",
        "W Li",
        "P Pasquier"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "154",
      "title": "The role of emotion in believable agents",
      "authors": [
        "J Bates"
      ],
      "year": "1994",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "155",
      "title": "Affect-expressive hand gestures synthesis and animation",
      "authors": [
        "E Bozkurt",
        "E Erzin",
        "Y Yemez"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "156",
      "title": "Effects of a robotic storyteller's moody gestures on storytelling perception",
      "authors": [
        "J Xu",
        "J Broekens",
        "K Hindriks",
        "M Neerincx"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "157",
      "title": "Laban movement analysis and affective movement generation for robots and other near-living creatures",
      "authors": [
        "S Burton",
        "A.-A Samadani",
        "R Gorbet",
        "D Kulić"
      ],
      "year": "2016",
      "venue": "Dance notations and robot motion"
    },
    {
      "citation_id": "158",
      "title": "Laban for actors and dancers: putting Laban's movement theory into practice: a step-by-step guide",
      "authors": [
        "J Newlove"
      ],
      "year": "1993",
      "venue": "Theatre Arts Books"
    },
    {
      "citation_id": "159",
      "title": "Endeffectors trajectories: An efficient low-dimensional characterization of affective-expressive body motions",
      "authors": [
        "P Carreno-Medrano",
        "S Gibet",
        "P.-F Marteau"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "160",
      "title": "From expressive end-effector trajectories to expressive bodily motions",
      "authors": [
        "P Carreno-Medrano",
        "S Gibet",
        "P.-F Marteau"
      ],
      "year": "2016",
      "venue": "Proceedings of the 29th International Conference on Computer Animation and Social Agents"
    },
    {
      "citation_id": "161",
      "title": "Realtime style transfer for unlabeled heterogeneous human motion",
      "authors": [
        "S Xia",
        "C Wang",
        "J Chai",
        "J Hodgins"
      ],
      "year": "2015",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "162",
      "title": "Perceptual validation for the generation of expressive movements from end-effector trajectories",
      "authors": [
        "P Carreno-Medrano",
        "S Gibet",
        "P.-F Marteau"
      ],
      "year": "2018",
      "venue": "ACM Trans. Interact. Intell. Syst"
    },
    {
      "citation_id": "163",
      "title": "A deep learning framework for character motion synthesis and editing",
      "authors": [
        "D Holden",
        "J Saito",
        "T Komura"
      ],
      "year": "2016",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "164",
      "title": "Phase-functioned neural networks for character control",
      "authors": [
        "D Holden",
        "T Komura",
        "J Saito"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "165",
      "title": "Factored conditional restricted Boltzmann machines for modeling motion style",
      "authors": [
        "G Taylor",
        "G Hinton"
      ],
      "year": "2009",
      "venue": "Proceedings of the 26th annual international conference on machine learning"
    },
    {
      "citation_id": "166",
      "title": "Keepon",
      "authors": [
        "H Kozima",
        "M Michalowski",
        "C Nakagawa"
      ],
      "year": "2009",
      "venue": "Int. J. Soc. Robot"
    },
    {
      "citation_id": "167",
      "title": "Laban head-motions convey robot state: A call for robot body language",
      "authors": [
        "H Knight",
        "R Simmons"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on robotics and automation"
    },
    {
      "citation_id": "168",
      "title": "Robots expressing dominance: Effects of behaviours and modulation",
      "authors": [
        "R Peters",
        "J Broekens",
        "K Li",
        "M Neerincx"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "169",
      "title": "RNN with Russell's circumplex model for emotion estimation and emotional gesture generation",
      "authors": [
        "T Tsujimoto",
        "Y Takahashi",
        "S Takeuchi",
        "Y Maeda"
      ],
      "year": "2016",
      "venue": "IEEE Congress on Evolutionary Computation"
    },
    {
      "citation_id": "170",
      "title": "Affective robot movement generation using cyclegans",
      "authors": [
        "M Suguitan",
        "M Bretan",
        "G Hoffman"
      ],
      "year": "2019",
      "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "171",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "172",
      "title": "Emotion encoding in human-drone interaction",
      "authors": [
        "J Cauchard",
        "K Zhai",
        "M Spadafora",
        "J Landay"
      ],
      "year": "2016",
      "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "173",
      "title": "Leveraging morphological computation for expressive movement generation in a soft robotic artwork",
      "authors": [
        "J Jørgensen"
      ],
      "year": "2017",
      "venue": "Proceedings of the 4th International Conference on Movement Computing"
    },
    {
      "citation_id": "174",
      "title": "Interaction of robot with humans by communicating simulated emotional states through expressive movements",
      "authors": [
        "S Sial",
        "M Sial",
        "Y Ayaz",
        "S Shah",
        "A Zivanovic"
      ],
      "year": "2016",
      "venue": "Intell. Serv. Robot"
    },
    {
      "citation_id": "175",
      "title": "From motions to emotions: Can the fundamental emotions be expressed in a robot swarm?",
      "authors": [
        "M Santos",
        "M Egerstedt"
      ],
      "year": "2020",
      "venue": "Int. J. Soc. Robot"
    },
    {
      "citation_id": "176",
      "title": "Exploiting the robot kinematic redundancy for emotion conveyance to humans as a lower priority task",
      "authors": [
        "J.-A Claret",
        "G Venture",
        "L Basañez"
      ],
      "year": "2017",
      "venue": "Int. J. Soc. Robot"
    },
    {
      "citation_id": "177",
      "title": "Multimodal expression of artificial emotion in social robots using color, motion and sound",
      "authors": [
        "D Löffler",
        "N Schmidt",
        "R Tscharn"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "178",
      "title": "Evaluation of affective computing systems from a dimensional metaethical position",
      "authors": [
        "C Reynolds",
        "R Picard"
      ],
      "year": "2005",
      "venue": "Evaluation of affective computing systems from a dimensional metaethical position"
    },
    {
      "citation_id": "179",
      "title": "Context and contextual word meaning",
      "authors": [
        "N Dash"
      ],
      "year": "2008",
      "venue": "SKASE J. Theor. Linguist"
    },
    {
      "citation_id": "180",
      "title": "Context based text-generation using lstm networks",
      "authors": [
        "S Santhanam"
      ],
      "year": "2020",
      "venue": "Context based text-generation using lstm networks"
    },
    {
      "citation_id": "181",
      "title": "Emotional end-to-end neural speech synthesizer",
      "authors": [
        "Y Lee",
        "A Rabiee",
        "S.-Y Lee"
      ],
      "year": "2017",
      "venue": "Emotional end-to-end neural speech synthesizer"
    },
    {
      "citation_id": "182",
      "title": "Longitudinal impact of autonomous robotmediated joint attention intervention for young children with ASD",
      "authors": [
        "Z Zheng",
        "G Nie",
        "A Swanson",
        "A Weitlauf",
        "Z Warren",
        "N Sarkar"
      ],
      "year": "2016",
      "venue": "International Conference on Social Robotics"
    },
    {
      "citation_id": "183",
      "title": "Predicting Response to Joint Attention Performance in Human-Human Interaction Based on Human-Robot Interaction for Young Children with Autism Spectrum Disorder",
      "authors": [
        "G Nie"
      ],
      "year": "2018",
      "venue": "2018 27th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "184",
      "title": "An Immersive Computer-Mediated Caregiver-Child Interaction System for Young Children With Autism Spectrum Disorder",
      "authors": [
        "G Nie"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "185",
      "title": "Family caregiving of persons with dementia: prevalence, health effects, and support strategies",
      "authors": [
        "R Schulz",
        "L Martire"
      ],
      "year": "2004",
      "venue": "Am. J. Geriatr. psychiatry"
    },
    {
      "citation_id": "186",
      "title": "Affective computing and autism",
      "authors": [
        "R Kaliouby",
        "R Picard",
        "S Baron-Cohen"
      ],
      "year": "2006",
      "venue": "Ann. N. Y. Acad. Sci"
    },
    {
      "citation_id": "187",
      "title": "Association between social support and depression in the general population: the HUNT study, a cross-sectional survey",
      "authors": [
        "S Grav",
        "O Hellzèn",
        "U Romild",
        "E Stordal"
      ],
      "year": "2012",
      "venue": "J. Clin. Nurs"
    },
    {
      "citation_id": "188",
      "title": "Support seeking and support giving within couples in an anxietyprovoking situation: The role of attachment styles",
      "authors": [
        "J Simpson",
        "W Rholes",
        "J Nelligan"
      ],
      "year": "1992",
      "venue": "J. Pers. Soc. Psychol"
    }
  ]
}