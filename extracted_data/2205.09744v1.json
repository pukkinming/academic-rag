{
  "paper_id": "2205.09744v1",
  "title": "Overcoming Language Disparity In Online Content Classification With Multimodal Learning",
  "published": "2022-05-19T17:56:02Z",
  "authors": [
    "Gaurav Verma",
    "Rohit Mujumdar",
    "Zijie J. Wang",
    "Munmun De Choudhury",
    "Srijan Kumar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Advances in Natural Language Processing (NLP) have revolutionized the way researchers and practitioners address crucial societal problems. Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally. While existing research has developed better multilingual and monolingual language models to bridge this language disparity between English and non-English languages, we explore the promise of incorporating the information contained in images via multimodal machine learning. Our comparative analyses on three detection tasks focusing on crisis information, fake news, and emotion recognition, as well as five high-resource non-English languages, demonstrate that: (a) detection frameworks based on pre-trained large language models like BERT and multilingual-BERT systematically perform better on the English language compared against non-English languages, and (b) including images via multimodal learning bridges this performance gap. We situate our findings with respect to existing work on the pitfalls of large language models, and discuss their theoretical and practical implications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Users of social computing platforms use different languages to express themselves  (Mocanu et al. 2013 ). These expressions often give us a peek into personal-level and societallevel discourses, ideologies, emotions, and events  (Kern et al. 2016) . It is crucial to model all of these different languages to design equitable social computing systems and to develop insights that are applicable to a wider segment of the global population.\n\nIn recent years, we have seen remarkable ability in using linguistic signals and linguistic constructs extracted from social media and web activity toward tackling societal challenges, whether in detecting crisis-related information  (Houston et al. 2015)  or identifying depressionrelated symptoms  (De Choudhury et al. 2013) . While earlier approaches relied on qualitative language inference techniques  (Crook et al. 2016) , using pre-existing dictionaries  (Pennebaker, Francis, and Booth 2001) , and traditional We use multimodal (image + text) learning to overcome the language disparity that exists between English and non-English languages. The figure illustrates an example of a social media post that is correctly classified in English but misclassified in Spanish. Including the corresponding image leads to correct classification in Spanish as well as other non-English languages.\n\nclassifiers  (Glasgow, Fink, and Boyd-Graber 2014) , more recent approaches leverage the advances in deep learningbased language modeling techniques. Large pre-trained models like BERT  (Devlin et al. 2018 ) are frequently used to train classifiers in tasks pertaining to social good  (Singhal et al. 2019; Sun, Huang, and Qiu 2019)  and are now a new standard to build state-of-the-art classification systems to support real-world decision-making.\n\nAs  Joshi et al. (2020)  illustrate, these rapidly evolving language technologies and their applications are largely focused on only a very small number of over 7000 languages in the world. A majority of the research in natural language processing (NLP) is focused on a few high resource languages, and disproportionately on English  (Mielke 2016; Bender 2019 ). The development of systems that can model languages beyond English is important for ensuring (a) inclusion of communities, (b) equitable extension of services that are driven by these language technologies to diverse groups, and (c) preservation of endangered languages  (Muller et al. 2021) . Especially in the context of social computing, language-specific lapses can lead to in-equitable outcomes. For instance, lower detection abilities on Twitter posts published in Spanish could possibly lead to inequitable humanitarian interventions in times of crisis; and, the lack of powerful misinformation detectors for the Chinese language can possibly lead to situations where specific-language speaking individuals are more vulnerable to health-related misinformation. As BERT-like monolingual and multilingual models take a central role in building approaches to address crucial societal tasks, the bias toward the English language can propagate, reinforce, and even exacerbate the existing inequities that many underserved groups face  (PewResearch 2018) .\n\nExisting attempts to bridge this gap between English and non-English languages have focused on developing better multilingual and monolingual (non-English) language models  (Nozza, Bianchi, and Hovy 2020) . In this work, we explore the promise of information that lies in other complementary modalities, specifically images (1). Considering images as an additional modality has proven to be beneficial in a wide range of scenarios -from accurately estimating dietary intake in a pediatric population  (Higgins et al. 2009) , to creating effective questionnaires  (Reynolds and Johnson 2011) . The underlying idea stems from the simple fact that images are not bound by any language. We propose the use of multimodal learning, which jointly leverages the information in related images and text, to boost performance on the non-English text and effectively bring it closer to the performance on English text. More concretely, we study the following two research questions in this work: RQ1: Does using large language models for social computing tasks lead to lower performance on non-English languages when compared to the English language? RQ2: Can inclusion of images with multimodal learning help in bridging the performance gap between English and non-English models?\n\nTo this end, we study the performance of fine-tuned BERT-based monolingual models and multilingual-BERT on three distinct classification tasks that are relevant to social computing: (i) humanitarian information detection during crisis  (Ofli, Alam, and Imran 2020) , (ii) fake news detection  (Shu et al. 2017) , and (iii) emotion detection  (Duong, Lebret, and Aberer 2017) . These tasks involve categorizing posts/articles published on the web into real-world concepts that help determine, for instance, the type of humanitarian effort required during a crisis or the veracity of published news. Besides English, we consider five high-resource languages: Spanish, French, Portuguese, (Simplified) Chinese, and Hindi. Via extensive comparative analysis on these existing datasets, we demonstrate that (a) large language models -whether monolingual or multilingual -systematically perform better on English text compared to other highresource languages, and (b) incorporating images as an additional modality leads to considerably lesser deviation of performance on non-English languages with respect to that on English 1 . We conclude by discussing the implications of these findings from both practical and theoretical stand-1 Project webpage with resources: https://multimodalitylanguage-disparity.github.io/ points, and situate them with respect to prior knowledge from the domains of NLP and social computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "We discuss three major themes of research that are relevant to our work: the use of large language models in developing approaches for social computing tasks, the discussion of the pitfalls of large language models and their treatment of non-English languages, and the role of multimodal learning in developing social media classification systems. Large language models for social computing tasks: Development and deployment of large language modelsdeep learning models trained on massive amounts of data collected from the web, have transformed not only the field of NLP but also related fields that leverage text data to make inferences  (Rasmy et al. 2021) . To this end, large language models have been used for various applications in social computing  (Arviv, Hanouna, and Tsur 2021; Choi et al. 2021) . The effectiveness of language models in addressing these tasks can be primarily attributed to two factors: (i) they are trained on massive amounts of unannotated text data, leading to a general understanding of natural language, and (ii) they can be easily fine-tuned for specific tasks with moderately-sized annotated data to demonstrate task-specific understanding. Several language models such as BERT  (Devlin et al. 2018 ) and T5  (Raffel et al. 2020)  have been developed for the English language. Since these models cover only English, large multilingual variants like mBERT  (Devlin et al. 2018 ) and mT5  (Xue et al. 2021)  have also been developed to model over a hundred other languages beyond English. These language models (both monolingual and multilingual) are widely adopted to develop state-of-the-art approaches for several tasks where the textual modality withholds key information. Language disparity in NLP: Scholars have discussed the disproportionate focus in NLP research on the English language  (Bender 2019; Joshi et al. 2020; Mielke 2016) . Since approaches to address social computing tasks are increasingly relying on NLP techniques centered around large language models, it is important to understand the possible implications of this disproportionate focus on the state of social computing research. Prior studies have tried to understand the pitfalls of using large language models -environmental and financial costs  (Strubell, Ganesh, and McCallum 2019) , reliance on data that represents hegemonic viewpoints  (Bender et al. 2021) , encoding biases against marginalized populations  (Basta, Costa-jussà, and Casas 2019) . However, our work focuses on comparing English language models with non-English language models in a social computing context. Similar to English, multilingual variants of language models are used to develop the state-of-the-art 2  approaches for multiple high-resource non-English languages  (Nozza, Bianchi, and Hovy 2020) . To this end, previous research has focused on understanding how multilingual language models treat various non-English languages relative to each other, especially the contrast between high-resource and low-resource  languages  (Pires, Schlinger, and Garrette 2019; Wu and Dredze 2020; Nozza, Bianchi, and Hovy 2020; Muller et al. 2021) . In this work, we do not focus on the general pitfalls of large language models or comparisons across non-English languages. Instead, we aim to establish the language disparity between English and non-English languages that is caused due to the adoption of large language models. Multimodal learning: Multimodal learning involves relating information from multiple content sources. On the web, the text is often associated with images, especially on social media platforms like Twitter, Instagram, and Facebook. Multimodal learning allows us to combine modality-specific information into a joint representation that captures the realworld concept corresponding to the data  (Ngiam et al. 2011) .\n\nTo this end, inference based on multimodal learning has demonstrated better performance than both text-only and image-only methods, especially in scenarios where access to complementary information can be crucial (e.g., assessing whether a Twitter post (image + text) is about disaster  (Ofli, Alam, and Imran 2020) , or if a news article (image + title) is fake  (Singhal et al. 2020) , whether the Reddit post conveys rage  (Duong, Lebret, and Aberer 2017) ). However, the studies that demonstrate the effectiveness of multimodal learning do so while making comparisons against language-specific text-only methods, without making any comparisons across different languages. In this work, we aim to use multimodal learning, more specifically images, to bridge the gap between English and non-English languages.",
      "page_start": 2,
      "page_end": 10
    },
    {
      "section_name": "Datasets",
      "text": "To achieve robust and generalizable findings, we utilize a comparative analytic approach on three different preexisting datasets that cover issues like humanitarian information processing, fake news detection, and emotion detection. Figure  2  presents some examples from the three datasets discussed below as well as the proportion of classes.\n\nMultimodal crisis humanitarian dataset: In times of crises, social media often serves as a channel of commu-nication between affected parties and humanitarian organizations that process this information to respond in a timely and effective manner. Multimodal fake news dataset: Ease of publishing news on online platforms, without fact-checking and editorial rigor, has often led to the widespread circulation of misleading information on the web  (Lazer et al. 2018) .  Shu et al. (2017; 2018)  curated the FakeNewsNet dataset to promote research on multimodal fake news detection; it comprises full-length news articles (title + body) from two different domains: politics (fake/real labels provided by PolitiFact) and entertainment (fake/real labels provided by GossipCop) and the corresponding images in the articles. The fake news detection task can therefore be formulated as a binary classification task, where the label:0 corresponds to the real class and the label:1 corresponds to the fake class. We use the preprocessed version of the dataset provided by  Singhal et al. (2020)  and consider only the title of the news article for our experiments while dropping the body of the article. Furthermore, we combine the two domains (entertainment and politics) to create a single dataset and use the same train and test splits like Singhal et al. We, however, randomly split the original train set in 90 : 10 ratio to create an updated train and validation set. Effectively, our final train, validation, and   Horvitz 2013 ). To this end, we collect the dataset introduced by  Duong, Lebret, and Aberer ( 2017)  for the task of multimodal emotion detection. The dataset comprises Reddit posts categorized into 4 emotion-related classes, creepy, gore, happy, and rage, where each post contains an image and text. We crawled the images from Reddit using the URLs provided by the authors and randomly split the dataset in a 80:10:10 ratio to obtain the train (n = 2568), validation (n = 321), and test (n = 318) sets. Similar to other datasets, we maintain the exact same splits for all the experiments that involve this dataset to ensure consistent comparisons.\n\nCurating non-English datasets: All the three datasets discussed above only have texts (Twitter posts, news articles, and Reddit posts) in English. Given the lack of non-English multimodal datasets, we employ machine translation to convert English text into different target languages. For translation, we use the MarianNMT system, which is an industrialgrade machine translation system that powers Microsoft Translator  (Junczys-Dowmunt et al. 2018) . As target languages, we consider the following five non-English languages: Spanish (es), French (fr), Portuguese (pt), Simplified Chinese (zh), and Hindi (hi). Together, these five languages represent culturally diverse populations -minority groups in the United States (Hispanics), Asians, and the Global South, and are written in various scripts -Latin (es, fr, and pt), Hanzi (zh), and Devanagari (hi). It is worth noting that none of these five non-English languages are considered to be low-resource languages  (Hedderich et al. 2021 )which is a more appropriate designation for languages like Sinhala, the Fijian language, and Swahili. However, since these languages are sufficiently high-resource languages, MarianNMT can produce high-quality translations in these languages from the original English text.\n\nWe use the pre-trained language-specific translation models of MarianNMT, made available via HuggingFace  (Wolf et al. 2019) , to translate the text part of each example in the three datasets to the five target language (en → es, fr, pt, Human-translated subset for crisis humanitarianism:\n\nBesides the machine-translated text, we also obtain manual translations for a subset of examples from the test set of the Crisis Humanitarianism dataset. For Spanish, French, and Portuguese, we recruited workers from Amazon Mechanical Turk (AMT) who were designated as 'Masters' and proficient in both English and the target language. For Chinese and Hindi, we obtained annotations from doctoral students fluent in both English and Chinese/Hindi. The recruited participants translated 200 examples from the test set for each non-English language. The annotators were shown both the original Twitter post and were instructed to translate the text to the target language while maintaining grammatical coherence and preserving semantic meaning. We use this manually-translated subset of the test set for evaluation purposes alone -allowing us to observe the validity of observed trends on a cleaner dataset. Next, we assess the quality of machine-and human-translated text.\n\nHuman evaluation of translation quality: MarianNMT is the engine behind Microsoft Translator, a system that demonstrates translation quality that is close to human parity for specific languages and in constrained settings (Microsoft 2019). We conduct an independent evaluation of the generated translations of examples from our datasets. For this, we randomly sampled 200 examples from each dataset (600 examples in total) and asked human annotators to assess the translation quality. Similar to above, the recruited annotators were AMT workers for Spanish, French, and Portuguese, and doctoral students for Chinese and Hindi. Each of the 3000 (i.e., 600 × 5) translation pairs was annotated by 3 annotators where they responded to the following two questions using a five-point Likert scale (1: strongly dis- agree, . . . , 5: strongly agree): (i) Is the <Spanish> 3 text a good translation of the English text?, and (ii) Does the <Spanish> text convey the same meaning as the English text? While the first question encouraged the annotators (i.e., AMT workers for Spanish, French, and Portuguese, and doctoral students for Chinese and Hindi) to evaluate the quality of the translations, including grammatical coherence, the second question encouraged them to assess the preservation of meaning in the generated translation, a relatively relaxed assessment. As shown in Table  1 , the annotators' responses to the first question indicate that the translation qualities were reliable. We observe high average scores on the Likert scale as well as strong inter-annotator agreements (computed using Cohen's κ) for all five languages. For the second question, the average scores on the Likert scale are consistently ≥ 4.10 for all the five languages, indicating almost perfect preservation of meaning after translation from the English text to the target language. Finally, we conducted a similar assessment of the quality of the human-translated subset of the Crisis Humanitarianism dataset. Each of 1000 (i.e., 200 × 5) translation pairs were similarity annotated by 3 annotators. As expected, Table 2 shows that the fluency and meaning preservation in the human-translated text is better than the machine-translated text with strong inter-annotator agreement scores.\n\nIn the upcoming sections, we describe the training and evaluation of the classification models, and the results for RQ1 and RQ2. Figure  3  provides an overview of our method. 3 The language name was changed as per the target language for which the annotators were rating. Also, we inserted some \"attention-check\" examples during the annotation process to ensure the annotators read the text carefully before responding. This was done by explicitly asking the annotators to mark a randomlychosen score on the Likert scale regardless of the original and translated text. We discard the annotations from annotators who did not respond to all the attention-check examples correctly.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Language Disparity With Language Models",
      "text": "In this section, we focus on RQ1: whether using large language models for classification tasks results in a systematic disparity between the performance on English and non-English text. We use pre-trained language models and finetune them on the specific classification task using languagespecific labeled datasets. Classification models for English: We use two pretrained language models: DistilBERT  (Sanh et al. 2019 ) (distilbert-base-cased on HuggingFace) and Distilm-BERT (distilbert-base-multilingual-cased on Hug-gingFace) to classify the English text. We fine-tune the pretrained language models on the 3 datasets discussed above by using the respective training sets. The process of finetuning a language model involves taking a pre-trained language model 4  and replacing the \"pre-training head\" of the model with a randomly initialized \"classification head\". The randomly initialized parameters in the classification head are learned by fine-tuning the model on classification examples while minimizing the cross-entropy loss. To train the English language classification models for each dataset, we use Adam optimizer  (Kingma and Ba 2014)  with a learning rate initialized at 10 -4 ; hyper-parameters are set by observing the classification performance achieved on the respective validation set. We use early stopping  (Caruana, Lawrence, and Giles 2000)  to stop training when the loss value on the validation set stops to improve for 5 consecutive epochs. Classification models for non-English languages: To classify the non-English languages into task-specific categories, we use two set of pre-trained language models: (a) monolingual models and (b) multilingual model called Distilm-BERT (distilbert-base-multilingual-cased on Hug- Table  3 : Disparity between English and non-English languages using monolingual and multilingual models. Performance of the task and language-specific text-only classification models on 3 datasets and 6 languages.\n\ngingFace). For monolingual models, we refer to the leaderboard maintained by Nozza, Bianchi, and Hovy (2020) and select the best performing models for a specific language. Namely, we select BETO for modeling Spanish text  (Cañete et al. 2020) , CamemBERT for French  (Martin et al. 2020) , BERTimbau for Portuguese  (Souza, Nogueira, and Lotufo 2020) , ChineseBERT for Chinese  (Cui et al. 2020) , and HindiBERT for Hindi  (Doiron 2020) . We adopt the same model training and hyper-parameter selection strategies as for the English language models discussed above. Training a classification model for each of the five non-English languages across the three tasks gives us a total of 30 non-English text classification models. Our training strategies allow us to compare the best text classification models for all the languages for each of the three tasks individually.\n\nFine-tuned text representations: Once fine-tuned, the text classifiers can be used to extract representations for any input text by taking the output of the penultimate layers. These representations, also called embeddings, capture attributes of the text that the model has learned to use for categorizing the input into the target classes, and therefore can be fed to the multimodal classifier as a representation of the text part of the multimodal input. We obtain this latent representation of input text, denoted by vector T (with dimension 768), by averaging the token-level outputs from the penultimate layer of the fine-tuned classification models.\n\nEvaluation metrics: We compute standard classification metrics to evaluate the performance these text-only classifiers on the test sets of respective datasets. Since crisis humanitarian post detection and emotion detection are multiclass classification tasks, we compute macro averages of class-wise F 1 , precision, and recall scores along with the overall classification accuracy. However, since fake news detection is a binary classification task, we compute the F 1 , precision, and recall scores for the positive class (i.e., label:1 = fake). Table  3  summarizes the performance of the text-only classifiers discussed above. Since the performance of deep learning models, especially BERT-based large language models, can possibly change with initialization schemes  (Sellam et al. 2021) , we vary the random ini-tialization across different runs of the models and report the averages from 10 different runs.\n\nPerformance on English vs. non-English languages: In Table  3 , we observe that the performance of text-only classification models is higher when the input is in the English language when compared against the performance of models that take other high-resource non-English languages as input. This trend is consistent across (i) both monolingual and multilingual models, (ii) the three tasks considered in this work as well as (iii) across all the classification metrics.\n\nFor monolingual and multilingual models, the gap in performance on English and non-English languages varies with the task at hand as well as the non-English language being considered. For instance, for the crisis humanitarianism task with monolingual models, the drop in F 1 score of Spanish with respect to that of English is 9.5%, while it is 5.1% for the emotion detection task. For the same task, e.g., emotion detection, using monolingual models leads to performance drops that vary from 5.1% for Spanish to 11.4% for Hindi. It is noteworthy that the performance on non-English languages relative to each other maintains a near-uniform pattern across the three tasks for both monolingual and multilingual models -the performance is consistently the worst for Hindi; the performance on Chinese and Portuguese is relatively better, and the performance on Spanish and French is best when compared against other non-English languages.\n\nWe revisit this observation and its potential causes in the Discussion section. In sum, our results indicate a language disparity exists due to the use of large language models in varied classification tasks -whether monolingual or multilingual. We recall that the adopted methodology -finetuning of pre-trained language models -is representative of the state-of-the-art NLP techniques that are frequently adopted for solving classification tasks  (Li et al. 2020) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Benefits Of Multimodal Learning",
      "text": "In this section, we focus on RQ2: can we leverage images with the help of multimodal learning to overcome the disparity between English and non-English languages. and task-and language-specific multimodal classifiers (both monolingual and multilingual).\n\nImage-Only classification model: To investigate the predictive power of images without textual information, we develop and evaluate image-only classifiers for each dataset.\n\nSimilar to text classifiers, we apply a fine-tuning approach to train the task-specific image classifiers. We first freeze the weights of VGG-16  (Simonyan and Zisserman 2015) , a popular deep Convolutional Neural Network, pre-trained on ImageNet  (Deng et al. 2009 ), a large-scale generic image classification dataset. Then, we swap the last layer from the original model to three fully connected hidden layers with dimensions 4096, 256, and num-of-classes, respectively. Finally, retrain these three layers to adapt the image distribution in each dataset. As images in our datasets have various dimensions, we apply a standard image pre-processing pipeline so that they can fit the pre-trained VGG-16 model's input requirement. We first resize the image so that its shorter dimension is 224, then we crop the square region in the center and normalize the square image with the mean and standard deviation of the ImageNet images  (Deng et al. 2009) .\n\nTo train and evaluate image-specific classifiers, we use the same splits in text-only models to divide images into the train, validation, and test sets. We use Adam optimizer  (Kingma and Ba 2014)  with a learning rate of 10 -4 for each dataset. To avoid overfitting, we use early stopping to stop training when the loss value on the validation set stops to improve for 10 consecutive epochs. Finally, we extract the image embeddings, denoted by I, from image-specific classifiers by computing the neuron activations from the penultimate layer (with dimension 256) as a latent representation of the image information for our multimodal models.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Classification Model:",
      "text": "We implement a multimodal classifier  (Ngiam et al. 2011 ) that fuses the latent representations of individual modalities (text and image) to perform classification based on the joint modeling of both input modalities. We feed the concatenation of fine-tuned text and image representations (i.e., T ⊕ I) to the multimodal classifier, which is essentially a series of fully connected layers with ReLU activation  (Agarap 2018) . The architecture of the multimodal classifier comprises an input layer (1024 neurons), 3 hidden layers (512, 128, 32 neurons), and an output layer (neurons = number of classes in the dataset). We train a multimodal classifier for each language in each task. Similar to image-only and text-only classification models discussed above, for each training instance, we use Adam optimizer  (Kingma and Ba 2014)  with a learning rate initialized at 10 -4 . We use early stopping based on the validation set loss to stop the training and avoid overfitting on the train set.\n\nWe use the same evaluation metrics to evaluate the imageonly and multimodal classifiers as we did for the text-only ones, and report the average of 10 different runs in Table  4 . Additionally, in Figures  4  and 5  we present the root-meansquared deviation (RMSD en ) values of F 1 scores of non-English languages with respect to that of the English language for text-only and multimodal classifiers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Learning Boosts Classification Performance:",
      "text": "As Table  4  shows, the classification performance for all the languages (English as well as non-English) improves considerably with the inclusion of images as an additional modality when compared against the performance of corresponding text-only classification models. This trend is consistent across all three datasets and both the set of models considered in our study -monolingual as well as multilingual. It is interesting to note that the benefit of including images, as indicated by the increase in performance metrics, is largely dependent on the informativeness of the images towards the classification task. For instance, for fake news detection, the image-only classifier achieves an F 1 score of 0.15, indicating poor distinguishability between real and fake news solely based on images in a news article. Consequently, the increase in the performance of the multimodal classifier over that of the monolingual text-only classifier is relatively marginal, ranging from 1.5% (F 1 increases from 0.59 to 0.60 for English) to 3.7% (F 1 increases from 0.54 to 0.56 for Hindi). In contrast, for the emotion detection task, the image-only classifier achieves an F 1 score of 0.94,  Figure  4 : Comparing F 1 scores on non-English and English text for both text-only and multimodal classifiers using monolingual language models. RMSD en denotes the root-mean-square deviation of the F 1 scores achieved by non-English classifiers with respect to the that of the corresponding English classifier. The RMSD en values for multimodal models are lower than those for monolingual text-only models. Figure  5 : Comparing F 1 scores on non-English and English text for both text-only and multimodal classifiers using multilingual models. RMSD en denotes the root-mean-square deviation of the F 1 scores achieved by non-English classifiers with respect to the that of the corresponding English classifier. The RMSD en values for multimodal models are lower than those for multilingual text-only models.\n\nindicating extremely good distinguishability between emotion categories solely based on images. As a consequence, the increase in the performance of the multimodal classifier over that of the monolingual text-only classifier ranges from 7.6% (F 1 increases from 0.79 to 0.85 for English) to 11.4% (F 1 increases from 0.70 to 0.78 for Hindi). We observe the same trends for multilingual models as well.\n\nMultimodal learning helps in bridging the gap between English and non-English languages: The results discussed so far indicate: (i) the performance of the state-of-the-art techniques for non-English languages is worse than the performance of the state-of-the-art techniques for the English language, and (ii) incorporating images as an additional modality using multimodal learning leads to better classification performance when compared against the performance of text-only counterparts. However, a crucial question remains to be answered: can multimodal learning help in overcoming the language disparity between English and non-English languages? To answer this, we focus on the root-mean-square deviation (RMSD en ) scores presented in Figures  4  and 5 . RMSD en is calculated by taking the root of the average of the squared pairwise differences between F 1 scores for English and other non-English languages.\n\nWe compute the RMSD en scores for both monolingual and multilingual models. It is clear that the RMSD en of F 1 scores achieved by non-English classifiers with respect to the F 1 score achieved by the English classifier are lesser with multimodal input when compared against text-only input. For monolingual models, the drops in RMSD en values are 50.0% (0.06 → 0. Results on human-translated test set: To evaluate the performance of trained models on a sample that is free from the noise introduced by automated translators, we evaluate all the trained models for the crisis humanitarian task on the human-translated subset of the test set. Table  5  reinforces our observations -the disparity between English and non-English languages exists due to both monolingual and multilingual language models and multimodal learning helps in reducing this performance gap. For monolingual and multilingual models, the RMSD en values drop from 0.05 to 0.04 and from 0.15 to 0.06, respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Our study demonstrates that in the context of societal tasks -as demonstrated by our focus on three datasets -the performance of large language models on non-English text is subpar when compared to the performance on English text.\n\nIn the subsequent discussion, we highlight how this could have possibly threatening implications on the lives of many individuals who belong to underserved communities. Furthermore, we empirically demonstrate that using images as an additional modality leads to a lesser difference between the performance on English and non-English text, as indicated by decreased RMSD en values. While existing studies have focused on developing advanced monolingual language models that can boost the performance on specific non-English languages to bridge the performance gap, we demonstrate the benefits of including other complementary modalities, especially those that are language-agnostic. Decreased RMSD en values indicate that if images are considered along with the text, the performance on all languages is not only better than when only text is considered, but it is also comparable across English and non-English languages. Implications of language disparity with text-only models: In the context of social computing, disparities between English and non-English languages can lead to inequitable outcomes. For instance, as per our observations, if stateof-the-art NLP techniques that are centered around BERTbased language models are adopted to detect humanitarian information during crises, the detection abilities would be poorer for social media posts in non-English languages than those in English, causing delayed interventions. In coun-  tries like the United States, where non-English languages like Spanish and Chinese are spoken by a considerable number of people (AAAS 2016), this disparity could exacerbate the effects of discrimination and prejudice that they already face (PewResearch 2018). Similarly, poor emotion recognition in specific non-English languages can lead to unhelpful or even harmful outcomes in scenarios where the output of emotion recognition informs mental health interventions. Furthermore, poor fake news detection in specific non-English languages can lead to lacking correction and mitigation efforts, leading to relatively worse outcomes for non-English speaking populations.\n\nImplications of reduced language disparity with multimodal models: People use multiple content modalitiesimages, text, videos, and audio clips, to share updates on social platforms. Visual modalities (like images and videos) transcend languages and are extremely informative in scenarios like crisis information detection and emotion detection. Combining our multimodal approach with existing text-only approaches for better modeling of non-English text can present complementary gains, leading to a reduced gap between English and non-English languages. In other words, an approach that complements existing approaches that focus on only text can be expected to provide gains even as the language-specific text-only approaches improve and evolve.\n\nDependence of performance on pre-training corpus size:\n\nThe multilingual language model used in this study -mBERT, was pre-trained on huge corpora using selfsupervised objectives (Devlin et al. The data sizes (in GiB) in mBERT's pre-training corpus have the relative order en > fr > es > pt > zh > hi  (Conneau et al. 2020) . As shown in Figure  6 (a), the relationship between the languagespecific corpus that mBERT is trained on and the classification performance obtained after task-specific fine-tuning, is clear: larger representation in the pre-training corpus is related to better performance on downstream tasks. This trend reinforces the findings of Wu and Dredze (2020) in our context -the performance of large language models drops significantly as the considered languages have less pre-training data. This is concerning because, as  Bender et al. (2021)  argue, \"the large, uncurated, and Internet-based datasets\" that these language models are trained on \"encode the dominant/hegemonic view, which further harms people at the margins.\" However, as shown in Figure  6 (b), incorporating images using multimodal learning leads to a weakened dependence on the pre-training corpus size. This is indicated by the reduced slopes (m) of the trend lines across all three tasks. In effect, we demonstrate that multimodal learning, if adopted in the fine-tuning stages of approaches that employ large language models, could help in overcoming the well-recognized dependence of downstream performance on language-specific pre-training corpus size.\n\nBeyond the theoretical implications discussed above, we believe our methods demonstrate the crucial role that multimodal learning can play in the equitable dissemination of NLP-based services to a broader range of the global population. The systems that make inferences based on text data alone can be adapted to include the information contained in images, wherever possible, leading to better detection abilities on the non-English text and thereby bridging the gap between English and non-English languages. As our evaluation on human-translated and machine-translated text demonstrates, our proposed approach is compatible with setups that infer information directly from non-English text and with the approaches that first translate non-English text to English and then infer information from the translations.\n\nLimitations and future work: Large language models such as T5 and their corresponding multilingual variants mT5 overcome several limitations of BERT and mBERT by adopting different pre-training strategies. We specifically focused on BERT-based language models as representatives of large language models -note that our study aimed to understand the effectiveness of multimodal learning in overcoming the language disparity and not the relative performance of different language models. Since the underlying idea of fusing image and text representations can be applied to other language models as well, we believe that our insights and takeaways will also generalize to them.\n\nIn the future, we intend to experiment with low-resource languages to expand our claims to a wider set of languages. There are two major challenges on those fronts: (i) availability of parallel data, and (ii) identifying and developing the state-of-the-art text-only classification approaches for low-resource languages. A translation-based data creation pipeline will not work for low-resource languages and hence we may either curate the data by recruiting native speakers to translate the original examples from English or by collecting real data from social media for different languages. Furthermore, since the state-of-the-art classification approach for low resource languages may not be based on large language models  (Wu and Dredze 2020; Nozza, Bianchi, and Hovy 2020) , we intend to identify and develop those languagespecific approaches.\n\nLastly, the current study focuses on bridging the gap that exists in classification tasks. As part of future work, we intend to explore other types of tasks that are relevant to the social computing theme. Such tasks include, analyz-ing the lifestyle choices of social media users  (Islam and Goldwasser 2021)  and context-based quotation recommendation  (MacLaughlin et al. 2021) . By including other modalities like images, these approaches may be extended to non-English speaking populations. However, while images are not bound by languages, their production and perception are culturally influenced  (Hong et al. 2003) . This cultural influence is more prominent in user-generated content that is abundant on social platforms  (Shen, Wilson, and Mihalcea 2019) . Therefore, it is important to consider the cultural confounds in the production and consumption of images while using them to train and infer from machine learning models. Broader perspective, ethics, and competing interests: Developing powerful, accessible, and equitable resources for modeling non-English languages remains an open challenge. Our work argues that including information from other modalities, specifically images, can present new avenues to progress research in this direction. We believe this work will positively impact society by motivating researchers and practitioners to develop more reliable classifiers for non-English languages with applications to societal tasks. That said, it is worth noting that since images alone do not represent the entire cultural context, modeling techniques for non-English languages should continue to develop. Incorporation of new modalities alongside text also comes with additional challenges -for instance, the biases that computer vision models encode  (Hendricks et al. 2018)  need to be taken into consideration, and methods need to be developed to model cultural shifts in meaning for similar images  (Liu et al. 2021) . The authors involved in this study do not have any competing interests that could have influenced any part of the conduct of this research.",
      "page_start": 9,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In sum, we have demonstrated that the adoption of large language models for building approaches for tasks aimed at detecting humanitarian information, fake news, and emotion leads to systematically lower performance on non-English languages when compared to the performance on English. We discussed how such a disparity could lead to inequitable outcomes. Furthermore, we empirically show that including images via multimodal learning bridges this performance gap. Our experiments yield consistent insights on 3 different datasets and 5 non-English languages, indicating their generalizability. We also discussed the reliance of large guage models on pre-training corpus size and how adopting multimodal learning during fine-tuning stages can weaken this dependence, leading to a more consistent performance across all languages under consideration.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview ﬁgure. We use multimodal (image +",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustrative examples from considered multimodal datasets. We consider three classiﬁcation datasets for our experi-",
      "page": 3
    },
    {
      "caption": "Figure 2: presents some examples from the three",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of the adopted methodology. After using machine translation to obtain high-quality translations of the",
      "page": 5
    },
    {
      "caption": "Figure 3: provides an overview of our method.",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparing F1 scores on non-English and English text for both text-only and multimodal classiﬁers using monolingual",
      "page": 8
    },
    {
      "caption": "Figure 5: Comparing F1 scores on non-English and English text for both text-only and multimodal classiﬁers using multi-",
      "page": 8
    },
    {
      "caption": "Figure 4: (a)), 50.0% (0.04 →0.02;",
      "page": 8
    },
    {
      "caption": "Figure 4: (b)), and 28.6% (0.07 →0.05; Figure 4(c)) for cri-",
      "page": 8
    },
    {
      "caption": "Figure 5: (b)), and 50.0%",
      "page": 8
    },
    {
      "caption": "Figure 5: (c)) for crisis humanitarianism, fake",
      "page": 8
    },
    {
      "caption": "Figure 6: Relation between pre-training corpus size and clas-",
      "page": 9
    },
    {
      "caption": "Figure 6: (a), the relationship between the language-",
      "page": 9
    },
    {
      "caption": "Figure 6: (b), incorporating im-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Image-only and multimodal classfication performance. Performance of task-specific image-only classifiers (Row 1)",
      "data": [
        {
          "0.42 0.45 0.42 0.52": "0.73 0.74 0.72 0.82\n0.72 0.73 0.71 0.82\n0.71 0.72 0.69 0.81\n0.71 0.71 0.70 0.80\n0.70 0.69 0.70 0.80\n0.68 0.69 0.67 0.80",
          "0.15 0.54 0.09 0.81": "0.60 0.63 0.58 0.85\n0.59 0.63 0.57 0.85\n0.58 0.61 0.55 0.84\n0.59 0.60 0.58 0.84\n0.58 0.62 0.54 0.84\n0.56 0.61 0.51 0.84"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "en",
          "F1 score": "0.71",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "es",
          "F1 score": "0.64",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "fr",
          "F1 score": "0.69",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "pt\nzh\nhi\nen + img\nes + img",
          "F1 score": "0.67\n0.65\n0.63\n0.73\n0.72",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Inp\nen\nes",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "ut F1 score\n0.59\n0.54",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "fr + img\npt + img\nzh + img\nhi + img",
          "F1 score": "0.71\n0.71\n0.7\n0.68",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "fr\npt\nRMSDen = 0.0z h\nh i\nen",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "0.56\n0.57\n6 RM0.5S6\n0.54\n+ img 0.6",
          "Column_11": "Den = 0.",
          "Column_12": "",
          "Column_13": "03",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "RMSD",
          "Column_17": "",
          "Column_18": "zh 0.72\nhi 0.7\nen =e e n s 0 + + . i i m m0 g g4 0 0 R. . 8 8 5 2 MSD\nfr + img 0.81",
          "Column_19": "",
          "Column_20": "en = 0.02",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "RMSDen = 0.07",
          "Column_24": "",
          "Column_25": "RMSDen = 0.05",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "es\nfr +\npt +\nzh\nhi +",
          "Column_9": "",
          "Column_10": "+ img 0.59\nimg 0.58\nimg 0.59\n+ img 0.58\nimg 0.56",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "pt + img\nzh + im\nhi + img",
          "Column_19": "0.81\ng 0.8\n0.78",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Text-only (monoling",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "ual) Mu",
          "Column_11": "ltimodal",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "Text-only (m",
          "Column_17": "",
          "Column_18": "onolingual) Multi",
          "Column_19": "",
          "Column_20": "modal",
          "Column_21": "",
          "Column_22": "Tex",
          "Column_23": "t-only (monolingual)",
          "Column_24": "",
          "Column_25": "Multimodal",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "(a)CrisisHu",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "manitariani",
          "Column_11": "sm",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "(b)Fa",
          "Column_17": "",
          "Column_18": "keNewsDetection",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "(c)EmotionDetec",
          "Column_24": "",
          "Column_25": "tion",
          "Column_26": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input": "en",
          "F1 score": "0.7",
          "Column_3": "",
          "Column_4": "l",
          "Column_5": "",
          "Column_6": "angua",
          "Column_7": "gemodels.R",
          "Column_8": "",
          "Column_9": "",
          "1": "MSD de\nen",
          "Column_11": "notes",
          "Column_12": "",
          "Column_13": "thero",
          "Column_14": "",
          "Column_15": "ot-me",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "es",
          "F1 score": "0.62",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "1": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "fr",
          "F1 score": "0.68",
          "Column_3": "",
          "Column_4": "r",
          "Column_5": "",
          "Column_6": "espect",
          "Column_7": "tothethatof",
          "Column_8": "",
          "Column_9": "",
          "1": "thecorres",
          "Column_11": "pondi",
          "Column_12": "",
          "Column_13": "ngEn",
          "Column_14": "",
          "Column_15": "glish",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "pt",
          "F1 score": "0.66",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "1": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "zh\nhi\nen + img\nes + img\nfr + img\npt + img\nzh + img\nhi + img",
          "F1 score": "0.62\n0.47\n0.75\n0.75\n0.74\n0.76\n0.73\n0.64",
          "Column_3": "",
          "Column_4": "m",
          "Column_5": "",
          "Column_6": "onoli",
          "Column_7": "ngualtext-on\nInput\nen\nes\nfr\npt\nRMSDen = 0z .h 1\nh i\nen + i\nes + i\nfr + i",
          "Column_8": "",
          "Column_9": "",
          "1": "lymodels.\nF1 score\n0.61\n0.57\n0.58\n0.54\n1 R0M.54S\n0.43\nmg 0.61\nmg 0.6\nmg 0.58",
          "Column_11": "Den = 0.",
          "Column_12": "",
          "Column_13": "05",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "RMSDe",
          "Column_17": "",
          "Column_18": "en 0.77\nes 0.74\nfr 0.72\npt 0.71\nzh 0.69\nhi 0.64\nn = 0e.n0 + 9img R0M.8SD\nes + img 0.76\nfr + img 0.76\npt + img 0.77",
          "Column_19": "",
          "Column_20": "en = 0.08",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "RMSDen = 0.08",
          "Column_24": "",
          "Column_25": "RMSDen = 0.04",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "pt + i\nzh + i\nhi + i",
          "1": "mg 0.56\nmg 0.55\nmg 0.46",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "zh +\nhi +",
          "Column_19": "img 0.77\nimg 0.75",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Text-only (multiling",
          "Column_8": "",
          "Column_9": "",
          "1": "ual) M",
          "Column_11": "ultimodal",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "Text-only (m",
          "Column_17": "",
          "Column_18": "ultilingual) Mult",
          "Column_19": "",
          "Column_20": "imodal",
          "Column_21": "",
          "Column_22": "Tex",
          "Column_23": "t-only (multilingual)",
          "Column_24": "",
          "Column_25": "Multimodal",
          "Column_26": ""
        },
        {
          "Input": "",
          "F1 score": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "(a)CrisisHu",
          "Column_8": "",
          "Column_9": "",
          "1": "manitariani",
          "Column_11": "sm",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "(b)Fa",
          "Column_17": "",
          "Column_18": "keNewsDetection",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "(c)EmotionDetec",
          "Column_24": "",
          "Column_25": "tion",
          "Column_26": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The state of languages in the US: A statistical portrait",
      "year": "2016",
      "venue": "The state of languages in the US: A statistical portrait"
    },
    {
      "citation_id": "2",
      "title": "Deep learning using rectified linear units (ReLU)",
      "authors": [
        "A Agarap"
      ],
      "year": "2018",
      "venue": "Deep learning using rectified linear units (ReLU)",
      "arxiv": "arXiv:1803.08375"
    },
    {
      "citation_id": "3",
      "title": "Crisismmd: Multimodal twitter datasets from natural disasters",
      "authors": [
        "F Alam",
        "F Ofli",
        "M Imran"
      ],
      "year": "2018",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "4",
      "title": "It'sa Thin Line Between Love and Hate: Using the Echo in Modeling Dynamics of Racist Online Communities",
      "authors": [
        "E Arviv",
        "S Hanouna",
        "O Tsur"
      ],
      "year": "2021",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "5",
      "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings",
      "authors": [
        "C Basta",
        "M Costa-Jussà",
        "N Casas"
      ],
      "year": "2019",
      "venue": "Proc. of the Workshop on Gender Bias in NLP. Bender, E. 2019. The #BenderRule: On Naming the Languages We Study and Why It Matters"
    },
    {
      "citation_id": "6",
      "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
      "authors": [
        "E Bender",
        "T Gebru",
        "A Mcmillan-Major",
        "S Shmitchell"
      ],
      "year": "2021",
      "venue": "ACM FAccT"
    },
    {
      "citation_id": "7",
      "title": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping",
      "authors": [
        "R Caruana",
        "S Lawrence",
        "C Giles"
      ],
      "year": "2000",
      "venue": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping"
    },
    {
      "citation_id": "8",
      "title": "Spanish Pre-Trained BERT Model and Evaluation Data",
      "authors": [
        "J Cañete",
        "G Chaperon",
        "R Fuentes",
        "J.-H Ho",
        "H Kang",
        "J Pérez"
      ],
      "year": "2020",
      "venue": "Spanish Pre-Trained BERT Model and Evaluation Data"
    },
    {
      "citation_id": "9",
      "title": "More than Meets the Tie: Examining the Role of Interpersonal Relationships in Social Networks",
      "authors": [
        "M Choi",
        "C Budak",
        "D Romero",
        "D Jurgens"
      ],
      "year": "2021",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "10",
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "É Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "11",
      "title": "Content analysis of a live CDC Twitter chat during the 2014 Ebola outbreak",
      "authors": [
        "B Crook",
        "E Glowacki",
        "M Suran",
        "K Harris",
        "J Bernhardt"
      ],
      "year": "2016",
      "venue": "Comm'n. Res. Reports"
    },
    {
      "citation_id": "12",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "authors": [
        "Y Cui",
        "W Che",
        "T Liu",
        "B Qin",
        "S Wang",
        "G Hu"
      ],
      "year": "2020",
      "venue": "EMNLP (Findings)"
    },
    {
      "citation_id": "13",
      "title": "Social media as a measurement tool of depression in populations",
      "authors": [
        "M De Choudhury",
        "S Counts",
        "E Horvitz"
      ],
      "year": "2013",
      "venue": "Social media as a measurement tool of depression in populations"
    },
    {
      "citation_id": "14",
      "title": "Predicting depression via social media",
      "authors": [
        "M De Choudhury",
        "M Gamon",
        "S Counts",
        "E Horvitz"
      ],
      "year": "2013",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "15",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "16",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "17",
      "title": "Hindi BERT on HuggingFace",
      "authors": [
        "N Doiron"
      ],
      "year": "2020",
      "venue": "Hindi BERT on HuggingFace"
    },
    {
      "citation_id": "18",
      "title": "Multimodal classification for analysing social media",
      "authors": [
        "C Duong",
        "R Lebret",
        "K Aberer"
      ],
      "year": "2017",
      "venue": "Multimodal classification for analysing social media",
      "arxiv": "arXiv:1708.02099"
    },
    {
      "citation_id": "19",
      "title": "Our Grief is Unspeakable\": Automatically Measuring the Community Impact of a Tragedy",
      "authors": [
        "K Glasgow",
        "C Fink",
        "J Boyd-Graber"
      ],
      "year": "2014",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "20",
      "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
      "authors": [
        "M Hedderich",
        "L Lange",
        "H Adel",
        "J Strötgen",
        "D Klakow"
      ],
      "year": "2021",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "21",
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "authors": [
        "L Hendricks",
        "K Burns",
        "K Saenko",
        "T Darrell",
        "A Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "22",
      "title": "Validation of photographic food records in children: are pictures really worth a thousand words?",
      "authors": [
        "J Higgins",
        "A Lasalle",
        "P Zhaoxing",
        "M Kasten",
        "K Bing",
        "S Ridzon",
        "T Witten"
      ],
      "year": "2009",
      "venue": "Euro. J. of Clinical Nutrition"
    },
    {
      "citation_id": "23",
      "title": "Boundaries of cultural influence: Construct activation as a mechanism for cultural differences in social perception",
      "authors": [
        "Y.-Y Hong",
        "V Benet-Martinez",
        "C.-Y Chiu",
        "M Morris"
      ],
      "year": "2003",
      "venue": "Boundaries of cultural influence: Construct activation as a mechanism for cultural differences in social perception"
    },
    {
      "citation_id": "24",
      "title": "Social media and disasters: a functional framework for social media use in disaster planning",
      "authors": [
        "J Houston",
        "J Hawthorne",
        "M Perreault",
        "E Park",
        "M Goldstein Hode",
        "M Halliwell",
        "S Turner Mcgowen",
        "R Davis",
        "S Vaid",
        "J Mcelderry"
      ],
      "year": "2015",
      "venue": "Social media and disasters: a functional framework for social media use in disaster planning"
    },
    {
      "citation_id": "25",
      "title": "Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model",
      "authors": [
        "T Islam",
        "D Goldwasser"
      ],
      "year": "2021",
      "venue": "AAAI ICWSM"
    },
    {
      "citation_id": "26",
      "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
      "authors": [
        "P Joshi",
        "S Santy",
        "A Budhiraja",
        "K Bali",
        "M Choudhury"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "27",
      "title": "Marian: Fast Neural Machine Translation in C++",
      "authors": [
        "M Junczys-Dowmunt",
        "R Grundkiewicz",
        "T Dwojak",
        "H Hoang",
        "K Heafield",
        "T Neckermann",
        "F Seide",
        "U Germann",
        "A Fikri Aji",
        "N Bogoychev",
        "A Martins",
        "A Birch"
      ],
      "year": "2018",
      "venue": "ACL 2018"
    },
    {
      "citation_id": "28",
      "title": "Gaining insights from social media language: Methodologies and challenges",
      "authors": [
        "M Kern",
        "G Park",
        "J Eichstaedt",
        "H Schwartz",
        "M Sap",
        "L Smith",
        "L Ungar"
      ],
      "year": "2016",
      "venue": "Gaining insights from social media language: Methodologies and challenges"
    },
    {
      "citation_id": "29",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "30",
      "title": "Science",
      "authors": [
        "D Lazer",
        "M Baum",
        "Y Benkler",
        "A Berinsky",
        "K Greenhill",
        "F Menczer",
        "M Metzger",
        "B Nyhan",
        "G Pennycook",
        "D Rothschild"
      ],
      "venue": "Science"
    },
    {
      "citation_id": "31",
      "title": "A survey on text classification: From shallow to deep learning",
      "authors": [
        "Q Li",
        "H Peng",
        "J Li",
        "C Xia",
        "R Yang",
        "L Sun",
        "P Yu",
        "L He"
      ],
      "year": "2020",
      "venue": "A survey on text classification: From shallow to deep learning",
      "arxiv": "arXiv:2008.00364"
    },
    {
      "citation_id": "32",
      "title": "Visually Grounded Reasoning across Languages and Cultures",
      "authors": [
        "F Liu",
        "E Bugliarello",
        "E Ponti",
        "S Reddy",
        "N Collier",
        "D Elliott"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "33",
      "title": "Context-based quotation recommendation",
      "authors": [
        "A Maclaughlin",
        "T Chen",
        "B Ayan",
        "D Roth"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "34",
      "title": "CamemBERT: a Tasty French Language Model",
      "authors": [
        "L Martin",
        "B Muller",
        "P Ortiz Suárez",
        "Y Dupont",
        "L Romary",
        "É De La Clergerie",
        "D Seddah",
        "B Sagot"
      ],
      "year": "2019",
      "venue": "Neural Machine Translation Enabling Human Parity Innovations In the Cloud"
    },
    {
      "citation_id": "35",
      "title": "Language diversity in ACL 2004 -2016",
      "authors": [
        "S Mielke"
      ],
      "year": "2016",
      "venue": "Language diversity in ACL 2004 -2016"
    },
    {
      "citation_id": "36",
      "title": "The twitter of babel: Mapping world languages through microblogging platforms",
      "authors": [
        "D Mocanu",
        "A Baronchelli",
        "N Perra",
        "B Gonc ¸alves",
        "Q Zhang",
        "A Vespignani"
      ],
      "year": "2013",
      "venue": "PloS One"
    },
    {
      "citation_id": "37",
      "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
      "authors": [
        "B Muller",
        "A Anastasopoulos",
        "B Sagot",
        "D Seddah"
      ],
      "year": "2021",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "38",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "39",
      "title": "What the [mask]? making sense of language-specific BERT models",
      "authors": [
        "D Nozza",
        "F Bianchi",
        "D Hovy"
      ],
      "year": "2020",
      "venue": "What the [mask]? making sense of language-specific BERT models",
      "arxiv": "arXiv:2003.02912"
    },
    {
      "citation_id": "40",
      "title": "Analysis of Social Media Data using Multimodal Deep Learning for Disaster Response",
      "authors": [
        "F Ofli",
        "F Alam",
        "M Imran"
      ],
      "year": "2020",
      "venue": "Analysis of Social Media Data using Multimodal Deep Learning for Disaster Response",
      "arxiv": "arXiv:2004.11838"
    },
    {
      "citation_id": "41",
      "title": "Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates. PewResearch. 2018. Latinos and discrimination",
      "authors": [
        "J Pennebaker",
        "M Francis",
        "R Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates. PewResearch. 2018. Latinos and discrimination"
    },
    {
      "citation_id": "42",
      "title": "How Multilingual is Multilingual BERT",
      "authors": [
        "T Pires",
        "E Schlinger",
        "D Garrette"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "43",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
    },
    {
      "citation_id": "44",
      "title": "Med-BERT: pretrained contextualized embeddings on largescale structured electronic health records for disease prediction",
      "authors": [
        "L Rasmy",
        "Y Xiang",
        "Z Xie",
        "C Tao",
        "D Zhi"
      ],
      "year": "2021",
      "venue": "NPJ Digital Med"
    },
    {
      "citation_id": "45",
      "title": "Is a picture is worth a thousand words? Creating effective questionnaires with pictures",
      "authors": [
        "L Reynolds",
        "R Johnson"
      ],
      "year": "2011",
      "venue": "Is a picture is worth a thousand words? Creating effective questionnaires with pictures"
    },
    {
      "citation_id": "46",
      "title": "Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "47",
      "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
      "authors": [
        "T Sellam",
        "S Yadlowsky",
        "J Wei",
        "N Saphra",
        "A D'amour",
        "T Linzen",
        "J Bastings",
        "I Turc",
        "J Eisenstein",
        "D Das"
      ],
      "year": "2021",
      "venue": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
      "arxiv": "arXiv:2106.16163"
    },
    {
      "citation_id": "48",
      "title": "Measuring personal values in cross-cultural user-generated content",
      "authors": [
        "Y Shen",
        "S Wilson",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Int. Conf. on Social Informatics"
    },
    {
      "citation_id": "49",
      "title": "FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for",
      "authors": [
        "K Shu",
        "D Mahudeswaran",
        "S Wang",
        "D Lee",
        "H Liu"
      ],
      "year": "2018",
      "venue": "Studying Fake News on Social Media",
      "arxiv": "arXiv:1809.01286"
    },
    {
      "citation_id": "50",
      "title": "Fake News Detection on Social Media: A Data Mining Perspective",
      "authors": [
        "K Shu",
        "A Sliva",
        "S Wang",
        "J Tang",
        "H Liu"
      ],
      "year": "2017",
      "venue": "ACM SIGKDD Explorations Newsletter"
    },
    {
      "citation_id": "51",
      "title": "Spotfake+: A multimodal framework for fake news detection via transfer learning (student abstract)",
      "authors": [
        "K Simonyan",
        "A Zisserman",
        "S Singhal",
        "A Kabra",
        "M Sharma",
        "R Shah",
        "T Chakraborty",
        "P Kumaraguru"
      ],
      "year": "2015",
      "venue": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "52",
      "title": "Spotfake: A multi-modal framework for fake news detection",
      "authors": [
        "S Singhal",
        "R Shah",
        "T Chakraborty",
        "P Kumaraguru",
        "S Satoh"
      ],
      "year": "2019",
      "venue": "IEEE Int. Cont. on Multimedia Big Data (BigMM)"
    },
    {
      "citation_id": "53",
      "title": "BERTimbau: pretrained BERT models for Brazilian Portuguese",
      "authors": [
        "F Souza",
        "R Nogueira",
        "R Lotufo"
      ],
      "year": "2020",
      "venue": "9th Brazilian Conference on Intelligent Systems BRACIS"
    },
    {
      "citation_id": "54",
      "title": "Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
      "authors": [
        "E Strubell",
        "A Ganesh",
        "A Mccallum",
        "L Huang",
        "X Qiu"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "55",
      "title": "Emotioninduced engagement in internet video advertisements",
      "authors": [
        "T Teixeira",
        "M Wedel",
        "R Pieters"
      ],
      "year": "2012",
      "venue": "J. of Marketing Res"
    },
    {
      "citation_id": "56",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "57",
      "title": "Are All Languages Created Equal in Multilingual BERT?",
      "authors": [
        "S Wu",
        "M Dredze"
      ],
      "year": "2020",
      "venue": "Proc. of the Workshop on Representation Learning for NLP"
    },
    {
      "citation_id": "58",
      "title": "A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "authors": [
        "L Xue",
        "N Constant",
        "A Roberts",
        "M Kale",
        "R Al-Rfou",
        "A Siddhant",
        "A Barua",
        "C Raffel"
      ],
      "year": "2021",
      "venue": "NAACL-HLT"
    }
  ]
}