{
  "paper_id": "2508.03780v1",
  "title": "Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition",
  "published": "2025-08-05T13:29:29Z",
  "authors": [
    "Katharina Hoedt",
    "Arthur Flexer",
    "Gerhard Widmer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "One of the desired key properties of deep learning models is the ability to generalise to unseen samples. When provided with new samples that are (perceptually) similar to one or more training samples, deep learning models are expected to produce correspondingly similar outputs. Models that succeed in predicting similar outputs for similar inputs are often called robust. Deep learning models, on the other hand, have been shown to be highly vulnerable to minor (adversarial) perturbations of the input, which manage to drastically change a model's output and simultaneously expose its reliance on spurious correlations. In this work, we investigate whether inherently interpretable deep models, i.e., deep models that were designed to focus more on meaningful and interpretable features, are more robust to irrelevant perturbations in the data, compared to their black-box counterparts. We test our hypothesis by comparing the robustness of an interpretable and a black-box music emotion recognition model when challenged with adversarial examples. Furthermore, we include an adversarially trained model, which is optimised to be more robust, in the comparison. Our results indicate that inherently more interpretable emotion recognition models can indeed be more robust than their black-box counterparts, and achieve similar levels of robustness as adversarially trained models, at lower computational cost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In Music Information Retrieval (MIR), as in any other application domain of machine learning, a central desideratum is that learned models should generalise, that is, make correct predictions on new observations outside of the training data. A special aspect of this is robustness: the ability of a model to make similar predictions when presented with similar inputs. In particular, we expect a good model to be robust against irrelevant changes in its input. However, it has previously been shown that most traditional, black-box deep learning models are not robust in this sense. Instead, they are vulnerable to imperceptible perturbations of the inputs  [1] , which can drastically change the predictions and hence the performance. While this issue was first discussed in the image domain  [1] , it has similarly been found to be a relevant problem in other domains in which deep learning became dominant, such as speech  [2]  and MIR  [3] . Generally, the relationship between (adversarial) robustness and the ability to generalise has been shown to be a complex one  [4] .\n\nIn this work, we hypothesise that deep learning models that are inherently interpretable, i.e., models that by design focus more on interpretable features, may also be inherently more robust against minor alterations of the input. Specifically, we look into a variant of a concept bottleneck model  [5] . Here, predictions are not obtained directly from an input via complex computations, but instead are based on human-understandable concepts (here: so-called midlevel features), which are extracted with the help of a bottleneck layer. Based on these concepts, the final class predictions are obtained via a linear layer, resulting in more transparent model decisions as we can track which concept contributed to which extent towards a prediction. We test the robustness of an inherently interpretable model subsequently by focusing on a specific task -music emotion recognition from audio -and using an adversarial attack as a probing tool. We compare the robustness of traditional, black-box models, of their adversarially trained versions, and of an inherently interpretable model from the recent MIR literature. As the model treats emotion recognition as a regression task, we also need to adapt an existing attack method to fit a regression setting. We use this adversarial attack to investigate to what extent it manages to distort the performance of the different types of models. Our results indicate that the inherently interpretable model is substantially more robust than a black-box model, and even exhibits similar robustness as models that have been deliberately adversarially trained. This documents -beyond the usefulness of interpretability per se -another advantage of inherently interpretable models in MIR, calling for more research in this direction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Linking Interpretability and Robustness. In what follows, we hypothesise about a connection between interpretability and adversarial robustness. Previously, it was shown that robust models lead to explanations (here: saliency maps) that are easier to comprehend than nonrobust models (e.g.,  [6] ). An idea similar to ours, yet with a focus on post-hoc interpretability, is an adversarial defence based on the promotion of model interpretations that are robust to adversarial perturbations  [7] . Also, different studies observed that models trained to present interpretable gradients (e.g., via some type of regularisation  [8]  or saliency-guided training  [9] ) result in more robust models. Here, the main difference from our hypothesis is that we do not require interpretable gradients, but look at models that were designed to focus on human-interpretable features during training. Somewhat related, input modifications have been used to gain insights about the learned embedding space of deep MIR models  [10] . The work most similar to ours investigates the same hypothesis as we argue for, and look into the robustness of an inherently interpretable deep image classifier  [11] . The setup is similar to ours, as the robustness of a standard (black-box) Convolutional Neural Network (CNN), an inherently interpretable model, and adversarially trained variants are compared, but in the image domain. Our work can therefore be seen as an extension of these tests to a different domain.\n\nInherently Interpretable Deep Learning Models. The topic of inherently interpretable deep models is still mostly in early stages of development in various domains. In the image domain, prototype-based models (e.g.,  [12] ) were introduced as systems that learn to represent prototypes (i.e., (parts of) representative inputs of a class), and base their predictions on these representations. Related, concept bottleneck models  [13]  try to extract (human-interpretable) concepts from input data, on which the final predictions are based. Concepts can here be predefined and learned, e.g., by using corresponding annotations if available  [13] , or extracted automatically, e.g., by learning concepts in a self-supervised way during training  [14] . The original idea of concept bottleneck models  [13]  is very similar to the model we will look at in our experiments  [5] .\n\nThe work we build upon in this paper was one of the first approaches in MIR where a model was designed so that its predictions relied on human-understandable concepts  [5] . Next to this, systems have been proposed that base their predictions on automatically learned prototypes, or meaningful transformations thereof  [15, 16] . A different approach previously proposed is the idea to train a classifier and an interpreter simultaneously to develop an interpretable system  [17] . Another avenue investigated in MIR is the use of (self-)attention for interpretability purposes, e.g.,  [18] , but the assumptions underlying this type of approach have been questioned  [19] . Beyond the scope of classification, a deep music generation system that is controllable via interpretable latent factors (chord and texture) was proposed in the past  [20] .\n\nAdversarial Attacks in Regression Tasks. Most of the literature concerning adversarial attacks in any domain is focused on systems trying to solve a classification task, e.g., image classification, emotion classification, or tasks like speech-to-text  [2, 3, [21] [22] [23] . Significantly less work covers regression tasks. One idea is to take an existing classification attack (e.g., Fast Gradient Sign Method (FGSM)  [21] , Projected Gradient Descent (PGD)  [22]  or Basic Iterative Method (BIM)  [24] ) and use it to maximise the error between a system's output and its target  [25, 26] . In our subsequent experiments, we follow a similar approach, and apply BIM  [24]  to maximise the prediction error of an emotion recognition system. Other approaches include using a sensitivity analysis to formulate an adversarial regression attack  [27] , or utilising the Jacobian of the function learned by a system to compute adversarial perturbations  [28] .\n\nAdversarial Defences. To make systems more robust against adversarial input perturbations, various methods have been proposed. They often aim at either trying to detect these adversarial perturbations, or reduce them  [29] . One of the most notable methods is adversarial training  [22] , during which adversarially perturbed input samples are included in the training routine of a model, to improve its robustness. Due to the high computational complexity of adversarial training, also various adaptations were proposed, e.g., with a particular focus on efficiency  [30, 31] . In the field of MIR, there has also been some early work on adversarial training  [32] . In this work, we will use an adversarially trained system as a baseline regarding the robustness against adversarial attacks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "In this section, we elaborate on the methods we use to investigate our hypothesis. The focus is a model proposed for the task of emotion recognition  [5] , which uses a bottleneck to learn human-interpretable (\"mid-level\") features, before deriving the final emotion prediction. We consider this to be an \"inherently interpretable MIR model\" because its very design -specifically, the bottleneck structure that forces the model to encode its predictions in terms of human-interpretable concepts -is targeted towards interpretability. In the following paragraphs, we briefly summarise the data used to train and test this model, the model itself, and some variations thereof. We also discuss the adversarial attack with which we test the robustness of the investigated models, as well as adversarial training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Data",
      "text": "The emotion recognition system by Chowdhury et al.  [5]  was trained on two different datasets. The first one, Soundtracks (Stimulus Set 1)  [33] , contains 360 excerpts originating from 110 movie soundtracks. The annotations consist of expert ratings for five discrete emotion categories (anger, fear, sadness, happiness, and tenderness) and three additional categories following a dimensional emotion model (valence, energy, tension). All eight emotion annotations are subsequently used as targets to solve multiple (i.e., eight) regression tasks. The ratings are real numbers and range from 1.0 to 7.83. The song excerpts have an average length of ‚âà 17 seconds. The Soundtracks dataset contains the core emotion information that we want to learn from, which is why it is used to train all model variants that are discussed subsequently.\n\nThe second dataset is the Mid-Level Features Dataset  [34] , which includes human-interpretable feature annotations by means of seven mid-level descrip- tors (melodiousness, articulation, rhythmic and tonal stability, rhythmic complexity, dissonance, and modality / majorness). It contains ratings by musicians for each mid-level descriptor (range: 1 to 10) for a total of 5,000 song snippets (‚âà 15 seconds). This dataset is subsequently used to train a bottleneck on human-interpretable feature annotations within the interpretable deep model.\n\nNote that all 360 song excerpts of the Soundtracks dataset are also included in the Mid-Level Features dataset. Therefore, the mid-level ground-truth annotations are available for all song excerpts that we try to predict emotions for.\n\nThe inputs of the recognition systems are logarithmicscaled spectrograms. To compute them, we follow the preprocessing steps proposed by Chowdhury et al.  [5] . First, audio is resampled to 22.05 kHz, and converted to a mono signal. Then, we choose 10 arbitrary seconds for each audio excerpt. From these, we compute spectrograms (frame size = 2048 and hop size = 705, leading to 31.25 frames per second, number of filters per octave = 24) which are amplitude-normalised before logarithmic scaling.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Recognition Systems",
      "text": "Chowdhury et al. propose multiple system variants, which are all based on a VGG-style CNN architecture  [5] . The authors compare systems where emotions are predicted directly from the input spectrogram, and more interpretable counterparts, in which mid-level features are predicted first. For this type of model, the final emotion prediction is derived from the predicted mid-level features with a linear layer, maintaining a more transparent prediction decision.\n\nA2E. The first system we look at is the \"A2E\" variant  [5] . This represents a traditional, black-box model that predicts emotions for the input directly based on (logarithmically scaled) input spectrograms. During training, each input is fed through multiple convolutional and pooling layers, and the output is optimised to predict eight different emotions. This model variant is trained on the Soundtracks dataset, and a Mean Squared Error (MSE) loss is used to optimise for the emotion annotations. Figure  1  depicts a schematic representation of this type of model. A2M2E. The second variant we look at was originally proposed as \"A2Mid2E-Joint\"  [5]  and will be called \"A2M2E\" for brevity in what follows. This architecture is the same as for A2E except the last linear (classification) layer. Instead of this, a linear bottleneck layer is introduced, with the number of neurons corresponding to the number of mid-level features we would like to learn (here: seven). During training, this bottleneck layer is optimised to predict mid-level features (with an MSE loss on the Mid-Level Features dataset), and its output is fed into a second linear layer, optimised to predict (eight) emotion annotations (with MSE on the Soundtracks dataset). More precisely, to train the A2M2E model, both the predictions on mid-level features and on emotions are optimised simultaneously. This is done by first scaling the two losses (with a factor 0.5), and minimising their sum. In the following, the A2M2E model will be our inherently interpretable model, as it is designed to have a linear mapping from mid-level feature predictions -which are assumed to be humaninterpretable -to final emotion predictions. A schematic view of this kind of network is shown in Figure  2 .\n\nA2B2E. Next to the A2M2E bottleneck model, we train a second model with the same (bottleneck) architecture, as depicted in Figure  2 . For this second bottleneck model, we leave the bottleneck architecture as is, but remove the loss for optimising the mid-level features, and only learn to predict the emotion annotations. This model will be called A2B2E in what follows, and is again a traditional, blackbox model (as it directly predicts emotions based on input spectrograms) trained only on the Soundtracks dataset, yet with the same architecture as the interpretable A2M2E model. With this model, we will try to rule out any differences in robustness between our black-box and inherently interpretable model(s) solely being due to differences in their architectures (as A2E and A2M2E differ slightly).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attacking The Emotion Recognition System",
      "text": "To test the robustness of the previously described models, we utilise adversarial attacks at test time. These allow us to compute marginal perturbations for input data, and determine how far they can change the output of a trained system. Specifically, we can assess the robustness or vulnerability of a model by looking at the performance of the model on the test inputs after completed training (i.e., performance before an attack), computing perturbations for the test data, and comparing the performance before the attack to the performance on the modified test inputs (i.e., performance after the attack).\n\nIn contrast to previous adversarial attacks in MIR, where mostly classification systems were targeted (e.g.,  [3, 35] ), the models at hand perform multiple regression tasks by predicting various emotion annotations. This requires us to rephrase previously introduced attacks. In particular, we formulate an untargeted attack, meaning that instead of changing the desired output of a model from one prediction to another (i.e., a target), we want to get as far away as possible from the available ground-truth emotion annotations. At the same time, the perturbed samples are restricted to lie within an ùúñ-ball around the original samples, so that we can ensure a certain degree of similarity.\n\nFor the untargeted attack, we adapt BIM  [24] . This method was introduced as an iterative variant of FGSM  [21] , and is also similar to a method that was introduced as PGD attack  [22]  (with one of the main differences being a random vs. zero initialisation for the perturbation). Intuitively, BIM computes an adversarial perturbation by iteratively going into the gradient direction that maximises a model's loss w.r.t. the ground-truth. More precisely, let ùëì be a model (e.g., our emotion recognition system), and ùêø the loss function it was trained with (e.g., MSE). Then, let us denote an input by ùë• ‚àà R ùëÜ , the ground-truth (emotion) annotation by ùë¶ ‚àà R ùëå , and the additive adversarial perturbation by ùõø ‚àà R ùëÜ (i.e., making an adversarial example x = ùë• + ùõø). We compute ùõø ùëíùëù+1 , i.e., the perturbation of the next iteration, as\n\n(1) Here, ‚àá ùõø is the gradient w.r.t. ùõø, and updates are scaled with multiplicative factor ùúÇ and performed based on the sign of the gradient. Furthermore, we clip each perturbation to stay in the range of [-ùúñ, ùúñ]. We initialise ùõø 0 = 0.\n\nThis adapted regression attack is iterative, i.e., we perform updates to ùõø multiple times. We do so, until 1) a number of maximum iterations is carried out, or 2) we reach a certain threshold either on the current MSE -which is also the loss function used during training of the modelor on the average correlation between prediction and target emotion. More specifically, we perform experiments with both the MSE as well as the correlation threshold, and stop updating ùõø ahead of the maximum number of iterations if a pre-defined upper / lower limit is reached.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Adversarial Training To Improve Robustness",
      "text": "Adversarial training is one of the most prominent methods to increase a deep learning model's robustness  [22, 30, 31] . The main idea of adversarial training is to include adversarial examples in the training data, to decrease a model's sensitivity against small alterations of clean input data (i.e., adversarial perturbations). More precisely, for a network ùëì defined via the parameters ùúÉ and trained with loss ùêø, adversarial training aims at solving the saddle point problem  [22]  min\n\nHere, (ùë•, ùë¶) ‚àº ùíü denotes the data that a system ùëì is trained on, and ùêµ(ùë•, ùúñ) is the set of allowed adversarial perturbations ùõø for a data point ùë•, i.e., ùêµ(ùë•, ùúñ) = {ùõø ‚àà R ùëÜ | ‚Äñùõø‚Äñ ùëù ‚â§ ùúñ}. Intuitively, this means we train a model to have a low loss on the training data, which can, however, be perturbed by an adversarial attack before it is fed into the system.\n\nIn what follows, we obtain our robust models by optimising on a clean batch first, perturbing said batch by means of an adversarial attack, and then optimising on the according adversarial batch. We do so every ùëõ ùë°‚Ñé epoch of the training process. In our experiments, the perturbations for adversarial training are computed with the adapted BIM method. We use \"aA2E\" and \"aA2B2E\" to denote the robust version of models A2E and A2B2E, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first describe our overall experimental setup and the performance of previously discussed emotion recognition systems. Thereafter, we present and discuss our main findings. To support the reproduction of the following experiments, the code is available via Github.  1",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "In the subsequent experiments, we follow Chowdhury et al. when training the emotion recognition systems  [5] , with minor adaptions. We use a training, validation, and a testset with 80%-10%-10% of the data (in contrast to Chowdhury et al., who only use a training and validation-set  [5] ). We do this to have a better approximation of the final model performance  [36] . As we adopt the hyperparameters used in the original work  [5] , we refrain from using e.g., cross-validation to estimate the prediction error for hyperparameter tuning, and instead use a fixed training and validation-set. We repeat the training of the models 10 times for different random initialisations of the networks. Furthermore, we take Adam as an optimiser to minimise the loss (or combined loss, in case of A2M2E) for training, and a learning rate of 0.0005  [5] . The batch size for training is 8, and we train for at most 200 epochs, with early stopping being used against overfitting (patience of 50).\n\nTo find suitable hyperparameters for the adversarial attack on the system, we perform a grid-search on the validation data. We choose a hyperparameter setting that trades off a high Mean Absolute Error (MAE) and low correlation between annotations and labels after an attack. This leads to a number of maximal iterations of 1, 000, ùúñ = 0.001, ùúÇ = 0.002, and a lower correlation threshold of -1. A repetition of the attack on one particular model (for multiple random seeds) is not necessary; due to the lack of a source of randomness for the adversarial attack (e.g., randomly sampled targets), the attack is deterministic.\n\nIn the case of adversarial training, we opt to train on adversarial batches every five epochs, which in preliminary experiments led to a suitable trade-off between performance on clean data, and adversarial robustness. We use the same attack parameters as for the attack on the systems, yet with a maximal number of 50 iterations.\n\nRegarding the evaluation of model performance, we look at two measures, computed on the test-set. First, we state the correlation between emotion annotations and model predictions, as was done by Chowdhury et al.  [5] , to determine the performance of the emotion recognition models. Table  1 . Performance of emotion recognition models reported by  [5]  (orig.) and our reproduction, as well as two adversarially trained models. Our results given as mean ¬± standard deviation over 10 runs on test data. Note that original results (*) are computed on a validation-set, while ours are reported on a test-set.\n\nAdditionally, we have a look at the MAE, as we cannot guarantee a linear relation between emotion annotations and predictions (particularly on adversarial samples; see also Section 4.4), making MAE a better suited metric to capture the performance (cf.  [37] ).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition Results",
      "text": "We now look at the performance of our reproduction on the task of emotion prediction on the Soundtracks dataset.\n\nIn Table  1 , we summarise the performance with Pearson's correlation coefficient, as done in the original work  [5] . Specifically, we list the overall average (i.e., over 8 targets and 10 runs) correlation between emotion annotations and predictions. The first two entries of the table show the performance reported by Chowdhury et al.  [5] , as the average over ten random runs (i.e., running on 10 different training and validation splits). Entries 3-5 in Table  1  show our reproduction results as the average and standard deviation over ten runs, with different random initialisations of the networks. We also add the performance of adversarially trained variants of the previous networks in the final two entries of the table. Our results are reported on the test-set. We achieve almost on-par performance of the A2E model compared to the original work  [5]  for the average label correlation (difference of 0.03). However, for the interpretable A2M2E variant, this difference is larger (0.08), which can likely be attributed to our performance estimate being on a hold-out test-set, as opposed to a (more biased) crossvalidation estimate (i.e., on validation-sets).  2 In addition to the label correlation, we also look at the MAE as a performance measure. The MAE is, across all our standard training settings, between 0.10 and 0.11, with low standard deviation for different runs. This performance will be used as a point of reference to measure the robustness of the models in subsequent experiments.\n\nIn the last two entries of Table  1  we show the results of adversarially trained versions of the black-box mod-   [38] , we observe a slightly reduced performance of the more robust models on the clean test-set, in terms of average correlation, compared to the black-box variants. For the MAE, the performance remains at 0.10, and a low (here: 0.00) standard deviation across the ten random runs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Robustness Of Black-Box Vs. Interpretable Models",
      "text": "After looking at the performance of the different types of models (i.e., black-box, adversarially trained, and inherently interpretable) in the previous section, we now look at their robustness against adversarial attacks, by computing the performance after such an attack (i.e., on the modified test data). In our experiments, we look at the MAE of the models. We expect robust models to remain at a lower MAE, and less robust models to exhibit higher MAEs after an attack than before (i.e., on the clean test data).\n\nIn Figure  3 , we show the difference in performance of our models before and after an attack, subsequently denoted by ‚àÜMAE. Note here that the performance before an attack is rather similar for all models (Table  1 ). The ‚àÜMAE is computed across the ten random runs again. The first and third box-plot represent the losses in performance for the black-box models; the second and fourth box-plots for their respective adversarially trained versions. Finally, the last box-plot depicts the losses in performance for the inherently interpretable model across ten runs.\n\nFigure  3  shows, first of all, that the two black-box models A2E and A2B2E have the highest (median) loss in performance. They also exhibit the largest variations between different training runs, yet with a clear tendency to be more susceptible to input perturbations. The figure also shows the benefit of including adversarial examples during training, as the two adversarially trained models aA2E and aA2B2E appear distinctly more robust than their black-box counterparts. Also the variations across different runs are lower. Remarkably enough, the performance (loss) of the inherently interpretable model, shown in the last boxplot of Figure  3 , is much more similar to the adversarially trained networks. It tends to have much lower MAE than the black-box models after an attack, without the need for a supplementary addition of adversarially perturbed data during training. Note that preliminary experiments show, that the robustness of the concept bottleneck model could be further improved via adversarial training, yet with an even greater loss in performance on clean data. We also run a two-sided t-test with a Bonferroni correction to validate the experiments, using a significance threshold of ùõº = 0.05/2 (correcting for two hypothesis tests). This test shows that the difference in robustness between both black-box models and the interpretable model is significant (A2E vs. A2M2E: t(359)=13.04, p=0.00; A2B2E vs. A2M2E: t(359)=12.10, p=0.00).\n\nNote that another aspect of adversarial perturbations, namely their perceptibility, is largely neglected here, except for the restrictions of staying within an ùúñ-ball around the original samples. This is done as our main focus is to determine the overall susceptibility of the models to small input perturbations. However, looking at the approximate signal-to-noise ratio over all perturbed samples, we find comparable levels of perceptibility across the different models (‚âà 32-37dB, cf.  [35] , where samples with around 40dB were moderately perceptible).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A Closer Look At The Predictions",
      "text": "In this section, we look more closely at some individual predictions made by our models. Figure  4  contrasts true emotion annotations (x-axis) and predictions (y-axis) of different models, for two of the eight predicted emotions (\"tension\" and \"tender\"). Depicted are predictions both on original, clean test data (\"x\"), and on adversarially perturbed data (\"o\"). Note that the ground truth emotion annotations (horizontal axes) are strictly confined to the range [0, 1]; the emotion recognition model that we are using  [5] , however, as a regression model, does not restrict the predicted numeric output to any particular range. Zooming in on the [0, 1] range on the y-axis (see inserts in the individual plots), we see that the models' predictions for the original data stay well within the desired range, while the perturbed samples lead the models to output wildly different numbers.\n\nMoreover, while the original predictions appear to have mostly linear relationships with the ground-truth annotations, the adversarial predictions exhibit less thereof, justifying our choice of using the MAE instead of the correlation when comparing the robustness of different models.\n\nFrom Figure  4 , we also see that for the emotions of tension and tender, for the black-box model A2B2E (left), the predictions on adversarially perturbed data appear a lot further from the original predictions than for both the interpretable model (A2M2E, middle) and the adversarially trained version (aA2B2E, right), which again illustrates the latter models' higher robustness. Interestingly, for emotions with a lot of very low ground-truth annotations (e.g., tender), the adversarial perturbations tend to lead to widely spread out predictions. We also note that for other emotions (e.g., anger, energy) predictions on adversarial data look more similar across different models, suggesting that different emotions are easier / harder to perturb.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "In this paper, we investigated whether an inherently more interpretable system could provide a more robust solution to MIR tasks. We looked at an emotion recognition system designed to encode its predictions in terms of human-understandable concepts, and compared its robustness against adversarial input perturbations to the robustness of a traditional, black-box model. We found that this kind of concept bottleneck model can present a more robust alternative to a black-box model. In fact, it can even exhibit robustness similar to adversarially trained models -which are specifically trained to be more robust against adversarial input alterations -yet without the cost of needing to compute adversarial examples during training. An obvious limitation of this set of experiments is that it considered only one single model and recognition task so far. This calls for broader and more systematic analyses to further support the proposed hypothesis, and provide more insights into the complex connection between interpretability and robustness. As there has only been a limited amount of research in MIR on interpretable models (e.g.,  [5, 15, 16] ), we additionally hope to see more work in this direction.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic A2E (black-box) model. During train-",
      "page": 3
    },
    {
      "caption": "Figure 1: depicts a schematic",
      "page": 3
    },
    {
      "caption": "Figure 2: Schematic A2M2E (inherently interpretable)",
      "page": 3
    },
    {
      "caption": "Figure 2: A2B2E. Next to the A2M2E bottleneck model, we train",
      "page": 3
    },
    {
      "caption": "Figure 2: For this second bottleneck model,",
      "page": 3
    },
    {
      "caption": "Figure 3: Model robustness, illustrated by impact of ad-",
      "page": 5
    },
    {
      "caption": "Figure 3: , we show the difference in performance of our",
      "page": 5
    },
    {
      "caption": "Figure 3: shows, first of all, that the two black-box mod-",
      "page": 5
    },
    {
      "caption": "Figure 3: , is much more similar to the adversarially",
      "page": 6
    },
    {
      "caption": "Figure 4: contrasts true",
      "page": 6
    },
    {
      "caption": "Figure 4: , we also see that for the emotions of ten-",
      "page": 6
    },
    {
      "caption": "Figure 4: True emotion annotation (x-axis) vs. predictions (y-axis) for two different emotions (tension and tender), and",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "2LIT AI Lab, Linz Institute of Technology, Austria"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "ABSTRACT"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": ""
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "One of\nthe desired key properties of deep learning mod-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "els is the ability to generalise to unseen samples. When"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "provided with new samples\nthat are (perceptually)\nsimi-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "lar to one or more training samples, deep learning models"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "are expected to produce correspondingly similar outputs."
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "Models that succeed in predicting similar outputs for sim-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "ilar inputs are often called robust. Deep learning models,"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "on the other hand, have been shown to be highly vulnera-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "ble to minor (adversarial) perturbations of the input, which"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "manage to drastically change a model‚Äôs output and simulta-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "neously expose its reliance on spurious correlations. In this"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "work, we investigate whether inherently interpretable deep"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "models, i.e., deep models that were designed to focus more"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "on meaningful and interpretable features, are more robust"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "to irrelevant perturbations in the data, compared to their"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "black-box counterparts. We test our hypothesis by com-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "paring the robustness of an interpretable and a black-box"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "music emotion recognition model when challenged with"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "adversarial examples. Furthermore, we include an adver-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "sarially trained model, which is optimised to be more ro-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "bust,\nin the comparison. Our\nresults indicate that\ninher-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "ently more interpretable emotion recognition models can"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "indeed be more robust\nthan their black-box counterparts,"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "and achieve similar\nlevels of\nrobustness as adversarially"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "trained models, at lower computational cost."
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": ""
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": ""
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "1.\nINTRODUCTION"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": ""
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "In Music Information Retrieval (MIR), as in any other ap-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "plication domain of machine learning, a central desider-"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "atum is\nthat\nlearned models\nshould generalise,\nthat\nis,"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "make correct predictions on new observations outside of"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "the training data. A special aspect of\nthis is robustness:"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "the ability of a model\nto make similar predictions when"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "presented with similar\ninputs.\nIn particular, we expect a"
        },
        {
          "1Institute of Computational Perception, Johannes Kepler University, Linz, Austria": "good model\nto be robust against\nirrelevant changes in its"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "fence based on the promotion of model interpretations that",
          "the error between a system‚Äôs output and its target [25, 26].": "In our\nsubsequent experiments, we follow a similar ap-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "are robust\nto adversarial perturbations\n[7].\nAlso, differ-",
          "the error between a system‚Äôs output and its target [25, 26].": "proach, and apply BIM [24]\nto maximise the prediction"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "ent studies observed that models trained to present\ninter-",
          "the error between a system‚Äôs output and its target [25, 26].": "error of an emotion recognition system. Other approaches"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "pretable gradients (e.g., via some type of regularisation [8]",
          "the error between a system‚Äôs output and its target [25, 26].": "include using a sensitivity analysis to formulate an adver-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "or saliency-guided training [9]) result in more robust mod-",
          "the error between a system‚Äôs output and its target [25, 26].": "sarial\nregression attack [27], or utilising the Jacobian of"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "els. Here,\nthe main difference from our hypothesis is that",
          "the error between a system‚Äôs output and its target [25, 26].": "the function learned by a system to compute adversarial"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "we do not require interpretable gradients, but look at mod-",
          "the error between a system‚Äôs output and its target [25, 26].": "perturbations [28]."
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "els that were designed to focus on human-interpretable fea-",
          "the error between a system‚Äôs output and its target [25, 26].": "Adversarial Defences.\nTo make systems more robust"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "tures during training.\nSomewhat\nrelated,\ninput modifica-",
          "the error between a system‚Äôs output and its target [25, 26].": "against adversarial\ninput perturbations, various methods"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "tions have been used to gain insights about the learned em-",
          "the error between a system‚Äôs output and its target [25, 26].": "have been proposed. They often aim at either trying to de-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "bedding space of deep MIR models [10]. The work most",
          "the error between a system‚Äôs output and its target [25, 26].": "tect\nthese adversarial perturbations, or\nreduce them [29]."
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "similar to ours investigates the same hypothesis as we ar-",
          "the error between a system‚Äôs output and its target [25, 26].": "train-\nOne of\nthe most notable methods\nis adversarial"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "gue for, and look into the robustness of an inherently inter-",
          "the error between a system‚Äôs output and its target [25, 26].": "ing [22], during which adversarially perturbed input sam-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "pretable deep image classifier [11]. The setup is similar to",
          "the error between a system‚Äôs output and its target [25, 26].": "ples are included in the training routine of a model,\nto"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "ours, as the robustness of a standard (black-box) Convolu-",
          "the error between a system‚Äôs output and its target [25, 26].": "improve its\nrobustness.\nDue to the high computational"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "tional Neural Network (CNN), an inherently interpretable",
          "the error between a system‚Äôs output and its target [25, 26].": "complexity of\nadversarial\ntraining,\nalso various\nadapta-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "model, and adversarially trained variants are compared, but",
          "the error between a system‚Äôs output and its target [25, 26].": "tions were proposed, e.g., with a particular focus on effi-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "in the image domain. Our work can therefore be seen as an",
          "the error between a system‚Äôs output and its target [25, 26].": "ciency [30, 31].\nIn the field of MIR,\nthere has also been"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "extension of these tests to a different domain.",
          "the error between a system‚Äôs output and its target [25, 26].": "some early work on adversarial training [32]. In this work,"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "we will use an adversarially trained system as a baseline"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "Inherently Interpretable Deep Learning Models. The",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "regarding the robustness against adversarial attacks."
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "topic of inherently interpretable deep models is still mostly",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "in early stages of development\nin various domains.\nIn the",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "image domain, prototype-based models (e.g.,\n[12]) were",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "3. METHODS"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "introduced as\nsystems\nthat\nlearn to represent prototypes",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "(i.e.,\n(parts of)\nrepresentative inputs of a class), and base",
          "the error between a system‚Äôs output and its target [25, 26].": "In this section, we elaborate on the methods we use to in-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "their predictions on these representations. Related, concept",
          "the error between a system‚Äôs output and its target [25, 26].": "vestigate our hypothesis. The focus is a model proposed"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "bottleneck models [13] try to extract (human-interpretable)",
          "the error between a system‚Äôs output and its target [25, 26].": "for the task of emotion recognition [5], which uses a bottle-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "concepts from input data, on which the final predictions",
          "the error between a system‚Äôs output and its target [25, 26].": "neck to learn human-interpretable (‚Äúmid-level‚Äù)\nfeatures,"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "are based. Concepts can here be predefined and learned,",
          "the error between a system‚Äôs output and its target [25, 26].": "before deriving the final emotion prediction. We consider"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "e.g., by using corresponding annotations if available [13],",
          "the error between a system‚Äôs output and its target [25, 26].": "this\nto be\nan ‚Äúinherently interpretable MIR model‚Äù be-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "or extracted automatically, e.g., by learning concepts in a",
          "the error between a system‚Äôs output and its target [25, 26].": "cause its very design ‚Äî specifically,\nthe bottleneck struc-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "self-supervised way during training [14]. The original idea",
          "the error between a system‚Äôs output and its target [25, 26].": "ture that forces the model to encode its predictions in terms"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "of concept bottleneck models [13]\nis very similar\nto the",
          "the error between a system‚Äôs output and its target [25, 26].": "of human-interpretable concepts ‚Äî is targeted towards in-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "model we will look at in our experiments [5].",
          "the error between a system‚Äôs output and its target [25, 26].": "terpretability. In the following paragraphs, we briefly sum-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "marise the data used to train and test this model, the model"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "The work we build upon in this paper was one of\nthe",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "itself, and some variations thereof. We also discuss the ad-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "first approaches in MIR where a model was designed so",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "versarial attack with which we test\nthe robustness of\nthe"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "that\nits predictions relied on human-understandable con-",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "investigated models, as well as adversarial training."
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "cepts [5]. Next\nto this, systems have been proposed that",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "base their predictions on automatically learned prototypes,",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "3.1 The Data"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "or meaningful transformations thereof [15,16]. A different",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "approach previously proposed is the idea to train a classi-",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "The emotion recognition system by Chowdhury et al. [5]"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "fier and an interpreter simultaneously to develop an inter-",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "was trained on two different datasets. The first one, Sound-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "pretable system [17]. Another avenue investigated in MIR",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "tracks (Stimulus Set 1)\n[33], contains 360 excerpts orig-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "is the use of\n(self-)attention for\ninterpretability purposes,",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "inating from 110 movie\nsoundtracks.\nThe\nannotations"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "e.g., [18], but\nthe assumptions underlying this type of ap-",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "consist of\nexpert\nratings\nfor five discrete\nemotion cate-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "proach have been questioned [19]. Beyond the scope of",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "gories (anger, fear, sadness, happiness, and tenderness) and"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "classification, a deep music generation system that is con-",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "three additional categories following a dimensional emo-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "trollable via interpretable latent factors (chord and texture)",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "tion model\n(valence, energy,\ntension). All eight emotion"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "was proposed in the past [20].",
          "the error between a system‚Äôs output and its target [25, 26].": ""
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "",
          "the error between a system‚Äôs output and its target [25, 26].": "annotations are subsequently used as targets to solve mul-"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "Adversarial Attacks in Regression Tasks. Most of the",
          "the error between a system‚Äôs output and its target [25, 26].": "tiple\n(i.e.,\neight)\nregression tasks.\nThe\nratings\nare\nreal"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "literature concerning adversarial attacks in any domain is",
          "the error between a system‚Äôs output and its target [25, 26].": "numbers and range from 1.0 to 7.83.\nThe song excerpts"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "focused on systems\ntrying to solve a classification task,",
          "the error between a system‚Äôs output and its target [25, 26].": "have an average length of ‚âà 17 seconds. The Soundtracks"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "e.g.,\nimage classification, emotion classification, or\ntasks",
          "the error between a system‚Äôs output and its target [25, 26].": "dataset contains the core emotion information that we want"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "like speech-to-text\n[2, 3, 21‚Äì23].\nSignificantly less work",
          "the error between a system‚Äôs output and its target [25, 26].": "to learn from, which is why it\nis used to train all model"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "covers\nregression tasks.\nOne\nidea\nis\nto take\nan exist-",
          "the error between a system‚Äôs output and its target [25, 26].": "variants that are discussed subsequently."
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "ing classification attack (e.g., Fast Gradient Sign Method",
          "the error between a system‚Äôs output and its target [25, 26].": "Mid-Level\nFeatures\nThe\nsecond\ndataset\nis\nthe"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "(FGSM)\n[21], Projected Gradient Descent\n(PGD)\n[22] or",
          "the error between a system‚Äôs output and its target [25, 26].": "Dataset\n[34],\nwhich\nincludes\nhuman-interpretable"
        },
        {
          "a focus on post-hoc interpretability,\nis an adversarial de-": "Basic Iterative Method (BIM) [24]) and use it to maximise",
          "the error between a system‚Äôs output and its target [25, 26].": "feature annotations by means of seven mid-level descrip-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "ing, only output is optimised to predict emotions."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "tors\n(melodiousness,\narticulation,\nrhythmic\nand\ntonal"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "stability,\nrhythmic complexity, dissonance, and modality"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "/ majorness).\nIt contains\nratings by musicians\nfor each"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "mid-level descriptor\n(range:\n1 to 10)\nfor a total of 5,000"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "song snippets (‚âà 15 seconds). This dataset is subsequently"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "used to train a bottleneck on human-interpretable feature"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "annotations within the interpretable deep model."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "Note that all 360 song excerpts of the Soundtracks dataset"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "are also included in the Mid-Level Features dataset. There-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "fore,\nthe mid-level ground-truth annotations are available"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "for all song excerpts that we try to predict emotions for."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "The inputs of\nthe recognition systems are logarithmic-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "scaled spectrograms. To compute them, we follow the pre-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "processing steps proposed by Chowdhury et al. [5]. First,"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "audio is resampled to 22.05 kHz, and converted to a mono"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "signal. Then, we choose 10 arbitrary seconds for each au-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "dio excerpt. From these, we compute spectrograms (frame"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "size = 2048 and hop size = 705,\nleading to 31.25 frames"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "per second, number of filters per octave = 24) which are"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "amplitude-normalised before logarithmic scaling."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "3.2 The Recognition Systems"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "Chowdhury et al. propose multiple system variants, which"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "are all based on a VGG-style CNN architecture [5]. The"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "authors compare systems where emotions are predicted di-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "rectly from the input spectrogram, and more interpretable"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "counterparts,\nin which mid-level\nfeatures\nare predicted"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "first. For this type of model, the final emotion prediction is"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "derived from the predicted mid-level features with a linear"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "layer, maintaining a more transparent prediction decision."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "A2E. The first system we look at is the ‚ÄúA2E‚Äù variant [5]."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": ""
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "This represents a traditional, black-box model that predicts"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "emotions for\nthe input directly based on (logarithmically"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "scaled) input spectrograms. During training, each input\nis"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "fed through multiple convolutional and pooling layers, and"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "the output is optimised to predict eight different emotions."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "This model variant\nis trained on the Soundtracks dataset,"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "and a Mean Squared Error (MSE) loss is used to optimise"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "for the emotion annotations. Figure 1 depicts a schematic"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "representation of this type of model."
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "A2M2E. The\nsecond\nvariant we\nlook\nat was\norigi-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "nally proposed as ‚ÄúA2Mid2E-Joint‚Äù [5] and will be called"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "‚ÄúA2M2E‚Äù for brevity in what follows. This architecture is"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "the same as for A2E except\nthe last\nlinear (classification)"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "layer.\nInstead of\nthis, a linear bottleneck layer\nis\nintro-"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "duced, with the number of neurons corresponding to the"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "number of mid-level features we would like to learn (here:"
        },
        {
          "Figure 1. Schematic A2E (black-box) model. During train-": "seven). During training,\nthis bottleneck layer is optimised"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "we formulate an untargeted attack, meaning that instead of": "changing the desired output of a model from one prediction",
          "In what follows, we obtain our robust models by optimis-": "ing on a clean batch first, perturbing said batch by means"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "to another (i.e., a target), we want to get as far away as pos-",
          "In what follows, we obtain our robust models by optimis-": "of an adversarial attack, and then optimising on the accord-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "sible from the available ground-truth emotion annotations.",
          "In what follows, we obtain our robust models by optimis-": "ing adversarial batch. We do so every ùëõùë°‚Ñé epoch of\nthe"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "At\nthe same time,\nthe perturbed samples are restricted to",
          "In what follows, we obtain our robust models by optimis-": "training process.\nIn our experiments,\nthe perturbations for"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "lie within an ùúñ‚àíball around the original samples, so that",
          "In what follows, we obtain our robust models by optimis-": "adversarial\ntraining are computed with the adapted BIM"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "we can ensure a certain degree of similarity.",
          "In what follows, we obtain our robust models by optimis-": "method. We use ‚ÄúaA2E‚Äù and ‚ÄúaA2B2E‚Äù to denote the ro-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "For\nthe untargeted attack, we\nadapt BIM [24].\nThis",
          "In what follows, we obtain our robust models by optimis-": "bust version of models A2E and A2B2E, respectively."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "method\nwas\nintroduced\nas\nan\niterative\nvariant\nof",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "FGSM [21], and is also similar to a method that was intro-",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "4. EXPERIMENTS"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "duced as PGD attack [22] (with one of the main differences",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "being a random vs. zero initialisation for the perturbation).",
          "In what follows, we obtain our robust models by optimis-": "In this section, we first describe our overall experimental"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "Intuitively, BIM computes an adversarial perturbation by",
          "In what follows, we obtain our robust models by optimis-": "setup and the performance of previously discussed emo-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "iteratively going into the gradient direction that maximises",
          "In what follows, we obtain our robust models by optimis-": "tion recognition systems. Thereafter, we present and dis-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "a model‚Äôs loss w.r.t. the ground-truth. More precisely, let ùëì",
          "In what follows, we obtain our robust models by optimis-": "cuss our main findings. To support the reproduction of the"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "be a model (e.g., our emotion recognition system), and ùêø",
          "In what follows, we obtain our robust models by optimis-": "following experiments, the code is available via Github. 1"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "the loss function it was trained with (e.g., MSE). Then, let",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "us denote an input by ùë• ‚àà RùëÜ, the ground-truth (emotion)",
          "In what follows, we obtain our robust models by optimis-": "4.1 Experimental Setup"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "annotation by ùë¶ ‚àà Rùëå , and the additive adversarial per-",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "In the subsequent experiments, we follow Chowdhury et"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "turbation by ùõø ‚àà RùëÜ (i.e., making an adversarial example",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "al. when training the emotion recognition systems [5], with"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "i.e.,\nthe perturbation of\nùë• = ùë• + ùõø). We compute ùõøùëíùëù+1,",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "minor adaptions. We use a training, validation, and a test-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "the next iteration, as",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "set with 80%-10%-10% of the data (in contrast to Chowd-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "ùõøùëíùëù+1 = clipùúñ(ùõøùëíùëù + ùúÇ * sign(‚àáùõøùëíùëù ùêø(ùëì (ùë• + ùõøùëíùëù), ùë¶))).",
          "In what follows, we obtain our robust models by optimis-": "hury et al., who only use a training and validation-set [5])."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "(1)",
          "In what follows, we obtain our robust models by optimis-": "We do this\nto have\na better\napproximation of\nthe final"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "is the gradient w.r.t. ùõø, and updates are scaled\nHere, ‚àáùõø",
          "In what follows, we obtain our robust models by optimis-": "model performance [36]. As we adopt\nthe hyperparam-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "with multiplicative factor ùúÇ and performed based on the",
          "In what follows, we obtain our robust models by optimis-": "eters used in the original work [5], we refrain from us-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "sign of the gradient. Furthermore, we clip each perturba-",
          "In what follows, we obtain our robust models by optimis-": "ing e.g., cross-validation to estimate the prediction error"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "tion to stay in the range of [‚àíùúñ, ùúñ]. We initialise ùõø0 = 0.",
          "In what follows, we obtain our robust models by optimis-": "for hyperparameter tuning, and instead use a fixed training"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "This adapted regression attack is iterative,\ni.e., we per-",
          "In what follows, we obtain our robust models by optimis-": "and validation-set. We repeat the training of the models 10"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "form updates to ùõø multiple times. We do so, until 1) a num-",
          "In what follows, we obtain our robust models by optimis-": "times for different random initialisations of the networks."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "ber of maximum iterations is carried out, or 2) we reach a",
          "In what follows, we obtain our robust models by optimis-": "Furthermore, we take Adam as an optimiser\nto minimise"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "certain threshold either on the current MSE ‚Äî which is",
          "In what follows, we obtain our robust models by optimis-": "the loss (or combined loss, in case of A2M2E) for training,"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "also the loss function used during training of the model ‚Äî",
          "In what follows, we obtain our robust models by optimis-": "and a learning rate of 0.0005 [5]. The batch size for train-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "or on the average correlation between prediction and target",
          "In what follows, we obtain our robust models by optimis-": "ing is 8, and we train for at most 200 epochs, with early"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "emotion. More specifically, we perform experiments with",
          "In what follows, we obtain our robust models by optimis-": "stopping being used against overfitting (patience of 50)."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "both the MSE as well as the correlation threshold, and stop",
          "In what follows, we obtain our robust models by optimis-": "To find suitable hyperparameters for\nthe adversarial at-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "updating ùõø ahead of the maximum number of iterations if",
          "In what follows, we obtain our robust models by optimis-": "tack on the system, we perform a grid-search on the valida-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "a pre-defined upper / lower limit is reached.",
          "In what follows, we obtain our robust models by optimis-": "tion data. We choose a hyperparameter setting that\ntrades"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "off a high Mean Absolute Error (MAE) and low correlation"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "3.4 Adversarial Training to Improve Robustness",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "between annotations and labels after an attack. This leads"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "to a number of maximal\niterations of 1, 000, ùúñ = 0.001,"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "Adversarial training is one of the most prominent methods",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "ùúÇ = 0.002, and a lower correlation threshold of -1. A rep-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "to increase a deep learning model‚Äôs robustness [22, 30, 31].",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "etition of the attack on one particular model (for multiple"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "The main idea of adversarial\ntraining is to include adver-",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "random seeds) is not necessary; due to the lack of a source"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "sarial examples in the training data,\nto decrease a model‚Äôs",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "of\nrandomness for\nthe adversarial attack (e.g.,\nrandomly"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "sensitivity against small alterations of clean input data (i.e.,",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "sampled targets), the attack is deterministic."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "adversarial perturbations). More precisely,\nfor a network",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "In the case of adversarial\ntraining, we opt\nto train on"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "ùëì\ndefined via the parameters ùúÉ and trained with loss ùêø,",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "adversarial batches every five epochs, which in prelimi-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "adversarial\ntraining aims at solving the saddle point prob-",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "nary experiments led to a suitable trade-off between per-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "lem [22]",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "formance on clean data, and adversarial\nrobustness. We"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "[Ô∏Ç\n]Ô∏Ç",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "use the same attack parameters as for the attack on the sys-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "min\nmax\nùêø(ùëì (ùë• + ùõø), ùë¶)\n.\n(2)\nE(ùë•,ùë¶)‚àºùíü",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "ùúÉ\nùõø‚ààùêµ(ùë•,ùúñ)",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "tems, yet with a maximal number of 50 iterations."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "Regarding the evaluation of model performance, we look"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "Here,\n(ùë•, ùë¶) ‚àº ùíü denotes the data that a system ùëì\nis",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "at\ntwo measures, computed on the test-set. First, we state"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "trained on, and ùêµ(ùë•, ùúñ) is the set of allowed adversarial",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "the correlation between emotion annotations and model"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "perturbations ùõø for a data point ùë•,\ni.e., ùêµ(ùë•, ùúñ) = {ùõø ‚àà",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "predictions, as was done by Chowdhury et al. [5], to deter-"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "Intuitively,\nthis means we train a model\nRùëÜ | ‚Äñùõø‚Äñùëù ‚â§ ùúñ}.",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "mine the performance of the emotion recognition models."
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "to have a low loss on the training data, which can, however,",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "be perturbed by an adversarial attack before it\nis fed into",
          "In what follows, we obtain our robust models by optimis-": ""
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "",
          "In what follows, we obtain our robust models by optimis-": "1 https://github.com/CPJKU/robustness_mer_"
        },
        {
          "we formulate an untargeted attack, meaning that instead of": "the system.",
          "In what follows, we obtain our robust models by optimis-": "bottleneck"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance of emotion recognition models re-",
      "data": [
        {
          "System": "A2E (orig.)",
          "Avg Corr.": "0.76*",
          "MAE": "-"
        },
        {
          "System": "A2M2E (orig.)",
          "Avg Corr.": "0.75*",
          "MAE": "-"
        },
        {
          "System": "A2E",
          "Avg Corr.": "0.73 ¬± 0.02",
          "MAE": "0.10 ¬± 0.01"
        },
        {
          "System": "A2B2E",
          "Avg Corr.": "0.70 ¬± 0.02",
          "MAE": "0.11 ¬± 0.01"
        },
        {
          "System": "A2M2E",
          "Avg Corr.": "0.67 ¬± 0.02",
          "MAE": "0.11 ¬± 0.01"
        },
        {
          "System": "aA2E",
          "Avg Corr.": "0.67 ¬± 0.03",
          "MAE": "0.10 ¬± 0.00"
        },
        {
          "System": "aA2B2E",
          "Avg Corr.": "0.66 ¬± 0.03",
          "MAE": "0.10 ¬± 0.00"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Performance of emotion recognition models re-",
      "data": [
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "aA2B2E\n0.66 ¬± 0.03\n0.10 ¬± 0.00"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "Table 1.\nPerformance of emotion recognition models re-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "ported by [5] (orig.) and our reproduction, as well as two"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "adversarially trained models. Our\nresults given as mean"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "¬± standard deviation over 10 runs on test data. Note that"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "original results (*) are computed on a validation-set, while"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "ours are reported on a test-set."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "Additionally, we have a look at\nthe MAE, as we cannot"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "guarantee a linear\nrelation between emotion annotations"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "and predictions (particularly on adversarial samples;\nsee"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "also Section 4.4), making MAE a better suited metric to"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "capture the performance (cf. [37])."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "4.2 Emotion Recognition Results"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "We now look at\nthe performance of our\nreproduction on"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "the task of emotion prediction on the Soundtracks dataset."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "In Table 1, we summarise the performance with Pearson‚Äôs"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "correlation coefficient, as done in the original work [5]."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "Specifically, we list the overall average (i.e., over 8 targets"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "and 10 runs) correlation between emotion annotations and"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "predictions. The first two entries of the table show the per-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "formance reported by Chowdhury et al. [5], as the average"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "over\nten random runs (i.e.,\nrunning on 10 different\ntrain-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "ing and validation splits). Entries 3-5 in Table 1 show our"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "reproduction results as the average and standard deviation"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "over\nten runs, with different\nrandom initialisations of\nthe"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "networks. We also add the performance of adversarially"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "trained variants of\nthe previous networks in the final\ntwo"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "entries of the table. Our results are reported on the test-set."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "We achieve almost on-par performance of the A2E model"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "compared to the original work [5] for the average label cor-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "relation (difference of 0.03). However, for the interpretable"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "A2M2E variant, this difference is larger (0.08), which can"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "likely be attributed to our performance estimate being on"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "a hold-out\ntest-set, as opposed to a (more biased) cross-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "validation estimate (i.e., on validation-sets). 2"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "In addition to the label correlation, we also look at\nthe"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "MAE as a performance measure.\nThe MAE is,\nacross"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "all our standard training settings, between 0.10 and 0.11,"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "with low standard deviation for different runs. This perfor-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "mance will be used as a point of reference to measure the"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "robustness of the models in subsequent experiments."
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "In the last\ntwo entries of Table 1 we show the results"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "of adversarially trained versions of\nthe black-box mod-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "2 Note that when recreating the data-splits in [5], we achieve higher"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "performing models, and a notably larger standard deviation between dif-"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": ""
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "ferent runs (e.g., A2E: 0.88 ¬± 0.07, A2M2E: 0.80 ¬± 0.06), supporting"
        },
        {
          "aA2E\n0.67 ¬± 0.03\n0.10 ¬± 0.00": "this assumption."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "box counterparts. Also the variations across different runs": "are lower. Remarkably enough,\nthe performance (loss) of",
          "tender), the adversarial perturbations tend to lead to widely": "spread out predictions. We also note that\nfor other emo-"
        },
        {
          "box counterparts. Also the variations across different runs": "the inherently interpretable model, shown in the last box-",
          "tender), the adversarial perturbations tend to lead to widely": "tions (e.g., anger, energy) predictions on adversarial data"
        },
        {
          "box counterparts. Also the variations across different runs": "plot of Figure 3, is much more similar to the adversarially",
          "tender), the adversarial perturbations tend to lead to widely": "look more similar across different models, suggesting that"
        },
        {
          "box counterparts. Also the variations across different runs": "trained networks.\nIt\ntends to have much lower MAE than",
          "tender), the adversarial perturbations tend to lead to widely": "different emotions are easier / harder to perturb."
        },
        {
          "box counterparts. Also the variations across different runs": "the black-box models after an attack, without the need for",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "a supplementary addition of adversarially perturbed data",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "5. CONCLUSION AND DISCUSSION"
        },
        {
          "box counterparts. Also the variations across different runs": "during training. Note that preliminary experiments show,",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "that\nthe robustness of the concept bottleneck model could",
          "tender), the adversarial perturbations tend to lead to widely": "In this paper, we investigated whether an inherently more"
        },
        {
          "box counterparts. Also the variations across different runs": "be further\nimproved via adversarial\ntraining, yet with an",
          "tender), the adversarial perturbations tend to lead to widely": "interpretable\nsystem could provide\na more\nrobust\nsolu-"
        },
        {
          "box counterparts. Also the variations across different runs": "even greater loss in performance on clean data.",
          "tender), the adversarial perturbations tend to lead to widely": "tion to MIR tasks. We\nlooked at\nan emotion recogni-"
        },
        {
          "box counterparts. Also the variations across different runs": "We also run a two-sided t-test with a Bonferroni cor-",
          "tender), the adversarial perturbations tend to lead to widely": "tion system designed to encode its predictions in terms of"
        },
        {
          "box counterparts. Also the variations across different runs": "rection to validate the experiments, using a significance",
          "tender), the adversarial perturbations tend to lead to widely": "human-understandable concepts, and compared its robust-"
        },
        {
          "box counterparts. Also the variations across different runs": "threshold of ùõº = 0.05/2 (correcting for\ntwo hypothesis",
          "tender), the adversarial perturbations tend to lead to widely": "ness against adversarial\ninput perturbations to the robust-"
        },
        {
          "box counterparts. Also the variations across different runs": "tests). This test shows that the difference in robustness be-",
          "tender), the adversarial perturbations tend to lead to widely": "ness of a traditional, black-box model. We found that\nthis"
        },
        {
          "box counterparts. Also the variations across different runs": "tween both black-box models and the interpretable model",
          "tender), the adversarial perturbations tend to lead to widely": "kind of concept bottleneck model can present a more ro-"
        },
        {
          "box counterparts. Also the variations across different runs": "is\nsignificant\n(A2E vs. A2M2E:\nt(359)=13.04,\np=0.00;",
          "tender), the adversarial perturbations tend to lead to widely": "bust alternative to a black-box model.\nIn fact,\nit can even"
        },
        {
          "box counterparts. Also the variations across different runs": "A2B2E vs. A2M2E: t(359)=12.10, p=0.00).",
          "tender), the adversarial perturbations tend to lead to widely": "exhibit robustness similar to adversarially trained models"
        },
        {
          "box counterparts. Also the variations across different runs": "Note\nthat\nanother\naspect\nof\nadversarial\nperturbations,",
          "tender), the adversarial perturbations tend to lead to widely": "‚Äî which are specifically trained to be more robust against"
        },
        {
          "box counterparts. Also the variations across different runs": "namely their perceptibility,\nis largely neglected here, ex-",
          "tender), the adversarial perturbations tend to lead to widely": "adversarial\ninput\nalterations ‚Äî yet without\nthe\ncost of"
        },
        {
          "box counterparts. Also the variations across different runs": "cept for the restrictions of staying within an ùúñ-ball around",
          "tender), the adversarial perturbations tend to lead to widely": "needing to compute adversarial examples during training."
        },
        {
          "box counterparts. Also the variations across different runs": "the original samples.\nThis is done as our main focus is",
          "tender), the adversarial perturbations tend to lead to widely": "An obvious limitation of\nthis set of experiments is that"
        },
        {
          "box counterparts. Also the variations across different runs": "to determine the overall\nsusceptibility of\nthe models\nto",
          "tender), the adversarial perturbations tend to lead to widely": "it considered only one single model and recognition task"
        },
        {
          "box counterparts. Also the variations across different runs": "small input perturbations. However, looking at the approx-",
          "tender), the adversarial perturbations tend to lead to widely": "so far. This calls for broader and more systematic analy-"
        },
        {
          "box counterparts. Also the variations across different runs": "imate signal-to-noise ratio over all perturbed samples, we",
          "tender), the adversarial perturbations tend to lead to widely": "ses to further support the proposed hypothesis, and provide"
        },
        {
          "box counterparts. Also the variations across different runs": "find comparable levels of perceptibility across the different",
          "tender), the adversarial perturbations tend to lead to widely": "more insights into the complex connection between inter-"
        },
        {
          "box counterparts. Also the variations across different runs": "models (‚âà 32‚àí37dB, cf. [35], where samples with around",
          "tender), the adversarial perturbations tend to lead to widely": "pretability and robustness. As there has only been a lim-"
        },
        {
          "box counterparts. Also the variations across different runs": "40dB were moderately perceptible).",
          "tender), the adversarial perturbations tend to lead to widely": "ited amount of\nresearch in MIR on interpretable models"
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "(e.g., [5,15,16]), we additionally hope to see more work in"
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "this direction."
        },
        {
          "box counterparts. Also the variations across different runs": "4.4 A Closer Look at the Predictions",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "In this section, we look more closely at some individual",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "Acknowledgments"
        },
        {
          "box counterparts. Also the variations across different runs": "predictions made by our models.\nFigure 4 contrasts true",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "This research was funded in part by the Austrian Science"
        },
        {
          "box counterparts. Also the variations across different runs": "emotion annotations\n(x-axis) and predictions\n(y-axis) of",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "Fund\n(FWF)\n[10.55776/AR821\nand\n10.55776/P36653]."
        },
        {
          "box counterparts. Also the variations across different runs": "different models,\nfor two of the eight predicted emotions",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "GW‚Äôs work is supported by the European Research Coun-"
        },
        {
          "box counterparts. Also the variations across different runs": "(‚Äútension‚Äù and ‚Äútender‚Äù).\nDepicted are predictions both",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "cil\n(ERC)\nunder\nthe EU‚Äôs Horizon\n2020\nresearch\nand"
        },
        {
          "box counterparts. Also the variations across different runs": "on original, clean test data (‚Äúx‚Äù), and on adversarially per-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "innovation programme, grant\nagreement No 101019375"
        },
        {
          "box counterparts. Also the variations across different runs": "turbed data (‚Äúo‚Äù). Note that the ground truth emotion anno-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "(Whither Music?)."
        },
        {
          "box counterparts. Also the variations across different runs": "tations (horizontal axes) are strictly confined to the range",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "[0, 1]; the emotion recognition model that we are using [5],",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "however, as a regression model, does not restrict\nthe pre-",
          "tender), the adversarial perturbations tend to lead to widely": "6. REFERENCES"
        },
        {
          "box counterparts. Also the variations across different runs": "dicted numeric output\nto any particular\nrange.\nZooming",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "[1] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-"
        },
        {
          "box counterparts. Also the variations across different runs": "in on the [0, 1] range on the y-axis (see inserts in the indi-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "han, I. J. Goodfellow, and R. Fergus, ‚ÄúIntriguing Prop-"
        },
        {
          "box counterparts. Also the variations across different runs": "vidual plots), we see that\nthe models‚Äô predictions for\nthe",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "the 2nd Int.\nerties of Neural Networks,‚Äù\nin Proc. of"
        },
        {
          "box counterparts. Also the variations across different runs": "original data stay well within the desired range, while the",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "Conf. on Learning Representations, ICLR, 2014."
        },
        {
          "box counterparts. Also the variations across different runs": "perturbed samples lead the models to output wildly differ-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "ent numbers.",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "[2] N. Carlini and D. A. Wagner, ‚ÄúAudio Adversarial Ex-"
        },
        {
          "box counterparts. Also the variations across different runs": "Moreover, while the original predictions appear to have",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "amples: Targeted Attacks on Speech-to-Text,‚Äù in Proc."
        },
        {
          "box counterparts. Also the variations across different runs": "mostly linear\nrelationships with the ground-truth annota-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "of the IEEE Security and Privacy Workshops, SP Work-"
        },
        {
          "box counterparts. Also the variations across different runs": "tions, the adversarial predictions exhibit less thereof, justi-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "shops.\nIEEE, 2018, pp. 1‚Äì7."
        },
        {
          "box counterparts. Also the variations across different runs": "fying our choice of using the MAE instead of the correla-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "tion when comparing the robustness of different models.",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "[3] C. Kereliuk, B. L. Sturm, and J. Larsen, ‚ÄúDeep Learn-"
        },
        {
          "box counterparts. Also the variations across different runs": "From Figure 4, we also see that for the emotions of ten-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "ing, Audio Adversaries,\nand Music Content Analy-"
        },
        {
          "box counterparts. Also the variations across different runs": "sion and tender,\nfor\nthe black-box model A2B2E (left),",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "sis,‚Äù in Proc. of the IEEE Workshop on Applications of"
        },
        {
          "box counterparts. Also the variations across different runs": "the predictions on adversarially perturbed data appear a lot",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "Signal Processing to Audio and Acoustics, WASPAA."
        },
        {
          "box counterparts. Also the variations across different runs": "further from the original predictions than for both the in-",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "",
          "tender), the adversarial perturbations tend to lead to widely": "IEEE, 2015, pp. 1‚Äì5."
        },
        {
          "box counterparts. Also the variations across different runs": "terpretable model (A2M2E, middle) and the adversarially",
          "tender), the adversarial perturbations tend to lead to widely": ""
        },
        {
          "box counterparts. Also the variations across different runs": "trained version (aA2B2E, right), which again illustrates the",
          "tender), the adversarial perturbations tend to lead to widely": "[4] D. Stutz, M. Hein, and B. Schiele, ‚ÄúDisentangling Ad-"
        },
        {
          "box counterparts. Also the variations across different runs": "latter models‚Äô higher\nrobustness.\nInterestingly,\nfor emo-",
          "tender), the adversarial perturbations tend to lead to widely": "versarial Robustness and Generalization,‚Äù in Proc. of"
        },
        {
          "box counterparts. Also the variations across different runs": "tions with a lot of very low ground-truth annotations (e.g.,",
          "tender), the adversarial perturbations tend to lead to widely": "the 2019 IEEE Conf. on Computer Vision and Pattern"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "three different models. Leftmost plot (A2B2E) presents predictions of black-box model, middle column (A2M2E) inher-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "ently interpretable model, and rightmost plot\n(aA2B2E) shows predictions of adversarially trained model. Depicted are"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "predictions on original (clean) data (‚Äúx‚Äù), and adversarially perturbed data (‚Äúo‚Äù)."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "in Applied Mathematics\nRecognition, CVPR.\nComputer Vision Foundation /\nsic Embeddings,‚Äù Frontiers"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "IEEE, 2019, pp. 6976‚Äì6987.\nand Statistics, vol. 5, p. 53, 2019."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[5] S. Chowdhury, A. Vall, V. Haunschmid, and G. Wid-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[11] B. Rasheed, M. Abdelhamid, A. Khan,\nI. Menezes,"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "mer, ‚ÄúTowards Explainable Music Emotion Recogni-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "and A. M. Khattak, ‚ÄúExploring the Impact of Concep-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "tion: The Route via Mid-level Features,‚Äù in Proc. of"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "tual Bottlenecks on Adversarial Robustness of Deep"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "the 20th Int. Society for Music Information Retrieval"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Neural Networks,‚Äù IEEE Access, vol. 12, pp. 131 323‚Äì"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Conf., ISMIR, 2019, pp. 237‚Äì243."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "131 335, 2024."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[6] C. Etmann, S. Lunz, P. Maass, and C. Sch¬®onlieb, ‚ÄúOn"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[12] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin,\nand"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "the Connection Between Adversarial Robustness and"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "J. Su, ‚ÄúThis Looks Like That: Deep Learning for\nIn-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "the 36th\nSaliency Map Interpretability,‚Äù\nin Proc. of"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "terpretable Image Recognition,‚Äù in Advances in Neu-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Int. Conf. on Machine Learning,\nICML, ser. Proc. of"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "ral\nInformation Processing Systems 32: Annu. Conf."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Machine Learning Research, vol. 97, 2019, pp. 1823‚Äì"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "on Neural\nInformation Processing Systems, NeurIPS,"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "1832."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "2019, pp. 8928‚Äì8939."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[7] A. Boopathy,\nS. Liu, G. Zhang, C. Liu,\nP. Chen,"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[13] P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann,\nS. Chang,\nand L. Daniel,\n‚ÄúProper Network\nInter-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "E. Pierson, B. Kim, and P. Liang, ‚ÄúConcept Bottleneck\npretability Helps Adversarial Robustness in Classifica-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "the 37th Int. Conf. on Machine\nModels,‚Äù in Proc. of\ntion,‚Äù in Proc. of the 37th Int. Conf. on Machine Learn-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Learning,\nICML, ser. Proceedings of Machine Learn-\ning, ICML, ser. Proceedings of Machine Learning Re-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "ing Research, vol. 119, 2020, pp. 5338‚Äì5348.\nsearch, vol. 119, 2020, pp. 1014‚Äì1023."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[8] A. Noack, I. Ahern, D. Dou, and B. Li, ‚ÄúAn Empirical"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[14] B. Wang, L. Li, Y. Nakashima,\nand H. Nagahara,"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Study on the Relation Between Network Interpretabil-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "‚ÄúLearning Bottleneck Concepts\nin Image Classifica-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "SN Computer Sci-\nity and Adversarial Robustness,‚Äù"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "tion,‚Äù in Proc. of the 2023 IEEE / CVF Conf. on Com-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "ence, vol. 2, no. 1, p. 32, 2021."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "puter Vision and Pattern Recognition, CVPR.\nCom-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "puter Vision Foundation /\nIEEE, 2023,\npp. 10 962‚Äì"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[9] A. Guesmi, N. S. Aswani, and M. Shafique, ‚ÄúExplor-"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "10 971."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "ing the Interplay of Interpretability and Robustness in"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "Deep Neural Networks: A Saliency-guided Approach,‚Äù"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[15] P. Zinemanas, M. Rocamora, M. Miron, F. Font, and"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "CoRR, vol. abs/2405.06278, 2024."
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "X. Serra, ‚ÄúAn Interpretable Deep Learning Model for"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "[10]\nJ. Kim,\nJ. Urbano, C. C. S. Liem,\nand A. Hanjalic,\nAutomatic Sound Classification,‚Äù Electronics, vol. 10,"
        },
        {
          "Figure 4. True emotion annotation (x-axis) vs.\npredictions (y-axis) for two different emotions (tension and tender), and": "‚ÄúAre Nearby Neighbors Relatives? Testing Deep Mu-\nno. 7, p. 850, 2021."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "M. Aubry,\nand L. Landrieu,\n‚ÄúA Model You Can",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "and F. D. Malliaros, ‚ÄúAn Adversarial Attacker for Neu-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Hear: Audio Identification with Playable Prototypes,‚Äù",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "the\nral Networks in Regression Problems,‚Äù in Proc. of"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "in Proc. of the 23rd Int. Society for Music Information",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Workshop on Artificial\nIntelligence Safety co-located"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Retrieval Conf., ISMIR, 2022, pp. 694‚Äì700.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "with the 30th Int. Joint Conf. on Artificial Intelligence,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "IJCAI, ser. CEUR Workshop Proceedings, vol. 2916,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[17]\nJ. Parekh, S. Parekh, P. Mozharovskyi, G. Richard,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "2021."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "and F. d‚ÄôAlch¬¥e-Buc, ‚ÄúTackling Interpretability in Au-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "dio Classification Networks With Non-negative Ma-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[29] X. Huang, D. Kroening, W. Ruan,\nJ. Sharp, Y. Sun,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "trix Factorization,‚Äù IEEE ACM Transactions on Audio,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "E. Thamo, M. Wu, and X. Yi, ‚ÄúA Survey of Safety"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Speech, and Language Processing, vol. 32, pp. 1392‚Äì",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "and Trustworthiness of Deep Neural Networks: Veri-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "1405, 2024.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "fication, Testing, Adversarial Attack and Defence, and"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Interpretability,‚Äù Computer Science Review, vol. 37, p."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[18] M. Won,\nS. Chun,\nand X.\nSerra,\n‚ÄúToward\nInter-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "100270, 2020."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "pretable Music Tagging with Self-Attention,‚Äù CoRR,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "vol. abs/1906.04972, 2019.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[30] D. Zhang, T. Zhang, Y. Lu, Z. Zhu, and B. Dong, ‚ÄúYou"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Only Propagate Once: Accelerating Adversarial Train-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[19] S. Jain and B. C. Wallace, ‚ÄúAttention is not Explana-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ing via Maximal Principle,‚Äù in Advances in Neural In-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "tion,‚Äù in Proc. of the 2019 Conf. of the North American",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "formation Processing Systems 32: Annu. Conf. on Neu-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Chapter of the Association for Computational Linguis-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ral\nInformation Processing Systems, NeurIPS, 2019,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "tics:\nHuman Language Technologies, NAACL-HLT.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "pp. 227‚Äì238."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Association for Computational Linguistics, 2019, pp.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "3543‚Äì3556.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[31] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. P. Dicker-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "son, C. Studer, L. S. Davis, G. Taylor, and T. Goldstein,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[20] Z. Wang, D. Wang, Y. Zhang,\nand G. Xia,\n‚ÄúLearn-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "‚ÄúAdversarial Training for Free!‚Äù\nin Advances in Neu-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "ing Interpretable Representation for Controllable Poly-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ral\nInformation Processing Systems 32: Annu. Conf."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "phonic Music Generation,‚Äù in Proc. of the 21st Int. So-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "on Neural\nInformation Processing Systems, NeurIPS,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "ciety for Music Information Retrieval Conf.,\nISMIR,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "2019, pp. 3353‚Äì3364."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "2020, pp. 662‚Äì669.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[32] C. Kereliuk, B. L. Sturm, and J. Larsen, ‚ÄúDeep Learn-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[21]\nI. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplain-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "IEEE Transactions on\ning and Music Adversaries,‚Äù"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "ing and Harnessing Adversarial Examples,‚Äù in Proc. of",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Multimedia, vol. 17, no. 11, pp. 2059‚Äì2071, 2015."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "the 3rd Int. Conf. on Learning Representations, ICLR,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "2015.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[33] T. Eerola and J. K. Vuaskoski, ‚ÄúA Comparison of the"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Discrete and Dimensional Models of Emotion in Mu-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[22] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "sic,‚Äù Psychology of Music, vol. 39, no. 1, pp. 18‚Äì49,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "A. Vladu, ‚ÄúTowards Deep Learning Models Resistant",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "2011."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "the 6th Int. Conf.\nto Adversarial Attacks,‚Äù in Proc. of",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "on Learning Representations, ICLR, 2018.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[34] A. Aljanaki and M. Soleymani, ‚ÄúA Data-Driven Ap-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "proach to Mid-level Perceptual Musical Feature Mod-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[23] B. L. Sturm, ‚ÄúA Simple Method to Determine if a Mu-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "eling,‚Äù in Proc. of the 19th Int. Society for Music Infor-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "sic Information Retrieval System is a ‚ÄúHorse‚Äù,‚Äù IEEE",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "mation Retrieval Conf., ISMIR, 2018, pp. 615‚Äì621."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Transactions on Multimedia, vol. 16, no. 6, pp. 1636‚Äì",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "1644, 2014.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[35] K. Prinz, A. Flexer, and G. Widmer, ‚ÄúOn End-to-End"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "White-Box Adversarial Attacks in Music Information"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[24] A. Kurakin,\nI. J. Goodfellow, and S. Bengio, ‚ÄúAdver-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "the Int. Society for Music\nRetrieval,‚Äù Transactions of"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "sarial Examples in the Physical World,‚Äù in Workshop",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Information Retrieval, vol. 4, no. 1, pp. 93‚Äì104, 2021."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Track Proc. of\nthe 5th Int. Conf. on Learning Repre-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "sentations, ICLR, 2017.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[36] T. Hastie, R. Tibshirani, and J. H. Friedman, The El-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ements of Statistical Learning: Data Mining,\nInfer-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[25] A. T. Nguyen\nand E. Raff,\n‚ÄúAdversarial Attacks,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ence, and Prediction, 2nd Edition, ser. Springer Series"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "Regression, and Numerical Stability Regularization,‚Äù",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "in Statistics.\nSpringer, 2009."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "CoRR, vol. abs/1812.02885, 2018.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[37] A. Flexer, T. Lallai, and K. Rasl, ‚ÄúOn Evaluation of"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[26] G. R. Mode and K. A. Hoque, ‚ÄúAdversarial Examples",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Inter- and Intra-Rater Agreement in Music Recommen-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "in Deep Learning for Multivariate Time Series Regres-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "the Int. Society for Music In-\ndation,‚Äù Transactions of"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "the 49th IEEE Applied Imagery Pat-\nsion,‚Äù in Proc. of",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "formation Retrieval, vol. 4, no. 1, pp. 182‚Äî-194, 2021."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "tern Recognition Workshop, AIPR.\nIEEE, 2020, pp.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "1‚Äì10.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "[38] A.\nIlyas,\nS.\nSanturkar, D.\nTsipras,\nL.\nEngstrom,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "B. Tran,\nand A. Madry,\n‚ÄúAdversarial Examples Are"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "[27] E. R. Balda, A. Behboodi,\nand R. Mathar,\n‚ÄúPertur-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "Not Bugs, They Are Features,‚Äù in Advances in Neu-"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "bation Analysis of Learning Algorithms: Generation",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "ral\nInformation Processing Systems 32: Annu. Conf."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "of Adversarial Examples From Classification to Re-",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "on Neural\nInformation Processing Systems, NeurIPS,"
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "gression,‚Äù\nIEEE Transactions on Signal Processing,",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": "2019, pp. 125‚Äì136."
        },
        {
          "[16] R.\nLoiseau,\nB. Bouvier,\nY\n.\nTeytaut,\nE. Vincent,": "vol. 67, no. 23, pp. 6078‚Äì6091, 2019.",
          "[28] K. Gupta, J. Pesquet, B. Pesquet-Popescu, F. Kaakai,": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Intriguing Properties of Neural Networks",
      "authors": [
        "C Szegedy",
        "W Zaremba",
        "I Sutskever",
        "J Bruna",
        "D Erhan",
        "I Goodfellow",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "Proc. of the 2nd Int. Conf. on Learning Representations, ICLR"
    },
    {
      "citation_id": "3",
      "title": "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2018",
      "venue": "Proc. of the IEEE Security and Privacy Workshops, SP Workshops"
    },
    {
      "citation_id": "4",
      "title": "Deep Learning, Audio Adversaries, and Music Content Analysis",
      "authors": [
        "C Kereliuk",
        "B Sturm",
        "J Larsen"
      ],
      "year": "2015",
      "venue": "Proc. of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics"
    },
    {
      "citation_id": "5",
      "title": "Leftmost plot (A2B2E) presents predictions of black-box model, middle column (A2M2E) inherently interpretable model, and rightmost plot (aA2B2E) shows predictions of adversarially trained model. Depicted are predictions on original (clean) data (\"x\"), and adversarially perturbed data",
      "authors": [
        "D Stutz",
        "M Hein",
        "B Schiele"
      ],
      "year": "2019",
      "venue": "Proc. of the 2019 IEEE Conf. on Computer Vision and Pattern Figure 4"
    },
    {
      "citation_id": "6",
      "title": "Towards Explainable Music Emotion Recognition: The Route via Mid-level Features",
      "authors": [
        "S Chowdhury",
        "A Vall",
        "V Haunschmid",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proc. of the 20th Int. Society for Music Information Retrieval Conf., ISMIR"
    },
    {
      "citation_id": "7",
      "title": "On the Connection Between Adversarial Robustness and Saliency Map Interpretability",
      "authors": [
        "C Etmann",
        "S Lunz",
        "P Maass",
        "C Sch√∂nlieb"
      ],
      "year": "2019",
      "venue": "Proc. of the 36th Int. Conf. on Machine Learning, ICML, ser. Proc. of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "Proper Network Interpretability Helps Adversarial Robustness in Classification",
      "authors": [
        "A Boopathy",
        "S Liu",
        "G Zhang",
        "C Liu",
        "P Chen",
        "S Chang",
        "L Daniel"
      ],
      "year": "2020",
      "venue": "Proc. of the 37th Int. Conf. on Machine Learning, ICML, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness",
      "authors": [
        "A Noack",
        "I Ahern",
        "D Dou",
        "B Li"
      ],
      "year": "2021",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Exploring the Interplay of Interpretability and Robustness in Deep Neural Networks: A Saliency-guided Approach",
      "authors": [
        "A Guesmi",
        "N Aswani",
        "M Shafique"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "11",
      "title": "Are Nearby Neighbors Relatives? Testing Deep Mu-sic Embeddings",
      "authors": [
        "J Kim",
        "J Urbano",
        "C Liem",
        "A Hanjalic"
      ],
      "year": "2019",
      "venue": "Frontiers in Applied Mathematics and Statistics"
    },
    {
      "citation_id": "12",
      "title": "Exploring the Impact of Conceptual Bottlenecks on Adversarial Robustness of Deep Neural Networks",
      "authors": [
        "B Rasheed",
        "M Abdelhamid",
        "A Khan",
        "I Menezes",
        "A Khattak"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
      "authors": [
        "C Chen",
        "O Li",
        "D Tao",
        "A Barnett",
        "C Rudin",
        "J Su"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annu. Conf. on Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Concept Bottleneck Models",
      "authors": [
        "P Koh",
        "T Nguyen",
        "Y Tang",
        "S Mussmann",
        "E Pierson",
        "B Kim",
        "P Liang"
      ],
      "year": "2020",
      "venue": "Proc. of the 37th Int. Conf. on Machine Learning, ICML, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "15",
      "title": "Learning Bottleneck Concepts in Image Classification",
      "authors": [
        "B Wang",
        "L Li",
        "Y Nakashima",
        "H Nagahara"
      ],
      "year": "2023",
      "venue": "Proc. of the 2023 IEEE / CVF Conf. on Computer Vision and Pattern Recognition, CVPR. Computer Vision Foundation / IEEE"
    },
    {
      "citation_id": "16",
      "title": "An Interpretable Deep Learning Model for Automatic Sound Classification",
      "authors": [
        "P Zinemanas",
        "M Rocamora",
        "M Miron",
        "F Font",
        "X Serra"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "17",
      "title": "A Model You Can Hear: Audio Identification with Playable Prototypes",
      "authors": [
        "R Loiseau",
        "B Bouvier",
        "Y Teytaut",
        "E Vincent",
        "M Aubry",
        "L Landrieu"
      ],
      "year": "2022",
      "venue": "Proc. of the 23rd Int. Society for Music Information Retrieval Conf., ISMIR"
    },
    {
      "citation_id": "18",
      "title": "Tackling Interpretability in Audio Classification Networks With Non-negative Matrix Factorization",
      "authors": [
        "J Parekh",
        "S Parekh",
        "P Mozharovskyi",
        "G Richard",
        "F Alch√©-Buc"
      ],
      "year": "2024",
      "venue": "IEEE ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Toward Interpretable Music Tagging with Self-Attention",
      "authors": [
        "M Won",
        "S Chun",
        "X Serra"
      ],
      "year": "1906",
      "venue": "CoRR"
    },
    {
      "citation_id": "20",
      "title": "Attention is not Explana",
      "authors": [
        "S Jain",
        "B Wallace"
      ],
      "year": "2019",
      "venue": "Proc. of the 2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT"
    },
    {
      "citation_id": "21",
      "title": "Learning Interpretable Representation for Controllable Polyphonic Music Generation",
      "authors": [
        "Z Wang",
        "D Wang",
        "Y Zhang",
        "G Xia"
      ],
      "year": "2020",
      "venue": "Proc. of the 21st Int. Society for Music Information Retrieval Conf., ISMIR"
    },
    {
      "citation_id": "22",
      "title": "Explaining and Harnessing Adversarial Examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Proc. of the 3rd Int. Conf. on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": [
        "A Madry",
        "A Makelov",
        "L Schmidt",
        "D Tsipras",
        "A Vladu"
      ],
      "year": "2018",
      "venue": "Proc. of the 6th Int. Conf. on Learning Representations, ICLR"
    },
    {
      "citation_id": "24",
      "title": "A Simple Method to Determine if a Music Information Retrieval System is a \"Horse",
      "authors": [
        "B Sturm"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Adversarial Examples in the Physical World",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Workshop Track Proc. of the 5th Int. Conf. on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Adversarial Attacks, Regression, and Numerical Stability Regularization",
      "authors": [
        "A Nguyen",
        "E Raff"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "Adversarial Examples in Deep Learning for Multivariate Time Series Regression",
      "authors": [
        "G Mode",
        "K Hoque"
      ],
      "year": "2020",
      "venue": "Proc. of the 49th IEEE Applied Imagery Pattern Recognition Workshop"
    },
    {
      "citation_id": "28",
      "title": "Perturbation Analysis of Learning Algorithms: Generation of Adversarial Examples From Classification to Regression",
      "authors": [
        "E Balda",
        "A Behboodi",
        "R Mathar"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "An Adversarial Attacker for Neural Networks in Regression Problems",
      "authors": [
        "K Gupta",
        "J Pesquet",
        "B Pesquet-Popescu",
        "F Kaakai",
        "F Malliaros"
      ],
      "year": "2021",
      "venue": "Proc. of the Workshop on Artificial Intelligence Safety co-located with the 30th Int. Joint Conf. on Artificial Intelligence, IJCAI, ser. CEUR Workshop Proceedings"
    },
    {
      "citation_id": "30",
      "title": "A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability",
      "authors": [
        "X Huang",
        "D Kroening",
        "W Ruan",
        "J Sharp",
        "Y Sun",
        "E Thamo",
        "M Wu",
        "X Yi"
      ],
      "year": "2020",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "31",
      "title": "You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle",
      "authors": [
        "D Zhang",
        "T Zhang",
        "Y Lu",
        "Z Zhu",
        "B Dong"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annu. Conf. on Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Adversarial Training for Free",
      "authors": [
        "A Shafahi",
        "M Najibi",
        "A Ghiasi",
        "Z Xu",
        "J Dickerson",
        "C Studer",
        "L Davis",
        "G Taylor",
        "T Goldstein"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annu. Conf. on Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Deep Learning and Music Adversaries",
      "authors": [
        "C Kereliuk",
        "B Sturm",
        "J Larsen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "A Comparison of the Discrete and Dimensional Models of Emotion in Music",
      "authors": [
        "T Eerola",
        "J Vuaskoski"
      ],
      "year": "2011",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "35",
      "title": "A Data-Driven Approach to Mid-level Perceptual Musical Feature Modeling",
      "authors": [
        "A Aljanaki",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "Proc. of the 19th Int. Society for Music Information Retrieval Conf., ISMIR"
    },
    {
      "citation_id": "36",
      "title": "On End-to-End White-Box Adversarial Attacks in Music Information Retrieval",
      "authors": [
        "K Prinz",
        "A Flexer",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "Transactions of the Int. Society for Music Information Retrieval"
    },
    {
      "citation_id": "37",
      "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition",
      "authors": [
        "T Hastie",
        "R Tibshirani",
        "J Friedman"
      ],
      "year": "2009",
      "venue": "ser. Springer Series in Statistics"
    },
    {
      "citation_id": "38",
      "title": "On Evaluation of Inter-and Intra-Rater Agreement in Music Recommendation",
      "authors": [
        "A Flexer",
        "T Lallai",
        "K Rasl"
      ],
      "year": "2021",
      "venue": "Transactions of the Int. Society for Music Information Retrieval"
    },
    {
      "citation_id": "39",
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "authors": [
        "A Ilyas",
        "S Santurkar",
        "D Tsipras",
        "L Engstrom",
        "B Tran",
        "A Madry"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annu. Conf. on Neural Information Processing Systems"
    }
  ]
}