{
  "paper_id": "2204.11312v1",
  "title": "Emoca: Emotion Driven Monocular Face Capture And Animation",
  "published": "2022-04-24T15:58:35Z",
  "authors": [
    "Radek Danecek",
    "Michael J. Black",
    "Timo Bolkart"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Figure 1. EMOCA regresses 3D faces from images with facial geometry that captures the original emotional content. Top row: images of people with challenging expressions. Middle row: coarse shape reconstruction. Bottom row: reconstruction with detailed displacements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Teaching computers to see humans and understand their behavior is a long-standing goal of computer vision. To accomplish this, computers need to understand how humans look, how they move, and what they feel. Faces and their emotional expressions provide an important source of information about a person's internal emotional state. To support automated analysis of emotional state, we capture a person's face, including its 3D shape, pose, and facial expression, given a single RGB image. To do so, we go beyond prior work to extract 3D geometry that carries rich emotional content. We focus on parametric methods (i.e., animatable and model-based) due to their wide applicability for 3D avatar creation  [38] , image synthesis  [34, 83] , video editing  [43, 86]  and face recognition  [9, 70] .\n\nThe field of 3D face reconstruction has rapidly advanced over the last two decades; see Egger et al.  [23]  for a review. Existing methods that estimate 3D face models struggle to capture facial expressions in detail and often produce 3D shapes that do not carry the emotional content of the input image. This has several causes. First, some 3D face models lack sufficient expressiveness to capture subtle or extreme expressions. Second, reconstruction metrics like landmark reprojection loss  [8] , photometric loss  [10] , face recognition loss  [32] , or multi-image consistency losses  [75, 82] , are either not affected by facial expressions, or require perfect image alignment to capture subtle cues. Subtle changes in geometry, however, can result in large differences in the perceived emotion. We argue that, to recover 3D expression accurately, we need a new reconstruction metric that measures differences in expressions between the 3D reconstruction and the input image.\n\nTo that end, we describe EMOCA (EMOtion Capture and Animation), a neural network that learns an animatable face model from in-the-wild images without 3D supervision. The design of our method is inspired by advances in the field of facial emotion recognition, which has made tremendous progress to date on estimating affect (or emotion) from in-the-wild-images  [52] . Specifically, we train a state-of-the-art emotion recognition model, and leverage this during training of EMOCA as supervision. EMOCA introduces a novel perceptual emotion consistency loss that encourages the similarity of emotional content between the input and rendered reconstruction.\n\nWhile the new emotion consistency loss results in better reconstructed emotions, this alone is insufficient. Large image datasets used by previous 3D reconstruction methods, while containing a large number of subjects of diverse ethnicities, lack emotional expressivity  [14, 17, 46, 94] . Large datasets with facial expressions, valence, and arousal inthe-wild, on the other hand, while rich in emotions, do not provide multiple images per subject in diverse conditions  [7, 13, [48] [49] [50] 59, 93]  and smaller datasets in controlled settings are not suitable for deep learning  [56-58, 62, 80] . Multiple images of the same person, however, are required to train current state-of-the-art 3D face reconstruction methods  [20, 28, 75] . To overcome this, EMOCA builds on top of DECA  [28] , a publicly available 3D face reconstruction framework that achieves state-of-the-art identity shape reconstruction accuracy  [30, 75] . Specifically, we augment DECA's architecture with an additional trainable prediction branch for facial expression, while keeping other parts fixed. This enables us to only train the expression part of EMOCA on emotion-rich image data  [59] , which results in improved emotion reconstruction performance, while retaining DECA's identity face shape quality.\n\nOnce trained, EMOCA reconstructs a 3D face from a single image (Fig.  1 ), it significantly outperforms previous state-of-the-art methods in terms of the reconstructed expression quality, it preserves the state-of-the-art identity shape reconstruction accuracy, and the reconstructed face can be readily animated. Further, the expression parameters regressed by EMOCA convey sufficient information for in-the-wild emotion recognition, with on-par performance with the best image-based methods  [88] .\n\nIn summary, our main contributions are: 1) The first approach to reconstruct an animatable 3D face model from an in-the-wild image, that is capable of recovering facial expressions that convey the correct emotional state. 2) A novel perceptual emotion-consistency loss that rewards the accuracy of the reconstructed emotion. 3) The first 3D geometry-based framework for in-the-wild emotion recognition, with comparable performance to current state-ofthe-art image-based methods. 4) The code and model are publicly available for research purposes at https: //emoca.is.tue.mpg.de.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Monocular face reconstruction: Reconstructing 3D face shape from images has been studied extensively for more than two decades  [23, 101] . Model-free approaches directly regress 3D meshes  [19, 21, 29, 35, 42, 72, 76, 81, 95, 97, 99]  or voxels  [40]  from an image, or optimize a Signed Distance Function (SDF)  [63]  to fit a face image. Most of these methods require explicit 3D supervision during training. While the output is model-free, acquiring the training data typically relies on a 3D face model (3D Morphable Model, or 3DMM). Thus their ability to reconstruct expressive faces may be limited by the 3DMM-based reconstruction used to generate the paired training data  [19, 29, 35, 40, 42, 72, 95] , the domain gap between 3DMM-based synthetic training data and real images  [21, 76, 99] , or the regularization towards a fixed 3DMM fitting result  [16] . Instead, EMOCA is trained in a self-supervised fashion without any explicit 3D supervision, which enables it to capture less constrained expressions. Other self-supervised methods do not leverage face-domain-specific knowledge, which makes them applicable to general objects, but also limits the reconstruction quality  [81, 97] . Unlike EMOCA, none of these modelfree methods separate facial identity from facial expression, making them inappropriate for applications like expression re-targeting or animation.\n\nSeveral works reconstruct the parameters of fixed statistical models like the Basel Face Model (BFM)  [64] , FaceWarehouse  [12] , or FLAME  [53] , or jointly learn a model and reconstruct faces from images  [82, 84, 91] . Existing methods can be categorized into optimization-based  [4, 5, 9, 10, 33, 47, 65, 71, 86, 92]  and learning-based. The latter are trained fully supervised  [15, 36, 44, 69, 89, 90, 100]  or self-supervised with predicted 2D keypoints  [20, 28, 54, 75, 78, 82, 84, 85, 98] , 2D face contours  [54] , photometric constraints  [20, 28, 32, 78, 82, 84, 85, 98] , face recognition features  [20, 28, 32, 78] , multi-view constraints  [78] , or multiimage constraints  [20, 28, 32, 75, 82] . Each supervision signal impacts the reconstructed 3D face in a unique way. Explicit 3D mesh or model parameter supervision induces a bias towards the method used to generate the pseudo-ground truth. Using face recognition features or leveraging multiple images of the same identity during training mainly impacts identity shape and appearance. Keypoint losses impact the facial geometry and image alignment (global transformation, identity and expression shape parameters), but predicted keypoints are sparse (commonly 51-68 points), often inaccurate -especially for extreme expressions and head poses -and obtaining the optimal embedding of the corresponding keypoints on the model's surface is challenging. Photometric losses impact all model parameters (global transformation, identity and expression shape, appearance, and lighting), but, as with the keypoint losses, are strongly affected by misalignments between the predicted 3D face and the image. While using multi-view data during training has the potential to reconstruct more accurate 3D faces, there are no large datasets with a large number of identities and large diversity in expression, ethnicity, age, lighting conditions, etc. Consequently, while the field of monocular in-the-wild face capture has made tremendous progress, there are still limitations, particularly in the accuracy of the reconstructed expressions, which limit the emotions that can be perceived from the reconstructed 3D shapes. EMOCA instead learns to reconstruct expressive faces by combining emotion features that mainly propagate to the reconstructed expression, with a unique selfsupervised framework that enables us to leverage a large dataset of diverse expressions. Emotion analysis from images: Emotion analysis is a long-standing problem in computer vision and related fields (see  [3, 11]  for comprehensive surveys). Emotional states are commonly represented as discrete basic  [24, 25]  (e.g., Happiness, Surprise, ...) or compound expression categories  [22]  (e.g., happily surprised), continuous valence (positivenegative) and arousal (relaxed-intensive) values  [73] , or Facial Action Units (FACS) activations  [26] , where each action unit (AU) corresponds to a particular emotion-related facial muscle movement.\n\nEarly work on expression recognition extracts geometric features defining shape and location of face components  [61, 87] , appearance features  [27, 77] , or combinations of these  [41, Chapter 19] . Over the last decade, the availability of large datasets for single-image expression analysis  [7, 59]  and audio-visual videos  [48] [49] [50]  shifted the focus from manually designed features to end-to-end trained models  [52] . While early work like Wen and Huang  [96]  uses 3D non-rigid surface tracking to extract features for expression reconstruction, the majority of 3D-based methods focus on recognizing expressions from 3D scans  [60, 74] . Among these, the most relevant to EMOCA, is  [67]  as they use 3DMM features to classify three expressions (obtained by fitting the 3DMM to the scans); most other methods use diverse 2D and 3D features extracted from the textured 3D scans.\n\nFew 3DMM-based methods exist to recognize expressions from images. Bejaoui et al.  [6]  fit a 3DMM to images, while Chang et al.  [15]  and Koujan et al.  [51]  train a 3DMM parameter regressor, fully-supervised by parameters obtained by fitting a 3DMM to images and videos. From the 3DMM expression parameters, they then learn to classify different expressions. Most related to EMOCA, Shi et al.  [79]  use an expression recognition loss during training, but with the goal of obtaining a more discriminative latent representation. These methods focus on recognizing expressions, not improving 3D reconstruction. In contrast, EMOCA leverages recent advances in emotion recognition to reconstruct more expressive 3D faces.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preliminaries",
      "text": "Face model: FLAME  [53]  is a statistical 3D head model with parameters for identity shape β ∈ R |β| , facial expression ψ ∈ R |ψ| , and pose parameters θ ∈ R 3k+3 for rotations around k = 4 joints (neck, jaw, and eyeballs) and the global rotation. Given all parameters, FLAME outputs a mesh with n v = 5023 vertices. Formally, FLAME is:\n\nwith vertices V ∈ R nv×3 and n f = 9976 faces F ∈ R n f ×3 . FLAME comes with an appearance model, converted from Basel Face Model's albedo space  [64]  to FLAME's UV layout  [1] . Given parameters α ∈ R |α| , this model outputs a FLAME texture map A(α) ∈ R d×d×3 . Face reconstruction: DECA  [28]  is a publicly available framework to reconstruct a detailed, animatable 3D face model from a single image. We follow DECA's notation for simplicity. Given an image I, the coarse encoder\n\noutputs FLAME geometry parameters β, θ, ψ, albedo α, Spherical Harmonics (SH)  [66]  lighting l ∈ R 27 , and camera c ∈ R 3 , which is the concatenation of isotropic scale s ∈ R and translation t ∈ R 2 . The detail encoder\n\nencodes I to a subject-specific detail vector δ ∈ R 128 . To reconstruct dynamic expression wrinkles, the detail decoder\n\nuses δ to parametrize static person-specific details, and FLAME's expression ψ and jaw-pose parameters θ jaw to generate an expression-dependent detail UV displacement map D ∈ R d×d×3 . Denoting the rendering function with R [68], the coarse shape can be rendered to a 2D image as R(M (β, θ, ψ), α, l, c) → I Rc . To render the FLAME mesh, with expression-dependent details, to an image I Rd , the D are converted to a detailed normal map N d , and provided as additional parameters to R; formally R(M (β, θ, ψ), α, l, c, N d ) → I Rd . Relative keypoint loss: Given 2D face keypoints k i ∈ R 2 and the corresponding keypoints on FLAME's mesh surface M i ∈ R 3 , the relative keypoint loss  [28]  computes offset vectors between pairs of 2D keypoints and between the corresponding pairs of projected model keypoints, and penalizes the difference. Formally, the loss computes as\n\nwhere E is a set of landmark index pairs, and Π ∈ R 2×3 is the orthographic 3D-2D projection matrix. We denote the emotion network as A(I) → ϵ.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method: Emoca",
      "text": "The main goal of EMOCA is to address a significant limitation of the prior art -to recover 3D face shapes from single images that convey the full spectrum of emotion. Our technical contribution is twofold, first, we introduce a novel emotion consistency loss that is designed to encourage emotion similarity between the input image and the output rendering as training supervision. Second, we leverage parts of DECA's  [28]  trained model in order to only train the expression part of EMOCA on emotion-rich image data, while preserving DECA's identity shape reconstruction performance. Architecture: EMOCA's architecture is based on DECA  [28] . As with many state-of-the-art methods, DECA takes an input image and uses several neural networks to factor it into shape, albedo, lighting, etc. Given these factors, one can differentiably render an output image that should look like the input. Here we exploit this output image in a novel way by encouraging it to have the same expression as the input image.\n\nTraining models like DECA  [28]  on emotion-rich image data  [59]\n\nwith emotion consistency loss L emo , photometric loss L pho , eye closure loss L eye , mouth closure loss L mc , lip corner loss L lc , and expression regularizer L ψ , each weighted by a factor λ x .\n\nEmotion consistency loss: The emotion consistency loss computes the difference between the emotion features of the input image ϵ I = A(I) and those of the rendered image, ϵ Re = A(I Re ) as:\n\nwith d(ϵ 1 , ϵ 2 ) = ∥ϵ 1 -ϵ 2 ∥ 2 . Instead of measuring a geometric error, L emo computes a perceptual difference between the input image and the rendered image. Optimizing this loss during training ensures that the reconstructed 3D face conveys the emotional content of the input image. Photometric loss: The photometric loss computes the pixel error between the input image I and the output rendering I Re . L pho = ∥V I ⊙ (I -I Re )∥ 1,1 . V I denotes a rendered mask of the output face shape, with each pixel located in the face skin region is equal to 1, and 0 elsewhere. The operator ⊙ denotes the Hadamard product.\n\nEye closure loss: The eye closure loss computes as\n\n, where E eye is a set of upper/lower eyelid keypoint pairs. Due to slight misalignment between image landmarks and projected 3D landmarks, enforcing standard landmark reprojection losses produces incorrect predictions. Instead, using (translation-invariant) relative keypoint losses (for eye closure, mouth closure, and mouth width) is less susceptible to misalignments. Mouth closure loss: The loss computes as L mc = L Emc rk , where E mc is a set of upper/lower lip keypoint pairs. Lip corner loss: The lip corner loss computes as L lc = L E lc rk , where E lc is the pair of left and right lip corners. Expression regularization: The expression is regularized as L ψ = ∥ψ∥ , the input image is fed to the coarse shape encoder (initialized from DECA  [28]  and fixed) and EMOCA's trainable expression shape encoder. A textured 3D mesh is then reconstructed from the regressed identity shape, expression shape, pose, and albedo parameters with FLAME's geometry and albedo models as fixed decoders. This textured mesh is rendered by a differentiable renderer with the regressed camera and spherical harmonics lighting. Our novel emotion consistency loss (Eq. 8) penalizes the difference between the emotion features of the input image and those of the rendered coarse shape, after passing both images through a fixed emotion recognition network. For the detail training stage (yellow box), EMOCA's expression encoder is fixed, and the regressed expression (and jaw-pose) parameters are used to condition the detail decoder.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Detailed Stage:",
      "text": "The detail training stage adds wrinkle details that are animatable. Here we follow DECA's design, and use the same architecture and losses.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Setting",
      "text": "The first stage (coarse part) of EMOCA is trained with AffectNet  [59]  for a maximum of 20 epochs, with early stopping, using the Adam optimizer  [45]  and a learning rate of 5e -5. We use the same training/validation/testing split as proposed by  [88] . We set λ emo = 1, λ pho = 2, λ eye = λ lc = λ mc = 0.5 and λ ψ = 1e -4. EMOCA's second stage (detail part) training is comparable to DECA's second stage training. We use the same training data  [14, 17]  and train with the same settings. Please refer to the Appendix for more training details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quantitative Evaluation",
      "text": "While, for the task of 3D face reconstruction, standard benchmarks exist to quantitatively evaluate the identity face shape  [30, 75] , no such benchmark exists to assess the accuracy of the reconstructed expression. Unlike the identity shape benchmarks, quantitatively measuring the difference between a reconstructed 3D facial expression and a ground truth scan is less meaningful. The errors would be dom-inated by errors of the reconstructed identity face shape, and a low geometric error would not necessarily correspond to a small difference in human perception of the emotion. Instead, we evaluate EMOCA 1) qualitatively, 2) quantitatively for the task of in-the-wild emotion recognition, and 3) perceptually in an Amazon Mechanical Turk (AMT) study. Emotion recognition: Our goal is to quantify how much of the input emotion is conveyed in the reconstructed 3D face. For this, we apply 3D face reconstruction methods to in-the-wild emotional face images and evaluate emotion recognition accuracy based on the 3D reconstruction. Here we focus on methods that reconstruct a parametric model of the face, i.e. a 3DMM. To that end, for each 3D face reconstruction method, we train a 4-layer MLP with Batch Normalization  [39]  and LeakyReLUs to regress valence and arousal levels, and classify expression labels directly from the predicted 3DMM parameters. The training details are described in the Appendix.\n\nWe evaluate emotion recognition on the AffectNet test set  [59]  and the AFEW-VA test set  [49] . For each method, we report Concordance correlation coefficients (CCC ↑), Pearson correlation coefficients (PCC ↑), root mean squared error (RMSE ↓), and sign agreement (SAGR ↑) for valence (V) and arousal (A) regression and accuracy for expression (E) classification on the test set defined by  [88] . EMOCA outperforms all 3D face reconstruction methods, and is on Model   1 . Emotion recognition performance on the AffectNet test set  [59] . The EmoNet performance is measured using the model that is publicly released by the authors. For EMOCA and the other 3D baselines, we train the recognition module as described in Sec. 5.2. DECA w/ detail means that DECA's detail code prediction was included in the input to the regressor, along with the 3DMM parameters. Please note that EMOCA's performance is on par with EmoNet and it outperforms all other 3D reconstruction-based methods. Note that EMOCA performs on par with EmoNet  [88] , which is a recent method for estimating emotion from images. This confirms that the emotional content is present in our 3D reconstruction and that 3D shape is sufficient to understand emotion. This has implications for future research on emotion recognition. Perceptual study: The 3D geometry reconstructed from an image must convey the emotion of the input image. Directly comparing rendered geometry with an image is difficult due to the domain gap. Instead, we perform a perceptual study using AMT to assess the perceived expression of emotion from rendered 3D reconstructions. Specifically, given an image, we ask participants to categorize the perceived expression of emotion into one of the 7 basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Contempt) or as a neutral expression (no emotion). A single evaluation task contains 75 images in random order; 35 real images, the 35 corresponding rendered reconstructions (from one method), and 5 qualification samples. The 5 qualification samples are duplicates sampled from the 35 real images, and they are chosen to be of easily recognizable emotion. Each task is performed by 10 participants. Participants that either misclassify the emotion, or inconsistently label the duplicated images, for at least 2 (of the 5) qualification samples are discarded from further analysis to filter out inattentive and/or uncooperative participants. For each method, we measure the classification consistency between each participant's labels for the rendered images and their labels for the corresponding real images. If the rendered 3D meshes contain the emotional content of the images, then the scores given to both should be consistent.\n\nWe select 35 images with balanced emotional content (i.e., 5 images per basic emotion) from the AffectNet test set  [59] . For each image, we reconstruct 3D faces using EMOCA, DECA  [28] , Deep3DFace  [20] , MGCNet  [78]  and 3DDFA V2  [36] . The classification consistency averaged across participants for each method are: EMOCA (coarse) 0.68, EMOCA (detail) 0.65, Deep3DFace 0.37, DECA (coarse) 0.33, DECA (detail) 0.31, MGCNet 0.32, 3DDFA V2 0.31. In summary, EMOCA preserves the emotional content of images better than the other methods. Note that, perhaps surprisingly, there is little difference between the scores for the coarse meshes of EMOCA/DECA and the detailed ones. Despite more wrinkle detail, our perceptual experiments suggest that the detailed meshes do not convey more emotional content. One possible explanation is that, in addition to adding valid wrinkle details, the detail generator sometimes adds artifacts in the lip region (e.g. Fig.  1 , col. 1 & 3), and hallucinates details in the forehead (e.g. Fig.  1 , col. 8). These could negatively impact participants' perception. For the full confusion tables, see the Appendix. Emotion recognition vs. perceptual study: There is a considerable discrepancy between the results of the automatic emotion recognition results and the perceptual study results, in particular for Deep3DFace  [20] . Deep3DFace performs much better on the emotion recognition task (slightly below SOTA), than on the perceptual study. Unlike EMOCA, it is not capable of producing highly emotional reconstructions (see Fig.  3 ). We hypothesize that the automatic predictors are capable of detecting more subtle cues than humans. We investigate this by measuring the agreement (i.e., percentage of matching predictions) between the method's classifier (from the reconstructed face parameters) and the participant's annotation of the input images from the perceptual study. The results are: EMOCA 62% and Deep3DFace 62%. This indicates that the predicted parameters for both methods contain a similar amount of information about the emotions compared to the annotations of the input images. However, the agreement between the method's classifier and the participant's annotation of the rendered reconstructions is for EMOCA 48%, and for Deep3DFace 26%. In other words, EMOCA is signficantly more in agreement with human perception.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Qualitative Evaluation",
      "text": "We provide a visual comparison of the coarse shape reconstruction methods in Fig.  3 . Observe that EMOCA outperforms all the previous methods in terms of capturing the emotional content of the original image in the reconstructed expression. In Fig.  4  we compare our detail reconstruc- tions to DECA's detail reconstruction. Compared to DECA, our detailed displacement better captures the fine details of highly emotional input images.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Experiment",
      "text": "Table  2  shows the effect of ablating the training data and the emotion consistency loss. The table summarizes the effect of EMOCA trained w/ and w/o the emotion consistency loss, and using the DECA data only  [14, 17]  instead of the AffectNet training data  [59] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion And Limitations",
      "text": "Baseline: EMOCA builds on top of DECA due to its state-of-the-art identity shape reconstruction performance. We found in our experiments that the recently released Deep3DFaceRecon  [2]  gives better 3D face reconstructions than reported in the paper  [20] , and in some cases, it outperforms DECA in terms of the reconstructed expression. Combining our emotion consistency loss with the Deep3DFaceRecon framework to further improve their reconstructed expressions is worth further investigation. Image alignment: DECA sometimes predicts 3D faces that are slightly misaligned with the input images. EMOCA inherits this limitation due to the fixed coarse shape encoder. Further, while EMOCA reconstructs more expressive faces that better convey the emotion of the input image, expressions are also sometimes misaligned. Mitigating these artifacts, and better balancing the trade-off between geometric alignment and emotion similarity, requires further work. Emotion embedding analysis: We assume that the emotion embedding extracted by the emotion recognition network has desirable properties to guide the optimization of FLAME's expression parameters. We found that the emotion recognition loss is more difficult to optimize and it requires more careful weighting of the loss compared to the identity recognition losses used in previous work  [20, 28, 32] . Directly using the pre-trained EmoNet  [88]  Model for instance did not provide sufficient supervision. However, our work is the first to demonstrate how to use emotion recognition features to guide the task of 3D geometry reconstruction. In addition using our emotion consistency loss to train EMOCA, we have experimented with the applicability of emotion features for the tasks of emotion retrieval and emotion retargetting via FLAME expression parameter optimization (see Appendix).\n\nEmotion network architecture: Using a pre-trained stateof-the-art emotion recognition network  [88]  does not provide satisfactory supervision during optimization or training. Instead, it produces strong artifacts in the reconstructed geometry. To overcome this, we investigate different ResNet  [37]  and Swin Transformer  [55]  based emotion network architectures, and show the effect of different networks in the Appendix. Based on this analysis, we use a ResNet-50 backbone for our emotion network. Jaw rotations: While FLAME's jaw rotation parameters θ jaw contribute to facial expressions, we found the optimization of θ jaw to be unstable while training EMOCA. We hypothesize, that this is due to the lack of a good prior for the jaw rotation. However, using different simplified priors for the jaw pose like a simple L2 regularizer did not give satisfactory results. We offer a more detailed discussion in the Appendix. Investigating the effect of more advanced data-driven jaw priors when optimizing the emotion loss is subject to future work.\n\nImplementation details: For details on all hyper parameters and discussion on design choices see the Appendix.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "We have presented EMOCA, a method that takes a single in-the-wild image and reconstructs a 3D face with sufficient facial expression detail to convey the emotional state of the input image. EMOCA is trained in a self-supervised fashion from a large dataset of emotion-rich images. A novel emotion similarity loss provides supervision on the reconstructed expressions during training. The emotion similarity relies on deep features extracted from a neural network trained for single-image affect (emotion) recognition in-thewild. EMOCA reconstructs 3D face shape on par with current state-of-the-art methods but outperforms them in terms of the quality of the reconstructed expression. Further, using the reconstructed expression parameters for the task of in-the-wild emotion recognition, EMOCA outperforms existing 3DMM-based face reconstruction methods and gives on par results with the best purely image-based method.\n\nIn summary, this is the first in-the-wild monocular face reconstruction work that puts explicit emphasis on the perceptual quality of the expression and the emotion it communicates instead of standard geometric and photometric losses. This presents a new direction for the monocular face reconstruction community. This work has potential to further combine the fields of monocular 3D face reconstruction and emotion analysis. Further, downstream application of this work can be employed in the industry, including but not limited to gaming, movies, AR/VR and communication.\n\nOf course, any improvement to 3D face acquisition and animation may also enable more realistic 'deep fakes.' Subtle emotional cues are individualistic and reproducing these could make it harder to detect such fakes. While cognizant of the risks, we are also sensitive the the importance of facial emotion in human communication. The trend towards emotional avatars in games and communication is clear. If communicative avatars do not properly communicate emotion, that, in itself presents a risk of misunderstandings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Appendix",
      "text": "Discussion of novelty: In Sup. Mat. we aim to shed more light on the process that eventually led to EMOCA and the challenges that had to be overcome. The idea of using deep perceptual losses to supervise face reconstruction is not new. A critic might argue, that the novelty of EMOCA is very limited for exactly that reason. However, the fact remains that previous SOTA methods have a clear limitation when it comes to reconstructing faces that communicate the correct emotional content. And from the knowledge of this limitation, we conceived the idea of leveraging emotion recognition, an idea not previously attempted by any work on face reconstruction. The inventive novelty in EMOCA was in coming up with the idea in the first place. This idea, once explained, makes such an intuitive sense, it may lead the reader into thinking it is a straightforward change to an already functioning system. The idea, although very simple and elegant, was by no means easy to get to work and this is what we aim to explain next. Designing EMOCA: Our work starts with the simple idea -how can we employ the findings from emotion recognition to improve face reconstruction? Leveraging a pretrained SOTA network for emotion recognition, similarly to the way face recognition networks were used seems like a natural choice. However, using its final outputs such as the expression class and valence and arousal levels is not sufficient. Clearly, these very low-dimensional labels, while they do carry some information about the emotional content, they likely exhibit a lot of ambiguity and are not sufficient to supervise 3D shapes. For instance, an expression classified as happy can take on many different shapes (a subtle smile, a big smile with an open mouth, an \"inverted\" smile, etc.) and similar reasoning could be applied for any other expression and for any levels of valence and arousal as well. Hence, these labels most likely do not provide a sufficient supervision signal for geometry. The next logical design choice is to leverage high dimensional deep features from a pretrained emotion recognition network. This choice can only make sense if the emotion feature in question is a \"well-behaved\" embedding space. Ideally we want similar features to represent faces of similar expressions and vice versa. Therefore, we conducted an emotion retrieval experiment, using a pretrained publicly available EmoNet model  [88]  and nearest neighbors search. This experiment is discussed in Sec. H. Having verified, that similar emotion features retrieve images of geometrically and semantically similar expressions, the next thing to be verified is whether the emotion feature carries a signal that is strong enough, to be utilizable for 3D reconstruction. This was particularly challenging and we comment on this further in Sec. D. Finally, having demonstrated that the emotion recognition features indeed carry enough information in order to supervise the geometry, we can finally incorporate the emotion consistency loss into a face reconstruction framework, arriving at EMOCA. In addition to the ablations listed in the main paper, we also add ablations on different architectures and weights for the emotion consistency loss in Sec. F",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Emotion recognition metrics: In the main paper, we evaluate emotion metrics in the same setting as Toisoul et al.  [88] . The metrics are defines as follows RMSE stands for root mean squared error:\n\nSAGR stands for sign agreement and it evaluates whether the predicted value has the same sign as the ground truth:\n\nPearson correlation coefficient (PCC) measures the correlation between predictions and GT:\n\nConcordance correlation coefficient (CCC) incorporates the PCC but also penalizes signals which are still correlated according to PCC but have different means:\n\nEmotion recognition loss function: We train our emotion networks with the same loss function as defined by Toisoul et al.  [88] .\n\nŷi log (y i )\n\nThe complete loss function for emotion recognition is then defined as:\n\nwhere α, β and γ are shake-shake regularization coefficients  [31]  uniformly sampled from the interval [0, 1] for each training batch and:\n\nUnlike the work of Toisoul et al.  [88] , we do not use knowledge distillation as its improvements are marginal and make the training process much more complex.\n\nImage-based emotion recognition: We investigate emotion recognition networks based on different architectures, ResNet-50  [37] , Swin Transformer  [55] , and EmoNet  [88] .\n\nWe train all models on AffectNet  [59] , using the training/validation/test split proposed by Toisoul et al.  [88] . The ResNet-50 and Swin Transformer based models are pretrained on ImageNet  [18] . During training, the training images are sampled such that each of the 7 expression labels appears with the same frequency. This sampling is crucial to maximize the performance of the emotion networks, as the AffectNet training set is not balanced. We use the Adam optimizer with learning rate of 0.0001, β 1 = 0.9 and β 2 = 0.999. The batch size used for training is 64. Each model is trained for a maximum of 20 epochs with early stopping, and the model with the lowest validation error is selected.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "3Dmm-Based Emotion Recognition:",
      "text": "In Section 5.2 of the paper (Tab. 1) and Table  3 , we evaluate different face reconstruction methods by recognizing emotions from the regressed 3DMM parameters. Specifically, we train a 4-layer MLP with Batch Normalization and LeakyReLUs to output valence and arousal levels and expression classes from the regressed identity and expression parameters (see Figs.  5  and 6  for details). The size of each hidden layer is 2048. We train the 3DMM-based recognition on AffectNet similarly to the image-based emotion recognition. The loss function is identical to the one used for image-based emotion recognition. The batch size used for training is 64. We use the Adam optimizer with a learning rate of 0.0001, β 1 = 0.9 and β 2 = 0.999.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Detail Stage Training:",
      "text": "The detail stage training follows the training protocol of DECA  [28] . The coarse model part is kept fixed, while detail encoder and decoder are trained. This stage uses VGGFace2 [14] and VoxCeleb2  [17]  images, due to the necessity of having multiple images per identity. We optimize following losses: photometric loss, ID-MRF perceptual loss which encourages reconstruction of higher frequency detail (compared to the coarse mesh), as well as the soft symmetry loss and displacement regularization. Further, to disentangle identity and expression dependent details, we employ DECA's detail consistency loss, where each batch contains k images of each subject, and the detail codes are exchanged randomly between the predictions for each identity For our training, we set k=3 and batch size of 4 identities, totalling 12 input images per batch. For more details, see the original DECA publication.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C. Qualitative Evaluation",
      "text": "In addition to the performance in emotion analysis on the AffectNet dataset in the main paper, we also test EMOCA on AFEW-VA  [49] . The results are reported in Tab. 3.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D. Emotion Optimization",
      "text": "We can use our emotion consistency loss for additional tasks. Here we consider the problem of expression retargeting. Given two face images, a source identity image I S and a target expression image I T of potentially two different people with different expressions, poses, cameras, and lighting, our goal is to optimize for the (unknown) target expression ψT . Formally, we infer the FLAME parameters E c (I S ) and E c (I T ) for both images. Then, with some abuse of notation, we render I R (ψ) = R(M (β S , θ T , ψ), α S , l T , c T ), the FLAME mesh with source identity shape β S , source albedo α S , and target pose θ T , target camera c T , target lighting l T , and the optimization expression parameters ψ. We then extract the emotion features of the rendering ϵ R (ψ) = A(I R (ψ)) and the target image ϵ T = A(I T ), and optimize:\n\nwith\n\n, and regularizer weight λ ψ = 1e-3. We use gradient descent for the optimization. Below we show optimization results, and an analysis of the convergence and sensitivity to the initialization. On Emotion Network Architecture: Figure  7  shows emotion optimization results using different emotion recognition network. This indicates that the original released EmoNet is not suitable for emotion optimization. Instead, we use the ResNet-50 architecture as default model. On Initialization: Figure  8  further shows the influence of the initialization on the optimized emotion. These results demonstrate that 3DMMs, when rendered, can in fact be animated with a deep perceptual emotion similarity loss. On Jaw Optimization: A perceptive reader may ask, why we optimize only for the expression parameters ψ and not also for the jaw pose θ jaw . After all, the jaw position most certainly has an effect on the perceived emotion. We have struggled with the jaw optimization issue for quite a long time, unable to get acceptable results as the jaw pose parameter optimization makes this optimization unstable -the jaw would always be posed to an unrealistic or at least very incorrect pose. Fixing the jaw pose to a reasonable estimate however (such as DECA's prediction) makes the optimization stable and produces good results. We hypothesise that this instability could be caused by the following:\n\n1. FLAME is missing a comprehensive prior for the jaw pose. We experimented with simplistic hand-crafted priors (such as distance or squared distance from the expected pose) but this did not yield any improvement. It is possible that the creating a more comprehensive prior (other than the Gaussian prior for FLAME's expression space), a prior that entangles the expression and jaw pose spaces is necessary. This makes for an interesting direction for future work.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Emotion Optimization Involves Optimizing A Deep Fea-",
      "text": "ture vector and while we have demonstrated that similar emotion features belong to similar expressions, we have not eliminated the possibility, that the emotion network can be \"attacked\" to produce the desired features with a distorted images. An optimization process, in which the jaw is not fixed could results in an adversarial attack on the network that forces it to produce a    3 . Emotion recognition performance on AFEW-VA  [49] . All emotion regressors are pretrained on AffectNet and finetuned on the AFEW-VA using 5-fold Cross-Validation (CV). The reported numbers are averaged across the 5-fold CV runs. EMOCA performs best, followed by Deep3DFace. Surprisingly, both of these methods outperform EmoNet. Other 3D-based methods follow.\n\nsimilar emotion feature vector.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E. Perceptual Study",
      "text": "Section 5.2 of the paper evaluates the amount emotion conveyed by the reconstructed 3D geometry in a perceptual study. Figure  9  gives the full confusion matrix of the participants' labels of real images (rows) and the labels of the reconstructions (columns). Figure  10  further compares the ground truth emotion labels with the participants' classifications of the reconstructions. For completeness, we also include the confusion matrix of participants' labeling of the real images in Fig.  11 .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "F. Emotion Consistency",
      "text": "Emotion network architecture: The choice of architecture for emotion supervision plays a critical role. While all architectures perform comparatively well on the emotion recognition task, they are not equally suitable as supervision for our 3D face reconstruction task. Fig.  12  visually compares EMOCA models trained with different emotion recognition networks as supervision. Again, the SOTA emotion recognition architecture -EmoNet, is not suitable as it produces unacceptable artifacts. Furthermore, the SWIN  [55]  transformer backbone, which is considered to be superior to the ResNet  [37]  architecture, also produces some undesirable artifacts. Hence, the ResNet backbone was used for the final model of the emotion recognition network. Emotion consistency weight: We have experimented with different values of the emotion consistency loss weight term λ emo . This is a crucial factor of successfully training EMOCA. If the weight is too small, the emotion is not captured well enough. At the same time, high values lead to unnaturally over-exaggerated expressions. A visual ablation of this phenomenon can be found in Fig.  13  and Fig.  14  for two different emotion network architectures; ResNet-50  [37]  and SWIN-B  [55] . Additional ablations: We further evaluate the impact of the similarity metric used for the emotion similarity, the effect The first row contains a source image, its DECA reconstruction, a target image, its DECA reconstruction, and the colored reconstruction. The following rows contain: the initialization of the optimization w/o and w/ color (left) and the optimization result w/o and w/ color (right). Note that the optimization process is only modifying the expression coefficients ψ and not the jaw rotation θjaw. While the process usually converges to meaningful results, the most favorable outcome is obtained, when initializing the process with the target expression coefficients ψ and pose θ, which correspond to the second row.\n\nof adding a landmark reprojection error to the loss function, and the effect of the relative landmark losses (mouth closure, lip corner distance and eye closure). Finally, we analyze the effect of using DECA's training data instead of AffectNet. You can see the results in Fig.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "G. Emotional Retargeting",
      "text": "EMOCA regresses FLAME  [53]  parameters and expression dependent geometric details. The disentanglement of the coarse identity and expression geometry and the identity and expression dependent details allows us to animate EMOCA's reconstructions. We demonstrate this by animat-  The x-axis of each cell gives the ratio of participants' reconstruction labels and real image labels and the absolute number is written next to each bar. The accuracy of each method for a particular expression class is on the diagonal. You can see that both variants of EMOCA (detail and coarse) are superior to the other methods. Furthermore, off-diagonal you can observe how the label of meshes reconstructed by EMOCA is much less confused for other labels, compared to other methods. Finally, the confusion matrix highlights how other methods are not capable of producing expressions of fear, disgust and anger. Instead these are confused with surprise. EMOCA does not suffer from the same limitation. However, participants did have some trouble distinguishing reconstructions of disgust and anger. Please note that the first row (neutral) shows a small number of samples. This happens because our perceptual study did not contain neutral images. Figure  10 . This figure contains the confusion matrices of participant's labels of the reconstructions w.r.t. to the ground truth labels (as opposed to users' subjective labels, which you can find in Fig.  9 . Please note that neutral expressions were not given in the study, which is why the matrix only has six rows (neutral excluded).\n\ning a source 3D face using a video sequence of another actor. Figure  16  demonstrates two things, first, EMOCA reconstructions convey emotions of the source images, and second, the animated faces of other subjects convey the same emotion. The emotional fidelity hence is preserved in the animated face of the other subject.  While our participants mostly agreed with our ground truth, there were disagreements for the negatively charged expressions of fear, disgust, anger and particularly contempt.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "H. Emotion Retrieval",
      "text": "Our work relies on the following key hypothesis. The emotion recognition networks learn a useful embedding of emotion. The following properties are desirable:\n\n• Images of faces with similar expressions conveying similar emotions are close in this embedding space.  • Images of faces with dissimilar expressions/emotions are farther apart in this space.\n\n• Invariance to pose, identity and lighting and back-ground.\n\nWe employ the publicly released model of EmoNet  [88]  and use the 256-dimensional feature output of the last con- volutional layer as emotion embedding. We then extract the emotion embedding for faces in the Aff-Wild2 video dataset  [48] . For the emotion retrieval given an image, we seek the nearest neighbors w.r.t. L2 distance metric in the dataset. Figure  17  shows the 10 nearest neighbors for multiple images. For comparison, we repeat the process for the ground truth (GT) valence and arousal labels of the Aff-Wild2 dataset in Fig.  18 . Observe that this has a negative effect on the samples, particularly the mouth region. Final row is EMOCA model trained on the same data as DECA instead of AffectNet. You can see that it achieves a very similar result compared to EMOCA trained on AffectNet. This highlights an interesting finding -once an emotion recognition network has been trained, it can be used for supervision even on datasets that do not strictly guarantee a balanced representation of emotional states, such as face recognition datasets.  Figure  18 . Examples of nearest neighbor retrieval using the ground truth annotated valence and arousal space on the AffWild2  [48]  dataset. While the retrieved faces do have some degree of similarity, the quality of retrieval compared to the EmoNet feature is lower.",
      "page_start": 20,
      "page_end": 26
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EMOCA regresses 3D faces from images with facial geometry that captures the original emotional content. Top row: images of",
      "page": 1
    },
    {
      "caption": "Figure 1: ), it significantly outperforms previ-",
      "page": 2
    },
    {
      "caption": "Figure 2: EMOCA overview. For the coarse training stage (green box), the input image is fed to the coarse shape encoder (initialized from",
      "page": 5
    },
    {
      "caption": "Figure 3: Comparison of coarse reconstruction methods, from left to right: Input, 3DDFA V2 [36], MGCNet [78], Deng et al. [20],",
      "page": 6
    },
    {
      "caption": "Figure 1: , col. 8). These could negatively impact participants’",
      "page": 7
    },
    {
      "caption": "Figure 3: ). We hypothesize that the automatic predictors",
      "page": 7
    },
    {
      "caption": "Figure 3: Observe that EMOCA out-",
      "page": 7
    },
    {
      "caption": "Figure 4: we compare our detail reconstruc-",
      "page": 7
    },
    {
      "caption": "Figure 4: Comparison of 3D reconstructions with detail displace-",
      "page": 7
    },
    {
      "caption": "Figure 7: shows emo-",
      "page": 14
    },
    {
      "caption": "Figure 8: further shows the influence of",
      "page": 14
    },
    {
      "caption": "Figure 5: The architecture of EMOCA-based emotion recognition. Top: EMOCA emotion recognition with coarse parameters. From the",
      "page": 15
    },
    {
      "caption": "Figure 6: The architecture of other emotion recognition netowrks. Top: emotion recognition for other 3DMM-based reconstruction",
      "page": 16
    },
    {
      "caption": "Figure 9: gives the full confusion matrix of the par-",
      "page": 16
    },
    {
      "caption": "Figure 10: further compares the",
      "page": 16
    },
    {
      "caption": "Figure 7: Emotion optimization example.",
      "page": 17
    },
    {
      "caption": "Figure 12: visually compares EMOCA models trained with different",
      "page": 17
    },
    {
      "caption": "Figure 13: and Fig. 14",
      "page": 17
    },
    {
      "caption": "Figure 8: Sensitivity of the emotion optimization to initialization.",
      "page": 17
    },
    {
      "caption": "Figure 15: G. Emotional retargeting",
      "page": 17
    },
    {
      "caption": "Figure 9: This figure contains the confusion matrices of participant’s labels of the real image and the reconstructed images for each method.",
      "page": 18
    },
    {
      "caption": "Figure 10: This figure contains the confusion matrices of participant’s labels of the reconstructions w.r.t. to the ground truth labels (as",
      "page": 19
    },
    {
      "caption": "Figure 9: Please note that neutral expressions were not given in the study, which is",
      "page": 19
    },
    {
      "caption": "Figure 16: demonstrates two things, first, EMOCA re-",
      "page": 19
    },
    {
      "caption": "Figure 11: This figure contains the confusion matrices of participant’s labels of the real images w.r.t. to ground truth images. While this",
      "page": 20
    },
    {
      "caption": "Figure 10: Classifying expression is subjective.",
      "page": 20
    },
    {
      "caption": "Figure 12: Comparison of different EMOCA models, supervised by different emotion networks. From top to bottom: ResNet-50 [37],",
      "page": 21
    },
    {
      "caption": "Figure 13: Comparison of models trained with different weights of the emotion consistency loss λemo. The emotion network used was",
      "page": 21
    },
    {
      "caption": "Figure 14: Comparison of models trained with different weights of the emotion consistency loss λemo. The emotion network used was",
      "page": 22
    },
    {
      "caption": "Figure 17: shows the 10 nearest neighbors for mul-",
      "page": 22
    },
    {
      "caption": "Figure 15: A visual comparison of model with different changes. First row consists of input images. The next three rows use different",
      "page": 23
    },
    {
      "caption": "Figure 16: Emotional retargetting. From left to right. The input image, coarse reconstruction, detailed reconstruction, emotion retargeted",
      "page": 24
    },
    {
      "caption": "Figure 17: Examples of nearest neighbor retrieval using the EmoNet [88] feature. We searched for up to 100 neighbors. We ony include",
      "page": 25
    },
    {
      "caption": "Figure 18: Examples of nearest neighbor retrieval using the ground truth annotated valence and arousal space on the AffWild2 [48] dataset.",
      "page": 26
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoNet [88]": "Deep3DFace [20]\nExpNet [15]\nMGCNet [78]\n3DDFA V2 [36]\nDECA [28]\nDECA w/ details [28]",
          "0.32\n0.80\n0.75\n0.73": "0.80\n0.75\n0.73\n0.33\n0.45\n0.42\n0.43\n0.73\n0.80\n0.71\n0.69\n0.35\n0.63\n0.62\n0.39\n0.75\n0.70\n0.69\n0.36\n0.76\n0.70\n0.69\n0.37\n0.77",
          "0.78\n0.29\n0.68\n0.65": "0.66\n0.65\n0.31\n0.78\n0.39\n0.36\n0.38\n0.64\n0.59\n0.58\n0.34\n0.77\n0.53\n0.50\n0.34\n0.73\n0.59\n0.58\n0.33\n0.74\n0.59\n0.57\n0.33\n0.77",
          "0.68": "0.65\n0.46\n0.60\n0.52\n0.59\n0.58"
        },
        {
          "EmoNet [88]": "EMOCA (Ours)\nEMOCA w/ details (Ours)",
          "0.32\n0.80\n0.75\n0.73": "0.78\n0.77\n0.31\n0.81\n0.77\n0.76\n0.31\n0.81",
          "0.78\n0.29\n0.68\n0.65": "0.69\n0.68\n0.30\n0.81\n0.70\n0.69\n0.29\n0.83",
          "0.68": "0.68\n0.69"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "BFM to FLAME",
      "venue": "BFM to FLAME"
    },
    {
      "citation_id": "2",
      "title": "Deep3DFaceRecon PyTorch",
      "year": "2021",
      "venue": "Deep3DFaceRecon PyTorch"
    },
    {
      "citation_id": "3",
      "title": "Multimodal behavior analysis in the wild: An introduction",
      "authors": [
        "Xavier Alameda-Pineda",
        "Elisa Ricci",
        "Nicu Sebe"
      ],
      "year": "2019",
      "venue": "Multimodal Behavior Analysis in the Wild, Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Inverse rendering of faces with a 3D morphable model. Transactions on Pattern Analysis and Machine Intelligence (PAMI)",
      "authors": [
        "Oswald Aldrian",
        "William Ap Smith"
      ],
      "year": "2013",
      "venue": "Inverse rendering of faces with a 3D morphable model. Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "5",
      "title": "Fitting a 3D morphable model to edges: A comparison between hard and soft correspondences",
      "authors": [
        "Anil Bas",
        "A William",
        "Timo Smith",
        "Stefanie Bolkart",
        "Wuhrer"
      ],
      "year": "2017",
      "venue": "Asian Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "6",
      "title": "Fully automated facial expression recognition using 3D morphable model and mesh-local binary pattern",
      "authors": [
        "Hela Bejaoui",
        "Haythem Ghazouani",
        "Walid Barhoumi"
      ],
      "year": "2017",
      "venue": "Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "7",
      "title": "EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Carlos Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martínez"
      ],
      "year": "2016",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Reanimating faces in images and video",
      "authors": [
        "Curzio Volker Blanz",
        "Tomaso Basso",
        "Thomas Poggio",
        "Vetter"
      ],
      "year": "2003",
      "venue": "Computer Graphics Forum (Proc. EUROGRAPHICS)"
    },
    {
      "citation_id": "9",
      "title": "Face identification across different poses and illuminations with a 3D morphable model",
      "authors": [
        "Sami Volker Blanz",
        "Thomas Romdhani",
        "Vetter"
      ],
      "year": "2002",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "10",
      "title": "A morphable model for the synthesis of 3D faces",
      "authors": [
        "Volker Blanz",
        "Thomas Vetter"
      ],
      "year": "1999",
      "venue": "SIGGRAPH"
    },
    {
      "citation_id": "11",
      "title": "The Oxford Handbook of Affective Computing",
      "authors": [
        "Rafael Calvo",
        "D' Sidney",
        "Jonathan Mello",
        "Arvid Gratch",
        "Kappas"
      ],
      "year": "2014",
      "venue": "The Oxford Handbook of Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "FaceWarehouse: A 3D facial expression database for visual computing",
      "authors": [
        "Chen Cao",
        "Yanlin Weng",
        "Shun Zhou",
        "Yiying Tong",
        "Keliang Zhou"
      ],
      "year": "2014",
      "venue": "Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "13",
      "title": "Crema-d: Crowdsourced emotional multimodal actors dataset. IEEE transactions on affective computing",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma ; Qiong",
        "Li Cao",
        "Weidi Shen",
        "Omkar Xie",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2014",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "14",
      "title": "ExpNet: Landmarkfree, deep, 3D facial expressions",
      "authors": [
        "Feng-Ju Chang",
        "Anh Tuan Tran",
        "Tal Hassner",
        "Iacopo Masi",
        "Ram Nevatia",
        "Gerard Medioni"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "15",
      "title": "SIDER: single-image neural optimization for facial geometric detail recovery",
      "authors": [
        "Aggelina Chatziagapi",
        "Shahrukh Athar",
        "Francesc Moreno-Noguer",
        "Dimitris Samaras"
      ],
      "year": "2021",
      "venue": "International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "16",
      "title": "VoxCeleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Conference of the International Speech Communication Association (IN-TERSPEECH)"
    },
    {
      "citation_id": "17",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "RetinaFace: Single-shot multi-level face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Evangelos Ververas",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Accurate 3D face reconstruction with weakly-supervised learning: From single image to image set",
      "authors": [
        "Yu Deng",
        "Jiaolong Yang",
        "Sicheng Xu",
        "Dong Chen",
        "Yunde Jia",
        "Xin Tong"
      ],
      "year": "2006",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)"
    },
    {
      "citation_id": "20",
      "title": "End-to-end 3D face reconstruction with deep neural networks",
      "authors": [
        "Pengfei Dou",
        "K Shishir",
        "Ioannis Shah",
        "Kakadiaris"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "National Academy of Sciences"
    },
    {
      "citation_id": "22",
      "title": "Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter. 3D morphable face models -past, present, and future",
      "authors": [
        "Bernhard Egger",
        "A William",
        "Ayush Smith",
        "Stefanie Tewari",
        "Michael Wuhrer",
        "Thabo Zollhöfer",
        "Florian Beeler",
        "Timo Bernard",
        "Adam Bolkart",
        "Kortylewski"
      ],
      "year": "2020",
      "venue": "Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "23",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "24",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "25",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "26",
      "title": "Facial expression recognition with local binary patterns and linear programming",
      "authors": [
        "Xiaoyi Feng",
        "M Pietikainen",
        "Abdenour Hadid"
      ],
      "year": "2005",
      "venue": "Pattern Recognition And Image Analysis"
    },
    {
      "citation_id": "27",
      "title": "Learning an animatable detailed 3D face model from in-the-wild images",
      "authors": [
        "Yao Feng",
        "Haiwen Feng",
        "Michael Black",
        "Timo Bolkart"
      ],
      "year": "2021",
      "venue": "Proc. SIGGRAPH)"
    },
    {
      "citation_id": "28",
      "title": "Joint 3D face reconstruction and dense alignment with position map regression network",
      "authors": [
        "Yao Feng",
        "Fan Wu",
        "Xiaohu Shao",
        "Yanfeng Wang",
        "Xi Zhou"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "29",
      "title": "Evaluation of dense 3D reconstruction from 2D face images in the wild",
      "authors": [
        "Zhen-Hua Feng",
        "Patrik Huber",
        "Josef Kittler",
        "Peter Hancock",
        "Xiao-Jun Wu",
        "Qijun Zhao",
        "Paul Koppen",
        "Matthias Rätsch"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "30",
      "title": "Shake-shake regularization",
      "authors": [
        "Xavier Gastaldi"
      ],
      "year": "2017",
      "venue": "Shake-shake regularization"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised training for 3D morphable model regression",
      "authors": [
        "Kyle Genova",
        "Forrester Cole",
        "Aaron Maschinot",
        "Aaron Sarna",
        "Daniel Vlasic",
        "William Freeman"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "Morphable face models-an open framework",
      "authors": [
        "Thomas Gerig",
        "Andreas Morel-Forster",
        "Clemens Blumer",
        "Bernhard Egger",
        "Marcel Luthi",
        "Sandro Schönborn",
        "Thomas Vetter"
      ],
      "year": "2018",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "33",
      "title": "GIF: Generative interpretable faces",
      "authors": [
        "Partha Ghosh",
        "Pravir Singh Gupta",
        "Roy Uziel",
        "Anurag Ranjan",
        "Michael Black",
        "Timo Bolkart"
      ],
      "year": "2020",
      "venue": "International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "34",
      "title": "DenseReg: Fully convolutional dense shape regression in-the-wild",
      "authors": [
        "Alp Riza",
        "George Güler",
        "Epameinondas Trigeorgis",
        "Patrick Antonakos",
        "Snape"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "35",
      "title": "Towards fast, accurate and stable 3D dense face alignment",
      "authors": [
        "Jianzhu Guo",
        "Xiangyu Zhu",
        "Yang Yang",
        "Fan Yang",
        "Zhen Lei",
        "Stan Li"
      ],
      "year": "2006",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "36",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "37",
      "title": "Avatar digitization from a single image for real-time rendering",
      "authors": [
        "Liwen Hu",
        "Shunsuke Saito",
        "Lingyu Wei",
        "Koki Nagano",
        "Jaewoo Seo",
        "Jens Fursund",
        "Iman Sadeghi",
        "Carrie Sun",
        "Yen-Chun Chen",
        "Hao Li"
      ],
      "year": "2017",
      "venue": "Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "38",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "39",
      "title": "Large pose 3D face reconstruc-tion from a single image via direct volumetric CNN regression",
      "authors": [
        "Adrian Aaron S Jackson",
        "Vasileios Bulat",
        "Georgios Argyriou",
        "Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "40",
      "title": "Handbook of face recognition",
      "authors": [
        "K Anil",
        "Stan Jain",
        "Li"
      ],
      "year": "2011",
      "venue": "Handbook of face recognition"
    },
    {
      "citation_id": "41",
      "title": "Learning free-form deformation for 3D face reconstruction from in-the-wild images",
      "authors": [
        "Harim Jung",
        "Myeong-Seok Oh",
        "Seong-Whan Lee"
      ],
      "year": "2021",
      "venue": "International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "42",
      "title": "Deep video portraits",
      "authors": [
        "Hyeongwoo Kim",
        "Pablo Garrido",
        "Ayush Tewari",
        "Weipeng Xu",
        "Justus Thies",
        "Matthias Nießner",
        "Patrick Pérez",
        "Christian Richardt",
        "Michael Zollhöfer",
        "Christian Theobalt"
      ],
      "year": "2018",
      "venue": "Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "43",
      "title": "Justus Thies, Christian Richardt, and Christian Theobalt",
      "authors": [
        "Hyeongwoo Kim",
        "Michael Zollhöfer",
        "Ayush Tewari"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "45",
      "title": "Pushing the frontiers of unconstrained face detection and recognition: IARPA janus benchmark A",
      "authors": [
        "Brendan Klare",
        "Ben Klein",
        "Emma Taborsky",
        "Austin Blanton",
        "Jordan Cheney",
        "Kristen Allen",
        "Patrick Grother",
        "Alan Mah",
        "Mark Burge",
        "Anil Jain"
      ],
      "year": "2015",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "46",
      "title": "look ma, no landmarks!\" -unsupervised, model-based dense face alignment",
      "authors": [
        "Tatsuro Koizumi",
        "William Smith"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "47",
      "title": "Aff-Wild2: Extending the Aff-Wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-Wild2: Extending the Aff-Wild database for affect recognition"
    },
    {
      "citation_id": "48",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "49",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Björn Schuller",
        "Kam Star",
        "Elnar Hajiyev",
        "Maja Pantic"
      ],
      "year": "2021",
      "venue": "Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "50",
      "title": "Real-time facial expression recognition \"in the wild\" by disentangling 3d expression from identity",
      "authors": [
        "Mohammad Rami Koujan",
        "Luma Alharbawee"
      ],
      "year": "2020",
      "venue": "International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "51",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Learning a model of facial shape and expression from 4D scans",
      "authors": [
        "Tianye Li",
        "Timo Bolkart",
        "Michael Black",
        "Hao Li",
        "Javier Romero"
      ],
      "year": "2017",
      "venue": "Proc. SIGGRAPH Asia)"
    },
    {
      "citation_id": "53",
      "title": "Dense face alignment",
      "authors": [
        "Yaojie Liu",
        "Amin Jourabloo",
        "William Ren",
        "Xiaoming Liu"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision Workshops (ICCV-W)"
    },
    {
      "citation_id": "54",
      "title": "Swin Transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "55",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)"
    },
    {
      "citation_id": "56",
      "title": "Extended DISFA dataset: Investigating posed and spontaneous facial expressions",
      "authors": [
        "Mohammad Mavadati",
        "Peyten Sanger",
        "Mohammad Mahoor"
      ],
      "year": "2016",
      "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)"
    },
    {
      "citation_id": "57",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "Mohammad Mahoor",
        "Kevin Bartlett",
        "Philip Trinh",
        "Jeffrey Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "3D approaches and challenges in facial expression recognition algorithms -a literature review",
      "authors": [
        "Francesca Nonis",
        "Nicole Dagnes",
        "Federica Marcolin",
        "Enrico Vezzetti"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "60",
      "title": "Expert system for automatic analysis of facial expressions",
      "authors": [
        "Maja Pantic",
        "J Léon",
        "Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "61",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "Michel Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "62",
      "title": "Learning continuous signed distance functions for shape representation",
      "authors": [
        "Jeong Joon Park",
        "Peter Florence",
        "Julian Straub",
        "Richard Newcombe",
        "Steven Lovegrove",
        "Deepsdf"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "63",
      "title": "A 3D face model for pose and illumination invariant face recognition",
      "authors": [
        "Pascal Paysan",
        "Reinhard Knothe",
        "Brian Amberg",
        "Sami Romdhani",
        "Thomas Vetter"
      ],
      "year": "2009",
      "venue": "International Conference on Advanced Video and Signal based Surveillance (AAAI)"
    },
    {
      "citation_id": "64",
      "title": "Towards a complete 3D morphable model of the human head",
      "authors": [
        "Stylianos Ploumpis",
        "Evangelos Ververas",
        "O' Eimear",
        "Stylianos Sullivan",
        "Haoyang Moschoglou",
        "Nick Wang",
        "Pears",
        "A William",
        "Baris Smith",
        "Stefanos Gecer",
        "Zafeiriou"
      ],
      "year": "2021",
      "venue": "Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "65",
      "title": "Annual Conference on Computer Graphics and Interactive Techniques",
      "authors": [
        "Ravi Ramamoorthi",
        "Pat Hanrahan"
      ],
      "year": "2001",
      "venue": "Annual Conference on Computer Graphics and Interactive Techniques"
    },
    {
      "citation_id": "66",
      "title": "Human facial expression recognition using a 3D morphable model",
      "authors": [
        "Subramanian Ramanathan",
        "A Ashraf",
        "Y Kassim",
        "Wu Venkatesh",
        "Wah Sin"
      ],
      "year": "2006",
      "venue": "International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "67",
      "title": "Accelerating 3D deep learning with pytorch3d",
      "authors": [
        "Nikhila Ravi",
        "Jeremy Reizenstein",
        "David Novotny",
        "Taylor Gordon",
        "Wan-Yen Lo",
        "Justin Johnson",
        "Georgia Gkioxari"
      ],
      "year": "2020",
      "venue": "Accelerating 3D deep learning with pytorch3d",
      "arxiv": "arXiv:2007.08501"
    },
    {
      "citation_id": "68",
      "title": "3D face reconstruction by learning from synthetic data",
      "authors": [
        "E Richardson",
        "M Sela",
        "R Kimmel"
      ],
      "year": "2016",
      "venue": "International Conference on 3D Vision (3DV)"
    },
    {
      "citation_id": "69",
      "title": "Face identification by fitting a 3D morphable model using linear shape and texture error functions",
      "authors": [
        "Sami Romdhani",
        "Thomas Volker Blanz",
        "Vetter"
      ],
      "year": "2002",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "70",
      "title": "Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior",
      "authors": [
        "S Romdhani",
        "T Vetter"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "71",
      "title": "SADRNet: Self-aligned dual face regression networks for robust 3D dense face alignment and reconstruction",
      "authors": [
        "Changqing Zeyu Ruan",
        "Longhai Zou",
        "Gangshan Wu",
        "Limin Wu",
        "Wang"
      ],
      "year": "2021",
      "venue": "Transactions on Image Processing"
    },
    {
      "citation_id": "72",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "73",
      "title": "Static and dynamic 3D facial expression recognition: A comprehensive survey",
      "authors": [
        "Georgia Sandbach",
        "Stefanos Zafeiriou",
        "Maja Pantic",
        "Lijun Yin"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "74",
      "title": "Learning to regress 3D face shape and expression from an image without 3D supervision",
      "authors": [
        "Soubhik Sanyal",
        "Timo Bolkart",
        "Haiwen Feng",
        "Michael Black"
      ],
      "year": "2005",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "75",
      "title": "Unrestricted facial geometry reconstruction using image-toimage translation",
      "authors": [
        "Matan Sela",
        "Elad Richardson",
        "Ron Kimmel"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "76",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "77",
      "title": "Self-supervised monocular 3D face reconstruction by occlusion-aware multi-view geometry consistency",
      "authors": [
        "Jiaxiang Shang",
        "Tianwei Shen",
        "Shiwei Li",
        "Lei Zhou",
        "Mingmin Zhen",
        "Tian Fang",
        "Long Quan"
      ],
      "year": "2006",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "78",
      "title": "Pose-robust facial expression recognition by 3D morphable model learning",
      "authors": [
        "Yingyan Shi",
        "Qiaosha Zou",
        "Yiyun Zhang"
      ],
      "year": "2020",
      "venue": "International Conference on Computer and Communications (ICCC)"
    },
    {
      "citation_id": "79",
      "title": "The belfast induced natural emotion database",
      "authors": [
        "Ian Sneddon",
        "Margaret Mcrorie",
        "Gary Mckeown",
        "Jennifer Hanratty"
      ],
      "year": "2013",
      "venue": "Transactions on Affective Computing"
    },
    {
      "citation_id": "80",
      "title": "Unsupervised generative 3D shape learning from natural images",
      "authors": [
        "Attila Szabó",
        "Givi Meishvili",
        "Paolo Favaro"
      ],
      "year": "2019",
      "venue": "Unsupervised generative 3D shape learning from natural images"
    },
    {
      "citation_id": "81",
      "title": "FML: face model learning from videos",
      "authors": [
        "Ayush Tewari",
        "Florian Bernard",
        "Pablo Garrido",
        "Gaurav Bharaj",
        "Mohamed Elgharib",
        "Hans-Peter Seidel",
        "Patrick Pérez",
        "Michael Zollhöfer",
        "Christian Theobalt"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "82",
      "title": "StyleRig: Rigging stylegan for 3D control over portrait images",
      "authors": [
        "Ayush Tewari",
        "Mohamed Elgharib",
        "Gaurav Bharaj",
        "Florian Bernard",
        "Hans-Peter Seidel",
        "Patrick Pérez",
        "Michael Zollhöfer",
        "Christian Theobalt"
      ],
      "year": "2020",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "83",
      "title": "Self-supervised multi-level face model learning for monocular reconstruction at over 250 Hz",
      "authors": [
        "Ayush Tewari",
        "Michael Zollhöfer",
        "Pablo Garrido",
        "Florian Bernard",
        "Hyeongwoo Kim",
        "Patrick Pérez",
        "Christian Theobalt"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "84",
      "title": "MoFA: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction",
      "authors": [
        "Ayush Tewari",
        "Michael Zollhöfer",
        "Hyeongwoo Kim",
        "Pablo Garrido",
        "Florian Bernard",
        "Patrick Perez",
        "Christian Theobalt"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "85",
      "title": "Face2Face: Real-time face capture and reenactment of RGB videos",
      "authors": [
        "Justus Thies",
        "Michael Zollhöfer",
        "Marc Stamminger",
        "Christian Theobalt",
        "Matthias Nießner"
      ],
      "year": "2016",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "86",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Ying-Li Tian",
        "Takeo Kanade",
        "Jeffrey Cohn"
      ],
      "year": "2001",
      "venue": "Transactions on Pattern Analysis and Machine Intelligence (PAMI)"
    },
    {
      "citation_id": "87",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "88",
      "title": "Regressing robust and discriminative 3D morphable models with a very deep neural network",
      "authors": [
        "Anh Tuan Tran",
        "Tal Hassner",
        "Iacopo Masi",
        "Gerard Medioni"
      ],
      "year": "2017",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "89",
      "title": "Extreme 3D face reconstruction: Seeing through occlusions",
      "authors": [
        "Anh Tuan Tran",
        "Tal Hassner",
        "Iacopo Masi",
        "Eran Paz",
        "Yuval Nirkin",
        "Gérard Medioni"
      ],
      "year": "2018",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "90",
      "title": "Towards highfidelity nonlinear 3D face morphable model",
      "authors": [
        "Luan Tran",
        "Feng Liu",
        "Xiaoming Liu"
      ],
      "year": "2019",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "91",
      "title": "Estimating coloured 3D face models from single images: An example based approach",
      "authors": [
        "Thomas Vetter",
        "Volker Blanz"
      ],
      "year": "1998",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "92",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Linsen Song",
        "Zhuoqian Yang",
        "Wayne Wu",
        "Chen Qian",
        "Ran He",
        "Yu Qiao",
        "Chen Loy"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "93",
      "title": "Racial faces in the wild: Reducing racial bias by information maximization adaptation network",
      "authors": [
        "Mei Wang",
        "Weihong Deng",
        "Jiani Hu",
        "Xunqiang Tao",
        "Yaohai Huang"
      ],
      "year": "2002",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "94",
      "title": "3D dense face alignment via graph convolution networks",
      "authors": [
        "Huawei Wei",
        "Shuang Liang",
        "Yichen Wei"
      ],
      "year": "2019",
      "venue": "3D dense face alignment via graph convolution networks",
      "arxiv": "arXiv:1904.05562"
    },
    {
      "citation_id": "95",
      "title": "Capturing subtle facial motions in 3D face tracking",
      "authors": [
        "Zhen Wen",
        "Thomas Huang"
      ],
      "year": "2003",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "96",
      "title": "Unsupervised learning of probably symmetric deformable 3D objects from images in the wild",
      "authors": [
        "Shangzhe Wu",
        "Christian Rupprecht",
        "Andrea Vedaldi"
      ],
      "year": "2020",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "97",
      "title": "FaceScape: a largescale high quality 3D face dataset and detailed riggable 3D face prediction",
      "authors": [
        "Haotian Yang",
        "Hao Zhu",
        "Yanru Wang",
        "Mingkai Huang",
        "Qiu Shen",
        "Ruigang Yang",
        "Xun Cao"
      ],
      "year": "2020",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "98",
      "title": "DF2Net: A dense-fine-finer network for detailed 3D face reconstruction",
      "authors": [
        "Xiaoxing Zeng",
        "Xiaojiang Peng",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "99",
      "title": "Face alignment across large poses: A 3D solution",
      "authors": [
        "Xiangyu Zhu",
        "Zhen Lei",
        "Xiaoming Liu",
        "Hailin Shi",
        "Stan Li"
      ],
      "year": "2016",
      "venue": "Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "100",
      "title": "State of the art on monocular 3D face reconstruction, tracking, and applications",
      "authors": [
        "Michael Zollhöfer",
        "Justus Thies",
        "Pablo Garrido",
        "Derek Bradley",
        "Thabo Beeler",
        "Patrick Pérez",
        "Marc Stamminger",
        "Matthias Nießner",
        "Christian Theobalt"
      ],
      "year": "2018",
      "venue": "Computer Graphics Forum"
    }
  ]
}