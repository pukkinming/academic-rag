{
  "paper_id": "2503.16584v1",
  "title": "Eva-Med: An Enhanced Valence-Arousal Multimodal Emotion Dataset For Emotion Recognition",
  "published": "2025-03-20T14:41:50Z",
  "authors": [
    "Xin Huang",
    "Shiyao Zhu",
    "Ziyu Wang",
    "Yaping He",
    "Hao Jin",
    "Zhengkui Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We introduce a novel multimodal emotion recognition dataset that enhances the precision of Valence-Arousal Model while accounting for individual differences. This dataset includes electroencephalography (EEG), electrocardiography (ECG), and pulse interval (PI) from 64 participants. Data collection employed two emotion induction paradigms: video stimuli that targeted different valence levels (positive, neutral, and negative) and the Mannheim Multicomponent Stress Test (MMST), which induced high arousal through cognitive, emotional, and social stressors. To enrich the dataset, participants' personality traits, anxiety, depression, and emotional states were assessed using validated questionnaires. By capturing a broad spectrum of affective responses while accounting for individual differences, this dataset provides a robust resource for precise emotion modeling. The integration of multimodal physiological data with psychological assessments lays a strong foundation for personalized emotion recognition. We anticipate this resource will support the development of more accurate, adaptive, and individualized emotion recognition systems across diverse applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Summary",
      "text": "Emotion recognition plays a vital role in various fields, including mental health support, human-computer interaction, education, and marketing. By accurately identifying and measuring emotional states, emotion recognition has the potential to create more personalized experiences, enhance user engagement, and support mental health and well-being  [2] .\n\nFor instance, in mental health, emotion recognition could monitor fluctuations in an individual's mood in real-time, enabling the early detection of risks for psychological disorders such as depression and anxiety  [3] , which allows for timely interventions. In recent years, advancements in the interdisciplinary fields of affective computing and neuroscience have significantly accelerated the development of emotion recognition technology  [16] . Furthermore, progress in deep learning algorithms and multimodal data fusion has greatly improved the accuracy and adaptability of emotion recognition systems  [5] .\n\nExisting multimodal emotion datasets, such as DEAP  [8] , AMIGOS  [13] , and SEED-VII  [6] , have significantly advanced the field by integrating diverse physiological signals, including electroencephalography (EEG), electrocardiography (ECG), and self-reported emotional labels. These datasets have enabled the development of machine learning models that can recognize complex emotional states. However, several limitations still exist  [14] . One major issue is that most current emotion studies rely on the continuous emotional model to differentiate between valence but do not address the distinctions in arousal  [7, 13] . Additionally, while individual differences such as personality traits and anxiety levels are known to significantly impact emotional processing, there is a lack of comprehensive datasets that systematically incorporate these factors  [14] . This gap restricts the depth of analysis, particularly in understanding how personal characteristics influence emotional responses under various conditions.\n\nTo address these limitations, we present a novel multimodal emotion recognition dataset designed to enhance the precision of emotional dimension modeling and systematically account for individual differences. Our approach is grounded in the widely accepted two-dimensional model of emotion, which conceptualizes emotions along the orthogonal dimensions of valence and arousal  [22] . To comprehensively capture emotional variability, we employed two complementary emotion elicitation paradigms: video-based emotion induction and the Mannheim Multicomponent Stress Test (MMST). While both paradigms collect data on valence and arousal, they exhibit distinct strengths in eliciting specific emotional responses. Video-based tasks are particularly effective in inducing a range of emotional valence, including positive, negative, and neutral states, while simultaneously triggering moderate levels of arousal. In contrast, the MMST is designed to evoke high arousal levels through a combination of stress-inducing components, such as time pressure, negative feedback, and cognitive load, while also eliciting emotional valence shifts associated with stress responses  [15] . This dual-paradigm approach ensures broad coverage of emotional states, ranging from low to high arousal and spanning the full spectrum of valence (positive, neutral, and negative), thereby enhancing ecological validity and dataset diversity.\n\nIn addition to comprehensive emotional dimension modeling, our dataset integrates assessments of individual differences to explore their influence on emotional processing. Participants completed a series of psychometrically validated questionnaires designed to measure personality traits, anxiety, depression, and life events. By incorporating these individual characteristics, the study enables a deeper analysis of how physiological and behavioral responses are modulated by emotional stimuli, providing a richer context for emotion recognition research.\n\nRegarding data collection, we combined high-precision physiological recording techniques with exploratory applications of wearable device technology. EEG and ECG data were collected using laboratory-grade equipment, whereas pulse interval (PI) data were obtained from wrist-worn wearable devices. Although wearable technology is currently constrained by accuracy and data resolution limitations, we acknowledge its potential for future large-scale data collection. The portability and ease of use of wearable devices create possibilities for emotion data acquisition in real-world settings, enabling long-term, low-intrusion monitoring of emotional dynamics  [20, 10] . This capability not only enhances the ecological validity of emotion recognition models but also supports the development of personalized emotion-aware technologies, such as emotion-adaptive interfaces and mental health monitoring tools  [9, 17] .\n\nIn summary, this novel multimodal emotion recognition dataset is distinguished by its precise modeling of emotional dimensions and its comprehensive consideration of individual differences. The integration of controlled laboratory data with exploratory wearable device applications lays the foundation for future scalability and real-world applicability. We believe this dataset will significantly contribute to the advancement of emotion recognition research, fostering the development of more accurate, personalized, and context-aware affective computing systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Participants",
      "text": "A total of 64 university students (33 males and 31 females) participated in this study, with ages ranging from 19 to 26 years (M = 22.06, SD = 2.08). Participants were recruited through advertisements at universities in Beijing. All individuals were physically and mentally healthy. Inclusion criteria required participants to be over 18 years old, enrolled in college, not currently using psychotropic medications, and free from significant neurological or cardiovascular conditions. Exclusion criteria included the use of psychiatric medications within the past six months, a diagnosis of major mental health disorders such as schizophrenia or major depressive disorder, and any physiological abnormalities affecting cardiac function. The study was approved by the Ethics Committee of the Institute of Psychology, Chinese Academy of Sciences, and all participants provided informed consent before their involvement.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Procedure",
      "text": "The experiment was divided into three distinct phases (see Figure  1 ). In the initial phase, participants were instructed to relax while completing a questionnaire and undergoing physiological data collection. This phase, corresponding to a \"calm\" state, served as the baseline reference for subsequent emotional change.\n\nIn the second phase, participants underwent the emotion induction phase, which involved watching video clips corresponding to different emotional valences (positive, neutral, negative). For the first thirty participants, the viewing order was positive, neutral, and then negative; for the remaining participants, the order was reversed to negative, neutral, and then positive. This counterbalancing minimized potential carryover effects between emotional states. At the end of each video, participants completed Self-Assessment Manikin (SAM) ratings to evaluate both valence and arousal, providing subjective reports of their emotional responses to the viewed materials.\n\nIn the third phase, participants first watched a two-minute neutral video to stabilize their emotional state following the emotion induction phase. They then proceeded to perform the MMST, during which SAM ratings were obtained at 0, 3, 5, and 8 minutes, while EEG, ECG, and PI data were continuously recorded. Figure  1 . Experiment Procedures.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Stimuli",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Clips",
      "text": "To ensure emotional efficacy, a pre-experiment was conducted with 55 college students who rated the emotional intensity and differentiation of candidate clips. The final selections demonstrated strong emotional induction effects, validated through subjective ratings on a four-point Likert scale.\n\nThe video materials were classified into three categories according to emotional valence, with each video lasting for 10 minutes.\n\n• Positive: humorous short clips sourced from Chinese internet media platforms to evoke positive emotions.\n\n• Neutral: videos depicting everyday objects and landscapes with neutral background music to maintain a baseline emotional state. • Negative: documentary footage featuring interviews with left-behind children to elicit negative emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mmst Paradigm",
      "text": "The Mannheim Multicomponent Stress Test (MMST) combines five distinct stressors to elicit a heightened state of arousal. Cognitive stress was induced using the Paced Auditory Serial Addition Test (PASAT)  [11] , in which participants added consecutive numbers and selected the correct answer from 21 options within approximately 3.5 seconds per trial. Real-time feedback was provided throughout the 8-minute task. Additionally, participants performed the arithmetic task while emotionally aversive images from the Geneva Affective Picture Database (GAPED)  [4]  were presented, thereby incorporating an emotional stressor. Acoustic stress was introduced via countdown signals and explosion-like noises that intensified after incorrect responses. A motivational stressor was implemented by deducting monetary amounts from an initial balance for incorrect or delayed responses. Finally, social evaluative stress was applied through on-screen feedback, especially at the third and fifth minutes, indicating that performance was below average compared to peers. To further increase task difficulty, only responses made within a designated central area on the screen were accepted; any responses outside this area were deemed invalid regardless of correctness.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Measures",
      "text": "Participants completed a series of questionnaires and self-assessment scales to evaluate demographic information, personality traits, anxiety, depression.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Demographic Information Questionnaire",
      "text": "Collected basic demographic data, including gender, age, education level, family socioeconomic status, physical health status, and relationship status.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Psychosocial Measures",
      "text": "• Chinese Big Five Personality Inventory-15 (CBF-PI-15): assessed personality traits across the five dimensions, openness, conscientiousness, extraversion, agreeableness, and neuroticism, using 15 items rated on a 6-point scale (1 = strongly disagree, 6 = strongly agree)  [21] .\n\n• Patient Health Questionnaire-9 (PHQ-9): measured the severity of depressive symptoms through 9 items rated on a 4-point scale (0 = not at all, 3 = nearly every day)  [19] .\n\n• State-Trait Anxiety Inventory (STAI): evaluated trait anxiety levels through 20 items rated on a 4-point scale (1 = seldom, 4 = almost always)  [18] .\n\n• Adolescent Self-Rating Life Events Checklist (ASLEC): assessed the frequency and intensity of recent life events and associated stress with 27 items rated on a 6-point scale (0 = not at all, 5 = extremely)  [12] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Subjective Emotional Experience",
      "text": "SAM: quantified participants' current emotional valence and arousal levels  [1] , using a graphical representation with a 9-point scale, later converted to a 1-5 scale in increments of 0.5, shown in Figure  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Equipment",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Equipment",
      "text": "EEG signals were collected using the NeuroScan system with 64 electrodes arranged according to the international 10-20 electrode placement system. The online reference electrode was placed on the left mastoid, while the offline reference was the arithmetic average of the bilateral mastoids. The ground electrode was positioned between FPz and Fz. Vertical eye movement electrodes were placed above and below the left eye, and horizontal eye movement electrodes were placed laterally on both sides of the eyes. Data collection and real-time monitoring were performed using Scan 4.5 software.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ecg Equipment",
      "text": "ECG data were collected using the Biopac MP150 system equipped with an ECG module, which recorded single-channel ECG signals. Before data acquisition, the skin was cleaned with alcohol and the disposable electrodes were affixed to three locations: the right clavicular midline at the sternal notch level, the left lower rib middle area, and the right lower abdominal area (ground). Data were captured using AcqKnowledge 4.2 software at a sampling rate of 2000 Hz.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pi Data Collection",
      "text": "PI refers to the time between consecutive pulse waves, reflecting the variability in heartbeats over time (See Figure  4 ). PI data were collected using the consumer-grade Huawei Band 7. The device sampled data at 25 Hz, capturing data on a per-minute basis. The data were transmitted via Bluetooth for further analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Software For Data Processing",
      "text": "Data preprocessing was conducted using Python with the following packages. EEG and ECG data preprocessing were performed using the MNE toolkit, R-R interval (RRI) values were extracted from the ECG data using NumPy, outlier filtering for PI data was done using the HRV package, and the analysis of PI values was carried out using the NeuroKit2 package (Makowski et al., 2021).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Preprocessing",
      "text": "The dataset comprises EEG, ECG, and PI signals collected from 64 participants. All data segments are meticulously labeled with valence and arousal scores to facilitate comprehensive emotion recognition tasks. The preprocessing pipeline ensures data quality, consistency, and precise temporal alignment across the different physiological modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Eeg Data Preprocessing",
      "text": "Initially, spherical spline interpolation was performed to correct any bad channels in the EEG recordings. Subsequently, the data were filtered using a fourth-order zero-phase Butterworth bandpass filter with a frequency range of 0.5 Hz to 45 Hz to remove unwanted noise. Artifact-contaminated segments were then automatically identified and removed based on excessive high-frequency activity. Next, Independent Component Analysis (ICA) was applied to further eliminate artifacts such as eye blinks, cardiac signals, and movement-related noise. Finally, the cleaned EEG data were segmented into 4-second epochs according to the emotional labels and synchronized with the ECG data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ecg Data Preprocessing",
      "text": "Initially, the raw ECG data were filtered using a high-pass filter at 0.5 Hz, a 5th-order Butterworth filter, and a notch filter at 50 Hz to eliminate power line interference. Then the data were segmented based on emotional labels into 4-second intervals for subsequent statistical analysis. R-wave detection was performed based on the absolute gradient of the ECG signals, identifying the peaks corresponding to heartbeats. Subsequently, RRI, defined as the time between consecutive R-wave peaks, was calculated for further analysis.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pi Data Preprocessing",
      "text": "Anomaly filtering was performed to eliminate outliers and artifacts. Due to the minute-by-minute sampling rate of the PI data, only short-term heartbeat dynamic features could be extracted, limiting the analysis of long-term heart rate variability metrics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Records",
      "text": "The dataset was organized into four sections, each with a README.txt file explaining its contents.\n\nPhysiological Data: This folder contains three subfolders.\n\n• Collected Data: This folder contains three subfolders, each dedicated to a specific type of data: raw EEG recordings in .cnt format, multi-channel physiological data in .acq format, and raw PI data collected via a wearable device. Stimuli: This folder contains three .mp4 file.\n\n• Positive_video.mp4: short humorous clips sourced from Chinese media platforms.\n\n• Neutral_video.mp4: scenic footage with soft instrumental music.\n\n• Negative_video.mp4: documentary excerpts featuring interviews with left-behind children.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Technical Validation",
      "text": "In this study, an emotion recognition prediction model was established for the two dimensions: emotional valence and emotional arousal. Emotional valence ratings on the SAM scale were categorized into three classes: negative (1-2 points), neutral (3 points), and positive (4-5 points). In addition, an independent label calm was considered, resulting in four overall valence labels. Emotional arousal was transformed into calm (1-2 points) and stress (4-5 points) as the two emotional arousal labels. Multiple machine learning and deep learning algorithms were used for modeling.\n\nInitially, features were extracted from the preprocessed data. For EEG signals, Principal Component Analysis (PCA) was applied to reduce spatial dimensionality, and the first 10 principal components were retained. The cumulative variance explained by the principal components reached 75%. Frequency-domain features as well as time-domain and time-frequency features, were computed for each component, yielding a total of 80 features. RRI was extracted from ECG data, and a comprehensive set of 94 features was computed using time-domain, frequency-domain, and nonlinear analysis. An identical feature extraction procedure was applied to the PI data; however, due to the per-minute aggregation of PI data, the analysis was restricted to short-term heart rate dynamics, resulting in 78 features.\n\nNext, predictive models were constructed for emotional valence and emotional arousal using multiple machine learning and deep learning algorithms. The models included traditional machine learning techniques such as Support Vector Machines (SVM), Random Forest, Decision Trees, and Extreme Gradient Boosting (XGBoost), along with deep learning models such as Multi-Layer Perceptron (MLP) and 1D Convolutional Neural Networks (1dCNN). The dataset was split into 80% for training and 20% for testing, with 10-fold cross-validation used to assess model generalizability.\n\nModel performance was evaluated using accuracy and F1-score.\n\nNotably, the 1dCNN model outperformed traditional machine learning models in predicting both emotional valence and arousal, with accuracies of 90.46% and 93.44% for the EEG-based model, 82.00% and 86.65% for the ECG-based model, and 73.23% and 73.92% for the PI-based model, respectively. Despite the moderate performance of the PI-based models, their suitability for long-term, low-intrusion monitoring underscores their practical applicability in real-world emotion recognition systems.\n\nIn this study, we present a comprehensive multimodal emotion recognition dataset aimed at improving the precision of Valence-Arousal Model while systematically incorporating individual differences. By integrating EEG, ECG, and PI signals collected under controlled emotional stimuli, this dataset serves as a valuable resource for advancing research in emotion recognition and computational affective science.\n\nFuture studies can expand the dataset to include a larger and more diverse population across different age groups, cultural backgrounds, and real-world scenarios. Furthermore, enhancing the real-time performance and robustness of emotion recognition systems is crucial. By refining data diversity and enhancing real-time processing capabilities, emotion recognition systems can achieve enhanced accuracy and applicability in everyday settings and human-computer interaction.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). In the initial phase, participants were instructed to",
      "page": 2
    },
    {
      "caption": "Figure 1: Experiment Procedures.",
      "page": 3
    },
    {
      "caption": "Figure 2: Figure 2. SAM Diagram.",
      "page": 4
    },
    {
      "caption": "Figure 3: Experimental Setup for Multimodal Physiological Data Collection. Participants wore an EEG cap to record",
      "page": 5
    },
    {
      "caption": "Figure 4: Pulse Interval.",
      "page": 5
    },
    {
      "caption": "Figure 5: Dataset Structure.",
      "page": 7
    },
    {
      "caption": "Figure 6: 1dCNN Models for Emotional Valence and Arousal. (a) Confusion matrix for emotional valence prediction",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Classification Performance of Emotion Valence and Arousal Across Different Modalities (EEG, ECG, and PI).",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "Margaret Bradley",
        "Peter Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "2",
      "title": "Emotion Recognition Using Different Sensors, Emotion Models, Methods and Datasets: A Comprehensive Review",
      "authors": [
        "Yujian Cai",
        "Xingguang Li",
        "Jinsong Li"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "Psychiatric symptom versus neurocognitive correlates of diminished expressivity in schizophrenia and mood disorders",
      "authors": [
        "Alex Cohen",
        "Yunjung Kim",
        "Gina Najolia"
      ],
      "year": "2013",
      "venue": "Schizophrenia Research"
    },
    {
      "citation_id": "4",
      "title": "The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance",
      "authors": [
        "Elise Dan-Glauser",
        "Klaus Scherer"
      ],
      "year": "2011",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "SEED-VII: A Multimodal Dataset of Six Basic Emotions with Continuous Labels for Emotion Recognition",
      "authors": [
        "Wei-Bang Jiang",
        "Xuan-Hao Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "Conference Name: IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "8",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Emotion Recognition Using a Glasses-Type Wearable Device via Multi-Channel Facial Responses",
      "authors": [
        "Jangho Kwon",
        "Jihyeon Ha",
        "Da-Hye Kim",
        "Jun Choi",
        "Laehyun Kim"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Current Advances in Wearable Devices and Their Sensors in Patients With Depression",
      "authors": [
        "Seunggyu Lee",
        "Hyewon Kim",
        "Jin Park",
        "Hong Jin"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "11",
      "title": "A modified computer version of the Paced Auditory Serial Addition Task (PASAT) as a laboratory-based stressor",
      "authors": [
        "C Lejuez",
        "Christopher Kahler",
        "Richard Brown"
      ],
      "year": "2003",
      "venue": "the Behavior Therapist"
    },
    {
      "citation_id": "12",
      "title": "The Adolescent Self-Rating Life Events Checklist and its reliability and validity",
      "authors": [
        "Xianchen Liu",
        "Lianqi Liu",
        "Jie Yang",
        "Fuxun Chai",
        "Aizhen Wang",
        "Liangming Sun",
        "Guifan Zhao",
        "Dengdai Ma"
      ],
      "year": "1997",
      "venue": "Chinese Journal of Clinical Psychology"
    },
    {
      "citation_id": "13",
      "title": "WEMAC: Women and Emotion Multi-modal Affective Computing dataset. Scientific Data",
      "authors": [
        "Jose Miranda Calero",
        "Laura Gutiérrez-Martín",
        "Esther Rituerto-González",
        "Elena Romero-Perales",
        "Jose Lanza-Gutiérrez",
        "Carmen Peláez-Moreno",
        "Celia López-Ongil"
      ],
      "year": "2024",
      "venue": "WEMAC: Women and Emotion Multi-modal Affective Computing dataset. Scientific Data"
    },
    {
      "citation_id": "14",
      "title": "Multimodal emotion recognition: A comprehensive review, trends, and challenges",
      "authors": [
        "Manju Priya",
        "Arthanarisamy Ramaswamy",
        "Suja Palaniswamy"
      ],
      "year": "1563",
      "venue": "WIREs Data Mining and Knowledge Discovery",
      "doi": "10.1002/widm.1563"
    },
    {
      "citation_id": "15",
      "title": "Salivary cortisol, heart rate, electrodermal activity and subjective stress responses to the Mannheim Multicomponent Stress Test (MMST)",
      "authors": [
        "Tatyana Reinhardt",
        "Christian Schmahl",
        "Stefan Wüst",
        "Martin Bohus"
      ],
      "year": "2012",
      "venue": "Psychiatry Research"
    },
    {
      "citation_id": "16",
      "title": "Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables",
      "authors": [
        "Stanisław Saganowski",
        "Joanna Komoszyńska",
        "Maciej Behnke",
        "Bartosz Perz",
        "Dominika Kunc",
        "Bartłomiej Klich",
        "D Łukasz",
        "Przemysław Kaczmarek",
        "Kazienko"
      ],
      "year": "2022",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "17",
      "title": "Wearable Emotion Recognition Using Heart Rate Data from a Smart Bracelet",
      "authors": [
        "Lin Shu",
        "Yang Yu",
        "Wenzhuo Chen",
        "Haoqiang Hua",
        "Qin Li",
        "Jianxiu Jin",
        "Xiangmin Xu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "The State-Trait Anxiety Inventory",
      "authors": [
        "Charles Spielberger",
        "Fernando Gonzalez-Reigosa",
        "Angel Martinez-Urrutia",
        "F Luiz",
        "Diana Natalicio",
        "Natalicio"
      ],
      "year": "2017",
      "venue": "The State-Trait Anxiety Inventory"
    },
    {
      "citation_id": "19",
      "title": "Reliability and validity of the Chinese version of the Patient Health Questionnaire (PHQ-9) in the general population",
      "authors": [
        "Wenzheng Wang",
        "Qian Bian",
        "Yan Zhao",
        "Xu Li",
        "Wenwen Wang",
        "Jiang Du",
        "Guofang Zhang",
        "Qing Zhou",
        "Min Zhao"
      ],
      "year": "2014",
      "venue": "General Hospital Psychiatry"
    },
    {
      "citation_id": "20",
      "title": "A Survey of Emotion Recognition using Physiological Signal in Wearable Devices",
      "authors": [
        "Z Hamidan",
        "Ridi Wijasena",
        "Sunu Ferdiana",
        "Wibirama"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS)"
    },
    {
      "citation_id": "21",
      "title": "The development and psychometric evaluation of the Chinese Big Five Personality Inventory-15",
      "authors": [
        "Xintong Zhang",
        "Meng-Cheng Wang",
        "Lingnan He",
        "Luo Jie",
        "Jiaxin Deng"
      ],
      "year": "2019",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "22",
      "title": "Emotion Recognition From Multiple Modalities: Fundamentals and methodologies",
      "authors": [
        "Sicheng Zhao",
        "Guoli Jia",
        "Jufeng Yang",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2021",
      "venue": "Conference Name: IEEE Signal Processing Magazine"
    }
  ]
}