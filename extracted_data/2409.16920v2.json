{
  "paper_id": "2409.16920v2",
  "title": "Cross-Lingual Speech Emotion Recognition: Humans Vs. Self-Supervised Models",
  "published": "2024-09-25T13:27:17Z",
  "authors": [
    "Zhichen Han",
    "Tianqi Geng",
    "Hui Feng",
    "Jiahong Yuan",
    "Korin Richmond",
    "Yuanchao Li"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Speech Emotion Diarization",
    "Cross-Lingual Evaluation",
    "Self-Supervised Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance-and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The advancement of Self-Supervised Learning (SSL) has led to the development of powerful pre-trained models, such as Wav2vec 2.0 (W2V2)  [1]  and WavLM  [2] , including their multilingual variants. These models have demonstrated remarkable success across a range of downstream speech tasks, including Speech Emotion Recognition (SER)  [3] . To further enhance their adaptability across different languages and datasets for SER, Parameter-Efficient Fine-Tuning (PEFT) has been utilized to improve the efficacy of SSL models while minimizing fine-tuning requirements  [4] ,  [5] .\n\nNevertheless, cross-lingual SER remains a significant challenge due to language and cultural differences  [6] . Typically, both traditional and SSL models require sufficient training data in the target language to achieve satisfactory cross-lingual SER performance, which is often infeasible for languages lacking emotional speech datasets  [7] ,  [8] . For humans, however, although cross-lingual barriers exist  [9] , emotions in speech are universally distinguishable as humans are less affected by cross-lingual differences  [10] .\n\nWhile some research has explored the use of SSL models for crosslingual and multilingual SER  [11] , there has been little investigation into how these models compare to human performance. To this end, we raise four key questions:\n\n1) Can SSL-based models achieve competitive SER performance to that of humans?\n\n2) How to better fine-tune SSL models for SER in cross-lingual scenarios?\n\n3) Does dialect have an impact on human perception in crosslingual SER? † Corresponding author. yuanchao.li@ed.ac.uk 4) Can SSL-based models identify emotionally salient segments similar to human behaviors?\n\nTo answer the above questions, we conduct a comparative study between humans and SSL models, specifically:\n\n• We perform a layer-wise analysis and investigate various PEFT strategies for SSL models in monolingual, cross-lingual, and transfer learning settings, comparing SER performance with human performance across emotions. • We evaluate SER performance on Tianjin speech (a Chinese dialect), exploring the impact of dialect on human listeners with and without linguistic and paralinguistic background knowledge. • We assess both human and SSL model performance on the Speech Emotion Diarization (SED) task (i.e., segment-level SER), aiming to compare their ability to detect prominent emotion segments.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "On the model side, previous studies have typically fine-tuned SER models using target language data, but have observed a significant drop in performance when shifting from monolingual to cross-lingual conditions  [7] ,  [12] . Additionally, adversarial neural networks in unsupervised settings have been explored for cross-lingual adaptation  [13] ,  [14] . More recently,  [15]  introduced a layer-anchoring mechanism to facilitate emotion transfer, accounting for the taskspecific nature and hierarchical structure of speech models. On the human side,  [10]  found that SVM models outperformed humans in monolingual settings, whereas humans were less affected by crosslingual challenges. Further research by  [16]  concluded that human cross-lingual capabilities in SER are generally robust. Despite this progress, comparative studies between humans and models remain lacking, leading to an insufficient understanding of human-model comparison. To our knowledge, we are the first to conduct a comparative study between humans and SSL models, exploring not only utterance-level SER but also fine-grained emotion perception (i.e., SED), the impact of dialect, and fine-tuning strategies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Materials And Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Datasets And Models",
      "text": "As various tasks are investigated in this work, we use multiple datasets and models. For the datasets, four public emotion corpora and a non-public dialect corpus are used:\n\n• ESD: a Mandarin Chinese (CN) emotion corpus  [17] , containing utterances spoken by ten native CN speakers (five male, five female) across five emotion categories. • PAVOQUE: a German (DE) emotion corpus  [18] , featuring a professional male actor with five emotion categories, where neutral comprises over 50% of the dataset.\n\n• IEMOCAP: an English (EN) emotion corpus  [19] , where five male and female speakers were paired to record scripted and improvised emotional utterances, divided into nine emotion categories.\n\n• ZED: an English emotion corpus specifically designed for the SED task  [20] , with speech data annotated by humans at both utterance and sub-utterance (segment) levels.\n\n• TJD: a non-public Tianjin (TJ) Chinese dialect corpus collected in our previous work  [21] . It was recorded and annotated at Tianjin University by two native Tianjin dialect speakers. It includes three functional categories (question, negation, expectation), approximated to emotions due to high acoustic similarity. According to annotators, negation resembles anger, and expectation resembles happiness. Tianjin dialect is known for its complex tone sandhi patterns while featuring a similar but slightly different tone system to Mandarin  [22] . Native speakers of the Tianjin dialect convey emotions more directly with noticeable sonorous vowels and faster speech  [23] . For the models, we use three W2V2 base models pre-trained on Mandarin CN 1  , DE 2  , and EN  3  , along with a WavLM large model trained on EN emotional speech  4  . The following tasks are conducted using different models and datasets for specific purposes.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Layer-Wise Analysis Of Ssl Models",
      "text": "In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP; the models: W2V2-CN, -DE, -EN; and the emotions: angry, happy, neutral, and sad.\n\nSSL models encode speech information across different layers; specifically, in SER tasks, speech representations from the middle layers often yield higher performance  [24] . Therefore, we perform a layer-wise analysis to identify the optimal layer for monolingual and cross-lingual SER. SSL models are used as feature extractors with all parameters frozen, and Unweighted Accuracy (UA) is used as the evaluation metric. The analysis is conducted in the following settings:\n\n• Monolingual (Mono): The model is fine-tuned with both training and test data from speech in the same language as its pre-training language. For example, W2V2-CN is fine-tuned using CN data (ESD) as both training and test data. • Cross-lingual (Cross): The model is fine-tuned using its pretraining language as training data and a different language as test data. For example, W2V2-CN is fine-tuned using CN data (ESD) and tested on DE data (PAVOQUE) or EN data (IEMOCAP). • Transfer learning (Trans): The model is fine-tuned and tested on a language different from its pre-training language. For example, W2V2-CN is fine-tuned and tested on either DE data (PAVOQUE) or EN data (IEMOCAP).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Peft Of Ssl Models For Cross-Lingual Ser",
      "text": "In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP; the models: W2V2-CN, -DE; and the emotions: angry, happy, neutral, and sad.\n\nAfter the layer-wise analysis, the best-performing layers are further fine-tuned using various PEFT strategies to enhance performance. We apply the Low-Rank Adapter (LoRA)  [25] , Bottleneck Adapter (BA)  [26] , and Weighted Gating (WG)  [5] . Additionally, a two-stage fine-tuning  [5]  is performed: the model is first fine-tuned on the source language, then on the target language once the first fine-tuning converges.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Comparison Of Ssl Models With Human Evaluation",
      "text": "In this task, we use all the datasets and models. For SER, we use the emotions: angry, happy, neutral, and sad; while for SED, we exclude neutral as it does not contain emotion variation to perceive and segment.\n\nSix native DE speakers (one male, five female) and six native CN speakers (two male, four female), with no prior knowledge of each other's language, are recruited for the human evaluation from the University of Edinburgh and Tianjin University. All participants have studied English for many years with sufficient skills (e.g., IELTS score ≥ 6.5). The webMUSHRA interface  [27]  is used to create the experimental tests.\n\nFor SER, participants listen to speech samples and identify the conveyed emotion. We use UA as the evaluation metric, consistent with the model performance evaluation. Additionally, to investigate fine-grained speech emotion expression, we perform SED, where participants first listen to speech samples and label the emotion, as in the SER task. Subsequently, they clip the speech and select the segment that most prominently expresses the emotion. Following  [20] , we use the Emotion Diarization Error Rate (EDER) as the metric, which calculates the error rate of diarization results, including missed emotions (ME), false alarms (FA), overlaps (OL), and confusion (CF):\n\nFor comparison with the SSL models, we compare participants' performance on their native language with the monolingual setting, their performance on the non-native languages with the cross-lingual or transfer learning settings. Finally, we explore whether dialect has an impact on human perception of cross-lingual SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Settings",
      "text": "For SER, to reduce the effect of varying training data sizes, we use the same amount of data for CN, DE, and EN. To ensure a balanced emotion distribution, we use an equal number of samples for each emotion. Specifically, for ESD, PAVOQUE, and IEMOCAP, we apply 5-fold cross-validation for model training: 400 utterances per emotion category, totaling 1,600 utterances per dataset, are used for training. Similarly, 200 utterances are randomly selected for validation and test sets, respectively. Given the difficulty of performing human evaluation on all the data, for comparison with human evaluation, we select 12 sentences per emotion category, totaling 144 utterances for all languages (12 sentences × 4 emotions × 3 datasets). The model settings are as follows:\n\n1) Layer-wise analysis: We use a classification head projecting from dimension 768 to 4 for SER, with a learning rate of 1e-4, epsilon of 1e-8, and weight decay of 1e-5, trained for 100 epochs with a batch size of 32. Cross-entropy is used as the loss criterion. Training stops if the validation loss does not decrease for 10 consecutive epochs.\n\n2) PEFT strategies: We use the same classification head configuration as in the layer-wise analysis for PEFT. For the LoRA module, the attention head is set to 8, alpha for scaling is 16, with a dropout rate of 0.1. For the BA module, the reduction factor is 16. Models are trained for 100 epochs with a batch size of 16. The loss and stopping criteria from the layer-wise analysis remain the same. For SED, given the considerable effort required for segmenting speech, only 8 utterances per emotion are randomly selected from ZED, totaling 24 utterances, for comparison with human evaluation and model results 5 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Results And Discussions",
      "text": "The results of the layer-wise analysis are presented in Figure  1 . In the monolingual setting, both the CN and DE models demonstrate strong performance on their respective source languages, as expected, given that the models are pre-trained on these languages. However, in the cross-lingual setting, both models show a significant drop in accuracy. While this is reasonable due to language differences, the extent of the drop is beyond our expectations, considering the shared characteristics of emotional acoustics  [28] ,  [29] . One possible explanation is that SSL models not only encode low-level acoustic features but also transform them into high-level, linguistically related information, such as word identity and meaning  [30] . This process creates a linguistic gap across languages, exacerbating the accuracy decline. Nonetheless, under the transfer learning setting, the models can achieve performance levels comparable to the monolingual setting, demonstrating the ability of SSL models to adapt to different languages for SER with appropriate techniques of knowledge transfer. The variations in the contours are related to the training objectives of SSL models, particularly the contrastive masked segment prediction (since these patterns align with previous research on layer-wise analysis of SSL models  [24] ,  [30] ,  [31] , we omit further detailed explanation).\n\nThe results of PEFT under monolingual, cross-lingual, and transfer learning settings, are shown in Table  I . Human performance on SER is shown in Table  II , and SED comparison is presented in Table  IV . From these results, we make the following observations:\n\n1) SER: monolingual model vs. native speakers   In terms of overall accuracy, as shown in Table  I  and Table  II , both models outperform their respective human native speakers. For predictions across all emotion categories, Table  III presents   2) SER: cross-lingual models vs. humans In terms of overall accuracy, as shown in Table  I  and Table  II , both humans and models experience a performance decrease in the crosslingual condition, with cross-lingual models being more significantly affected than humans. This aligns with findings from  [10] , which demonstrated that humans are capable of handling cross-lingual scenarios better. In terms of performance on every emotion category, as shown in Table  III , DE cross-lingual model struggles to recognize neutral and sad in CN data, exhibiting low recall. Additionally, the DE model confuses angry and happy more frequently compared to humans in both languages. Conversely, the CN cross-lingual model closely aligns with CN natives when recognizing DE speech, with both often predicting happy as neutral.\n\nMoreover, we conduct a two-sided Welch's t-test on humans' precision, recall, and F1-scores. We notice significant difference in the recall of happy on DE data between CN and DE speakers (t(10) = -7.511, p < 0.001), as well as in the precision of neutral (t(10) = -5.614, p < 0.001). CN speakers also exhibit lower recall for happy in DE data than in CN data (t(10) = -5.137, p < 0.001), suggesting a linguistic and paralinguistic knowledge gap between two speaker groups. Particularly, significant differences are found in the recall of sad across CN, DE, and EN data (t(10) = -2.708, p = 0.022) and in the precision of neutral (t(10) = -7.511, p < 0.001).\n\nThe precision of neutral is largely impacted by CN speakers' difficulty in perceiving happy in DE data, indicating that linguistic and paralinguistic differences affect the perception of sad across languages.\n\n3) SER: transfer learning models vs. L2 learners As the transfer learning setting resembles the human learning process of a second language (i.e., fine-tuning ≈ language study), we compare the models with human speakers using EN data. As shown in Table  I , SSL models with transfer learning achieve monolinguallevel performance and surpass human accuracy on CN and DE data. However, for EN data, DE speakers exhibit higher accuracy than CN speakers and all models tested on EN data. Additionally, two-stage fine-tuning does not result in a significant performance boost, which was observed in the cross-corpus scenario under the same language  [5] . These findings suggest that while transfer learning helps SSL models in adapting to new languages, performance varies depending on the specific target language dataset. In terms of performance on every emotion category, shown in Table  III , CN speakers only outperform the model in recognizing happy, whereas the CN transfer learning model outperforms humans in the other three emotion categories. For DE speakers, humans perform better at predicting happy and neutral compared to the DE transfer learning model. In addition, an effective PEFT strategy used in monolingual scenarios is not necessarily useful in cross-lingual or multilingual scenarios.\n\nMoreover, Table  II  reveals that recognizing emotion in EN is more challenging than in CN and DE, despite CN and DE speakers being L2 learners. This difficulty is likely attributed to the selection of only improvised utterances from IEMOCAP, which are more natural and real-life emotions, thus making SER more challenging.\n\n4) SER: linguistic and paralinguistic impact of dialect In addition to the finding in Observation 2 that linguistic and paralinguistic differences impact emotion perception across languages, the results on the TJ data in Table  II  further indicate the existence of such differences, particularly due to dialect. The SER results demonstrate the generalizability of human emotion perception across languages. However, in the TJD dataset, performance varies significantly between the two speaker groups. While DE speakers excel with CN speech data, the unique prosody of the TJ dialect leads to a notable performance decline among DE speakers. This discrepancy is plausible given that TJ prosody and tones differ significantly from CN (and likely many other major languages), making emotion recognition challenging for DE speakers. Even with some background knowledge, CN speakers also struggle to recognize emotions in TJ data as effectively as in CN data, confirming the linguistic and paralinguistic impact of dialect.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "5) Sed: Models Vs. Humans In Prominent Emotion Perception",
      "text": "The results in Table  IV  indicate that both human groups outperform the model, with the DE speakers achieving the lowest EDER. The model performs best on happy and worst on sad. Between the human groups, CN speakers are slightly better at perceiving angry segments, while DE speakers are better at identifying sad segments. This pattern is consistent with SER results in Table  III , where CN speakers show a higher threshold for predicting sad, leading to higher recall but lower precision. Conversely, DE speakers demonstrate higher precision but lower recall. The difference in sensitivity to sad among CN speakers results in more false negatives for sad in the SED task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this study, we conduct a comparative analysis of cross-lingual SER between humans and SSL models, including both modeling and human experiments, and compare their performance in monolingual, cross-lingual, and transfer learning settings. We perform a layerwise analysis and apply PEFT to the best-performing layers using multiple strategies to enhance model performance. Additionally, we implement SED for fine-grained detection of salient emotion segments to evaluate the ability of SSL models to capture segmentlevel emotion. The results show that humans excel in cross-lingual SER and SED, while models can adapt to the target language through transfer learning to achieve native speaker-level performance. We also reveal the linguistic and paralinguistic impact of dialect in the crosslingual setting through human evaluations. Our study provides novel insights into human emotion perception and the application of SSL models for cross-lingual SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Layer-wise analysis of CN and DE models under monolingual,",
      "page": 3
    },
    {
      "caption": "Figure 1: In the monolingual setting, both the CN and DE models demonstrate",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Abstract—Utilizing Self-Supervised Learning (SSL) models for Speech\nidentify\nemotionally\nsalient\nsegments\n4) Can SSL-based models"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Emotion Recognition (SER) has proven effective,\nyet\nlimited research"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "similar to human behaviors?"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "has explored cross-lingual\nscenarios. This\nstudy presents a comparative"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "To answer\nthe above questions, we conduct a comparative study"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "analysis between human performance and SSL models, beginning with a"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "between humans and SSL models, specifically:"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "layer-wise analysis and an exploration of parameter-efficient fine-tuning"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "strategies\nin monolingual, cross-lingual, and transfer learning contexts.\n• We perform a layer-wise analysis and investigate various PEFT"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "We\nfurther\ncompare\nthe SER ability\nof models\nand humans\nat both"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "strategies\nfor SSL models\nin monolingual,\ncross-lingual,\nand"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "utterance- and segment-levels. Additionally, we investigate the impact of"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "transfer\nlearning\nsettings,\ncomparing\nSER performance with"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "dialect on cross-lingual SER through human evaluation. Our findings"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "human performance across emotions.\nreveal\nthat models, with\nappropriate\nknowledge\ntransfer,\ncan\nadapt"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "to the\ntarget\nlanguage and achieve performance\ncomparable\nto native\n• We\nevaluate SER performance on Tianjin speech (a Chinese"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "speakers. We also demonstrate\nthe\nsignificant\neffect of dialect on SER"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "dialect), exploring the impact of dialect on human listeners with"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "for\nindividuals without prior\nlinguistic and paralinguistic background."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "and without linguistic and paralinguistic background knowledge."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Moreover, both humans\nand models\nexhibit distinct behaviors\nacross"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "• We\nassess\nboth\nhuman\nand SSL model\nperformance\non\nthe"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "different emotions. These results offer new insights into the cross-lingual"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Speech Emotion Diarization\n(SED)\ntask\n(i.e.,\nsegment-level\nSER capabilities of SSL models, underscoring both their similarities\nto"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "and differences from human emotion perception.\nSER),\naiming\nto\ncompare\ntheir\nability\nto\ndetect\nprominent"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Index Terms—Speech Emotion Recognition, Speech Emotion Diariza-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "emotion segments."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "tion, Cross-Lingual Evaluation, Self-Supervised Models"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "II. RELATED WORK"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "I.\nINTRODUCTION"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "On the model side, previous studies have typically fine-tuned SER"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "models using target\nlanguage data, but have observed a significant\nThe advancement of Self-Supervised Learning (SSL) has led to the"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "drop in performance when shifting from monolingual to cross-lingual\ndevelopment of powerful pre-trained models,\nsuch as Wav2vec 2.0"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "conditions\n[7],\n[12]. Additionally,\nadversarial\nneural\nnetworks\nin\n(W2V2)\n[1] and WavLM [2],\nincluding their multilingual variants."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "unsupervised settings have been explored for\ncross-lingual\nadapta-\nThese models have demonstrated remarkable success across a range"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "tion\n[13],\n[14]. More\nrecently,\n[15]\nintroduced\na\nlayer-anchoring\nof downstream speech tasks,\nincluding Speech Emotion Recognition"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "mechanism to facilitate\nemotion transfer,\naccounting for\nthe\ntask-\n(SER)\n[3]. To\nfurther\nenhance\ntheir\nadaptability\nacross\ndifferent"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "specific nature and hierarchical\nstructure of\nspeech models. On the\nlanguages\nand\ndatasets\nfor\nSER,\nParameter-Efficient\nFine-Tuning"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "human side,\n[10]\nfound that SVM models outperformed humans\nin\n(PEFT) has been utilized to improve\nthe\nefficacy of SSL models"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "monolingual\nsettings, whereas humans were less affected by cross-\nwhile minimizing fine-tuning requirements [4],\n[5]."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "lingual challenges. Further\nresearch by [16] concluded that human"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Nevertheless,\ncross-lingual SER remains\na\nsignificant\nchallenge"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "cross-lingual capabilities in SER are generally robust."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "due to language and cultural differences\n[6]. Typically, both tradi-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "Despite\nthis progress,\ncomparative\nstudies between humans\nand"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "tional and SSL models require sufficient training data in the target lan-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "models\nremain lacking,\nleading to an insufficient understanding of"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "guage to achieve satisfactory cross-lingual SER performance, which"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "human-model comparison. To our knowledge, we are the first\nto con-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "is often infeasible for\nlanguages\nlacking emotional\nspeech datasets"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "duct a comparative study between humans and SSL models, exploring"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "[7],\n[8]. For humans, however, although cross-lingual barriers exist"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "not only utterance-level SER but also fine-grained emotion perception"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "[9], emotions in speech are universally distinguishable as humans are"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "(i.e., SED),\nthe impact of dialect, and fine-tuning strategies."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "less affected by cross-lingual differences [10]."
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "While some research has explored the use of SSL models for cross-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "III. MATERIALS AND METHODOLOGY"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "lingual and multilingual SER [11],\nthere has been little investigation"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "A. Datasets and Models"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "into how these models compare to human performance. To this end,"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "As various\ntasks\nare\ninvestigated in this work, we use multiple"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "we raise four key questions:"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "datasets and models. For\nthe datasets,\nfour public emotion corpora"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "1) Can SSL-based models achieve competitive SER performance"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "and a non-public dialect corpus are used:"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "to that of humans?"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "• ESD: a Mandarin Chinese (CN) emotion corpus [17], containing\nfor SER in cross-lingual\n2) How to better fine-tune SSL models"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "utterances\nspoken by ten native CN speakers\n(five male, five\nscenarios?"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "female) across five emotion categories.\n3) Does dialect have an impact on human perception in cross-"
        },
        {
          "University of Science and Technology of China, China\nUniversity of Edinburgh, UK\nUniversity of Edinburgh, UK": "• PAVOQUE:\na German (DE)\nemotion corpus\n[18],\nfeaturing a\nlingual SER?"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "male\nand female\nspeakers were paired to record scripted and",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "source language, then on the target language once the first fine-tuning"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "improvised\nemotional\nutterances,\ndivided\ninto\nnine\nemotion",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "converges."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "categories.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "D. Comparison of SSL Models with Human Evaluation"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "•\nZED: an English emotion corpus\nspecifically designed for\nthe",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "SED task [20], with speech data annotated by humans at both",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "In this task, we use all\nthe datasets and models. For SER, we use"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "utterance and sub-utterance (segment)\nlevels.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "the\nemotions: angry, happy, neutral,\nand sad; while\nfor SED, we"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "•\nTJD: a non-public Tianjin (TJ) Chinese dialect corpus collected",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "exclude neutral as it does not contain emotion variation to perceive"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "in\nour\nprevious work\n[21].\nIt was\nrecorded\nand\nannotated",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "and segment."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "at Tianjin University\nby\ntwo\nnative Tianjin\ndialect\nspeakers.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "Six native DE speakers (one male, five female) and six native CN"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "It\nincludes\nthree functional categories\n(question, negation, ex-",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "speakers (two male,\nfour\nfemale), with no prior knowledge of each"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "pectation),\napproximated\nto\nemotions\ndue\nto\nhigh\nacoustic",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "other’s\nlanguage,\nare\nrecruited for\nthe human evaluation from the"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "similarity. According to annotators, negation resembles anger,",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "University of Edinburgh and Tianjin University. All participants have"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "and expectation resembles happiness. Tianjin dialect\nis known",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "studied English for many years with sufficient\nskills\n(e.g.,\nIELTS"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "for\nits complex tone sandhi patterns while featuring a similar",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "score ≥ 6.5). The webMUSHRA interface [27]\nis used to create the"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "but\nslightly\ndifferent\ntone\nsystem to Mandarin\n[22]. Native",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "experimental\ntests."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "speakers of\nthe Tianjin dialect convey emotions more directly",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "For SER, participants\nlisten to speech samples\nand identify the"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "with noticeable sonorous vowels and faster speech [23].",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "conveyed emotion. We use UA as\nthe evaluation metric, consistent"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "For\nthe models, we use three W2V2 base models pre-trained on",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "with the model performance evaluation. Additionally,\nto investigate"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "Mandarin CN1, DE2,\nand EN3,\nalong with a WavLM large model",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "fine-grained\nspeech\nemotion\nexpression, we\nperform SED, where"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "trained on EN emotional speech 4. The following tasks are conducted",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "participants first\nlisten to speech samples and label\nthe emotion, as in"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "using different models and datasets for specific purposes.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "the SER task. Subsequently,\nthey clip the speech and select\nthe seg-"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "ment\nthat most prominently expresses\nthe emotion. Following [20],"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "B. Layer-wise Analysis of SSL Models",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "we use the Emotion Diarization Error Rate (EDER) as\nthe metric,"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "In this\ntask, we use\nthe datasets: ESD, PAVOQUE,\nIEMOCAP;",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "which calculates the error rate of diarization results, including missed"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "-DE,\nthe models: W2V2-CN,\n-EN; and the emotions: angry, happy,",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "emotions (ME), false alarms (FA), overlaps (OL), and confusion (CF):"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "neutral, and sad.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "M E + F A + OL + CF"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "SSL models\nencode\nspeech\ninformation\nacross\ndifferent\nlayers;",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "EDER =\n(1)"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "U ttrance Duration"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "specifically,\nin SER tasks,\nspeech representations\nfrom the middle",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "layers often yield higher performance [24]. Therefore, we perform",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "For comparison with the SSL models, we compare participants’"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "a layer-wise analysis\nto identify the optimal\nlayer\nfor monolingual",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "performance on their native language with the monolingual\nsetting,"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "and cross-lingual SER. SSL models\nare used as\nfeature\nextractors",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "their performance on the non-native languages with the cross-lingual"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "with all parameters frozen, and Unweighted Accuracy (UA)\nis used",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "or\ntransfer\nlearning settings. Finally, we explore whether dialect has"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "as the evaluation metric. The analysis is conducted in the following",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "an impact on human perception of cross-lingual SER."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "settings:",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "IV. EXPERIMENTS"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "• Monolingual (Mono): The model is fine-tuned with both training",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "and test data from speech in the same language as its pre-training",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "A. Experimental Settings"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "language. For example, W2V2-CN is fine-tuned using CN data",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "For SER, to reduce the effect of varying training data sizes, we use"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "(ESD) as both training and test data.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "the same amount of data for CN, DE, and EN. To ensure a balanced"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "• Cross-lingual\n(Cross): The model\nis fine-tuned using its pre-",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "emotion distribution, we use an equal number of\nsamples\nfor each"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "training language as training data and a different language as test",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "emotion. Specifically, for ESD, PAVOQUE, and IEMOCAP, we apply"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "data. For example, W2V2-CN is fine-tuned using CN data (ESD)",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "5-fold cross-validation for model training: 400 utterances per emotion"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "and tested on DE data (PAVOQUE) or EN data (IEMOCAP).",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "category,\ntotaling 1,600 utterances per dataset, are used for\ntraining."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "•\nTransfer\nlearning (Trans): The model\nis fine-tuned and tested",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "Similarly, 200 utterances\nare\nrandomly selected for validation and"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "on\na\nlanguage\ndifferent\nfrom its\npre-training\nlanguage.\nFor",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "test\nsets,\nrespectively. Given\nthe\ndifficulty\nof\nperforming\nhuman"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "example, W2V2-CN is fine-tuned and tested on either DE data",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "evaluation on all\nthe data,\nfor\ncomparison with human evaluation,"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "(PAVOQUE) or EN data (IEMOCAP).",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "we select 12 sentences per emotion category,\ntotaling 144 utterances"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "for\nall\nlanguages\n(12 sentences × 4 emotions × 3 datasets). The"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "C. PEFT of SSL Models for Cross-Lingual SER",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "model settings are as follows:"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP;\nthe",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "1) Layer-wise analysis: We use\na\nclassification head projecting"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "models: W2V2-CN,\n-DE; and the emotions: angry, happy, neutral,",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "from dimension 768 to 4 for SER, with a learning rate of 1e-4, epsilon"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "and sad.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "of 1e-8, and weight decay of 1e-5, trained for 100 epochs with a batch"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "After the layer-wise analysis, the best-performing layers are further",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "size of 32. Cross-entropy is used as the loss criterion. Training stops"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "fine-tuned using various PEFT strategies\nto enhance performance.",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "if\nthe validation loss does not decrease for 10 consecutive epochs."
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "We apply the Low-Rank Adapter\n(LoRA)\n[25], Bottleneck Adapter",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "2) PEFT strategies: We use the same classification head configu-"
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "(BA) [26], and Weighted Gating (WG) [5]. Additionally, a two-stage",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": ""
        },
        {
          "•\nIEMOCAP: an English (EN) emotion corpus\n[19], where five": "",
          "fine-tuning\n[5]\nis\nperformed:\nthe model\nis first fine-tuned\non\nthe": "ration as in the layer-wise analysis for PEFT. For the LoRA module,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "transfer\nlearning with various PEFT strategies."
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "PEFT strategy"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "Model\nSetting\nSource\nTarget\nUA%"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "LoRA\nBA+WG\n2-stg"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "91.4"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\nMono\nCN\nCN\n87.0"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n93.9"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "62.8"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\nCross\nCN\nDE\n65.3"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n70.7"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "W2V2"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "98.5"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "-CN"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n98.8"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "Trans\nDE\nDE"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n98.8"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n✓\n98.9"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "65.5"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\nTrans\nEN\nEN\n66.3"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n67.7"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "98.9"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\nMono\nDE\nDE\n97.8"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n97.9"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "52.2"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n58.5\nCross\nDE\nCN"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n56.0"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "W2V2"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "84.2"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "-DE"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n83.6"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "Trans\nCN\nCN"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n87.5"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n✓\n85.8"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "62.4"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\nTrans\nEN\nEN\n65.0"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "✓\n✓\n66.0"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "TABLE II: Human performance (UA%) on all\nlanguages. The higher"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "the value,\nthe better\nthe SER performance."
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "CN\nDE\nEN\nTJ"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "CN participants\n79.5\n73.3\n63.5\n67.5"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "DE participants\n82.6\n91.7\n73.6\n29.2"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "In terms of overall\naccuracy,\nas\nshown in Table\nI\nand Table\nII,"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "both models\noutperform their\nrespective\nhuman\nnative\nspeakers."
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "For predictions across all emotion categories, Table III presents the"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "confusion matrices of the CN and DE monolingual models alongside"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "those of CN and DE natives for their respective languages. Compared"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "to the DE monolingual model, DE natives are more likely to report"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "false alarms for sad in neutral DE speech. CN natives, compared to"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "the CN monolingual model, demonstrate\nlower precision in happy"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "and neutral. These results indicate that SSL models exhibit excellent"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "monolingual\nperformance\non\nthe\nSER task when\nprovided with"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "sufficient\ntraining data."
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "2) SER: cross-lingual models vs. humans"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "In terms of overall accuracy, as shown in Table I and Table II, both"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "humans and models experience a performance decrease in the cross-"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "lingual condition, with cross-lingual models being more significantly"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "affected than humans. This\naligns with findings\nfrom [10], which"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "demonstrated\nthat\nhumans\nare\ncapable\nof\nhandling\ncross-lingual"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "scenarios better. In terms of performance on every emotion category,"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "as shown in Table III, DE cross-lingual model struggles to recognize"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "neutral and sad in CN data, exhibiting low recall. Additionally,\nthe"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "DE model confuses angry and happy more frequently compared to"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "humans\nin both languages. Conversely,\nthe CN cross-lingual model"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "closely aligns with CN natives when recognizing DE speech, with"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "both often predicting happy as neutral."
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "Moreover, we\nconduct\na\ntwo-sided Welch’s\nt-test\non\nhumans’"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "precision,\nrecall,\nand\nF1-scores. We\nnotice\nsignificant\ndifference"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "in the\nrecall of happy on DE data between CN and DE speakers"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": "(t(10) = −7.511, p < 0.001), as well as in the precision of neutral"
        },
        {
          "TABLE I: Model performance under monolingual, cross-lingual, and": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "learning (third row) settings. No humans under",
          "(first": "",
          "row), cross-lingual": "",
          "(second row) and transfer": ""
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "N",
          "row), cross-lingual": "S",
          "(second row) and transfer": "A"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "A",
          "(first": "0.00",
          "row), cross-lingual": "0.00",
          "(second row) and transfer": "0.98"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "H",
          "(first": "0.05",
          "row), cross-lingual": "0.00",
          "(second row) and transfer": "0.02"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "N",
          "(first": "0.83",
          "row), cross-lingual": "0.17",
          "(second row) and transfer": "0.00"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "S",
          "(first": "0.04",
          "row), cross-lingual": "0.96",
          "(second row) and transfer": "0.00"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "",
          "row), cross-lingual": "",
          "(second row) and transfer": "(d) Model: DE mono (on DE)"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "N",
          "row), cross-lingual": "S",
          "(second row) and transfer": "A"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "A",
          "(first": "0.06",
          "row), cross-lingual": "0.00",
          "(second row) and transfer": "0.52"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "H",
          "(first": "0.15",
          "row), cross-lingual": "0.01",
          "(second row) and transfer": "0.17"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "N",
          "(first": "0.76",
          "row), cross-lingual": "0.18",
          "(second row) and transfer": "0.00"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "S",
          "(first": "0.06",
          "row), cross-lingual": "0.92",
          "(second row) and transfer": "0.05"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "",
          "row), cross-lingual": "",
          "(second row) and transfer": "(h) Model: DE cross (on CN)"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "N",
          "row), cross-lingual": "S",
          "(second row) and transfer": "A"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "A",
          "(first": "0.04",
          "row), cross-lingual": "0.15",
          "(second row) and transfer": "0.70"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "H",
          "(first": "0.22",
          "row), cross-lingual": "0.03",
          "(second row) and transfer": "0.00"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "N",
          "(first": "0.60",
          "row), cross-lingual": "0.13",
          "(second row) and transfer": "0.08"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "S",
          "(first": "0.10",
          "row), cross-lingual": "0.88",
          "(second row) and transfer": "0.00"
        },
        {
          "TABLE III: Confusion matrices of CN and DE speakers and models under monolingual": "",
          "(first": "",
          "row), cross-lingual": "",
          "(second row) and transfer": "(n) Model: DE trans (on EN)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "S\nS\n0.03\n0.01\n0.15\n0.81\n0.00\n0.05\n0.02\n0.93",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "S\nS\n0.01\n0.01\n0.10\n0.88\n0.00\n0.00\n0.25\n0.75"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "(k) Human: CN (L2 on EN)\n(l) Model: CN trans (on EN)",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "(m) Human: DE (L2 on EN)\n(n) Model: DE trans (on EN)"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "suggesting a linguistic and paralinguistic knowledge gap between two",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "plausible given that TJ prosody and tones differ significantly from CN"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "speaker groups. Particularly, significant differences are found in the",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "(and likely many other major languages), making emotion recognition"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "recall of\nsad across CN, DE, and EN data (t(10) = −2.708, p =",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "challenging for DE speakers. Even with some background knowledge,"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "0.022) and in the precision of neutral (t(10) = −7.511, p < 0.001).",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "CN speakers\nalso\nstruggle\nto\nrecognize\nemotions\nin TJ\ndata\nas"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "The precision of neutral\nis\nlargely impacted by CN speakers’ dif-",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "effectively as in CN data, confirming the linguistic and paralinguistic"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "happy\nficulty\nin\nperceiving\nin DE data,\nindicating\nthat\nlinguistic",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "impact of dialect."
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "sad\nand\nparalinguistic\ndifferences\naffect\nthe\nperception\nof\nacross",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "5) SED: models vs. humans in prominent emotion perception"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "languages.",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "The results in Table IV indicate that both human groups outperform"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "3) SER:\ntransfer learning models vs. L2 learners",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "the model, with the DE speakers achieving the lowest EDER. The"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "As\nthe\ntransfer\nlearning\nsetting\nresembles\nthe\nhuman\nlearning",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "model performs best on happy and worst on sad. Between the human"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "process of a second language (i.e., fine-tuning ≈ language study), we",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "groups, CN speakers are slightly better at perceiving angry segments,"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "compare the models with human speakers using EN data. As shown",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "while DE speakers are better at identifying sad segments. This pattern"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "in Table I, SSL models with transfer\nlearning achieve monolingual-",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "is consistent with SER results in Table III, where CN speakers show a"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "level performance and surpass human accuracy on CN and DE data.",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "higher threshold for predicting sad,\nleading to higher recall but\nlower"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "However, for EN data, DE speakers exhibit higher accuracy than CN",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "precision. Conversely, DE speakers demonstrate higher precision but"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "speakers and all models\ntested on EN data. Additionally,\ntwo-stage",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "lower recall. The difference in sensitivity to sad among CN speakers"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "fine-tuning does not result\nin a significant performance boost, which",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "results in more false negatives for sad in the SED task."
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "was observed in the cross-corpus scenario under\nthe same language",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "TABLE IV: EDER (%) comparison of WavLM and humans on ZED"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "[5]. These findings\nsuggest\nthat while\ntransfer\nlearning helps SSL",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "data. The lower\nthe score,\nthe better\nthe performance."
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "models in adapting to new languages, performance varies depending",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "on\nthe\nspecific\ntarget\nlanguage\ndataset.\nIn\nterms\nof\nperformance",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "WavLM\nCN participants\nDE participants"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "on every emotion category,\nshown in Table\nIII, CN speakers only",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "25.8\nAngry\n36.6\n27.5"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "outperform the model\nin recognizing happy, whereas the CN transfer",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "27.5\nHappy\n31.8\n28.7"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "learning model\noutperforms\nhumans\nin\nthe\nother\nthree\nemotion",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "28.3\nSad\n50.3\n38.6"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "categories. For DE speakers, humans perform better\nat predicting",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "28.2\nAverage\n38.2\n32.1"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "happy and neutral compared to the DE transfer\nlearning model.\nIn",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "addition, an effective PEFT strategy used in monolingual\nscenarios",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "V. CONCLUSION"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "is not necessarily useful\nin cross-lingual or multilingual scenarios.",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": ""
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "Moreover, Table II reveals that recognizing emotion in EN is more",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "In this study, we conduct a comparative analysis of cross-lingual"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "challenging than in CN and DE, despite CN and DE speakers being",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "SER between humans and SSL models,\nincluding both modeling and"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "L2 learners. This difficulty is likely attributed to the selection of only",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "human experiments, and compare their performance in monolingual,"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "improvised utterances from IEMOCAP, which are more natural and",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "cross-lingual,\nand\ntransfer\nlearning\nsettings. We\nperform a\nlayer-"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "real-life emotions,\nthus making SER more challenging.",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "wise analysis and apply PEFT to the best-performing layers using"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "4) SER:\nlinguistic and paralinguistic impact of dialect",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "multiple\nstrategies\nto\nenhance model\nperformance. Additionally,"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "In addition to the finding in Observation 2 that\nlinguistic and par-",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "we\nimplement SED for fine-grained\ndetection\nof\nsalient\nemotion"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "alinguistic differences\nimpact emotion perception across\nlanguages,",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "segments to evaluate the ability of SSL models to capture segment-"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "the results on the TJ data in Table II\nfurther\nindicate the existence",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "level emotion. The results\nshow that humans excel\nin cross-lingual"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "of\nsuch\ndifferences,\nparticularly\ndue\nto\ndialect. The SER results",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "SER and SED, while models can adapt to the target language through"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "demonstrate the generalizability of human emotion perception across",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "transfer learning to achieve native speaker-level performance. We also"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "languages. However,\nin the TJD dataset, performance varies\nsignif-",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "reveal\nthe linguistic and paralinguistic impact of dialect\nin the cross-"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "icantly between the two speaker groups. While DE speakers excel",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "lingual setting through human evaluations. Our study provides novel"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "with CN speech data,\nthe unique prosody of the TJ dialect\nleads to a",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "insights into human emotion perception and the application of SSL"
        },
        {
          "N\nN\n0.04\n0.28\n0.58\n0.10\n0.07\n0.18\n0.75\n0.00": "notable performance decline among DE speakers. This discrepancy is",
          "N\nN\n0.15\n0.13\n0.60\n0.13\n0.08\n0.30\n0.58\n0.03": "models for cross-lingual SER."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "anchoring strategy for enhancing cross-lingual speech emotion recogni-"
        },
        {
          "REFERENCES": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "tion,” ICASSP 2024 - 2024 IEEE International Conference on Acoustics,"
        },
        {
          "REFERENCES": "Auli, “wav2vec 2.0: A framework for self-supervised learning of speech",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Speech and Signal Processing (ICASSP), 2024."
        },
        {
          "REFERENCES": "Advances\nin neural\nrepresentations,”\ninformation processing systems,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[16]\nStefan Werner and Georgii K Petrenko,\n“Speech emotion recognition:"
        },
        {
          "REFERENCES": "vol. 33, pp. 12449–12460, 2020.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "humans vs machines,” Discourse, vol. 5, no. 5, pp. 136–152, 2019."
        },
        {
          "REFERENCES": "[2]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[17] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, “Emotional voice"
        },
        {
          "REFERENCES": "Zhuo Chen,\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "conversion: Theory, databases and esd,”\nSpeech Communication, vol."
        },
        {
          "REFERENCES": "et al.,\n“Wavlm: Large-scale self-supervised pre-training for\nfull\nstack",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "137, pp. 1–18, 2022."
        },
        {
          "REFERENCES": "speech processing,” IEEE Journal of Selected Topics in Signal Process-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[18]\nIngmar Steiner, Marc Schr¨oder, and Annette Klepp,\n“The PAVOQUE"
        },
        {
          "REFERENCES": "ing, vol. 16, no. 6, pp. 1505–1518, 2022.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "corpus as a resource for analysis and synthesis of expressive speech,”"
        },
        {
          "REFERENCES": "[3] Yuanchao Li, Peter Bell, and Catherine Lai, “Fusing ASR outputs in joint",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Proc. Phonetik & Phonologie, vol. 9, 2013."
        },
        {
          "REFERENCES": "training for\nspeech emotion recognition,”\nin ICASSP 2022-2022 IEEE",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[19] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily"
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S"
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2022, pp. 7362–7366.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Narayanan,\n“IEMOCAP:\nInteractive emotional dyadic motion capture"
        },
        {
          "REFERENCES": "[4] Tiantian\nFeng\nand\nShrikanth Narayanan,\n“PEFT-SER: On\nthe\nuse",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "database,”\nLanguage resources and evaluation, vol. 42, pp. 335–359,"
        },
        {
          "REFERENCES": "of parameter efficient\ntransfer\nlearning approaches\nfor\nspeech emotion",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2008."
        },
        {
          "REFERENCES": "recognition using pre-trained speech models,” in 2023 11th International",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[20] Yingzhi Wang, Mirco Ravanelli, and Alya Yacoubi,\n“Speech emotion"
        },
        {
          "REFERENCES": "Conference on Affective Computing and Intelligent\nInteraction (ACII).",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "diarization: Which emotion appears when?,”\nin 2023 IEEE Automatic"
        },
        {
          "REFERENCES": "IEEE, 2023, pp. 1–8.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023,"
        },
        {
          "REFERENCES": "[5] Nineli Lashkarashvili, Wen Wu, Guangzhi Sun, and Philip C Woodland,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "pp. 1–7."
        },
        {
          "REFERENCES": "“Parameter efficient finetuning for speech emotion recognition and do-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[21] Tianqi Geng and Hui Feng, “Form and function in prosodic representa-"
        },
        {
          "REFERENCES": "main adaptation,” in ICASSP 2024-2024 IEEE International Conference",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "tion:\nIn the case of\n‘ma’in tianjin mandarin,”\nin Interspeech, 2024."
        },
        {
          "REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[22] Qian Li, Yiya Chen, and Ziyu Xiong, “Tianjin mandarin,” Journal of the"
        },
        {
          "REFERENCES": "10986–10990.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "International Phonetic Association, vol. 49, no. 1, pp. 109–128, 2019."
        },
        {
          "REFERENCES": "[6] Moazzam Shoukat, Muhammad Usama, Hafiz Shehbaz Ali, and Siddique",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[23]\nShuling Qi, A Study of Tianjin Dialect’s Grammar, Shanghai Jiaotong"
        },
        {
          "REFERENCES": "Latif,\n“Breaking barriers: Can multilingual\nfoundation models bridge",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "University Press, 2020."
        },
        {
          "REFERENCES": "the gap in cross-language speech emotion recognition?,”\nin 2023 Tenth",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[24] Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai, “Explo-"
        },
        {
          "REFERENCES": "International Conference on Social Networks Analysis, Management and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "ration of a self-supervised speech model: A study on emotional corpora,”"
        },
        {
          "REFERENCES": "Security (SNAMS).\nIEEE, 2023, pp. 1–9.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2022\nIEEE Spoken Language Technology Workshop\nin\n(SLT).\nIEEE,"
        },
        {
          "REFERENCES": "[7]\nSiddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2023, pp. 868–875."
        },
        {
          "REFERENCES": "“Cross lingual speech emotion recognition: Urdu vs. western languages,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[25] Edward\nJ Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean"
        },
        {
          "REFERENCES": "information technology\nin 2018 International conference on frontiers of",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Wang, Lu Wang, Weizhu Chen,\net\nal.,\n“LoRA: Low-rank adaptation"
        },
        {
          "REFERENCES": "(FIT).\nIEEE, 2018, pp. 88–93.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "of\nlarge\nlanguage models,”\nin International Conference on Learning"
        },
        {
          "REFERENCES": "[8] Youngdo Ahn, Sung Joo Lee, and Jong Won Shin, “Cross-corpus speech",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Representations, 2021."
        },
        {
          "REFERENCES": "emotion recognition based on few-shot learning and domain adaptation,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[26] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,"
        },
        {
          "REFERENCES": "IEEE Signal Processing Letters, vol. 28, pp. 1190–1194, 2021.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Quentin De\nLaroussilhe, Andrea Gesmundo, Mona Attariyan,\nand"
        },
        {
          "REFERENCES": "[9] Hillary Anger Elfenbein and Nalini Ambady,\n“On the universality and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Sylvain Gelly,\n“Parameter-efficient\ntransfer\nlearning\nfor\nnlp,”\nin"
        },
        {
          "REFERENCES": "Psycho-\ncultural\nspecificity of emotion recognition: a meta-analysis.,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "International conference on machine learning. PMLR, 2019, pp. 2790–"
        },
        {
          "REFERENCES": "logical bulletin, vol. 128, no. 2, pp. 203, 2002.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2799."
        },
        {
          "REFERENCES": "[10]\nJe Hun Jeon, Duc Le, Rui Xia, and Yang Liu,\n“A preliminary study of",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[27] Michael Schoeffler, Sarah Bartoschek, Fabian-Robert St¨oter, Marlene"
        },
        {
          "REFERENCES": "cross-lingual emotion recognition from speech: automatic classification",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Roess,\nSusanne Westphal, Bernd Edler,\nand\nJ¨urgen Herre,\n“web-"
        },
        {
          "REFERENCES": "versus human perception.,”\nin Interspeech, 2013, pp. 2837–2840.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "MUSHRA—a comprehensive framework for web-based listening tests,”"
        },
        {
          "REFERENCES": "[11] Anant Singh and Akshat Gupta, “Decoding emotions: A comprehensive",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2018."
        },
        {
          "REFERENCES": "multilingual\nstudy of\nspeech models\nfor\nspeech emotion recognition,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[28] Klaus R Scherer,\n“Vocal\ncommunication\nof\nemotion: A review of"
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2308.08713, 2023.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "research paradigms,” Speech communication, vol. 40, no. 1-2, pp. 227–"
        },
        {
          "REFERENCES": "[12] Michael Neumann et al., “Cross-lingual and multilingual speech emotion",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "256, 2003."
        },
        {
          "REFERENCES": "ICASSP 2018-2018\nIEEE\nrecognition\non English\nand\nFrench,”\nin",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[29] Rainer Banse and Klaus R Scherer, “Acoustic profiles in vocal emotion"
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "expression.,” Journal of personality and social psychology, vol. 70, no."
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2018, pp. 5769–5773.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "3, pp. 614, 1996."
        },
        {
          "REFERENCES": "[13]\nSiddique Latif,\nJunaid Qadir,\nand Muhammad Bilal,\n“Unsupervised",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[30] Ankita Pasad, Ju-Chieh Chou, and Karen Livescu, “Layer-wise analysis"
        },
        {
          "REFERENCES": "adversarial domain adaptation for cross-lingual speech emotion recogni-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2021\nIEEE\nof\na\nself-supervised\nspeech\nrepresentation model,”\nin"
        },
        {
          "REFERENCES": "tion,”\nin 2019 8th international conference on affective computing and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Automatic Speech Recognition and Understanding Workshop (ASRU)."
        },
        {
          "REFERENCES": "intelligent\ninteraction (ACII).\nIEEE, 2019, pp. 732–737.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "IEEE, 2021, pp. 914–921."
        },
        {
          "REFERENCES": "[14] Xiong Cai, Zhiyong Wu, Kuo Zhong, Bin Su, Dongyang Dai,\nand",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[31] Alexandra Saliba, Yuanchao Li, Ramon Sanabria,\nand Catherine Lai,"
        },
        {
          "REFERENCES": "Helen Meng,\n“Unsupervised cross-lingual\nspeech emotion recognition",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "“Layer-wise analysis of\nself-supervised acoustic word embeddings: A"
        },
        {
          "REFERENCES": "using domain adversarial neural network,”\nin 2021 12th International",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2024\nIEEE International\nstudy\non\nspeech\nemotion\nrecognition,”\nin"
        },
        {
          "REFERENCES": "Symposium on Chinese Spoken Language Processing (ISCSLP).\nIEEE,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Conference\non Acoustics,\nSpeech,\nand\nSignal Processing Workshops"
        },
        {
          "REFERENCES": "2021, pp. 1–5.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "(ICASSPW).\nIEEE, 2024."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "anchoring strategy for enhancing cross-lingual speech emotion recogni-"
        },
        {
          "REFERENCES": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "tion,” ICASSP 2024 - 2024 IEEE International Conference on Acoustics,"
        },
        {
          "REFERENCES": "Auli, “wav2vec 2.0: A framework for self-supervised learning of speech",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Speech and Signal Processing (ICASSP), 2024."
        },
        {
          "REFERENCES": "Advances\nin neural\nrepresentations,”\ninformation processing systems,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[16]\nStefan Werner and Georgii K Petrenko,\n“Speech emotion recognition:"
        },
        {
          "REFERENCES": "vol. 33, pp. 12449–12460, 2020.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "humans vs machines,” Discourse, vol. 5, no. 5, pp. 136–152, 2019."
        },
        {
          "REFERENCES": "[2]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[17] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li, “Emotional voice"
        },
        {
          "REFERENCES": "Zhuo Chen,\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "conversion: Theory, databases and esd,”\nSpeech Communication, vol."
        },
        {
          "REFERENCES": "et al.,\n“Wavlm: Large-scale self-supervised pre-training for\nfull\nstack",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "137, pp. 1–18, 2022."
        },
        {
          "REFERENCES": "speech processing,” IEEE Journal of Selected Topics in Signal Process-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[18]\nIngmar Steiner, Marc Schr¨oder, and Annette Klepp,\n“The PAVOQUE"
        },
        {
          "REFERENCES": "ing, vol. 16, no. 6, pp. 1505–1518, 2022.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "corpus as a resource for analysis and synthesis of expressive speech,”"
        },
        {
          "REFERENCES": "[3] Yuanchao Li, Peter Bell, and Catherine Lai, “Fusing ASR outputs in joint",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Proc. Phonetik & Phonologie, vol. 9, 2013."
        },
        {
          "REFERENCES": "training for\nspeech emotion recognition,”\nin ICASSP 2022-2022 IEEE",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[19] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily"
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S"
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2022, pp. 7362–7366.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": ""
        },
        {
          "REFERENCES": "",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Narayanan,\n“IEMOCAP:\nInteractive emotional dyadic motion capture"
        },
        {
          "REFERENCES": "[4] Tiantian\nFeng\nand\nShrikanth Narayanan,\n“PEFT-SER: On\nthe\nuse",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "database,”\nLanguage resources and evaluation, vol. 42, pp. 335–359,"
        },
        {
          "REFERENCES": "of parameter efficient\ntransfer\nlearning approaches\nfor\nspeech emotion",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2008."
        },
        {
          "REFERENCES": "recognition using pre-trained speech models,” in 2023 11th International",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[20] Yingzhi Wang, Mirco Ravanelli, and Alya Yacoubi,\n“Speech emotion"
        },
        {
          "REFERENCES": "Conference on Affective Computing and Intelligent\nInteraction (ACII).",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "diarization: Which emotion appears when?,”\nin 2023 IEEE Automatic"
        },
        {
          "REFERENCES": "IEEE, 2023, pp. 1–8.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023,"
        },
        {
          "REFERENCES": "[5] Nineli Lashkarashvili, Wen Wu, Guangzhi Sun, and Philip C Woodland,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "pp. 1–7."
        },
        {
          "REFERENCES": "“Parameter efficient finetuning for speech emotion recognition and do-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[21] Tianqi Geng and Hui Feng, “Form and function in prosodic representa-"
        },
        {
          "REFERENCES": "main adaptation,” in ICASSP 2024-2024 IEEE International Conference",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "tion:\nIn the case of\n‘ma’in tianjin mandarin,”\nin Interspeech, 2024."
        },
        {
          "REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[22] Qian Li, Yiya Chen, and Ziyu Xiong, “Tianjin mandarin,” Journal of the"
        },
        {
          "REFERENCES": "10986–10990.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "International Phonetic Association, vol. 49, no. 1, pp. 109–128, 2019."
        },
        {
          "REFERENCES": "[6] Moazzam Shoukat, Muhammad Usama, Hafiz Shehbaz Ali, and Siddique",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[23]\nShuling Qi, A Study of Tianjin Dialect’s Grammar, Shanghai Jiaotong"
        },
        {
          "REFERENCES": "Latif,\n“Breaking barriers: Can multilingual\nfoundation models bridge",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "University Press, 2020."
        },
        {
          "REFERENCES": "the gap in cross-language speech emotion recognition?,”\nin 2023 Tenth",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[24] Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai, “Explo-"
        },
        {
          "REFERENCES": "International Conference on Social Networks Analysis, Management and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "ration of a self-supervised speech model: A study on emotional corpora,”"
        },
        {
          "REFERENCES": "Security (SNAMS).\nIEEE, 2023, pp. 1–9.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2022\nIEEE Spoken Language Technology Workshop\nin\n(SLT).\nIEEE,"
        },
        {
          "REFERENCES": "[7]\nSiddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2023, pp. 868–875."
        },
        {
          "REFERENCES": "“Cross lingual speech emotion recognition: Urdu vs. western languages,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[25] Edward\nJ Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean"
        },
        {
          "REFERENCES": "information technology\nin 2018 International conference on frontiers of",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Wang, Lu Wang, Weizhu Chen,\net\nal.,\n“LoRA: Low-rank adaptation"
        },
        {
          "REFERENCES": "(FIT).\nIEEE, 2018, pp. 88–93.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "of\nlarge\nlanguage models,”\nin International Conference on Learning"
        },
        {
          "REFERENCES": "[8] Youngdo Ahn, Sung Joo Lee, and Jong Won Shin, “Cross-corpus speech",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Representations, 2021."
        },
        {
          "REFERENCES": "emotion recognition based on few-shot learning and domain adaptation,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[26] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,"
        },
        {
          "REFERENCES": "IEEE Signal Processing Letters, vol. 28, pp. 1190–1194, 2021.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Quentin De\nLaroussilhe, Andrea Gesmundo, Mona Attariyan,\nand"
        },
        {
          "REFERENCES": "[9] Hillary Anger Elfenbein and Nalini Ambady,\n“On the universality and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Sylvain Gelly,\n“Parameter-efficient\ntransfer\nlearning\nfor\nnlp,”\nin"
        },
        {
          "REFERENCES": "Psycho-\ncultural\nspecificity of emotion recognition: a meta-analysis.,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "International conference on machine learning. PMLR, 2019, pp. 2790–"
        },
        {
          "REFERENCES": "logical bulletin, vol. 128, no. 2, pp. 203, 2002.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2799."
        },
        {
          "REFERENCES": "[10]\nJe Hun Jeon, Duc Le, Rui Xia, and Yang Liu,\n“A preliminary study of",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[27] Michael Schoeffler, Sarah Bartoschek, Fabian-Robert St¨oter, Marlene"
        },
        {
          "REFERENCES": "cross-lingual emotion recognition from speech: automatic classification",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Roess,\nSusanne Westphal, Bernd Edler,\nand\nJ¨urgen Herre,\n“web-"
        },
        {
          "REFERENCES": "versus human perception.,”\nin Interspeech, 2013, pp. 2837–2840.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "MUSHRA—a comprehensive framework for web-based listening tests,”"
        },
        {
          "REFERENCES": "[11] Anant Singh and Akshat Gupta, “Decoding emotions: A comprehensive",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2018."
        },
        {
          "REFERENCES": "multilingual\nstudy of\nspeech models\nfor\nspeech emotion recognition,”",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[28] Klaus R Scherer,\n“Vocal\ncommunication\nof\nemotion: A review of"
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2308.08713, 2023.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "research paradigms,” Speech communication, vol. 40, no. 1-2, pp. 227–"
        },
        {
          "REFERENCES": "[12] Michael Neumann et al., “Cross-lingual and multilingual speech emotion",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "256, 2003."
        },
        {
          "REFERENCES": "ICASSP 2018-2018\nIEEE\nrecognition\non English\nand\nFrench,”\nin",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[29] Rainer Banse and Klaus R Scherer, “Acoustic profiles in vocal emotion"
        },
        {
          "REFERENCES": "International Conference on Acoustics, Speech and Signal Processing",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "expression.,” Journal of personality and social psychology, vol. 70, no."
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2018, pp. 5769–5773.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "3, pp. 614, 1996."
        },
        {
          "REFERENCES": "[13]\nSiddique Latif,\nJunaid Qadir,\nand Muhammad Bilal,\n“Unsupervised",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[30] Ankita Pasad, Ju-Chieh Chou, and Karen Livescu, “Layer-wise analysis"
        },
        {
          "REFERENCES": "adversarial domain adaptation for cross-lingual speech emotion recogni-",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2021\nIEEE\nof\na\nself-supervised\nspeech\nrepresentation model,”\nin"
        },
        {
          "REFERENCES": "tion,”\nin 2019 8th international conference on affective computing and",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Automatic Speech Recognition and Understanding Workshop (ASRU)."
        },
        {
          "REFERENCES": "intelligent\ninteraction (ACII).\nIEEE, 2019, pp. 732–737.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "IEEE, 2021, pp. 914–921."
        },
        {
          "REFERENCES": "[14] Xiong Cai, Zhiyong Wu, Kuo Zhong, Bin Su, Dongyang Dai,\nand",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "[31] Alexandra Saliba, Yuanchao Li, Ramon Sanabria,\nand Catherine Lai,"
        },
        {
          "REFERENCES": "Helen Meng,\n“Unsupervised cross-lingual\nspeech emotion recognition",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "“Layer-wise analysis of\nself-supervised acoustic word embeddings: A"
        },
        {
          "REFERENCES": "using domain adversarial neural network,”\nin 2021 12th International",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "2024\nIEEE International\nstudy\non\nspeech\nemotion\nrecognition,”\nin"
        },
        {
          "REFERENCES": "Symposium on Chinese Spoken Language Processing (ISCSLP).\nIEEE,",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "Conference\non Acoustics,\nSpeech,\nand\nSignal Processing Workshops"
        },
        {
          "REFERENCES": "2021, pp. 1–5.",
          "[15]\nShreya G Upadhyay, Carlos Busso,\nand Chi-Chun Lee,\n“A layer-": "(ICASSPW).\nIEEE, 2024."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "PEFT-SER: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pre-trained speech models",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "Parameter efficient finetuning for speech emotion recognition and domain adaptation",
      "authors": [
        "Nineli Lashkarashvili",
        "Wen Wu",
        "Guangzhi Sun",
        "Philip Woodland"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Breaking barriers: Can multilingual foundation models bridge the gap in cross-language speech emotion recognition?",
      "authors": [
        "Moazzam Shoukat",
        "Muhammad Usama",
        "Shehbaz Hafiz",
        "Siddique Ali",
        "Latif"
      ],
      "year": "2023",
      "venue": "2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS)"
    },
    {
      "citation_id": "7",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "2018 International conference on frontiers of information technology (FIT)"
    },
    {
      "citation_id": "8",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee",
        "Jong Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "9",
      "title": "On the universality and cultural specificity of emotion recognition: a meta-analysis",
      "authors": [
        "Hillary Anger",
        "Nalini Ambady"
      ],
      "year": "2002",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "10",
      "title": "A preliminary study of cross-lingual emotion recognition from speech: automatic classification versus human perception",
      "authors": [
        "Je Hun Jeon",
        "Duc Le",
        "Rui Xia",
        "Yang Liu"
      ],
      "year": "2013",
      "venue": "A preliminary study of cross-lingual emotion recognition from speech: automatic classification versus human perception"
    },
    {
      "citation_id": "11",
      "title": "Decoding emotions: A comprehensive multilingual study of speech models for speech emotion recognition",
      "authors": [
        "Anant Singh",
        "Akshat Gupta"
      ],
      "year": "2023",
      "venue": "Decoding emotions: A comprehensive multilingual study of speech models for speech emotion recognition",
      "arxiv": "arXiv:2308.08713"
    },
    {
      "citation_id": "12",
      "title": "Cross-lingual and multilingual speech emotion recognition on English and French",
      "authors": [
        "Michael Neumann"
      ],
      "year": "2018",
      "venue": "ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Muhammad Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "14",
      "title": "Unsupervised cross-lingual speech emotion recognition using domain adversarial neural network",
      "authors": [
        "Xiong Cai",
        "Zhiyong Wu",
        "Kuo Zhong",
        "Bin Su",
        "Dongyang Dai",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "15",
      "title": "A layeranchoring strategy for enhancing cross-lingual speech emotion recognition",
      "authors": [
        "Carlos Shreya G Upadhyay",
        "Chi-Chun Busso",
        "Lee"
      ],
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition: humans vs machines",
      "authors": [
        "Stefan Werner",
        "Georgii Petrenko"
      ],
      "year": "2019",
      "venue": "Discourse"
    },
    {
      "citation_id": "17",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "The PAVOQUE corpus as a resource for analysis and synthesis of expressive speech",
      "authors": [
        "Ingmar Steiner",
        "Marc Schröder",
        "Annette Klepp"
      ],
      "year": "2013",
      "venue": "Proc. Phonetik & Phonologie"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion diarization: Which emotion appears when?",
      "authors": [
        "Yingzhi Wang",
        "Mirco Ravanelli",
        "Alya Yacoubi"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "21",
      "title": "Form and function in prosodic representation: In the case of 'ma'in tianjin mandarin",
      "authors": [
        "Tianqi Geng",
        "Hui Feng"
      ],
      "year": "2024",
      "venue": "Form and function in prosodic representation: In the case of 'ma'in tianjin mandarin"
    },
    {
      "citation_id": "22",
      "title": "Tianjin mandarin",
      "authors": [
        "Qian Li",
        "Yiya Chen",
        "Ziyu Xiong"
      ],
      "year": "2019",
      "venue": "Journal of the International Phonetic Association"
    },
    {
      "citation_id": "23",
      "title": "A Study of Tianjin Dialect's Grammar",
      "authors": [
        "Shuling Qi"
      ],
      "year": "2020",
      "venue": "A Study of Tianjin Dialect's Grammar"
    },
    {
      "citation_id": "24",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "25",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "27",
      "title": "web-MUSHRA-a comprehensive framework for web-based listening tests",
      "authors": [
        "Michael Schoeffler",
        "Sarah Bartoschek",
        "Fabian-Robert Stöter",
        "Marlene Roess",
        "Susanne Westphal",
        "Bernd Edler",
        "Jürgen Herre"
      ],
      "year": "2018",
      "venue": "web-MUSHRA-a comprehensive framework for web-based listening tests"
    },
    {
      "citation_id": "28",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "29",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "Rainer Banse",
        "Klaus Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "30",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "Ankita Pasad",
        "Ju-Chieh Chou",
        "Karen Livescu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "31",
      "title": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "authors": [
        "Alexandra Saliba",
        "Yuanchao Li",
        "Ramon Sanabria",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    }
  ]
}