{
  "paper_id": "2003.10724v2",
  "title": "Evaluation Of Error And Correlation-Based Loss Functions For Multitask Learning Dimensional Speech Emotion Recognition",
  "published": "2020-03-24T09:09:49Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The choice of a loss function is a critical part in machine learning. This paper evaluates two different loss functions commonly used in regression-task dimensional speech emotion recognition -error-based and correlation-based loss functions. We found that using correlation-based loss function with concordance correlation coefficient (CCC) loss resulted in better performance than error-based loss functions with mean squared error (MSE) and mean absolute error (MAE). The evaluations were measured in averaged CCC among three emotional attributes. The results are consistent with two input feature sets and two datasets. The scatter plots of test prediction by those two loss functions also confirmed the results measured by CCC scores.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Dimensional emotion recognition is scientifically more challenging than categorical emotion recognition. In dimensional emotion recognition, the goal is to predict the continuous degree of emotional attributes, while in categorical emotion recognition, the task is to predict emotion category of speakers, whether they are angry, happy, sad, fearful, disgusting, or surprised. While the practical application of this dimensional emotion recognition is not clear yet, Russel  [1]  argued that categorical emotion can be derived from two-space dimensional emotion, i.e., valence (positive or negative) and arousal (high or low).\n\nIn the dimensional emotion model, several models have been introduced by psychological researchers, including 2D, 3D, and 4D models. In the 3D model, either dominance (power control) or liking is used as the third attribute. In the 4D model, either expectancy or unpredictability is used, such as in  [2]  and  [3] . This research used 3D emotions with valence, arousal, and dominance (VAD) model, as suggested in  [4] .\n\nIn the 3D emotion model, the emotion recognizer system (classifier) needs to predict three emotional attributes. This task is often performed by simultaneous or jointly learning prediction of VAD. This simultaneous learning technique is known as multitask learning (MTL). Compared to single-task learning (STL), MTL attempts to optimize three parameters at the same time, while STL only attempts to optimize one parameter (either valence, arousal, or dominance). To train the model over those three attributes, the choice of loss function for the MTL is vital for the performance of the system. The traditional regression task used mean squared error (MSE) as both loss function and evaluation metric. Dimensional emotion recognition, as a regression task, conventionally follow that rule by applying MSE for both loss and evaluation metric. Additionally, mean absolute error (MAE) is another metric to train and evaluate regression task. MAE loss function is less used since it only can be compared on the same-scale data.\n\nRecently, affective computing researchers argued that using correlation-based metric to evaluate the performance of dimensional emotion recognition is more appropriate than calculating its errors  [5, 6, 7] . The concordance correlation coefficient (CCC)  [8]  is often used to measure the performance of dimensional emotion recognition since it takes the bias into Pearson's correlation coefficient (CC). Hence, we hypothesized that using CCC loss (1 -CCC) is more relevant than using MSE and MAE as loss functions for MTL dimensional speech emotion recognition. This paper aims to evaluate this hypothesis.\n\nTo the best of our knowledge, there is no experimental research reporting direct comparison of the impact of using error-vs. correlation-based loss function for dimensional speech emotion recognition. The mathematical foundation of this issue has been thoroughly explained in  [9] . Some authors used MSE loss, such as in  [10, 11]  while the others used CCC loss, such as in  [7, 12] . Both groups reported the performance of the evaluated method using CCC. We choose speech emotion recognition as our task since the target application is speech-based applications like voice assistant and call center service.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Statement",
      "text": "We focused our work to evaluate which loss function performs better on multitask learning dimensional emotion recognition: MSE, MAE, or CCC loss. To achieve this goal, we used two different datasets and two different acoustic feature sets. We expected consistent results across four scenarios or parts (2 datasets × 2 feature sets). The same experiment condition (i.e., the same architecture with the same parameters) is used to evaluate both error-based and correlation-based loss functions in four scenarios. The evaluation is measured by CCC scores. The averaged CCC score among three emotion dimensions is used to evaluate the overall performance of evaluated loss function on each scenario.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data And Feature Sets",
      "text": "Two datasets and two acoustic feature sets were used to evaluate three different loss functions (CCC loss, MSE, MAE). Datasets: IEMOCAP and MSP-IMPROV datasets are utilized to evaluate error and correlation-based loss functions. Among many modalilites provided by both datasets, only speech data is used to extract acoustic feature sets. The first dataset consists of 10039 utterances while the second consists of 8438 utterances. For both datasets, only dimensional labels are used, i.e., valence, arousal, and dominance, in the range  [1, 5] . We scaled those labels into the range [-1, 1] following the work in  [10]  when fed it into deep learning-based dimensional speech emotion recognition system. This new scale is more readable because valence, arousal, and dominance are from negative to positive scale by their definitions. The detail of IEMOCAP dataset is given in  [13] , while for MSP-IMPROV dataset is available in  [14] . All scenarios in the two datasets are performed in speaker-independent configuration for test data, i.e., the last one session is left out for test partition (LOSO, leave one session out). For IEMOCAP data, the number of utterances in training partition is 7869 utterances, and the rest 2170 utterances (session fife) are used for the test partition. On the MSP-IMPROV dataset, 6816 utterances are used for the training partition, and the rest 1622 utterances (session six) are used for the test partition. On both training partitions, 20% of data is used for validation (development). Acoustic Features: High-level statistical function (HSF) of two feature sets are used. The first is HSF from Geneva minimalistic acoustic and parameter set (GeMAPS), as described in  [15] . The second is HSF from pyAudioAnalysis (pAA)  [16] . Note that the definition of HSF referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD) listed in both feature sets. GeMAPS feature set consists of 23 LLDs, while pAA contains 34 LLDs. A list of LLDs in those two feature sets is presented in table  1 . The use of Mean+Std in this research follows the finding in  [17] . Additionally, we implement the Mean+Std of LLDs from pAA to observe its difference from GeMAPS.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Error-Based Loss Function",
      "text": "A mean squared error to measure the deviation between predicted emotion degree x and goldstandard label y is given by\n\nwhere n is the number of measurement (calculated per batch size). For three emotion dimensions, the total MSE is the sum of MSE from valence, arousal, and dominance:\n\nFollowing the work of  [10] , we added weighting factors for valence and arousal. Hence, the total MSE became,\n\nwhere α and β are weighting factors for valence and arousal. The weighting factor for dominance is obtained by subtracting 1 with those two variables.\n\nSimilarly, mean absolute errors as an objective function can be formulated as follows,\n\nAlthough the use of MAE as a loss function is less common in machine learning due to a scaledependent accuracy measure, we evaluated it since both dataset labels are in the same scale (  [1, 5]  standardized to [-1, 1]). This additional loss function might support the comparison of error-based to correlation-based loss functions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Correlation-Based Loss Function",
      "text": "CCC is a common metric in dimensional emotion recognition to measure the agreement between the true emotion dimension with predicted emotion degree. If the predictions shifted in value, the score is penalized in proportion to deviation  [5] . Hence, CCC is more reliable than Pearson correlation, MAE and MSE to evaluate the performance of dimensional speech emotion recognition. CCC is formulated as\n\nwhere ρ xy is the Pearson coefficient correlation between x and y, σ is the standard deviation, and µ is a mean value. This CCC is based on Lin's calculation  [8] . The range of CCC is from -1 (perfect disagreement) to 1 (perfect agreement). Therefore, the CCC loss function (CCCL) to maximize the agreement between true value and predicted emotion can be defined as\n\nSimilar to multitask learning in MSE, we accommodate the loss functions from arousal (CCCL V ), valence (CCCL A ), and dominance (CCCL D ). The CCCL T is a combination of these three CCC loss functions:\n\nwhere α are β are the weighting factors for each emotion dimension loss function. The same weighing factors are used for MSE, MAE, and CCC losses, i.e., α = 0.1 and β = 0.5 for IEMOCAP dataset, and α = 0.3 and β = 0.6 for MSP-IMPROV dataset. Those weighting factors are obtained via linear search in range [0.0, 1.0] with 0.1 increment dependently each other.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architecture Of Dimensional Speech Emotion Recognition",
      "text": "We used deep learning-based architecture to evaluate all loss functions, i.e., three layers of stacked LSTM networks  [18] . For the input, either 46 HSFs from GeMAPS or 68 HSFs from pAA are fed into the network. A batch normalization layer is performed to speed up the computation process  [19] . Three LSTM layers are stacked; the first two layers return all sequences while the last LSTM layer returns final outputs only. A dense network with 64 nodes is coupled after the last LSTM layer. Three dense layers with one unit each ended the network to predict the degree of valence, arousal, and dominance with tanh activation function to bound the output in range [-1, 1]. Either MSE, MAE, or CCC loss is used as the loss function with RMSprop optimizer  [20] . The architecture of this dimensional speech emotion recognition is shown in Fig.  1 .\n\nAs an additional analysis tool, we used scatter plots of predicted valence and arousal degrees compared to the gold-standard labels. These plots will show how similar or different between labels and predicted degrees. This similarity between labels and predictions can be used to confirm obtained CCC scores by different loss functions.\n\nThe implementation of the evaluation methods is available in the following repository, https://github.com/bagustris/ccc_mse_ser. The LSTM-based dimensional speech emotion recognition is implemented using Keras toolkit  [21]  with TensorFlow backend  [22] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Of Evaluated Loss Functions",
      "text": "The main issue of this research is to find which loss function works better for multitask learning dimensional emotion recognition. The following results report evaluation of three loss functions in two datasets and two features sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lstm (256)",
      "text": "Dense(  1 ) Dense(  1  Table  2  shows the result of using different loss function in the same dataset using the same network architecture. We can divide the results into four parts: two datasets with two different feature sets for each dataset. The first part is the IEMOCAP dataset with HSF of GeMAPS as the input feature. Using CCC loss, the obtained CCC score for each dimension is higher than the obtained scores using MSE and MAE losses. The resulted average CCC scores were 0.304 for MAE, 0.310 for MSE, and 0.400 for CCCL. Clearly, it is shown that CCCL obtained better performance than MSE and MAE in this part and continued to other three parts.\n\nWe evaluated HSFs from pAA on the second part of the table. Although the feature set is not designed specifically for an affective application, however, it showed a similar performance to the result obtained by affective-designed GeMAPS feature set. In this IEMOCAP dataset, HSF of pAA even performed marginally better than HSFs of GeMAPS for both CCC and MSE losses. The comparison of performance between MSE, MAE, and CCCL is similar to those were obtained by GeMAPS, with the average CCC score of 0.333, 0.344, and 0.410 for MSE, MAE, and CCC loss, respectively.\n\nMoving to the MSP-IMPROV dataset, a similar trend was observed. On the third part with MSP-IMPROV and GeMAPS feature set, CCCL obtained the averaged CCC score of 0.363 compared to MAE with 0.323 and MSE with 0.327. Finally, on the fourth part with MSP-IMPROV and pAA feature set, the CCCL obtained 0.34 of the average CCC score, while MSE and MAE obtained 0.305 and 0.324.\n\nThe overall results above suggest that, in terms of CCC, CCC loss is better than MSE and MAE for multitask learning dimensional emotion recognition. Four scenarios with CCC loss function obtained higher scores than other four scenarios with MSE and MAE, on both individual emotion dimensions scores (CCC of valence, arousal, and dominance) and on the averaged score. We extend the discussion to the results obtained by MSE scores and different feature sets.\n\nWe found that the averaged MSE scores across data and feature sets are almost identical (last column in Table  2 ). If so, the MSE metrics might be more stable to generalize the model generated by dimensional speech emotion recognition system across different datasets. However, this consistent error for the generalization of dimensional speech emotion recognition system needs to be investigated with other datasets in different scale of labels. Another possible cause for the consistent error is the small range of the output, i.e., 0-1 scale after squared by the MSE. On the use of different feature sets, we observed no significant differences between results obtained by HSF of affective-designed GeMAPS and general-purpose pAA feature sets. The results obtained by those two feature sets are quite similar for the same loss function. Not only on CCC scores, but the similarity of performance is also observed on MSE scores. This results can be viewed as the generalization from the previous research  [7]  that not only Mean+Std of GeMAPS useful for dimensional emotion recognition but also Mean+Std of other feature sets, in this case, pAA feature set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Scatter Plot Of Predicted Emotion Degrees",
      "text": "We showed the scatter plots of prediction by and MSE, MAE, and CCC loss in Fig.  2 . In these cases, the plots showed the prediction from GeMAPS test partition. From both plots, we can infer that the prediction from CCC loss is more similar to gold-standard labels than the prediction from MSE and MAE. This result confirms the obtained CCC scores from valence, arousal, and dominance and its average. Although we only showed the result from IEMOCAP with GeMAPS feature, the plots are consistent with other parts. Note in these scatter plots that a single dot may represent more than one label (overlapped), since there is a possibility to have the same labels (score of valence and arousal) for several utterances.\n\nComparing the shape of three predictions shows that CCC loss predicts better than MSE and MAE. The calculation of CCC takes account of the shifted values of prediction into concordance correlation calculation while MSE and MAE only count their errors. If the concordance line represents the true valence-arousal label in 2D space (like in 2), CCC loss minimizes the variation of trained labels to this line. It can be concluded from both metric and visualization that CCC loss performs better than MSE and MAE.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "This paper reported an evaluation of different loss functions for multitask learning dimensional emotion recognition. The result shows that the use of CCC loss obtained better performance than the MAE and MSE in CCC scores measure across four scenarios. We are confident that this result is universal since we use two different datasets and two different feature sets that resulting consistent results. The process is also straightforward, CCC loss as the loss function with CCC scores as evaluation metrics. These results are also supported by scatter plots of the valence-arousal prediction from both losses compared to the gold-standard labels.\n\nFurther study to investigate the relationship between error and (concordance) correlation from both theoretical and practical approaches may improve our understanding on it. Although it is suggested to use CCC as the main metric for dimensional emotion recognition, additional metrics such as MSE and RMSE might be useful to accompany CCC measure for tracking the pattern of the performance across different datasets, feature sets, and methods, particularly in dimensional emotion recognition.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: As an additional analysis tool, we used scatter plots of predicted valence and arousal degrees",
      "page": 4
    },
    {
      "caption": "Figure 1: Architecture of dimensional speeh emotion recognition system to evaluate loss",
      "page": 5
    },
    {
      "caption": "Figure 2: Scatter plot of valence and arousal dimensions (in CCC [-1, 1]) on test partition from",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Functions For Multitask Learning Dimensional": "Speech Emotion Recognition"
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": "E-mail: bagus@ep.its.ac.id,"
        },
        {
          "Functions For Multitask Learning Dimensional": "Abstract.\nThe choice of a loss"
        },
        {
          "Functions For Multitask Learning Dimensional": "evaluates\ntwo diﬀerent\nloss"
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": ""
        },
        {
          "Functions For Multitask Learning Dimensional": "scores."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Additionally, mean absolute\nerror\n(MAE)\nis another metric\nto train and evaluate\nregression"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "task. MAE loss function is less used since it only can be compared on the same-scale data."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Recently,\naﬀective\ncomputing\nresearchers\nargued\nthat\nusing\ncorrelation-based metric\nto"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "evaluate\nthe\nperformance\nof\ndimensional\nemotion\nrecognition\nis more\nappropriate\nthan"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "calculating its errors [5, 6, 7]. The concordance correlation coeﬃcient (CCC) [8]\nis often used"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "to measure\nthe performance of dimensional\nemotion recognition since\nit\ntakes\nthe bias\ninto"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Pearson’s correlation coeﬃcient (CC). Hence, we hypothesized that using CCC loss (1 − CCC)"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "is more relevant than using MSE and MAE as loss functions for MTL dimensional speech emotion"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "recognition. This paper aims to evaluate this hypothesis."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "To the best of our knowledge, there is no experimental research reporting direct comparison"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "of the impact of using error- vs. correlation-based loss function for dimensional speech emotion"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "recognition. The mathematical\nfoundation of\nthis\nissue has been thoroughly explained in [9]."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Some authors used MSE loss,\nsuch as\nin [10, 11] while the others used CCC loss,\nsuch as\nin"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "[7, 12]. Both groups reported the performance of the evaluated method using CCC. We choose"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "speech emotion recognition as our task since the target application is speech-based applications"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "like voice assistant and call center service."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "2. Problem Statement"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "We\nfocused our work to evaluate which loss\nfunction performs better on multitask learning"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "dimensional\nemotion recognition: MSE, MAE, or CCC loss.\nTo achieve\nthis goal, we used"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "two diﬀerent datasets and two diﬀerent acoustic feature sets. We expected consistent\nresults"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "across\nfour\nscenarios or parts\n(2 datasets × 2 feature sets).\nThe same experiment condition"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "(i.e.,\nthe\nsame architecture with the\nsame parameters)\nis used to evaluate both error-based"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "and correlation-based loss\nfunctions\nin four\nscenarios.\nThe\nevaluation is measured by CCC"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "scores. The averaged CCC score among three emotion dimensions is used to evaluate the overall"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "performance of evaluated loss function on each scenario."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "3. Evaluation Methods"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "3.1. Data and Feature Sets"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Two datasets and two acoustic feature sets were used to evaluate three diﬀerent loss functions"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "(CCC loss, MSE, MAE)."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Datasets:\nIEMOCAP\nand MSP-IMPROV datasets\nare\nutilized\nto\nevaluate\nerror\nand"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "correlation-based loss\nfunctions.\nAmong many modalilites provided by both datasets,\nonly"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "speech data is used to extract acoustic feature sets. The ﬁrst dataset consists of 10039 utterances"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "while the second consists of 8438 utterances. For both datasets, only dimensional labels are used,"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "i.e., valence, arousal, and dominance,\nin the range [1, 5]. We scaled those labels into the range"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "[-1, 1] following the work in [10] when fed it into deep learning-based dimensional speech emotion"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "recognition system. This new scale is more readable because valence, arousal, and dominance"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "are from negative to positive scale by their deﬁnitions. The detail of IEMOCAP dataset is given"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "in [13], while for MSP-IMPROV dataset is available in [14]. All scenarios in the two datasets"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "are performed in speaker-independent conﬁguration for\ntest data,\ni.e.,\nthe last one session is"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "left out for test partition (LOSO,\nleave one session out). For IEMOCAP data,\nthe number of"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "utterances\nin training partition is 7869 utterances, and the rest 2170 utterances\n(session ﬁfe)"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "are used for the test partition. On the MSP-IMPROV dataset, 6816 utterances are used for the"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "training partition, and the rest 1622 utterances (session six) are used for the test partition. On"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "both training partitions, 20% of data is used for validation (development)."
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "Acoustic Features: High-level\nstatistical\nfunction (HSF) of\ntwo feature sets are used. The"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "ﬁrst is HSF from Geneva minimalistic acoustic and parameter set (GeMAPS), as described in"
        },
        {
          "task,\nconventionally follow that\nrule by applying MSE for both loss and evaluation metric.": "[15]. The second is HSF from pyAudioAnalysis\n(pAA)\n[16]. Note that\nthe deﬁnition of HSF"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The use of Mean+Std",
      "data": [
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "listed in both feature sets. GeMAPS feature set consists of 23 LLDs, while pAA contains 34"
        },
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "LLDs. A list of LLDs in those two feature sets is presented in table 1. The use of Mean+Std"
        },
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "in this research follows the ﬁnding in [17]. Additionally, we implement the Mean+Std of LLDs"
        },
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "from pAA to observe its diﬀerence from GeMAPS."
        },
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs"
        },
        {
          "referred here is only mean and standard deviation (Mean+Std) from low-level descriptors (LLD)": "are used as input features)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The use of Mean+Std",
      "data": [
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": ""
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "LLDs"
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "intensity,\nalpha\nratio,\nHammarberg"
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "500 Hz,\nspectral\nslope\n500-1500 Hz,"
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "jitter,\nfo,"
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": ""
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": ""
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "zero crossing rate,\nenergy,\nentropy of"
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": ""
        },
        {
          "Table 1. Acoustic features used to evaluate the loss functions (only Mean+Std of those LLDs": "MFCCs, 12 chroma vectors, chroma deviation."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3. Correlation-based Loss Function": "CCC is a common metric in dimensional emotion recognition to measure the agreement between"
        },
        {
          "3.3. Correlation-based Loss Function": "the true emotion dimension with predicted emotion degree.\nIf the predictions shifted in value,"
        },
        {
          "3.3. Correlation-based Loss Function": "the\nscore\nis\npenalized\nin\nproportion\nto\ndeviation\n[5].\nHence, CCC is more\nreliable\nthan"
        },
        {
          "3.3. Correlation-based Loss Function": "Pearson correlation, MAE and MSE to evaluate the performance of dimensional speech emotion"
        },
        {
          "3.3. Correlation-based Loss Function": "recognition. CCC is formulated as"
        },
        {
          "3.3. Correlation-based Loss Function": "2ρxyσxσy"
        },
        {
          "3.3. Correlation-based Loss Function": "CCC =\n(7)"
        },
        {
          "3.3. Correlation-based Loss Function": "σ2\nx + σ2\ny + (µx − µy)2 ,"
        },
        {
          "3.3. Correlation-based Loss Function": "is the Pearson coeﬃcient correlation between x and y, σ is the standard deviation,\nwhere ρxy"
        },
        {
          "3.3. Correlation-based Loss Function": "and µ is a mean value. This CCC is based on Lin’s calculation [8]. The range of CCC is from"
        },
        {
          "3.3. Correlation-based Loss Function": "−1 (perfect disagreement) to 1 (perfect agreement). Therefore, the CCC loss function (CCCL)"
        },
        {
          "3.3. Correlation-based Loss Function": "to maximize the agreement between true value and predicted emotion can be deﬁned as"
        },
        {
          "3.3. Correlation-based Loss Function": "CCCL = 1 − CCC.\n(8)"
        },
        {
          "3.3. Correlation-based Loss Function": "Similar\nto multitask\nlearning\nin MSE, we\naccommodate\nthe\nloss\nfunctions\nfrom arousal"
        },
        {
          "3.3. Correlation-based Loss Function": "is a combination of\n(CCCLV ), valence (CCCLA), and dominance (CCCLD).\nThe CCCLT"
        },
        {
          "3.3. Correlation-based Loss Function": "these three CCC loss functions:"
        },
        {
          "3.3. Correlation-based Loss Function": "(9)\nCCCLT = α CCCLV + β CCCLA + (1 − α − β) CCCLD,"
        },
        {
          "3.3. Correlation-based Loss Function": "where α are β are the weighting factors\nfor each emotion dimension loss\nfunction. The same"
        },
        {
          "3.3. Correlation-based Loss Function": "weighing factors are used for MSE, MAE, and CCC losses,\ni.e., α = 0.1 and β = 0.5 for"
        },
        {
          "3.3. Correlation-based Loss Function": "IEMOCAP dataset, and α = 0.3 and β = 0.6 for MSP-IMPROV dataset.\nThose weighting"
        },
        {
          "3.3. Correlation-based Loss Function": "factors are obtained via linear\nsearch in range [0.0, 1.0] with 0.1 increment dependently each"
        },
        {
          "3.3. Correlation-based Loss Function": "other."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows the result of using different loss function in the same dataset using the same",
      "data": [
        {
          "Loss calculation (MSE/MAE/CCCL)": "Figure\n1.\nArchitecture\nof dimensional\nspeeh emotion recognition system to\nevaluate\nloss"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "functions."
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "Table 2 shows the result of using diﬀerent loss function in the same dataset using the same"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "network architecture. We can divide the results into four parts:\ntwo datasets with two diﬀerent"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "feature sets for each dataset. The ﬁrst part is the IEMOCAP dataset with HSF of GeMAPS as"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "the input feature. Using CCC loss, the obtained CCC score for each dimension is higher than"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "the obtained scores using MSE and MAE losses. The resulted average CCC scores were 0.304"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "for MAE, 0.310 for MSE, and 0.400 for CCCL. Clearly,\nit is shown that CCCL obtained better"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "performance than MSE and MAE in this part and continued to other three parts."
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "We evaluated HSFs from pAA on the second part of the table. Although the feature set is"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "not designed speciﬁcally for an aﬀective application, however,\nit showed a similar performance"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "to the result obtained by aﬀective-designed GeMAPS feature set.\nIn this IEMOCAP dataset,"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "HSF of pAA even performed marginally better than HSFs of GeMAPS for both CCC and MSE"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "losses. The comparison of performance between MSE, MAE, and CCCL is similar to those were"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "obtained by GeMAPS, with the average CCC score of 0.333, 0.344, and 0.410 for MSE, MAE,"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "and CCC loss, respectively."
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "Moving to the MSP-IMPROV dataset, a similar trend was observed. On the third part with"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "MSP-IMPROV and GeMAPS feature\nset, CCCL obtained the averaged CCC score of 0.363"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "compared to MAE with 0.323 and MSE with 0.327.\nFinally, on the fourth part with MSP-"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "IMPROV and pAA feature set, the CCCL obtained 0.34 of the average CCC score, while MSE"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "and MAE obtained 0.305 and 0.324."
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "The overall\nresults above\nsuggest\nthat,\nin terms of CCC, CCC loss\nis better\nthan MSE"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "and MAE for multitask learning dimensional emotion recognition.\nFour\nscenarios with CCC"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "loss\nfunction obtained higher\nscores\nthan other\nfour\nscenarios with MSE and MAE, on both"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "individual\nemotion dimensions\nscores\n(CCC of valence, arousal, and dominance) and on the"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "averaged score. We extend the discussion to the results obtained by MSE scores and diﬀerent"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "feature sets."
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "We found that\nthe averaged MSE scores across data and feature sets are almost\nidentical"
        },
        {
          "Loss calculation (MSE/MAE/CCCL)": "(last column in Table 2).\nIf so, the MSE metrics might be more stable to generalize the model"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Evaluation results of evaluated loss functions on IEMOCAP and MSP-IMPROV",
      "data": [
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "this consistent error\nfor\nthe generalization of dimensional\nspeech emotion recognition system"
        },
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "needs to be investigated with other datasets in diﬀerent scale of\nlabels. Another possible cause"
        },
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "for the consistent error is the small range of the output,\ni.e., 0-1 scale after squared by the MSE."
        },
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "Table 2.\nEvaluation results of\nevaluated loss\nfunctions on IEMOCAP and MSP-IMPROV"
        },
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "datasets."
        },
        {
          "generated by dimensional speech emotion recognition system across diﬀerent datasets. However,": "Feature\nLoss\nCCC"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Evaluation results of evaluated loss functions on IEMOCAP and MSP-IMPROV",
      "data": [
        {
          "Evaluation results of": "Feature",
          "evaluated loss": "Loss",
          "functions on IEMOCAP and MSP-IMPROV": ""
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "",
          "functions on IEMOCAP and MSP-IMPROV": "A"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "",
          "functions on IEMOCAP and MSP-IMPROV": ""
        },
        {
          "Evaluation results of": "GeMAPS",
          "evaluated loss": "MSE",
          "functions on IEMOCAP and MSP-IMPROV": "0.451"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "MAE",
          "functions on IEMOCAP and MSP-IMPROV": "0.431"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "CCCL",
          "functions on IEMOCAP and MSP-IMPROV": "0.553"
        },
        {
          "Evaluation results of": "pAA",
          "evaluated loss": "MSE",
          "functions on IEMOCAP and MSP-IMPROV": "0.522"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "MAE",
          "functions on IEMOCAP and MSP-IMPROV": "0.539"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "CCCL",
          "functions on IEMOCAP and MSP-IMPROV": "0.577"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "",
          "functions on IEMOCAP and MSP-IMPROV": ""
        },
        {
          "Evaluation results of": "GeMAPS",
          "evaluated loss": "MSE",
          "functions on IEMOCAP and MSP-IMPROV": "0.492"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "MAE",
          "functions on IEMOCAP and MSP-IMPROV": "0.466"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "CCCL",
          "functions on IEMOCAP and MSP-IMPROV": "0.525"
        },
        {
          "Evaluation results of": "pAA",
          "evaluated loss": "MSE",
          "functions on IEMOCAP and MSP-IMPROV": "0.475"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "MAE",
          "functions on IEMOCAP and MSP-IMPROV": "0.479"
        },
        {
          "Evaluation results of": "",
          "evaluated loss": "CCCL",
          "functions on IEMOCAP and MSP-IMPROV": "0.496"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.00": "0.75"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.00"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.75"
        },
        {
          "1.00": "1.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.00": "0.75"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.00"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.75"
        },
        {
          "1.00": "1.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.00": "0.75"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.00"
        },
        {
          "1.00": "0.25"
        },
        {
          "1.00": "0.50"
        },
        {
          "1.00": "0.75"
        },
        {
          "1.00": "1.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. Conclusions": "This paper reported an evaluation of diﬀerent loss functions for multitask learning dimensional"
        },
        {
          "5. Conclusions": "emotion recognition. The result shows that\nthe use of CCC loss obtained better performance"
        },
        {
          "5. Conclusions": "than the MAE and MSE in CCC scores measure across four scenarios. We are conﬁdent that"
        },
        {
          "5. Conclusions": "this\nresult\nis universal\nsince we use two diﬀerent datasets and two diﬀerent\nfeature sets\nthat"
        },
        {
          "5. Conclusions": "resulting consistent results. The process is also straightforward, CCC loss as the loss function"
        },
        {
          "5. Conclusions": "with CCC scores as evaluation metrics. These results are also supported by scatter plots of the"
        },
        {
          "5. Conclusions": "valence-arousal prediction from both losses compared to the gold-standard labels."
        },
        {
          "5. Conclusions": "Further\nstudy to investigate\nthe\nrelationship between error and (concordance)\ncorrelation"
        },
        {
          "5. Conclusions": "from both theoretical and practical approaches may improve our understanding on it. Although"
        },
        {
          "5. Conclusions": "it is suggested to use CCC as the main metric for dimensional emotion recognition, additional"
        },
        {
          "5. Conclusions": "metrics such as MSE and RMSE might be useful to accompany CCC measure for tracking the"
        },
        {
          "5. Conclusions": "pattern of the performance across diﬀerent datasets,\nfeature sets, and methods, particularly in"
        },
        {
          "5. Conclusions": "dimensional emotion recognition."
        },
        {
          "5. Conclusions": "References"
        },
        {
          "5. Conclusions": "[1] Russell J A 1979 J. Pers. Soc. Psychol."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dimensional emotion recognition.": "References"
        },
        {
          "dimensional emotion recognition.": "[1] Russell J A 1979 J. Pers. Soc. Psychol."
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[6] Han"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[8] Lin L I K 1989 Biometrics 45 255–68 ISSN 0006-341X"
        },
        {
          "dimensional emotion recognition.": "[9] Pandit V and Schuller B 2019 arXiv 1–32 (Preprint arXiv:1902.05180v6)"
        },
        {
          "dimensional emotion recognition.": "[10] Parthasarathy S and Busso C 2017 Jointly Predicting Arousal, Valence and Dominance with Multi-Task"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[11] AbdelWahab M and Busso C 2018 IEEE/ACM Trans. Audio Speech Lang. Process. 26 2423–2435 ISSN"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[12] Atmaja B T and Akagi M 2020 Multitask Learning and Multistage Fusion for Dimensional Audiovisual"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[13] Busso C, Bulut M, Lee C C C, Kazemzadeh A, Mower E, Kim S, Chang J N, Lee S and Narayanan S S 2008"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[14] Busso C, Parthasarathy S, Burmania A, AbdelWahab M, Sadoughi N and Provost E M 2017 IEEE Trans."
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[15] Eyben F, Scherer K R, Schuller B W, Sundberg J, Andre E, Busso C, Devillers L Y, Epps J, Laukka P,"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[16] Giannakopoulos"
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[18] Hochreiter S and Urgen Schmidhuber J J 1997 Mem. Neural Comput. 9 1735–1780 URL http://www7."
        },
        {
          "dimensional emotion recognition.": ""
        },
        {
          "dimensional emotion recognition.": "[19]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": ""
        },
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": ""
        },
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": ""
        },
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": "Irving G,"
        },
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": "Others 2016 Tensorﬂow: A system for large-scale machine learning 12th USENIX Symp. Oper. Syst. Des."
        },
        {
          "covariate shift 32nd Int. Conf. Mach. Learn. ICML 2015 vol 1 pp 448–456 ISBN 9781510810587 (Preprint": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "J. Pers. Soc. Psychol",
      "authors": [
        "J Russell"
      ],
      "year": "1979",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "2",
      "title": "Word-level emotion recognition using high-level features Int",
      "authors": [
        "J D Moore",
        "L Tian",
        "C Lai"
      ],
      "year": "2014",
      "venue": "Word-level emotion recognition using high-level features Int"
    },
    {
      "citation_id": "3",
      "title": "Psychol. Sci",
      "authors": [
        "J R J Fontaine",
        "Scherer K R",
        "E Roesch",
        "Phoebe Fontaine",
        "J Scherer K R",
        "E Roesch",
        "P C Ellsworth"
      ],
      "year": "2017",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "4",
      "title": "Curr. Psychol",
      "authors": [
        "I Bakker",
        "T Van Der Voordt",
        "P Vink",
        "J De Boon"
      ],
      "year": "2014",
      "venue": "Curr. Psychol"
    },
    {
      "citation_id": "5",
      "title": "AV+EC 2015 -The first affect recognition challenge bridging across audio, video, and physiological data AVEC",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proc. 5th Int. Work. Audio/Visual Emot. Challenge, co-Located with MM 2015"
    },
    {
      "citation_id": "6",
      "title": "From Hard to Soft: Towards more Human-like Emotion Recognition by Modelling the Perception Uncertainty ACMMM",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "B Schuller",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "From Hard to Soft: Towards more Human-like Emotion Recognition by Modelling the Perception Uncertainty ACMMM"
    },
    {
      "citation_id": "7",
      "title": "Continuous Emotion Recognition in Speech -Do We Need Recurrence? Interspeech 2019 (ISCA: ISCA",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Continuous Emotion Recognition in Speech -Do We Need Recurrence? Interspeech 2019 (ISCA: ISCA"
    },
    {
      "citation_id": "8",
      "title": "Biometrics",
      "authors": [
        "L I K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "V Pandit",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "",
      "arxiv": "arXiv:1902.05180v6"
    },
    {
      "citation_id": "10",
      "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning Interspeech pp",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning Interspeech pp"
    },
    {
      "citation_id": "11",
      "title": "IEEE/ACM Trans. Audio Speech Lang. Process",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "12",
      "title": "Multitask Learning and Multistage Fusion for Dimensional Audiovisual Emotion Recognition ICASSP 2020 -2020 IEEE Int",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "Acoust. Speech Signal Process"
    },
    {
      "citation_id": "13",
      "title": "Lang. Resour. Eval",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "Lee Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "14",
      "title": "IEEE Trans. Affect. Comput",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E M Provost"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "15",
      "title": "IEEE Trans. Affect. Comput",
      "authors": [
        "F Eyben",
        "Scherer K R",
        "B Schuller",
        "J Sundberg",
        "Andre Busso",
        "C Devillers",
        "L Epps",
        "J Laukka",
        "P Narayanan",
        "S Truong"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "16",
      "title": "PLoS One",
      "authors": [
        "T Giannakopoulos"
      ],
      "year": "2015",
      "venue": "PLoS One"
    },
    {
      "citation_id": "17",
      "title": "Deep Recurrent Neural Networks for Emotion Recognition in Speech DAGA pp",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Deep Recurrent Neural Networks for Emotion Recognition in Speech DAGA pp"
    },
    {
      "citation_id": "18",
      "title": "Mem. Neural Comput. 9 1735-1780 URL",
      "authors": [
        "S Hochreiter",
        "Urgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Mem. Neural Comput. 9 1735-1780 URL"
    },
    {
      "citation_id": "19",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift 32nd Int",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Conf. Mach. Learn. ICML"
    },
    {
      "citation_id": "20",
      "title": "COURSERA Neural networks Mach. Learn",
      "authors": [
        "T Tieleman",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "COURSERA Neural networks Mach. Learn"
    },
    {
      "citation_id": "21",
      "title": "Chollet F and Others",
      "year": "2015",
      "venue": "Chollet F and Others"
    },
    {
      "citation_id": "22",
      "title": "A system for large-scale machine learning 12th USENIX Symp",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard",
        "Others"
      ],
      "year": "2016",
      "venue": "A system for large-scale machine learning 12th USENIX Symp"
    }
  ]
}