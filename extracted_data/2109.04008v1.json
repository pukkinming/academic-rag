{
  "paper_id": "2109.04008v1",
  "title": "Graph Based Network With Contextualized Representations Of Turns In Dialogue",
  "published": "2021-09-09T03:09:08Z",
  "authors": [
    "Bongseok Lee",
    "Yong Suk Choi"
  ],
  "keywords": [
    "Hey Pheebs. S2: Hey! S1: Any sign of your brother? S2: No",
    "but he's always late. S1: I thought you only met him once? S2: Yeah",
    "I did. I think it sounds y'know big sistery",
    "y'know",
    "'Frank's always late.' S1: Well relax",
    "he'll be here. Subject: Frank Object: S2 relation: per:siblings Subject: S2 Object: Frank relation: per:siblings Subject: S2 Object: Pheeb relation:per:alternate_names"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialoguebased RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the state-of-theart models on most of the benchmark datasets. Our code is available at https://github. com/BlackNoodle/TUCORE-GCN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The task of relation extraction (RE) aims to identify semantic relations between arguments from a text, such as a sentence, a document, or even a dialogue. However, since a large number of relational facts are expressed in multiple sentences, sentence-level RE suffers from inevitable restrictions in practice  (Yao et al., 2019) . Therefore, cross-sentence RE, which aims to identify relations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically  (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013) . In this respect, because dialogues Table  1 : An example dialogue and its desired relations in DialogRE  (Yu et al., 2020) . S1, S2: anonymized speaker of each utterance.\n\nreadily exhibit cross-sentence relations  (Yu et al., 2020) , extracting relations from the dialogue is necessary.\n\nTo support the prediction of relation(s) between two arguments that appear within a dialogue,  Yu et al. (2020)  recently proposed DialogRE, which is a human-annotated dialogue-based RE dataset. Table  1  shows an example of DialogRE. In conversational texts such as DialogRE, because of its higher person pronoun frequency  (Biber, 1988)  and lower information density  (Wang and Liu, 2011)  compared to formal written texts, most relational triples require reasoning over multiple sentences in a dialogue. 65.9% of relational triples in DialogRE involve arguments that never appear in the same turn. Therefore, multi-turn information plays an important role in dialogue-based RE.\n\nThere are several major challenges in effective relation extraction from dialogue, inspired by the way how people understand dialogue in practice. First, the dialogue has speakers, and who speaks each utterance matters. The reason for it is because the subject and object of rela-tional triples depend on who is speaking which utterance. For example, if S3 answered \"Hey!\" after \"Hey Pheebs.\", the relational triple (S2, per:alternate_names, Pheebs) will be revised to (S3, per:alternate_names, Pheebs), in the case of Table  1 . Second, when understanding the meaning of each turn in a dialogue, it is important to know the meaning of the surrounding turns. For example, if we look at \"No, but he is always late.\" in Table 1, we don't know who's always late. However, if we look at the previous turn, we can see that S2's brother is always late. Third, the dialogue consists of several turns. Those turns are sequential, and the arguments may appear in different turns. Consequently, it is important to grasp the multiturn information in order to capture the relations between the two arguments. This could be done using the sequential characteristics of dialogues. Therefore, we aim to tackle these challenges to better extract relations from dialogues.\n\nIn this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) for dialogue-based RE. It is designed to tackle the aforementioned challenges. TUCORE-GCN encodes the input sequence to reflect speaker information in dialogue by applying BERT s  (Yu et al., 2020)  and speaker embedding of SA-BERT  (Gu et al., 2020) . Then, to better extract the representations of each turn from the encoded input sequence, Masked Multi-Head Self-Attention  (Vaswani et al., 2017)  is applied using a surrounding turn mask. Next, TUCORE-GCN constructs a heterogeneous dialogue graph to capture the relational information between arguments in the dialogue. It consists of four types of nodes, namely dialogue node, turn node, subject node, object node, and three different types of edges, i.e., speaker edge, dialogue edge, and argument edge. Then, the sequential characteristics of the turn nodes should be considered. To obtain a surrounding turn-aware representation for each node, we apply bidirectional LSTM (BiLSTM)  (Schuster and Paliwal, 1997)  to the turn nodes and a Graph Convolutional Network  (Kipf and Welling, 2017)  to the heterogeneous dialogue graph. Finally, we classify the relations between arguments with the obtained features.\n\nThe task of emotion recognition in conversations (ERC) aims to identify the emotion of utterances in dialogue. ERC is a challenging task that has recently gained popularity due to its potential applica-   tions  (Poria et al., 2019) . It can be used to analyze user behaviors  (Lee and Hong, 2016)  and detect fake news  (Guo et al., 2019) . Table  2  shows an example from EmoryNLP (Zahiri and Choi, 2018), a dataset widely used in the ERC task. We propose a novel approach to treat the ERC task as a dialoguebased RE. If we define the emotion relation of each utterance when the subject says the object with a particular emotion (e.g., joyful, neutral, scared), the emotion of each utterance in the dialogue can be seen as a triple (speaker of utterance, emotion, utterance) as shown in Table  3 . To the best of our knowledge, this approach was not introduced in previous studies.\n\nIn summary, our main contributions are as follows:\n\n• We propose a novel method, TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN), to better cope with a dialogue-based RE task.\n\n• We introduce a surrounding turn mask to bet-ter capture the representation of the turns.\n\n• We introduce a heterogeneous dialogue graph to model the interaction among elements (e.g., speakers, turns, arguments) across the dialogue and propose a GCN mechanism combined with BiLSTM.\n\n• We propose a novel approach to treat the ERC task as a dialogue-based RE.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue-Based Relation Extraction",
      "text": "Relation extraction has been studied extensively over the past few years and many approaches have achieved remarkable success. Most previous approaches focused on sentence-level RE  (Zeng et al., 2014; Wang et al., 2016; Zhang et al., 2017; Zhu et al., 2019) , but recently cross-sentence RE has been studied more because a large number of relational facts are expressed in multiple sentences in practice.\n\nRecent work begins to explore cross-sentence relation extraction on documents that are formal genres, such as professionally written and edited news reports or well-edited websites. In document-level RE, various approaches including transformer-based methods  (Tang et al., 2020; Ye et al., 2020; Wang et al., 2019)  and graphbased methods  (Christopoulou et al., 2019; Nan et al., 2020; Zeng et al., 2020)  have been proposed. Among these, graph-based methods are widely adopted in document-level RE due to their effectiveness and strength in representing complicated syntactic and semantic relations among structured language data. Unlike previous work, we focused on extracting relations from dialogues, which are texts with high pronoun frequencies and low information density.  (Yu et al., 2020; Xue et al., 2021)  were among the early works on dialogue-based RE.  Yu et al. (2020)  introduced several dialogue-based RE approaches with the DialogRE dataset. Among the various approaches, BERT s , a model that uses BERT  (Devlin et al., 2019) , shows good performance. BERT s is a model that slightly modified the original input sequence of BERT in consideration of speaker information. However, it has a limitation in that it cannot predict asymmetric inverse relations well. Our model basically follows the input sequence of BERT s , but we designed it to overcome this limitation to some extent. More detailed explanation is in Sec 4.1.5.  Xue et al. (2021)  proposed a graph-based approach, GDPNet, that constructs a latent multi-view graph to capture various possible relationships among tokens and refines this graph to select important words for relation prediction. In this approach, the refined graph and the BERT-based sequence representations are concatenated for relation extraction. The graph of GDPNet is a multi-view directed graph aiming to model all possible relationships between tokens. Unlike GDPNet, we combine tokens into meaningful units to form nodes and connect the nodes with speaker edges, dialogue edges, and argument edges to model what each edge means. In addition, GPDNet focuses on refining this multi-view graph to capture important words from long texts for RE, but we extract the relations using the features of the nodes in the graph.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversation has emerged as an important problem in recent years and many successful approaches have been proposed. In ERC, numerous approaches including recurrencebased methods  (Majumder et al., 2019; Ghosal et al., 2020)  and graph-based methods  (Ghosal et al., 2019; Ishiwatari et al., 2020)  have been proposed. For instance, DialogueRNN  (Majumder et al., 2019)  uses an attention mechanism to grasp the relevant utterance from the whole conversation and models the party state, global state, and emotional dynamics with several RNNs. COSMIC  (Ghosal et al., 2020 ) adopts a network structure, which is similar to DialogueRNN but adds external common sense knowledge to improve performance. DialogueGCN  (Ghosal et al., 2019)  treats each dialogue as a graph where each node represents utterance and is connected to the surrounding utterances. RGAT  (Ishiwatari et al., 2020)  is based on DialogueGCN. It adds relational positional encodings that can capture speaker dependency, along with sequential information. Many studies with remarkable success have been proposed, but none can be used in ERC as well as other dialogue-based tasks like our approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "TUCORE-GCN mainly consist of four modules: encoding module (Sec 3.1), turn attention module (Sec 3.2), dialogue graph with sequential nodes module (Sec 3.3), and classification mod-Figure  1 : The overall architecture of TUCORE-GCN. First, A contextualized representation of each token is obtained by feeding the input dialogue to the context encoder. Next, Masked Multi-Head Attention using surrounding turn mask is applied to obtain representations that enhance the meaning of each turn. Then, TUCORE-GCN constructs a dialogue graph and applies GCN mechanism combined with BiLSTM. Finally, the classification module predicts relations using information from the previous module. ule (Sec 3.4), as shown in Figure  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Encoding Module",
      "text": "We follow BERT s  (Yu et al., 2020)  as the input sequence of the encoding module. Given a dialogue d = s 1 : t 1 , s 2 : t 2 , ..., s M : t M and its associated argument pair (a 1 , a 2 ), where s i and t i denote the speaker ID and text of the i th turn, respectively, and M is the total number of turns, BERT s constructs d = ŝ1 : t 1 , ŝ2 : t 2 , ..., ŝM : t M , where ŝi is:\n\nwhere [S 1 ] and [S 2 ] are special tokens. In addition, it defines âk (k ∈ {1, 2}) to be [S k ] if ∃i(s i = a k ), and a k otherwise. Then, we concatenate d and (â 1 , â2 ) with a classification token [CLS] and a separator token [SEP] in BERT  (Devlin et al., 2019)  as the input sequence\n\nTo model the speaker change information, following SA-BERT  (Gu et al., 2020) , we add additional speaker embeddings to the token representations. E s (ŝ i ) is added to each token representation of ŝi :\n\n) is added to each token representation of âk if âk = [S k ], and E s ( ) is added to all token representations without speaker embedding added, where E s (•) denotes the speaker embedding layer. E s ( ) is an embedding output for token representations without speaker information. A visual architecture of our input representation is illustrated in Appendix.\n\nThen, token representations containing speaker change information are fed into an encoder to extract the speaker-sensitive token representations. The encoder can be BERT or BERT variants  (Liu et al., 2019; Conneau and Lample, 2019; Lan et al., 2020) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Turn Attention Module",
      "text": "To obtain the turn context-sensitive representation for each turn, we apply Masked Multi-Head Self-Attention  (Vaswani et al., 2017)  to the output of the encoder using the surrounding turn mask. The range of this surrounding turn is called the window, and the number of turns from the front and rear are viewed as the surrounding turn which is called the surround turn window size. The surround turn window size c is a hyper-parameter.\n\nLet X = [x 1 , x 2 , x 3 , ..., x N ] be an output of the encoding module, where x j is the j th token representation in the output and N is the number of tokens. For token representations corresponding to ŝi :\n\ndenotes representations of a dialogue d, and F (x m ) denotes the turn number in which x m is included (e.g.,\n\nWe implement the surrounding turn mask as follows:\n\nwhere R(x m ) denotes\n\nA visual architecture of an example regarding the surrounding turn mask is illustrated in Appendix.\n\nThen, we reinforce the representation of each turn from representations of surrounding turns.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dialogue Graph With Sequential Nodes Module",
      "text": "To model the dialogue-level information, interactions between turns and arguments, and interactions between turns, a heterogeneous dialogue graph is constructed.\n\nWe form four distinct types of nodes in the graph: dialogue node, turn node, subject node, and object node. The dialogue node is a node with the purpose of containing overall dialogue information. Turn nodes represent information about each turn in the dialogue and are created as many as the total number of turns in the dialogue. The subject node and object node represent the information of each argument. In our work, the initial representation of the dialogue node uses a feature corresponding to [CLS] in the output of the turn attention module. The initial representation of the i th turn node, subject node, and object node use the average of the token representations corresponding to ŝi : t i , â1 , and â2 in the output of the turn attention module, respectively.\n\nThere are three different types of edge:\n\n• dialogue edge: All turn nodes are connected to the dialogue node with the dialogue edge so that the dialogue node learns while being aware of turn-level information.\n\n• argument edge: To model the interaction between turns and arguments, the i th turn node and argument nodes (i.e., subject node and object node) are connected with the argument edge if the argument is mentioned in ŝi : t i .\n\n• speaker edge: To model the interaction among different turns of the same speaker, turn nodes uttered by the same speaker are fully connected with speaker edges.\n\nNext, we apply a Graph Convolutional Network (GCN)  (Kipf and Welling, 2017)  to aggregate each node feature from the features of the neighbors. At this time, in order to inject sequential information to the turn nodes, GCN is applied after the turn nodes pass through the bidirectional LSTM  (Schuster and Paliwal, 1997)  layers. Given node u at the l th GCN layer, h (l) u and ĥ(l) u denote the representation of the node before injecting sequential information and the representation of the node after injecting sequential information, respectively. ĥ(l) u can be defined as:\n\notherwise (4) where T i represents an i th turn node and ḣ(l) T i represents turn node feature injected sequential information in the dialogue by concatenating the hidden states of two directions.   and d  is the dimension. Then, the graph convolution operation can be defined as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Module",
      "text": "We concatenate the dialogue node, subject node, and object node to classify the relation between arguments. Furthermore, to cover features of all different abstract levels from each layer of the GCN, we concatenate the hidden states of each GCN layer as follows:\n\nwhere G is the number of GCN layers and d, s, and o denote the dialogue node, subject node, and object node, respectively. For each relation type r, we introduce a vector W r ∈ R 3(G+1)d and obtain the probability P r of the existence of r between arguments by P r = sigmoid(CW T r ). We use crossentropy loss as the classification loss to train our model in an end-to-end way.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we report our experimental results on two tasks, dialogue-based RE and ERC. We experiment with two versions of TUCORE-GCN, TUCORE-GCN BERT and TUCORE-GCN RoBERT a , respectively based on the uncased base model of BERT  (Devlin et al., 2019)  and the large model of RoBERTa  (Liu et al., 2019) . TUCORE-GCN is trained using Adam (Kingma and Ba, 2015) as an optimizer with weight decay 0.01. We run each experiment five times and report the average score along with the standard deviation (σ) for each metric. The full details of our training settings are provided in the Appendix.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dialogue Based Relation Extraction",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our model on DialogRE  (Yu et al., 2020) , an updated English version with a few annotation errors fixed 1 . DialogRE has 36 relation types, 1,788 dialogues, and 8,119 triples, not including no-relation argument pairs, in total. We follow the standard split of the dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Metrics",
      "text": "For DialogRE, We calculate both the F 1 and F 1 c  (Yu et al., 2020)  scores as the evaluation metrics. F 1 c is an evaluation metric to supplement the standard F 1. F 1 c is computed by taking in the part of dialogue as input, instead of only considering the entire dialogue.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baselines And State-Of-The-Art",
      "text": "For a comprehensive performance evaluation, we compared our model with the models using the following baseline and state-of-the-art methods:\n\n1 https://dataset.org/dialogre BERT  (Devlin et al., 2019) : The BERT baseline for dialog-based RE, initialized with pretrained parameters of BERT-base. It is classified using a final hidden vector corresponding to the [CLS] token.\n\nBERT s  (Yu et al., 2020) : A modification to the input sequence of the above BERT baseline. This modification prevents a model from overfitting to the training data and helps a model locate the start positions of relevant turns.\n\nGDPNet  (Xue et al., 2021) : A state-of-the-art model for the DialogRE. GDPNet finds indicative words from long sequences by constructing a latent multi-view graph and refining the graph. It uses the same input format of BERT s and pre-trained parameters of BERT-base.\n\nRoBERTa s : A model that uses the pre-trained parameters of RoBERTa-large  (Liu et al., 2019)  instead of pre-trained parameters of the BERT s above.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "We show the performance of TUCORE-GCN on the DialogRE dataset in Table  4  compared with other baselines.\n\nAmong the models using BERT, TUCORE-GCN BERT outperforms all baselines by 5.3 ∼ 7.6 F 1 scores and 2.9 ∼ 7.1 F 1 c scores on the test set. GDPNet, the state-of-the-art model, achieved highperformance improvement at F 1 c , but TUCORE-GCN showed high-performance improvement at both F 1 and F 1 c . Among the models using RoBERTa, TUCORE-GCN RoBERT a yields a great improvement of F 1/F 1 c on the test set by 1.8/2.2, in comparison with the strong baseline RoBERTa s . Our model can use BERT (or its variants) as an encoder, and in the experiment, we used both the BERT-base model and also the RoBERTa-large model. TUCORE-GCN show outstanding performance even when the BERT-base was used as the encoder. RoBERTa-large was also used, and it achieves state-of-the-art performance on DialogRE dataset with F 1 score 73.1 and F 1 c score 65.9. It suggests that TUCORE-GCN is very effective in this dialog-based RE task.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Analysis On Inverse Relations",
      "text": "We analyze asymmetric inverse relations and symmetric inverse relations performance on the dialogue-based RE task. We divide the DialogRE dev set into three groups depending on whether it was asymmetric inverse relation, symmetric inverse  relation, or other. Then, we report the F 1 score for each group in Appendix. In the dialogue-based RE task, when asymmetric inverse relations are predicted to exists, BERT makes more mistakes compared to symmetric inverse relations  (Yu et al., 2020) . Since BERT learns the tokenized representation of the input sequence through a Self-Attention mechanism, whether the arguments in the input sequence are a subject or an object is not learned in detail. As a result, the performance of asymmetric inverse relations that indicate different relations when subject and object are changed is significantly lower than in symmetric inverse relations that indicate the same relations even when subject and object are changed. However, TUCORE-GCN creates nodes for arguments separately, learns features of these nodes, and classifies relations. Thus, these issues with BERT can be improved. TUCORE-GCN BERT has improved performance in all groups compared to BERT and BERT s , especially for asymmetric inverse relations.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our model on three ERC benchmark datasets. We follow the standard split of the datasets and classify the emotion label of each utterance in the ERC benchmark datasets as the relation between the speaker and the utterance in the dialogue as in Table  3 . MELD  (Poria et al., 2019)  2 is a multimodal dataset collected from the TV show, Friends. We only used textual modality in this dataset. It has seven emotion labels, 2,458 dialogues, and 12,708 utterances. Each utterance is annotated with one of the seven emotion labels.\n\nEmoryNLP (Zahiri and Choi, 2018) 3 is also collected from the TV show, Friends. It has seven emotion labels, 897 dialogues, and 12,606 utter-2 https://affective-meld.github.io 3 https://github.com/emorynlp/emotion-detection ances. Each utterance is annotated with one of the seven emotion labels.\n\nDailyDialog  (Li et al., 2017)   4  reflects our daily communication way and covers various topics about our daily life. It has seven emotion labels, 13,118 dialogues, and 102,979 utterances. Each utterance is annotated with one of the seven emotion labels. Since it does not have speaker information, we consider the utterance turns as two anonymized speaker turns by default.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Metrics",
      "text": "For DailyDialog, we calculate micro-F 1 except for the neutral class, because of its extremely high majority. For MELD and EmoryNLP, we calculate weighted-F 1.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Baselines And State-Of-The-Art",
      "text": "For a comprehensive performance evaluation, we compared our model with the models using the following baseline and state-of-the-art methods:\n\nPrevious methods: CNN  (Kim, 2014) , CNN+cLSTM  (Poria et al., 2017) , DialogueRNN  (Majumder et al., 2019) , DialogueGCN  (Ghosal et al., 2019) , and RoBERTa  (Liu et al., 2019) .\n\nRGAT  (Ishiwatari et al., 2020) : A model that is provided with some information reflecting relation types through relational position encodings that can capture speaker dependency and sequential information.\n\nRoBERTa r : The RoBERTa baseline for ERC as our proposed approach, initialized with pretrained parameters of RoBERTa-large  (Liu et al., 2019) . We set the input sequence of RoBERTa to [CLS]d[SEP]a 1 [SEP]a 2 [SEP] and feed them into RoBERTa for classification.\n\nCOSMIC  (Ghosal et al., 2020)  Table  5 : Overall performance on three ERC datasets. \"-\" signifies that no results were reported for the given dataset. Performance scores of TUCORE-GCN BERT on MELD, EmoryNLP, and DailyDialog have standard deviations of 0.4, 0.7, and 0.4, respectively, and performance scores of TUCORE-GCN RoBERT a have standard deviations of 0.4, 0.6, and 0.8, respectively.\n\nactions, and cause-effect relations for emotional recognition in conversation.\n\nCESTa  (Wang et al., 2020) : A state-of-the-art model for DailyDialog that uses Conditional Random Field layer. The layer is used for sequential tagging, and it has an advantage in learning when there is an emotional consistency in conversation.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "We show the performance of TUCORE-GCN on the ERC datasets in Table  5 , in comparison with other baselines. In addition, the performance of TUCORE-GCN BERT (F 1(σ)) on the development sets of MELD, EmoryNLP, and DailyDialog is 59.75 (0.5), 37.95 (0.8), 60.25 (0.4), respectively, and the performance of TUCORE-GCN RoBERT a is 65.94 (0.5), 40.17 (0.6), 62.83 (0.5), respectively. We have quoted the results for the baselines and state-of-the-art results reported in  (Ishiwatari et al., 2020; Ghosal et al., 2020; Wang et al., 2020) , except for the results of RoBERTa r .\n\nThe only difference between RoBERTa and RoBERTa r is the form of the input sequence, but RoBERTa r is better at solving ERC task. Accordingly, we claim that treating the ERC as a dialogue-based RE is useful in practice. TUCORE-GCN RoBERT a outperforms COSMIC, the previous state-of-the-art model for MELD and EmoryNLP, by 0.15, 1.13, and 3.43 on test sets of MELD, EmoryNLP, and DailyDialog respectively. It shows state-of-art performance on both MELD and EmoryNLP. On the other hand, TUCORE-GCN RoBERT a shows slightly lower performance than CESTa on DailyDialog dataset. When utterances in a conversation are adjacent to one another, they tend to show similar emotions. This is called emotional consistency, and CRF layer of CESTa fits well with this tendency. Therefore, it has better performance on DailyDialog, which shows emotional consistency well. However, it shows very poor performance on MELD, where emotional consistency does not appear much  (Wang et al., 2020) . Considering these observations, our model generally shows outstanding performance on MELD, EmoryNLP, and also DailyDialog. It suggests that TUCORE-GCN is effective in ERC as well as dialogue-based RE.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies to evaluate the effectiveness of different modules and mechanisms in TUCORE-GCN. The results are shown in Table  6 .\n\nFirst, we removed the speaker embedding in the encoder module. To be specific, the encoder and input format of TUCORE-GCN RoBERT a are the same as RoBERTa s . Without speaker embedding, the performance of TUCORE-GCN RoBERT a drops by 0.4 F 1 score and 0.1 F 1 c score on the Di-alogRE test set and 0.72 and 1.65 F 1 scores on the MELD and EmoryNLP test set, respectively. This drop shows that when encoding a dialogue, a better representation can be obtained through speaker change information.\n\nNext, we removed the turn attention module. To be specific, the output of the encoding module is delivered to the dialogue graph with sequential nodes module. Without turn attention, the performance of TUCORE-GCN RoBERT a sharply drops by 1.1 F 1 score and 0.6 F 1 c score on DialogRE test set and 0.77 and 2.17 F 1 scores on the MELD and EmoryNLP test set, respectively. This drop shows that the turn attention module helps capture the representation of the turns and, therefore, improves dialogue-based RE and ERC performance.  Finally, we removed the turn-level BiLSTM for turn nodes in the dialogue graph with sequential nodes module. To be specific, in the module, we apply GCN without injecting sequential information of the turn nodes. Without turn-level BiLSTM, the performance of TUCORE-GCN RoBERT a drops by 0.6 F 1 score and 0.2 F 1 c score on DialogRE test set and 0.34 and 0.89 F 1 scores on MELD and EmoryNLP test set, respectively. This means that reflecting the characteristics of the sequential nodes when learning the graph helps to learn the features of each node and, therefore, improves dialoguebased RE and ERC performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we propose TUCORE-GCN for dialogue-based RE. TUCORE-GCN is designed according to the way people understand dialogues in practice to better cope with dialogue-based RE.\n\nIn addition, we propose a way to treat the ERC task as dialogue-based RE and showed its effectiveness through experiments. Experimental results on a dialogue-based RE dataset and three ERC task datasets demonstrate that TUCORE-GCN model significantly outperforms existing models and yields the new state-of-the-art results on both tasks.\n\nSince TUCORE-GCN is modeled for the dialogue text type, we expect it to perform well in dialogue-based natural language understanding tasks. In future work, we are going to explore the effectiveness of it on other dialogue-based tasks.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A Appendix",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.1 Input Representation",
      "text": "A visual architecture of our input representation is illustrated in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A.2 Surrounding Turn Mask",
      "text": "A visual architecture of our surrounding turn mask of Turn Attention Module is illustrated in Figure  3 . 1 was given as an example for surround turn window size c .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3 Experimental Settings",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3.1 Hyperparameter Settings",
      "text": "We truncate a dialogue when the input sequence length exceeds 512 and use the development set to manually tune the optimal hyperparameters for TUCORE-GCN, based on the F 1 score. Hyperparameter settings for TUCORE-GCN on a dialoguebased RE dataset are listed in Table  7  and the ones on ERC datasets are listed in Table  8 . The final values of hyperparameters we adopted are in bold. We do not tune all the hyperparameters.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.3.2 Other Settings",
      "text": "TUCORE-GCN BERT is implemented by using PyTorch 1.6.0 with CUDA 10.1 and TUCORE-GCN RoBERT a is implemented by using PyTorch 1.7.0 with CUDA 11.0. Our implementation of TUCORE-GCN BERT uses the DGL 5  0.4.3 and our implementation of TUCORE-GCN RoBERT a uses the DGL 5 0.5.3. We used the official code 6  of  (Yu et al., 2020)  to calculate F 1 and F 1 c scores on DialogRE, and scikit-learn 7  to calculate F 1 score on ERC datasets. It takes about 2 hours, 1.25 hours, 1.5 hours, 12 hours to run TUCORE-GCN BERT on DialogRE, MELD, EmoryNLP, and DailyDialog once, respectively. Additionally, it takes about 4 hours, 2.2 hours, 2.3 hours, 20 hours to run TUCORE-GCN RoBERT a on Dialo-gRE, MELD, EmoryNLP, and DailyDialog once, respectively. We conducted all experiments that uses TUCORE-GCN BERT on a Ubuntu server using Intel(R) Core(TM) i9-10900X CPU with 128GB of memory, and used GeForce RTX 2080 Ti GPU with 11GB of memory. We conducted all experiments that uses TUCORE-GCN RoBERT a on a Ubuntu server using Intel(R) Core(TM) i9-10980XE CPU with 128GB of memory, and used GeForce RTX 3090 GPU with 24GB of memory.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4 Experimental Results On Inverse Relations",
      "text": "We show the performance of TUCORE-GCN on asymmetric inverse relation, symmetric inverse relation, and other of DialogRE  (Yu et al., 2020)  in Table  9  compared with other baselines. Among the models using BERT  (Devlin et al., 2019) , TUCORE-GCN BERT has significantly reduced difference between the F 1 scores of asym- metric relation group and the symmetric relation group. The F 1 score difference between two groups were 5.8, which was the smallest F 1 score difference compared with the other models that use BERT. In addition, compared to RoBERTa s 's F 1 score difference between asymmetric relation group and the symmetric relation group, TUCORE-GCN RoBERT a 's F 1 score difference was reduced by 2.9. This suggests that TUCORE-GCN solves the limitations of BERT and its variants' inability to predict the inverse relation well.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Method",
      "text": "Asymmetric Symmetric Other BERT  (Devlin et al., 2019)  42.5 (3.2) 60.7 (1.2) 65.6 (0.8) BERTs  (Yu et al., 2020)  46.5 (3.3) 61.5 (0.7) 69.4 (0.3) GDPNet  (Xue et al., 2021)  47.4 (1.9)    (Yu et al., 2020) . The scores of BERT, BERT s , and GDPNet are based on our re-implementation.\n\nFigure  3 : When the surround turn window size is 1, it is the surrounding turn mask of TUCORE-GCN. For each token, the surrounding turn and its own turn correspond to 1, and the rest is -∞.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of TUCORE-GCN. First, A contextualized representation of each token is ob-",
      "page": 4
    },
    {
      "caption": "Figure 3: 1 was given as an example for surround turn win-",
      "page": 11
    },
    {
      "caption": "Figure 2: The input representation of TUCORE-GCN. The ﬁnal input embeddings are the sum of the token em-",
      "page": 13
    },
    {
      "caption": "Figure 3: When the surround turn window size is 1, it is",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Performance on DialogRE. The scores marked by “*” are based on our re-implementation, because of",
      "data": [
        {
          "Method": "BERT\nBERTs\nGDPNet\nRoBERTas",
          "Dev\nF1 (σ)\nF1c (σ)": "59.4 (0.7)\n54.7 (0.8)\n62.2 (1.3)\n57.0 (1.0)\n61.8 (1.4)*\n58.5 (1.4)*\n72.6 (1.7)\n65.1 (1.7)",
          "Test\nF1 (σ)\nF1c (σ)": "57.9 (1.0)\n53.1 (0.7)\n59.5 (2.1)\n54.2 (1.4)\n60.2 (1.0)*\n57.3 (1.2)*\n71.3 (1.6)\n63.7 (1.2)"
        },
        {
          "Method": "TUCORE-GCNBERT\nTUCORE-GCNRoBERT a",
          "Dev\nF1 (σ)\nF1c (σ)": "66.8 (0.7)\n61.0 (0.5)\n74.3 (0.6)\n67.0 (0.6)",
          "Test\nF1 (σ)\nF1c (σ)": "65.5 (0.4)\n60.2 (0.6)\n73.1 (0.4)\n65.9 (0.6)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TUCORE-GCNRoBERT a\nw/o speaker embedding\nw/o turn attention\nw/o turn-level BiLSTM",
          "DialogRE\nF1 (σ)\nF1c (σ)": "73.1 (0.4)\n65.9 (0.6)\n72.7 (1.0)\n65.8 (0.6)\n72.0 (0.6)\n65.3 (0.3)\n72.5 (0.4)\n65.7 (0.3)",
          "MELD\nF1 (σ)": "65.36 (0.4)\n64.64 (0.9)\n64.59 (0.4)\n65.02 (0.4)",
          "EmoryNLP\nF1 (σ)": "39.24 (0.6)\n37.59 (0.5)\n37.07 (1.1)\n38.35 (0.5)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 8: The final DropoutofGCN 0.6 0.6",
      "data": [
        {
          "Hyperparameter": "",
          "Value": "BERT"
        },
        {
          "Hyperparameter": "Epoch\nBatch Size\nLearning Rate\nSpeaker Embedding Size\nLayers of Turn Attention\nHeads of Turn Attention\nSurround Turn Window Size\nDropout of Turn Attention\nLayers of LSTM\nLSTM Hidden Size\nDropout of LSTM\nLayers of GCN\nGCN Hidden Size\nDropout of GCN",
          "Value": "20\n12\n3e-5\n768\n1\n12\n1, 2, 3\n0.1\n1, 2, 3\n768\n0.2, 0.4, 0.6\n2\n768\n0.6"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: The final DropoutofGCN 0.6 0.6",
      "data": [
        {
          "Hyperparameter": "",
          "Value": "BERT"
        },
        {
          "Hyperparameter": "Epoch\nBatch Size\nLearning Rate\nSpeaker Embedding Size\nLayers of Turn Attention\nHeads of Turn Attention\nSurround Turn Window Size\nDropout of Turn Attention\nLayers of LSTM\nLSTM Hidden Size\nDropout of LSTM\nLayers of GCN\nGCN Hidden Size\nDropout of GCN",
          "Value": "10\n12\n3e-5\n768\n1\n12\n1\n0.1\n1, 2, 3\n768\n0.2, 0.4\n2\n768\n0.6"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Variation across Speech and Writing",
      "authors": [
        "Douglas Biber"
      ],
      "year": "1988",
      "venue": "Variation across Speech and Writing"
    },
    {
      "citation_id": "2",
      "title": "Connecting the dots: Document-level neural relation extraction with edge-oriented graphs",
      "authors": [
        "Fenia Christopoulou",
        "Makoto Miwa",
        "Sophia Ananiadou"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1498"
    },
    {
      "citation_id": "3",
      "title": "Crosslingual language model pretraining",
      "authors": [
        "Alexis Conneau",
        "Guillaume Lample"
      ],
      "year": "2019",
      "venue": "Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "5",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "6",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "7",
      "title": "Speaker-aware BERT for multi-turn response selection in retrieval-based chatbots",
      "authors": [
        "Jia-Chen Gu",
        "Tianda Li",
        "Quan Liu",
        "Zhen-Hua Ling",
        "Zhiming Su",
        "Si Wei",
        "Xiaodan Zhu"
      ],
      "year": "2020",
      "venue": "CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event",
      "doi": "10.1145/3340531.3412330"
    },
    {
      "citation_id": "8",
      "title": "Dean: Learning dual emotion for fake news detection on social media",
      "authors": [
        "Chuan Guo",
        "Juan Cao",
        "Xueyao Zhang",
        "Kai Shu",
        "Huan Liu"
      ],
      "year": "2019",
      "venue": "Dean: Learning dual emotion for fake news detection on social media"
    },
    {
      "citation_id": "9",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "10",
      "title": "Overview of the tac 2010 knowledge base population track",
      "authors": [
        "Ji Heng",
        "Ralph Grishman",
        "Trang Dang",
        "Kira Griffitt",
        "Joe Ellis"
      ],
      "year": "2010",
      "venue": "Third Text Analysis Conference"
    },
    {
      "citation_id": "11",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1181"
    },
    {
      "citation_id": "12",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "13",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "14",
      "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
      "authors": [
        "Zhenzhong Lan",
        "Mingda Chen",
        "Sebastian Goodman",
        "Kevin Gimpel",
        "Piyush Sharma",
        "Radu Soricut"
      ],
      "year": "2020",
      "venue": "th International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "Predicting positive user responses to social media advertising: The roles of emotional appeal, informativeness, and creativity",
      "authors": [
        "Jieun Lee",
        "Ilyoo Hong"
      ],
      "year": "2016",
      "venue": "International Journal of Information Management",
      "doi": "10.1016/j.ijinfomgt.2016.01.001"
    },
    {
      "citation_id": "16",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "18",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "19",
      "title": "Reasoning with latent structure refinement for document-level relation extraction",
      "authors": [
        "Guoshun Nan",
        "Zhijiang Guo",
        "Ivan Sekulic",
        "Wei Lu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.141"
    },
    {
      "citation_id": "20",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "21",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "22",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing",
      "doi": "10.1109/78.650093"
    },
    {
      "citation_id": "23",
      "title": "Overview of the tac2013 knowledge base population evaluation: English slot filling and temporal slot filling",
      "authors": [
        "M Surdeanu"
      ],
      "year": "2013",
      "venue": "Theory and Applications of Categories"
    },
    {
      "citation_id": "24",
      "title": "Intersentential relations in information extraction corpora",
      "authors": [
        "Kumutha Swampillai",
        "Mark Stevenson"
      ],
      "year": "2010",
      "venue": "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10)"
    },
    {
      "citation_id": "25",
      "title": "HIN: hierarchical inference network for documentlevel relation extraction",
      "authors": [
        "Hengzhu Tang",
        "Yanan Cao",
        "Zhenyu Zhang",
        "Jiangxia Cao",
        "Fang Fang",
        "Wang Shi",
        "Pengfei Yin"
      ],
      "year": "2020",
      "venue": "Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference, PAKDD 2020",
      "doi": "10.1007/978-3-030-47426-3_16"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "A pilot study of opinion summarization in conversations",
      "authors": [
        "Dong Wang",
        "Yang Liu"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "28",
      "title": "Finetune bert for docred with two-step process",
      "authors": [
        "Hong Wang",
        "Christfried Focke",
        "Rob Sylvester",
        "Nilesh Mishra",
        "William Yang"
      ],
      "year": "2019",
      "venue": "Finetune bert for docred with two-step process"
    },
    {
      "citation_id": "29",
      "title": "Relation classification via multi-level attention CNNs",
      "authors": [
        "Linlin Wang",
        "Zhu Cao",
        "Gerard De Melo",
        "Zhiyuan Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P16-1123"
    },
    {
      "citation_id": "30",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "31",
      "title": "Gdpnet: Refining latent multi-view graph for relation extraction",
      "authors": [
        "Fuzhao Xue",
        "Aixin Sun",
        "Hao Zhang",
        "Eng Siong"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "32",
      "title": "DocRED: A large-scale document-level relation extraction dataset",
      "authors": [
        "Yuan Yao",
        "Deming Ye",
        "Peng Li",
        "Xu Han",
        "Yankai Lin",
        "Zhenghao Liu",
        "Zhiyuan Liu",
        "Lixin Huang",
        "Jie Zhou",
        "Maosong Sun"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1074"
    },
    {
      "citation_id": "33",
      "title": "Coreferential Reasoning Learning for Language Representation",
      "authors": [
        "Deming Ye",
        "Yankai Lin",
        "Jiaju Du",
        "Zhenghao Liu",
        "Peng Li",
        "Maosong Sun",
        "Zhiyuan Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.582"
    },
    {
      "citation_id": "34",
      "title": "Dialogue-based relation extraction",
      "authors": [
        "Dian Yu",
        "Kai Sun",
        "Claire Cardie",
        "Dong Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.444"
    },
    {
      "citation_id": "35",
      "title": "Emotion detection on TV show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Relation classification via convolutional deep neural network",
      "authors": [
        "Daojian Zeng",
        "Kang Liu",
        "Siwei Lai",
        "Guangyou Zhou",
        "Jun Zhao"
      ],
      "year": "2014",
      "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers"
    },
    {
      "citation_id": "37",
      "title": "Double graph based reasoning for documentlevel relation extraction",
      "authors": [
        "Shuang Zeng",
        "Runxin Xu",
        "Baobao Chang",
        "Lei Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.127"
    },
    {
      "citation_id": "38",
      "title": "Positionaware attention and supervised data improve slot filling",
      "authors": [
        "Yuhao Zhang",
        "Victor Zhong",
        "Danqi Chen",
        "Gabor Angeli",
        "Christopher Manning"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1004"
    },
    {
      "citation_id": "39",
      "title": "Graph neural networks with generated parameters for relation extraction",
      "authors": [
        "Yankai Hao Zhu",
        "Zhiyuan Lin",
        "Jie Liu",
        "Fu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1128"
    }
  ]
}