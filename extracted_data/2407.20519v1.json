{
  "paper_id": "2407.20519v1",
  "title": "Dua: Dual Attentive Transformer In Long-Term Continuous Eeg Emotion Analysis",
  "published": "2024-07-30T03:31:03Z",
  "authors": [
    "Yue Pan",
    "Qile Liu",
    "Qing Liu",
    "Li Zhang",
    "Gan Huang",
    "Xin Chen",
    "Fali Li",
    "Peng Xu",
    "Zhen Liang"
  ],
  "keywords": [
    "Electroencephalography",
    "Continuous EEG Analysis",
    "Affective Brain-Computer Interfaces",
    "Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective brain-computer interfaces (aBCIs) are increasingly recognized for their potential in monitoring and interpreting emotional states through electroencephalography (EEG) signals. Current EEG-based emotion recognition methods perform well with short segments of EEG data (known as segment-based emotion analysis). However, these methods encounter significant challenges in real-life scenarios where emotional states evolve over extended periods. To address this issue, we propose a Dual Attentive (DuA) transformer framework for long-term continuous EEG emotion analysis. Unlike segment-based approaches, the DuA transformer processes an entire EEG trial as a whole, identifying emotions at the trial level, referred to as trial-based emotion analysis. This framework is designed to adapt to varying signal lengths, providing a substantial advantage over traditional methods. The DuA transformer incorporates three key modules: the spatial-spectral network module, the temporal network module, and the transfer learning module. The spatial-spectral network module simultaneously captures spatial and spectral information from EEG signals, while the temporal network module detects temporal dependencies within long-term EEG data. The transfer learning module enhances the model's adaptability across different subjects and conditions. We extensively evaluate the DuA transformer using a self-constructed long-term EEG emotion database, along with two benchmark EEG emotion databases. On the basis of the trial-based leave-one-subject-out cross-subject cross-validation protocol, our experimental results demonstrate that the proposed DuA transformer significantly outperforms existing methods in long-term continuous EEG emotion analysis, with an average enhancement of 5.28%. The DuA transformer's ability to adapt to varying signal lengths and its superior performance across diverse subjects and conditions highlight its potential for real-world applications, enhancing the overall user experience and efficacy of aBCI systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E LECTROENCEPHALOGRAPHY (EEG), a non-invasive technique for monitoring brain electrophysiological activity, records neuronal electrical activity signals through scalp electrodes. Compared to other physiological signals, EEG more directly reflects changes in emotions, providing a neuroscientific interpretation of emotional states  [1] ,  [2] . For certain emotional states, a long-term continuous evolution may be necessary. Take the sadness as an example. Short 1. Equal contributions. * Corresponding authors. stimuli might not effectively evoke vivid sadness, nor might they induce intense, profound sadness in subjects or bring about significant changes in brain activity. This limitation can lead to poor results in emotion recognition. Saarimäki et al.  [3]  found the lowest average classification accuracy for sadness (18%) in their study. In Raz et al. 's work  [4] , it demonstrated that long and complex movie clips are more suitable for inducing the dynamic changes associated with sustained emotional experiences. Furthermore, Xu et al.  [5]  suggested that a 10-minute long stimulus could be more beneficial for brain signal analysis, closely mirroring sustained emotional experiences. This finding confirms that emotional representations induced by long-duration stimuli are superior to those induced by short-duration stimuli.\n\nHowever, most existing EEG-based emotion studies utilize short-duration stimuli to evoke responses  [6] ,  [7] . The algorithms used in these studies typically perform emotion analysis on segmented EEG signals (segment-based emotion analysis), rather than processing an entire trial as a whole (trial-based emotion analysis)  [8] ,  [9] ,  [10] . A more detailed introduction to these current algorithms is provided in Section 2. This approach neglects the temporal information of the entire signal sequence, thus impacting the recognition performance on the whole EEG signal. Implementing a long-duration, whole-segment EEG emotion recognition algorithm faces the challenge of handling variable-length data inputs.\n\nThe transformer network excels in processing variablelength data and has been increasingly applied in the field of arXiv:2407.20519v1 [cs.HC] 30 Jul 2024 EEG for emotion analysis  [10] ,  [11] ,  [12] . For example, Liu et al.  [13]  proposed four variant transformer frameworks (spatial attention, temporal attention, sequential spatialtemporal attention and simultaneous spatial-temporal attention) for EEG-based emotion recognition, exploring the relationship between emotion properties and EEG features. Similarly, Sun et al.  [12]  introduced a transformer-based dynamic graph convolutional neural network (CNN) designed for feature fusion, where the extracted graph features are further updated by the transformer. Additionally, Wei et al.  [14]  proposed a transformer capsule network that integrates an EEG transformer module to extract EEG features and an emotion capsule module to refine these features for easier classification. However, existing transformer-based EEG algorithms have several notable limitations, such as inadequate incorporation of spatial and spectral information, insufficient handling of long-term dependencies, and a lack of research on long-term EEG modeling methods. To tackle the existing limitations, we propose a novel DuA transformer framework for long-term continuous EEG emotion analysis. The main contributions of this study are summarized as below.\n\n• We propose a novel Dual Attentive (DuA) transformer framework for long-term continuous EEG analysis. This novel framework enhances traditional attention mechanisms by improving the performance of emotion decoding for long sequential signals, reducing GPU memory usage, and effectively utilizing the spatial-spectral and temporal information of EEG signals.\n\n• We approach emotion analysis with a trial-based interpretation. Unlike traditional methods that segment EEG trials into short, fixed-length segments (e.g., 1 second) and assign the same label to all segments within a single trial, our proposed framework processes entire EEG trials with variable lengths. This allows for a more flexible and comprehensive analysis of long-term continuous EEG data.\n\n• Extensive experiments are conducted on selfconstructed long-term EEG database and two benchmark databases, covering various data lengths and emotional states. The model's reliability and effectiveness are validated using a strict trial-based leave-one-subject-out cross-subject cross-validation protocol, resulting in an average performance enhancement of 5.28%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Short Segments Based Eeg Analysis",
      "text": "In recent years, a growing number of deep learning algorithms have been developed for EEG-based emotion recognition. For example, Liang et al.  [15]  introduced EEGFuseNet to dynamically capture both shallow and deep EEG features without requiring label information. Fernandez et al.  [16]  leveraged prior knowledge to train a CNN using differential entropy (DE) features, improving the network's ability to recognize emotions. To incorporate structural information, Zhong et al.  [9]  developed a restricted graph network with NodeDAT and EmotionDL, achieving an impressive accuracy of 85.30% in 3-class cross-subject emotion recognition.\n\nHowever, most existing EEG-based emotion recognition algorithms rely on segmenting EEG trials into fixed-length segments (e.g., 1 second), with each segment assigned the same emotional label  [7] ,  [17] ,  [18] . This approach assumes that the emotional state remains constant throughout each segment in a single trial.\n\nIn practical emotion experiments, labels are typically provided for entire trials rather than individual segments. Emotional states are dynamic and can fluctuate significantly over longer periods, making it unrealistic to assume a static emotional label for each short segment. Moreover, research has shown that emotion elicitation is an accumulative process, evolving rather than remaining fixed  [19] . Given these considerations, emotion analysis using EEG signals should be conducted in a trial-based manner rather than a segmentbased manner, especially in supervised learning contexts where accurate label information is crucial. This trial-based approach acknowledges the dynamic nature of emotional states and allows for more accurate and realistic emotion recognition in continuous EEG analysis.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Transfer Learning In Abci",
      "text": "Emotion recognition algorithms face significant challenges due to the variability and complexity of EEG signals and the individual differences in these signals. To address these issues, transfer learning has emerged as a valuable technique, helping to mitigate the discrepancies in feature distribution extracted from various subjects and enhancing model stability in cross-subject EEG-based emotion recognition tasks.\n\nFor non-deep transfer learning methods, Zheng et al.  [20]  employed transfer component analysis (TCA) and transductive parameter transfer (TPT) to achieve an average accuracy of 76.31%, compared to 56.73% without transfer learning. Considering the marginal and conditional distributions in the feature alignment process, Luo et al.  [21]  introduced a manifold-based domain adaptation method. This method adaptively and dynamically adjusted the importance of marginal and conditional distributions during the feature alignment process based on Grassmann manifold space, leading to a further average improvement of 3.54%.\n\nDeep transfer learning algorithms have also shown promise in this field. Jin et al.  [22]  introduced an EEG-based emotion recognition model using domain adversarial neural networks (DANN) The results demonstrated that models using deep transfer learning frameworks outperformed those using non-deep transfer methods, with average accuracy increasing from 76.31% to 79.19%. To further enhance feature alignment, a series of improved DANN frameworks were proposed  [23] ,  [24] . Considering the interaction feature representation between sample features and prototype features, Zhou et al.  [8]  introduced a prototype-representation pairwise learning-based transfer learning framework. This approach not only addressed individual differences but also tackled the issue of noisy labeling, achieving state-of-theart (SOTA) results in cross-subject emotion recognition. The above studies demonstrate that transfer learning, both nondeep learning and deep learning, plays a crucial role in advancing aBCI systems by improving the generalization and robustness of emotion recognition models across different individuals. Future research should continue to explore optimized transfer learning methods, aiming to further bridge\n\nSpectral Features the gap between subject-specific models and generalized emotion recognition frameworks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Spatial-Spectral Attention",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transformer Network With Attention Mechanism",
      "text": "Transformer networks, introduced by Vaswani et al.  [25] , have become integral in both computer vision and natural language processing. In computer vision, models such as vision transformers (ViT) and swin transformers have set new standards. In natural language processing, models like BERT and GPT exemplify their success. The strength of transformer networks lies in their extensive use of attention mechanisms to process sequential data. This mechanism enables the model to identify and focus on the most relevant parts of the input. For each input element, the model assigns a weight indicating its importance, and through learning, it adaptively adjusts these weights. This process mimics human attention to specific details, significantly enhancing the model's performance.\n\nIn the field of continuous EEG analysis, a number of studies have integrated attention mechanisms and transformer networks to enhance performance. For example, Jia et al.  [11]  proposed a 3D fully-connected network based on attention mechanisms for DE feature projection. Tao et al.  [10]  incorporated self-attention mechanisms into CNNs and long short-term memory (LSTM) networks for better feature representation. Additionally, Si et al.  [26]  introduced a temporal aware mixed attention-based convolution and transformer network, which combines CNN and attention mechanisms to jointly extract local and global emotional features of EEG segments. Furthermore, some researchers attempts to directly apply transformer networks. Wang et al.  [27]  proposed a transformer-based model to robustly capture temporal dynamics and spatial correlations of EEG signals. Sun et al.  [12]  extracted power spectral density (PSD) and DE features and then utilized graph and transformer networks to fuse and extract features representing emotions. Similarly, Cui et al.  [28]  introduced a multi-view graph transformer based on spatial relations that integrates information from the temporal frequency and spatial domains to enhance the expressive power of the model comprehensively. However, most existing methods emphasise learning short-term temporal patterns, neglecting significant longterm contextual and information related to emotional cognitive processes  [29] . In addition, these methods inadequate incorporate spatial and spectral information, further leading to poor model performance in long-term continuous EEG analysis tasks.\n\nTo address the challenge of processing long-term continuous EEG signals for emotion recognition, we propose a Dual Attention (DuA) transformer framework, as illustrated in Fig.  1 . Unlike typical one-dimensional time series, longterm EEG signals encompass rich temporal and spatial information. Directly applying transformer networks to these time series might overlook the spatial nuances, thereby diminishing the network's ability to accurately represent features and impacting the effectiveness of emotion recognition. On the other hand, traditional transformer networks, consisting of encoders and decoders, are typically employed for diverse tasks. Encoders are generally used for classification, while decoders are suited for generation tasks. Given that EEG-based emotion recognition is fundamentally a classification task, our approach focuses on utilizing the encoder architecture of the transformer network.\n\nOur proposed framework comprises three main modules: the spatial-spectral network, the temporal network, and a transfer learning module. This model extracts spatiotemporal features from EEG signals, fully exploiting both the temporal and spatial dimensions, overcoming the limitations inherent in traditional transformer networks. Additionally, to address the variability in EEG signals across different individuals, we incorporate a transfer learning strategy, enhancing the network's accuracy in cross-subject long-term EEG emotion recognition tasks. This combination of modules ensures a comprehensive analysis of trial-based EEG signals, leading to more accurate and reliable continuous EEG emotion analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "To address the limitations of traditional transformer networks in effectively utilizing the diverse aspects of longterm EEG signals, we propose a DuA transformer network specifically designed for continuous EEG analysis. DuA transformer includes three key modules. Spatial-Spectral Network Module. This module targets the spatial domain by analyzing the spatial distribution of EEG signals. It captures intricate spatial relationships between different EEG channels in terms of spectral patterns, enabling the model to understand complex spatial dependencies. Temporal Network Module. This module focuses on the temporal domain, effectively modeling the dynamic changes in EEG signals over time. By doing so, it captures the temporal dependencies and variations inherent in EEG data, allowing for a more nuanced understanding of the temporal dynamics. Transfer Learning Module. This module adapts the model to new individuals by leveraging knowledge from previously seen data, ensuring better generalization across different individuals. It enhances the model's ability to generalize and perform well on unseen subjects by transferring learned features and patterns from prior data. The details of each module will be illustrated below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Transformer Basics",
      "text": "In this section, we will introduce the encoder part of the transformer network and its various configurations. The input sequence is processed through the encoder to extract useful information and project the sequence into low-dimensional features for subsequent downstream tasks, such as classification. The encoder consists of several stacked encoder layers. The input and output of the first layer are sequences, and each subsequent layer's input is the output from the previous layer. Each encoder layer is composed of a multi-head self-attention (MHSA) mechanism and a feed-forward network (FFN), connected by residual connections for deeper network construction, followed by layer normalization (LN).\n\nThe MHSA mechanism consists of several self-attention units. Self-attention operates on a \"matching\" mechanism, mapping each element of the sequence into Key, Query, and Value vectors. It calculates the dot product between the Query and Key vectors, transforming the result into attention scores ranging from 0 to 1 using the Softmax function. These attention scores are then multiplied with the Value vectors as follows:\n\nwhere X = {X 1 , X 2 , ..., X n } is the input sequence, n is the length of the sequence, and d k is the dimension of the vectors, which also serves as the normalization factor to maintain the numerical stability of the attention scores. MHSA consists of multiple self-attentions, meaning the outputs from several self-attentions are integrated after mapping as:\n\nwhere head i is given as:\n\nFollowing MHSA, there is a FFN layer, composed of fully connected networks. It performs a nonlinear mapping individually and identically on the output of MHSA at each position in the sequence. First, the features from the MHSA output undergo an upscaling projection to obtain high-dimensional feature representations. Then, these highdimensional features are projected down to the original dimension size for seamless integration into the subsequent layer. The entire feed-forward network is as follows:\n\nAmong these,\n\nand b 2 ∈ R Dm are all trainable parameters. Typically, D f is larger than D m , and generally D f is four times D m . LN is applied after the residual connection, normalizing the output of the forward propagation to ensure that the output and input maintain a similar distribution. This helps to reduce the phenomenon of covariance shift and accelerate the convergence speed of network training.\n\nIn summary, the entire encoder network layer is defined as:\n\nTransformers can process entire sequences using a nonautoregressive approach. However, the self-attention mechanism within transformer networks is permutation invariant; for any given sequence, the attention scores remain the same regardless of the order of elements. However, the position within a sequence is often critical information. Therefore, in transformer networks, sinusoidal and cosine formulas of different frequencies are used across different dimensions to generate high-dimensional positional encodings that are added to the inputs. This method of incorporating positional information is called Positional Encoding, given as: P E(pos, 2i) = sim(pos/10000 2i/d model ), P E(pos, 2i + 1) = cos(pos/10000 2i/d model ),  (8)  where i refers to the specific dimension within the data, d model represents the total dimensionality of the input data, and 10000 signifies that the model can accommodate sequences with a maximum length of 10000.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Spatial-Spectral Network Module",
      "text": "To enhance the representational power of features in EEG signal processing, we propose a novel spatial-spectral network. This network is designed to simultaneously extract significant spatial and frequency domain information from EEG signals at each time point, rather than processing these types of information separately.\n\nTo realize this purpose, we optimize the multi-head attention component of the spatial-spectral network to handle both types of information simultaneously. Given an input signal X ∈ R c×f , where c is the number of channels and f is the number of frequency bands, the optimized multi-head self-attention mechanism maps the input signal X to Query, Key, and Value vectors. During the multi-head self-attention computation, even-indexed heads process the channels as sequences for spatial feature extraction, while odd-indexed heads process the frequency bands as sequences for spectral feature extraction, as;\n\nHere, i represents odd indices and j represents even indices.\n\nThe transformation of the Query, Key, and Value vectors in the transformer network is a linear mapping. Therefore, in the proposed spatial-spectral network, it is only necessary to transpose the Query, Key, and Value vectors of some heads to extract features from both spatial and spectral information, without the need to reconstruct the input sequences. Note that the inputs for spatial and spectral feature extraction are the sequences formed by the different channels of the input EEG signals at each time point and the extracted DE features at different frequency bands, respectively. This proposed network simplifies the encoder process, reducing computational complexity while effectively capturing both spatial and spectral features. By transposing the Query, Key, and Value vectors for specific heads, the network could efficiently handle the different types of information.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Temporal Network Module",
      "text": "The temporal network module receives input from the spatial-spectral network, which provides characterized spatial-spectral EEG information at each second. This information is represented as L ∈ R t×h , where t denotes the time length and h = c × f represents the concatenation of channels and frequency bands. The resulting dimensionality of h, formed by combining channels and frequency bands, is typically high. To address this high dimensionality and create a more manageable feature space, we incorporate an embedding layer for dimensionality reduction before feeding the data into the next transformer. This embedding layer consists of a fully connected network that performs nonlinear mapping on the features at each second within the time series.\n\nTo facilitate sequence classification, we introduce a trainable parameter, CLS, which is randomly initialized and appended to the first position of the sequence. The CLS parameter plays a crucial role in extracting temporal information from the entire long-term EEG signal. Once the sequence, including the CLS parameter, is processed through the transformer's encoder, the final output CLS can effectively represent the entire long-term EEG signal. This approach leverages the power of the transformer architecture to capture intricate temporal patterns and dependencies within the EEG data, enhancing the model's ability to perform accurate and robust sequence classification.\n\nIn addition to the dimensionality reduction and sequence classification capabilities, the temporal network is designed to handle varying lengths of EEG signals by employing positional encoding. This encoding helps the model maintain the temporal order of the data, ensuring that the sequence information is preserved throughout the processing stages.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Transfer Learning Module",
      "text": "One of the fundamental assumptions of deep learning is that training and testing data are drawn from the same distribution and are ideally independent and identically distributed (i.i.d.). However, EEG signals exhibit significant individual variability, making it challenging to satisfy this assumption in the task of emotion recognition from EEG signals. To address this issue, a transfer learning module is incorporated, which includes a feature extractor (Generator) and a domain discriminator (Discriminator). During training, the feature extractor and domain discriminator are trained in opposition to each other to learn domain-invariant features, effectively mitigating the impact of individual variability in EEG signals.\n\nIn the implementation, the subject data of the training set is defined as the source domain (Source), and the subject data of the test set is defined as the target domain (Target). The feature extractor is denoted as G f (θ f ), the domain discriminator as G d (θ d ), and the classifier as G y (θ y ). Here, z represents the extracted features at the output of the temporal network, y represents the label of the training set, and domain distribution alignment is achieved through adversarial loss:\n\nwhere L y is the classification loss, L d is the discriminator loss, and d i is the label for data domain. d i = 0 indicates that the data is from the source domain, while d i = 1 denotes that it is from the target domain. To facilitate implementation, a Gradient Reversal Layer (GRL) is incorporated into the network's backpropagation process. During forward propagation, GRL acts as an identity mapping:\n\nDuring backpropagation, GRL reverses the gradients:\n\nwhere λ is a dynamic balancing hyperparameter used to ensure the stability of the domain adversarial training process, given as:\n\nEEG signals from different subjects not only exhibit distributional differences but also individual temporal variations. To address this, our algorithm treats the spatial and temporal feature extraction networks as a unified feature extractor, rather than relying solely on the spatial feature extraction network. The loss function of the network is defined as follows:  (14)  where L C denotes the cross-entropy loss function for classification, L adv corresponds to the binary cross-entropy loss function for domain discrimination. f (•) is the feature extractor, c(•) is the classifier, and d(•) is the domain discriminator. X S and X T indicate the input data from the source and target domains, respectively. An overall algorithm of the proposed DuA transformer is illustrated in Algorithm 1.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Algorithm 1",
      "text": "The algorithm flow of DuA transformer.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Require:",
      "text": "-max iteration τ , batch size ξ, length t, channel c, the size of feature h, source data {(X s , Y s )}, target data {X t }.\n\n-feature extractor f (•) contains spatial-spectral network\n\nGenerate second's feature L = f 1 (X);\n\nGenerate sequential feature CLS = f 2 (L);",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "7:",
      "text": "Calculate classification probability q c = c(CLS s );",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "8:",
      "text": "Calculate discrimination probability q d = d(CLS);",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Eeg Data And Experimental Protocol",
      "text": "Given the limited availability of EEG emotional databases specifically designed for long-term EEG signals, we conducted a study to fill this gap by undertaking EEG emotional experiments under prolonged emotional stimulation.\n\nThe study involved 50 subjects (25 males and 25 females, average age: 18.72 ± 1.23 years). A total of 12 long-term videos (6 positive and 6 negative) was selected as stimuli, each with an average duration of 11.11 ± 1.64 minutes. There was no content overlap among the selected videos. After watching each video, the subjects were asked to recall their emotions and rate their valence levels on a scale of [-10, -5, 0, 5, 10]. Here, -10 indicates extremely negative, 10 indicates extremely positive, and 0 refers to neutral.\n\nEEG signals were recorded in real-time using a Brain Products device with 63 channels configured according to the 10-20 system, as each subject watched the 12 long-term videos. To manage fatigue, the experiment was conducted in two sessions, with each session featuring videos that induced a single type of emotion (either positive or negative). These sessions were spaced at least one week apart. In total, the database comprises 600 long-term continuous EEG signal recordings.\n\nFor the recorded EEG signals, we extracted DE features from each channel every second across the following ten frequency bands: Theta (4-6 Hz), Alpha1 (6-8 Hz), Alpha2 (8-10 Hz), Alpha3 (10-12 Hz), Beta1 (12-16 Hz), Beta2 (16-20 Hz), Beta3 (20-28 Hz), Gamma1 (28-34 Hz), Gamma2 (34-39 Hz), and Gamma3 (39-45 Hz). For each trial of the long-term continuous EEG signal recordings, the EEG signals were then transformed into a three-dimensional matrix by time, channel, and frequency band.\n\nFor model validation, we adopt a trial-based leave-onesubject-out cross-validation protocol. Using our database of 50 subjects, we iteratively use the data from 49 subjects as the training set and the data from the remaining one subject as the test set. This process is repeated until each subject has served as the test set exactly once. The final validation results are obtained by calculating the average and standard deviation of these 50 test iterations. Additionally, leveraging the collected subjective scores ([-10, -5, 0, 5, 10]), we define three types of emotion classification tasks (2-class, 3-class, and 5-class), as illustrated in Fig.  2 . This approach ensures a robust assessment of the model's ability to generalize across different subjects and emotional states. Fig.  2 : An illustration of different classification tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "In our experiment, we utilize the proposed DuA transformer, which integrates a spatial-spectral network and a temporal network as the feature extractor, and an MLP with a ReLU activation function as the domain discriminator. The parameters of the transformer network are randomly initialized from a uniform distribution. Specifically, the spatial-spectral network is configured as below. The input is X ∈ R c×f , where c is the number of channels (63), f is the number of frequency bands  (10) . The network uses 6 heads, with QKV vector mapping dimensions of 60 (each head having a QKV dimension of 10). The FFN layer is configured with a 10-40-10 mapping. The spatial-spectral network consists of one encoder layer. Subsequently, the channel and frequency band dimensions are concatenated and mapped from 630 to 128 before being fed into the temporal network. For the temporal network, it uses 3 heads, with QKV vector mapping dimensions of 384 (each head having a QKV dimension of 128). The FFN layer is configured with a 128-512-128 mapping. Similar as the spatial-spectral network, the temporal network also consists of one encoder layer.\n\nDuring training, the number of epochs is set to 300. We use a random mini-batch gradient optimization algorithm to update the model parameters. The batch size for the training process is 12, the optimizer is Adam with a momentum parameter of 0.9, and the learning rate is 1 × 10 -3 . The model is trained on an NVIDIA GeForce RTX 3090 GPU, using CUDA 10.0 drivers, and the training is conducted with the Pytorch API.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Emotion Recognition Results",
      "text": "We conduct an extensive evaluation of existing machine learning and deep learning methods on long-term continuous EEG signals using trial-based leave-one-subject-out cross-subject cross-validation. As shown in   We further validate our proposed DuA transformer model on two benchmark databases: SEED  [41]  and SEED-IV  [42] . Unlike existing experiments on these databases, which predominantly employ segment-based leave-onesubject-out cross-validation, our study adopts a trial-based leave-one-subject-out cross-validation approach. This crossvalidation shift enhances reliability, as emotional labels are assigned based on entire trials rather than segmented portions. This ensures a more accurate and holistic assessment of the model's performance in recognizing emotional states. The experimental results on the two benchmark databases are presented in Table  2  and Table  3 , respectively.\n\nIn our analysis, we observe that the trial-based approach not only aligns more closely with real-world scenarios, where emotional experiences are continuous rather than discrete, but also addresses potential inconsistencies that may arise from segmenting data. By validating the model on these well-established databases using a more robust cross-validation technique, we demonstrate the DuA transformer's superior capability in handling complex emotional data. Additionally, our findings suggest that the trial-based cross-validation method provides a more comprehensive understanding of the model's effectiveness, particularly in dynamic and fluctuating emotional environments. This approach could significantly impact future research in emotion recognition, setting a new benchmark for evaluation practices.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion Conclusion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation experiments to investigate the contributions of various components of our model to its overall performance. As shown in Table  4 , we evaluate the role of the spatial-spectral network module under three conditions. (1) Complete Removal of the Spatial-Spectral Network (Temp-only). When the spatial-spectral network is entirely removed, there is a significant drop in accuracy: 8.83% for 2class classification, 7.66% for 3-class classification, and 8.51% for 5-class classification. This highlights the importance of the spatial-spectral network in achieving higher accuracy across different classification tasks. (2) Spatial Information Only (Spat-Temp). Retaining the spatial-spectral network but utilizing only spatial information results in a noticeable decline in performance across all classification tasks. This indicates that spatial information alone is insufficient for optimal model performance. (3) Spectral Information Only (Spec-Temp). Similarly, when we retain the spatialspectral network but focus solely on spectral information, the results demonstrate that considering spatial information among EEG channels significantly benefits model performance. Balancing model performance and computational cost, our findings suggest that the proposed method strikes an effective balance. It not only reduces computational complexity but also preserves critical emotion representations, thereby maintaining robust performance across various classification tasks. This balanced approach ensures that the model remains both efficient and effective in recognizing and classifying emotions. In the ablation experiments of the temporal network module, we conduct the experiments under two conditions to evaluate its significance (Table  5 ). (1) Complete Removal of the Temporal Network (Spat-Spec w/o Temp). Eliminating the temporal network leads to a notable degradation in performance, with a 11.15% decrease in 2-class classification, a 18.33% decrease in 3-class classification, and a 20.02% decrease in 5-class classification. This substantial drop underscores the critical role of temporal features in emotion recognition within long-term EEG signals. Long-term signals inherently contain more temporal information redundancy compared to short-term signals. The transformer network's self-attention mechanism allows it to focus on key time points, thereby extracting robust temporal features from these redundant long-term sequences. This finding demonstrates that neglecting temporal features significantly impairs the model's performance. (2) Retaining the Temporal Network but Removing Positional Encoding (Spat-Spec-Temp w/o PE). In this condition, we maintain the temporal network but remove the positional encoding from the transformer network. The results reveal that positional encoding is crucial for capturing the temporal dynamics of the signals. The absence of positional encoding results in a 14.50% decrease in 2-class classification, a 11.51% decrease in 3-class classification, and a 9.17% decrease in 5-class classification. This decline highlights the importance of preserving the temporal order of signals in long-term EEG data, as it plays a vital role in the accurate classification of emotional states. Overall, these experiments illustrate the indispensable nature of temporal features and positional encoding in enhancing the performance of emotion recognition models utilizing long-term EEG signals. The findings emphasize that both the ability to capture temporal dependencies and the maintenance of signal order are essential for achieving robust and accurate emotion classification. The superior performance of our method can be attributed to the limitations of the LSTM network. LSTMs face issues with gradient accumulation due to their autoregressive calculation nature. As the length of the data sequence increases, the gradient accumulation becomes more pronounced, leading to the gradient vanishing problem. This issue hampers the LSTM's ability to effectively handle longterm signals. In contrast, the transformer network, which we employ in our method, does not rely on autoregressive calculations. Consequently, it avoids the gradient accumulation problem, making it more suitable for tasks involving longterm signals. The results further validate the advantages of using transformer networks over LSTMs for temporal analysis in long-term continuous EEG signal processing. Our findings highlight the importance of selecting appropriate temporal analysis methods to enhance efficiency and effectiveness in long-term continuous EEG emotion analysis.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Hyperparameter Analysis",
      "text": "We conduct multiple sets of comparative experiments by adjusting the hyperparameters of our algorithmic framework. Given the constraints of computational power, we systematically analyze the model's sensitivity to various parameters. Specifically, we examine the number of heads in the spatial-spectral network  (2, 4, 6, 8) , the number of layers in the spatial-spectral network (1, 2, 3), and the hidden size in the temporal network  (32, 64, 128, 256) .\n\nTo ensure a robust analysis, we vary one set of hyperparameters while keeping the others constant. This method allows us to isolate the effects of each parameter on the model's performance. The experimental results, presented in Fig.  3 , demonstrate that our model exhibits low sensitivity to changes in these hyperparameters. This stability indicates that the model is robust across a range of parameter values. Moreover, the results reveal that achieving optimal performance does not require extreme parameter values. Instead, moderate values are sufficient to attain the best outcomes, suggesting a balance between model complexity and computational efficiency. This finding is particularly important for practical applications where computational resources are limited.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present a novel Dual Attentive (DuA) transformer framework designed to address the challenges associated with processing long-term continuous EEG signals. Unlike existing segment-based processing pipelines, our approach treats a single trial as a comprehensive entity, analyzing dependencies across the entire trial by leveraging spatial-spectral and temporal information. The proposed DuA transformer outperforms various baseline methods, demonstrating the effectiveness of this optimized algorithm framework. By capturing the full scope of emotional experiences, our DuA transformer framework paves the way for the development of more accurate, reliable, and practical emotion recognition systems. Furthermore, the DuA transformer framework's ability to integrate and process comprehensive EEG data can lead to breakthroughs in understanding the neural underpinnings of emotions. This advancement holds significant potential for applications in mental health, human-computer interaction, and affective computing, offering improved tools for diagnosing and monitoring emotional states, enhancing user experience in interactive systems, and advancing research in affective sciences.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of the proposed Dual Attentive (DuA) transformer model. (a) the main pipeline of the",
      "page": 3
    },
    {
      "caption": "Figure 1: Unlike typical one-dimensional time series, long-",
      "page": 3
    },
    {
      "caption": "Figure 2: This approach ensures a",
      "page": 6
    },
    {
      "caption": "Figure 2: An illustration of different classification tasks.",
      "page": 6
    },
    {
      "caption": "Figure 3: , demonstrate that our model exhibits low sensitivity",
      "page": 9
    },
    {
      "caption": "Figure 3: Hyperparameter analysis of (a) the number of attention heads in the spatial-spectral network, (b) the number of",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The mean accuracy (%) and standard deviation",
      "page": 7
    },
    {
      "caption": "Table 2: The mean accuracy (%) and standard deviation",
      "page": 7
    },
    {
      "caption": "Table 3: The mean accuracy (%) and standard deviation",
      "page": 7
    },
    {
      "caption": "Table 2: and Table 3, respectively.",
      "page": 7
    },
    {
      "caption": "Table 4: , we evaluate the role",
      "page": 8
    },
    {
      "caption": "Table 4: Ablation study of the spatial-spectral network",
      "page": 8
    },
    {
      "caption": "Table 5: ). (1) Complete Removal",
      "page": 8
    },
    {
      "caption": "Table 5: Ablation study of temporal network in our pro-",
      "page": 8
    },
    {
      "caption": "Table 6: , our method significantly outperforms",
      "page": 8
    },
    {
      "caption": "Table 6: Experimental comparison between LSTM and the",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "An unsupervised eeg decoding system for human emotion recognition",
      "authors": [
        "Z Liang",
        "S Oba",
        "S Ishii"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Classification of emotion categories based on functional connectivity patterns of the human brain",
      "authors": [
        "H Saarimäki",
        "E Glerean",
        "D Smirnov",
        "H Mynttinen",
        "I Jääskeläinen",
        "M Sams",
        "L Nummenmaa"
      ],
      "year": "2022",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "4",
      "title": "Portraying emotions at their unfolding: A multilayered approach for probing dynamics of neural networks",
      "authors": [
        "G Raz",
        "Y Winetraub",
        "Y Jacob",
        "S Kinreich",
        "A Maron-Katz",
        "G Shaham",
        "I Podlipsky",
        "G Gilam",
        "E Soreq",
        "T Hendler"
      ],
      "year": "2012",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "5",
      "title": "Functional connectivity profiles of the default mode and visual networks reflect temporal accumulative effects of sustained naturalistic emotional experience",
      "authors": [
        "S Xu",
        "Z Zhang",
        "L Li",
        "Y Zhou",
        "D Lin",
        "M Zhang",
        "L Zhang",
        "G Huang",
        "X Liu",
        "B Becker",
        "Z Liang"
      ],
      "year": "2023",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "6",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "7",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "EEG-Based Emotion Recognition via Channel-Wise Attention and Self Attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "SST-EmotionNet: Spatial-Spectral-Temporal based Attention 3D Dense Network for EEG Emotion Recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "J Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20"
    },
    {
      "citation_id": "12",
      "title": "A Dual-Branch Dynamic Graph Convolution Based Adaptive TransFormer Feature Fusion Network for EEG Emotion Recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "S Yu",
        "H Han",
        "B Hu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Spatial-temporal transformers for eeg emotion recognition",
      "authors": [
        "J Liu",
        "H Wu",
        "L Zhang",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 6th International Conference on Advances in Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Tc-net: A transformer capsule network for eeg-based emotion recognition",
      "authors": [
        "Y Wei",
        "Y Liu",
        "C Li",
        "J Cheng",
        "R Song",
        "X Chen"
      ],
      "year": "2023",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "15",
      "title": "EEGFuseNet: Hybrid Unsupervised Deep Feature Characterization and Fusion for High-Dimensional EEG With an Application to Emotion Recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "16",
      "title": "Cross-Subject EEG-Based Emotion Recognition Through Neural Networks With Stratified Normalization",
      "authors": [
        "J Fdez",
        "N Guttenberg",
        "O Witkowski",
        "A Pasquali"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Emo-tionMeter: A Multimodal Framework for Recognizing Human Emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "19",
      "title": "Classification of Unmedicated Bipolar Disorder Using Whole-Brain Functional Activity and Connectivity: A Radiomics Analysis",
      "authors": [
        "Y Wang",
        "K Sun",
        "Z Liu",
        "G Chen",
        "Y Jia",
        "S Zhong",
        "J Pan",
        "L Huang",
        "J Tian"
      ],
      "year": "2020",
      "venue": "Classification of Unmedicated Bipolar Disorder Using Whole-Brain Functional Activity and Connectivity: A Radiomics Analysis"
    },
    {
      "citation_id": "20",
      "title": "Personalizing EEG-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Mddd: Manifold-based domain adaptation with dynamic distribution for non-deep transfer learning in cross-subject and cross-session eeg-based emotion recognition",
      "authors": [
        "T Luo",
        "J Zhang",
        "Y Qiu",
        "L Zhang",
        "Y Hu",
        "Z Yu",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "Mddd: Manifold-based domain adaptation with dynamic distribution for non-deep transfer learning in cross-subject and cross-session eeg-based emotion recognition",
      "arxiv": "arXiv:2404.15615"
    },
    {
      "citation_id": "22",
      "title": "EEG-based emotion recognition using domain adaptation network",
      "authors": [
        "Y.-M Jin",
        "Y.-D Luo",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Orange Technologies (ICOT)"
    },
    {
      "citation_id": "23",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "24",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention Is All You Need"
    },
    {
      "citation_id": "26",
      "title": "Temporal aware mixed attention-based convolution and transformer network (mactn) for eeg emotion recognition",
      "authors": [
        "X Si",
        "D Huang",
        "Y Sun",
        "D Ming"
      ],
      "year": "2023",
      "venue": "Temporal aware mixed attention-based convolution and transformer network (mactn) for eeg emotion recognition",
      "arxiv": "arXiv:2305.18234"
    },
    {
      "citation_id": "27",
      "title": "Temporal-spatial representation learning transformer for eeg-based emotion recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "Temporal-spatial representation learning transformer for eeg-based emotion recognition",
      "arxiv": "arXiv:2211.08880"
    },
    {
      "citation_id": "28",
      "title": "Mvgt: A multi-view graph transformer based on spatial relations for eeg emotion recognition",
      "authors": [
        "Y Cui",
        "X Liu",
        "J Liang",
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Mvgt: A multi-view graph transformer based on spatial relations for eeg emotion recognition",
      "arxiv": "arXiv:2407.03131"
    },
    {
      "citation_id": "29",
      "title": "Emt: A novel transformer for generalized cross-subject eeg emotion recognition",
      "authors": [
        "Y Ding",
        "C Tong",
        "S Zhang",
        "M Jiang",
        "Y Li",
        "K Liang",
        "C Guan"
      ],
      "year": "2024",
      "venue": "Emt: A novel transformer for generalized cross-subject eeg emotion recognition",
      "arxiv": "arXiv:2406.18345"
    },
    {
      "citation_id": "30",
      "title": "Least Squares Support Vector Machine Classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "31",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-Nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "32",
      "title": "Random Forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "Multi-class AdaBoost",
      "authors": [
        "T Hastie",
        "S Rosset",
        "J Zhu",
        "H Zou"
      ],
      "year": "2009",
      "venue": "Statistics and Its Interface"
    },
    {
      "citation_id": "34",
      "title": "Domain Adaptation via Transfer Component Analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "35",
      "title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI'16"
    },
    {
      "citation_id": "38",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation"
    },
    {
      "citation_id": "40",
      "title": "Domain-Adversarial Training of Neural Networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M March",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "41",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "42",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "43",
      "title": "Long Short-Term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    }
  ]
}