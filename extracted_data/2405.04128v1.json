{
  "paper_id": "2405.04128v1",
  "title": "Fine-Grained Speech Sentiment Analysis In Chinese Psychological Support Hotlines Based On Large-Scale Pre-Trained Model",
  "published": "2024-05-07T08:53:25Z",
  "authors": [
    "Zhonglong Chen",
    "Changwei Song",
    "Yining Chen",
    "Jianqiang Li",
    "Guanghui Fu",
    "Yongsheng Tong",
    "Qing Zhao"
  ],
  "keywords": [
    "Suicide",
    "Speech emotion recognition",
    "Deep learning",
    "Psychological support hotlines",
    "Language pre-trained models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Suicide and suicidal behaviors remain significant challenges for public policy and healthcare. In response, psychological support hotlines have been established worldwide to provide immediate help to individuals in mental crises. The effectiveness of these hotlines largely depends on accurately identifying callers' emotional states, particularly underlying negative emotions indicative of increased suicide risk. However, the high demand for psychological interventions often results in a shortage of professional operators, highlighting the need for an effective speech emotion recognition model. This model would automatically detect and analyze callers' emotions, facilitating integration into hotline services. Additionally, it would enable large-scale data analysis of psychological support hotline interactions to explore psychological phenomena and behaviors across populations. Our study utilizes data from the Beijing psychological support hotline, the largest suicide hotline in China. We analyzed speech data from 105 callers containing 20,630 segments and categorized them into 11 types of negative emotions. We developed a negative emotion recognition model and a fine-grained multi-label classification model using a large-scale pre-trained model. Our experiments indicate that the negative emotion recognition model achieves a maximum F1-score of 76.96%. However, it shows limited efficacy in the fine-grained multi-label classification task, with the best model achieving only a 41.74% weighted F1-score. We conducted an error analysis for this task, discussed potential future improvements, and considered the clinical application possibilities of our study. All the codes are public available at: https://github.com/czl0914/psy_hotline_analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Datasets",
      "text": "Our experimental data were sourced from the Beijing Psychological Intervention Hotline at Huilongguan Hospital in Beijing, which is the first and largest suicide hotline in China. From this resource, we randomly selected 105 sets of recorded conversations between operators and callers, resulting in a total of 20,630 segmented caller voice sentences. Three experts labeled these segments for negative emotions across 11 fine-grained categories, and the final labels were chosen as a concatenation of the labeling results of the three experts. The dataset includes 9,774 segments identified with negative emotions and 10,856 segments with non-negative emotions. Of the segments with negative emotions, \"Sadness\" was the most prevalent, accounting for 5,662 entries, making it the dominant emotion category. This data distribution for fine-grained sentiment analysis is detailed in Table  I . As shown in Table  II , 57% of the segments displayed a single emotion, though multiple emotions could co-occur, such as \"Sadness\" combined with \"Resentment\", or \"Sadness\" with \"Anxiety\". Some emotion labels, such as \"Sadness\" and \"Pain\" or \"Sadness\" and \"Grievance\", frequently co-occur, suggesting a possible intrinsic connection between these emotional states or a common likelihood of being triggered simultaneously in certain contexts. The relationships of label co-occurrence are further illustrated in Figure  1 .\n\nThe distribution of speech segments durations in our dataset is shown in Figure  2 . The data show a prevalence of short speech segments, with 35.30% of the samples ranging between 0 to 3 seconds and 23.82% ranging from 3 to 6 seconds. Speech segments with durations between 6 to 15 seconds also represent a large proportion (27.96%), which may indicate more complex expressions of emotion or more detailed descriptions of specific situations. Longer speech samples, exceeding 15 seconds, show a progressive decrease in frequency. In particular, there are only a few cases of samples that exceed 30 seconds. The average speech length in this dataset is 7.54 seconds, with the longest speech sample recorded at 63.61 seconds. The variability in speech duration, combined with the complexity of sentiment categories and the challenges posed by label attribution, complicates our analytical task.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "We selected several large-scale pre-trained models: Wav2Vec 2.0  [23] , HuBERT  [24] , and Whisper  [25]  which are based on the Transformer architecture  [27] . We conducted a comparative analysis using different versions of these models, varying in model size and training data. The details are as follows.\n\nA. Pre-trained feature extractor a) Wav2Vec 2.0: Wav2Vec 2.0  [23]  is a framework designed for building automatic speech recognition (ASR) systems. This self-supervised learning framework learns feature representations directly from raw audio data. It encodes audio using a multi-layer convolutional neural network, then masks spans of the resulting latent speech representations, an approach similar to the training of language model like BERT  [19] . These latent representations are subsequently fed into a Transformer  [27]  model to build contextualized representations. The model is trained via a contrastive task, allowing it to learn representations solely from speech and then fine-tune on transcribed speech. This method can outperform the best semi-supervised methods while being conceptually simpler. Wav2vec 2.0 captures subtle changes in speech, such as pitch and speed, which are key factors in identifying emotions.\n\nFor our experiments, we used a version of Wav2Vec 2.0 that has been fine-tuned on multiple Chinese speech datasets, including Common Voice 6.1  [28] , CSS10  [29] , and ST-CMDS  [30] . This fine-tuning enhances its relevance and effectiveness for the specific characteristics of Chinese speech.\n\nb) HuBERT: HuBERT  [24]  is a self-supervised learning model designed for speech processing tasks. It combines BERT's language model architecture and uses unique speech processing technology. The architecture of HuBERT is basically based on Transformers, which uses a self-attention mechanism to process sequence data. HuBERT is optimized for the characteristics of speech signals. Its training process consists of two stages. In the pre-training stage, the model is trained to predict hidden units in the speech signal. These hidden units are obtained by clustering analysis of pre-processed speech features, a process that mimics the masked language model task in traditional self-supervised learning. On the basis of pre-training, HuBERT uses more detailed labels or real downstream task labels for further training, such as the LibriSpeech  [31]  data set, which can improve the model's adaptability and accuracy for specific speech tasks. HuBERT is particularly suitable for processing large-scale unlabeled speech data and can effectively extract useful information to support complex speech analysis tasks. HuBERT performs well when processing speech data with complex background noise, and can effectively identify the emotional content in speech.\n\nIn our experiments, we used the base version of HuBERT, which strikes a balance between model size and performance. Compared with the large version, it requires less computing resources for training and inference, which can help us quickly detect whether the HuBERT architecture is Suitable for our mission. c) Whisper: Whisper is a pre-trained model designed for ASR and speech translation, trained on an extensive 680,000 hours of multilingual and multitask supervised data collected from the web  [25] . The architecture of Whisper is an end-to-end encoder-decoder Transformer  [32] . The process begins by splitting input audio into 30-second chunks and converted into a log-Mel spectrogram. This spectrogram is pass through an encoder, and a decoder is trained to predict the corresponding text caption, incorporating special tokens that enable the model to perform various tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and speech-to-English translation. Whisper demonstrates strong zero-shot performance, allowing it to generalize across many datasets and domains without the need of fine-tuning.\n\nIn our experiments, we utilized four versions of Whisper, including Whisper-small-Chinese-base, Whisper-small, Whisper-medium, and Whisper-large-v3. The Whisper-small-Chinese-base is a version of Whisper-small that has been fine-tuned on the Google Fleurs  [33]  \"cmn-hans-cn\" dataset, specifically optimized for Chinese language tasks. The Whisper-large-v3 model, compared to the standard large version, is trained for 2.5 times more epochs with added regularization to enhance performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Fine-Tuning On Downstream Tasks",
      "text": "We used a pre-trained model to extract features from the speech segments, utilizing a hyperbolic tangent activation function and average pooling for the initial processing of the speech data. Following the feature extraction, we constructed a classifier designed to utilize the extracted speech representations as inputs for performing downstream tasks.\n\nThe classifier is composed of two linear layers interspersed with a dropout layer. In the forward propagation function of our model, the input features are first processed by the dropout layer, which randomly drops certain features. This is followed by a dense linear layer where the hyperbolic tangent activation function is applied to introduce non-linearity, enhancing the model's capability to learn complex patterns in the data. Finally, the processed features pass through another linear layer, which outputs the classification scores for each category.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Implementation Details",
      "text": "During model training, we divided the 105 speeches into training and test datasets at a ratio of 4:1. Specifically, 84 speeches were used for training and the remaining 21 for testing. We conducted 5-fold cross-validation within our training set to divide the 84 speeches into separate training and validation set. Note that, we split the data at the level of entire speeches rather than at the segment level to prevent data leakage. In our experiments, we used accuracy, recall, and F1-score as evaluation metrics. Specifically, for the finegrained emotion multi-label classification task, we report the performance as a weighted average to address the issue of data imbalance.\n\nThe experiments were conducted using a NVIDIA 24GB RTX 4090 GPU. We set the batch size at 16 with a learning rate of 1e-4. Additionally, we implemented a gradient accumulation strategy, updating parameters every two accumulation steps. The model was trained for three epochs, utilizing half-precision training (fp16) to expedite the training process and decrease memory requirements.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Negative Emotion Recognition",
      "text": "The experimental results for the negative emotion recognition task can be seen in Table  III .\n\nThe evaluation of several pre-trained models for negative emotion recognition yielded close performance metrics,  with Wav2Vec 2.0 slightly leading with an F1 score of 76.6%. Wav2Vec 2.0's robust audio processing capabilities are particularly effective at interpreting emotional nuances in speech, which is critical for emotion recognition tasks. The Whisper Series models, which range from small to large, demonstrate that larger models do not necessarily yield better results in emotional recognition, as they all achieved approximately 75% F1 scores. This suggests that improvements in model performance may be due to refined architectures or improved training approaches rather than simply scaling up the models. Furthermore, the Chinese language finetuned version, Whisper-small-Chinese-base, showed only a marginal improvement, achieving a 1.11% point increase in F1 score over the original Whisper-small version. In addition, HuBERT performed poorly in this task, suggesting that certain models may require specific adjustments to increase their effectiveness in complex emotional recognition scenarios.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Fine-Grained Emotion Multi-Label Classification",
      "text": "The results of fine-grained emotion multi-label classification depicted in Table  IV  and Table\n\nOverall, the pretrained models showed limited performance in the fine-grained emotion recognition task, with a weighted F1 score of about 40%. Notably, despite its underperformance in the negative emotion categorization task, HuBERT outperformed Wav2Vec 2.0 by 5.86% points on the F1 score in this more detailed task. Similarly, the original version of Whisper-small outperformed its Chinese fine-tuned counterpart, Whisper-small-Chinese-base, by 1.94% points on F1-score. These results underscore that different models may be better suited for specific types of task scenarios. Despite the overall modest performance, the Whisper Series models still performed relatively well. However, the best performing model in the series, Whisper-large-v3, only achieved an F1 score of 41.74%. This suggests that while large-scale training may offer some advantages, it does not automatically guarantee superior performance in complex tasks such as fine-grained emotion recognition.\n\nLooking at the specifics of each category of classification, there is a direct correlation between model performance and sample size. For example, in the \"Sadness\" category, which contains the largest number of samples, all models achieve an F1 score above 50%. The best performing model in this category, Whisper-large-v3, achieves an F1 score of 56.69%. However, as the sample size decreases, there is a general trend of decreasing performance for all models. It is particularly noteworthy that Whisper-large-v3 significantly outperforms other models in categories with fewer samples. In the \"Fear\" category, which has the smallest sample size of 191, Whisper-large-v3 outperforms its nearest competitor by 35.44%. Similarly, in the \"Numbness\" category, which has 509 data points, it outperforms HuBERT's F1 score by 10.57% points. These results highlight Whisper-large-v3's robust ability to perform well even in categories with smaller sample sizes, demonstrating that the model has a strong foundation and can achieve relatively good performance even with limited data.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Discussion",
      "text": "We selected prediction results from the Whisper-large-v3 model, the best performer in the fine-grained multi-label classification task, for error analysis, as detailed in Table  VI .\n\nOur analysis showed that the model often confuses closely related emotional states, such as \"Sadness\" and \"Despair\" or \"Pain\" and \"Grievance\". For example, in example (A), the category \"Despair\" is incorrectly omitted. The model also struggles with emotions that require a deeper understanding of context and nuance, as shown in example (B), where despite the caller's calm tone, emotions such as \"Confusion\" and \"Helplessness\" are present but not recognized by the model. The challenge extends to handling subtle emotions, as shown in example (C), where four complex emotions are present, but the model recognizes only one.\n\nHowever, the model's performance is not uniformly poor. For example, in case (D), the described content suggests the emotion of \"Guilt\" which the model successfully predicts even though it was not annotated as such. This highlights the need to minimize annotator subjectivity and to better define and distinguish between emotions to improve data quality.\n\nThe dataset currently suffers from category imbalance, and future efforts should focus on expanding the training dataset with more diverse examples to improve categorization accuracy. In addition, the current model's approach, which is based on speech segments, lacks contextual links. Future development should aim to construct models that incorporate contextual data, allowing for nuanced recognition and linking of emotional states.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we explore the capability of large-scale pretrained models for automatic emotion recognition by classifying emotions in speech data from the psychological support hotline of Beijing Huilongguan Hospital. We analyzed 20,630 segments from 105 callers to build negative emotion recognition models and identify 11 specific negative emotions. While these models achieved a commendable F1 score of 76.96% in binary classification tasks, their performance in multilabel emotion recognition tasks was less impressive, with the best model achieving a weighted F1 score of only 41.74%. Our findings highlight the potential of using deep learning techniques and large datasets to improve the effectiveness of mental health hotline services. Future efforts will aim to improve the model's ability to detect fine-grained emotions. This will involve increasing the number of training samples and refining the model structure to increase accuracy across different emotion categories. Since the model's performance on specific emotions is strongly influenced by the amount of training data, increasing the variety of data samples and refining the model structure will be crucial for improving overall performance. Overall, our research contributes to the growing understanding of emotion recognition in the context of psychological crisis, highlighting both achievements and areas for further development.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The distribution of speech segments durations in our",
      "page": 2
    },
    {
      "caption": "Figure 2: The data show a prevalence of",
      "page": 2
    },
    {
      "caption": "Figure 1: Label co-occurrence relationships between categories",
      "page": 3
    },
    {
      "caption": "Figure 2: Distribution of speech segment durations. Due to",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "297": "53\n44\n382\n42\n524\n171\n143\n524"
        },
        {
          "297": "174"
        },
        {
          "297": "143\n54"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Global, regional, and national burden of suicide mortality 1990 to 2016: systematic analysis for the global burden of disease study 2016",
      "authors": [
        "M Naghavi"
      ],
      "year": "2019",
      "venue": "bmj"
    },
    {
      "citation_id": "2",
      "title": "Prevalence of suicidal ideation and suicide attempts in the general population of China: a meta-analysis",
      "authors": [
        "X.-L Cao",
        "B.-L Zhong",
        "Y.-T Xiang",
        "G Ungvari",
        "K Lai",
        "H Chiu",
        "E Caine"
      ],
      "year": "2015",
      "venue": "The International Journal of Psychiatry in Medicine"
    },
    {
      "citation_id": "3",
      "title": "Suicidal behaviour among children and adolescents in China",
      "authors": [
        "R Chen",
        "J An",
        "J Ou"
      ],
      "year": "2018",
      "venue": "The Lancet Child & Adolescent Health"
    },
    {
      "citation_id": "4",
      "title": "Follow-up with callers to the national suicide prevention lifeline: Evaluation of callers' perceptions of care",
      "authors": [
        "M Gould",
        "A Lake",
        "H Galfalvy",
        "M Kleinman",
        "J Munfakh",
        "J Wright",
        "R Mckeon"
      ],
      "year": "2018",
      "venue": "Suicide and Life-Threatening Behavior"
    },
    {
      "citation_id": "5",
      "title": "Helping callers to the national suicide prevention lifeline who are at imminent risk of suicide: Evaluation of caller risk profiles and interventions implemented",
      "authors": [
        "M Gould",
        "A Lake",
        "J Munfakh",
        "H Galfalvy",
        "M Kleinman",
        "C Williams",
        "A Glass",
        "R Mckeon"
      ],
      "year": "2016",
      "venue": "Suicide and Life-Threatening Behavior"
    },
    {
      "citation_id": "6",
      "title": "Examples of speech segment predictions by the best-performing Whisper-larve-v3 model in the fine-grained multi-label classification task. The table displays the original transcribed text alongside its English translation. Sentence Labels Predictions A [Chinese] 我和他们闹，一点用都没有，我甚至连哭都是自己一个。 [Translation] I argued with them",
      "authors": [
        "Vi"
      ],
      "venue": "Examples of speech segment predictions by the best-performing Whisper-larve-v3 model in the fine-grained multi-label classification task. The table displays the original transcribed text alongside its English translation. Sentence Labels Predictions A [Chinese] 我和他们闹，一点用都没有，我甚至连哭都是自己一个。 [Translation] I argued with them"
    },
    {
      "citation_id": "7",
      "title": "I'm very helpless, forget it, stop talking about it, it's a waste of words",
      "venue": "I'm very helpless, forget it, stop talking about it, it's a waste of words"
    },
    {
      "citation_id": "8",
      "title": "Confuse 6. Helplessness None C [Chinese] 我觉得是个坏事，让我害怕、担忧、坐立不安。 [Translation] I thought it was a bad thing and made me scared",
      "venue": "Confuse 6. Helplessness None C [Chinese] 我觉得是个坏事，让我害怕、担忧、坐立不安。 [Translation] I thought it was a bad thing and made me scared"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Confuse"
      ],
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Assessing suicide risk among callers to crisis hotlines: A confirmatory factor analysis",
      "authors": [
        "T Witte",
        "M Gould",
        "J Munfakh",
        "M Kleinman",
        "T Joiner",
        "J Kalafat"
      ],
      "year": "2010",
      "venue": "Journal of clinical psychology"
    },
    {
      "citation_id": "11",
      "title": "Comparisons of characteristics between psychological support hotline callers with and without covid-19 related psychological problems in China",
      "authors": [
        "L Zhao",
        "Z Li",
        "Y Tong",
        "M Wu",
        "C Wang",
        "Y Wang",
        "N Liu"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "12",
      "title": "An evaluation of suicide prevention hotline results in taiwan: Caller profiles and the effect on emotional distress and suicide risk",
      "authors": [
        "T Shaw",
        "W.-H Chiang"
      ],
      "year": "2019",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "13",
      "title": "Characteristics and proximal outcomes of calls made to suicide crisis hotlines in California",
      "authors": [
        "R Ramchand",
        "L Jaycox",
        "P Ebener",
        "M Gilbert",
        "D Barnes-Proby",
        "P Goutam"
      ],
      "year": "2016",
      "venue": "Crisis"
    },
    {
      "citation_id": "14",
      "title": "A phonetic case study on prosodic variability in suicidal emergency calls",
      "authors": [
        "L Tavi",
        "S Werner"
      ],
      "year": "2020",
      "venue": "International Journal of Speech, Language & the Law"
    },
    {
      "citation_id": "15",
      "title": "Self-adapted utterance selection for suicidal ideation detection in lifeline conversations",
      "authors": [
        "Z.-L Wang",
        "P.-H Huang",
        "W.-Y Hsu",
        "H.-H Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter"
    },
    {
      "citation_id": "16",
      "title": "A unified approach to suicide risk detection using text and voice",
      "authors": [
        "R Iyer"
      ],
      "year": "2023",
      "venue": "A unified approach to suicide risk detection using text and voice"
    },
    {
      "citation_id": "17",
      "title": "Detecting changes in help seeker conversations on a suicide prevention helpline during the COVID-19 pandemic: in-depth analysis using encoder representations from transformers",
      "authors": [
        "S Salmi",
        "S Mérelle",
        "R Gilissen",
        "R Van Der Mei",
        "S Bhulai"
      ],
      "year": "2022",
      "venue": "BMC public health"
    },
    {
      "citation_id": "18",
      "title": "BERTopic: Neural topic modeling with a classbased TF-IDF procedure",
      "authors": [
        "M Grootendorst"
      ],
      "year": "2022",
      "venue": "BERTopic: Neural topic modeling with a classbased TF-IDF procedure",
      "arxiv": "arXiv:2203.05794"
    },
    {
      "citation_id": "19",
      "title": "Management of suicide and self-harm risk by the national mental health helpline in the state of qatar",
      "authors": [
        "M Alabdulla",
        "Y Iqbal",
        "H Mohamed",
        "D Shinith",
        "R Buenaventura",
        "K Smith",
        "M Hamideh",
        "S Ouanes"
      ],
      "year": "2023",
      "venue": "BJPsych open"
    },
    {
      "citation_id": "20",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "S Liu",
        "A Mallol-Ragolta",
        "E Parada-Cabaleiro",
        "K Qian",
        "X Jing",
        "A Kathan",
        "B Hu",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Patterns"
    },
    {
      "citation_id": "21",
      "title": "A survey of vision-language pre-trained models",
      "authors": [
        "Y Du",
        "Z Liu",
        "J Li",
        "W Zhao"
      ],
      "year": "2022",
      "venue": "A survey of vision-language pre-trained models",
      "arxiv": "arXiv:2202.10936"
    },
    {
      "citation_id": "22",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "24",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "25",
      "title": "Towards a psychological generalist ai: A survey of current applications of large language models and future prospects",
      "authors": [
        "T He",
        "G Fu",
        "Y Yu",
        "F Wang",
        "J Li",
        "Q Zhao",
        "C Song",
        "H Qi",
        "D Luo",
        "H Zou"
      ],
      "year": "2023",
      "venue": "Towards a psychological generalist ai: A survey of current applications of large language models and future prospects",
      "arxiv": "arXiv:2312.04578"
    },
    {
      "citation_id": "26",
      "title": "Wav2Vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Wav2Vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "27",
      "title": "Wav2Vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Unlocking the emotional states of high-risk suicide callers through speech analysis",
      "authors": [
        "A Nfissi",
        "W Bouachir",
        "N Bouguila",
        "B Mishara"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Semantic Computing (ICSC)"
    },
    {
      "citation_id": "31",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "33",
      "title": "CSS10: A collection of single speaker speech datasets for 10 languages",
      "authors": [
        "K Park",
        "T Mulc"
      ],
      "year": "2019",
      "venue": "CSS10: A collection of single speaker speech datasets for 10 languages"
    },
    {
      "citation_id": "34",
      "title": "ST-CMDS-20170001 1, free st chinese mandarin corpus",
      "authors": [
        "Surfing",
        "Ai"
      ],
      "year": "2017",
      "venue": "ST-CMDS-20170001 1, free st chinese mandarin corpus"
    },
    {
      "citation_id": "35",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Fleurs: Few-shot learning evaluation of universal representations of speech",
      "authors": [
        "A Conneau",
        "M Ma",
        "S Khanuja",
        "Y Zhang",
        "V Axelrod",
        "S Dalmia",
        "J Riesa",
        "C Rivera",
        "A Bapna"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    }
  ]
}