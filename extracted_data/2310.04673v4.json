{
  "paper_id": "2310.04673v4",
  "title": "Lauragpt: Listen, Attend, Understand, And Regenerate Audio With Gpt",
  "published": "2023-10-07T03:17:59Z",
  "authors": [
    "Zhihao Du",
    "Jiaming Wang",
    "Qian Chen",
    "Yunfei Chu",
    "Zhifu Gao",
    "Zerui Li",
    "Kai Hu",
    "Xiaohuan Zhou",
    "Jin Xu",
    "Ziyang Ma",
    "Wen Wang",
    "Siqi Zheng",
    "Chang Zhou",
    "Zhijie Yan",
    "Shiliang Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We finetune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-tospeech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Large language models (LLMs) are neural networks that generate natural language texts based * equal contribution. † correspondence author. on a given context. LLMs can learn from massive amounts of text data and mimic human language to acquire human knowledge. LLMs such as  GPT-4 (OpenAI, 2023) , PaLM2  (Anil et al., 2023) , LLaMA  (Touvron et al., 2023)  have demonstrated impressive capabilities across various domains, exhibiting zero-shot generalization without the need for task-specific fine-tuning. However, these models are primarily limited to processing text data.\n\nRecent research aims to seamlessly integrate text and audio since they are two important modalities for human communication. These efforts include Audio-to-Text LLMs  (Radford et al., 2022; Zhang et al., 2023b; Deshmukh et al., 2023; Arora et al., 2023; Tang et al., 2023; Chu et al., 2023) , which can convert audio input into text and perform tasks such as automatic speech recognition (ASR) and spoken language understanding (SLU); Text-to-Audio LLMs  (Yang et al., 2023a; Vyas et al., 2023; Kreuk et al., 2023; Liu et al., 2023b; Huang et al., 2023a; Wang et al., 2023a) , which can convert text input into audio and perform tasks such as text-tospeech synthesis (TTS) and text-to-music synthesis. An emerging line of research focuses on develop more universal and comprehensive Audio-and-Text LLMs  (Ao et al., 2022; Chen et al., 2021b; Zhang et al., 2023a; Wang et al., 2023b; Rubenstein et al., 2023; Huang et al., 2023b) , which can support audio-and-text tasks, that is, process and generate both audio and text and perform tasks such as speech enhancement (SE) and speech-to-speech translation (S2ST), in addition to tasks supported by audio-to-text and text-to-audio LLMs. Audioto-text and text-to-audio LLMs can be considered as subsets of audio-and-text LLMs.\n\nAudio-and-Text LLMs can be categorized into two directions. One direction builds a collaborative AI system using LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to support various audio-andtext tasks  (Shen et al., 2023; Huang et al., 2023b) .\n\nThese methods have serious drawbacks, including high complexity, significant resource consumption, and unavoidable error accumulation problems. The other direction develops a unified Audio-and-Text LLM leveraging LLMs as the backbone to support audio-and-text tasks  (Ao et al., 2022; Chen et al., 2021b; Wang et al., 2023b; Rubenstein et al., 2023) . Decoder-only audio-and-text LLMs  (Zhang et al., 2023a; Wang et al., 2023b; Rubenstein et al., 2023)  are the dominant technique under this category. These models convert continuous audio into discrete tokens and integrate text and audio tokens into unified vocabulary. These models suffer from information loss from quantization of speech signals into discrete tokens, which leads to notable performance degradation on ASR compared to models using continuous speech features  (Chen et al., 2023a; Chang et al., 2023; Yang et al., 2023c; Puvvada et al., 2023) . In this paper, we focus on improving the second category of unified Audio-and-Text LLMs. Moreover, recent advances in audio generation from unified audio-and-text LLMs  (Wang et al., 2023a,b)  discretize speech into codec codes, then use an autoregressive language model (LM) to predict output tokens from the first quantizer and use a non-autoregressive model to predict tokens from the other quantizers individually. One limitation of this mechanism is that it needs many prediction steps (hence called multi-step audio synthesis scheme) to generate good quality speech. Another limitation is that predicting the indices of the other codec groups is challenging due to the multi-modal distribution nature of codec tokens  (Jenrungrot et al., 2023) .\n\nTo overcome the drawbacks of existing unified audio-and-text LLMs, we propose LauraGPT, a novel unified Audio-and-Text LLM based on the GPT framework for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities, with a single model. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. This data representation improves the performance of audio-input tasks and also facilitates joint autoregressive modeling of audio and text features for audio generation tasks.\n\nWe also propose a one-step codec vocoder in LauraGPT to address the two limitations of the popular multi-step audio synthesis scheme. Our one-step codec vocoder uses a transformer-based predictor to estimate the sum of all codec token groups instead of the individual indices, by minimizing the reconstruction losses. Our approach simplifies the audio generation process to a single feed-forward calculation and also overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.\n\nWe fine-tune LauraGPT using supervised multitask learning on diverse audio tasks, including tasks focusing on content, semantics, paralinguistics, and audio-signal analysis, such as ASR, speech-to-text translation (S2TT), TTS, SE, automated audio captioning (AAC), speech emotion recognition (SER), and SLU. Comprehensive experiments show that, to the best of our knowledge, LauraGPT 1  consistently achieves comparable to superior performance compared to strong baselines on the largest and the most diverse set of audio recognition, understanding, and generation tasks among existing decoderonly unified audio-and-text LLMs focusing on these tasks  (Zhang et al., 2023a; Wang et al., 2023b; Rubenstein et al., 2023) . The results are remarkable since existing general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Audio-to-Text LLMs Audio-to-Text LLMs can generate text from audio inputs. Whisper  (Radford et al., 2022)  and USM  (Zhang et al., 2023b)  can perform speech recognition and translation across multiple languages and domains. Pengi  (Deshmukh et al., 2023)  is an audio LM that formulates audio tasks as text-generation tasks. UniverSLU  (Arora et al., 2023)  is a universal SLU model that supports various speech classification and sequence generation tasks. SALMONN  (Tang et al., 2023)  and Qwen-Audio  (Chu et al., 2023)  integrate pretrained text LLMs with separate speech and audio encoders into a single multimodal model. Text-to-Audio LLMs Text-to-Audio LLMs can convert text input into audio output and perform tasks such as TTS or text-to-music synthesis. Recently, two prominent categories of approaches have emerged for generating audio from text prompts. In the first category, continuous representations such as utterance-level embeddings  (Elizalde et al., 2022; Liu et al., 2023a; Huang et al., 2023a)  and Mel-frequency spectrograms  (Nachmani et al., 2023)  are used as the targets. However, continuous representations present a challenge for unified modeling of text and audio within a single LM. In the second category, discrete codec tokens are employed as audio representations and generated by diffusion models  (Yang et al., 2023b)  or autoregressive LMs  (Kreuk et al., 2023; Borsos et al., 2023; Copet et al., 2023; Wang et al., 2023a) . Among models in the second category, in models such as AudioGen  (Kreuk et al., 2023) , Au-dioLM  (Borsos et al., 2023) , and MusicGen  (Copet et al., 2023) , multiple output heads are used after the LM to predict synchronized or delayed groups of codec tokens. However, this mechanism is only suitable for audio generation and may not be applicable to diverse audio-and-text tasks. Alternatively, in VALL-E  (Wang et al., 2023a) , the LM predicts output tokens of the first quantizer, while tokens of the remaining quantizers are predicted by a non-autoregressive model one by one. This mechanism requires numerous prediction procedures to generate acceptable speech quality. Moreover, the indices of the remaining codec groups are challenging to predict due to the multi-modal distribution nature of codec tokens  (Jenrungrot et al., 2023) . Audio-and-Text LLMs Audio-and-Text LLMs can process and generate both audio and text, which can be categorized into two directions. One direction uses LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to enable direct audio interaction with LLMs and support various audio-and-text tasks, such as  HuggingGPT (Shen et al., 2023)  and Audio-GPT  (Huang et al., 2023b) . However, these models are complex, resource-intensive, and prone to error accumulation. The second direction uses LLMs as the backbone for a unified model that handles audio-and-text tasks  (Ao et al., 2022; Chen et al., 2021b; Wang et al., 2023b; Rubenstein et al., 2023) . SpeechT5  (Ao et al., 2022)  and SpeechNet  (Chen et al., 2021b)  perform various speech tasks with an encoder-decoder model, but they require modalspecific pre-nets and post-nets to deal with different input&output modalities. VioLA  (Wang et al., 2023b) , AudioPaLM  (Rubenstein et al., 2023) , SpeechGPT  (Zhang et al., 2023a) , and Speech-Gen  (Wu et al., 2023)  use decoder-only Transform-ers to model discrete audio tokens and text tokens as a shared vocabulary, but they suffer from information loss from quantization of audio signals into discrete tokens  (Chen et al., 2023a; Chang et al., 2023; Yang et al., 2023c; Puvvada et al., 2023) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "Figure  1  depicts the architecture of the proposed LauraGPT. Section 3.1 describes the audio encoder, the text tokenizer, and the modified GPT LM for unified audio-and-text modeling. Section 3.2 elaborates the audio tokenizer. Section 3.3 introduces an efficient one-step codec vocoder for converting audio tokens into high-quality raw waveforms. Section 3.4 describes the multi-task fine-tuning and shows that LauraGPT provides an extensible framework for supporting more complex tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modified Language Model For Unifying",
      "text": "Audio-and-Text Modeling\n\nFor audio inputs, different from other audio-andtext LLMs using discrete tokens to represent audio inputs, we extract the log-compressed Mel spectrogram features and convert them into continuous representations using a Conformer-based audio encoder. Text inputs and outputs are tokenized using the Qwen tokenizer  (Bai et al., 2023) , which inherits the tiktoken tokenizer  (Jain, 2022 ) and incorporates additional augmentations for commonly used characters and words in different languages. The tokenized input text undergoes embedding matrix transformation to generate dense vectors. The audio representations and text embeddings have the same dimension D. The Conformer-based encoder is initialized with weights from a pre-trained ASR model  (Gao et al., 2023) . Since batch normalization can lead to endless loop decoding, we replace it with layer normalization in the Conformer-based encoder (details are in Appendix C.2).\n\nTo achieve audio generation capabilities, the audio outputs are discretized into tokens using an audio tokenizer (Section 3.2) to obtain discrete representations and the softmax output layer is augmented with the audio tokens. As a result, the weight matrix W in the output layer is of size (N + M + L) × D and is utilized to calculate the logits for audio and text tokens at each position, where N , M , and L denote the vocabulary sizes of text, audio, and task tokens, respectively. Task tokens are used to inform the model which task should be performed. Note that in order to control the sequence length, we perform the low frame  ⃝ and E ⃝ denote the \"start of sequence\" and \"end of sequence\" tokens. We omit the text tokenizer and detokenizer for simplicity. rate (LFR) method  (Gao et al., 2020)  to downsample audio inputs to 60ms and only select the first codec group of the audio outputs.\n\nBased on the aforementioned representations, the GPT backbone is trained to model various audio and text tasks by minimizing the cross-entropy loss:\n\n(1) where u denotes the input embeddings with a sequence length T u and v represents the sequence of target tokens with a length T v . To specify a task, a special task-related token u task is inserted between the input embeddings and output tokens. Note that only the losses of outputs are taken into account, while losses on inputs and task token embeddings are masked out. After the final output layer, audio tokens are decoded to raw waveforms using a codec vocoder (Section 3.3). Since it is challenging to train an LLM from scratch with limited data and computational resources, we use the open-source GPT LLM, Qwen  (Bai et al., 2023) , as the backbone. Qwen is pre-trained on a diverse corpus covering various domains in English and Chinese and supports 8192 context length. Compared with other open-source GPT models with similar model sizes, Qwen models demonstrate impressive competitiveness, achieving better performance on widely used benchmarks, especially on Chinese tasks  (Bai et al., 2023) . Within LauraGPT, all parameters including the Qwen backbone are jointly optimized, except for the codec vocoder, which is trained independently and kept frozen during both training and inference stages of LauraGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Tokenizer",
      "text": "For audio generation, we utilize a codec model as the audio tokenizer to extract discrete representations. Our codec model shares a similar architecture as EnCodec  (Défossez et al., 2022) , which comprises convolutional recurrent encoder and decoder  (Tagliasacchi et al., 2020)  and a residual vector quantizer (RVQ)  (Vasuki and Vanathi, 2006) . We enhance the original EnCodec model with the following modifications: 1) Add reconstruction losses in the magnitude spectrum domain to improve the quality of middle-and high-frequency signals. 2) Stack five strided convolution blocks with strides of  [8, 5, 4, 2, 2]  to address the challenge of long sequence lengths, resulting in a token rate of 25Hz for each token group. 3) Use 32 quantizers with structured dropout in the RVQ module, each with vocabulary size 1024. This revision improves speech quality with more quantizers while preserving most information in the shallow quantizers. The encoder and the first RVQ quantizer are used as the audio tokenizer, and the outputs of the first quantizer are used as the audio tokens. The choice of the first N RVQ quantizers to use is a tradeoff between performance and sequence length (hence efficiency). The remaining quantizers and the decoder are only used when training the codec model. Details of training and the pre-trained codec model are in  (Du et al., 2023) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "One-Step Codec Vocoder For Audio Generation",
      "text": "We propose a one-step codec vocoder in LauraGPT to generate waveforms from the audio tokens, which are extracted from the first quantizer as de-scribed in Section 3.2. Our vocoder comprises two components: a transformer-based predictor and a codec decoder. The predictor is trained to estimate the summation of codec embeddings from the 32 RVQ quantizers by minimizing the L1 and L2 distances between the predicted embeddings Ê and their corresponding ground truth E:\n\nwhere T denotes the total number of frames and D c denotes the dimension of the codec embeddings.\n\nAfter obtaining the estimated embeddings, the decoder of an pre-trained codec model is utilized to reconstruct the raw audio waveforms.\n\nAlongside the predicted audio tokens from the LLM, text and audio inputs are used as conditions and fed to the predictor. For zero-shot TTS task, the text inputs serve as a condition as well as the prompt audio features. For SE task, the input noisy speech features are employed as conditions. Such text and audio conditionings allow the model to generate high-quality audio signals by leveraging the diverse information in prompt audios and noisy speeches, which is lacked in the discrete tokens (output from the first quantizer). Therefore, different from existing Text-to-Audio LLMs, our approach simplifies the audio generation process to a single feed-forward calculation and overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Task Finetuning",
      "text": "Basic Tasks We unify modeling of the following basic tasks in the single LauraGPT model and use these tasks for multi-task fine-tuning: Automatic Speech Recognition (ASR), Spoken Language Understanding (SLU), Speech-to-Text Translation (S2TT), Speech Emotion Recognition (SER), Automated Audio Captioning (AAC), Speech Enhancement (SE), and Text-to-speech Synthesis (TTS). Task definitions are in Appendix A.1.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unified Task Expression",
      "text": "LauraGPT operates based on a unified task expression: [input embeddings, task ID, output tokens]. With the same inputs, the desired outputs can differ across tasks. For instance, ASR and S2TT tasks require different outputs even for the same audio input. Task tokens are included in both input embedding and output weight matrices. The TTS task takes text embeddings as inputs, while the ASR, S2TT, SLU, SE, ACC, and SER tasks take audio encodings as inputs. The TTS and SE tasks use audio tokens as the target outputs, while the remaining tasks use text tokens as the target outputs. Support More Complex Tasks With its modularized design, LauraGPT provides an extensible framework to support complex tasks. By breaking a task into sub-tasks among the basic tasks and cascading the raw inputs and model outputs of subtasks, LauraGPT can perform more complex tasks. For example, we demonstrate that LauraGPT is capable of performing the advanced speech-to-speech translation (S2ST) task by combining the S2TT and TTS tasks. Initially, a sequence is constructed to translate the speech content into the target language text using the S2TT task token: [audio encoding, <S2TT>]. Subsequently, the translated text is combined with the TTS task token to synthesize speech: [text embedding, <TTS>]. If maintaining the speaker identity is desired, the original inputs and content can be incorporated to perform personalized TTS. This can be achieved with an input sequence as [ASR recognized text embedding, S2TT translated text embedding, <TTS>, audio token of input speech], where ASR recognized text embedding is obtained using the ASR task: [audio encoding, <ASR>]. This approach treats the bilingual text as the complete input and allows the model to generate an output sequence of codec tokens while maintaining the same speaker identity. Audio samples of S2ST can be found on the demo site. More examples of complex tasks are in Appendix D.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Settings",
      "text": "Model Architecture The Conformer-based audio encoder consists of 32 conformer blocks. Each block consists of a feed-forward module with 1536 units, an attention module with 16 heads and a dimension of 512, a convolutional module including the pointwise and depthwise convolution layers, and a second feed-forward module with 1536 units. Sinusoidal positional encoding is applied on the audio inputs. For a trade-off between performance and training efficiency, we use Qwen-1.8B 2  as the backbone and LauraGPT has 2B parameters. Qwen-1.8B comprises 24 transformer layers with a hidden size 2048 and 16 attention heads. Although Conformer and Qwen-1.8B are selected as the audio encoder and GPT backbone, they can be replaced by other encoders and GPT models.\n\nTraining Setup In all experiments, we initialize the Qwen backbone and audio encoder with the pretrained checkpoints. We then optimize the model parameters through multi-task fine-tuning. The training&test datasets and evaluation metrics are presented in Appendix A.2 and A.3. Appendix A.4 describes the three-stage training process to address the significant variation in data volume across different tasks, and details the inference process.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results And Analysis",
      "text": "Section 5.1 presents the main results of performance comparison on the basic tasks from the stateof-the-art (SOTA) model, a comparable baseline, and our LauraGPT. Ablation studies in Section 5.2 demonstrate the advantages of using continuous representations for audio inputs in LauraGPT by comparing to a counterpart with both discrete inputs and outputs (denoted Discrete IO), the superiority of our one-step codec vocoder, and effectiveness of multi-task finetuning. Further analyses include comparison with related unified Audioand-Text LLMs (Appendix B), more analysis of multi-task fine-tuning on SER task (Appendix C.1), comparing batch normalization with layer normalization in the audio encoder (Appendix C.2), and studying impact of initialization from pre-trained models (Appendix C.3).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results On All Tasks",
      "text": "Table  1  shows the results from the SOTA model, a comparable baseline, and our LauraGPT 3  , in that order, on a variety of speech recognition, understanding, and generation benchmarks. The SOTA model yields the best results on each test set based on our literature review. The baseline for each task is chosen to facilitate fair comparison with LauraGPT: they are comparable to LauraGPT in model architecture or training data and are also common competitive baselines in the literature. We cite the SOTA results to validate that LauraGPT consistently performs competitively on all the speech recognition, understanding, and generation tasks and the baselines are competitive. However, LauraGPT results cannot be fairly compared to the SOTA results. Specifically, QwenAudio achieves SOTA performance on most speech understanding tasks, but compared to LauraGPT, QwenAudio uses a much larger LLM (∼7B VS. our 1.8B LLM), and uses the Whisper audio encoder trained on a large amount of ASR data while we use a Conformer encoder trained on much less data. Moreover, QwenAudio does not support speech generative tasks hence cannot handle SE and TTS tasks. Paraformer-large and UniverSLU achieve SOTA results on AISHELL-2 test-ios for Chinese ASR and on SLURP test for SLU; however, they only support single tasks and also train on more data than LauraGPT on the corresponding task. Appendix B shows that LauraGPT greatly outperforms Whisper Large V2 on Chinese ASR test sets while the gap on English ASR test sets are primarily attributed to the much smaller English data used for training LauraGPT. For TTS, the SOTA VALL-E Phone outperforms baseline VALL-E Token 4  , suggesting the importance of text representation for TTS. Compared to both VALL-E models, LauraGPT achieves comparable speaker similarity (SECS) and speech quality (MOSNet). The degradation in content consistency (WER) from LauraGPT results from the generalization issue, since the training data is too limited for LauraGPT with 2B parameters. Overall, the results show that LauraGPT consistently achieves comparable to superior performance than strong baselines on diverse speech tasks, demonstrating the general effectiveness of LauraGPT on speech recognition, understanding, and generative tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Analysis",
      "text": "Discrete VS. Continuous Representations for Audio Inputs Existing unified Audio-and-Text LLMs use discrete tokens to represent audio inputs. We analyze the efficacy of using continuous representations for audio inputs in LauraGPT by comparing to its counterpart Discrete IO on ASR, S2TT, and SE tasks, representing audio-input recognition and understanding, and audio generation capacities. In Discrete IO, both audio inputs and outputs are represented by flattened codec tokens from the first four quantizers 5  , resulting in a token rate of 100Hz. In LauraGPT, audio inputs are represented by continuous acoustic features, which Qwen-Audio  (Chu et al., 2023)  1.3 MMSpeech-large  (Zhou et al., 2022)  1.9 LauraGPT 1.8\n\nAISHELL-2 test-ios CER ↓\n\nParaformer-large  (Gao et al., 2023)  2.9 MMSpeech-large  (Zhou et al., 2022)  3.9 LauraGPT 3.2\n\nLibriSpeech test-clean WER ↓\n\nQwen-Audio  (Chu et al., 2023)  2.0 Whisper Large V2  (Radford et al., 2023)  2 are also fed into our one-step vocoder as a condition to achieve high-quality outputs. Table  2  shows that LauraGPT consistently outperforms Discrete IO with remarkable gains on all tasks. For ASR task, the performance degrades drastically when replacing continuous features with discrete audio tokens. Although the performance degradation can be reduced by using more quantizers (more codec groups), e.g. 32  (Puvvada et al., 2023) , more codec groups always cause higher token rates and longer sequence and in turn higher computational demands. For S2TT task, Discrete IO only yields BLEU scores of 5.1 and 5.0 on test sets, basically suggesting lack of translation capability. For SE task, using codec tokens as inputs cannot improve the quality and intelligibility of noisy speeches, suggesting lack of enhancement capability, probably because the distribution of noisy speech is too complicated to be accurately represented by four groups of discrete audio tokens.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Comparison On Audio Synthesis Schemes",
      "text": "VALL-E  (Wang et al., 2023a ) introduces a commonly used scheme formulating audio synthesis as a classification problem: A neural network is shared to predict the codec tokens in the following group with the previous ones as inputs and synthesizing target audio requires multiple steps or iterations to achieve a reasonable speech qual- ity. In contrast, our one-step codec vocoder formulates audio synthesis as a regression problem.\n\nAs described in Section 3.3, our one-step codec vocoder simplifies audio synthesis into a single feed-forward calculation and overcomes the prediction challenge caused by the multimodal distribution of codec tokens. Table  3  shows that our one-step codec vocoder greatly outperforms the multi-step scheme in terms of content consistency (CER, WER) and speech quality (PESQ), while obtaining the same intelligibility (STOI).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Effectiveness Of Multi-Task Finetuning",
      "text": "The multi-task fine-tuned LauraGPT (Section 3.4) could be advantageous over individual single-task models: (1) Multi-task learning could exploit synergy between tasks and reduce over-fitting, in turn yield high performance on diverse tasks and achieve better performance than single-task training.\n\n(2) Multi-task learning could learn a single model capable of supporting a wide range of tasks, hence practical deployment is greatly simplified through unified model implementation and API. We investigate whether the multi-task trained LauraGPT could achieve better performance than single-task training for tasks with limited training data. Among the basic tasks (Table  5 ), AAC, SLU, and SER tasks all have limited training data. We initialize the Qwen backbone and the audio encoder the same as LauraGPT before conducting multitask training, then train the single-task model only using the task-specific training data. The results are shown in Table  4 .\n\nFor the AAC task, we find that the multitask trained LauraGPT outperforms the single-task model on SPICE, CIDEr and SPIDEr on the Clotho evaluation set. For the SLU task, on the SLURP test set, LauraGPT greatly outperforms the singletask model on intent accuracy by +2.9 absolute and on SLU-F1 by +22.5 absolute. For the SER task, on the MELD test set, LauraGPT substantially outperforms the single-task model in terms of UA and the primary WF1 metrics, while the WA result is slightly worse. More analyses in Appendix C.1 show that multi-task learning dramatically improves accuracies of the minority classes. In summary, these results verify that multi-task learning for LauraGPT consistently achieves better performance than single-task training for tasks with limited training data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "We propose LauraGPT that can handle both audio and text inputs and outputs and perform audio recognition, understanding, and generation. We propose combining continuous and discrete features for audio and a one-step codec vocoder, and employ multi-task learning. Experiments demonstrate that LauraGPT achieves comparable to superior performance compared to strong baselines on a wide range of speech tasks on content, semantics, paralinguistics, and audio-signal analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Limitations",
      "text": "In this work, in order to support a wide range of audio recognition, understanding, and generation tasks, we choose to train all parameters in LauraGPT during supervised multi-task finetuning, including the Qwen backbone, except for the codec vocoder. This strategy results in substantial computations for training. In future work, we plan to investigate parameter-efficient fine-tuning to reduce computation demands. Also, due to the limited computation resources, our comparisons between the multi-task trained LauraGPT and single-task models are focused on the low-resource tasks, that is, AAC, SLU, and SER tasks. We find that multitask learning for LauraGPT consistently achieves better performance than single-task training for tasks with limited training data. Next, we plan to complete comparisons of LauraGPT and singletask models on all tasks, including relatively richresource tasks such as ASR. These studies will promote understandings on where tasks could benefit from each other, including tasks with even conflicting objectives. We also plan to conduct deeper analysis on the potential risk of catastrophic forgetting of the original text capabilities of the pre-trained text LLM, due to multi-task learning of speech tasks. Note that exploration of parameter-efficient fine-tuning may also help preserve the original text capabilities of the pre-trained text LLMs.\n\nLauraGPT relies on discrete audio tokens for speech generative tasks. Our research shows that the performance of this paradigm strongly depends on the quality of the audio tokenizer. We plan to systematically analyze the impact of various audio tokenizers on diverse audio generative tasks. We plan to develop new audio tokenizers that are more suitable for unified Auio-and-Text LLMs and provide desirable representations for generative tasks.\n\nThere are great emerging interests in fundamental speech models that are similar to those in the field of NLP. This is a tremendously valuable research direction. Our work achieves important milestone for this research question, as we explore and provide promising answers to the following question: How to design more efficient and scal-able unified GPT-style Audio-and-Text LLMs than existing approaches that can leverage large-scale labeled data and achieve highly competitive performance on a diverse set of speech tasks, including speech recognition, understanding and generation, using a single model? Note that previous general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.\n\nInspired by the recent advances of LLMs in NLP, we envision that the fundamental speech models should have the following capabilities:\n\n• In-context learning ability like GPT-3, which can learn from few-shot examples and adapt to new tasks, such as predicting the age of the speaker from a speech sample.\n\n• Instruction-following ability like InstructGPT and ChatGPT, which can perform the appropriate speech-related task given a natural language instruction, such as synthesizing a speech with a specific emotion or style.\n\n• General audio modeling abilities, i.e., speech, non-speech audio, and music, such as music generation.\n\nOur work demonstrates that the current LauraGPT has made solid progress and reached one important milestone toward a speech foundation model. From LauraGPT to the next-generation speech foundation model we envision, most remaining efforts are in more task data collection and more self-supervised and/or supervised pre-training and supervised fine-tuning. There is no need to modify the model architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Appendices A Experimental Details",
      "text": "A.1 Basic Tasks\n\nThe following tasks are used in supervised multitask learning of LauraGPT and also in evaluations: Automatic speech recognition (ASR) is a vital task in the speech processing community. It focuses on transcribing speech into textual content. Spoken language understanding (SLU) is a task of directly deriving high-level semantic meaning from audio input. It aims to identify the user's intent and the relevant entity slots that fill the intent. An intent is usually composed of a scenario type and an action type, while slots and fillers are keyvalue pairs that specify the details of the intent. Speech-to-text translation (S2TT) is similar to machine translation, but it directly translates the source language speech into the target language text. Speech emotion recognition (SER) categorizes the emotions in speech input. Compared to textual emotion recognition, speech signals convey additional information, including tone and speaking rate, which enhances emotion recognition. Automated audio captioning (AAC) aims to generate a natural language sentence that describes the content of an audio clip. Speech enhancement (SE) is an audio-to-audio task that aims to improve speech quality through noise suppression and dereverberation. In order to incorporate this task into a unified modeling framework, we reformulate the task as a classification problem using codec tokens. Text-to-speech synthesis (TTS) can be considered as the inverse process of ASR, where it generates speech that matches the given text.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A.2 Training Datasets",
      "text": "To ensure reproducibility, all training data and test data for LauraGPT are publicly available datasets, with licenses of Apache 2.0, CC BY 4.0, CC0, noncommercial research and education use, etc. The training data for the basic tasks listed in Section 3.4 and defined in Appendix A.1 are prepared as follows.\n\nFor the ASR task, we utilize open-source Chinese datasets such as AISHELL-1  (Bu et al., 2017) , AISHELL-2  (Du et al., 2018) , Wenet-Speech  (Zhang et al., 2022) , as well as open-source English datasets including LibriSpeech  (Panayotov et al., 2015)  and GigaSpeech  (Chen et al., 2021a) .\n\nFor the S2TT task, we employ the commonly used BSTC  (Zhang et al., 2021)  and CoVOST 2  (Wang et al., 2020)  datasets. Due to the limited data volumes of BSTC and CoVoST 2, we further augment the training set by translating AISHELL-1 and AISHELL-2 datasets into English and translating LibriSpeech dataset into Chinese using a publicly available text translation model  (Wei et al., 2022) . Consequently, we obtain approximately 2,000 hours of supplementary data for Chinese-to-English and English-to-Chinese S2TT tasks. As a supplement of training data for S2TT, we also add the ParaCrawl v9 dataset  (Kocmi et al., 2022) , which consists of 14M parallel text sentences for Zh→En (Chinese-to-English) and En→Zh (English-to-Chinese) translations.\n\nFor the SER task, we collect corpora including MELD  (Poria et al., 2018) , IEMOCAP  (Busso et al., 2008) , RAVDESS (Livingstone and Russo, 2018), TESS (Pichora-Fuller and Dupuis, 2020), Crema-D  (Cao et al., 2014) , Emov-DB  (Adigwe et al., 2018) , and SAVEE  (Jackson and Haq, 2014) . These corpora are recorded in multi-modal formats, comprising audio or visual data. No other corpora are used for the SER task.\n\nFor the SLU task, we use the multi-domain Spoken Language Understanding Resource Package (SLURP) dataset  (Bastianelli et al., 2020) , which covers 18 scenarios.\n\nFor the AAC task, we use AudioCaps  (Kim et al., 2019) , WavCaps  (Mei et al., 2023) , and Clotho  (Drossos et al., 2020)  datasets.\n\nFor the SE task, pairs of noisy and clean speech are required for training. The clean utterances are extracted from the AISHELL-1, AISHELL-2, Lib-riSpeech, and WSJ datasets  (Paul and Baker, 1992) , while the noisy counterparts are generated by mixing the clean speech with noises from the FSD-50K dataset  (Fonseca et al., 2022)  at random signal-tonoise rates (SNR) ranging from 2 to 15. Besides the additional noises, we also simulate convolutional noises by convolving the clean speech data with room impulse responses  (Ko et al., 2017) . As a result, we obtain approximately 6000 hours of paired data for the SE task.\n\nFor the TTS task, we use the open-source Lib-riTTS and 3D-speaker datasets  (Zheng et al., 2023) . Further details of the training data for all tasks can be found in Table  5 .\n\nNote that for all the training and test datasets, our use of the data is consistent with their intended use. We use all data sets in the same ways as prior research works, hence we did not check whether the data that was used contains any information that names or uniquely identifies individual people or offensive content.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "A.3 Evaluation Datasets And Metrics",
      "text": "Table  6  presents the evaluation datasets and evaluation metrics for various tasks. The metrics used in our experiments are described below:\n\n• CER stands for Character Error Rate, a commonly used metric to evaluate the recognition performance of Chinese and English utterances.\n\nWe also utilize CER to assess the content consistency in TTS task. • WER stands for Word Error Rate, which considers entire words rather than individual characters.\n\nIn our experiments, we use WER to evaluate ASR recognition performance, content consistency in TTS, and speech intelligibility in SE. • SECS, which stands for Speaker Encoder Cosine Similarity, utilizes speaker embeddings extracted from a pre-trained speaker verification model 6  for both prompt and synthesized speech. The cosine similarity between the two embeddings is then employed to measure the speaker similarity between the prompt speech and the synthesized speech. Furthermore, the naturalness of the synthesized speech is evaluated using MOSNet, a non-intrusive score derived from a pre-trained neural network 7  . WA corresponds to the overall accuracy, UA corresponds to the average class-wise accuracy, and WF1 corresponds to the average class-wise F1 score. • ACC measures the accuracy of predicting the intent. SLU-F1 is a metric that balances Word-F1 and Char-F1, computed as the sum of the confusion matrices.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A.4 Details Of Training And Inference",
      "text": "In all experiments, we optimize the model parameters through the following steps: (1) We initialize the Qwen backbone and the audio encoder with the pre-trained checkpoints.\n\n(2) We then perform multi-task finetuning.\n\nDue to the significant variation in data volume across different tasks, the training process is conducted in three stages. In the first training stage, the model is fine-tuned on all tasks using the complete training data as shown in Table  5 . The AdamW optimizer is utilized with a peak learning rate of 5 × 10 -4 and 10K warmup steps. In the second stage, we further fine-tune the model on tasks that have small-scale datasets, including TTS, SE, AAC, SER, and SLU tasks. The AdamW optimizer is utilized with a peak learning rate of 2 × 10 -4 and 10K warmup steps. In the third training stage, we fine-tune the model on all tasks using the complete training data again. The peak learning rate of the AdamW optimizer for the third stage is reduced by half as 1 × 10 -4 , while the warmup step remains at 10K.\n\nFor the codec vocoder, we train the predictor on the training data of the TTS and SE tasks. We use the Adam optimizer with a peak learning rate of 0.001 and 25K warmup steps. The decoder of the codec vocoder is initialized with the pre-trained checkpoints 8  and kept frozen during the multi-task finetuning of LauraGPT.\n\nAs stated in Section 3, during the training stage, the input is converted into input embeddings by the audio encoder if the input is audio, or converted by the embedding matrix W if the input is text, while the output is converted into output embeddings by the same embedding matrix W for teacher-forcing. Meanwhile, this matrix W is also used to convert  the task-ID token into an embedding. Then, these embeddings are composed into an embedding sequence as [input embeddings, task-ID embedding, output embeddings], which is taken as the input of Qwen LLM. To train the model, a masked crossentropy loss function is applied, as shown in Eq. 1.\n\nAs described in Section 3, in addition to masking out the losses on inputs, the cross-entropy loss at the position of the task token is also masked out.\n\nDuring the inference stage, the input is converted into input embeddings as done during the training stage. Then the corresponding task-ID embedding is added at the end of the input embedding sequence. Next, the Qwen LLM generates output tokens in an autoregressive manner until the \"end of sequence\" token is generated. Finally, for textformat output, the Qwen tokenizer is employed to convert tokens into final output, while for audio-format output, the codec vocoder is employed to convert tokens into raw waveforms.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "A.5 Details Of The Ser Evaluation",
      "text": "During the training stage, emotion labels within different training corpora are unified into the following nine classes: anger, disgust, neutral, like, sadness, surprise, happiness, joy, and fear. At the test stage, we map the \"like\" and \"happiness\" emotion classes into the \"joy\" class to match the MELD test set. LauraGPT uses an autoregressive structure to generate emotion labels. Out-of-domain outputs are considered as classification errors, making the task harder. Both WavLM Base model and WavLM Large model utilize the weighted sum of multiple layers with learnable parameters as speech features, which are fed into a downstream network for classification.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "B Comparison With Related Unified",
      "text": "Audio-and-Text Models SpeechT5  (Ao et al., 2022)  is evaluated on ASR, TTS, S2TT, voice conversion (VC), SE, and speaker identification (SID). Since the training data of tasks other than ASR for SpeechT5 differs remarkably from those for LauraGPT, we compare LauraGPT against SpeechT5 only on ASR. For SpeechT5, the model is first pre-trained with large-scale unlabeled speech and text data. Then, it is finetuned on the Librispeech-960 corpus via the hybrid cross-entropy and CTC loss. As claimed in their paper, SpeechT5 achieves a WER of 7.3% on the Librispeech test-other subset without CTC and LM. Under a fair comparison, our LauraGPT achieves a comparable WER of 7.7%. Note that different from SpeechT5, LauraGPT is directly trained on multi-task labeled datasets without benefiting from any self-supervised pretraining.\n\nVioLA  (Wang et al., 2023b ) is evaluated on ASR, S2TT, TTS and S2ST tasks. Considering the substantial differences in training data on tasks between VioLA and LauraGPT and lack of open-sourced VioLA codebase and models, it is difficult to fairly compare LauraGPT with VioLA. Among the tasks, direct comparison on ASR is also challenging since VioLA only conducts speech-to-phoneme recognition and reports Phoneme Error Rate (PER) rather than recognizing words/characters and reporting WER/CER as conducted by LauraGPT. According to their paper, Vi-oLA underperforms their in-house Attention-based Encoder-Decoder (AED) model (which we also have no access to) with relative 19.96% phoneme error rate (PER) degradation from 9.47% to 11.36% on Mandarin WenetSpeech dev set. Since higher PER always corresponds to much higher WER as a word comprises multiple phonemes, it would be safe to hypothesize that the relative degradation on WER from VioLA over AED is even greater. In contrast, compared with the Paraformer baseline, our LauraGPT achieves comparable CER on the Mandarin AISHELL-2 test-ios set and outperforms it on the English Librispeech test-other set, i.e., overall LauraGPT performs comparably to Paraformer. Note that Paraformer is a nonautoregressive AED model performing comparably to conventional auto-regressive AED model  (Gao et al., 2022) . Therefore, through this chain of comparisons, we are confident to conclude that LauraGPT notably outperforms VioLA on ASR task.\n\nAudioPaLM  (Rubenstein et al., 2023 ) is evaluated on ASR, S2TT and TTS tasks. Since the training and evaluation datasets for AudioPaLM and LauraGPT are disjoint, their performance results cannot be directly compared. In addition, the pretrained model of AudioPaLM has not been released. Therefore, empirically comparing LauraGPT to Au-dioPaLM will require great effort and is not conducted in this work.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C More Analyses Of Critical Design Choices",
      "text": "C.1 Effectiveness of Multi-task Finetuning on the SER task Table  4  shows that for the SER task, on the MELD test set, the multi-task trained LauraGPT substantially outperforms the single-task model in terms of UA and WF1 metrics, while the WA result is slightly worse.\n\nTo further analyze the results of the SER task, we conduct a statistical analysis of the number of samples for each emotion class in both training and test sets of the MELD dataset, as well as their corresponding test accuracy. The results are shown in Table  10 . Compared to the single-task model, the multi-task trained LauraGPT results in degradation in accuracy for classes with a larger number of    (Chen et al., 2023b) . That is, on the primary metric WF1, the multi-task trained LauraGPT greatly outperforms the single-task model. Furthermore, the accuracy of the disgust and fear classes from the single-task model is 0, which aligns with the fact that these two classes have the fewest training sam-ples in the MELD dataset. Multi-task training not only remarkably improves the performance of emotion classes with low accuracy (joy, sadness, surprise), but also greatly improves the performance of classes that cannot be predicted with single-task training (disgust, fear).",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "C.2 Batch Normalization Versus Layer Normalization In Audio Encoder",
      "text": "In the original design, batch normalization is applied after the convolution module in the Conformer-based audio encoder. However, we discover that this choice leads to endless looping decoding due to inaccurate estimations of mean and variance, particularly for tasks with long sequence lengths. When the issue of endless looping decoding occurs, the model generates several fixed tokens repeatedly and cannot stop the generation until achieving a pre-defined maximum length. To address this issue, we replace batch normalization with layer normalization, which is more robust to various mini-batch sizes. We validate this design",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "C.3 Impact Of Initialization From Pre-Trained Models",
      "text": "In LauraGPT, both the GPT backbone and audio encoder are initialized with the weights of pre-trained checkpoints. We investigate how the initialization affects the performance of LauraGPT. The experimental results for the ASR, S2TT and SE tasks are presented in Table  11 . From the results, we observe that the initialization has a significant impact on the performance of ASR and S2TT tasks, while its influence on the SE task is relatively limited. This suggests that the prior knowledge learned by the GPT backbone is crucial for text generation tasks, but less important for audio generation tasks. Consequently, we hypothesize that a reasonable approach to enhance the quality of generated audios could be to pre-train the GPT backbone not only with text sequences but also with audio token sequences.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "D Supporting More Complex Tasks",
      "text": "As stated in Section 3.4, with its modular and flexible design, LauraGPT provides an extensible framework to support complex tasks. By breaking a task into sub-tasks among the basic tasks used in training and cascading the raw inputs and model outputs of sub-tasks, LauraGPT can perform more complex tasks than the basic tasks. Similar to the speech-to-speech translation (S2ST) example, LauraGPT can perform more complex tasks by chaining together basic tasks as described above. Here are a few examples of other complex tasks that LauraGPT can support rather than doing them one by one.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Rich Transcription",
      "text": "We can extend LauraGPT to simultaneously transcribe audio into content, speaker information (speaker identification, etc), paralinguistic information (emotion, etc.) and highlevel semantic information (intent, slots, etc.) by including different task IDs at the generation process. This approach could avoid error accumulation in a pipelined approach and is more efficient than performing these tasks individually.\n\nNoise-robust ASR We can implement noiserobust ASR by chaining tasks and creating the following input sequence: [noisy speech embedding, <SE>, embedding of the enhanced speech, <ASR>]. Since SE and ASR are jointly trained for LauraGPT, LauraGPT could effectively exploit embeddings of the original noisy speech and enhanced speech for noise-robust ASR.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts the architecture of the proposed",
      "page": 3
    },
    {
      "caption": "Figure 1: The overview of the proposed LauraGPT model. The right part provides an enlarged view of the one-step",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "Abstract"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "Generative\nPre-trained\nTransformer\n(GPT)"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "models have achieved remarkable performance"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "on various natural\nlanguage processing tasks,"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "and\nhave\nshown\ngreat\npotential\nas\nback-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "bones for audio-and-text large language models"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "(LLMs). Previous mainstream audio-and-text"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "LLMs use discrete audio tokens to represent"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "both input and output audio; however, they suf-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "fer from performance degradation on tasks such"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "as automatic speech recognition, speech-to-text"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "translation, and speech enhancement over mod-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "els using continuous speech features.\nIn this"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "paper, we propose LauraGPT, a novel uni-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "fied audio-and-text GPT-based LLM for au-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "dio recognition, understanding, and generation."
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "LauraGPT is a versatile LLM that can process"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "both audio and text\ninputs and generate out-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "puts in either modalities. We propose a novel"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "data representation that combines continuous"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "and discrete features for audio: LauraGPT en-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "codes input audio into continuous representa-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "tions using an audio encoder and generates out-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "put audio from discrete codec codes. We pro-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "pose a one-step codec vocoder\nto overcome"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "the prediction challenge caused by the multi-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "modal distribution of codec tokens. We fine-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "tune LauraGPT using supervised multi-task"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "learning.\nExtensive\nexperiments\nshow that"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "LauraGPT consistently achieves comparable"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "to superior performance compared to strong"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "baselines on a wide range of audio tasks re-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "lated to content, semantics, paralinguistics, and"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "audio-signal analysis, such as automatic speech"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "recognition, speech-to-text translation, text-to-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "speech synthesis, speech enhancement, auto-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "mated audio captioning, speech emotion recog-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "nition, and spoken language understanding."
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "1\nIntroduction"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "Large language models\n(LLMs) are neural net-"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "works that generate natural\nlanguage texts based"
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": ""
        },
        {
          "{neo.dzh,sly.zsl}@alibaba-inc.com": "*equal contribution. † correspondence author."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "These methods have serious drawbacks, including": "high complexity, significant resource consumption,",
          "popular multi-step audio synthesis scheme. Our": "one-step codec vocoder uses a transformer-based"
        },
        {
          "These methods have serious drawbacks, including": "and unavoidable error accumulation problems. The",
          "popular multi-step audio synthesis scheme. Our": "predictor to estimate the sum of all codec token"
        },
        {
          "These methods have serious drawbacks, including": "other direction develops a unified Audio-and-Text",
          "popular multi-step audio synthesis scheme. Our": "groups instead of the individual\nindices, by min-"
        },
        {
          "These methods have serious drawbacks, including": "LLM leveraging LLMs as the backbone to support",
          "popular multi-step audio synthesis scheme. Our": "imizing the reconstruction losses. Our approach"
        },
        {
          "These methods have serious drawbacks, including": "audio-and-text tasks (Ao et al., 2022; Chen et al.,",
          "popular multi-step audio synthesis scheme. Our": "simplifies the audio generation process to a single"
        },
        {
          "These methods have serious drawbacks, including": "2021b; Wang et al., 2023b; Rubenstein et al., 2023).",
          "popular multi-step audio synthesis scheme. Our": "feed-forward calculation and also overcomes the"
        },
        {
          "These methods have serious drawbacks, including": "Decoder-only audio-and-text LLMs (Zhang et al.,",
          "popular multi-step audio synthesis scheme. Our": "prediction challenge caused by the multi-modal"
        },
        {
          "These methods have serious drawbacks, including": "2023a; Wang et al., 2023b; Rubenstein et al., 2023)",
          "popular multi-step audio synthesis scheme. Our": "distribution of codec tokens."
        },
        {
          "These methods have serious drawbacks, including": "are the dominant\ntechnique under\nthis category.",
          "popular multi-step audio synthesis scheme. Our": "We fine-tune LauraGPT using supervised multi-"
        },
        {
          "These methods have serious drawbacks, including": "These models convert continuous audio into dis-",
          "popular multi-step audio synthesis scheme. Our": "task learning on diverse audio tasks,\ninclud-"
        },
        {
          "These methods have serious drawbacks, including": "crete tokens and integrate text and audio tokens into",
          "popular multi-step audio synthesis scheme. Our": "ing tasks focusing on content, semantics, paralin-"
        },
        {
          "These methods have serious drawbacks, including": "unified vocabulary. These models suffer from in-",
          "popular multi-step audio synthesis scheme. Our": "guistics, and audio-signal analysis, such as ASR,"
        },
        {
          "These methods have serious drawbacks, including": "formation loss from quantization of speech signals",
          "popular multi-step audio synthesis scheme. Our": "speech-to-text translation (S2TT), TTS, SE, auto-"
        },
        {
          "These methods have serious drawbacks, including": "into discrete tokens, which leads to notable perfor-",
          "popular multi-step audio synthesis scheme. Our": "mated audio captioning (AAC), speech emotion"
        },
        {
          "These methods have serious drawbacks, including": "mance degradation on ASR compared to models us-",
          "popular multi-step audio synthesis scheme. Our": "recognition (SER), and SLU. Comprehensive ex-"
        },
        {
          "These methods have serious drawbacks, including": "ing continuous speech features (Chen et al., 2023a;",
          "popular multi-step audio synthesis scheme. Our": "periments show that, to the best of our knowl-"
        },
        {
          "These methods have serious drawbacks, including": "Chang et al., 2023; Yang et al., 2023c; Puvvada",
          "popular multi-step audio synthesis scheme. Our": "edge, LauraGPT1\nconsistently achieves com-"
        },
        {
          "These methods have serious drawbacks, including": "et al., 2023).\nIn this paper, we focus on improv-",
          "popular multi-step audio synthesis scheme. Our": "parable to superior performance compared to"
        },
        {
          "These methods have serious drawbacks, including": "ing the second category of unified Audio-and-Text",
          "popular multi-step audio synthesis scheme. Our": "strong baselines on the largest and the most di-"
        },
        {
          "These methods have serious drawbacks, including": "LLMs. Moreover, recent advances in audio gen-",
          "popular multi-step audio synthesis scheme. Our": "verse set of audio recognition, understanding,"
        },
        {
          "These methods have serious drawbacks, including": "eration from unified audio-and-text LLMs (Wang",
          "popular multi-step audio synthesis scheme. Our": "and generation tasks among existing decoder-"
        },
        {
          "These methods have serious drawbacks, including": "et al., 2023a,b) discretize speech into codec codes,",
          "popular multi-step audio synthesis scheme. Our": "only unified audio-and-text LLMs focusing on"
        },
        {
          "These methods have serious drawbacks, including": "then use an autoregressive language model (LM)",
          "popular multi-step audio synthesis scheme. Our": "these\ntasks\n(Zhang et\nal., 2023a; Wang et\nal.,"
        },
        {
          "These methods have serious drawbacks, including": "to predict output\ntokens from the first quantizer",
          "popular multi-step audio synthesis scheme. Our": "2023b; Rubenstein et al., 2023). The results are"
        },
        {
          "These methods have serious drawbacks, including": "and use a non-autoregressive model to predict to-",
          "popular multi-step audio synthesis scheme. Our": "remarkable since existing general speech models"
        },
        {
          "These methods have serious drawbacks, including": "kens from the other quantizers individually. One",
          "popular multi-step audio synthesis scheme. Our": "either focus solely on speech recognition and under-"
        },
        {
          "These methods have serious drawbacks, including": "limitation of this mechanism is that it needs many",
          "popular multi-step audio synthesis scheme. Our": "standing tasks but neglect speech generative tasks,"
        },
        {
          "These methods have serious drawbacks, including": "prediction steps (hence called multi-step audio",
          "popular multi-step audio synthesis scheme. Our": "or support speech generation but suffer\nfrom se-"
        },
        {
          "These methods have serious drawbacks, including": "synthesis scheme) to generate good quality speech.",
          "popular multi-step audio synthesis scheme. Our": "vere performance degradation on speech recogni-"
        },
        {
          "These methods have serious drawbacks, including": "Another\nlimitation is that predicting the indices",
          "popular multi-step audio synthesis scheme. Our": "tion and understanding tasks."
        },
        {
          "These methods have serious drawbacks, including": "of\nthe other codec groups is challenging due to",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "the multi-modal distribution nature of codec to-",
          "popular multi-step audio synthesis scheme. Our": "2\nRelated Work"
        },
        {
          "These methods have serious drawbacks, including": "kens (Jenrungrot et al., 2023).",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "Audio-to-Text LLMs Audio-to-Text LLMs can"
        },
        {
          "These methods have serious drawbacks, including": "To overcome the drawbacks of existing unified",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "generate text from audio inputs. Whisper (Radford"
        },
        {
          "These methods have serious drawbacks, including": "audio-and-text LLMs, we propose LauraGPT, a",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "et al., 2022) and USM (Zhang et al., 2023b) can per-"
        },
        {
          "These methods have serious drawbacks, including": "novel unified Audio-and-Text LLM based on the",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "form speech recognition and translation across mul-"
        },
        {
          "These methods have serious drawbacks, including": "GPT framework for audio recognition, understand-",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "tiple languages and domains. Pengi (Deshmukh"
        },
        {
          "These methods have serious drawbacks, including": "ing, and generation. LauraGPT is a versatile LLM",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "et al., 2023) is an audio LM that formulates audio"
        },
        {
          "These methods have serious drawbacks, including": "that can process both audio and text\ninputs and",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "tasks as text-generation tasks. UniverSLU (Arora"
        },
        {
          "These methods have serious drawbacks, including": "generate outputs in either modalities, with a single",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "et al., 2023)\nis a universal SLU model\nthat sup-"
        },
        {
          "These methods have serious drawbacks, including": "model. We propose a novel data representation",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "ports various speech classification and sequence"
        },
        {
          "These methods have serious drawbacks, including": "that combines continuous and discrete features",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "generation tasks. SALMONN (Tang et al., 2023)"
        },
        {
          "These methods have serious drawbacks, including": "for audio: LauraGPT encodes input audio into con-",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "and Qwen-Audio (Chu et al., 2023) integrate pre-"
        },
        {
          "These methods have serious drawbacks, including": "tinuous representations using an audio encoder and",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "trained text LLMs with separate speech and audio"
        },
        {
          "These methods have serious drawbacks, including": "generates output audio from discrete codec codes.",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "encoders into a single multimodal model."
        },
        {
          "These methods have serious drawbacks, including": "This data representation improves the performance",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "Text-to-Audio LLMs Text-to-Audio LLMs can"
        },
        {
          "These methods have serious drawbacks, including": "of audio-input\ntasks and also facilitates joint au-",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "convert\ntext\ninput\ninto\naudio\noutput\nand\nper-"
        },
        {
          "These methods have serious drawbacks, including": "toregressive modeling of audio and text features",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "form tasks\nsuch\nas TTS or\ntext-to-music\nsyn-"
        },
        {
          "These methods have serious drawbacks, including": "for audio generation tasks.",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "",
          "popular multi-step audio synthesis scheme. Our": "thesis.\nRecently,\ntwo prominent\ncategories of"
        },
        {
          "These methods have serious drawbacks, including": "We also propose a one-step codec vocoder in",
          "popular multi-step audio synthesis scheme. Our": ""
        },
        {
          "These methods have serious drawbacks, including": "LauraGPT to address the two limitations of the",
          "popular multi-step audio synthesis scheme. Our": "1Demos are available at https://lauragpt.github.io"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "approaches have\nemerged for generating audio": "from text prompts.\nIn the first category, contin-",
          "ers to model discrete audio tokens and text tokens": "as a shared vocabulary, but they suffer from infor-"
        },
        {
          "approaches have\nemerged for generating audio": "uous representations such as utterance-level em-",
          "ers to model discrete audio tokens and text tokens": "mation loss from quantization of audio signals into"
        },
        {
          "approaches have\nemerged for generating audio": "beddings (Elizalde et al., 2022; Liu et al., 2023a;",
          "ers to model discrete audio tokens and text tokens": "discrete tokens (Chen et al., 2023a; Chang et al.,"
        },
        {
          "approaches have\nemerged for generating audio": "Huang et al., 2023a) and Mel-frequency spectro-",
          "ers to model discrete audio tokens and text tokens": "2023; Yang et al., 2023c; Puvvada et al., 2023)."
        },
        {
          "approaches have\nemerged for generating audio": "grams (Nachmani et al., 2023) are used as the tar-",
          "ers to model discrete audio tokens and text tokens": ""
        },
        {
          "approaches have\nemerged for generating audio": "",
          "ers to model discrete audio tokens and text tokens": "3\nMethodology"
        },
        {
          "approaches have\nemerged for generating audio": "gets. However, continuous representations present",
          "ers to model discrete audio tokens and text tokens": ""
        },
        {
          "approaches have\nemerged for generating audio": "a challenge for unified modeling of text and audio",
          "ers to model discrete audio tokens and text tokens": "Figure 1 depicts the architecture of the proposed"
        },
        {
          "approaches have\nemerged for generating audio": "within a single LM. In the second category, discrete",
          "ers to model discrete audio tokens and text tokens": "LauraGPT. Section 3.1 describes the audio encoder,"
        },
        {
          "approaches have\nemerged for generating audio": "codec tokens are employed as audio representations",
          "ers to model discrete audio tokens and text tokens": "the text tokenizer, and the modified GPT LM for"
        },
        {
          "approaches have\nemerged for generating audio": "and generated by diffusion models (Yang et al.,",
          "ers to model discrete audio tokens and text tokens": "unified audio-and-text modeling. Section 3.2 elab-"
        },
        {
          "approaches have\nemerged for generating audio": "2023b) or autoregressive LMs (Kreuk et al., 2023;",
          "ers to model discrete audio tokens and text tokens": "orates the audio tokenizer. Section 3.3 introduces"
        },
        {
          "approaches have\nemerged for generating audio": "Borsos et al., 2023; Copet et al., 2023; Wang et al.,",
          "ers to model discrete audio tokens and text tokens": "an efficient one-step codec vocoder\nfor convert-"
        },
        {
          "approaches have\nemerged for generating audio": "2023a). Among models in the second category, in",
          "ers to model discrete audio tokens and text tokens": "ing audio tokens into high-quality raw waveforms."
        },
        {
          "approaches have\nemerged for generating audio": "models such as AudioGen (Kreuk et al., 2023), Au-",
          "ers to model discrete audio tokens and text tokens": "Section 3.4 describes the multi-task fine-tuning and"
        },
        {
          "approaches have\nemerged for generating audio": "dioLM (Borsos et al., 2023), and MusicGen (Copet",
          "ers to model discrete audio tokens and text tokens": "shows that LauraGPT provides an extensible frame-"
        },
        {
          "approaches have\nemerged for generating audio": "et al., 2023), multiple output heads are used after",
          "ers to model discrete audio tokens and text tokens": "work for supporting more complex tasks."
        },
        {
          "approaches have\nemerged for generating audio": "the LM to predict synchronized or delayed groups",
          "ers to model discrete audio tokens and text tokens": ""
        },
        {
          "approaches have\nemerged for generating audio": "",
          "ers to model discrete audio tokens and text tokens": "3.1\nModified Language Model for Unifying"
        },
        {
          "approaches have\nemerged for generating audio": "of codec tokens. However, this mechanism is only",
          "ers to model discrete audio tokens and text tokens": ""
        },
        {
          "approaches have\nemerged for generating audio": "",
          "ers to model discrete audio tokens and text tokens": "Audio-and-Text Modeling"
        },
        {
          "approaches have\nemerged for generating audio": "suitable for audio generation and may not be ap-",
          "ers to model discrete audio tokens and text tokens": ""
        },
        {
          "approaches have\nemerged for generating audio": "plicable to diverse audio-and-text\ntasks. Alterna-",
          "ers to model discrete audio tokens and text tokens": "For audio inputs, different from other audio-and-"
        },
        {
          "approaches have\nemerged for generating audio": "tively,\nin VALL-E (Wang et al., 2023a),\nthe LM",
          "ers to model discrete audio tokens and text tokens": "text LLMs using discrete tokens to represent audio"
        },
        {
          "approaches have\nemerged for generating audio": "predicts output tokens of the first quantizer, while",
          "ers to model discrete audio tokens and text tokens": "inputs, we extract\nthe log-compressed Mel spec-"
        },
        {
          "approaches have\nemerged for generating audio": "tokens of the remaining quantizers are predicted by",
          "ers to model discrete audio tokens and text tokens": "trogram features and convert them into continuous"
        },
        {
          "approaches have\nemerged for generating audio": "a non-autoregressive model one by one. This mech-",
          "ers to model discrete audio tokens and text tokens": "representations using a Conformer-based audio en-"
        },
        {
          "approaches have\nemerged for generating audio": "anism requires numerous prediction procedures to",
          "ers to model discrete audio tokens and text tokens": "coder. Text inputs and outputs are tokenized using"
        },
        {
          "approaches have\nemerged for generating audio": "generate acceptable speech quality. Moreover, the",
          "ers to model discrete audio tokens and text tokens": "the Qwen tokenizer (Bai et al., 2023), which inher-"
        },
        {
          "approaches have\nemerged for generating audio": "indices of the remaining codec groups are challeng-",
          "ers to model discrete audio tokens and text tokens": "its the tiktoken tokenizer (Jain, 2022) and incorpo-"
        },
        {
          "approaches have\nemerged for generating audio": "ing to predict due to the multi-modal distribution",
          "ers to model discrete audio tokens and text tokens": "rates additional augmentations for commonly used"
        },
        {
          "approaches have\nemerged for generating audio": "nature of codec tokens (Jenrungrot et al., 2023).",
          "ers to model discrete audio tokens and text tokens": "characters and words in different languages. The"
        },
        {
          "approaches have\nemerged for generating audio": "Audio-and-Text LLMs Audio-and-Text LLMs can",
          "ers to model discrete audio tokens and text tokens": "tokenized input text undergoes embedding matrix"
        },
        {
          "approaches have\nemerged for generating audio": "process and generate both audio and text, which",
          "ers to model discrete audio tokens and text tokens": "transformation to generate dense vectors. The au-"
        },
        {
          "approaches have\nemerged for generating audio": "can be categorized into two directions. One direc-",
          "ers to model discrete audio tokens and text tokens": "dio representations and text embeddings have the"
        },
        {
          "approaches have\nemerged for generating audio": "tion uses LLMs as controllers to interface special-",
          "ers to model discrete audio tokens and text tokens": "same dimension D. The Conformer-based encoder"
        },
        {
          "approaches have\nemerged for generating audio": "ized audio models, such as ASR and TTS mod-",
          "ers to model discrete audio tokens and text tokens": "is initialized with weights from a pre-trained ASR"
        },
        {
          "approaches have\nemerged for generating audio": "els, to enable direct audio interaction with LLMs",
          "ers to model discrete audio tokens and text tokens": "model (Gao et al., 2023). Since batch normaliza-"
        },
        {
          "approaches have\nemerged for generating audio": "and support various\naudio-and-text\ntasks,\nsuch",
          "ers to model discrete audio tokens and text tokens": "tion can lead to endless loop decoding, we replace"
        },
        {
          "approaches have\nemerged for generating audio": "as HuggingGPT (Shen et al., 2023) and Audio-",
          "ers to model discrete audio tokens and text tokens": "it with layer normalization in the Conformer-based"
        },
        {
          "approaches have\nemerged for generating audio": "GPT (Huang et al., 2023b). However, these models",
          "ers to model discrete audio tokens and text tokens": "encoder (details are in Appendix C.2)."
        },
        {
          "approaches have\nemerged for generating audio": "are complex, resource-intensive, and prone to error",
          "ers to model discrete audio tokens and text tokens": "To achieve audio generation capabilities, the au-"
        },
        {
          "approaches have\nemerged for generating audio": "accumulation. The second direction uses LLMs",
          "ers to model discrete audio tokens and text tokens": "dio outputs are discretized into tokens using an"
        },
        {
          "approaches have\nemerged for generating audio": "as the backbone for a unified model\nthat handles",
          "ers to model discrete audio tokens and text tokens": "audio tokenizer (Section 3.2) to obtain discrete rep-"
        },
        {
          "approaches have\nemerged for generating audio": "audio-and-text tasks (Ao et al., 2022; Chen et al.,",
          "ers to model discrete audio tokens and text tokens": "resentations and the softmax output\nlayer is aug-"
        },
        {
          "approaches have\nemerged for generating audio": "2021b; Wang et al., 2023b; Rubenstein et al., 2023).",
          "ers to model discrete audio tokens and text tokens": "mented with the audio tokens. As a result,\nthe"
        },
        {
          "approaches have\nemerged for generating audio": "SpeechT5 (Ao et al., 2022) and SpeechNet (Chen",
          "ers to model discrete audio tokens and text tokens": "weight matrix W in the output\nlayer\nis of\nsize"
        },
        {
          "approaches have\nemerged for generating audio": "et al., 2021b) perform various speech tasks with",
          "ers to model discrete audio tokens and text tokens": "(N + M + L) × D and is utilized to calculate the"
        },
        {
          "approaches have\nemerged for generating audio": "an encoder-decoder model, but they require modal-",
          "ers to model discrete audio tokens and text tokens": "logits for audio and text\ntokens at each position,"
        },
        {
          "approaches have\nemerged for generating audio": "specific pre-nets and post-nets to deal with differ-",
          "ers to model discrete audio tokens and text tokens": "where N , M , and L denote the vocabulary sizes"
        },
        {
          "approaches have\nemerged for generating audio": "ent input&output modalities. VioLA (Wang et al.,",
          "ers to model discrete audio tokens and text tokens": "of text, audio, and task tokens, respectively. Task"
        },
        {
          "approaches have\nemerged for generating audio": "2023b), AudioPaLM (Rubenstein et\nal., 2023),",
          "ers to model discrete audio tokens and text tokens": "tokens are used to inform the model which task"
        },
        {
          "approaches have\nemerged for generating audio": "SpeechGPT (Zhang et al., 2023a),\nand Speech-",
          "ers to model discrete audio tokens and text tokens": "should be performed. Note that\nin order to con-"
        },
        {
          "approaches have\nemerged for generating audio": "Gen (Wu et al., 2023) use decoder-only Transform-",
          "ers to model discrete audio tokens and text tokens": "trol the sequence length, we perform the low frame"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "Target Outputs"
        },
        {
          "Audio Encoder\nEmbedding": "Audio\nText\nTask ID\nor",
          "Codec Encoder": "Audio"
        },
        {
          "Audio Encoder\nEmbedding": "Figure 1: The overview of the proposed LauraGPT model. The right part provides an enlarged view of the one-step",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "Codec Vocoder (Section 3.3) in LauraGPT. The dashed modules are only used in the training stage. S⃝ and E⃝ denote",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "the “start of sequence” and “end of sequence” tokens. We omit the text tokenizer and detokenizer for simplicity.",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "rate (LFR) method (Gao et al., 2020) to downsam-",
          "Codec Encoder": "3.2\nAudio Tokenizer"
        },
        {
          "Audio Encoder\nEmbedding": "ple audio inputs to 60ms and only select\nthe first",
          "Codec Encoder": "For audio generation, we utilize a codec model as"
        },
        {
          "Audio Encoder\nEmbedding": "codec group of the audio outputs.",
          "Codec Encoder": "the audio tokenizer to extract discrete representa-"
        },
        {
          "Audio Encoder\nEmbedding": "Based on the aforementioned representations,",
          "Codec Encoder": "tions. Our codec model shares a similar architec-"
        },
        {
          "Audio Encoder\nEmbedding": "the GPT backbone is trained to model various audio",
          "Codec Encoder": "ture as EnCodec (Défossez et al., 2022), which"
        },
        {
          "Audio Encoder\nEmbedding": "and text tasks by minimizing the cross-entropy loss:",
          "Codec Encoder": "comprises convolutional recurrent encoder and de-"
        },
        {
          "Audio Encoder\nEmbedding": "Tv(cid:88)",
          "Codec Encoder": "coder (Tagliasacchi et al., 2020) and a residual vec-"
        },
        {
          "Audio Encoder\nEmbedding": "1 T\nLLM = −\nlog pθ (vj|u1:Tu, utask, v1:j−1)",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "tor quantizer (RVQ) (Vasuki and Vanathi, 2006)."
        },
        {
          "Audio Encoder\nEmbedding": "v",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "j=1",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "We enhance the original EnCodec model with the"
        },
        {
          "Audio Encoder\nEmbedding": "(1)",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "following modifications:\n1) Add reconstruction"
        },
        {
          "Audio Encoder\nEmbedding": "where u denotes the input embeddings with a se-",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "losses in the magnitude spectrum domain to im-"
        },
        {
          "Audio Encoder\nEmbedding": "quence length Tu and v represents the sequence of",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "prove the quality of middle- and high-frequency"
        },
        {
          "Audio Encoder\nEmbedding": "target tokens with a length Tv. To specify a task, a",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "signals. 2) Stack five strided convolution blocks"
        },
        {
          "Audio Encoder\nEmbedding": "special task-related token utask is inserted between",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "with strides of [8, 5, 4, 2, 2] to address the challenge"
        },
        {
          "Audio Encoder\nEmbedding": "the input embeddings and output tokens. Note that",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "of long sequence lengths, resulting in a token rate"
        },
        {
          "Audio Encoder\nEmbedding": "only the losses of outputs are taken into account,",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "of 25Hz for each token group.\n3) Use 32 quan-"
        },
        {
          "Audio Encoder\nEmbedding": "while losses on inputs and task token embeddings",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "tizers with structured dropout in the RVQ module,"
        },
        {
          "Audio Encoder\nEmbedding": "are masked out. After the final output layer, audio",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "each with vocabulary size 1024. This revision im-"
        },
        {
          "Audio Encoder\nEmbedding": "tokens are decoded to raw waveforms using a codec",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "proves speech quality with more quantizers while"
        },
        {
          "Audio Encoder\nEmbedding": "vocoder (Section 3.3). Since it\nis challenging to",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "preserving most information in the shallow quan-"
        },
        {
          "Audio Encoder\nEmbedding": "train an LLM from scratch with limited data and",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "tizers. The encoder and the first RVQ quantizer are"
        },
        {
          "Audio Encoder\nEmbedding": "computational resources, we use the open-source",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "used as the audio tokenizer, and the outputs of the"
        },
        {
          "Audio Encoder\nEmbedding": "GPT LLM, Qwen (Bai et al., 2023), as the back-",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "first quantizer are used as the audio tokens. The"
        },
        {
          "Audio Encoder\nEmbedding": "bone. Qwen is pre-trained on a diverse corpus cov-",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "choice of the first N RVQ quantizers to use is a"
        },
        {
          "Audio Encoder\nEmbedding": "ering various domains in English and Chinese and",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "tradeoff between performance and sequence length"
        },
        {
          "Audio Encoder\nEmbedding": "supports 8192 context length. Compared with other",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "(hence efficiency). The remaining quantizers and"
        },
        {
          "Audio Encoder\nEmbedding": "open-source GPT models with similar model sizes,",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "the decoder are only used when training the codec"
        },
        {
          "Audio Encoder\nEmbedding": "Qwen models demonstrate impressive competitive-",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "model. Details of training and the pre-trained codec"
        },
        {
          "Audio Encoder\nEmbedding": "ness, achieving better performance on widely used",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "model are in\n(Du et al., 2023)."
        },
        {
          "Audio Encoder\nEmbedding": "benchmarks, especially on Chinese tasks (Bai et al.,",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "3.3\nOne-step Codec Vocoder for Audio"
        },
        {
          "Audio Encoder\nEmbedding": "2023). Within LauraGPT, all parameters including",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "",
          "Codec Encoder": "Generation"
        },
        {
          "Audio Encoder\nEmbedding": "the Qwen backbone are jointly optimized, except",
          "Codec Encoder": ""
        },
        {
          "Audio Encoder\nEmbedding": "for the codec vocoder, which is trained indepen-",
          "Codec Encoder": "We propose a one-step codec vocoder in LauraGPT"
        },
        {
          "Audio Encoder\nEmbedding": "dently and kept\nfrozen during both training and",
          "Codec Encoder": "to generate waveforms\nfrom the\naudio tokens,"
        },
        {
          "Audio Encoder\nEmbedding": "inference stages of LauraGPT.",
          "Codec Encoder": "which are extracted from the first quantizer as de-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scribed in Section 3.2. Our vocoder comprises two": "components: a transformer-based predictor and a",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "codings as inputs. The TTS and SE tasks use audio"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "codec decoder. The predictor is trained to estimate",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "tokens as the target outputs, while the remaining"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "the summation of codec embeddings from the 32",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "tasks use text tokens as the target outputs."
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "RVQ quantizers by minimizing the L1 and L2 dis-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "Support More Complex Tasks\nWith its modu-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "tances between the predicted embeddings ˆE and",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "larized design, LauraGPT provides an extensible"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "their corresponding ground truth E:",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "framework to support complex tasks. By breaking"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "a task into sub-tasks among the basic tasks and"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "T,Dc",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "cascading the raw inputs and model outputs of sub-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "(cid:88) t\n(2)\nLpre =\n|Et,i − ˆEt,i|1 + |Et,i − ˆEt,i|2",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": ",i",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "tasks, LauraGPT can perform more complex tasks."
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "For example, we demonstrate that LauraGPT is ca-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "where T denotes the total number of frames and Dc",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "pable of performing the advanced speech-to-speech"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "denotes the dimension of the codec embeddings.",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "translation (S2ST) task by combining the S2TT and"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "After obtaining the estimated embeddings, the de-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "TTS tasks.\nInitially, a sequence is constructed to"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "coder of an pre-trained codec model is utilized to",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "translate the speech content into the target language"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "reconstruct the raw audio waveforms.",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "text using the S2TT task token: [audio encoding,"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "Alongside the predicted audio tokens from",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "<S2TT>]. Subsequently, the translated text is com-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "the LLM, text and audio inputs are used as con-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "bined with the TTS task token to synthesize speech:"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "ditions and fed to the predictor. For zero-shot",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "<TTS>].\nIf maintaining the"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "TTS task,\nthe text\ninputs serve as a condition as",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "speaker identity is desired, the original inputs and"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "well as the prompt audio features. For SE task, the",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "content can be incorporated to perform personal-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "input noisy speech features are employed as con-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "ized TTS. This can be achieved with an input se-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "ditions. Such text and audio conditionings allow",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "quence as [ASR"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "the model\nto generate high-quality audio signals",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "S2TT\ntranslated\ntext\nembedding,\n<TTS>,"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "by leveraging the diverse information in prompt",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "speech], where ASR"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "audios and noisy speeches, which is lacked in the",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "embedding is obtained using"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "discrete tokens (output\nfrom the first quantizer).",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "the ASR task: [audio\n<ASR>]. This"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "Therefore, different from existing Text-to-Audio",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "approach treats the bilingual text as the complete"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "LLMs, our approach simplifies the audio genera-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "input and allows the model to generate an output se-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "tion process to a single feed-forward calculation",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "quence of codec tokens while maintaining the same"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "and overcomes the prediction challenge caused",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "speaker identity. Audio samples of S2ST can be"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "by the multi-modal distribution of codec tokens.",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "found on the demo site. More examples of complex"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "tasks are in Appendix D."
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "3.4\nMulti-task Finetuning",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "4\nExperimental Settings"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "Basic Tasks\nWe unify modeling of the following",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "basic tasks in the single LauraGPT model and use",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "Model Architecture The Conformer-based audio"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "these tasks for multi-task fine-tuning: Automatic",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "encoder consists of 32 conformer blocks.\nEach"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "Speech Recognition (ASR), Spoken Language Un-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "block consists of a feed-forward module with 1536"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "derstanding (SLU), Speech-to-Text Translation",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "units, an attention module with 16 heads and a"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "(S2TT), Speech Emotion Recognition (SER), Au-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "dimension of 512, a convolutional module includ-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "tomated Audio Captioning (AAC), Speech En-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "ing the pointwise and depthwise convolution lay-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "hancement\n(SE),\nand Text-to-speech Synthesis",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "ers, and a second feed-forward module with 1536"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "(TTS). Task definitions are in Appendix A.1.",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "units. Sinusoidal positional encoding is applied on"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "the audio inputs. For a trade-off between perfor-"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "Unified Task Expression\nLauraGPT operates",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "mance and training efficiency, we use Qwen-1.8B2"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "[input\nbased\non\na\nunified\ntask\nexpression:",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "as the backbone and LauraGPT has 2B parameters."
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "embeddings, task ID, output\ntokens]. With",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "Qwen-1.8B comprises 24 transformer layers with a"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "the same inputs,\nthe desired outputs can differ",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "hidden size 2048 and 16 attention heads. Although"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "across tasks. For instance, ASR and S2TT tasks",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "Conformer and Qwen-1.8B are selected as the"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "require different outputs even for the same audio",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "audio encoder and GPT backbone, they can be"
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "input. Task tokens are included in both input em-",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": "replaced by other encoders and GPT models."
        },
        {
          "scribed in Section 3.2. Our vocoder comprises two": "bedding and output weight matrices. The TTS task",
          "S2TT, SLU, SE, ACC, and SER tasks take audio en-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: shows the results from the SOTA model,",
      "data": [
        {
          "Training Setup In all experiments, we initialize": "the Qwen backbone and audio encoder with the pre-",
          "1.8B LLM), and uses the Whisper audio encoder": "trained on a large amount of ASR data while we"
        },
        {
          "Training Setup In all experiments, we initialize": "trained checkpoints. We then optimize the model",
          "1.8B LLM), and uses the Whisper audio encoder": "use a Conformer encoder trained on much less data."
        },
        {
          "Training Setup In all experiments, we initialize": "parameters through multi-task fine-tuning.\nThe",
          "1.8B LLM), and uses the Whisper audio encoder": "Moreover, QwenAudio does not support speech"
        },
        {
          "Training Setup In all experiments, we initialize": "training&test datasets and evaluation metrics are",
          "1.8B LLM), and uses the Whisper audio encoder": "generative tasks hence cannot handle SE and TTS"
        },
        {
          "Training Setup In all experiments, we initialize": "presented in Appendix A.2 and A.3. Appendix A.4",
          "1.8B LLM), and uses the Whisper audio encoder": "tasks. Paraformer-large and UniverSLU achieve"
        },
        {
          "Training Setup In all experiments, we initialize": "describes the three-stage training process to address",
          "1.8B LLM), and uses the Whisper audio encoder": "SOTA results on AISHELL-2 test-ios for Chinese"
        },
        {
          "Training Setup In all experiments, we initialize": "the significant variation in data volume across dif-",
          "1.8B LLM), and uses the Whisper audio encoder": "ASR and on SLURP test for SLU; however, they"
        },
        {
          "Training Setup In all experiments, we initialize": "ferent tasks, and details the inference process.",
          "1.8B LLM), and uses the Whisper audio encoder": "only support single tasks and also train on more"
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "data than LauraGPT on the corresponding task."
        },
        {
          "Training Setup In all experiments, we initialize": "5\nResults and Analysis",
          "1.8B LLM), and uses the Whisper audio encoder": "Appendix B shows that LauraGPT greatly outper-"
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "forms Whisper Large V2 on Chinese ASR test sets"
        },
        {
          "Training Setup In all experiments, we initialize": "Section 5.1 presents\nthe main results of perfor-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "while the gap on English ASR test sets are pri-"
        },
        {
          "Training Setup In all experiments, we initialize": "mance comparison on the basic tasks from the state-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "marily attributed to the much smaller English data"
        },
        {
          "Training Setup In all experiments, we initialize": "of-the-art (SOTA) model, a comparable baseline,",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "used for training LauraGPT. For TTS,\nthe SOTA"
        },
        {
          "Training Setup In all experiments, we initialize": "and our LauraGPT. Ablation studies in Section 5.2",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "VALL-E Phone outperforms baseline VALL-E To-"
        },
        {
          "Training Setup In all experiments, we initialize": "demonstrate the advantages of using continuous",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "ken4, suggesting the importance of text representa-"
        },
        {
          "Training Setup In all experiments, we initialize": "representations for audio inputs in LauraGPT by",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "tion for TTS. Compared to both VALL-E models,"
        },
        {
          "Training Setup In all experiments, we initialize": "comparing to a counterpart with both discrete in-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "LauraGPT achieves comparable speaker similar-"
        },
        {
          "Training Setup In all experiments, we initialize": "puts and outputs (denoted Discrete IO), the supe-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "ity (SECS) and speech quality (MOSNet).\nThe"
        },
        {
          "Training Setup In all experiments, we initialize": "riority of our one-step codec vocoder, and effec-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "degradation in content consistency (WER)\nfrom"
        },
        {
          "Training Setup In all experiments, we initialize": "tiveness of multi-task finetuning.\nFurther analy-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "LauraGPT results from the generalization issue,"
        },
        {
          "Training Setup In all experiments, we initialize": "ses include comparison with related unified Audio-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "since the training data is too limited for LauraGPT"
        },
        {
          "Training Setup In all experiments, we initialize": "and-Text LLMs (Appendix B), more analysis of",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "with 2B parameters. Overall, the results show that"
        },
        {
          "Training Setup In all experiments, we initialize": "multi-task fine-tuning on SER task (Appendix C.1),",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "LauraGPT consistently achieves comparable to"
        },
        {
          "Training Setup In all experiments, we initialize": "comparing batch normalization with layer normal-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "superior performance than strong baselines on"
        },
        {
          "Training Setup In all experiments, we initialize": "ization in the audio encoder (Appendix C.2), and",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "diverse speech tasks, demonstrating the general"
        },
        {
          "Training Setup In all experiments, we initialize": "studying impact of initialization from pre-trained",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "effectiveness of LauraGPT on speech recogni-"
        },
        {
          "Training Setup In all experiments, we initialize": "models (Appendix C.3).",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "tion, understanding, and generative tasks."
        },
        {
          "Training Setup In all experiments, we initialize": "5.1\nResults on All Tasks",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "5.2\nAnalysis"
        },
        {
          "Training Setup In all experiments, we initialize": "Table 1 shows the results from the SOTA model,",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "Discrete VS. Continuous Representations\nfor"
        },
        {
          "Training Setup In all experiments, we initialize": "a comparable baseline,\nand our LauraGPT3,\nin",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "Audio Inputs\nExisting unified Audio-and-Text"
        },
        {
          "Training Setup In all experiments, we initialize": "that order, on a variety of speech recognition, un-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "LLMs use discrete tokens to represent audio inputs."
        },
        {
          "Training Setup In all experiments, we initialize": "derstanding,\nand generation benchmarks.\nThe",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "We analyze the efficacy of using continuous repre-"
        },
        {
          "Training Setup In all experiments, we initialize": "SOTA model yields the best results on each test",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "sentations for audio inputs in LauraGPT by compar-"
        },
        {
          "Training Setup In all experiments, we initialize": "set based on our\nliterature\nreview.\nThe base-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "ing to its counterpart Discrete IO on ASR, S2TT,"
        },
        {
          "Training Setup In all experiments, we initialize": "line for each task is chosen to facilitate fair com-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "and SE tasks, representing audio-input recogni-"
        },
        {
          "Training Setup In all experiments, we initialize": "parison with LauraGPT:\nthey are comparable to",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "tion and understanding, and audio generation"
        },
        {
          "Training Setup In all experiments, we initialize": "LauraGPT in model architecture or training data",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "capacities. In Discrete IO, both audio inputs and"
        },
        {
          "Training Setup In all experiments, we initialize": "and are also common competitive baselines in the",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "outputs are represented by flattened codec tokens"
        },
        {
          "Training Setup In all experiments, we initialize": "literature. We cite the SOTA results to validate",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "from the first\nfour quantizers5,\nresulting in a to-"
        },
        {
          "Training Setup In all experiments, we initialize": "that LauraGPT consistently performs competitively",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "ken rate of 100Hz. In LauraGPT, audio inputs are"
        },
        {
          "Training Setup In all experiments, we initialize": "on all the speech recognition, understanding, and",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "represented by continuous acoustic features, which"
        },
        {
          "Training Setup In all experiments, we initialize": "generation tasks and the baselines are competitive.",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "However, LauraGPT results cannot be fairly com-",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "4We re-implement two VALL-E models with 0.34B train-"
        },
        {
          "Training Setup In all experiments, we initialize": "pared to the SOTA results. Specifically, QwenAu-",
          "1.8B LLM), and uses the Whisper audio encoder": "able parameters, both trained with the same data as LauraGPT."
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "VALL-E Phone uses phonemes as the text input representation,"
        },
        {
          "Training Setup In all experiments, we initialize": "dio achieves SOTA performance on most speech",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "while VALL-E Token uses WordPiece tokens from the text"
        },
        {
          "Training Setup In all experiments, we initialize": "understanding tasks, but compared to LauraGPT,",
          "1.8B LLM), and uses the Whisper audio encoder": ""
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "tokenizer."
        },
        {
          "Training Setup In all experiments, we initialize": "QwenAudio uses a much larger LLM (∼7B VS. our",
          "1.8B LLM), and uses the Whisper audio encoder": "5Using outputs of the first quantizer (as in LauraGPT) for"
        },
        {
          "Training Setup In all experiments, we initialize": "",
          "1.8B LLM), and uses the Whisper audio encoder": "audio tokenizer renders very poor performance for audio-input"
        },
        {
          "Training Setup In all experiments, we initialize": "3Our results are from single runs due to the stability of the",
          "1.8B LLM), and uses the Whisper audio encoder": "tasks with the Discrete IO models. Using more quantizers"
        },
        {
          "Training Setup In all experiments, we initialize": "models and limited computational resources.",
          "1.8B LLM), and uses the Whisper audio encoder": "improves performance but reduces efficiency."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "Task"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "ASR"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "SLU"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "S2TT"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "SER"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "AAC"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "SE"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": "TTS"
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        },
        {
          "Table 1: Results from the SOTA, a comparable baseline, and our LauraGPT, in that order, on speech recognition,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "task, using codec tokens as inputs cannot improve"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "the quality and intelligibility of noisy speeches,"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "suggesting lack of enhancement capability, proba-"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "bly because the distribution of noisy speech is too"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "complicated to be accurately represented by four"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "groups of discrete audio tokens."
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "Comparison\non\nAudio\nSynthesis\nSchemes"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "VALL-E (Wang et al., 2023a) introduces a com-"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "monly used scheme formulating audio synthesis"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "as a classification problem: A neural network is"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "shared to predict\nthe codec tokens in the follow-"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "ing group with the previous ones as inputs and"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "synthesizing target audio requires multiple steps"
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": ""
        },
        {
          "LauraGPT\n8.62 | 0.91 | 3.26": "or iterations to achieve a reasonable speech qual-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Comparison of our one-step audio synthesis areshowninTable4.",
      "data": [
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": "Task"
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": "ASR"
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": "S2TT"
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": "SE"
        },
        {
          "Table 2: Comparison of Discrete IO models and LauraGPT on ASR, S2TT, and SE tasks for analysis of discrete VS.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Comparison of our one-step audio synthesis areshowninTable4.",
      "data": [
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "scheme and the multi-step scheme on the SE task."
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "PESQ ↑"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "2.55"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "2.97"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "In contrast, our one-step codec vocoder for-"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "mulates audio synthesis as a regression problem."
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "vocoder simplifies audio synthesis into a single"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "feed-forward calculation and overcomes the pre-"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "diction challenge caused by the multimodal distri-"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "bution of codec tokens. Table 3 shows that our"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "one-step codec vocoder greatly outperforms the"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "multi-step scheme in terms of content consis-"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": "tency (CER, WER) and speech quality (PESQ),"
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        },
        {
          "Table 3: Comparison of our one-step audio synthesis": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Comparison of our one-step audio synthesis areshowninTable4.",
      "data": [
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "multi-task\nfine-tuned LauraGPT (Section\n3.4)"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "could be advantageous over individual single-task"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "models: (1) Multi-task learning could exploit syn-"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "ergy between tasks\nand reduce over-fitting,\nin"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "turn yield high performance on diverse tasks and"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "achieve better performance than single-task train-"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "ing.\n(2) Multi-task learning could learn a single"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "model capable of supporting a wide range of tasks,"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "hence practical deployment\nis greatly simplified"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "through unified model implementation and API."
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "We investigate whether\nthe multi-task trained"
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": ""
        },
        {
          "Effectiveness\nof Multi-task Finetuning\nThe": "LauraGPT could achieve better performance than"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "including the Qwen backbone, except for the codec"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "vocoder. This strategy results in substantial compu-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "tations for training. In future work, we plan to in-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "vestigate parameter-efficient fine-tuning to reduce"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "computation demands. Also, due to the limited"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "computation resources, our comparisons between"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "the multi-task trained LauraGPT and single-task"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "models are focused on the low-resource tasks, that"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "is, AAC, SLU, and SER tasks. We find that multi-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "task learning for LauraGPT consistently achieves"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "better performance than single-task training for"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "tasks with limited training data. Next, we plan"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "to complete comparisons of LauraGPT and single-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "task models on all tasks, including relatively rich-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "resource tasks such as ASR. These studies will pro-"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "mote understandings on where tasks could benefit"
        },
        {
          "LauraGPT during supervised multi-task finetuning,": ""
        },
        {
          "LauraGPT during supervised multi-task finetuning,": "from each other, including tasks with even conflict-"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "employ multi-task learning. Experiments demon-": "strate that LauraGPT achieves comparable to supe-",
          "able unified GPT-style Audio-and-Text LLMs than": "existing approaches that can leverage large-scale"
        },
        {
          "employ multi-task learning. Experiments demon-": "rior performance compared to strong baselines on",
          "able unified GPT-style Audio-and-Text LLMs than": "labeled data and achieve highly competitive perfor-"
        },
        {
          "employ multi-task learning. Experiments demon-": "a wide range of speech tasks on content, semantics,",
          "able unified GPT-style Audio-and-Text LLMs than": "mance on a diverse set of speech tasks, including"
        },
        {
          "employ multi-task learning. Experiments demon-": "paralinguistics, and audio-signal analysis.",
          "able unified GPT-style Audio-and-Text LLMs than": "speech recognition, understanding and generation,"
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "using a single model? Note that previous general"
        },
        {
          "employ multi-task learning. Experiments demon-": "Limitations",
          "able unified GPT-style Audio-and-Text LLMs than": "speech models either focus solely on speech recog-"
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "nition and understanding tasks but neglect speech"
        },
        {
          "employ multi-task learning. Experiments demon-": "In this work,\nin order\nto support a wide range",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "generative tasks, or support speech generation but"
        },
        {
          "employ multi-task learning. Experiments demon-": "of audio recognition, understanding, and gener-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "suffer\nfrom severe performance degradation on"
        },
        {
          "employ multi-task learning. Experiments demon-": "ation tasks, we choose to train all parameters in",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "speech recognition and understanding tasks."
        },
        {
          "employ multi-task learning. Experiments demon-": "LauraGPT during supervised multi-task finetuning,",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Inspired by the recent advances of LLMs in NLP,"
        },
        {
          "employ multi-task learning. Experiments demon-": "including the Qwen backbone, except for the codec",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "we envision that\nthe fundamental speech models"
        },
        {
          "employ multi-task learning. Experiments demon-": "vocoder. This strategy results in substantial compu-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "should have the following capabilities:"
        },
        {
          "employ multi-task learning. Experiments demon-": "tations for training. In future work, we plan to in-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "vestigate parameter-efficient fine-tuning to reduce",
          "able unified GPT-style Audio-and-Text LLMs than": "•\nIn-context learning ability like GPT-3, which"
        },
        {
          "employ multi-task learning. Experiments demon-": "computation demands. Also, due to the limited",
          "able unified GPT-style Audio-and-Text LLMs than": "can learn from few-shot examples and adapt"
        },
        {
          "employ multi-task learning. Experiments demon-": "computation resources, our comparisons between",
          "able unified GPT-style Audio-and-Text LLMs than": "to new tasks, such as predicting the age of the"
        },
        {
          "employ multi-task learning. Experiments demon-": "the multi-task trained LauraGPT and single-task",
          "able unified GPT-style Audio-and-Text LLMs than": "speaker from a speech sample."
        },
        {
          "employ multi-task learning. Experiments demon-": "models are focused on the low-resource tasks, that",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "•\nInstruction-following ability like InstructGPT"
        },
        {
          "employ multi-task learning. Experiments demon-": "is, AAC, SLU, and SER tasks. We find that multi-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "and ChatGPT, which can perform the\nap-"
        },
        {
          "employ multi-task learning. Experiments demon-": "task learning for LauraGPT consistently achieves",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "propriate speech-related task given a natural"
        },
        {
          "employ multi-task learning. Experiments demon-": "better performance than single-task training for",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "language instruction, such as synthesizing a"
        },
        {
          "employ multi-task learning. Experiments demon-": "tasks with limited training data. Next, we plan",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "speech with a specific emotion or style."
        },
        {
          "employ multi-task learning. Experiments demon-": "to complete comparisons of LauraGPT and single-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "task models on all tasks, including relatively rich-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "• General audio modeling abilities, i.e., speech,"
        },
        {
          "employ multi-task learning. Experiments demon-": "resource tasks such as ASR. These studies will pro-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "non-speech audio, and music, such as music"
        },
        {
          "employ multi-task learning. Experiments demon-": "mote understandings on where tasks could benefit",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "generation."
        },
        {
          "employ multi-task learning. Experiments demon-": "from each other, including tasks with even conflict-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Our\nwork\ndemonstrates\nthat\nthe\ncurrent"
        },
        {
          "employ multi-task learning. Experiments demon-": "ing objectives. We also plan to conduct deeper anal-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "LauraGPT has made solid progress and reached"
        },
        {
          "employ multi-task learning. Experiments demon-": "ysis on the potential risk of catastrophic forgetting",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "one important milestone toward a speech founda-"
        },
        {
          "employ multi-task learning. Experiments demon-": "of the original\ntext capabilities of the pre-trained",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "tion model. From LauraGPT to the next-generation"
        },
        {
          "employ multi-task learning. Experiments demon-": "text LLM, due to multi-task learning of speech",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "speech foundation model we envision, most remain-"
        },
        {
          "employ multi-task learning. Experiments demon-": "tasks. Note that exploration of parameter-efficient",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "ing efforts are in more task data collection and more"
        },
        {
          "employ multi-task learning. Experiments demon-": "fine-tuning may also help preserve the original text",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "self-supervised and/or supervised pre-training and"
        },
        {
          "employ multi-task learning. Experiments demon-": "capabilities of the pre-trained text LLMs.",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "supervised fine-tuning. There is no need to modify"
        },
        {
          "employ multi-task learning. Experiments demon-": "LauraGPT relies on discrete audio tokens for",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "the model architecture."
        },
        {
          "employ multi-task learning. Experiments demon-": "speech generative tasks. Our research shows that",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "the performance of this paradigm strongly depends",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "on the quality of the audio tokenizer. We plan to",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "References"
        },
        {
          "employ multi-task learning. Experiments demon-": "systematically analyze the impact of various audio",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Adaeze Adigwe, Noé Tits, Kevin El Haddad, Sarah Os-"
        },
        {
          "employ multi-task learning. Experiments demon-": "tokenizers on diverse audio generative tasks. We",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "tadabbas, and Thierry Dutoit. 2018. The emotional"
        },
        {
          "employ multi-task learning. Experiments demon-": "plan to develop new audio tokenizers that are more",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "voices database: Towards controlling the emotion di-"
        },
        {
          "employ multi-task learning. Experiments demon-": "suitable for unified Auio-and-Text LLMs and pro-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "mension in voice generation systems. arXiv preprint"
        },
        {
          "employ multi-task learning. Experiments demon-": "vide desirable representations for generative tasks.",
          "able unified GPT-style Audio-and-Text LLMs than": "arXiv:1806.09514."
        },
        {
          "employ multi-task learning. Experiments demon-": "There are great emerging interests in fundamen-",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-"
        },
        {
          "employ multi-task learning. Experiments demon-": "tal speech models that are similar to those in the",
          "able unified GPT-style Audio-and-Text LLMs than": "son, Dmitry Lepikhin, Alexandre Passos, Siamak"
        },
        {
          "employ multi-task learning. Experiments demon-": "field of NLP. This is a tremendously valuable re-",
          "able unified GPT-style Audio-and-Text LLMs than": "Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng"
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Chen, Eric Chu,\nJonathan H. Clark, Laurent El"
        },
        {
          "employ multi-task learning. Experiments demon-": "search direction.\nOur work achieves\nimportant",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-"
        },
        {
          "employ multi-task learning. Experiments demon-": "milestone for this research question, as we explore",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "rav Mishra, Erica Moreira, Mark Omernick, Kevin"
        },
        {
          "employ multi-task learning. Experiments demon-": "and provide promising answers to the following",
          "able unified GPT-style Audio-and-Text LLMs than": ""
        },
        {
          "employ multi-task learning. Experiments demon-": "",
          "able unified GPT-style Audio-and-Text LLMs than": "Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,"
        },
        {
          "employ multi-task learning. Experiments demon-": "question: How to design more efficient and scal-",
          "able unified GPT-style Audio-and-Text LLMs than": "Yuanzhong Xu, Yujing Zhang, Gustavo Hernández"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Jan A. Botha, James Bradbury, Siddhartha Brahma,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "IEEE transactions on affective comput-\ntors dataset."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Kevin Brooks, Michele Catasta, Yong Cheng, Colin",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "ing, 5(4):377–390."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Cherry, Christopher A. Choquette-Choo, Aakanksha",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Chowdhery, Clément Crepy, Shachi Dave, Mostafa",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Ruizhe Cao, Sherif Abdulatif, and Bin Yang. 2022. CM-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Dehghani, Sunipa Dev,\nJacob Devlin, Mark Díaz,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "GAN: Conformer-based Metric GAN for Speech En-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "hancement.\nIn Proc. Interspeech 2022, pages 936–"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "aoyu Feng, Vlad Fienber, Markus Freitag, Xavier",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "940."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Garcia, Sebastian Gehrmann, Lucas Gonzalez, and",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "et\nal.\n2023.\nPalm 2\ntechnical\nreport.\nCoRR,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Xuankai Chang, Brian Yan, Yuya Fujita,\nTakashi"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "abs/2305.10403.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Maekaku, and Shinji Watanabe. 2023. Exploration of"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "efficient end-to-end ASR using discretized input from"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "self-supervised learning. CoRR, abs/2305.18108."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Zhihua Wei, Yao Qian,\nJinyu Li,\nand Furu Wei.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Guoguo Chen, Shuzhou Chai, Guanbo Wang,\nJiayu"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "2022. Speecht5: Unified-modal encoder-decoder pre-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "training for spoken language processing.\nIn Proceed-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Povey, Jan Trmal, Junbo Zhang, et al. 2021a. Gi-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "ings of the 60th Annual Meeting of the Association",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "gaspeech: An evolving, multi-domain asr corpus with"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "for Computational Linguistics (Volume 1: Long Pa-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "arXiv preprint\n10,000 hours of\ntranscribed audio."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "pers), ACL 2022, Dublin, Ireland, May 22-27, 2022,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "arXiv:2106.06909."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "pages 5723–5738. Association for Computational",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Linguistics.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng,"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Shiliang Zhang, Chong Deng, Yukun Ma, Hai Yu,"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Jiaqing Liu, and Chong Zhang. 2023a. Loss mask-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Peng, Roshan S. Sharma, Yosuke Kashiwagi, Emiru",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "ing is not needed in decoder-only transformer\nfor"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Tsunoo, and Shinji Watanabe. 2023. Universlu: Uni-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "discrete-token based ASR. CoRR, abs/2311.04534."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "versal\nspoken language understanding for diverse",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "classification and sequence generation tasks with a",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Weidong Chen, Xiaofen Xing, Peihao Chen, and Xi-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "single network. CoRR, abs/2310.02973.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "angmin Xu. 2023b. Vesper: A compact and effec-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "tive pretrained model for speech emotion recognition."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "CoRR, abs/2307.10757."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Huang, and et. al. 2023. Qwen technical report. arxiv",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "preprint, 2309.16609.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Yi-Chen Chen, Po-Han Chi, Shu-Wen Yang, Kai-Wei"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Chang, Jheng-Hao Lin, Sung-Feng Huang, Da-Rong"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Emanuele Bastianelli, Andrea Vanzo, Pawel Swieto-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "janski, and Verena Rieser. 2020.\nSlurp: A spoken",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "yi Lee. 2021b.\nSpeechnet: A universal modular-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "arXiv\nlanguage understanding resource package.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "ized model\nfor\nspeech processing tasks.\nCoRR,"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "preprint arXiv:2011.13205.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "abs/2105.03070."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Zalán Borsos, Raphaël Marinier, Damien Vincent, Eu-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shil-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "gene Kharitonov, Olivier Pietquin, Matthew Sharifi,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "iang Zhang, Zhijie Yan, Chang Zhou, and Jingren"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Dominik Roblek, Olivier Teboul, David Grangier,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Zhou. 2023.\nQwen-audio: Advancing universal"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Marco Tagliasacchi, and Neil Zeghidour. 2023. Audi-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "audio understanding via unified large-scale audio-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "olm: A language modeling approach to audio genera-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "language models. CoRR, abs/2311.07919."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "tion.\nIEEE ACM Trans. Audio Speech Lang. Process.,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "31:2523–2533.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Défossez. 2023. Simple and controllable music gen-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Zheng. 2017. Aishell-1: An open-source mandarin",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "eration. CoRR, abs/2306.05284."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "speech corpus and a speech recognition baseline.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "the oriental chapter of\nIn 2017 20th conference of",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Soham Deshmukh, Benjamin Elizalde, Rita Singh, and"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "the international coordinating committee on speech",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Huaming Wang. 2023.\nPengi: An audio language"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "databases and speech I/O systems and assessment",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "model for audio tasks. CoRR, abs/2305.11834."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "(O-COCOSDA), pages 1–5. IEEE.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Konstantinos Drossos, Samuel Lipping, and Tuomas"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Virtanen. 2020. Clotho: An audio captioning dataset."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Kazemzadeh, Emily Mower, Samuel Kim,\nJean-",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "In ICASSP 2020-2020 IEEE International Confer-"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "nette N Chang,\nSungbok Lee,\nand Shrikanth S",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "ence on Acoustics, Speech and Signal Processing"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "(ICASSP), pages 736–740. IEEE."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "dyadic motion capture database. Language resources",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "and evaluation, 42:335–359.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": ""
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. 2018."
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Houwei Cao, David G Cooper, Michael K Keutmann,",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "Aishell-2: Transforming mandarin asr research into"
        },
        {
          "Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,": "Ruben C Gur, Ani Nenkova, and Ragini Verma. 2014.",
          "Crema-d: Crowd-sourced emotional multimodal ac-": "industrial scale. arXiv preprint arXiv:1808.10583."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "2023.\nFuncodec:\nA fundamental,\nreproducible",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "and Gunhee Kim. 2019. Audiocaps: Generating cap-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "and integrable open-source toolkit for neural speech",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "the\ntions for audios in the wild.\nIn Proceedings of"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "codec.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "2019 Conference of the North American Chapter of"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "the Association for Computational Linguistics: Hu-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "man Language Technologies, Volume 1 (Long and"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Yossi Adi. 2022. High fidelity neural audio compres-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Short Papers), pages 119–132."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "sion. arXiv:2210.13438.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Benjamin Elizalde, Soham Deshmukh, Mahmoud Al",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Seltzer, and Sanjeev Khudanpur. 2017. A study on"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Ismail, and Huaming Wang. 2022. CLAP: learning",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "data augmentation of reverberant speech for robust"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "audio concepts from natural\nlanguage supervision.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "speech recognition.\nIn ICASSP, pages 5220–5224."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "CoRR, abs/2206.04769.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Dvorkovich, Christian Federmann, Mark Fishel,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Font, and Xavier Serra. 2022.\nFSD50K: an open",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Thamme Gowda, Yvette Graham, Roman Grund-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "IEEE ACM\ndataset of human-labeled sound events.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "kiewicz, Barry Haddow, et al. 2022. Findings of the"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Trans. Audio Speech Lang. Process., 30:829–852.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "2022 conference on machine translation (wmt22).\nIn"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Proceedings of the Seventh Conference on Machine"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Translation (WMT), pages 1–45."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Du, Zhangyu Xiao, and Shiliang Zhang. 2023. Fu-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Yuma Koizumi, Daiki Takeuchi, Yasunori Ohishi,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "nasr: A fundamental end-to-end speech recognition",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Noboru Harada, and Kunio Kashino. 2020. The ntt"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "toolkit.\nIn INTERSPEECH.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "dcase2020 challenge task 6 system: Automated au-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "dio captioning with keywords and sentence length"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Zhifu Gao,\nShiliang\nZhang, Ming\nLei,\nand\nIan",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "estimation. arXiv preprint arXiv:2007.00225."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "McLoughlin. 2020. San-m: Memory equipped self-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "attention for end-to-end speech recognition. arXiv",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "preprint arXiv:2006.01713.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Singer, Alexandre Défossez, Jade Copet, Devi Parikh,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Yaniv Taigman, and Yossi Adi. 2023. Audiogen: Tex-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "tually guided audio generation.\nIn ICLR. OpenRe-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Yan. 2022. Paraformer: Fast and accurate parallel",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "view.net."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "transformer for non-autoregressive end-to-end speech",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "recognition.\nIn INTERSPEECH, pages 2063–2067.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "ISCA.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Liu, Danilo P. Mandic, Wenwu Wang, and Mark D."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Plumbley. 2023a. Audioldm: Text-to-audio genera-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "tion with latent diffusion models.\nIn ICML, volume"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xi-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "202 of Proceedings of Machine Learning Research,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "ang Yin, and Zhou Zhao. 2023a. Make-an-audio:",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "pages 21450–21474. PMLR."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Text-to-audio generation with prompt-enhanced dif-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "fusion models.\nIn International Conference on Ma-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "chine Learning, ICML 2023, 23-29 July 2023, Hon-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "olulu, Hawaii, USA, volume 202 of Proceedings of",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Yuxuan Wang, and Mark D. Plumbley. 2023b. Au-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Machine Learning Research, pages 13916–13932.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "dioldm 2: Learning holistic audio generation with"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "PMLR.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "self-supervised pretraining. CoRR, abs/2308.05734."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Rongjie Huang, Mingze Li, Dongchao Yang,\nJia-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Steven R Livingstone and Frank A Russo. 2018. The"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "tong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "ryerson audio-visual database of emotional speech"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren,",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "and song (ravdess): A dynamic, multimodal set of fa-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Zhou Zhao, and Shinji Watanabe. 2023b. Audiogpt:",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "cial and vocal expressions in north american english."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Understanding and generating speech, music, sound,",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "PloS one, 13(5):e0196391."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "and talking head. CoRR, abs/2304.12995.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Philip Jackson and SJUoSG Haq. 2014. Surrey audio-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley,"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "visual expressed emotion (savee) database. Univer-",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Yuexian Zou, and Wenwu Wang. 2023. Wavcaps:"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "sity of Surrey: Guildford, UK.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "A chatgpt-assisted weakly-labelled audio caption-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "ing dataset for audio-language multimodal research."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Shantanu Jain. 2022.\ntiktoken: A fast BPE tokeniser for",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "arXiv preprint arXiv:2303.17395."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "use with OpenAI’s models.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": ""
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chu-"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Teerapat\nJenrungrot, Michael Chinen, W. Bastiaan",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "layuth Asawaroengchai, Soroosh Mariooryad, R. J."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "Kleijn, and et al. 2023.\nLMCodec: A low bitrate",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "Skerry-Ryan,\nand Michelle Tadmor Ramanovich."
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "speech codec with causal\ntransformer models.\nIn",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "2023. Lms with a voice: Spoken language modeling"
        },
        {
          "Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng.": "ICASSP.",
          "Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,": "beyond speech tokens. CoRR, abs/2305.15255."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "abs/2303.08774.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Weiming Lu, and Yueting Zhuang. 2023. Hugging-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "gpt: Solving AI tasks with chatgpt and its friends in"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "huggingface. CoRR, abs/2303.17580."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Sanjeev Khudanpur. 2015. Librispeech: an asr cor-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas, and"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "pus based on public domain audio books.\nIn 2015",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "IEEE international conference on acoustics, speech",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Dominik Roblek. 2020.\nSeanet: A multi-modal"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "speech enhancement network.\nIn INTERSPEECH,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "and signal processing (ICASSP), pages 5206–5210.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "pages 1126–1130."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "IEEE.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Douglas B. Paul and Janet M. Baker. 1992. The design",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma,\nand"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "for\nthe wall street\njournal-based CSR corpus.\nIn",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Chao Zhang. 2023.\nSALMONN:\ntowards generic"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "ICSLP, pages 899–902. ISCA.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "hearing abilities for large language models. CoRR,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "abs/2310.13289."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "M Kathleen Pichora-Fuller and Kate Dupuis. 2020.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Toronto emotional speech set (tess). Scholars Portal",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Dataverse, 1:2020.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Martinet, Marie-Anne Lachaux, Timothée Lacroix,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Azhar, Aurélien Rodriguez, Armand Joulin, Edouard"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Grave, and Guillaume Lample. 2023. Llama: Open"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "halcea. 2018.\nMeld:\nA multimodal multi-party",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "and efficient\nfoundation language models.\nCoRR,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "dataset\nfor\nemotion recognition in conversations.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "abs/2302.13971."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "arXiv preprint arXiv:1810.02508.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "A Vasuki and PT Vanathi. 2006. A review of vector"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Krishna C. Puvvada, Nithin Rao Koluguri, Kunal",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "quantization techniques.\nIEEE Potentials, 25(4):39–"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Dhawan,\nJagadeesh Balam,\nand Boris Ginsburg.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "47."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "2023. Discrete audio representation as an alternative",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "to mel-spectrograms for speaker and speech recogni-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "tion. CoRR, abs/2309.10922.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Zhang, Robert Adkins, William Ngan, Jeff Wang,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "man, Christine McLeavey, and Ilya Sutskever. 2022.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Ellis, Rashel Moritz, Yael Yungster, Alice Rakotoari-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Robust speech recognition via large-scale weak su-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "son, Liang Tan, Chris Summers, Carleigh Wood,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "pervision. CoRR, abs/2212.04356.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Joshua Lane, Mary Williamson, and Wei-Ning Hsu."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "2023. Audiobox: Unified audio generation with nat-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "ural language prompts. CoRR, abs/2312.15821."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "man, Christine McLeavey, and Ilya Sutskever. 2023.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Changhan Wang, Anne Wu, and Juan Pino. 2020. Cov-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Robust speech recognition via large-scale weak su-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "ost 2 and massively multilingual speech-to-text trans-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "pervision.\nIn International Conference on Machine",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "lation. arXiv preprint arXiv:2007.10310."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Learning, pages 28492–28518. PMLR.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Mirco Ravanelli, Titouan Parcollet, Peter Plantinga,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Subakan, Nauman Dawalatabad, Abdelwahab Heba,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Furu Wei. 2023a. Neural codec language models"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Jianyuan Zhong,\nJu-Chieh Chou, Sung-Lin Yeh,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "are zero-shot\ntext\nto speech synthesizers.\nCoRR,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "abs/2301.02111."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "François Grondin, William Aris, Hwidong Na, Yan",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Gao, Renato De Mori, and Yoshua Bengio. 2021.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shu-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Speechbrain:\nA general-purpose\nspeech\ntoolkit.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "jie Liu, Yashesh Gaur, Zhuo Chen,\nJinyu Li, and"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "CoRR, abs/2106.04624.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Furu Wei. 2023b. Viola: Unified codec language"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "models for speech recognition, synthesis, and trans-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Paul\nK.\nRubenstein,\nChulayuth\nAsawaroengchai,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "lation. CoRR, abs/2305.16107."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Duc Dung Nguyen, Ankur Bapna, Zalán Borsos,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Félix de Chaumont Quitry, Peter Chen, Dalia El",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng,"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Badawy, Wei Han, Eugene Kharitonov, Hannah",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Weihua Luo, and Rong Jin. 2022. Learning to gen-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Muckenhirn, Dirk\nPadfield,\nJames Qin, Danny",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "eralize to more: Continuous semantic augmentation"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Rozenberg,\nTara N.\nSainath,\nJohan\nSchalkwyk,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "for neural machine translation.\nIn Proceedings of the"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Matthew Sharifi, Michelle Tadmor Ramanovich,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "60th Annual Meeting of the Association for Compu-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Marco Tagliasacchi, Alexandru Tudor, Mihajlo Ve-",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "tational Linguistics, ACL 2022."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "limirovic, Damien Vincent,\nJiahui Yu, Yongqiang",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": ""
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang,",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, and Hung-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Zhishuai Zhang, Lukas Zilka, and Christian Havnø",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "yi Lee. 2023.\nSpeechgen: Unlocking the genera-"
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "Frank. 2023. Audiopalm: A large language model",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "tive power of speech language models with prompts."
        },
        {
          "OpenAI.\n2023.\nGPT-4\ntechnical\nreport.\nCoRR,": "that can speak and listen. CoRR, abs/2306.12925.",
          "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,": "CoRR, abs/2306.02207."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Zhao,\nJiang Bian, Xixin Wu, Zhou Zhao, Shinji"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Watanabe, and Helen Meng. 2023a. Uniaudio: An"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "audio foundation model toward universal audio gen-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "eration. CoRR, abs/2310.00704."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Chao Weng, Yuexian Zou, and Dong Yu. 2023b. Diff-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "sound: Discrete diffusion model\nfor\ntext-to-sound"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "IEEE ACM Trans. Audio Speech Lang.\ngeneration."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Process., 31:1720–1733."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Yifan Yang, Feiyu Shen, Chenpeng Du, Ziyang Ma, Kai"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Yu, Daniel Povey, and Xie Chen. 2023c. Towards"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "universal speech discrete tokens: A case study for"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "ASR and TTS. CoRR, abs/2309.07377."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Chenchen Zeng, et al. 2022. Wenetspeech: A 10000+"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "hours multi-domain mandarin corpus\nfor\nspeech"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "recognition.\nIn ICASSP 2022-2022 IEEE Interna-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "tional Conference on Acoustics, Speech and Signal"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Processing (ICASSP), pages 6182–6186. IEEE."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Dong Zhang,\nShimin Li, Xin\nZhang,\nJun\nZhan,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Speechgpt: Empowering large language models with"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "intrinsic cross-modal conversational abilities. CoRR,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "abs/2305.11000."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, Ying"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Chen,\nand Qinfei Li. 2021.\nBstc: A large-scale"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "arXiv\nchinese-english speech translation dataset."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "preprint arXiv:2104.03575."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Yu Zhang, Wei Han,\nJames Qin, Yongqiang Wang,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Andrew Rosenberg, Rohit Prabhavalkar, Daniel S."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Park, Parisa Haghani,\nJason Riesa, Ginger Perng,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Hagen Soltau, Trevor Strohman, Bhuvana Ramab-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "hadran, Tara N. Sainath, Pedro J. Moreno, Chung-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Cheng Chiu, Johan Schalkwyk, Françoise Beaufays,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "and Yonghui Wu. 2023b. Google USM: scaling au-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "tomatic speech recognition beyond 100 languages."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "CoRR, abs/2303.01037."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Siqi Zheng, Luyao Cheng, Yafeng Chen, Hui Wang,"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "and Qian Chen. 2023.\n3d-speaker: A large-scale"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "multi-device, multi-distance, and multi-dialect cor-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "pus for speech representation disentanglement. arxiv"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "preprint, 2306.15354."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Xiaohuan Zhou,\nJiaming Wang, Zeyu Cui, Shiliang"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "Zhang, Zhijie Yan, Jingren Zhou, and Chang Zhou."
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "2022. Mmspeech: Multi-modal multi-task encoder-"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "decoder pre-training for speech recognition. arXiv"
        },
        {
          "Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,": "preprint arXiv:2212.00500."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 5: The AdamW",
      "data": [
        {
          "our use of the data is consistent with their intended": "use. We use all data sets in the same ways as prior",
          "scription Evaluation, and SPIDEr represents the": "average of SPICE and CIDEr."
        },
        {
          "our use of the data is consistent with their intended": "research works, hence we did not check whether",
          "scription Evaluation, and SPIDEr represents the": "• WA, UA and WF1 stands for weighted accuracy,"
        },
        {
          "our use of the data is consistent with their intended": "the data that was used contains any information",
          "scription Evaluation, and SPIDEr represents the": "unweighted accuracy and the weighted F1 score."
        },
        {
          "our use of the data is consistent with their intended": "that names or uniquely identifies individual people",
          "scription Evaluation, and SPIDEr represents the": "WA corresponds to the overall accuracy, UA cor-"
        },
        {
          "our use of the data is consistent with their intended": "or offensive content.",
          "scription Evaluation, and SPIDEr represents the": "responds to the average class-wise accuracy, and"
        },
        {
          "our use of the data is consistent with their intended": "",
          "scription Evaluation, and SPIDEr represents the": "WF1 corresponds to the average class-wise F1"
        },
        {
          "our use of the data is consistent with their intended": "A.3\nEvaluation Datasets and Metrics",
          "scription Evaluation, and SPIDEr represents the": ""
        },
        {
          "our use of the data is consistent with their intended": "",
          "scription Evaluation, and SPIDEr represents the": "score."
        },
        {
          "our use of the data is consistent with their intended": "Table 6 presents the evaluation datasets and evalua-",
          "scription Evaluation, and SPIDEr represents the": "• ACC measures the accuracy of predicting the"
        },
        {
          "our use of the data is consistent with their intended": "tion metrics for various tasks. The metrics used in",
          "scription Evaluation, and SPIDEr represents the": "intent. SLU-F1 is a metric that balances Word-"
        },
        {
          "our use of the data is consistent with their intended": "our experiments are described below:",
          "scription Evaluation, and SPIDEr represents the": "F1 and Char-F1, computed as the sum of\nthe"
        },
        {
          "our use of the data is consistent with their intended": "",
          "scription Evaluation, and SPIDEr represents the": "confusion matrices."
        },
        {
          "our use of the data is consistent with their intended": "• CER stands for Character Error Rate, a com-",
          "scription Evaluation, and SPIDEr represents the": ""
        },
        {
          "our use of the data is consistent with their intended": "monly used metric to evaluate the recognition",
          "scription Evaluation, and SPIDEr represents the": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 5: The AdamW",
      "data": [
        {
          "Table 6 presents the evaluation datasets and evalua-": "tion metrics for various tasks. The metrics used in",
          "• ACC measures the accuracy of predicting the": "intent. SLU-F1 is a metric that balances Word-"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "our experiments are described below:",
          "• ACC measures the accuracy of predicting the": "F1 and Char-F1, computed as the sum of\nthe"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "confusion matrices."
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• CER stands for Character Error Rate, a com-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "monly used metric to evaluate the recognition",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "performance of Chinese and English utterances.",
          "• ACC measures the accuracy of predicting the": "A.4\nDetails of Training and Inference"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "We also utilize CER to assess the content consis-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "In all experiments, we optimize the model parame-"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "tency in TTS task.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "ters through the following steps: (1) We initialize"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• WER stands for Word Error Rate, which consid-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the Qwen backbone and the audio encoder with"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "ers entire words rather than individual characters.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the pre-trained checkpoints. (2) We then perform"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "In our experiments, we use WER to evaluate ASR",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "multi-task finetuning."
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "recognition performance, content consistency in",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "Due to the significant variation in data volume"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "TTS, and speech intelligibility in SE.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "across different tasks, the training process is con-"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• SECS, which stands for Speaker Encoder Cosine",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "ducted in three stages. In the first training stage, the"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "Similarity, utilizes speaker embeddings extracted",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "model is fine-tuned on all tasks using the complete"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "from a pre-trained speaker verification model 6",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "training data as shown in Table 5. The AdamW"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "for both prompt and synthesized speech.\nThe",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "optimizer is utilized with a peak learning rate of"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "cosine similarity between the two embeddings is",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "5 × 10−4 and 10K warmup steps.\nIn the second"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "then employed to measure the speaker similarity",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "stage, we further fine-tune the model on tasks that"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "between the prompt speech and the synthesized",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "have small-scale datasets, including TTS, SE, AAC,"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "speech. Furthermore, the naturalness of the syn-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "SER, and SLU tasks. The AdamW optimizer is uti-"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "thesized speech is evaluated using MOSNet, a",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "lized with a peak learning rate of 2 × 10−4 and"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "non-intrusive score derived from a pre-trained",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "10K warmup steps. In the third training stage, we"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "neural network 7.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "fine-tune the model on all tasks using the complete"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• BLEU represent\nthe Bilingual Evaluation Un-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "training data again. The peak learning rate of the"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "derstudy metric. BLEU is commonly used to",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "AdamW optimizer for the third stage is reduced by"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "assess the quality of machine-generated text by",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "half as 1 × 10−4, while the warmup step remains"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "comparing it\nto reference translations.\nIn our",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "at 10K."
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "experiments, we use BLEU to evaluate S2TT.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "For the codec vocoder, we train the predictor on"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• PESQ represents\nPerceptual\nEvaluation\nof",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the training data of the TTS and SE tasks. We use"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "Speech Quality, while STOI stands for Short-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the Adam optimizer with a peak learning rate of"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "time Objective\nIntelligibility.\nBoth metrics",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "0.001 and 25K warmup steps. The decoder of the"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "are widely used to assess speech enhancement.",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "codec vocoder\nis initialized with the pre-trained"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "PESQ ranges from −0.5 to 4.5, whereas STOI is",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "checkpoints8 and kept frozen during the multi-task"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "in the range of [0, 1].",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "finetuning of LauraGPT."
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "• SPICE, CIDEr and SPIDEr are metrics bor-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "As stated in Section 3, during the training stage,"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "rowed from the image captioning task and em-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the input is converted into input embeddings by the"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "ployed for AAC evaluation. SPICE stands for",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "audio encoder if the input is audio, or converted by"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "Semantic Propositional Image Caption Evalua-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the embedding matrix W if the input is text, while"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "tion, CIDEr denotes Consensus-based Image De-",
          "• ACC measures the accuracy of predicting the": ""
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the output is converted into output embeddings by"
        },
        {
          "Table 6 presents the evaluation datasets and evalua-": "",
          "• ACC measures the accuracy of predicting the": "the same embedding matrix W for teacher-forcing."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "this corpus are copied N times during training."
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "Task"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "ASR"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "SLU"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "S2TT"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": ""
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "SER"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": ""
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "AAC"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "SE"
        },
        {
          "Table 5: Statistics of the training data for basic tasks in Section 3.4. Corpus×N means that the training samples in": "TTS"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": ""
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "Evaluation Datasets"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "AISHELL-1 test, AISHELL-2 test-ios,"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": ""
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "Librispeech test-clean & test-other"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "SLURP test"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "BSTC dev, En→Zh subset of CoVOST2"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "MELD test"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "Clotho eval"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "LibriSpeech test-clean, FSD50K, noise-92"
        },
        {
          "Table 6: Evaluation datasets and metrics for different tasks. ↑ indicates that higher values of the metric are desirable,": "AISHELL-1 test, LibriTTS test-clean"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "TTS\nAISHELL-1 test, LibriTTS test-clean",
          "PESQ ↑, STOI ↑, WER ↓": "CER ↓, WER ↓, SECS ↑, MOS ↑"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "the task-ID token into an embedding. Then, these",
          "PESQ ↑, STOI ↑, WER ↓": "format output,\nthe codec vocoder is employed to"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "embeddings are composed into an embedding se-",
          "PESQ ↑, STOI ↑, WER ↓": "convert tokens into raw waveforms."
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "quence as [input embeddings, task-ID embedding,",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "output embeddings], which is taken as the input of",
          "PESQ ↑, STOI ↑, WER ↓": "A.5\nDetails of the SER Evaluation"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "Qwen LLM. To train the model, a masked cross-",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "",
          "PESQ ↑, STOI ↑, WER ↓": "During the training stage, emotion labels within"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "entropy loss function is applied, as shown in Eq. 1.",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "",
          "PESQ ↑, STOI ↑, WER ↓": "different\ntraining corpora are unified into the fol-"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "As described in Section 3, in addition to masking",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "",
          "PESQ ↑, STOI ↑, WER ↓": "lowing nine classes: anger, disgust, neutral,\nlike,"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "out the losses on inputs, the cross-entropy loss at",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "",
          "PESQ ↑, STOI ↑, WER ↓": "sadness, surprise, happiness, joy, and fear. At the"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "the position of the task token is also masked out.",
          "PESQ ↑, STOI ↑, WER ↓": ""
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "",
          "PESQ ↑, STOI ↑, WER ↓": "test stage, we map the “like” and “happiness” emo-"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "During the inference stage, the input is converted",
          "PESQ ↑, STOI ↑, WER ↓": "tion classes into the “joy” class to match the MELD"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "into input embeddings as done during the train-",
          "PESQ ↑, STOI ↑, WER ↓": "test set. LauraGPT uses an autoregressive structure"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "ing stage. Then the corresponding task-ID embed-",
          "PESQ ↑, STOI ↑, WER ↓": "to generate emotion labels. Out-of-domain outputs"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "ding is added at\nthe end of the input embedding",
          "PESQ ↑, STOI ↑, WER ↓": "are considered as classification errors, making the"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "sequence. Next, the Qwen LLM generates output",
          "PESQ ↑, STOI ↑, WER ↓": "task harder. Both WavLM Base model and WavLM"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "tokens in an autoregressive manner until the “end",
          "PESQ ↑, STOI ↑, WER ↓": "Large model utilize the weighted sum of multiple"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "of sequence” token is generated. Finally, for text-",
          "PESQ ↑, STOI ↑, WER ↓": "layers with learnable parameters as speech features,"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "format output, the Qwen tokenizer is employed to",
          "PESQ ↑, STOI ↑, WER ↓": "which are fed into a downstream network for clas-"
        },
        {
          "SE\nLibriSpeech test-clean, FSD50K, noise-92": "convert\ntokens into final output, while for audio-",
          "PESQ ↑, STOI ↑, WER ↓": "sification."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 8: , on the Chinese to Paraformer. Note that Paraformer is a non-",
      "data": [
        {
          "B\nComparison with Related Unified": "Audio-and-Text Models",
          "Phoneme Error Rate (PER) rather than recognizing": "words/characters and reporting WER/CER as con-"
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "ducted by LauraGPT. According to their paper, Vi-"
        },
        {
          "B\nComparison with Related Unified": "Table 7 compares our LauraGPT against the most",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "oLA underperforms their in-house Attention-based"
        },
        {
          "B\nComparison with Related Unified": "related works, which,\nsimilar\nto LauraGPT, are",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "Encoder-Decoder\n(AED) model\n(which we also"
        },
        {
          "B\nComparison with Related Unified": "all multi-task unified audio-and-text models. Due",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "have no access to) with relative 19.96% phoneme"
        },
        {
          "B\nComparison with Related Unified": "to the drastic differences in experimental settings,",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "error rate (PER) degradation from 9.47% to 11.36%"
        },
        {
          "B\nComparison with Related Unified": "datasets used and lack of open source codebase",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "on Mandarin WenetSpeech dev set. Since higher"
        },
        {
          "B\nComparison with Related Unified": "and checkpoints,\nit\nis difficult\nto conduct a fair",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "PER always corresponds to much higher WER as"
        },
        {
          "B\nComparison with Related Unified": "comparison between LauraGPT and these most",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "a word comprises multiple phonemes, it would be"
        },
        {
          "B\nComparison with Related Unified": "related multi-task unified audio-and-text models.",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "safe to hypothesize that\nthe relative degradation"
        },
        {
          "B\nComparison with Related Unified": "Despite all these difficulties, below we provide the",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "on WER from VioLA over AED is even greater."
        },
        {
          "B\nComparison with Related Unified": "most relevant results for comparing LauraGPT and",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "In contrast, compared with the Paraformer base-"
        },
        {
          "B\nComparison with Related Unified": "these related models.",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "line, our LauraGPT achieves comparable CER on"
        },
        {
          "B\nComparison with Related Unified": "Whisper (Radford et al., 2022) is solely stud-",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "the Mandarin AISHELL-2 test-ios\nset and out-"
        },
        {
          "B\nComparison with Related Unified": "ied on the ASR task in the original paper, hence",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "performs it on the English Librispeech test-other"
        },
        {
          "B\nComparison with Related Unified": "we compare LauraGPT to Whisper only on the",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "set,\ni.e., overall LauraGPT performs comparably"
        },
        {
          "B\nComparison with Related Unified": "ASR task. As shown in Table 8, on the Chinese",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "to Paraformer.\nNote that Paraformer\nis a non-"
        },
        {
          "B\nComparison with Related Unified": "test sets AISHELL-1 test and AISHELL-2 test-ios,",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "autoregressive AED model performing comparably"
        },
        {
          "B\nComparison with Related Unified": "LauraGPT greatly outperforms Whisper by -3.9",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "to conventional auto-regressive AED model (Gao"
        },
        {
          "B\nComparison with Related Unified": "and -2.3 absolute on CER with much smaller train-",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "through this chain of\net al., 2022).\nTherefore,"
        },
        {
          "B\nComparison with Related Unified": "ing data. On the English test sets Librispeech test-",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "comparisons, we are confident to conclude that"
        },
        {
          "B\nComparison with Related Unified": "clean and test-other, LauraGPT performs worse",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "LauraGPT notably outperforms VioLA on ASR"
        },
        {
          "B\nComparison with Related Unified": "than Whisper Large V2 as Whisper Large V2 uses",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "task."
        },
        {
          "B\nComparison with Related Unified": "much more English training data than LauraGPT.",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "AudioPaLM (Rubenstein et al., 2023) is evalu-"
        },
        {
          "B\nComparison with Related Unified": "SpeechT5 (Ao et\nal., 2022)\nis\nevaluated on",
          "Phoneme Error Rate (PER) rather than recognizing": "ated on ASR, S2TT and TTS tasks. Since the train-"
        },
        {
          "B\nComparison with Related Unified": "ASR, TTS, S2TT, voice conversion (VC), SE, and",
          "Phoneme Error Rate (PER) rather than recognizing": "ing and evaluation datasets for AudioPaLM and"
        },
        {
          "B\nComparison with Related Unified": "speaker\nidentification (SID). Since the training",
          "Phoneme Error Rate (PER) rather than recognizing": "LauraGPT are disjoint,\ntheir performance results"
        },
        {
          "B\nComparison with Related Unified": "data of\ntasks other\nthan ASR for SpeechT5 dif-",
          "Phoneme Error Rate (PER) rather than recognizing": "cannot be directly compared. In addition, the pre-"
        },
        {
          "B\nComparison with Related Unified": "fers\nremarkably from those\nfor LauraGPT, we",
          "Phoneme Error Rate (PER) rather than recognizing": "trained model of AudioPaLM has not been released."
        },
        {
          "B\nComparison with Related Unified": "compare LauraGPT against SpeechT5 only on",
          "Phoneme Error Rate (PER) rather than recognizing": "Therefore, empirically comparing LauraGPT to Au-"
        },
        {
          "B\nComparison with Related Unified": "ASR. For SpeechT5, the model is first pre-trained",
          "Phoneme Error Rate (PER) rather than recognizing": "dioPaLM will require great effort and is not con-"
        },
        {
          "B\nComparison with Related Unified": "with large-scale unlabeled speech and text data.",
          "Phoneme Error Rate (PER) rather than recognizing": "ducted in this work."
        },
        {
          "B\nComparison with Related Unified": "Then,\nit\nis finetuned on the Librispeech-960 cor-",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "pus via the hybrid cross-entropy and CTC loss. As",
          "Phoneme Error Rate (PER) rather than recognizing": "C\nMore Analyses of Critical Design"
        },
        {
          "B\nComparison with Related Unified": "claimed in their paper, SpeechT5 achieves a WER",
          "Phoneme Error Rate (PER) rather than recognizing": "Choices"
        },
        {
          "B\nComparison with Related Unified": "of 7.3% on the Librispeech test-other subset with-",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "C.1\nEffectiveness of Multi-task Finetuning on"
        },
        {
          "B\nComparison with Related Unified": "out CTC and LM. Under a fair comparison, our",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "",
          "Phoneme Error Rate (PER) rather than recognizing": "the SER task"
        },
        {
          "B\nComparison with Related Unified": "LauraGPT achieves a comparable WER of 7.7%.",
          "Phoneme Error Rate (PER) rather than recognizing": ""
        },
        {
          "B\nComparison with Related Unified": "Note that different from SpeechT5, LauraGPT",
          "Phoneme Error Rate (PER) rather than recognizing": "Table 4 shows that for the SER task, on the MELD"
        },
        {
          "B\nComparison with Related Unified": "is directly trained on multi-task labeled datasets",
          "Phoneme Error Rate (PER) rather than recognizing": "test set, the multi-task trained LauraGPT substan-"
        },
        {
          "B\nComparison with Related Unified": "without benefiting from any self-supervised pre-",
          "Phoneme Error Rate (PER) rather than recognizing": "tially outperforms the single-task model in terms"
        },
        {
          "B\nComparison with Related Unified": "training.",
          "Phoneme Error Rate (PER) rather than recognizing": "of UA and WF1 metrics, while the WA result\nis"
        },
        {
          "B\nComparison with Related Unified": "VioLA (Wang et al., 2023b)\nis evaluated on",
          "Phoneme Error Rate (PER) rather than recognizing": "slightly worse."
        },
        {
          "B\nComparison with Related Unified": "ASR, S2TT, TTS and S2ST tasks.\nConsider-",
          "Phoneme Error Rate (PER) rather than recognizing": "To further analyze the results of the SER task,"
        },
        {
          "B\nComparison with Related Unified": "ing the substantial differences in training data on",
          "Phoneme Error Rate (PER) rather than recognizing": "we conduct a statistical analysis of the number of"
        },
        {
          "B\nComparison with Related Unified": "tasks between VioLA and LauraGPT and lack",
          "Phoneme Error Rate (PER) rather than recognizing": "samples for each emotion class in both training and"
        },
        {
          "B\nComparison with Related Unified": "of open-sourced VioLA codebase\nand models,",
          "Phoneme Error Rate (PER) rather than recognizing": "test sets of the MELD dataset, as well as their cor-"
        },
        {
          "B\nComparison with Related Unified": "it\nis difficult\nto fairly compare LauraGPT with",
          "Phoneme Error Rate (PER) rather than recognizing": "responding test accuracy. The results are shown"
        },
        {
          "B\nComparison with Related Unified": "VioLA. Among the tasks, direct comparison on",
          "Phoneme Error Rate (PER) rather than recognizing": "in Table 10. Compared to the single-task model,"
        },
        {
          "B\nComparison with Related Unified": "ASR is also challenging since VioLA only con-",
          "Phoneme Error Rate (PER) rather than recognizing": "the multi-task trained LauraGPT results in degrada-"
        },
        {
          "B\nComparison with Related Unified": "ducts speech-to-phoneme recognition and reports",
          "Phoneme Error Rate (PER) rather than recognizing": "tion in accuracy for classes with a larger number of"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "each model is trained and evaluated on."
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": ""
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Date"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Organization"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Model Size"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Pair Data (hrs)"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Unsup. Pretrain"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Audio Input"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Audio Output"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": "Languages"
        },
        {
          "Table 7: Comparisons with the most related multi-task unified audio-and-text models. The table shows the tasks that": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": ""
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": ""
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": "Model"
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": ""
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": "Paraformer (CN)"
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": "Paraformer (EN)"
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": "Whisper Large V2"
        },
        {
          "Table 8: Comparison of different models on the ASR task in terms of CER(%) ↓ for Chinese and WER(%) ↓ for": "LauraGPT (Ours)"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "training (disgust, fear).": ""
        },
        {
          "training (disgust, fear).": ""
        },
        {
          "training (disgust, fear).": "C.2\nBatch normalization versus layer"
        },
        {
          "training (disgust, fear).": "normalization in audio encoder"
        },
        {
          "training (disgust, fear).": "In\nthe\noriginal\ndesign,\nbatch\nnormalization\nis"
        },
        {
          "training (disgust, fear).": "applied\nafter\nthe\nconvolution module\nin\nthe"
        },
        {
          "training (disgust, fear).": "Conformer-based audio encoder. However, we dis-"
        },
        {
          "training (disgust, fear).": "cover that this choice leads to endless looping de-"
        },
        {
          "training (disgust, fear).": "coding due to inaccurate estimations of mean and"
        },
        {
          "training (disgust, fear).": "variance, particularly for tasks with long sequence"
        },
        {
          "training (disgust, fear).": "lengths. When the issue of endless looping de-"
        },
        {
          "training (disgust, fear).": "coding occurs,\nthe model generates several fixed"
        },
        {
          "training (disgust, fear).": "tokens repeatedly and cannot stop the generation"
        },
        {
          "training (disgust, fear).": "until achieving a pre-defined maximum length. To"
        },
        {
          "training (disgust, fear).": "address this issue, we replace batch normalization"
        },
        {
          "training (disgust, fear).": "with layer normalization, which is more robust to"
        },
        {
          "training (disgust, fear).": "various mini-batch sizes. We validate this design"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task",
      "data": [
        {
          "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task": "Model"
        },
        {
          "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task": "#Training Samples"
        },
        {
          "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task": "#Testing Samples"
        },
        {
          "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task": "Single-task"
        },
        {
          "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task": "LauraGPT"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 10: Accuracy on different emotion classes in the SER task from single-task finetuning and multi-task",
      "data": [
        {
          "#Testing Samples": "Single-task",
          "345": "0.396",
          "68": "0.000",
          "1256": "0.875",
          "402": "0.119",
          "208": "0.029",
          "281": "0.128",
          "50": "0.000"
        },
        {
          "#Testing Samples": "LauraGPT",
          "345": "0.333",
          "68": "0.103",
          "1256": "0.708",
          "402": "0.381",
          "208": "0.236",
          "281": "0.381",
          "50": "0.040"
        },
        {
          "#Testing Samples": "by focusing on the SE task, which generally has",
          "345": "",
          "68": "",
          "1256": "D",
          "402": "",
          "208": "Supporting More Complex Tasks",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "the longest sequence among all the included tasks.",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "As stated in Section 3.4, with its modular and flexi-",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "The results are shown in Table 9. BN means batch",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "ble design, LauraGPT provides an extensible frame-",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "normalization while LN means layer normalization.",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "work to support complex tasks. By breaking a task",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "To evaluate the occurring probability of endless",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "into sub-tasks among the basic tasks used in train-",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "loop decoding, we define the metric, “loop ratio”,",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "ing and cascading the raw inputs and model outputs",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "of sub-tasks, LauraGPT can perform more complex",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "test cases.",
          "345": "The results indicate",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "tasks than the basic tasks.",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "that batch normalization causes a significantly high",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "Similar",
          "208": "to\nthe",
          "281": "",
          "50": "speech-to-speech"
        },
        {
          "#Testing Samples": "the inference stage,",
          "345": "",
          "68": "leading to unac-",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "(S2ST) example, LauraGPT can perform more",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "ceptable PESQ and STOI scores.",
          "345": "",
          "68": "In contrast, by",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "complex tasks by chaining together basic tasks as",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "replacing batch normalization with layer nor-",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "described above. Here are a few examples of other",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "malization, we observe a considerable reduction",
          "345": "",
          "68": "",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "complex tasks that LauraGPT can support rather",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "in the loop ratio to a very low level,",
          "345": "",
          "68": "thereby",
          "1256": "",
          "402": "",
          "208": "",
          "281": "",
          "50": ""
        },
        {
          "#Testing Samples": "",
          "345": "",
          "68": "",
          "1256": "",
          "402": "than doing them one by one.",
          "208": "",
          "281": "",
          "50": ""
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "Dataset"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "AISHELL-1 test"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "AISHELL-2 test-ios"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": ""
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "LibriSpeech test-clean"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "LibriSpeech test-other"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "BSTC dev (Zh→En)"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": ""
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "CoVOST2 test set (En→Zh)"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "Mixup of LibriSpeech"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "test-clean, FSD50K and"
        },
        {
          "Table 11: Impact of initialization on the ASR, S2TT and SE tasks.": "noise-92"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "Adaeze Adigwe",
        "Noé Tits",
        "Kevin Haddad",
        "Sarah Ostadabbas",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Rohan Anil",
        "Andrew Dai",
        "Orhan Firat",
        "Melvin Johnson",
        "Dmitry Lepikhin",
        "Alexandre Passos",
        "Siamak Shakeri",
        "Emanuel Taropa",
        "Paige Bailey",
        "Zhifeng Chen",
        "Eric Chu",
        "Jonathan Clark",
        "Laurent Shafey",
        "Yanping Huang",
        "Kathy Meier-Hellstern",
        "Gaurav Mishra",
        "Erica Moreira",
        "Mark Omernick",
        "Kevin Robinson",
        "Sebastian Ruder",
        "Yi Tay",
        "Kefan Xiao",
        "Yuanzhong Xu",
        "Yujing Zhang",
        "Gustavo Hernández Ábrego",
        "Junwhan Ahn",
        "Jacob Austin",
        "Paul Barham",
        "Jan Botha",
        "James Bradbury",
        "Siddhartha Brahma",
        "Kevin Brooks",
        "Michele Catasta",
        "Yong Cheng",
        "Colin Cherry",
        "Christopher Choquette-Choo",
        "Aakanksha Chowdhery",
        "Clément Crepy",
        "Shachi Dave",
        "Mostafa Dehghani",
        "Sunipa Dev",
        "Jacob Devlin",
        "Mark Díaz",
        "Nan Du",
        "Ethan Dyer",
        "Vladimir Feinberg",
        "Fangxiaoyu Feng",
        "Vlad Fienber",
        "Markus Freitag",
        "Xavier Garcia",
        "Sebastian Gehrmann",
        "Lucas Gonzalez"
      ],
      "venue": "",
      "doi": "10.48550/arXiv.2305.10403"
    },
    {
      "citation_id": "3",
      "title": "Unified-modal encoder-decoder pretraining for spoken language processing",
      "authors": [
        "Junyi Ao",
        "Rui Wang",
        "Long Zhou",
        "Chengyi Wang",
        "Shuo Ren",
        "Yu Wu",
        "Shujie Liu",
        "Tom Ko",
        "Qing Li",
        "Yu Zhang",
        "Zhihua Wei",
        "Yao Qian",
        "Jinyu Li",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.393"
    },
    {
      "citation_id": "4",
      "title": "Universlu: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network",
      "authors": [
        "Siddhant Arora",
        "Hayato Futami",
        "Jee-Weon Jung",
        "Yifan Peng",
        "Roshan Sharma",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Universlu: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network",
      "doi": "10.48550/ARXIV.2310.02973"
    },
    {
      "citation_id": "5",
      "title": "Qwen technical report. arxiv preprint",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report. arxiv preprint"
    },
    {
      "citation_id": "6",
      "title": "Pawel Swietojanski, and Verena Rieser",
      "authors": [
        "Emanuele Bastianelli",
        "Andrea Vanzo"
      ],
      "year": "2020",
      "venue": "Slurp: A spoken language understanding resource package",
      "arxiv": "arXiv:2011.13205"
    },
    {
      "citation_id": "7",
      "title": "Audiolm: A language modeling approach to audio generation",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matthew Sharifi",
        "Dominik Roblek",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2023.3288409"
    },
    {
      "citation_id": "8",
      "title": "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline",
      "authors": [
        "Hui Bu",
        "Jiayu Du",
        "Xingyu Na",
        "Bengu Wu",
        "Hao Zheng"
      ],
      "year": "2017",
      "venue": "the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "CM-GAN: Conformer-based Metric GAN for Speech Enhancement",
      "authors": [
        "Ruizhe Cao",
        "Sherif Abdulatif",
        "Bin Yang"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-517"
    },
    {
      "citation_id": "12",
      "title": "Exploration of efficient end-to-end ASR using discretized input from self-supervised learning",
      "authors": [
        "Xuankai Chang",
        "Brian Yan",
        "Yuya Fujita",
        "Takashi Maekaku",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Exploration of efficient end-to-end ASR using discretized input from self-supervised learning",
      "doi": "10.48550/ARXIV.2305.18108"
    },
    {
      "citation_id": "13",
      "title": "2021a. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "Guoguo Chen",
        "Shuzhou Chai",
        "Guanbo Wang",
        "Jiayu Du",
        "Wei-Qiang Zhang",
        "Chao Weng",
        "Dan Su",
        "Daniel Povey",
        "Jan Trmal",
        "Junbo Zhang"
      ],
      "venue": "2021a. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "arxiv": "arXiv:2106.06909"
    },
    {
      "citation_id": "14",
      "title": "2023a. Loss masking is not needed in decoder-only transformer for discrete-token based ASR",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "venue": "2023a. Loss masking is not needed in decoder-only transformer for discrete-token based ASR",
      "doi": "10.48550/ARXIV.2311.04534"
    },
    {
      "citation_id": "15",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "Vesper: A compact and effective pretrained model for speech emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Speechnet: A universal modularized model for speech processing tasks",
      "authors": [
        "Yi-Chen Chen",
        "Po-Han Chi",
        "Shu-Wen Yang",
        "Kai-Wei Chang",
        "Jheng-Hao Lin",
        "Sung-Feng Huang",
        "Da-Rong Liu",
        "Chi-Liang Liu",
        "Cheng-Kuang Lee",
        "Hungyi Lee"
      ],
      "year": "2021",
      "venue": "Speechnet: A universal modularized model for speech processing tasks"
    },
    {
      "citation_id": "17",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "doi": "10.48550/ARXIV.2311.07919"
    },
    {
      "citation_id": "18",
      "title": "Yossi Adi, and Alexandre Défossez. 2023. Simple and controllable music generation",
      "authors": [
        "Jade Copet",
        "Felix Kreuk",
        "Itai Gat",
        "Tal Remez",
        "David Kant",
        "Gabriel Synnaeve"
      ],
      "venue": "Yossi Adi, and Alexandre Défossez. 2023. Simple and controllable music generation"
    },
    {
      "citation_id": "19",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "Pengi: An audio language model for audio tasks",
      "doi": "10.48550/arXiv.2305.11834"
    },
    {
      "citation_id": "20",
      "title": "Clotho: An audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Aishell-2: Transforming mandarin asr research into industrial scale",
      "authors": [
        "Jiayu Du",
        "Xingyu Na",
        "Xuechen Liu",
        "Hui Bu"
      ],
      "year": "2018",
      "venue": "Aishell-2: Transforming mandarin asr research into industrial scale",
      "arxiv": "arXiv:1808.10583"
    },
    {
      "citation_id": "22",
      "title": "Funcodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec",
      "authors": [
        "Zhihao Du",
        "Shiliang Zhang",
        "Kai Hu",
        "Siqi Zheng"
      ],
      "year": "2023",
      "venue": "Funcodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec"
    },
    {
      "citation_id": "23",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "24",
      "title": "CLAP: learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Mahmoud Ismail",
        "Huaming Wang"
      ],
      "year": "2022",
      "venue": "CLAP: learning audio concepts from natural language supervision"
    },
    {
      "citation_id": "25",
      "title": "FSD50K: an open dataset of human-labeled sound events",
      "authors": [
        "Eduardo Fonseca",
        "Xavier Favory",
        "Jordi Pons",
        "Frederic Font",
        "Xavier Serra"
      ],
      "year": "2022",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "26",
      "title": "Funasr: A fundamental end-to-end speech recognition toolkit",
      "authors": [
        "Zhifu Gao",
        "Zerui Li",
        "Jiaming Wang",
        "Haoneng Luo",
        "Xian Shi",
        "Mengzhe Chen",
        "Yabin Li",
        "Lingyun Zuo",
        "Zhihao Du",
        "Zhangyu Xiao",
        "Shiliang Zhang"
      ],
      "year": "2023",
      "venue": "Funasr: A fundamental end-to-end speech recognition toolkit"
    },
    {
      "citation_id": "27",
      "title": "San-m: Memory equipped selfattention for end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ming Lei",
        "Ian Mcloughlin"
      ],
      "year": "2020",
      "venue": "San-m: Memory equipped selfattention for end-to-end speech recognition",
      "arxiv": "arXiv:2006.01713"
    },
    {
      "citation_id": "28",
      "title": "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition",
      "authors": [
        "Zhifu Gao",
        "Shiliang Zhang",
        "Ian Mcloughlin",
        "Zhijie Yan"
      ],
      "year": "2022",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "29",
      "title": "2023a. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models",
      "authors": [
        "Rongjie Huang",
        "Jiawei Huang",
        "Dongchao Yang",
        "Yi Ren",
        "Luping Liu",
        "Mingze Li",
        "Zhenhui Ye",
        "Jinglin Liu",
        "Xiang Yin",
        "Zhou Zhao"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning, ICML 2023"
    },
    {
      "citation_id": "30",
      "title": "2023b. Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "Rongjie Huang",
        "Mingze Li",
        "Dongchao Yang",
        "Jiatong Shi",
        "Xuankai Chang",
        "Zhenhui Ye",
        "Yuning Wu",
        "Zhiqing Hong",
        "Jiawei Huang",
        "Jinglin Liu",
        "Yi Ren",
        "Zhou Zhao",
        "Shinji Watanabe"
      ],
      "venue": "2023b. Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "doi": "10.48550/arXiv.2304.12995"
    },
    {
      "citation_id": "31",
      "title": "Surrey audiovisual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audiovisual expressed emotion (savee) database"
    },
    {
      "citation_id": "32",
      "title": "tiktoken: A fast BPE tokeniser for use with OpenAI's models",
      "authors": [
        "Shantanu Jain"
      ],
      "year": "2022",
      "venue": "tiktoken: A fast BPE tokeniser for use with OpenAI's models"
    },
    {
      "citation_id": "33",
      "title": "LMCodec: A low bitrate speech codec with causal transformer models",
      "authors": [
        "Teerapat Jenrungrot",
        "W Michael Chinen",
        "Bastiaan Kleijn"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "34",
      "title": "A study on data augmentation of reverberant speech for robust speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Michael Seltzer",
        "Sanjeev Khudanpur"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "35",
      "title": "Proceedings of the Seventh Conference on Machine Translation (WMT)",
      "authors": [
        "Tom Kocmi",
        "Rachel Bawden",
        "Ondřej Bojar",
        "Anton Dvorkovich",
        "Christian Federmann",
        "Mark Fishel",
        "Thamme Gowda",
        "Yvette Graham",
        "Roman Grundkiewicz",
        "Barry Haddow"
      ],
      "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT)"
    },
    {
      "citation_id": "36",
      "title": "The ntt dcase2020 challenge task 6 system: Automated audio captioning with keywords and sentence length estimation",
      "authors": [
        "Yuma Koizumi",
        "Daiki Takeuchi",
        "Yasunori Ohishi",
        "Noboru Harada",
        "Kunio Kashino"
      ],
      "year": "2020",
      "venue": "The ntt dcase2020 challenge task 6 system: Automated audio captioning with keywords and sentence length estimation",
      "arxiv": "arXiv:2007.00225"
    },
    {
      "citation_id": "37",
      "title": "Audiogen: Textually guided audio generation",
      "authors": [
        "Felix Kreuk",
        "Gabriel Synnaeve",
        "Adam Polyak",
        "Uriel Singer",
        "Alexandre Défossez",
        "Jade Copet",
        "Devi Parikh",
        "Yaniv Taigman",
        "Yossi Adi"
      ],
      "year": "2023",
      "venue": "Audiogen: Textually guided audio generation"
    },
    {
      "citation_id": "38",
      "title": "2023a. Audioldm: Text-to-audio generation with latent diffusion models",
      "authors": [
        "Haohe Liu",
        "Zehua Chen",
        "Yi Yuan",
        "Xinhao Mei",
        "Xubo Liu",
        "Danilo Mandic",
        "Wenwu Wang",
        "Mark Plumbley"
      ],
      "venue": "ICML"
    },
    {
      "citation_id": "39",
      "title": "Audioldm 2: Learning holistic audio generation with self-supervised pretraining",
      "authors": [
        "Haohe Liu",
        "Qiao Tian",
        "Yi Yuan",
        "Xubo Liu",
        "Xinhao Mei",
        "Qiuqiang Kong",
        "Yuping Wang",
        "Wenwu Wang",
        "Yuxuan Wang",
        "Mark Plumbley"
      ],
      "year": "2023",
      "venue": "Audioldm 2: Learning holistic audio generation with self-supervised pretraining",
      "doi": "10.48550/ARXIV.2308.05734"
    },
    {
      "citation_id": "40",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "41",
      "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "Xinhao Mei",
        "Chutong Meng",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Tom Ko",
        "Chengqi Zhao",
        "Mark Plumbley",
        "Yuexian Zou",
        "Wenwu Wang"
      ],
      "year": "2023",
      "venue": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "arxiv": "arXiv:2303.17395"
    },
    {
      "citation_id": "42",
      "title": "Lms with a voice: Spoken language modeling beyond speech tokens",
      "authors": [
        "Eliya Nachmani",
        "Alon Levkovitch",
        "Julian Salazar",
        "Chulayuth Asawaroengchai",
        "R Soroosh Mariooryad",
        "Michelle Skerry-Ryan",
        "Ramanovich"
      ],
      "year": "2023",
      "venue": "Lms with a voice: Spoken language modeling beyond speech tokens"
    },
    {
      "citation_id": "43",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report",
      "doi": "10.48550/arXiv.2303.08774"
    },
    {
      "citation_id": "44",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "The design for the wall street journal-based CSR corpus",
      "authors": [
        "Douglas Paul",
        "Janet Baker"
      ],
      "year": "1992",
      "venue": "ICSLP"
    },
    {
      "citation_id": "46",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "Kathleen Pichora-Fuller",
        "Kate Dupuis"
      ],
      "year": "2020",
      "venue": "Scholars Portal Dataverse"
    },
    {
      "citation_id": "47",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "48",
      "title": "Discrete audio representation as an alternative to mel-spectrograms for speaker and speech recognition",
      "authors": [
        "Krishna Puvvada",
        "Nithin Rao Koluguri",
        "Kunal Dhawan",
        "Boris Ginsburg"
      ],
      "year": "2023",
      "venue": "Discrete audio representation as an alternative to mel-spectrograms for speaker and speech recognition"
    },
    {
      "citation_id": "49",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "doi": "10.48550/arXiv.2212.04356"
    },
    {
      "citation_id": "50",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "51",
      "title": "Speechbrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong",
        "Ju-Chieh Chou",
        "Sung-Lin Yeh",
        "Szu-Wei Fu",
        "Chien-Feng Liao",
        "Elena Rastorgueva",
        "François Grondin",
        "William Aris",
        "Hwidong Na",
        "Yan Gao",
        "Renato Mori",
        "Yoshua Bengio"
      ],
      "year": "2021",
      "venue": "Speechbrain: A general-purpose speech toolkit"
    },
    {
      "citation_id": "52",
      "title": "Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface",
      "authors": [
        "Paul Rubenstein",
        "Chulayuth Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "Zalán Bapna",
        "Félix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Hannah Kharitonov",
        "Dirk Muckenhirn",
        "James Padfield",
        "Danny Qin",
        "Tara Rozenberg",
        "Johan Sainath",
        "Matthew Schalkwyk",
        "Michelle Sharifi",
        "Marco Ramanovich",
        "Alexandru Tagliasacchi",
        "Mihajlo Tudor",
        "Damien Velimirovic",
        "Jiahui Vincent",
        "Yongqiang Yu",
        "Vicky Wang",
        "Neil Zayats",
        "Yu Zeghidour",
        "Zhishuai Zhang",
        "Lukas Zhang",
        "Christian Zilka",
        "Kaitao Havnø Frank ; Shen",
        "Xu Song",
        "Dongsheng Tan",
        "Weiming Li",
        "Yueting Lu",
        "Zhuang"
      ],
      "year": "2023",
      "venue": "Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface",
      "doi": "10.48550/arXiv.2303.17580"
    },
    {
      "citation_id": "53",
      "title": "Seanet: A multi-modal speech enhancement network",
      "authors": [
        "Marco Tagliasacchi",
        "Yunpeng Li"
      ],
      "year": "2020",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "54",
      "title": "SALMONN: towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "SALMONN: towards generic hearing abilities for large language models",
      "doi": "10.48550/ARXIV.2310.13289"
    },
    {
      "citation_id": "55",
      "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurélien Rodriguez",
        "Armand Joulin"
      ],
      "venue": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "doi": "10.48550/arXiv.2302.13971"
    },
    {
      "citation_id": "56",
      "title": "A review of vector quantization techniques",
      "authors": [
        "A Vasuki",
        "Vanathi"
      ],
      "year": "2006",
      "venue": "IEEE Potentials"
    },
    {
      "citation_id": "57",
      "title": "Audiobox: Unified audio generation with natural language prompts",
      "authors": [
        "Apoorv Vyas",
        "Bowen Shi",
        "Matthew Le",
        "Andros Tjandra",
        "Yi-Chiao Wu",
        "Baishan Guo",
        "Jiemin Zhang",
        "Xinyue Zhang",
        "Robert Adkins",
        "William Ngan",
        "Jeff Wang",
        "Ivan Cruz",
        "Bapi Akula",
        "Akinniyi Akinyemi",
        "Brian Ellis",
        "Rashel Moritz",
        "Yael Yungster",
        "Alice Rakotoarison",
        "Liang Tan",
        "Chris Summers",
        "Carleigh Wood",
        "Joshua Lane",
        "Mary Williamson",
        "Wei-Ning Hsu"
      ],
      "year": "2023",
      "venue": "Audiobox: Unified audio generation with natural language prompts",
      "doi": "10.48550/ARXIV.2312.15821"
    },
    {
      "citation_id": "58",
      "title": "Covost 2 and massively multilingual speech-to-text translation",
      "authors": [
        "Changhan Wang",
        "Anne Wu",
        "Juan Pino"
      ],
      "year": "2020",
      "venue": "Covost 2 and massively multilingual speech-to-text translation",
      "arxiv": "arXiv:2007.10310"
    },
    {
      "citation_id": "59",
      "title": "2023a. Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "venue": "2023a. Neural codec language models are zero-shot text to speech synthesizers",
      "doi": "10.48550/ARXIV.2301.02111"
    },
    {
      "citation_id": "60",
      "title": "2023b. Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "authors": [
        "Tianrui Wang",
        "Long Zhou",
        "Ziqiang Zhang",
        "Yu Wu",
        "Shujie Liu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Jinyu Li",
        "Furu Wei"
      ],
      "venue": "2023b. Viola: Unified codec language models for speech recognition, synthesis, and translation",
      "doi": "10.48550/arXiv.2305.16107"
    },
    {
      "citation_id": "61",
      "title": "Learning to generalize to more: Continuous semantic augmentation for neural machine translation",
      "authors": [
        "Xiangpeng Wei",
        "Heng Yu",
        "Yue Hu",
        "Rongxiang Weng",
        "Weihua Luo",
        "Rong Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022"
    },
    {
      "citation_id": "62",
      "title": "Speechgen: Unlocking the generative power of speech language models with prompts",
      "authors": [
        "Haibin Wu",
        "Kai-Wei Chang",
        "Yuan-Kuei Wu",
        "Hungyi Lee"
      ],
      "year": "2023",
      "venue": "Speechgen: Unlocking the generative power of speech language models with prompts",
      "doi": "10.48550/arXiv.2306.02207"
    },
    {
      "citation_id": "63",
      "title": "2023a. Uniaudio: An audio foundation model toward universal audio generation",
      "authors": [
        "Dongchao Yang",
        "Jinchuan Tian",
        "Xu Tan",
        "Rongjie Huang",
        "Songxiang Liu",
        "Xuankai Chang",
        "Jiatong Shi",
        "Sheng Zhao",
        "Jiang Bian",
        "Xixin Wu",
        "Zhou Zhao",
        "Shinji Watanabe",
        "Helen Meng"
      ],
      "venue": "2023a. Uniaudio: An audio foundation model toward universal audio generation",
      "doi": "10.48550/ARXIV.2310.00704"
    },
    {
      "citation_id": "64",
      "title": "2023b. Diffsound: Discrete diffusion model for text-to-sound generation",
      "authors": [
        "Dongchao Yang",
        "Jianwei Yu",
        "Helin Wang",
        "Wen Wang",
        "Chao Weng",
        "Yuexian Zou",
        "Dong Yu"
      ],
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "65",
      "title": "2023c. Towards universal speech discrete tokens: A case study for ASR and TTS",
      "authors": [
        "Yifan Yang",
        "Feiyu Shen",
        "Chenpeng Du",
        "Ziyang Ma",
        "Kai Yu",
        "Daniel Povey",
        "Xie Chen"
      ],
      "venue": "2023c. Towards universal speech discrete tokens: A case study for ASR and TTS",
      "doi": "10.48550/ARXIV.2309.07377"
    },
    {
      "citation_id": "66",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "Binbin Zhang",
        "Hang Lv",
        "Pengcheng Guo",
        "Qijie Shao",
        "Chao Yang",
        "Lei Xie",
        "Xin Xu",
        "Hui Bu",
        "Xiaoyu Chen",
        "Chenchen Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "67",
      "title": "2023a. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "Dong Zhang",
        "Shimin Li",
        "Xin Zhang",
        "Jun Zhan",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "venue": "2023a. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "doi": "10.48550/arXiv.2305.11000"
    },
    {
      "citation_id": "68",
      "title": "Bstc: A large-scale chinese-english speech translation dataset",
      "authors": [
        "Ruiqing Zhang",
        "Xiyang Wang",
        "Chuanqiang Zhang",
        "Zhongjun He",
        "Hua Wu",
        "Zhi Li",
        "Haifeng Wang",
        "Ying Chen",
        "Qinfei Li"
      ],
      "year": "2021",
      "venue": "Bstc: A large-scale chinese-english speech translation dataset",
      "arxiv": "arXiv:2104.03575"
    },
    {
      "citation_id": "69",
      "title": "Françoise Beaufays, and Yonghui Wu. 2023b. Google USM: scaling automatic speech recognition beyond 100 languages",
      "authors": [
        "Yu Zhang",
        "Wei Han",
        "James Qin",
        "Yongqiang Wang",
        "Ankur Bapna",
        "Zhehuai Chen",
        "Nanxin Chen",
        "Bo Li",
        "Vera Axelrod",
        "Gary Wang",
        "Zhong Meng",
        "Ke Hu",
        "Andrew Rosenberg",
        "Rohit Prabhavalkar",
        "Daniel Park",
        "Parisa Haghani",
        "Jason Riesa",
        "Ginger Perng",
        "Hagen Soltau",
        "Trevor Strohman",
        "Bhuvana Ramabhadran",
        "Tara Sainath",
        "Pedro Moreno",
        "Chung-Cheng Chiu",
        "Johan Schalkwyk"
      ],
      "venue": "Françoise Beaufays, and Yonghui Wu. 2023b. Google USM: scaling automatic speech recognition beyond 100 languages",
      "doi": "10.48550/arXiv.2303.01037"
    },
    {
      "citation_id": "70",
      "title": "3d-speaker: A large-scale multi-device, multi-distance, and multi-dialect corpus for speech representation disentanglement",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "year": "2023",
      "venue": "3d-speaker: A large-scale multi-device, multi-distance, and multi-dialect corpus for speech representation disentanglement"
    },
    {
      "citation_id": "71",
      "title": "Mmspeech: Multi-modal multi-task encoderdecoder pre-training for speech recognition",
      "authors": [
        "Xiaohuan Zhou",
        "Jiaming Wang",
        "Zeyu Cui",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou",
        "Chang Zhou"
      ],
      "year": "2022",
      "venue": "Mmspeech: Multi-modal multi-task encoderdecoder pre-training for speech recognition",
      "arxiv": "arXiv:2212.00500"
    }
  ]
}