{
  "paper_id": "2303.13802v1",
  "title": "Decoupled Multimodal Distilling For Emotion Recognition",
  "published": "2023-03-24T04:54:44Z",
  "authors": [
    "Yong Li",
    "Yuanzhi Wang",
    "Zhen Cui"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acoustic modalities. Despite the impressive performance of previous MER approaches, the inherent multimodal heterogeneities still haunt and the contribution of different modalities varies significantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each modality. Specially, the representation of each modality is decoupled into two parts, i.e., modalityirrelevant/-exclusive spaces, in a self-regression manner. DMD utilizes a graph distillation unit (GD-Unit) for each decoupled part so that each GD can be performed in a more specialized and effective manner. A GD-Unit consists of a dynamic graph where each vertice represents a modality and each edge indicates a dynamic knowledge distillation. Such GD paradigm provides a flexible knowledge transfer manner where the distillation weights can be automatically learned, thus enabling diverse crossmodal knowledge transfer patterns. Experimental results show DMD consistently obtains superior performance than state-of-the-art MER methods. Visualization results show the graph edges in DMD exhibit meaningful distributional patterns w.r.t. the modality-irrelevant/-exclusive feature spaces. Codes are released at https://github.com/mdswyz/DMD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human multimodal emotion recognition (MER) aims to perceive the sentiment attitude of humans from video clips  [13, 17] . The video flows involve time-series data from various modalities, e.g., language, acoustic, and vision. This rich multimodality facilitates us in understanding human behaviors and intents from a collaborative perspective. Recently, MER has become one of the most active re-* The corresponding author. (a) illustrates the significant emotion recognition discrepancies using unimodality, adapted from Mult  [28] . (b) shows the conventional cross-modal distillation. (c) shows our proposed decoupled multimodal distillation (DMD) method. DMD consists of two graph distillation (GD) units: homogeneous GD and heterogeneous GD. The decoupled GD paradigm decreases the burden of absorbing knowledge from the heterogeneous data and allows each GD to be performed in a more specialized and effective manner.\n\nsearch topics of affective computing with abundant appealing applications, such as intelligent tutoring systems  [24] , product feedback estimation  [18] , and robotics  [15] . For MER, different modalities in the same video segment are often complementary to each other, providing extra cues for semantic and emotional disambiguation. The core part of MER is multimodal representation learning and fusion, in which a model aims to encode and integrate representations from multiple modalities to understand the emotion behind the raw data. Despite the achievement of the mainstream MER methods  [7, 28, 33] , the intrinsic heterogeneities among different modalities still perplex us and increase the difficulty of robust multimodal representation learning. Different modalities, e.g., image, language, and acoustic, contain different ways of conveying semantic information. Typically, the language modality consists of limited transcribed texts and has more abstract semantics than nonverbal behaviors. As illustrated in Fig.  1 (a) , language plays the most important role in MER and the intrinsic heterogeneities result in significant performance discrepancies among different modalities  [25, 28, 31] .\n\nOne way to mitigate the conspicuous modality heterogeneities is to distill the reliable and generalizable knowledge from the strong modality to the weak modality  [6] , as illustrated in Fig.  1 (b) . However, such manual assignment for the distillation direction or weights should be cumbersome because there are various potential combinations. Instead, the model should learn to automatically adapt the distillation according to different examples, e.g, many emotions are easier to recognize via language while some are easier by vision. Furthermore, the significant feature distribution mismatch cross the modalities makes the direct crossmodal distillation sub-optimal  [21, 37] .\n\nTo this end, we propose a decoupled multimodal distillation (DMD) method to learn dynamic distillations across modalities, as illustrated in Fig.  1 (c ). Typically, the features of each modality are decoupled into modality-irrelevant/exclusive spaces via shared encoder and the private encodes, respectively. As to achieve the feature decoupling, we devise a self-regression mechanism that predicts the decoupled modality features and then regresses them selfsupervisedly. To consolidate the feature decoupling, we incorporate a margin loss that regularizes the proximity in relationships of the representations across modalities and emotions. Consequently, the decoupled GD paradigm would decrease the burden of absorbing knowledge from the heterogeneous data and allows each GD to be performed in a more specialized and effective manner.\n\nBased on the decoupled multimodal feature spaces, DMD utilizes a graph distillation unit (GD-Unit) in each space so that the crossmodal knowledge distillation can be performed in a more specialized and effective manner. A GD-Unit consists of a graph that (1) vertices representing the representations or logits from the modalities and (2) edges indicating the knowledge distillation directions and weights. As the distribution gap among the modalityirrelevant (homogeneous) features is sufficiently reduced, GD can be directly applied to capture the inter-modality semantic correlations. For the modality-exclusive (heterogeneous) counterparts, we exploit the multimodal transformer  [28]  to build the semantic alignment and bridge the distribution gap. The cross-modal attention mechanism in the multimodal transformer reinforces the multimodal rep-resentations and reduces the discrepancy between the highlevel semantic concepts that exist in different modalities. For simplification, we respectively name the distillation on the decoupled multimodal features as homogeneous graph knowledge distillation (HomoGD) and heterogeneous graph knowledge distillation (HeteroGD). The reformulation allows us to explicitly explore the interaction between different modalities in each decoupled space.\n\nThe contributions of this work can be summarized as:\n\n• We propose a decoupled multimodal distillation framework, Decoupled Multimodal Distillation (DMD), to learn the dynamic distillations across modalities for robust MER. In DMD, we explicitly decouple the multimodal representations into modality-irrelevant/-exclusive spaces to facilitate KD on the two decoupled spaces. DMD provides a flexible knowledge transfer manner where the distillation directions and weights can be automatically learned, thus enabling flexible knowledge transfer patterns.\n\n• We conduct comprehensive experiments on public MER datasets and obtain superior or comparable results than the state-of-the-arts. Visualization results verify the feasibility of DMD and the graph edges exhibit meaningful distributional patterns w.r.t. Ho-moGD and HeteroGD.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal emotion recognition (MER) aims to infer human sentiment from the language, visual and acoustic information embedded in the video clips. The heterogeneity across modalities can provide various levels of information for MER. The mainstream MER approaches can be divided into two categories: fusion strategy-based  [14, 33, 34]  and crossmodal attention-based  [13, 17, 28] .\n\nThe former aims to design sophisticated multimodal fusion strategies to generate discriminative multimodal representations, e.g., Zadeh et al.  [33]  designed a Tensor Fusion Network (TFN) that can fuse multimodal information progressively. However, the inherent heterogeneity and the intrinsic information redundancy across modalities hinders the fusion between the multimodal features. Therefore, some work aims to explore the characteristics and commonalities of multimodal representations via feature decoupling to facilitate more effective multimodal representation fusion  [7, 29, 32] . Hazarika et al.  [7]  decomposed multimodal features into modality-invariant/-specific components to learn the refined multimodal representations. The decoupled multimodal representations reduce the information redundancy and provide a holistic view of the multimodal data. Recently, crossmodal attention-based approaches have driven the development of MER since they learn the cross-modal correlations to obtain the reinforced modality representation. A representative work is MulT  [28] . This work proposes the multimodal transformer that consists of a cross-modal attention mechanism to learn the potential adaption and correlations from one modality to another, thereby achieving semantic alignment between modalities. Lv et al.  [17]  designed a progressive modality reinforcement method based on  [28] , it aims to learn the potential adaption and correlations from multimodal representation to unimodal representation. Our proposed DMD has an essential difference with the previous feature-decoupling methods  [7, 29, 32]  because DMD is capable of distilling cross-modal knowledge in the decoupled feature spaces.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "The concept of knowledge distillation (KD) was first proposed in  [9]  to transfer knowledge via minimizing the KL-Divergence between prediction logits of teachers and students. Subsequently, various KD methods were proposed  [4, 5, 20, 39]  based on  [9]  and further extended to distillation between intermediate features  [1, 8, 22, 27] .\n\nMost KD methods focus on transferring knowledge from the teacher to the student, while some recent studies have used graph structures to explore the effective message passing mechanism between multiple teachers and students with multiple instances of knowledge  [16, 19, 38] . Zhang et al.  [38]  proposed a graph distillation (GD) method for video classification, where each vertex represented a selfsupervised teacher and edges represented the direction of distillation from multiple self-supervised teachers to the student. Luo et al.  [16]  considered the modality discrepancy to incorporate privileged information from the source domain and modeled a directed graph to explore the relationship between different modalities. Each vertex represented a modality and the edges indicated the connection strength (i.e., distillation strength) between one modality and another. Different from them, we aim to use exclusive GD-Units in the decoupled feature spaces to facilitate effective cross-modality distillation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Proposed Method",
      "text": "The framework of our DMD is illustrated in Fig.  2 . It mainly consists of three parts: multimodal feature decoupling, homogeneous GD (HomoGD), hetergeneous GD (HeteroGD). Considering the significant distribution mismatch of modalities, we decouple multimodal representations into homogeneous and heterogeneous multimodal features through learning shared and exclusive multimodal encoders. The decoupling detail is introduced in Sec. 3.1. To facilitate a flexible knowledge transfer, we next distill the knowledge from homo/heterogeneous features, which are framed in two graph distillation units (GD-Unit), i.e., Ho-moGD and HeteroGD. In HomoGD, homogeneous multimodal features are mutually distilled to compensate the representation ability for each other. In HeteroGD, multimodal transformers are introduced to explicitly build inter-modal correlations and semantic alignment for further distilling. The GD detail is introduced in Sec. 3.2. Finally, the refined multimodal features through distilling are adaptively fused for robust MER. Below, we present the details of the three parts of DMD.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Feature Decoupling",
      "text": "We consider three modalities, i.e., language (L), visual (V), acoustic (A). Firstly, we exploit three separate 1D temporal convolutional layers to aggregate temporal information and obtain the low-level multimodal features: X m ∈ R Tm×dm , where m ∈ {L, V, A} indicates a modality. After this shallow encoding, each modality preserves the input temporal dimension to facilitate handling unaligned and aligned cases simultaneously. Moreover, all modalities are scaled to the same feature dimension, i.e., d\n\nTo decouple the multimodal features into homogeneous (modality-irrelevant) part X com m and heterogeneous (modality-exclusive) part X prt m , we exploit a shared multimodal encoder E com and three private encoders E prt m to explicitly predict the decoupled features. Formally,\n\nTo distinguish the differences between X com m and X prt m and mitigate the feature ambiguity, we synthesize the vanilla coupled features X m in a self-regression manner. Mathematically speaking, we concatenate X com m and X prt m for each modality and exploit a private decoder D m to produce the coupled feature, i.e., D m ([X com m , X prt m ]). Subsequently, the coupled feature will be re-encoded via the private encoders E prt m to regress the heterogeneous features. The notation [.] means feature concatenation. Formally, the discrepancy between the vanilla/synthesized coupled multimodal features can be formulated as:\n\nFurther, the discrepancy between the vanilla/synthesized heterogeneous features can be formulated as:\n\nFor the above reconstruction losses, it still cannot guarantee the complete feature decoupling. In fact, information can freely leak between representations, e.g., all the modality information can be merely encoded in X prt m so that the decoders can easily synthesize the input, leaving homogeneous multimodal features meaningless. To consolidate the In feature decoupling, DMD exploits the decoupled homo-/heterogeneous multimodal features X com m / X prt m via the shared and exclusive encoders, respectively. X prt m will be reconstructed in a self-regression manner (Sec. 3.1). Subsequently, X com m will be fed into a GD-Unit for adaptive knowledge distillation in HomoGD. In HeteroGD, X prt m are reinforced to Z prt →m via multimodal transformers to bridge the distribution gap. The GD-Unit in HeteroGD takes Z prt →m as input for distillation (Sec. 3.2). Finally, X com m and Z prt →m will be adaptively fused for MER.\n\nfeature decoupling, we argue that homogeneous representations from the same emotion but different modalities should be more similar than those from the same modality but different emotions. To this end, we define a margin loss as:\n\nwhere we collect a triple tuple set\n\nis the class label of sample i, and cos(•, •) means the cosine similarity between two feature vectors. The loss in Eq. 4 constrains the homogeneous features that belong to the same emotion but different modalities or vice versa to differ, and thereby avoids deriving trivial homogeneous features. α is a distance margin. The distances of positive samples (same emotion; different modalities) are constrained to be smaller than that of negative samples (same modality; different emotions) by the margin α. Considering that the decoupled features respectively capture the modality-irrelevant/-exclusive characteristics, we further formulate a soft orthogonality to reduce the information redundancy between the homogeneous and the heterogeneous multimodal features:\n\nFinally, we combine these constraints to form the decoupling loss,\n\nwhere γ is the balance factor.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gd With Decoupled Multimodal Features",
      "text": "For the decoupled homogeneous and heterogeneous multimodal features, we design a graph distillation unit (GD-Unit) on each of them to conduct adaptive knowledge distillation. Typically, a GD-Unit consists of a directed graph G. Let v i denote a node w.r.t a modality and w i→j indicates the distillation strength from modality v i to v j . The distillation from v i to v j is defined as the difference between their corresponding logits, denoted with i→j . Let E denotes the distillation matrix with E ij = i→j . For a target modality j, the weighted distillation loss can be formulated by considering the injection edges as,\n\nwhere N (v j ) represents the set of vertices injected to v j .\n\nTo learn a dynamic and adaptive weight that corresponds to the distillation strength w, we propose to encode the modality logits and the representations into the graph edges. Formally,  (8)  where [•, •] means feature concatenation, g is a fullyconnected (FC) layer with the learnable parameters θ 2 , and f is a FC layer for regressing logits with the parameters θ 1 . The graph edge weights W with W ij = w i→j can be constructed and learned by repetitively applying Eq. 8 over all pairs of modalities. To reduce the scale effects, we normalize W through the sof tmax operation. Consequently, the graph distillation loss to all modalities can be written as:\n\nwhere means element-wise product. Obviously, the distillation graph in a GD-Unit provides a base for learning dynamic inter-modality interactions. Meanwhile, it facilitates a flexible knowledge transfer manner where the distillation strengths can be automatically learned, thus enabling diverse knowledge transfer patterns. Below, we elaborate on the details of HomoGD and HeteroGD.\n\nHomoGD. As illustrated in Fig.  2 , for the decoupled homogeneous features X com m , as the distribution gap among the modalities is already reduced sufficiently, we input the features X com m and the corresponding logits f (X com m ) to a GD-Unit and calculate the graph edge matrix W and the distillation loss matrix E according to Eq. 8. Then, the overall distillation loss L homo dtl can be naturally obtained via Eq. 9.\n\nHeteroGD. The decoupled heterogeneous features X prt m focus on the diversity and the unique characteristics of each modality, and thus exhibit a significant distribution gap. To mitigate this issue, we exploit the multimodal transformer  [28]  to bridge the feature distribution gap and build the modality adaptation. The core of the multimodal transformer is the crossmodal attention unit (CA), which receives features from a pair of modalities and fuses crossmodal information. Take the language modality X prt L as the source and the visual modality X prt V as the target, the cross-modal attention can be defined as:\n\nL P k , and V L = X prt L P v where P q , P k , P v are the learnable parameters. The individual head of can be expressed as:\n\nwhere Z prt L→V is the enhanced features from Language to Visual, d means the dimension of Q V and K L . For the three modalities in MER, each modality will be reinforced by the two others and the resulting features will be concatenated.\n\nFor each target modality, we concatenate all enhanced features from other modalities to the target as the reinforced features, denotes with Z prt →m , which are used in the distillation loss function as L hetero dtl that can be naturally obtained via Eq. 9.\n\nFeature fusion. We use the reinforced heterogeneous features Z prt →m and the vanilla decoupled homogeneous features X com m for adaptive feature fusion with an adaptive weight learned from each of them. Hereby, we obtain the fused feature for multimodal emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Objective Optimization",
      "text": "We integrate the above losses to reach the full objective:\n\nwhere L task is the emotion task related loss (here mean absolute error), L dtl = L homo dtl + L hetero dtl means the distillation losses generated by HomoGD and HeteroGD, and λ 1 , λ 2 control the importance of different constraints.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "Datasets. We evaluate DMD on CMU-MOSI  [35]  and CMU-MOSEI  [36]  datasets. The experiments are conducted under the word-aligned and unaligned settings for a more comprehensive comparison. CMU-MOSI consists of 2,199 short monologue video clips. The acoustic and visual features in CMU-MOSI are extracted at a sampling rate of 12.5 and 15 Hz, respectively. Among the samples, 1,284, 229 and 686 samples are used as training, validation and testing set. CMU-MOSEI contains 22,856 samples of movie review video clips from YouTube (approximately 10× the size of CMU-MOSI). The acoustic and visual features were extracted at a sampling rate of 20 and 15 Hz, respectively. According to the predetermined protocol, 16,326 samples are used for training, the remainng 1,871 and 4,659 samples are used for for validation and testing. Each sample in CMU-MOSI and CMU-MOSEI was labeled with a sentiment score which ranges from -3 to 3, including highly negative, negative, weakly negative, neutral, weakly positive, positive, and highly positive. Following previous work  [13, 17] , we evaluate the MER performance using the following metrics: 7-class accuracy (ACC 7 ), binary accuracy (ACC 2 ) and F1 score.\n\nImplementation details. On the two datasets, we extract the unimodal language features via GloVe  [23]  and obtain 300-dimensional word features. For a fair comparison with  [7, 32]  under the aligned setting, we additationally exploit a BERT-base-uncased pre-trained model  [10]  to obtain a 768-dimensional hidden state as the word features. For visual modality, each video frame was encoded via Facet  [2]  to represent the presence of the totally 35 facial action units  [11, 12] . The acoustic modality was pro-    [36]  45.0 76.9 77.0 RAVEN  [30]  50.0 79.1 79.5 MCTN  [26]  49.6 79.8 80.6 MulT  [28]  51.8 82.5 82.3 PMR  [17]  52. cessed by COVAREP  [3]  to obtain the 74-dimensional features. The detailed neural network configurations in DMD are listed in the supplementary file. The optimal setting for λ 1 , λ 2 , γ was set as 0.1, 0.05, 0.1 via the MER performance on the validation set. We implemented all the experiments using PyTorch on a RTX 3090 GPU with 24GB memory. We set the training batch size as 16 and trained DMD for 30 epoches until convergence.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "We compare DMD with the current state-of-the-art MER methods under the same dataset settings (unaligned or aligned), including EF-LSTM, LF-LSTM, TFN  [33] , LMF  [14] , MFM  [29] , RAVEN  [30] , Graph-MFN  [36] , MCTN  [26] , MulT  [28] , PMR  [17] , MICA  [13] , MISA  [7] , and FDMER  [32] .\n\nTab. 1 and Tab. 2 illustrate the comparison on CMU-MOSI and CMU-MOSEI datasets, respectively. Obviously, our proposed DMD obtains superior MER accuracy than other MER approaches under the unaligned and aligned settings. Compared with the feature-disentangling-based MER methods  [7, 29, 32] , our proposed DMD obtains consistent improvements, indicating the feasibility of the incorporated GD-Unit, which is capable of perceiving the various intermodality dynamics. For a further inverstigation, we will visualize the learned graph edges in each GD-Unit in Sec. 4.2. DMD consistently outperforms the methods  [13, 17, 28]  that use multimodal transformer to learn crossmodal interactions and perform multimodal fusion. The reasons are two-fold: (1) DMD takes the modality-irrelevant/-exclusive spaces into consideration concurrently and recudes the information redundancy via feature decoupling. (2) DMD exploits twin GD-Units to adaptively distil knowledge among the modalities. On CMU-MOSEI dataset, Graph-MFN  [36]    illustrates unsatisfactory results because the heterogeneity and distribution gap across modalities hinder the learning of the modality fusion in it. As a comparison, the multimodal features into DMD are decoupled into modalityirrelevant/-exclusive spaces. For the latter space, we use multimodal transformer to bridge the distribution gap and align the high-level semantics, thereby reducing the burden of absorbing knowledge from the heterogeneous features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Quantitative analysis. We evaluate the effects of DMD's key components, including feature decoupling (FD), HomoGD, crossmodal attention unit (CA), Het-eroGD. The results are illustrated in Tab. 3. We conclude the observations below.\n\nFirstly, FD enhances MER performance significantly, it indicates the decoupled and refined features can reduce information redundancy and provide discriminative multimodal features. To further prove the effectiveness of FD, we conduct experiments on our baseline model with and without FD on MOSEI dataset. As shown in Tab. 4, FD brings consistent improvements for each unimodality. Meantime, the performance gap for the three modalities is reduced as the standard deviations of ACC 2 and F1 are both decreased. Secondly, combing FD with HomoGD brings further benefits. Although the homogeneous features are embedded in the same-dimension space, there still exists different discriminative capabilities for the modalities. HomoGD can improve the weak modalities through GD. To verify this, we conduct experiments with or without HomoGD on MO-SEI dataset. The ACC 2 results are: 80.9% vs. 82.4% for language, 56.5% vs. 61.8% for vision, 54.4% vs. 64.1% for audio. However, conducting HeteroGD without the crossmodal attention units will generate degraded performance, indicating the multimodal transformer plays a key role in bridging the multimodal distribution gap. Thirdly, with CA units and HeteroGD, DMD obtains conspicuous improvements, suggesting the importance of the taking advantage of the modality-exclusive features for robust MER.\n\nBesides, we compare our proposed DMD with the classical MulT  [28]  for further investigation. The results are shown in Tab. 5, where MulT (w/ GD) means we add a GD-Unit on MulT to conduct adaptive knowledge with the reinforced multimodal features. Essentially, the core difference between MulT (w/ GD) and DMD is that DMD incorporates feature decoupling. The quantitative comparison in Tab. 5 shows that DMD obtains consistent improvements than MulT (w/ GD). It suggests decoupling the multimodal features before distillation is feasible and reasonable. Furthermore, DMD achieves more significant improvements than the vanilla MulT, indicating the benefits of combing the feature decoupling and the graph distillation mechanisms.\n\nVisualization of the decoupled features. We visualize the decoupled homogeneous and heterogeneous features of DMD, DMD (w/o Hom., Het.), DMD (w/o Het.) in Fig.  3  and Fig.  4  for a quantitative comparison. DMD (w/o Hom., Het.) denotes DMD without HomoGD and HeteroGD. Besides, DMD (w/o Het.) means DMD without HeteroGD. To visualize the homogeneous features, we randomly select 28 samples (four samples for each emotion category) in the testing set of the CMU-MOSEI dataset For the heterogeneous features, we randomly select 400 samples in the testing set of the CMU-MOSEI dataset. The features of the selected samples are projected into a 2D space by t-SNE.\n\nFor the homogeneous multimodal features of DMD and DMD (w/o Het.), the samples belonging to the same emotion category naturally cluster together due to their intermodal homogeneity. With the decoupled homogeneous fea- tures but without graph distillation in DMD (w/o Hom., Het.), the samples merely show basic separability for the binary non-negative and negative categories. However, the samples are not distinguishable under the 7class setting, indicating the features are not so discriminative than that of DMD or DMD (w/o Het.). The comparison between DMD (w/o Hom., Het.) and DMD, and the comparison between DMD (w/o Hom., Het.) and DMD (w/o Het.) verifies the effectiveness of the graph distillation on the homogeneous multimodal features.\n\nIn the heterogeneous space, due to its inter-modal heterogeneity, the features of different samples should cluster by modalities. As shown in Fig  4 , DMD shows the best feature separability, indicating the complementarity between modalities is mostly enhanced. DMD (w/o Hom., Het.) and DMD (w/o Het.) show less feature separability than DMD, indicating the importance of the graph distillation on the heterogeneous multimodal features.\n\nVisualization of graph edges in the GD-Units. We show the dynamic edges in each GD-Unit in Fig.  5  for analysis. Each graph edge corresponds to the strength of a directed distillation. We conclude the observations as:  (1)  The distillation in HomoGD are mainly dominated by L → A and L → V . This is because the decoupled homogeneous language modality still plays the most critical role and outperforms visual or acoustic modality with significant advantages. For binary MER on the CMU-MOSEI dataset, language, visual, acoustic modality respectively obtains 80.9%, 56.5%, 54.4% accuracy using the decoupled homogeneous features. (2) For HeteroGD, L → A, L → V , and V → A are dominated. An interesting phenomenon is that V → A emerges. This should be reasonable because the visual modality enhanced its feature discriminability via the multimodal transformer mechanism in HeteroGD. Actually, the three modalities obtain 84.5%, 83.8%, 71.0% accuracy, respectively. Conclusively, the graph edges learn meaningful patterns for adaptive crossmodal distillation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "Within this paper we have proposed a decoupled multimodal distillation method (DMD) for MER. Our method is inspired by the observation that the contribution of different modalities varies significantly. Therefore, robust MER can be achieved by distilling the reliable and generalizable knowledge across the modalities. To mitigate the modality heterogeneities, DMD decouples the modal features into modality-irrelevant/-exclusive spaces in a selfregression manner. Two GD-Units are incorporated for each decoupled features to facilitate adaptive cross-modal distillation. Quantitative and qualitative experiments consistently demonstrate the effectiveness of DMD. A limitation is that DMD does not explicitly consider the intra-modal interactions. We will explore this in future work.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) illustrates the signiﬁcant emotion recognition dis-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), language",
      "page": 2
    },
    {
      "caption": "Figure 1: (b). However, such manual assign-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c). Typically, the features",
      "page": 2
    },
    {
      "caption": "Figure 2: The framework of DMD. Given the input multimodal data, DMD encodes their respective shallow features eXm, where m ∈",
      "page": 4
    },
    {
      "caption": "Figure 2: , for the decoupled ho-",
      "page": 5
    },
    {
      "caption": "Figure 3: t-SNE visualization of decoupled homogeneous space on MOSEI. DMD shows the promising emotion category (binary or",
      "page": 7
    },
    {
      "caption": "Figure 4: Visualization of the decoupled heterogeneous features",
      "page": 7
    },
    {
      "caption": "Figure 5: Illustration of the graph edges in HomoGD and HeteroGD. In (a), L →A and L →V are dominated because the homogeneous",
      "page": 8
    },
    {
      "caption": "Figure 4: , DMD shows the best fea-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EF-LSTM\nLF-LSTM\nGraph-MFN [36]\nRAVEN [30]\nMCTN [26]\nMulT [28]\nPMR [17]\nDMD (Ours)": "MISA [7]∗\nFDMER [32]∗\nDMD (Ours)∗",
          "Aligned": "Aligned",
          "47.4\n78.2\n77.9\n48.8\n80.6\n80.6\n45.0\n76.9\n77.0\n50.0\n79.1\n79.5\n49.6\n79.8\n80.6\n51.8\n82.5\n82.3\n52.5\n83.3\n82.6\n53.7\n85.0\n84.9": "52.2\n85.5\n85.3\n54.1\n86.1\n85.8\n54.5\n86.6\n86.6"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "3",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Born again neural networks",
      "authors": [
        "Tommaso Furlanello",
        "Zachary Lipton",
        "Michael Tschannen",
        "Laurent Itti",
        "Anima Anandkumar"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Online knowledge distillation via collaborative learning",
      "authors": [
        "Qiushan Guo",
        "Xinjiang Wang",
        "Yichao Wu",
        "Zhipeng Yu",
        "Ding Liang",
        "Xiaolin Hu",
        "Ping Luo"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "6",
      "title": "Cross modal distillation for supervision transfer",
      "authors": [
        "Saurabh Gupta",
        "Judy Hoffman",
        "Jitendra Malik"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2006",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "8",
      "title": "Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation",
      "authors": [
        "Byeongho Heo",
        "Jeesoo Kim",
        "Sangdoo Yun"
      ],
      "year": "1921",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "NIPS workshop"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Lee Kristina"
      ],
      "year": "2019",
      "venue": "Proceedings of naacL-HLT"
    },
    {
      "citation_id": "11",
      "title": "Learning representations for facial actions from unlabeled videos",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Selfsupervised representation learning from videos for facial action unit detection",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion",
      "authors": [
        "Tao Liang",
        "Guosheng Lin",
        "Lei Feng",
        "Yan Zhang",
        "Fengmao Lv"
      ],
      "year": "2006",
      "venue": "ICCV"
    },
    {
      "citation_id": "14",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the con-ference"
    },
    {
      "citation_id": "15",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Zhentao Liu",
        "Min Wu",
        "Weihua Cao",
        "Luefeng Chen",
        "Jianping Xu",
        "Ri Zhang",
        "Mengtian Zhou",
        "Junwei Mao"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "16",
      "title": "Graph distillation for action detection with privileged modalities",
      "authors": [
        "Zelun Luo",
        "Jun-Ting Hsieh",
        "Lu Jiang",
        "Juan Niebles",
        "Li Fei-Fei"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "17",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2006",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Sentiment analysis of blogs by combining lexical knowledge with text classification",
      "authors": [
        "Prem Melville",
        "Wojciech Gryc",
        "Richard Lawrence"
      ],
      "year": "2009",
      "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining"
    },
    {
      "citation_id": "19",
      "title": "Knowledge transfer graph for deep collaborative learning",
      "authors": [
        "Soma Minami",
        "Tsubasa Hirakawa",
        "Takayoshi Yamashita",
        "Hironobu Fujiyoshi"
      ],
      "year": "2020",
      "venue": "ACCV"
    },
    {
      "citation_id": "20",
      "title": "Improved knowledge distillation via teacher assistant",
      "authors": [
        "Mehrdad Seyed Iman Mirzadeh",
        "Ang Farajtabar",
        "Nir Li",
        "Akihiro Levine",
        "Hassan Matsukawa",
        "Ghasemzadeh"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "21",
      "title": "Knowledge distillation with distribution mismatch",
      "authors": [
        "Dang Nguyen",
        "Sunil Gupta",
        "Trong Nguyen",
        "Santu Rana",
        "Phuoc Nguyen",
        "Truyen Tran",
        "Ky Le",
        "Shannon Ryan",
        "Svetha Venkatesh"
      ],
      "year": "2021",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "22",
      "title": "Relational knowledge distillation",
      "authors": [
        "Wonpyo Park",
        "Dongju Kim",
        "Yan Lu",
        "Minsu Cho"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in affective tutoring systems: Collection of ground-truth data",
      "authors": [
        "Sintija Petrovica",
        "Alla Anohina-Naumeca",
        "Hazim Kemal Ekenel"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "25",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "27",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "Adriana Romero",
        "Nicolas Ballas",
        "Samira Kahou",
        "Antoine Chassang",
        "Carlo Gatta",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Fitnets: Hints for thin deep nets"
    },
    {
      "citation_id": "28",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2007",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "29",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2006",
      "venue": "ICLR"
    },
    {
      "citation_id": "30",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "31",
      "title": "A text-centered shared-private framework via crossmodal prediction for multimodal sentiment analysis",
      "authors": [
        "Yang Wu",
        "Zijie Lin",
        "Yanyan Zhao",
        "Bing Qin",
        "Li-Nan Zhu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "32",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2006",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "35",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "36",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Matching distributions between model and data: Cross-domain knowledge distillation for unsupervised domain adaptation",
      "authors": [
        "Bo Zhang",
        "Xiaoming Zhang",
        "Yun Liu",
        "Lei Cheng",
        "Zhoujun Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Better and faster: knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification",
      "authors": [
        "Chenrui Zhang",
        "Yuxin Peng"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "39",
      "title": "Decoupled knowledge distillation",
      "authors": [
        "Borui Zhao",
        "Quan Cui",
        "Renjie Song",
        "Yiyu Qiu",
        "Jiajun Liang"
      ],
      "year": "2022",
      "venue": "CVPR"
    }
  ]
}