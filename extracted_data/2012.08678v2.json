{
  "paper_id": "2012.08678v2",
  "title": "Improved Digital Therapy For Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study",
  "published": "2020-12-16T00:08:51Z",
  "authors": [
    "Peter Washington",
    "Haik Kalantarian",
    "John Kent",
    "Arman Husic",
    "Aaron Kline",
    "Emilie Leblanc",
    "Cathy Hou",
    "Onur Cezmi Mutlu",
    "Kaitlyn Dunlap",
    "Yordan Penev",
    "Maya Varma",
    "Nate Tyler Stockham",
    "Brianna Chrisman",
    "Kelley Paskov",
    "Min Woo Sun",
    "Jae-Yoon Jung",
    "Catalin Voss",
    "Nick Haber",
    "Dennis Paul Wall"
  ],
  "keywords": [
    "computer vision",
    "emotion recognition",
    "affective computing",
    "autism spectrum disorder",
    "pediatrics",
    "mobile health",
    "digital therapy",
    "convolutional neural network",
    "machine learning",
    "artificial intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Background: Automated emotion classification could aid those who struggle to recognize emotions, including children with developmental behavioral conditions such as autism. However, most computer vision emotion recognition models are trained on adult emotion and therefore underperform when applied to child faces. \n Objective: We designed a strategy to gamify the collection and labeling of child emotion-enriched images to boost the performance of automatic child emotion recognition models to a level closer to what will be needed for digital health care approaches. \n Methods: We leveraged our prototype therapeutic smartphone game, GuessWhat, which was designed in large part for children with developmental and behavioral conditions, to gamify the secure collection of video data of children expressing a variety of emotions prompted by the game. Independently, we created a secure web interface to gamify the human labeling effort, called HollywoodSquares, tailored for use by any qualified labeler. We gathered and labeled 2155 videos, 39,968 emotion frames, and 106,001 labels on all images. With this drastically expanded pediatric emotion-centric database (>30 times larger than existing public pediatric emotion data sets), we trained a convolutional neural network (CNN) computer vision classifier of happy, sad, surprised, fearful, angry, disgust, and neutral expressions evoked by children. \n Results: The classifier achieved a 66.9% balanced accuracy and 67.4% F1-score on the entirety of the Child Affective Facial Expression (CAFE) as well as a 79.1% balanced accuracy and 78% F1-score on CAFE Subset A, a subset containing at least 60% human agreement on emotions labels. This performance is at least 10% higher than all previously developed classifiers evaluated against CAFE, the best of which reached a 56% balanced accuracy even when combining \"anger\" and \"disgust\" into a single class. \n Conclusions: This work validates that mobile games designed for pediatric therapies can generate high volumes of domain-relevant data sets to train state-of-the-art classifiers to perform tasks helpful to precision health efforts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automated emotion classification can serve in pediatric care solutions, particularly to aid those who struggle to recognize emotion, such as children with autism who have trouble with emotion evocation and recognizing emotions displayed by others  [1] [2] [3] . In prior work, computer vision models for emotion recognition  [4] [5] [6]  used in digital therapeutics have shown significant treatment effects in children with autism  [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] . The increasing use of signals from sensors on mobile devices, such as the selfie camera, opens many possibilities for real-time analysis of image data for continuous phenotyping and repeated diagnoses in home settings  [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] . However, facial emotion classifiers and the underlying data sets on which they are trained have been tailored to neurotypical adults, as demonstrated by repeatedly low performance on image data sets of pediatric emotion expressions  [34] [35] [36] [37] [38] [39] .\n\nThe Child Affective Facial Expression (CAFE) data set is currently the most popular facial expression data set pertaining to children. Prior machine learning efforts that do not include CAFE images in the training set have reached 56% accuracy on CAFE  [36, 37, 39] , even after combining facial expressions (eg, \"anger\" and \"disgust\") into a single class, thus limiting granularity. We do not discuss prior publications that report higher accuracy using subsets of the CAFE data set in the training and testing sets. This overall lack of performance in prior work highlights the need for developing facial emotion classifiers that work for children. With a lack of labeled data being the fundamental bottleneck to achieving clinical-grade performance, low-cost and speedy data generation and labeling techniques are pertinent.\n\nAs a first step toward the creation of a large-scale data set of child emotions, we have previously designed GuessWhat, a dual-purpose smartphone app that serves as a therapeutic for children with autism while simultaneously collecting highly structured image data enriched for emoting in children. GuessWhat was designed for children aged 2 and above to encourage prosocial interaction with a gameplay partner (eg, mom or dad), focusing the camera on the child while presenting engaging but challenging prompts for the child to try to act out  [40] [41] [42] [43] . We have previously tested GuessWhat's potential to increase socialization in children with autism as well as its potential to collect structured videos of children emoting facial expressions  [44] . In addition to collecting videos enriched with emotions, GuessWhat gameplay generates user-derived labels of emotion by leveraging the charades-style gameplay structure of the therapy.\n\nHere, we document the full pipeline for training a classifier using emotion-enriched video streams coming from GuessWhat gameplay, resulting in a state-of-the-art pediatric facial emotion classifier that outperforms all prior classifiers when evaluated on CAFE. We first recruited parents and children from around the world to play GuessWhat and share videos recorded by the smartphone app during gameplay. We next extracted frames from the videos, automatically discarding some frames through quality control algorithms, and uploaded the frames on a custom behavioral annotation labeling platform named HollywoodSquares. We prioritized the high entropy frames and shared them with a group of 9 human annotators who annotated emotions in the frames. In total, we have collected 39,968 unique labeled frames of emotions that appear in the CAFE data set. Using the resulting frames and labels, we trained a facial emotion classifier that can distinguish happy, sad, surprised, fearful, angry, disgust, and neutral expressions in naturalistic images, achieving state-of-the-art performance on CAFE and outperforming existing classifiers by over 10%. This work demonstrates that therapeutic games, while primarily providing a behavioral intervention, can simultaneously generate sufficient data for training state-of-the-art domain-specific computer vision classifiers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Collection",
      "text": "The primary methodological contribution of this work is a general-purpose paradigm and pipeline (Figure  1 ) consisting of (1) passive collection of prelabeled structured videos from therapeutic interventions, (2) active learning to rank the collected frames leveraging the user-derived labels generated during gameplay, (3) human annotation of the frames in the order produced in the previous step, and (4) training a classifier while artificially augmenting the training set. We describe our instantiation of this general paradigm in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ethical Considerations",
      "text": "All study procedures, including data collection, were approved by the Stanford University Institutional Review Board (IRB number 39562) and the Stanford University Privacy Office. In addition, informed consent was obtained from all participants, all of whom had the opportunity to participate in the study without sharing videos.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Recruitment",
      "text": "To recruit child video subjects, we ran a marketing campaign to gather rich and diverse video inputs of children playing GuessWhat while evoking a range of emotions. We posted advertisements on social media (Facebook, Instagram, and Twitter) and contacted prior study participants for other digital smartphone therapeutics developed by the lab  [13] [14] [15] . All recruitment and study procedures were approved by the Stanford University IRB.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "User Interfaces",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Guesswhat Smartphone Therapeutic",
      "text": "GuessWhat is a mobile autism therapy implemented on iOS and Android, which has been previously documented as a useful tool for the collection of structured video streams of children behaving in constrained manners  [40] [41] [42] [43] [44] , including evocation of targeted emotions. GuessWhat features a charades game where the parents place the phone on their forehead facing the child, while the child acts out the emotion prompt displayed on the screen. The front-facing camera on the phone records a video of the child in addition to corresponding prompt metadata. All sessions last for 90 seconds. Upon approval by the parent, each session video is uploaded to a Simple Storage Service (S3) bucket on Amazon Web Services (AWS). The app has resulted in 2155 videos shared by 456 unique children. Parents are asked to sign an electronic consent and assent form prior to playing GuessWhat. After each gameplay session, parents can (1) delete the videos, (2) share the videos with the research team only, or (3) share the videos publicly.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotions Considered",
      "text": "We sought labels for Paul Ekman's list of six universal emotions: anger, disgust, fear, happiness, sadness, and surprise  [45] [46] [47] [48] . Ekman originally included contempt in the list of emotions but has since revised the list of universal emotions.\n\nBecause CAFE does not include labels of contempt, we did not train our classifier to predict contempt. We added a seventh category named neutral, indicating the absence of an expressed emotion. Our aim was to train a 7-way emotion classifier distinguishing among Ekman's 6 universal emotions plus neutral.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hollywoodsquares Frame Labeling",
      "text": "We developed a frame-labeling website named HollywoodSquares. The website provides human labelers with an interface to speedily annotate a sequential grid of frames (Figure  2 ) that were collected during the GuessWhat gameplay. To enable rapid annotation, HollywoodSquares enables users to label frames by pressing hot keys, where each key corresponds to a particular emotion label. To provide a label, users can hover their mouse over a frame and press the hot key corresponding to the emotion they want to label. As more frames are collected by GuessWhat, they continue to appear on the interface. Because the HollywoodSquares system displays over 20 images on the screen at once, it encourages rapid annotation and enables simultaneous engagement by many independent labelers. This permits rapid convergence of a majority rules consensus on image labels.\n\nWe ran a labeling contest with 9 undergraduate and high school annotators, where we challenged each annotator to produce labels that would result in the highest performing classifier on the CAFE data set. Raters were aged between 15 and 24 years and were from the Bay Area, Northeastern United States, and Texas. The raters included 2 males and 7 females. For the frames produced by each individual annotator, we trained a ResNet-152 model (see Model Training). We updated annotators about the number of frames they labeled each week and the performance of the classifier trained with their individual labels. We awarded a cash prize to the annotator with the highest performance at the end of the 9-week labeling period. HollywoodSquares was also used for a testing phase, during which iterations of the frame-labeling practices were made between the research and annotation teams. All the labeled frames acquired during this testing phase were discarded for final classifier training.\n\nAll annotators were registered as research team members through completion of the Health Insurance Portability and Accountability Act of 1996 and Collaborative Institutional Training Initiative training protocols in addition to encrypting their laptop with Stanford Whole Disk Encryption. This provided annotators with read-only access to all the videos and derived frames from GuessWhat gameplay that were shared with the research team.\n\nThe final labels were chosen by the following process. If all annotators agreed unanimously about the final frame label, then this label was assigned as the final frame label. If disagreements existed between raters, then the emotion gameplay prompt associated with that frame (the \"automatic label\") was assigned as the final label for that frame, as long as at least 1 of the human annotators agreed with the automatic label. If disagreements existed between raters but the automatic label did not match any human annotations, then the frame was not included in the final training data set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Machine Learning",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Training",
      "text": "We leveraged an existing CNN architecture, ResNet-152  [49] , with pretrained weights from ImageNet  [50] . We used categorical cross entropy loss and Adam optimization with a learning rate of 3 × 10 -4 , with β 1 set to .99 and β 2 set to .999. We retrained every layer of the network until the training accuracy converged. The model converged when it did not improve against a validation data set for 20 consecutive epochs. We applied the following data augmentation strategies in conjunction and at random for each training image and each batch of training: rotation of frames between -15 and 15 degrees, zooming by a factor between 0.85 and 1.15, shifting images in every direction by up to 1/10th of the width and height, changing brightness by a factor between 80% and 120%, and potential horizontal flipping.\n\nThe CNN was trained in parallel on 16 graphics processing unit (GPU) cores with a p2.16xlarge Elastic Cloud Compute instance on AWS using the Keras library in Python with a Tensorflow 2 backend. With full GPU usage, the training time was 35 minutes and 41 seconds per epoch for a batch size of 1643, translating to US $14.4 per hour.\n\nWe trained 2 versions of the model, with 1 exclusively using non-GuessWhat public data set frames from (1) the Japanese Female Facial Expression (JAFFE)  [51] , (2) a random subset of 30,000 AffectNet  [52]  images (a subset was acquired to avoid an out of memory error), and (3) the Extended Cohn-Kanade (CK+) data set  [53] ; the other model was trained with these public data set frames plus all 39,968 labeled and relevant GuessWhat frames.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Model Evaluation",
      "text": "We evaluated our models against the entirety of the CAFE data set  [54] , a set of front-facing images of racially and ethnically diverse children aged 2 to 8 years expressing happy, sad, surprised, fear, angry, fearful, and neutral emotions. CAFE is currently the largest data set of facial expressions from children and has become a standard benchmark for this field.\n\nAlthough existing studies have evaluated models exclusively against the entirety of the CAFE data set  [34] [35] [36] [37] [38] [39] , we additionally evaluated them on Subset A and Subset B of CAFE, as defined by the authors of the data set. Subset A contains images that were identified with an accuracy of 60% or above by 100 adult participants  [54] , with a Cronbach α internal consistency score of .82 (versus .77 for the full CAFE data set). Subset B contains images showing \"substantial variability while minimizing floor and ceiling effects\"  [54] , with a Cronbach α score of .768 (close to the score of .77 for the full data set).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Frame Processing",
      "text": "The HollywoodSquares annotators processed 106,001 unique frames (273,493 including the testing phase and 491,343 unique labels when counting multiple labels for the same frame as a different label). Of the 106,001 unique frames labeled, 39,968 received an emotion label corresponding to 1 of the 7 CAFE emotions (not including the testing phase labels). Table  1  contains the number of frames that were included in the training set for each emotion class, including how many children and videos are represented for each emotion category. The frames that were not included received labels of \"None\" (corresponding to a situation where no face or an incomplete face appears in the frame), \"Unknown\" (corresponding to the face not expressing a clear emotion), or \"Contempt\" (corresponding to the face not expressing an emotion in the CAFE set). The large number of curated frames displaying emotion demonstrates the usefulness of HollywoodSquares in filtering out emotion events from noisy data streams. The lack of balance across emotion categories is a testament particularly to the difficulty of evoking anger and sadness as well as disgust and fear, although to a lesser extent.\n\nOf the children who completed 1 session of the Emoji challenge in GuessWhat and uploaded a video to share with the research team, 75 were female, 141 were male, and 51 did not specify their gender. Table  2  presents the racial and ethnic makeup of the participant cohort. Representative GuessWhat frames and cropped faces used to train the classifier, obtained from the subset of participants who consented explicitly to public sharing of their images, are displayed in Figure  3 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Performance On Cafe, Cafe-Defined Subsets, And Cafe Subset Balanced In Terms Of Race, Gender, And Emotions",
      "text": "The ResNet-152 network trained on the entire labeled HollywoodSquares data set as well as the JAFFE, AffectNet subset, and CK+ data sets achieved a balanced accuracy of 66.9% and an F1-score of 67.4% on the entirety of the CAFE data set (confusion matrix in Figure  4 ). When only the HollywoodSquares data set was included in the training set, the model achieved a balanced accuracy of 64.12% and an F1-score of 64.2%. When only including the JAFFE, AffectNet subset, and CK+ sets, the classifier achieved an F1-score of 56.14% and a balanced accuracy of 52.5%, highlighting the contribution of the HollywoodSquares data set. To quantify the contribution of the neural network architecture itself, we compared the performance of several state-of-the-art neural network architectures when only including the HollywoodSquares data set in the training set (Table  3 ). We evaluated the following models: ResNet152V2  [49] , ResNet50V2  [49] , InceptionV3  [55] , MobileNetV2  [56] , DenseNet121  [57] , DenseNet201  [57] , and Xception  [58] . The same training conditions and hyperparameters were used across all models. We found that ResNet152V2 performed better than the other networks when trained with our data, so we used this model for the remainder of our experiments.\n\nThe performance improved, resulting in a balanced accuracy of 79.1% and an F1-score of 78% on CAFE Subset A (confusion matrix in Figure  5 ), a subset containing more universally accepted emotions labels. When only including the non-GuessWhat public images in the training set, the model achieved a balanced accuracy of 65.3% and an F1-score of 69.2%. On CAFE Subset B, the balanced accuracy was 66.4% and the F1-score was 67.2% (confusion matrix in Figure  6 ); the balanced accuracy was 57.2% and F1-score was 57.3% when exclusively training on the non-GuessWhat public images.   [58]  a Default hyperparameters were used for all networks.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Classifier Performance Based On Image Difficulty",
      "text": "CAFE images were labeled by 100 adults, and the percentage of participants who labeled the correct class are reported with the data set  [54] . We binned frames into 10 difficulty classes (ie, 90%-100% correct human labels, 80%-90% correct human labels, etc). Figure  7  shows that our classifier performs exceedingly well on unambiguous images. Of the 233 images with 90%-100% agreement between the original CAFE labelers, our classifier correctly classifies 90.1% of the images. The true label makeup of these images is as follows: 131 happy, 58 neutral, 20 anger, 9 sad, 8 surprise, 7 disgust, and 0 fear images. This confirms that humans have trouble identifying nonhappy and nonneutral facial expressions. Of the 455 images with 80%-100% agreement between the original CAFE labelers, our classifier correctly classifies 81.1% of the images.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Principal Results",
      "text": "Through the successful application of an in-the-wild child developmental health therapeutic that simultaneously captures video data, we show that a pipeline for intelligently and continuously labeling image frames collected passively from mobile gameplay can generate sufficient training data for a high-performing computer vision classifier (relative to prior work). We curated a data set that contains images enriched for naturalistic facial expressions of children, including but not limited to children with autism.\n\nWe demonstrate the best-performing pediatric facial emotion classifier to date according to the CAFE data set. The best-performing classifiers evaluated in earlier studies involving facial emotion classification on the CAFE data set, including images from CAFE in the training set, achieved an accuracy of up to 56% on CAFE  [36, 37, 39]  and combined \"anger\" and \"disgust\" into a single class. By contrast, we achieved a balanced accuracy of 66.9% and an F1-score of 67.4% without including any CAFE images in the training set. This is a clear illustration of the power of parallel data curation from distributed mobile devices in conjunction with deep learning, and this approach can possibly be generalized to the collection of training data for other domains.\n\nWe collected a sufficiently large training sample to alleviate the need for extracting facial keypoint features, as was the case in prior works. Instead, we used the unaltered images as inputs to a deep CNN.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "A major limitation of this work is the use of 7 discrete and distinct emotion categories. Some images in the training set might have exhibited more than 1 emotion, such as \"happily surprised\" or \"fearfully surprised.\" This could be addressed in future work by a more thorough investigation of the final emotion classes. Another limitation is that similar to existing emotion data sets, our generated data set contains fake emotion evocations by the children. This is due to limitations imposed by ethics review committees and the IRB who, understandably so, do not allow provoking real fear or sadness in participants, especially young children who may have a developmental delay. This issue of fake emotion evocation has been documented in prior studies  [4, 5, 59, 60] . Finding a solution to this issue that would appease ethical review committees is an open research question.\n\nAnother limitation is that we did not address the possibility of complex or compound emotions  [61] . A particular facial expression can consist of multiple universal expressions. For example, \"happily surprised,\" \"fearfully surprised,\" and even \"angrily surprised\" are all separate subclasses of \"surprised.\" We have not separated these categories in this study. We recommend that future studies explore the possibility of predicting compound and complex facial expressions.\n\nThere are several fruitful avenues for future work. The paradigm of passive data collection during mobile intervention gameplay could be expanded to other digital intervention modalities, such as wearable autism systems with front-facing cameras  [7, 8, 11, [13] [14] [15] [16] [17] . This paradigm can also be applied toward the curation of data and subsequent training of other behavioral classifiers. Relevant computer vision models for diagnosing autism could include computer vision-powered quantification of hand stimming, eye contact, and repetitive behavior, as well as audio-based classification of abnormal prosody, among others.\n\nThe next major research step will be to evaluate how systems like GuessWhat can benefit from the incorporation of the machine learning models back into the system in a closed-loop fashion while preserving privacy and trust  [62] . Quantification of autistic behaviors during gameplay via machine learning models trained with gameplay videos can enable a feedback loop that provides a dynamic and adaptive therapy for the child. Models can be further personalized to the child's unique characteristics, providing higher performance through customized fine-tuning of the network.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusions",
      "text": "We have demonstrated that gamified digital therapeutic interventions can generate sufficient data for training state-of-the-art computer vision classifiers, in this case for pediatric facial emotion. Using this data curation and labeling paradigm, we trained a state-of-the-art 7-way pediatric facial emotion classifier.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) consisting of",
      "page": 2
    },
    {
      "caption": "Figure 1: Pipeline of the model training process. Structured videos enriched with child emotion evocation are collected from a mobile autism therapeutic",
      "page": 2
    },
    {
      "caption": "Figure 2: ) that were collected during the GuessWhat gameplay.",
      "page": 3
    },
    {
      "caption": "Figure 2: HollywoodSquares rating interface. Annotators use keyboard shortcuts and the mouse to speedily annotate a sequence of frames acquired",
      "page": 3
    },
    {
      "caption": "Figure 3: JMIR Pediatr Parent 2022 | vol. 5 | iss. 2 | e26760 | p. 4",
      "page": 4
    },
    {
      "caption": "Figure 3: Example of frames collected from GuessWhat gameplay, including examples of cropped (A) and original (B) frames. We have displayed",
      "page": 6
    },
    {
      "caption": "Figure 4: ). When only the",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrix for the entirety of the Child Affective Facial Expression data set.",
      "page": 7
    },
    {
      "caption": "Figure 5: ), a subset containing more universally",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion matrix for Child Affective Facial Expression Subset A.",
      "page": 8
    },
    {
      "caption": "Figure 6: Confusion matrix for Child Affective Facial Expression Subset B.",
      "page": 9
    },
    {
      "caption": "Figure 7: shows that our classifier performs",
      "page": 9
    },
    {
      "caption": "Figure 7: Classifier performance versus original CAFE annotator performance for 10 difficulty bins. The classifier tends to perform well when humans",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: contains the number of frames that were included in the training",
      "page": 4
    },
    {
      "caption": "Table 2: presents the racial and ethnic makeup of",
      "page": 4
    },
    {
      "caption": "Table 1: Emotions represented in the HollywoodSquares data set, including how many children and videos are represented for each emotion category.",
      "page": 5
    },
    {
      "caption": "Table 2: Representation of race and ethnicity of children whose who played the “Emoji” charades category and uploaded a video to the cloud.",
      "page": 5
    },
    {
      "caption": "Table 3: Comparison of several popular neural network architectures trained on the same data seta.",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial emotion recognition in autism spectrum disorders: a review of behavioral and neuroimaging studies",
      "authors": [
        "M Harms",
        "A Martin",
        "G Wallace"
      ],
      "year": "2010",
      "venue": "Neuropsychol Rev",
      "doi": "10.1007/s11065-010-9138-6"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in autism: coordinating faces and voices",
      "authors": [
        "R Hobson",
        "J Ouston",
        "A Lee"
      ],
      "year": "2009",
      "venue": "Psychol Med",
      "doi": "10.1017/S0033291700009843"
    },
    {
      "citation_id": "3",
      "title": "Emotion regulation and internalizing symptoms in children with autism spectrum disorders",
      "authors": [
        "C Rieffe",
        "P Oosterveld",
        "M Terwogt",
        "S Mootz",
        "E Van Leeuwen",
        "L Stockmann"
      ],
      "year": "2011",
      "venue": "Autism",
      "doi": "10.1177/1362361310366571"
    },
    {
      "citation_id": "4",
      "title": "Cognitive emotions recognition in e-learning: exploring the role of age differences and personality traits",
      "authors": [
        "B Carolis",
        "D 'errico",
        "F Paciello",
        "M Palestra"
      ],
      "year": "2019",
      "venue": "Methodologies and Intelligent Systems for Technology Enhanced Learning, 9th International Conference",
      "doi": "10.1007/978-3-030-23990-9_12"
    },
    {
      "citation_id": "5",
      "title": "Socio-affective technologies [SI 1156 T]",
      "authors": [
        "B De Carolis",
        "D 'errico",
        "F Rossano"
      ],
      "year": "2020",
      "venue": "Multimed Tools Appl",
      "doi": "10.1007/s11042-020-10015-3"
    },
    {
      "citation_id": "6",
      "title": "Enhancing mouth-based emotion recognition using transfer learning",
      "authors": [
        "V Franzoni",
        "G Biondi",
        "D Perri",
        "O Gervasi"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20185222"
    },
    {
      "citation_id": "7",
      "title": "Feasibility testing of a wearable behavioral aid for social learning in children with autism",
      "authors": [
        "J Daniels",
        "N Haber",
        "C Voss",
        "J Schwartz",
        "S Tamura",
        "A Fazel"
      ],
      "year": "2018",
      "venue": "Appl Clin Inform",
      "doi": "10.1055/s-0038-1626727"
    },
    {
      "citation_id": "8",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism",
      "authors": [
        "J Daniels",
        "J Schwartz",
        "C Voss",
        "N Haber",
        "A Fazel",
        "A Kline"
      ],
      "year": "2018",
      "venue": "NPJ Digital Med",
      "doi": "10.1038/s41746-018-0035-3"
    },
    {
      "citation_id": "9",
      "title": "A practical approach to real-time neutral feature subtraction for facial expression recognition",
      "authors": [
        "N Haber",
        "C Voss",
        "A Fazel",
        "T Winograd",
        "D Wall"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV.2016.7477675"
    },
    {
      "citation_id": "10",
      "title": "Making emotions transparent: Google Glass helps autistic kids understand facial expressions through augmented-reaiity therapy",
      "authors": [
        "N Haber",
        "C Voss",
        "D Wall"
      ],
      "year": "2020",
      "venue": "IEEE Spectrum",
      "doi": "10.1109/MSPEC.2020.9055973"
    },
    {
      "citation_id": "11",
      "title": "Superpower glass",
      "authors": [
        "A Kline",
        "C Voss",
        "P Washington",
        "N Haber",
        "H Schwartz",
        "Q Tariq"
      ],
      "year": "2019",
      "venue": "GetMobile: Mobile Comp Comm",
      "doi": "10.1145/3372300.3372308"
    },
    {
      "citation_id": "12",
      "title": "Toward continuous social phenotyping: analyzing gaze patterns in an emotion recognition task for children with autism through wearable smart glasses",
      "authors": [
        "A Nag",
        "N Haber",
        "C Voss",
        "S Tamura",
        "J Daniels",
        "J Ma"
      ],
      "year": "2020",
      "venue": "J Med Internet Res",
      "doi": "10.2196/13810"
    },
    {
      "citation_id": "13",
      "title": "The potential for machine learning-based wearables to improve socialization in teenagers and adults with autism spectrum disorder-reply",
      "authors": [
        "C Voss",
        "N Haber",
        "D Wall"
      ],
      "year": "2019",
      "venue": "JAMA Pediatr",
      "doi": "10.1001/jamapediatrics.2019.2969"
    },
    {
      "citation_id": "14",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels",
        "A Kline",
        "N Haber",
        "P Washington"
      ],
      "year": "2019",
      "venue": "JAMA Pediatr",
      "doi": "10.1001/jamapediatrics.2019.0285"
    },
    {
      "citation_id": "15",
      "title": "Superpower glass: delivering unobtrusive real-time social cues in wearable systems",
      "authors": [
        "C Voss",
        "P Washington",
        "N Haber",
        "A Kline",
        "J Daniels",
        "A Fazel"
      ],
      "year": "2016",
      "venue": "UbiComp '16: Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct. 2016 Sep Presented at: UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
      "doi": "10.1145/2968219.2968310"
    },
    {
      "citation_id": "16",
      "title": "A wearable social interaction aid for children with autism",
      "authors": [
        "P Washington",
        "C Voss",
        "N Haber",
        "S Tanaka",
        "J Daniels",
        "C Feinstein"
      ],
      "year": "2016",
      "venue": "CHI EA '16: Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems. 2016 May Presented at: CHI'16: CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/2851581.2892282"
    },
    {
      "citation_id": "17",
      "title": "SuperpowerGlass: a wearable aid for the at-home therapy of children with autism",
      "authors": [
        "P Washington",
        "C Voss",
        "A Kline",
        "N Haber",
        "J Daniels",
        "A Fazel"
      ],
      "year": "2017",
      "venue": "Proc ACM Interact Mob Wearable Ubiquitous Technol",
      "doi": "10.1145/3130977"
    },
    {
      "citation_id": "18",
      "title": "Machine learning for early detection of autism (and other conditions) using a parental questionnaire and home video screening",
      "authors": [
        "H Abbas",
        "F Garberson",
        "E Glover",
        "D Wall"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Big Data (Big Data)",
      "doi": "10.1109/bigdata.2017.8258346"
    },
    {
      "citation_id": "19",
      "title": "Multi-modular AI approach to streamline autism diagnosis in young children",
      "authors": [
        "H Abbas",
        "F Garberson",
        "S Liu-Mayo",
        "E Glover",
        "D Wall"
      ],
      "year": "2020",
      "venue": "Sci Rep",
      "doi": "10.1038/s41598-020-61213-w"
    },
    {
      "citation_id": "20",
      "title": "Testing the accuracy of an observation-based classifier for rapid detection of autism risk",
      "authors": [
        "M Duda",
        "J Kosmicki",
        "D Wall"
      ],
      "year": "2014",
      "venue": "Transl Psychiatry",
      "doi": "10.1038/tp.2014.65"
    },
    {
      "citation_id": "21",
      "title": "Use of machine learning for behavioral distinction of autism and ADHD",
      "authors": [
        "M Duda",
        "R Ma",
        "N Haber",
        "D Wall"
      ],
      "year": "2016",
      "venue": "Transl Psychiatry",
      "doi": "10.1038/tp.2015.221"
    },
    {
      "citation_id": "22",
      "title": "Crowdsourced validation of a machine-learning classification system for autism and ADHD",
      "authors": [
        "M Duda",
        "N Haber",
        "J Daniels",
        "D Wall"
      ],
      "year": "1133",
      "venue": "Transl Psychiatry",
      "doi": "10.1038/tp.2017.86"
    },
    {
      "citation_id": "23",
      "title": "The potential of accelerating early detection of autism through content analysis of YouTube videos",
      "authors": [
        "V Fusaro",
        "J Daniels",
        "M Duda",
        "T Deluca",
        "D' Angelo",
        "O Tamburello"
      ],
      "year": "2014",
      "venue": "PLoS ONE",
      "doi": "10.1371/journal.pone.0093533"
    },
    {
      "citation_id": "24",
      "title": "Sparsifying machine learning models identify stable subsets of predictive features for behavioral detection of autism",
      "authors": [
        "S Levy",
        "M Duda",
        "N Haber",
        "D Wall"
      ],
      "year": "2017",
      "venue": "Mol Autism",
      "doi": "10.1186/s13229-017-0180-6"
    },
    {
      "citation_id": "25",
      "title": "Feature replacement methods enable reliable home video analysis for machine learning detection of autism",
      "authors": [
        "E Leblanc",
        "P Washington",
        "M Varma",
        "K Dunlap",
        "Y Penev",
        "A Kline"
      ],
      "year": "2020",
      "venue": "Sci Rep",
      "doi": "10.1038/s41598-020-76874-w"
    },
    {
      "citation_id": "26",
      "title": "The quantified brain: a framework for mobile device-based assessment of behavior and neurological function",
      "authors": [
        "D Stark",
        "R Kumar",
        "C Longhurst",
        "D Wall"
      ],
      "year": "2017",
      "venue": "Appl Clin Inform",
      "doi": "10.4338/ACI-2015-12-LE-0176"
    },
    {
      "citation_id": "27",
      "title": "Mobile detection of autism through machine learning on home video: a development and prospective validation study",
      "authors": [
        "Q Tariq",
        "J Daniels",
        "J Schwartz",
        "P Washington",
        "H Kalantarian",
        "D Wall"
      ],
      "year": "2018",
      "venue": "PLoS Med",
      "doi": "10.1371/journal.pmed.1002705"
    },
    {
      "citation_id": "28",
      "title": "Detecting developmental delay and autism through machine learning models using home videos of Bangladeshi children: development and validation study",
      "authors": [
        "Q Tariq",
        "S Fleming",
        "J Schwartz",
        "K Dunlap",
        "C Corbin",
        "P Washington"
      ],
      "year": "2019",
      "venue": "J Med Internet Res",
      "doi": "10.2196/13822"
    },
    {
      "citation_id": "29",
      "title": "Validity of online screening for autism: crowdsourcing study comparing paid and unpaid diagnostic tasks",
      "authors": [
        "P Washington",
        "H Kalantarian",
        "Q Tariq",
        "J Schwartz",
        "K Dunlap",
        "B Chrisman"
      ],
      "year": "2019",
      "venue": "J Med Internet Res",
      "doi": "10.2196/13668"
    },
    {
      "citation_id": "30",
      "title": "Precision telemedicine through crowdsourced machine learning: testing variability of crowd workers for video-based autism feature recognition",
      "authors": [
        "P Washington",
        "E Leblanc",
        "K Dunlap",
        "Y Penev",
        "A Kline",
        "K Paskov"
      ],
      "year": "2020",
      "venue": "J Pers Med",
      "doi": "10.3390/jpm10030086"
    },
    {
      "citation_id": "31",
      "title": "Selection of trustworthy crowd workers for telemedical diagnosis of pediatric autism spectrum disorder",
      "authors": [
        "P Washington",
        "E Leblanc",
        "K Dunlap",
        "Y Penev",
        "M Varma",
        "J Jung"
      ],
      "year": "2020",
      "venue": "Biocomputing 2021: Proceedings of the Pacific Symposium",
      "doi": "10.1142/9789811232701_0002"
    },
    {
      "citation_id": "32",
      "title": "Feature selection and dimension reduction of social autism data",
      "authors": [
        "P Washington",
        "K Paskov",
        "H Kalantarian",
        "N Stockham",
        "C Voss",
        "A Kline"
      ],
      "year": "2020",
      "venue": "Biocomputing",
      "doi": "10.1142/9789811215636_0062"
    },
    {
      "citation_id": "33",
      "title": "Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry",
      "authors": [
        "P Washington",
        "N Park",
        "P Srivastava",
        "C Voss",
        "A Kline",
        "M Varma"
      ],
      "year": "2020",
      "venue": "Biol Psychiatry Cogn Neurosci Neuroimaging",
      "doi": "10.1016/j.bpsc.2019.11.015"
    },
    {
      "citation_id": "34",
      "title": "Towards automated classification of emotional facial expressions",
      "authors": [
        "L Baker",
        "V Lobue",
        "E Bonawitz",
        "P Shafto"
      ],
      "year": "2017",
      "venue": "Towards automated classification of emotional facial expressions"
    },
    {
      "citation_id": "35",
      "title": "Annealed label transfer for face expression recognition",
      "authors": [
        "C Florea",
        "L Florea",
        "M Badea",
        "C Vertan",
        "A Racoviteanu"
      ],
      "year": "2019",
      "venue": "BMVC",
      "doi": "10.1109/ecai50035.2020.9223242"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition using facial expressions in children using the NAO Robot",
      "authors": [
        "A Lopez-Rincon"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Electronics, Communications and Computers (CONIELECOMP)",
      "doi": "10.1109/CONIELECOMP.2019.8673111"
    },
    {
      "citation_id": "37",
      "title": "Expression classification in children using mean supervised deep Boltzmann machine",
      "authors": [
        "S Nagpal",
        "M Singh",
        "M Vatsa",
        "R Singh",
        "A Noore"
      ],
      "year": "2019",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "doi": "10.1109/CVPRW.2019.00033"
    },
    {
      "citation_id": "38",
      "title": "Emotion recognition from facial expressions in children and adults using deep neural network. Intelligent Systems, Technologies and Applications",
      "authors": [
        "A Rao",
        "S Ajri",
        "A Guragol",
        "R Suresh",
        "S Tripathi"
      ],
      "year": "2020",
      "venue": "Emotion recognition from facial expressions in children and adults using deep neural network. Intelligent Systems, Technologies and Applications",
      "doi": "10.1007/978-981-15-3914-5_4"
    },
    {
      "citation_id": "39",
      "title": "Transfer learning approach to multiclass classification of child facial expressions. Applications of Machine Learning",
      "authors": [
        "M Witherow",
        "M Samad",
        "K Iftekharuddin"
      ],
      "year": "2019",
      "venue": "Transfer learning approach to multiclass classification of child facial expressions. Applications of Machine Learning",
      "doi": "10.1117/12.2530397"
    },
    {
      "citation_id": "40",
      "title": "Labeling images with facial emotion and the potential for pediatric healthcare",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "Q Tariq",
        "K Dunlap",
        "J Schwartz"
      ],
      "year": "2019",
      "venue": "Artif Intell Med",
      "doi": "10.1016/j.artmed.2019.06.004"
    },
    {
      "citation_id": "41",
      "title": "A mobile game for automatic emotion-labeling of images",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "D Wall"
      ],
      "year": "2020",
      "venue": "IEEE Trans Games",
      "doi": "10.1109/TG.2018.2877325"
    },
    {
      "citation_id": "42",
      "title": "A gamified mobile system for crowdsourcing video for autism research",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Healthcare Informatics (ICHI)",
      "doi": "10.1109/ICHI.2018.00052"
    },
    {
      "citation_id": "43",
      "title": "Guess What?",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2018",
      "venue": "J Healthc Inform Res",
      "doi": "10.1007/s41666-018-0034-9"
    },
    {
      "citation_id": "44",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "K Dunlap",
        "J Schwartz",
        "P Washington",
        "A Husic"
      ],
      "year": "2020",
      "venue": "JMIR Ment Health",
      "doi": "10.2196/13174"
    },
    {
      "citation_id": "45",
      "title": "Are there basic emotions?",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Psychological Rev",
      "doi": "10.1037/0033-295x.99.3.550"
    },
    {
      "citation_id": "46",
      "title": "Expression and the nature of emotion",
      "authors": [
        "P Ekman",
        "K Scherer"
      ],
      "year": "1984",
      "venue": "Approaches to Emotion"
    },
    {
      "citation_id": "47",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "P Molnar",
        "U Segerstrale"
      ],
      "year": "1997",
      "venue": "Nonverbal Communication: Where Nature Meets Culture"
    },
    {
      "citation_id": "48",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "J Pers Soc Psychol",
      "doi": "10.1037/h0030377"
    },
    {
      "citation_id": "49",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "50",
      "title": "Imagenet: a large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2009.5206848"
    },
    {
      "citation_id": "51",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition",
      "doi": "10.1109/afgr.1998.670949"
    },
    {
      "citation_id": "52",
      "title": "AffectNet: a database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans Affective Comput",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "53",
      "title": "The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "Aug Presented at: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "54",
      "title": "The Child Affective Facial Expression (CAFE) set: validity and reliability from untrained adults",
      "authors": [
        "V Lobue",
        "C Thrasher"
      ],
      "year": "2015",
      "venue": "Front Psychol",
      "doi": "10.3389/fpsyg.2014.01532"
    },
    {
      "citation_id": "55",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2016.308"
    },
    {
      "citation_id": "56",
      "title": "Mobilenetv2: inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L Chen"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00474"
    },
    {
      "citation_id": "57",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2017.243"
    },
    {
      "citation_id": "58",
      "title": "Xception: deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2017.195"
    },
    {
      "citation_id": "59",
      "title": "Perceived emotion genuineness: normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets",
      "authors": [
        "A Dawel",
        "L Wright",
        "J Irons",
        "R Dumbleton",
        "R Palermo",
        "O' Kearney"
      ],
      "year": "2016",
      "venue": "Behav Res",
      "doi": "10.3758/s13428-016-0813-2"
    },
    {
      "citation_id": "60",
      "title": "Fake empathy and human-robot interaction (HRI): A preliminary study",
      "authors": [
        "J Vallverdú",
        "T Nishida",
        "Y Ohmoto",
        "S Moran",
        "S Lázare"
      ],
      "year": "2018",
      "venue": "IJTHI",
      "doi": "10.4018/IJTHI.2018010103"
    },
    {
      "citation_id": "61",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proc Natl Acad Sci",
      "doi": "10.1073/pnas.1322355111"
    },
    {
      "citation_id": "62",
      "title": "Achieving trustworthy biomedical data solutions",
      "authors": [
        "P Washington",
        "S Yeung",
        "B Percha",
        "N Tatonetti",
        "J Liphardt",
        "D Wall"
      ],
      "venue": "Biocomputing 2021: Proceedings of the Pacific Symposium",
      "doi": "10.1142/9789811232701_0001"
    }
  ]
}