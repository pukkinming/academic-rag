{
  "paper_id": "2105.00173v2",
  "title": "Emotion Recognition Of The Singing Voice: Toward A Real-Time Analysis Tool For Singers",
  "published": "2021-05-01T05:47:15Z",
  "authors": [
    "Daniel Szelogowski"
  ],
  "keywords": [
    "Biofeedback",
    "Cepstrum",
    "Convolutional Neural Network (CNN)",
    "Fast Fourier Transform",
    "Mel-frequency Cepstral Coefficients",
    "Mel Scale",
    "Psychoacoustics",
    "Spectral Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current computational-emotion research has focused on applying acoustic properties to analyze how emotions are perceived mathematically or used in natural language processing machine learning models. While recent interest has focused on analyzing emotions from the spoken voice, little experimentation has been performed to discover how emotions are recognized in the singing voiceboth in noiseless and noisy data (i.e., data that is either inaccurate, difficult to interpret, has corrupted/distorted/nonsense information like actual noise sounds in this case, or has a low ratio of usable/unusable information). Not only does this ignore the challenges of training machine learning models on more subjective data and testing them with much noisier data, but there is also a clear disconnect in progress between advancing the development of convolutional neural networks and the goal of emotionally cognizant artificial intelligence. By training a new model to include this type of information with a rich comprehension of psycho-acoustic properties, not only can models be trained to recognize information within extremely noisy data, but advancement can be made toward more complex biofeedback applicationsincluding creating a model which could recognize emotions given any human information (language, breath, voice, body, posture) and be used in any performance medium (music, speech, acting) or psychological assistance for patients with disorders such as BPD, alexithymia, autism, among others. This paper seeks to reflect and expand upon the findings of related research and present a stepping-stone toward this end goal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "While speech-emotion analysis research has become more prominent in recent years, there has been an overwhelming lack of study on emotional analysis of singing voices. Most current research is aimed toward classifying an audio clip of one emotion and has been primarily focused on pre-recorded audio clips of spoken (and sung) voices, none providing real-time feedback and audio-sectional descriptors. Recent focus has been tested on pure audio clips, none attempting to analyze noisy (especially accompanied voice) data. Ideally, this research will serve as the backend architecture for a real-time analysis tool or mobile app for singers and teachers to gauge the emotional valence and activation of various points within a piece of music, either through prerecorded audio or live feedback through the utilization of machine learning. Visual feedback, or biofeedback, has been scientifically proven to strengthen \"mind-to-motor\" coordination and help to influence production toward improvement  1  through the implementation of a visual analysis tool for recognizing sung emotions in real-time, such a tool would be extremely useful for musicians in training and professionals alike, andeventuallyfor full ensembles.\n\nEmotional expression is a challenge well-known to actors and musicians alike, especially for singers of any sort. While the portrayal of emotions in music is highly subjective, musicians must have a thorough interpretation in mind before performing for an audience if they seek to truly exhibit the characterization of the music; given that musical harmony alone can portray an expression, this generally coincides with the setting of the text of the piece. As such, the voice alone exhibits many features which express various emotions: the breath and its fullness or depth, the volume, pressure, and stability of the voice, among others. While reading these expressions is instinctual for most people, replicating the same ability in artificial intelligence is challenging.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Machine Learning And Training",
      "text": "The most likely candidate for building an emotion-detecting system in the field of machine learning is the neural network (NN)a form of artificial intelligence that seeks to replicate biological learning in the brain through the development of neurons and synaptic connections. One specialized form of NN, known as a Convolutional Neural Network (CNN), is a type of deeplearning neural network primarily applied to analyzing visual imagery for image/video recognition and classification, and natural language processing.  2  This type of artificial intelligence is inspired by biological processes, wherein the connectivity pattern between neurons resembles the organization of the animal visual cortex.  3  To train these types of networks, a large, consistent dataset is especially necessary for creating a reliable and accurate prediction and classification system. One such dataset is the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): a database of 24 professional actors (12 male and female, each) containing audio and video recordings of 8 different emotions (neutral, calm, happy, sad, angry, fearful, disgust, surprise) demonstrated both spoken and sung, primarily used to study the relationship between face and voice expressions.  4",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "While the topic of sung emotion recognition has not been discussed frequently in recent research, a great amount of experimentation and analysis has been performed on speech emotion recognitionnot only from a machine learning perspective but also purely scientific through acoustic and psychological studies with valence (intrinsic attractiveness/aversiveness) and activation (arousal, stimulation response) analysis. Spectral analysisa type of visual-based audio analysis used in measuring the distribution of acoustic energy across frequencies  5  has Daniel Szelogowski 5   become increasingly useful in the fields of both singing and speech pathology alikeit provides a means of either analyzing energy distribution in speech/sung sound (through the Fast Fourier Transform, FFT), including voice harmonics, or estimating the vocal tract filter that shaped the sound (Linear Predictive Coding, LPC).  6  As such, spectrographic tools may be utilized as a means of biofeedback to aid in the teaching of voice-related studies, especially for visual learners (see Figure  1 ).  7  By applying the inverse FFT to an audio signal, however, we are able to return the cepstrum of the spectral data, or \"spectrum of a spectrum\"a means of analyzing sound and vibration of a signalallowing us to measure how the power of the signal is distributed over frequency and its density through the power cepstrum.  8  More precisely, the Mel-frequency cepstrum (MFC) can be used to represent this data on a short-term basis 9 from the Mel scale, a scale relating a sound's perceived frequency to its actual frequency, which can be used to scale a sound close to how it would be perceived by the human ear.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition And Mel-Frequency Cepstral Coefficients",
      "text": "Recognizing emotions in normal speech is a process carried out by the amygdala, a region of the brain within the medial temporal lobe involved in emotional processes as part of the limbic system: the body's own neural network that handles various aspects of memory and emotion.  11  The amygdala responds to two properties of emotions: valence, measured as positive (attractive) or negative (aversive), and intensity, measured from low to high.  12  Thus, a CNN is the most effective machine learning model to represent the amygdala given its classification ability. To recreate the analytical process of the amygdala within the CNN, one way to measure the emotional properties of sound is through the coefficients of the MFC measurements of an audio clip, known as Mel-frequency cepstral coefficients (MFCCs) which also assists in better representing compressed audio on the Mel scale.  13",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotional Expression In Performance",
      "text": "For singers, performing expressively through the voice is a challenging form of actingespecially for languages foreign to the vocalist; 'confidence' alone is insufficient.  14  Miller discusses: Suppose [that] the teacher is dealing with a singer who has a good voice and considerable technical facility, but [finds] difficulty in achieving communication out of fear of \"looking silly\" in public. […] It sometimes occurs that the outgoing tendencies a person displays in everyday social encounters are not necessarily transferable when that person turns to the field of performance. On the other hand, the reserved personality may more readily find a communicative channel through the musical setting of text and the drama than does the demonstrative person. But most often, special attention must be taken to awaken the taciturn person to his or her potential for improving communication skills in performance.  15  Musical characterization, which often includes some physical movement, creates a sort of secondary reality where the actor is freed from behavioral responsibility and thus embodies the character of the text.  16  In a cycle of repertoire, such as an individual recital or ensemble, however, the character often changes drastically either within more complex songs or between piecesthis challenge alone is the goal of this research. As such, singers may utilize various techniques to express the desired characterwhich the music often assists with: stronger emotions such as anger are often expressed through an increase in sound volume and faster, often agitated rhythmsthus, a 'stronger' or 'angrier' soundwhile weaker emotions are most often seen concurrent to more somber, quiet, and slower music.  17  Other factors may also be taken into account, such as the depth of the breath, the width of the singer's vibrato, the range of musical pitches (i.e., higher or lower), among vocal timbre or 'color' and other vocal effects or tones (whispered, 'hooty', harsh, brassy, etc.).  18",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vocal Isolation",
      "text": "One aspect that may potentially affect the outcome of the CNN is the noise of the data it trains and/or tests on. In the case of this research, the training data is pure audio containing raw vocals with no noise  19  this will help to keep the information received from the MFCCs of the audio clean and reliable; however, this is an unrealistic expectation for a biofeedback app and is difficult to achieve from a field scenario. Vocal isolation continues to be an ongoing battle of its own, especially for noisy data such as an accompanied voice (even with complex machine learning algorithms),  20  although simple solutions using FFT and soft masking (separating the desired information from noisy data using a threshold mask) appear viable for the time being, especially given that FFT replicates how the ear transforms audio information into sound.  21",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sung Emotion Color And Recognition",
      "text": "Recognizing emotional expression and coloring has also proven to be equally, if not more challenging than vocal isolation. Recent attempts have included analyzing the acoustic features of a vocal signal, including low-level descriptors (LLDs, data features closely related to the origin signal/sound), delta (difference) coefficients, and aggregation of long-term average spectrumbased features (or LTAS, features that provide means to view the average power cepstrum over time),  22  rather than training a NN modelthough this research proved an increased correlation between MFCCs and emotion recognition. Further acoustic research showed a strong correlation in emotional expression with additional acoustic parameters including proportion energy below 500Hz and 1000Hz, alpha ratio (sound absorption), spectral flatness, Hammarberg index (quantification of difference in volume between high and low speech frequencies), maximum flow declination rate (or MFDR, regarding vocal intensity in the glottis) especially, among others.  23  Lastly, another recent approach was performed by the measurement of Signal-to-Noise Ratio (SNR) levels and their correlation to emotional valence, comparing mathematically predicted emotion values to actual perceived emotions in humans.  24",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Approach",
      "text": "The backend system presented in this research will consist of three primary components: the CNN model, a vocal isolator, and a WAV-file audio recorder/splitter. The CNN will be trained using the RAVDESS dataset (see Section 1.1) which contains 1,012 sung text examples across the various actors, expressing one of the following emotions per clip: neutral, calm, happy, angry, sad, or fear. The WAV-file system will simply record the user's microphone as a WAV file and feature a system to divide an audio file into user-defined seconds-long segment files for evaluation purposes as well as in testing real-time biofeedback (such as recording the user's voice and providing new feedback every N-seconds), both as live and historical data. Lastly, the vocal isolator will use the spectral data obtained from an audio file to separate the voice from any accompaniment or background noise through FFT and Wiener filtering (see Section 2.2).  25  Accuracy is an issue regardless of training due to the lack of datasets intentionally designed for such testing, but approximately 75% accuracy is a reasonable goal in this instance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implementation",
      "text": "The architecture of the model (built using Python's Keras and TensorFlow libraries) will be based on Muriel Kosaka's speech emotion recognition model which utilizes the Librosa library for the Python programming language to extract the Mel spectrogram and obtain the cepstral data, subsequently tuning the hyperparameters (parameters with values that control a model's learning) to obtain the most optimized model.  26  The extraction process of this data can be seen in Figure  2 , and the architecture of the model can be seen in Figure  3   For the vocal isolator, the algorithm will be based upon the REPET-SIM method  28  with slight modifications, including utilizing a smaller FFT window overlap and converting non-local filters into soft masks using Wiener filtering.  29  The audio (WAV) file will be converted into a spectrogram, apply a filter to aggregate and constrain similar audio frames, reduce the bleed of the vocal and instrumental/accompaniment (or noise) masks, then separate the masks into background (noise) and foreground (voice) spectrums. Once these spectrums are separated, the foreground will contain the newly filtered audio with the vocalswhich can then be exported to a new audio file.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation",
      "text": "The final model was trained to 2,000 epochs, achieving approximately 73% accuracy from the test data (see Appendix B). The loss from the modelalso known as the cost or objective function, used to find the best parameters, known as weights for the modelwas much greater in the test validation than the training validation; while accuracy scaled relatively proportionally between testing and training (see Figure  4a ), loss diverged away from minimization compared to the training model and continued to grow exponentially over time (see Figure  4b ). model predicts classes of emotions, a confusion matrix can be used to compare actual versus predicted labels (see Figure  4c ). While this level of accuracy is decent, a much more complex architecture is necessary to train a better performing model in the future, especially including training on acoustic properties among the MFCCs.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "While the accuracy of the model may appear questionable, most results are surprisingly correctalbeit subjectively in some instances. Appendix A contains the data analyzed from having the CNN attempt to classify audio through multiple user-defined splits of a given file (in seconds), rather than testing the overall emotion of the sound. Tests were performed with and without isolating the vocals from the accompaniment of various art songs, including one choral piece and one purely instrumental piece; however, making these changes did not appear to make a visible difference in how the model predicted various segments of a song. Vocals, both isolated and accompanied, showed nearly identical results so long as the segment contains a voicesilence was typically the only unidentical factor (seen especially in Appendix A, Sections 1 and 5), and this may be for one of two reasons in particular: either the silence contained residual harmony (even minuscule) from the accompaniment or noise and the isolated version of the file did not, or the file is comprised of just the accompaniment which became silent after isolating the vocals (also seen in Appendix A, Sections 3 and 4). As well, changing the length of the segments also did not appear to make a difference in the perceived emotion either, as the smaller splits only added to the precision of the model's accuracy (see Appendix A, Section 2.1). Of course, as expected, the model did not fare well attempting to classify emotions of instrumental music given its lack of training and more precise psychoacoustical parameters (see Appendix A, Section 4).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Daniel Szelogowski 12",
      "text": "One major flaw of the model appears to be its evolutionary progression; once the model achieves approximately 70% accuracy, it seldom improves. In an attempt to train the CNN 10,000 epochs, the model eventually failed after over 7500 epochs and its accuracy flatlined at less than 15% accuracy and never recovered, though the loss continued to change over time (see Figure  5a  and 5b). Strangely, these weights were saved as the \"new best\" by the model anyway, and the CNN appeared to predict \"angry\" for every given piece of data (see Figure  5c ). Additionally, in the final model, classifying in real-time also causes the fully trained (73% accurate) model to predict \"angry\" as well, but classifying these files later on yields normal results.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "This paper focused on the creation of a CNN with the ability to accurately recognize emotions in the singing voice using MFCCs as the primary feature, as well as laying out the developmental plans for a future mobile (or desktop) application that utilizes this type of model for providing biofeedback. By training the model on pure vocal audio, the model was able to create a working classification memory even when data is noisy or includes instrumental accompaniment.\n\nIdeally, the model should be nearly or just as accurate without vocal isolation as with itthis appeared to be the case even with the current final model, fortunately.\n\nIn the future, this type of model and architecture could be expanded upon and utilized as the backend system of a much more complex piece of softwarehopefully used in a biofeedback app as intended, providing visual feedback of both real-time and historical data for use in private music lessons. With finer tuning and the inclusion of more acoustic properties, this model has the potential to become much more accurate in a more refined architecture. Hopefully, a larger sungemotion dataset will be created as well soon. Eventually, more training may be done on the analysis of choral music and, when a dataset permits, analyzing instrumental music both individually and for ensembles.\n\nWith a much more expansive and precise dataset, a future model could also be trained to recognize a wider array of emotions in both singing and speaking voices, as well as recognizing emotions in the breath, language (textthrough natural language processing), face, and potentially body language, as a means of creating a near-true neural network amygdala.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ).7 By applying the inverse FFT to an audio signal, however, we are able to return the",
      "page": 5
    },
    {
      "caption": "Figure 1: Spectral analysis of a baritone voice singing “Allerseelen” by Richard Strauss with piano",
      "page": 5
    },
    {
      "caption": "Figure 2: , and the architecture of the model can be seen in Figure 3 (see Appendix B for model",
      "page": 9
    },
    {
      "caption": "Figure 2: Audio extraction of a ‘neutral’ male",
      "page": 9
    },
    {
      "caption": "Figure 3: EmotioNN model architecture as",
      "page": 9
    },
    {
      "caption": "Figure 4: a), loss diverged away from minimization compared to",
      "page": 10
    },
    {
      "caption": "Figure 4: Final trained model after 2,000 epochs: accuracy (a), loss (b), and confusion matrix (c)",
      "page": 10
    },
    {
      "caption": "Figure 4: c). While this level of accuracy is decent, a much more complex",
      "page": 11
    },
    {
      "caption": "Figure 5: Early trained model to 10,000 epochs: accuracy (a), loss (b), and confusion matrix (c)",
      "page": 12
    },
    {
      "caption": "Figure 6: Spectral analysis of a baritone voice singing “Allerseelen” by Strauss (5 seconds)",
      "page": 14
    },
    {
      "caption": "Figure 7: Spectral analysis of a tenor voice singing Lenski’s Aria by Tchaikovsky (5 seconds)",
      "page": 15
    },
    {
      "caption": "Figure 8: Spectral analysis of an SATB choir singing “Sure on this Shining Night” by Lauridsen",
      "page": 17
    },
    {
      "caption": "Figure 9: Spectral analysis of an orchestra performing the Coriolan Overture by Beethoven (5",
      "page": 18
    },
    {
      "caption": "Figure 10: Spectral analysis of a baritone voice singing Der Erlkönig by Franz Schubert (5 sec.)",
      "page": 19
    },
    {
      "caption": "Figure 10: Layer diagram of the",
      "page": 20
    },
    {
      "caption": "Figure 4: Final trained model after 2000 epochs,",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of isolated versus non-isolated vocals split into 20-second-long segments",
      "data": [
        {
          "Non-Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200",
          "Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 2: Comparison of isolated versus non-isolated vocals split into 20-second-long segments",
      "data": [
        {
          "Non-Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260 \n280 \n300 \n320 \n340 \n360 \n380",
          "Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260 \n280 \n300 \n320 \n340 \n360 \n380"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 3: Comparison of isolated vocals split into various segment lengths",
      "data": [
        {
          "Split 10 Seconds": "0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n110 \n120 \n130 \n140 \n150 \n160 \n170 \n180 \n190 \n200 \n210 \n220 \n230 \n240 \n250 \n260 \n270 \n280 \n290 \n300 \n310 \n320 \n330 \n340 \n350 \n360 \n370 \n380",
          "Split 20 Seconds": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260 \n280 \n300 \n320 \n340 \n360 \n380",
          "Split 40 Seconds": "0 \n40 \n80 \n120 \n160 \n200 \n240 \n280 \n320 \n360",
          "Split 60 Seconds": "0 \n60 \n120 \n180 \n240 \n300 \n360",
          "Split 120 \nSeconds": "0 \n120 \n240 \n360"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 4: Comparison of isolated versus non-isolated vocals split into 20-second-long segments",
      "data": [
        {
          "Non-Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260",
          "Isolated Vocals": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 5: Comparison of instrumental piece split into segments 20- versus 40-seconds long",
      "data": [
        {
          "Orchestral Split 20 Seconds": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240 \n260 \n280 \n300 \n320 \n340 \n360 \n380 \n400 \n420 \n440 \n460 \n480",
          "Orchestral Split 40 Seconds": "0 \n40 \n80 \n120 \n160 \n200 \n240 \n280 \n320 \n360 \n400 \n440 \n480"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 6: Comparison of isolated versus non-isolated vocals split into 10- and 20-second-long",
      "data": [
        {
          "Non-Isolated Vocals \n(10 seconds)": "0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n110 \n120 \n130 \n140 \n150 \n160 \n170 \n180 \n190 \n200 \n210 \n220 \n230 \n240",
          "Isolated Vocals \n(10 seconds)": "0 \ncalm             \n10 \ncalm             \n20 \ncalm             \n30 \ncalm             \n40 \ncalm             \n50 \ncalm             \n60 \ncalm             \n70 \ncalm             \n80 \ncalm              \n90 \ncalm              \n100 \nfear              \n110 \ncalm              \n120 \ncalm              \n130 \ncalm             \n140 \nhappy              \n150 \ncalm              \n160 \n170 \n180 \n190 \nangry            \n200 \n210 \nhappy             \n220 \ncalm             \n230 \n240",
          "Non-Isolated Vocals \n(20 seconds)": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240",
          "Isolated Vocals \n(20 seconds)": "0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n240"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mobile Apps and Biofeedback in Voice Pedagogy",
      "authors": [
        "Heidi Erickson"
      ],
      "year": "2021",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "2",
      "title": "What Are Convolutional Neural Networks?",
      "authors": [
        "Ibm Cloud Education"
      ],
      "year": "2020",
      "venue": "What Are Convolutional Neural Networks?"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Ibid"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "5",
      "title": "Welcome to SWPhonetics",
      "authors": [
        "Sidney Wood"
      ],
      "year": "2013",
      "venue": "Welcome to SWPhonetics"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Ibid"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "The Basics of Voice Science & Pedagogy",
      "authors": [
        "Scott Mccoy"
      ],
      "year": "2020",
      "venue": "The Basics of Voice Science & Pedagogy"
    },
    {
      "citation_id": "8",
      "title": "Fundamentals of Noise and Vibration Analysis for Engineers",
      "authors": [
        "Michael Norton",
        "Dennis Karczub"
      ],
      "year": "2007",
      "venue": "Fundamentals of Noise and Vibration Analysis for Engineers"
    },
    {
      "citation_id": "9",
      "title": "HMM-Based Audio Keyword Generation",
      "authors": [
        "Min Xu",
        "Ling-Yu Duan",
        "Jianfei Cai",
        "Liang-Tien Chia",
        "Changsheng Xu",
        "Qi Tian"
      ],
      "year": "2004",
      "venue": "Advances in Multimedia Information Processing -PCM 2004",
      "doi": "10.1007/978-3-540-30543-9_71"
    },
    {
      "citation_id": "10",
      "title": "Speech Emotion Recognition (SER) through Machine Learning",
      "authors": [
        "Analytics Insight"
      ],
      "year": "2021",
      "venue": "Analytics Insight"
    },
    {
      "citation_id": "11",
      "title": "Amygdala",
      "authors": [
        "C Salzman",
        "Daniel"
      ],
      "venue": "Encyclopedia Britannica, Invalid Date"
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "Ibid"
      ],
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "HMM-Based Audio Keyword Generation",
      "authors": [
        "Min Xu",
        "Ling-Yu Duan",
        "Jianfei Cai",
        "Liang-Tien Chia",
        "Changsheng Xu",
        "Qi Tian"
      ],
      "year": "2004",
      "venue": "Advances in Multimedia Information Processing -PCM 2004",
      "doi": "10.1007/978-3-540-30543-9_71"
    },
    {
      "citation_id": "14",
      "title": "Learning to Portray Emotion",
      "authors": [
        "Richard Miller"
      ],
      "year": "2001",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "15",
      "title": "",
      "authors": [
        "Ibid"
      ],
      "venue": ""
    },
    {
      "citation_id": "16",
      "title": "I Second That Emotion",
      "authors": [
        "Robert Edwin"
      ],
      "year": "1988",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "17",
      "title": "Emotion and Empathy: How Voice Can Save the Culture",
      "authors": [
        "Lynn Helding"
      ],
      "year": "2017",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "Ibid"
      ],
      "venue": ""
    },
    {
      "citation_id": "19",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "20",
      "title": "Singing Voice Separation Using a Deep Convolutional Neural Network Trained by Ideal Binary Mask and Cross Entropy",
      "authors": [
        "Lin Wah",
        "B Edward Kin",
        "Enyan Balamurali",
        "Simon Koh",
        "Dorien Lui",
        "Herremans"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-018-3933-z"
    },
    {
      "citation_id": "21",
      "title": "An Introduction to Audio Processing and Machine Learning Using Python",
      "authors": [
        "Jyotika Singh"
      ],
      "year": "2019",
      "venue": "Opensource.com"
    },
    {
      "citation_id": "22",
      "title": "Emotion in the Singing Voice-A Deeper Look at Acoustic Features in the Light of Automatic Classification",
      "authors": [
        "Florian Eyben",
        "Gláucia Salomão",
        "Johan Sundberg",
        "Klaus Scherer",
        "Björn Schuller"
      ],
      "year": "2015",
      "venue": "EURASIP Journal on Audio",
      "doi": "10.1186/s13636-015-0057-6"
    },
    {
      "citation_id": "23",
      "title": "Emotional Coloring of the Singing Voice",
      "authors": [
        "Gláucia Salomão",
        "Johan Sundberg",
        "Klaus Scherer"
      ],
      "year": "2015",
      "venue": "Pan-European Conference: Pevoc 11"
    },
    {
      "citation_id": "24",
      "title": "Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions",
      "authors": [
        "Parada-Cabaleiro",
        "Maximilian Emilia",
        "Anton Schmitt",
        "Batliner",
        "Simone",
        "Hantke",
        "Giovanni",
        "Klaus Costantini",
        "Scherer",
        "Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "19 th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "25",
      "title": "Speech_Emotion_Recognition",
      "authors": [
        "Muriel Kosaka"
      ],
      "year": "2020",
      "venue": "Speech_Emotion_Recognition"
    },
    {
      "citation_id": "26",
      "title": "Vocal Separation Using Nearest Neighbours and Median Filtering",
      "authors": [
        "Derry Fitzgerald"
      ],
      "year": "2012",
      "venue": "IET Irish Signals and Systems Conference (ISSC 2012)",
      "doi": "10.1049/ic.2012.0225"
    },
    {
      "citation_id": "27",
      "title": "Sung-EmotioNN-Detector",
      "authors": [
        "Daniel Szelogowski"
      ],
      "year": "2020",
      "venue": "Github"
    },
    {
      "citation_id": "28",
      "title": "Music/Voice Separation Using the Similarity Matrix",
      "authors": [
        "Zafar Rafii",
        "Bryan Pardo"
      ],
      "year": "2012",
      "venue": "13 th International Society for Music Information Retrieval (ISMIR) Conference"
    },
    {
      "citation_id": "29",
      "title": "Vocal Separation Using Nearest Neighbours and Median Filtering",
      "authors": [
        "Derry Fitzgerald"
      ],
      "year": "2012",
      "venue": "IET Irish Signals and Systems Conference (ISSC 2012)",
      "doi": "10.1049/ic.2012.0225"
    },
    {
      "citation_id": "30",
      "title": "Emotion Detection in the Singing Voice Art Song Research Data Overview",
      "authors": [
        "Daniel Szelogowski"
      ],
      "venue": "Emotion Detection in the Singing Voice Art Song Research Data Overview"
    },
    {
      "citation_id": "31",
      "title": "",
      "authors": [
        "Dietrich Fischer-Dieskau ; Richard Strauss",
        "Youtube",
        "Youtube"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "Rolando Villazon -Kuda",
        "Kuda -Lensky's Aria",
        "Youtube",
        "Youtube"
      ],
      "year": "2007",
      "venue": ""
    },
    {
      "citation_id": "33",
      "title": "Sure on the Shining Night (Morten Lauridsen)",
      "year": "2018",
      "venue": "YouTube. YouTube"
    },
    {
      "citation_id": "34",
      "title": "Coriolan Overture, Op. 62 (with Score)",
      "authors": [
        "Beethoven"
      ],
      "year": "2019",
      "venue": "YouTube. YouTube"
    },
    {
      "citation_id": "35",
      "title": "References Beethoven: Coriolan Overture",
      "authors": [
        "Der Erlkönig",
        ": Franz Schubert",
        "Philippe Sly"
      ],
      "year": "2011",
      "venue": "References Beethoven: Coriolan Overture"
    },
    {
      "citation_id": "36",
      "title": "Philippe Sly: Bass-Baritone, Maria Fuller: Piano. YouTube. YouTube",
      "authors": [
        "Der Erlkönig",
        ": Franz Schubert"
      ],
      "year": "2011",
      "venue": "Philippe Sly: Bass-Baritone, Maria Fuller: Piano. YouTube. YouTube"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "Dietrich Fischer-Dieskau ; Richard Strauss",
        "Youtube",
        "Youtube"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "I Second That Emotion",
      "authors": [
        "Robert Edwin"
      ],
      "year": "1988",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "39",
      "title": "Mobile Apps and Biofeedback Voice Pedagogy",
      "authors": [
        "Heidi Erickson"
      ],
      "year": "2021",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "40",
      "title": "Emotion in the Singing Voice-A Deeper Look at Acoustic Features in the Light of Automatic Classification",
      "authors": [
        "Florian Eyben",
        "Gláucia Salomão",
        "Johan Sundberg",
        "Klaus Scherer",
        "Björn Schuller"
      ],
      "year": "2015",
      "venue": "EURASIP Journal on Audio",
      "doi": "10.1186/s13636-015-0057-6"
    },
    {
      "citation_id": "41",
      "title": "Vocal Separation Using Nearest Neighbours and Median Filtering",
      "authors": [
        "Derry Fitzgerald"
      ],
      "year": "2012",
      "venue": "IET Irish Signals and Systems Conference (ISSC 2012)",
      "doi": "10.1049/ic.2012.0225"
    },
    {
      "citation_id": "42",
      "title": "Emotion and Empathy: How Voice Can Save the Culture",
      "authors": [
        "Lynn Helding"
      ],
      "year": "2017",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "43",
      "title": "What Are Convolutional Neural Networks?",
      "authors": [
        "Ibm Cloud Education"
      ],
      "year": "2020",
      "venue": "What Are Convolutional Neural Networks?"
    },
    {
      "citation_id": "44",
      "title": "Speech Emotion Recognition (SER) through Machine Learning",
      "authors": [
        "Analytics Insight"
      ],
      "year": "2021",
      "venue": "Analytics Insight"
    },
    {
      "citation_id": "45",
      "title": "Speech_Emotion_Recognition",
      "authors": [
        "Muriel Kosaka"
      ],
      "year": "2020",
      "venue": "GitHub"
    },
    {
      "citation_id": "46",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "47",
      "title": "The Basics of Voice Science & Pedagogy",
      "authors": [
        "Scott Mccoy"
      ],
      "year": "2020",
      "venue": "The Basics of Voice Science & Pedagogy"
    },
    {
      "citation_id": "48",
      "title": "Learning to Portray Emotion",
      "authors": [
        "Richard Miller"
      ],
      "year": "2001",
      "venue": "NATS -Journal of Singing"
    },
    {
      "citation_id": "49",
      "title": "Fundamentals of Noise and Vibration Analysis for Engineers",
      "authors": [
        "Michael Norton",
        "Dennis Karczub"
      ],
      "year": "2007",
      "venue": "Fundamentals of Noise and Vibration Analysis for Engineers"
    },
    {
      "citation_id": "50",
      "title": "Identifying Emotions in Opera Singing: Implications of Adverse Acoustic Conditions",
      "authors": [
        "Parada-Cabaleiro",
        "Maximilian Emilia",
        "Anton Schmitt",
        "Batliner",
        "Simone",
        "Hantke",
        "Giovanni",
        "Klaus Costantini",
        "Scherer",
        "Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "19 th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "51",
      "title": "Emotions Understanding Model from Spoken Language Using Deep Neural Networks and Mel-Frequency Cepstral Coefficients",
      "authors": [
        "Marco De Pinto",
        "Marco Giuseppe",
        "Pasquale Polignano",
        "Giovanni Lops",
        "Semeraro"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)",
      "doi": "10.1109/eais48028.2020.9122698"
    },
    {
      "citation_id": "52",
      "title": "Power Spectral Density: What Is It and How Is It Measured?",
      "year": "2021",
      "venue": "Safe Load Testing Technologies"
    },
    {
      "citation_id": "53",
      "title": "Music/Voice Separation Using the Similarity Matrix",
      "authors": [
        "Zafar Rafii",
        "Bryan Pardo"
      ],
      "year": "2012",
      "venue": "13 th International Society for Music Information Retrieval (ISMIR) Conference"
    },
    {
      "citation_id": "54",
      "title": "",
      "authors": [
        "Rolando Villazon -Kuda",
        "Kuda -Lensky's Aria",
        "Youtube",
        "Youtube"
      ],
      "year": "2007",
      "venue": ""
    },
    {
      "citation_id": "55",
      "title": "Emotional Coloring of the Singing Voice",
      "authors": [
        "Gláucia Salomão",
        "Johan Sundberg",
        "Klaus Scherer"
      ],
      "year": "2015",
      "venue": "Emotional Coloring of the Singing Voice"
    },
    {
      "citation_id": "56",
      "title": "Amygdala.\" Encyclopedia Britannica",
      "authors": [
        "C Salzman",
        "Daniel"
      ],
      "venue": "Amygdala.\" Encyclopedia Britannica"
    },
    {
      "citation_id": "57",
      "title": "An Introduction to Audio Processing and Machine Learning Using Python",
      "authors": [
        "Jyotika Singh"
      ],
      "year": "2019",
      "venue": "Opensource.com"
    },
    {
      "citation_id": "58",
      "title": "Sure on the Shining Night (Morten Lauridsen)",
      "year": "2018",
      "venue": "YouTube. YouTube"
    },
    {
      "citation_id": "59",
      "title": "Sung-EmotioNN-Detector",
      "authors": [
        "Daniel Szelogowski"
      ],
      "year": "2020",
      "venue": "Github"
    },
    {
      "citation_id": "60",
      "title": "Emotion Detection in the Singing Voice Art Song Research Data Overview",
      "authors": [
        "Daniel Szelogowski"
      ],
      "venue": "Emotion Detection in the Singing Voice Art Song Research Data Overview"
    },
    {
      "citation_id": "61",
      "title": "Singing Voice Analysis in Popular Music Using Machine Learning Approaches",
      "authors": [
        "Lin Wah",
        "Edward Kin"
      ],
      "year": "2018",
      "venue": "Singing Voice Analysis in Popular Music Using Machine Learning Approaches"
    },
    {
      "citation_id": "62",
      "title": "Singing Voice Separation Using a Deep Convolutional Neural Network Trained by Ideal Binary Mask and Cross Entropy",
      "authors": [
        "Lin Wah",
        "B Edward Kin",
        "Enyan Balamurali",
        "Simon Koh",
        "Dorien Lui",
        "Herremans"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-018-3933-z"
    },
    {
      "citation_id": "63",
      "title": "Welcome to SWPhonetics",
      "authors": [
        "Sidney Wood"
      ],
      "year": "2013",
      "venue": "Welcome to SWPhonetics"
    },
    {
      "citation_id": "64",
      "title": "HMM-Based Audio Keyword Generation",
      "authors": [
        "Min Xu",
        "Ling-Yu Duan",
        "Jianfei Cai",
        "Liang-Tien Chia",
        "Changsheng Xu",
        "Qi Tian"
      ],
      "year": "2004",
      "venue": "Advances in Multimedia Information Processing -PCM 2004",
      "doi": "10.1007/978-3-540-30543-9_71"
    }
  ]
}