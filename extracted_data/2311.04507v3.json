{
  "paper_id": "2311.04507v3",
  "title": "Conversation Understanding Using Relational Temporal Graph Neural Networks With Auxiliary Cross-Modality Interaction",
  "published": "2023-11-08T07:46:25Z",
  "authors": [
    "Cam-Van Thi Nguyen",
    "Anh-Tuan Mai",
    "The-Son Le",
    "Hai-Dang Kieu",
    "Duc-Trong Le"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the globaland the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modalityspecific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies with the modality-specific manner for conversation understanding. Extensive experiments demonstrate the effectiveness of CORECT via its stateof-the-art results on the IEMOCAP and CMU-MOSEI datasets for the multimodal ERC task 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Our social interactions and relationships are all influenced by emotions. Given the transcript of a conversation and speaker information for each constituent utterance, the task of Emotion Recognition in Conversations (ERC) aims to identify the emotion expressed in each utterance from a predefined set of emotions  (Poria et al., 2019) . The multimodal nature of human communication, which involves verbal/textual, facial expressions, vocal/acoustic, bodily/postural, and symbolic/pictorial expressions, This isn't anything like I thought anything would be.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "This Is Just This‚Ä¶",
      "text": "Oh, sure this is standing on the beach, this is waiting, fighting.\n\nRight.\n\n...I can't believe it. I never thought you would get married.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I Know Me Neither.",
      "text": "Oh my gosh.\n\nJust a couple days ago.\n\nOh my gosh. adds complexity to the task of Emotion Recognition in Conversations (ERC)  (Wang et al., 2022) .\n\nMultimodal ERC, which aims to automatically detect the a speaker's emotional state during a conversation using information from text content, facial expressions, and audio signals, has garnered significant attention and research in recent years and has been applied to many real-world scenarios  (Sharma and Dhall, 2021; Joshi et al., 2022) .\n\nMassive methods have been developed to model conversation's context. These approaches can be categorized into two main groups: graph-based methods  (Ghosal et al., 2019; Zhang et al., 2019; Shen et al., 2021b)  and recurrence-based methods  (Hazarika et al., 2018a; Ghosal et al., 2020; Majumder et al., 2019; Hu et al., 2021) . In addition, there have been advancements in multimodal models that leverage the dependencies and complementarities of multiple modalities to improve the ERC performance  (Poria et al., 2017; Hazarika et al., 2018b; Zadeh et al., 2018) . One limitation of these methods is their heavy reliance on nearby utterances when updating the state of the query utterance, which can restrict their overall performance. Recently, Graph Neural Network (GNN)-based methods have been proposed for the multimodal ERC task due to their ability to capture long-distance contextual information through their relational modeling capabilities. However, those models rely on fused inputs being treated as a single node in the graph  (Ghosal et al., 2019; Joshi et al., 2022) , which limits their ability to capture modality-specific representations and ultimately hampers their overall performance.\n\nThe temporal aspect of conversations is crucial, as past and future utterances can significantly influence the query utterance as Figure  1 . The sentence \"I know me neither\" appears with opposing labels on different dialogues, which could be caused by sequential effects from previous or future steps. There are only a few methods that take into account the temporal aspect of conversations. MMGCN  (Wei et al., 2019)  represents modality-specific features as graph nodes but overlooks the temporal factor. DAG-ERC  (Shen et al., 2021b)  incorporates temporal information, but focuses solely on text modality. Recently, COGMEN  (Joshi et al., 2022)  proposes to learn contextual, inter-speaker, and intra-speaker relations, but neglects modalityspecific features and partially utilizes cross-modal information by fusing all modalities' representations at the input stage.\n\nThe aforementioned limitations motivate us to propose a COnversation understanding model using RElational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT). It comprises two key components: the (i) Relational Temporal Graph Convolutional Network (RT-GCN); and the (ii) Pairwise Crossmodal Feature Interaction (P-CM). The RT-GCN module is based on RGCNs  (Schlichtkrull et al., 2018)  and GraphTransformer  (Yun et al., 2019)  while the P-CM is built upon  (Tsai et al., 2019) . Overall, our main contributions are as follows:\n\n‚Ä¢ We propose the CORECT framework for Multimodal ERC, which concurrently exploit the utterance-level local context feature from multimodal interactions with temporal dependencies via RT-GCN, and the cross-modal global context feature at the conversation level by P-CM. These features are aggregated to enhance the performance of the utterance-level emotional recognition.\n\n‚Ä¢ We conduct extensive experiments to show that CORECT consistently outperforms the previous SOTA baselines on the two publicly real-life datasets, including IEMOCAP and CMU-MOSEI, for the multimodal ERC task.\n\n‚Ä¢ We conduct ablation studies to investigate the effect of various components and modalities on CORECT for conversation understanding.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "This section presents a literature review on Multimodal Emotion Recognition (ERC) and the application of Graph Neural Networks for ERC.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition In Conversation",
      "text": "The complexity of conversations, with multiple speakers, dynamic interactions, and contextual dependencies, presents challenges for the ERC task.\n\nThere are efforts to model the conversation context in ERC, with a primary focus on the textual modality. Several notable approaches include CMN  (Hazarika et al., 2018b) , DialogueGCN  (Ghosal et al., 2019) , COSMIC  (Ghosal et al., 2020) , Dia-logueXL  (Shen et al., 2021a) , DialogueCRN  (Hu et al., 2021) , DAG-ERC  (Shen et al., 2021b) .\n\nMultimodal machine learning has gained popularity due to its ability to address the limitations of unimodal approaches in capturing complex realworld phenomena  (Baltru≈°aitis et al., 2018) . It is recognized that human perception and understanding are influenced by the integration of multiple sensory inputs. There have been several notable approaches that aim to harness the power of multiple modalities in various applications  (Poria et al., 2017; Zadeh et al., 2018; Majumder et al., 2019) , etc. CMN  (Hazarika et al., 2018b)  combines features from different modalities by concatenating them directly and utilizes the Gated Recurrent Unit (GRU) to model contextual information. ICON  (Hazarika et al., 2018a ) extracts multimodal conversation features and employs global memories to model emotional influences hierarchically, resulting in improved performance for utterance-video emotion recognition. ConGCN  (Zhang et al., 2019)  models utterances and speakers as nodes in a graph, capturing context dependencies and speaker dependencies as edges. However, ConGCN focuses only on textual and acoustic features and does not consider other modalities. MMGCN  (Wei et al., 2019) , on the other hand, is a graph convolutional network (GCN)-based model that effectively captures both long-distance contextual information and multimodal interactive information.\n\nMore recently,  Lian et al. (2022)  propose a novel framework that combines semi-supervised learning with multimodal interactions. However, it currently addresses only two modalities, i.e., text and audio, with visual information reserved for future work.  Shi and Huang (2023)  introduces MultiEMO, an attention-based multimodal fusion framework that effectively integrates information from textual, audio and visual modalities. However, neither of these models addresses the temporal aspect in conversations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph Neural Networks",
      "text": "In the past few years, there has been a growing interest in representing non-Euclidean data as graphs. However, the complexity of graph data has presented challenges for traditional neural network models. From initial research on graph neural networks (GNNs)  (Gori et al., 2005; Scarselli et al., 2008) , generalizing the operations of deep neural networks were paid attention, such as convolution  (Kipf and Welling, 2017) , recurrence  (Nicolicioiu et al., 2019) , and attention  (Velickovic et al., 2018) , to graph structures. When faced with intricate interdependencies between modalities, GNN is a more efficient approach to exploit the potential of multimodal datasets. The strength of GNNs lies in its ability to capture and model intra-modal and intermodal interactions. This flexibility makes them an appealing choice for multimodal learning tasks.\n\nThere have been extensive studies using the capability of GNNs to model the conversations. Di-alogueGCN  (Ghosal et al., 2019 ) models conversation using a directed graph with utterances as nodes and dependencies as edges, fitting it into a GCN structure. MMGCN  (Wei et al., 2019)  adopts an undirected graph to effectively fuse multimodal information and capture long-distance contextual and inter-modal interactions.  Lian et al. (2020)  proposed a GNN-based architecture for ERC that utilizes both text and speech modalities. Dialogue-CRN  (Hu et al., 2021)  incorporates multiturn reasoning modules to extract and integrate emotional clues, enabling a comprehensive understanding of the conversational context from a cognitive perspective. MTAG  (Yang et al., 2021)  is capable of both fusion and alignment of asynchronously distributed multimodal sequential data. COGMEN  (Joshi et al., 2022)  uses GNN-based architecture to model complex dependencies, including local and global information in a conversation.  Chen et al. (2023)  presents Multivariate Multi-frequency Multimodal Graph Neural Network, M 3 Net for short, to explore the relationships between modalities and context. However, it primarily focuses on modalitylevel interactions and does not consider the temporal aspect within the graph.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Figure  2  illustrates the architecture of CORECT to tackle the multimodal ERC task. It consists of main components namely Relational Temporal Graph Convolution Network (RT-GCN) and Pairwise Cross-modal Feature Interaction. For a given utterance in a dialogue, the former is to learn the local-context representation via leveraging various topological relations between utterances and modalities, while the latter infers the cross-modal globalcontext representation from the whole dialogue.\n\nGiven a multi-speaker conversation C consisting of N utterances [u 1 , u 2 , . . . , u N ], let us denote S as the respective set of speakers. Each utterance u i is associated with three modalities, including audio (a), visual (v), and textual (l), that can be represented as u a i , u v i , u l i respectively. Using localand global context representations, the ERC task aims to predict the label for every utterance u i ‚àà C from a set of M predefined emotional labels Y = [y 1 , y 2 , . . . , y M ].",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Utterance-Level Feature Extraction",
      "text": "Here, we perform pre-processing procedures to extract utterance-level features to facilitate the learning of CORECT in the next section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Unimodal Encoder",
      "text": "Given an utterance u i , each data modality manifests a view of its nature. To capture this value, we employ dedicated unimodal encoders, which generate utterance-level features, namely\n\nfor the acoustic, visual, and lexical modalities respectively, and d a , d v , d l are the dimensions of the extracted features for each modality.\n\nFor textual modality, we utilize a Transformer  (Vaswani et al., 2017)  as the unimodal encoder to extract the semantic feature x l i from u l i as follows:\n\nwhere W l trans is the parameter of Transformer to be learned. For acoustic and visual modalities, we employ a fully-connected network as the unimodal encoder to extract context features for each modality type via the following procedure:\n\nwhere FC is the fully connected network, W œÑ f c ‚àà R dœÑ √ód œÑ in are trainable parameters; d œÑ in is the input dimension of modality œÑ",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Speaker Embedding",
      "text": "Inspired by MMGCN  (Wei et al., 2019) , we leverage the significance of speaker information. Let us define Embedding as a procedure that takes the identity of speakers and produce the respective latent representations. The embedding of multispeaker could be inferred as:\n\nwhere S emb ‚àà R N √óN S and N S is the total number of participants in the conversation. The extracted utterance-level feature could be enhanced by adding the corresponding speaker embedding:\n\nwhere X œÑ ‚àà R N √ódœÑ refers to the global-context representation from the whole dialogue obtained from the respective unimodal encoder; X œÑ represents the enhanced representation with the inclusion of the speaker embedding; Œ∑ ‚àà [0, 1] indicates the contribution ratio.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Relational Temporal Graph Convolutional Network (Rt-Gcn)",
      "text": "RT-GCN is proposed to capture local context information for each utterance in the conversation via exploiting the multimodal graph between utterances and their modalities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Graph Construction",
      "text": "Let us denote G(V, R, E) as the multimodal graph built from conversations, where {V, E, R} refers to the set of utterance nodes with the three modality types (|V| = 3 √ó N ), the set of edges and their relation types. Figure  3  provides an illustrative example of the relations represented on the constructed graph.\n\nNodes. Each utterance u i generates three nodes u a i , u v i , and u l i , which x a i , x v i , and x l i are the respective audio, visual, and lexical feature vectors.\n\nEdges. The edge (u œÑ i , u œÑ j , r ij ) ‚àà E, œÑ ‚àà {a, v, l} represents the interaction between u œÑ i and u œÑ j with the relation type r ij ‚àà R. In the scope of paper, we consider two groups of relations: R multi and R temp . Specifically, R multi represents the intra connections between the three modalities within the same utterance, reflecting multimodal interactions. On the other hand, R temp captures the inter connections between utterances of the same modality within a specified time window. This temporal relationship includes past/previous utterances de-noted as P and next/future utterances denoted as F. As a result, there are 15 edge types created with the definitions of the two groups.\n\nMultimodal Relation. Emotions in dialogues cannot be solely conveyed through lexical, acoustic, or visual modalities in isolation. The interactions between utterances across different modalities play a crucial role. For example, given an utterance in a graph, its visual node has different interactive magnitude with acoustic-and textual nodes. Additionally, each node has a self-aware connection to reinforce its own information. Therefore, we can formalize 9 edge types of R multi to capture the multimodal interactions within the dialogue as:\n\nTemporal Relation. It is vital to have distinct treatment for interactions between nodes that occur in different temporal orders  (Poria et al., 2017) . To capture this temporal aspect, we set a window slide [P, F] to control the number of past/previous and next/future utterances that are set has connection to current node u œÑ i . This window enables us to define the temporal context for each node and capture the relevant information from the dynamic surrounding utterances. Therefore, we have 6 edge types of R temp as follows:\n\nwhere œÑ ‚àà {a, v, l}; i, j ‚àà 1, N ; future ‚Üê and past ‚Üí indicate the past and future relation respectively.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Graph Learning",
      "text": "With the objective of leveraging the nuances and variations of heterogeneous interactions between utterances and modalities in the multimodal graph, we seek to employ Relational Graph Convolutional Networks (RGCN)  (Schlichtkrull et al., 2018) . For each relation type r ‚àà R, node representation is inferred via a mapping function f (H, W r ), where W r is the weighted matrix. Aggregating all 15 edge types, the final node representation could be computed by R r f (H, W r ). To be more specific, the representation for the i-th utterance is inferred as follows: where N r (i) is the set of the node i's neighbors with the relation r ‚àà R, W 0 , W r ‚àà R d h1 √ódœÑ are learnable parameters (h1 is the dimension of the hidden layer used by R-GCN), and x œÑ i ‚àà R dœÑ √ó1 denotes the feature vector of node u œÑ i ; œÑ ‚àà {a, v, l}. To extract rich representations from node features, we utilize a Graph Transformer model  (Yun et al., 2019) , where each layer comprises a selfattention mechanism followed by feed-forward neural networks. The self-attention mechanism allows vertices to exploit information from neighborhoods as well as capturing local and global patterns in the graph. Given g œÑ i is the representation of i th utterance with modality œÑ ‚àà {a, v, l} obtained from RGCNs, its representation is transformed into:\n\nwhere W 1 , W 2 ‚àà R d h 2 √ód h 1 are learned parameters (h2 is the dimension of the hidden layer used by Graph Transformer); N (i) is the set of nodes that has connections to node i; || is the concatenation for C head attention; and the attention coefficient of node j, i.e., Œ± œÑ i,j , is calculated by the softmax activation function:\n\nW 3 , W 4 ‚àà R dŒ±√ód h 1 are learned parameters.\n\nAfter the aggregation throughout the whole graph, we obtain new representation vectors:\n\nwhere œÑ ‚àà {a, v, l} indicates the corresponding audio, visual, or textual modality.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Pairwise Cross-Modal Feature Interaction",
      "text": "The cross-modal heterogeneities often elevate the difficulty of analyzing human language. Exploiting cross-modality interactions may help to reveal the \"unaligned\" nature and long-term dependencies across modalities. Inspired by the idea  (Tsai et al., 2019) , we design the Pairwise Cross-modal Feature Interaction (P-CM) method into our proposed framework for conversation understanding.\n\nA more detailed illustration of the P-CM module is presented in Appendix A.1.2 Given two modalities, e.g., audio a and textual l, let us denote X a ‚àà R N √óda , X l ‚àà R N √ód l as the respective modality-sensitive representations of the whole conversation using unimodal encoders. Based on the transformer architecture  (Vaswani et al., 2017) , we define the Queries as Q a = X a W Q a , Keys as K l = X l W K l , and Values as V l = X l W V l . The enriched representation of X a once performing cross-modal attention on the modality a by the modality l, referred to as CM l‚Üía ‚àà R N √ód V , is computed as:\n\nwhere œÉ is the softmax function;\n\nis the dimension of queues, keys and values respectively. ‚àö d k is a scaling factor and d (.) is the feature dimension.\n\nTo model the cross-modal interactions on unaligned multimodal sequences, e.g., audio, visual, and lexical, we utilize D cross-modal transformer layers. Suppose that Z a\n\n[i] is the modality-sensitive global-context representation of the whole conversation for the modality l at the i-th layer; Z a\n\n[0] = X a . The enriched representation of\n\nl‚Üía by applying cross-modal attention of the modality l on the modality a is computed as the following procedure:\n\nwhere )) F F N expresses the transformation by the position-wise feed-forward block as:\n\nwhere ‚Ñ¶ 1 and ‚Ñ¶ 2 are linear projection matrices; b 1 and b 2 are biases.\n\nLikewise, we can easily compute the crossmodal representation\n\na‚Üíl , indicating that information from the modality a is transferred to the modality l. Finally, we concatenate all representations at the last layer, i.e., the D-th layer, to get the final cross-modal global-context representation Z",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Multimodal Emotion Classification",
      "text": "The local-and global context representation resulted in by the RT-GCN and P-CM modules are fused together to create the final representation of the conversation:\n\nwhere œÑ ‚àà {a, v, l}; F usion represents the concatenation method. H is then fed to a fully connected layer to predict the emotion label y i for the utterance u i :\n\nwhere Œ¶ 0 , Œ¶ 1 are learned parameters.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiments",
      "text": "This section investigate the efficacy of CORECT for the ERC task through extensive experiments in comparing with state-of-the-art (SOTA) baselines.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Experimental Setup",
      "text": "Dataset. We investigate two public real-life datasets for the multimodal ERC task including IEMOCAP  (Busso et al., 2008)  and CMU-MOSEI  (Bagher Zadeh et al., 2018) . The dataset statistics are given in Table  1 .\n\nIEMOCAP contains 12 hours of videos of twoway conversations from 10 speakers. Each dialogue is divided into utterances. There are in total 7433 utterances and 151 dialogues. The 6-way dataset contains six emotion labels, i.e., happy, sad, neutral, angry, excited, and frustrated, assigned to the utterances. As a simplified version, ambiguous pairs such as (happy, exited) and (sad, frustrated) are merged to form the 4-way dataset.\n\nCMU-MOSEI provides annotations for 7 sentiments ranging from highly negative (-3) to highly positive (+3), and 6 emotion labels including happiness, sadness, disgust, fear, surprise, and anger.\n\nEvaluation Metrics. We use weighted F1-score (w-F1) and Accuracy (Acc.) as evaluation metrics. The w-F1 is computed K k=1 f req k √ó F 1 k , where f req k is the relative frequency of class k. The accuracy is defined as the percentage of correct predictions in the test set.\n\nBaseline Models. CORECT is compared against SOTA baselines specific to each dataset. For IEMO-CAP, we consider two model groups namely: i) RNN-based models include bc-LSTM  (Poria et al., 2017) , CMN  (Hazarika et al., 2018b) , ICON  (Hazarika et al., 2018a) , DialogueRNN  (Majumder et al., 2019) ; ii) Graph-based methods are Dia-logueGCN  (Ghosal et al., 2019) , MMGCN  (Wei et al., 2019) , DialougueCRN  (Hu et al., 2021) , CHFusion  (Majumder et al., 2018) , and COGMEN  (Joshi et al., 2022) . For CMU-MOSEI, we inves-Modality Settings IEMOCAP (4-way) Acc. (%) w-F1 (%) bc-LSTM  (Poria et al., 2017)  75.20 75.13 CHFusion  (Majumder et al., 2018)  76.59 76.80 COGMEN  (Joshi et al., 2022)  82  tigate multimodal models including Multilogue-Net  (Shenoy and Sardana, 2020) , TBJE  (Delbrouck et al., 2020) , and COGMEN  (Joshi et al., 2022) .\n\nImplementation Details. Due to the space limit, the implementation details for feature extraction and interaction are described in Appendix A.1.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Comparison With Baselines",
      "text": "We further qualitatively analyze CORECT and the baselines on the IEMOCAP (4-way), IEMOCAP (6-way) and MOSEI datasets.\n\nIEMOCAP: In the case of IEMOCAP (6-way) dataset (Table  2 ), CORECT performs better than the previous baselines in terms of F1 score for individual labels, excepts the Sad and the Excited labels. The reason could be the ambiguity between similar emotions, such as Happy & Excited, as well as Sad & Frustrated (see more details in Figure  6  in Appendix A.2). Nevertheless, the accuracy and weighted F1 score of CORECT are 2.89% and 2.75% higher than all baseline models on average. Likewise, we observe the similar phenomena on the IEMOCAP (4-way) dataset with a 2.49% improvement over the previous state-of-the-art models as Table  3 . These results affirm the efficiency of CORECT for the multimodal ERC task.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Study",
      "text": "Effect of Main Components. The impact of main components in our CORECT model is presented via Table  5 . The model performance on the 6-way IEMOCAP dataset is remarkably degraded when the RT-GCN or P-CM module is not adopted with the decrease by 3.47% and 3.38% respectively. Similar phenomena is observed on the 4-way IEMOCAP dataset. Therefore, we can deduce that the effect of RT-GCN in the CORECT model is more significant than that of P-CM.\n\nFor different relation types, ablating either R multi or R temp results in a significant decrease in the performance. However, the number of labels may affect on the multimodal graph construction, thus it is no easy to distinguish the importance of R multi and R temp for the multimodal ERC task.\n\nTable  8  (Appendix A.2) presents the ablation results for uni-and bi-modal combinations. In the unimodal settings, specifically for each individual modality (A, V, T), it's important to highlight that both P-CM module and multimodal relations R multi are non-existent. However, in bimodal combinations, the advantage of leveraging cross-modality information between audio and text (A+T) stands out, with a significant performance boost of over 2.75% compared to text and visual (T+V) modalities and a substantial 14.54% compared to visual and audio (V+A) modalities.\n\nAdditionally, our experiments have shown a slight drop in overall model performance (e.g., 68.32% in IEMOCAP 6-way, drop of 1.70%) when excluding Speaker Embedding S emb from CORECT.\n\nEffect of the Past and Future Utterance Nodes. We conduct an analysis to investigate the influence of past nodes (P) and future nodes (F) on the model's performance. Unlike previous studies   The red-dash line implies our best setting for P and F.  (Joshi et al., 2022; Li et al., 2023)  that treated P and F pairs equally, we explore various combinations of P and F settings to determine their effects. Figure  4  indicates that the number of past or future nodes can have different impacts on the performance. From the empirical analysis, the setting [P, F] of  [11, 9]  results in the best performance. This finding shows that the contextual information from the past has a stronger influence on the multimodal ERC task compared to the future context. For IEMOCAP (Table  2  and Table 3 ), the textual modality performs the best among the unimodal settings, while the visual modality yields the lowest results. This can be attributed to the presence of noise caused by factors, e.g., camera position, environmental conditions. In the bi-modal settings, combining the textual and acoustic modalities achieves the best performance, while combin-ing the visual and acoustic modalities produces the worst result. A similar trend is observed in the CMU-MOSEI dataset (Table  4 ), where fusing all modalities together leads to a better result compared to using individual or paired modalities.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Effect Of Modality.",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose CORECT, an novel network architecture for multimodal ERC. It consists of two main components including RT-GCN and P-CM. The former helps to learn local-context representations by leveraging modality-level topological relations while the latter supports to infer cross-modal global-context representations from the entire dialogue. Extensive experiments on two popular benchmark datasets, i.e., IEMOCAP and CMU-MOSEI, demonstrate the effectiveness of CORECT, which achieves the new state-of-theart record for multimodal conversational emotion recognition. Furthermore, we also provide ablation studies to investigate the contribution of various components in CORECT. Interestingly, by analyzing the temporal aspect of conversations, we have validated that capturing the long-term dependencies, e.g., past relation, improves the performance of the multimodal emotion recognition in conversations task.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Limitations",
      "text": "Hyper-parameter tuning is a vital part of optimizing machine learning models. Not an exception, the learning of CORECT is affected by hyperparameters such as the number of attention head in P-CM module, the size of Future and Past Window. Due to time constraints and limited computational resources, it was not possible to tune or exploring all possible combinations of these hyperparameters, which might lead to local-minima convergences. In future, one solution for this limitation is to employ automated hyper-parameter optimization algorithms, to systematically explore the hyperparameter space and improve the robustness of the model. As another solution, we may upgrade CORECT with learning mechanisms to automatically leverage important information, e.g., attention mechanism on future and past utterances.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A Appendix",
      "text": "A.1 Implementation Details",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.1.1 Multimodal Raw Feature Extraction",
      "text": "The multimodal feature extraction process involves extracting features from the acoustic, lexical, and visual modalities for each utterance.\n\nFor IEMOCAP, the audio features, with a size of 100, are obtained using the OpenSmile Toolkit  (Eyben et al., 2010) ; visual features, with a size of 512, are extracted using OpenFace  (Baltrusaitis et al., 2018) ; textual features, with a size of 768, are derived using sBERT  (Reimers and Gurevych, 2019) .\n\nFor MOSEI, the audio features are extracted using librosa  (McFee et al., 2015)  with 80 filter banks, resulting in a feature vector size of 80.The visual features, with a size of 35, are obtained from  (Bagher Zadeh et al., 2018) . The textual features, with a size of 768, are obtained using sBERT  (Reimers and Gurevych, 2019) .  x ùê∑ Layers ùê∂ùëÄ ùõΩ‚Üíùõº (ùëç ùõΩ‚Üíùõº [ùëñ-1] , ùëç ùõΩ [0] )\n\nFigure  5 : Illustration of the P-CM module between modality Œ≤ and Œ±.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.2 Additional Experiment Result",
      "text": "Table  8  showcases the results on the IEMOCAP dataset (both 6-way and 4-way) for all the modality combinations of the CORECT model, while Table 7 presents an ablation study conducted on the CMU-MOSEI dataset, considering various modality combination settings.\n\nFigure  6  shows the confusion matrix for prediction on IEMOCAP (4-way) and IEMOCAP (6way), respectively.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.3 Reproducibility",
      "text": "CORECT is implemented using Pytorch 2 , and run experiments on Google Colab Pro. We choose Adam as the optimizer and set the dropout rate to 0.5. The numbers of multi-head attentions used in Graph Transformer and P-CM are selected as 7 and 2, respectively. For IEMOCAP dataset, the learning rate is 0.0003; Window size [P, F] is tested on various settings in the range of  [1, 15] . For CMU-MOSEI dataset, the learning rate is 0.0006; Window size [P, F] is picked between  [5, 4]  due to the property of short dialogue in CMU-MOSEI. Refer-",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of temporal effects on conversations",
      "page": 1
    },
    {
      "caption": "Figure 1: The sentence",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the architecture of CORECT",
      "page": 3
    },
    {
      "caption": "Figure 2: Framework illustration of CORECT for the multimodal emotion recognition in conversations",
      "page": 4
    },
    {
      "caption": "Figure 3: provides an illustrative",
      "page": 4
    },
    {
      "caption": "Figure 3: An example construction of a graph illustrat-",
      "page": 5
    },
    {
      "caption": "Figure 6: in Appendix A.2). Nevertheless, the accuracy",
      "page": 7
    },
    {
      "caption": "Figure 4: The effects of P and F nodes in the past and",
      "page": 9
    },
    {
      "caption": "Figure 4: indicates that the number of past or future",
      "page": 9
    },
    {
      "caption": "Figure 5: illustrates details of Pairwise Cross-modal",
      "page": 13
    },
    {
      "caption": "Figure 5: Illustration of the P-CM module between",
      "page": 13
    },
    {
      "caption": "Figure 6: shows the confusion matrix for pre-",
      "page": 13
    },
    {
      "caption": "Figure 6: Visualization the confusion matrices of",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The results on IEMOCAP (6-way) multimodal (A+V+T) setting. The results in bold indicate the",
      "data": [
        {
          "Methods": "",
          "IEMOCAP (6-way)": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated"
        },
        {
          "Methods": "bc-LSTM (Poria et al., 2017)\nCMN (Hazarika et al., 2018b)\nICON (Hazarika et al., 2018a)\nDialogueRNN (Majumder et al., 2019)\nDialogueGCN (Ghosal et al., 2019)\nMMGCN (Wei et al., 2019)\nDialogueCRN (Hu et al., 2021)\nCOGMEN (Joshi et al., 2022)",
          "IEMOCAP (6-way)": "32.63\n70.34\n51.14\n63.44\n67.91\n61.06\n30.38\n62.41\n52.39\n59.83\n60.25\n60.69\n29.91\n64.57\n57.38\n63.04\n63.42\n60.81\n33.18\n78.80\n59.21\n65.28\n71.86\n58.91\n80.88\n47.10\n58.71\n66.08\n70.97\n61.21\n45.45\n77.53\n61.99\n66.70\n72.04\n64.12\n51.59\n74.54\n62.38\n67.25\n73.96\n59.97\n74.91\n55.76\n80.17\n63.21\n61.69\n63.90"
        },
        {
          "Methods": "CORECT (Ours)",
          "IEMOCAP (6-way)": "59.30\n66.94\n69.59\n68.50\n80.53\n72.69"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: The results on IEMOCAP (6-way) multimodal (A+V+T) setting. The results in bold indicate the",
      "data": [
        {
          "Modality Settings": "",
          "IEMOCAP (4-way)": "Acc. (%)\nw-F1 (%)"
        },
        {
          "Modality Settings": "bc-LSTM (Poria et al., 2017)\nCHFusion (Majumder et al., 2018)\nCOGMEN (Joshi et al., 2022)",
          "IEMOCAP (4-way)": "75.20\n75.13\n76.59\n76.80\n82.29\n82.15"
        },
        {
          "Modality Settings": "CORECT (Ours)",
          "IEMOCAP (4-way)": "84.73 (‚Üë 2.44)\n84.64 (‚Üë 2.49)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 8: (Appendix A.2) presents the ablation",
      "data": [
        {
          "Methods": "",
          "Sentiment Classification\nAccuracy (%)": "2 Class\n7 Class",
          "Emotion Classification (Binary, 1 vs. all)\nweighted F1-score (%)": "Happiness\nSadness\nAngry\nFear\nDisgust\nSurprise"
        },
        {
          "Methods": "Multilouge-Net (Shenoy and Sardana, 2020)\nTBJE (Delbrouck et al., 2020)\nCOGMEN (Joshi et al., 2022)",
          "Sentiment Classification\nAccuracy (%)": "82.88\n44.83\n82.40\n43.91\n82.95\n45.22",
          "Emotion Classification (Binary, 1 vs. all)\nweighted F1-score (%)": "87.79\n86.05\n67.84\n65.34\n67.03\n74.91\n87.79\n65.91\n70.78\n70.86\n82.57\n86.04\n87.79\n86.05\n70.88\n70.91\n74.20\n81.83"
        },
        {
          "Methods": "CORECT (Ours)",
          "Sentiment Classification\nAccuracy (%)": "83.66\n46.31",
          "Emotion Classification (Binary, 1 vs. all)\nweighted F1-score (%)": "71.35\n72.86\n76.77\n87.90\n84.26\n86.48"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 8: (Appendix A.2) presents the ablation",
      "data": [
        {
          "Sub-\nModules": "",
          "IEMOCAP\n(6-way)": "Acc. (%)\nw-F1 (%)",
          "IEMOCAP\n(4-way)": "Acc. (%)\nw-F1 (%)"
        },
        {
          "Sub-\nModules": "-w/o RT-GCN\n-w/o P-CM",
          "IEMOCAP\n(6-way)": "66.61\n66.55 (‚Üì 3.47)\n66.54\n66.64 (‚Üì 3.38)",
          "IEMOCAP\n(4-way)": "80.69\n80.54 (‚Üì 4.10)\n82.18\n82.16 (‚Üì 2.48)"
        },
        {
          "Sub-\nModules": "-w/o Rmulti\n-w/o Rtemp",
          "IEMOCAP\n(6-way)": "66.54\n66.82 (‚Üì 3.20)\n67.04\n67.34 (‚Üì 2.68)",
          "IEMOCAP\n(4-way)": "82.61\n82.53 (‚Üì 2.11)\n82.08\n82.07 (‚Üì 2.57)"
        },
        {
          "Sub-\nModules": "CORECT",
          "IEMOCAP\n(6-way)": "69.93\n70.02",
          "IEMOCAP\n(4-way)": "84.73\n84.64"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: and Table 3), the tex-",
      "data": [
        {
          "Modality\nSettings": "",
          "IEMOCAP\n(6-way)": "Acc. (%)\nw-F1 (%)",
          "IEMOCAP\n(4-way)": "Acc. (%)\nw-F1 (%)"
        },
        {
          "Modality\nSettings": "A\nT\nV",
          "IEMOCAP\n(6-way)": "52.31\n51.49\n67.22\n67.26\n38.63\n37.67",
          "IEMOCAP\n(4-way)": "67.02\n65.48\n82.82\n82.65\n49.73\n47.97"
        },
        {
          "Modality\nSettings": "A+T\nT+V\nV+A",
          "IEMOCAP\n(6-way)": "68.27\n68.36\n65.50\n65.61\n54.16\n53.82",
          "IEMOCAP\n(4-way)": "83.14\n83.13\n81.76\n81.75\n69.03\n68.21"
        },
        {
          "Modality\nSettings": "CORECT (A+T+V)",
          "IEMOCAP\n(6-way)": "69.93\n70.02",
          "IEMOCAP\n(4-way)": "84.73\n84.64"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "",
          "Modality\nSettings": "",
          "Sentiment Class\nAccuracy (%)": "2 Class\n7 Class",
          "Emotion Class\nweighted F1-score (%)": "Happiness\nSadness\nAngry\nFear\nDisgust\nSurprise"
        },
        {
          "Datasets": "Multilogue-Net\n(Shenoy and Sardana, 2020)",
          "Modality\nSettings": "A+T+V",
          "Sentiment Class\nAccuracy (%)": "82.88\n44.83",
          "Emotion Class\nweighted F1-score (%)": "87.79\n86.05\n67.84\n65.34\n67.03\n74.91"
        },
        {
          "Datasets": "TBJE\n(Delbrouck et al., 2020)",
          "Modality\nSettings": "A+T",
          "Sentiment Class\nAccuracy (%)": "82.4\n43.91",
          "Emotion Class\nweighted F1-score (%)": "87.79\n65.91\n70.78\n70.86\n82.57\n86.04"
        },
        {
          "Datasets": "COGMEN\n(Joshi et al., 2022)",
          "Modality\nSettings": "A+T+V",
          "Sentiment Class\nAccuracy (%)": "82.95\n45.22",
          "Emotion Class\nweighted F1-score (%)": "87.79\n86.05\n70.88\n70.91\n74.20\n81.83"
        },
        {
          "Datasets": "CORECT (Ours)",
          "Modality\nSettings": "T\nA+T\nA+T+V",
          "Sentiment Class\nAccuracy (%)": "84.13\n45.80\n84.28\n44.89\n46.31\n83.66",
          "Emotion Class\nweighted F1-score (%)": "87.79\n86.05\n67.82\n72.12\n75.55\n84.63\n87.79\n86.05\n84.69\n67.49\n71.53\n75.39\n71.35\n72.86\n76.77\n87.90\n86.48\n84.26"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "A": "Acc\nW-F1",
          "T": "Acc\nW-F1",
          "V": "Acc\nW-F1",
          "A+T": "Acc\nW-F1",
          "T+V": "Acc\nF1",
          "V+A": "Acc\nW-F1",
          "A+V+T": "Acc\nF1"
        },
        {
          "Dataset": "IEMOCAP\n(6-way)",
          "A": "35.12\n30.01\n-\n-\n-\n-\n41.53\n39.49",
          "T": "64.7\n64.34\n-\n-\n-\n-\n63.65\n63.72",
          "V": "30.99\n26.88\n-\n-\n-\n-\n27.66\n27.37",
          "A+T": "67.10\n66.92\n65.87\n65.89\n66.30\n66.27\n67.34\n67.33",
          "T+V": "65.37\n65.50\n65.00\n65.07\n64.76\n64.78\n65.43\n65.29",
          "V+A": "52.13\n51.80\n53.54\n52.86\n53.67\n53.48\n50.65\n49.67",
          "A+V+T": "66.61\n66.55\n66.54\n66.64\n66.54\n66.82\n67.04\n67.34"
        },
        {
          "Dataset": "",
          "A": "52.31\n51.49",
          "T": "67.22\n67.26",
          "V": "38.63\n37.67",
          "A+T": "68.27\n68.36",
          "T+V": "65.50\n65.61",
          "V+A": "54.16\n53.82",
          "A+V+T": "69.93\n70.02"
        },
        {
          "Dataset": "IEMOCAP\n(4-way)",
          "A": "55.25\n52.18\n-\n-\n-\n-\n56.84\n54.88",
          "T": "80.38\n80.25\n-\n-\n-\n-\n80.70\n80.70",
          "V": "34.04\n31.33\n-\n-\n-\n-\n41.04\n39.75",
          "A+T": "81.87\n81.18\n80.91\n80.94\n81.76\n81.78\n82.08\n81.99",
          "T+V": "80.17\n80.04\n80.38\n80.04\n80.38\n80.47\n81.34\n81.36",
          "V+A": "58.96\n58.57\n69.25\n69.00\n69.14\n68.84\n57.16\n56.62",
          "A+V+T": "80.69\n80.54\n82.18\n82.16\n82.61\n82.53\n82.08\n82.07"
        },
        {
          "Dataset": "",
          "A": "67.02\n65.48",
          "T": "82.82\n82.85",
          "V": "49.73\n47.97",
          "A+T": "83.14\n83.13",
          "T+V": "81.76\n81.75",
          "V+A": "69.03\n68.21",
          "A+V+T": "84.73\n84.64"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "2",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "3",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "No√© Tits",
        "Mathilde Brousmiche",
        "St√©phane Dupont"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.1"
    },
    {
      "citation_id": "8",
      "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "10",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "11",
      "title": "A new model for learning in graph domains",
      "authors": [
        "Marco Gori",
        "Gabriele Monfardini",
        "Franco Scarselli"
      ],
      "year": "2005",
      "venue": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "13",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "14",
      "title": "Dia-logueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "15",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "16",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "17",
      "title": "Graphmft: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Conversational emotion recognition using self-attention mechanisms and graph neural networks",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang",
        "Zhanlei Yang",
        "Rongjun Li"
      ],
      "year": "2020",
      "venue": "In INTERSPEECH"
    },
    {
      "citation_id": "20",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems"
    },
    {
      "citation_id": "21",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "23",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Andrei Nicolicioiu",
        "Iulia Duta",
        "Marius Leordeanu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1410"
    },
    {
      "citation_id": "26",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori",
        "Ah Chung Tsoi",
        "Markus Hagenbuchner",
        "Gabriele Monfardini"
      ],
      "year": "2008",
      "venue": "The graph neural network model"
    },
    {
      "citation_id": "27",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "Michael Schlichtkrull",
        "Thomas Kipf",
        "Peter Bloem",
        "Rianne Van Den",
        "Ivan Berg",
        "Max Titov",
        "Welling"
      ],
      "year": "2018",
      "venue": "The Semantic Web: 15th International Conference"
    },
    {
      "citation_id": "28",
      "title": "A survey on automatic multimodal emotion recognition in the wild",
      "authors": [
        "Garima Sharma",
        "Abhinav Dhall"
      ],
      "year": "2021",
      "venue": "Advances in data science: Methodologies and applications"
    },
    {
      "citation_id": "29",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "31",
      "title": "Multiloguenet: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "32",
      "title": "MultiEMO: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.824"
    },
    {
      "citation_id": "33",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1656"
    },
    {
      "citation_id": "34",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Graph attention networks",
      "authors": [
        "Petar Velickovic",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Li√≤",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "6th International Conference on Learning Representations, ICLR 2018"
    },
    {
      "citation_id": "36",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao",
        "Antonio Liotta",
        "Dawei Yang",
        "Xinlei Li",
        "Shuyong Gao",
        "Yixuan Sun",
        "Weifeng Ge",
        "Wei Zhang"
      ],
      "year": "2022",
      "venue": "A systematic review on affective computing: Emotion models, databases, and recent advances"
    },
    {
      "citation_id": "37",
      "title": "Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "38",
      "title": "MTAG: Modaltemporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.79"
    },
    {
      "citation_id": "39",
      "title": "Graph transformer networks",
      "authors": [
        "Seongjun Yun",
        "Minbyul Jeong",
        "Raehyun Kim",
        "Jaewoo Kang",
        "Hyunwoo Kim"
      ],
      "year": "2019",
      "venue": "Graph transformer networks"
    },
    {
      "citation_id": "40",
      "title": "Memory fusion network for multiview sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "41",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    }
  ]
}