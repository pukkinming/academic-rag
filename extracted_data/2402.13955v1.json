{
  "paper_id": "2402.13955v1",
  "title": "Bee-Net: A Deep Neural Network To Identify In-The-Wild Bodily Expression Of Emotions",
  "published": "2024-02-21T17:35:51Z",
  "authors": [
    "Mohammad Mahdi Dehshibi",
    "David Masip"
  ],
  "keywords": [
    "End-to-end",
    "Deep learning",
    "Body language",
    "Emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "H UMANS recognise and perceive the emotional expressions of others to use these critical cues for successful nonverbal communication. Equipping computers with the ability to recognise, perceive, process, and simulate human affects will aid in the development of empathetic devices that can be used in monitoring certain mental/physical disorders  [3, 10] , home-assistant devices, public safety, or the analysis of social media data  [8, 13] . Emotion perception has typically been studied by analysing facial expressions  [29] , body postures and gestures  [24, 28] , and physiological signs  [2, 9, 11] . However, psychological studies have shown that body language can convey individuals' emotional state  [1, 4] , and the lack of a high-quality and diverse database with ground truth makes Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE) a challenging research topic.\n\nSeveral studies have assessed the variables that humans naturally use from a young age to provide a more reliable measure of an individual's emotional state by fusing different modalities such as facial expressions, speech prosody, and context-related data  [20, 30] . Body language has recently been used to help understand other people's emotional states. According to Beck's cognitive depression theory  [5] , people suffering from depression tend to view themselves in mostly negative and dark environments. This intuition motivated us to propose a deep learning architecture that incorporates meta-information about the environment and the objects involved to reinforce AIBEE.\n\nWe hypothesise that the environment and the objects involved greatly influence our perception of the in-the-wild bodily expressions of emotions. Therefore, rather than isolating human bodies in video frames, we propose a multi-stream convolutional neural network (BEE-NET) that incorporates prior knowledge about the joint probability of emotions and both available and anticipated non-available places/objects. This scalable architecture is differentiable, allowing for end-to-end model training without additional post-processing or regularisation. We evaluate the proposed method using the Body Language database (BoLD)  [24] , which is by far the largest database available for AIBEE. Experimental results indicate that the proposed method outperforms the state-ofthe-art in identifying discrete and continuous in-the-wild bodily expressions of emotions.\n\nThe rest of this paper is organised as follows: Section 2 surveys the previous studies. The proposed method is detailed in Section 3. Architectural design and implementation details are given in Section 4. Section 5 presents experiments results. Finally, Section 6 concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "Detecting actions in videos is a critical step toward understanding human behaviour. In this context, high-level reasoning is required not only in the spatial dimension but also in the temporal dimension. Simonyan and Zisserman  [34]  proposed one of the most promising multi-task convolutional neural network (CNN) architectures for action recognition, integrating spatial and temporal networks. Tran et al.  [37]  proposed using 3D CNNs for general action recognition with the separate convolution of spatial and temporal filters, which increased recognition accuracy in AVA benchmark  [17] . Feichtenhofer et al.  [14]  suggested a two-pathway network with a low frame rate path focused on spatial information extraction and a high frame rate path focused on motion encoding. Hussein et al.  [18]  developed a multi-scale temporal convolution approach, employing various kernel sizes and dilation rates to capture temporal dependencies.\n\nUlutan et al.  [38]  proposed combining actor features with each Spatio-temporal region in the scene to generate attention maps between the actor and the context. Girdhar et al.  [16]  suggested a Transformer-style architecture for weighting actors based on contextual features. Wang and Gupta  [41]  suggested modelling arXiv:2402.13955v1 [cs.CV] 21 Feb 2024 a video clip to combine whole clip features and weighted proposal features computed by a graph convolutional network based on feature space similarities and Spatio-temporal distances between each detection. Tomei et al.  [36]  propose a graph-based framework for learning high-level interactions between people and objects. The Spatio-temporal relationships are learned through self-attention on a multi-layer graph structure to link entities from consecutive clips for a wide range of Spatio-temporal dependencies.\n\nHuman action recognition in-the-wild has to deal with challenges such as different degrees of freedom, heterogeneity in people's behaviour, cluttered backgrounds, and variations in size, pose, and camera viewpoint  [28, 43] . Furthermore, existing benchmark databases, such as AVA  [17] , lacked tags for human affects. Therefore, several studies have focused on facial expressions rather than body gestures to identify emotions that are less subjective thanks to the introduction of the Facial Action Coding System  [12]  and more flexible as a result of the face having fewer degrees of freedom than the body  [32] . Mollahosseini et al.  [27]  proposed a six-layer CNN with two convolution layers and four inception layers to classify facial expressions in still images. Pons and Masip  [29]  proposed a CNN committee in which a multi-task learning loss function integrates the detector of facial action units into the emotion learning model to solve the problem of learning multiple tasks with heterogeneously labelled data. Microexpressions were considered by Xu et al.  [42]  to add nuances to facial expression recognition. Luvizon et al.  [25]  used a multitask learning approach, and Li et al.  [22]  used a Spatio-temporal graph CNN to leverage pose knowledge and improve the accuracy of bodily expression recognition across multiple modalities.\n\nGiven that bodily expressions can convey emotional states and, in some cases, are the only modality that can be used to correctly disambiguate the corresponding facial expression  [1, 4, 8] , recent studies attempted to incorporate bodily expressions of emotions into affective computing by introducing benchmark models and databases such as the EMOTIC  [20]  and BoLD  [24] . Kosti et al.  [20]  introduced the EMOTIC database, which included information about valence, arousal, and dominance dimensions in addition to the six basic emotions. They used a two-stream CNN to extract features that represented the body expression and the scene description. Luo et al.  [24]  introduced the BoLD database and proposed a framework combining human identification, pose estimation, and learning representation to recognise bodily expressions of emotions. Kumar et al.  [21]  propose using the BoLD database to train a noisy student network in which various face regions are processed independently using a multilevel attention mechanism to enhance facial expression recognition incrementally.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bee-Net Multi-Stream Architecture",
      "text": "We formulate the AIBEE as a regression problem in which the response Y ∈ R dy is predicted given the input image X ∈ R dx , and the loss is measured by the mean squared error (L 2 ). Given a database of n images D = {(x i , y i )} n i=1 , with x i ∈ R dx and y i ∈ R dy , our goal is to learn a neural network H(x) = E[Y|X = x] that minimises the loss function.\n\nIn this study, we use BoLD database  [24]  in which the input image is labelled by d y = 29 emotions. We scale the input image X to the size of d x = 224 × 224. The emotion tag Y consists of 26 discrete emotions with values in the range of [0, 1] and three continuous emotions with values in the range of  [1, 10]  (see Fig.  1(a) ). We assume that each image consists of three components, namely emotion, place and object. Emotion tags are provided in D. To incorporate contextual information (place and object) that was not explicitly provided in the BoLD database, we used the transfer learning strategy to incorporate pseudo-ground truths into the learning model. Figures  1(b ) and 1(c) show the provided place and object tags for the input image using the Places-CNN scene descriptor and the YOLO object detector  [31] , which trained on the Places2  [45]  and Microsoft COCO  [23]  databases, respectively. Mensink et al.  [26]  reported that the co-occurrence of attributes in a model's training phase could occur with high probability in the testing phase. Integrating attribute co-occurrence also helps to diminish unwanted outliers introduced by domain integration and makes L 2 a good approximation for the loss function  [15, 44] . However, fine-tuning the pre-trained models with a database where attribute labels are not explicitly provided is not possible.\n\nTo address this problem, we propose a multi-stream convolutional neural network (BEE-NET) in which the pooling layer and loss function drive the emotion learning process during training using a priori contextual information about the joint probability of emotions and both available and anticipated non-available places/objects. We formulated a derivable pooling scheme based on Bayesian theory to fuse the extracted uncertain information with the predicted image-based emotional states, allowing endto-end model training to capture the hidden correlation between data without additional post-processing or regularisation. Fig.  2(a)  illustrates the proposed architecture.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Network Architecture",
      "text": "The proposed architecture is composed of three main streams: (i) the scene descriptor stream, which determines the probability of place categories associated with the input image; (ii) the object detector stream, which detects objects involved in the input image; and (iii) the emotion stream, which focuses on learning the map between the input image and the emotions. The initial network stem (H base ) is a feature encoder implemented with GoogLeNet  [35]  where the output of the Inception (4d) module (F ∈ R W ×H×D ) is fed into the rest of streams. W = 14, H = 14 and D = 528 are the width, height, and number of channels of the feature map, respectively. (i) Scene descriptor (H place ): resembles the architecture of the Places-CNN with the GoogLeNet backbone to provide the pseudoground-truth for place tags. The layers include the Inception (4e), (5a) and (5b), global average pooling, dropout, fully connected and softmax modules. More formally, H place : F → z place , where z place ∈ R 1×1×365 is a vector representing the probability of place categories. To incorporate z place into the proposed probabilistic model and harmonise the dimensionality of each stream, we convolve z place with a filter bank and add the bias using Eq. 1.\n\nwhere ⊛ performs the convolution, κ is the number of output dimension, f place ∈ R 1×1×365×κ is the trainable filter, and b place ∈ R κ is the bias term.\n\n(ii) Object detector(H object ): provides the pseudo-ground-truth for object tags. This stream is composed of three groups of serially connected convolution, ReLU, and batch normalisation layers, in which the entire topmost feature map is used to predict confidences for multiple categories of objects at a single stage. In Inception (4e), the filter size is set to 14 × 14 to match the number of channels in F . The second (5a) and third (5b) convolution layers have the filter size of 7 × 7 to enable the model to detect small objects. These layers are followed by the transform and output layers, respectively. The transform layer transforms raw CNN output into the form required for object detections and is followed by the output layer, which defines and implements the 7 anchor boxes and loss function used to train the detector. Anchor boxes extract the activations of the last convolutional layer and match predicted bounding boxes with the ground truth. More formally, H object : F → z object , where\n\nrepresents the probability of object categories. With the same purpose as in the scene descriptor stream, we convolve z object with a trainable filter bank f object ∈ R 1×1×80×κ and add it with bias b object ∈ R κ as in Eq. 2.\n\n(iii) Emotion stream (H emotion ): This stream learns a regression model that maps the output of the initial network stem F into emotion (y emotion ∈ R 1×1×dy ). This stream is mainly composed of the Inception (4e), (5a) and (5b), global average pooling, dropout, and fully connected modules. Inspired by  [19] , we formulate the regression task with a softmax likelihood in Eq. 3. Note that here we intentionally drop the emotion superscript to simplify mathematical notations.\n\nwhere σ is a learnable noise scalar that observes the uniformity of a discrete distribution. In the maximum likelihood inference, we maximise the log-likelihood of the model. To obtain the loglikelihood of the model (Eq. 3), we can write the cross-entropy loss as Eq. 4, where H j (F ) is the j th element of the vector H(F ).\n\nIn the formulation of the cross-entropy loss, we assumed that\n\nto simplify the optimisation objective. This approximation becomes equality when σ -→ 1. This loss function is differentiable and avoids any division by zero. It also prevents the weights of the emotion stream from converging to zero when the co-occurrence probability of available and non-available meta-information is incorporated into the architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Probabilistic Pooling For Late Fusion In Bee-Net",
      "text": "Due to the association between emotions and the diversity of objects/places, it is not easy to accurately estimate the conditional probabilities of a given image x with regard to all attribute labels at the same time. To address this issue, we propose a multistream architecture (BEE-NET), which includes place (y place ) and object (y object ) auxiliary streams in addition to the emotion stream (y emotion ). As shown in Fig.  2 (a), the lower layers (F ) are shared across streams, while the top layers are separated to focus on the attributes for different contextual information. To fuse the extracted uncertain information from the place and object streams with the predicted image-based emotional states, we build a matrix, as given in Eq. 5, to incorporate prior knowledge about the joint probability of emotions and places/objects, which can be considered of as softmax classifier outputs stacked into a 2 × d y matrix P. Note that we substituted y place , y object , and y emotion with with A 1 , A 2 and B, to simplify mathematical notations.\n\nwhere Pr(B i |A j ) is the conditional probability of B i given the feature of the j th , j ∈ {1, 2} stream. To calculate Pr(B i |A j ) in the training set, we must first determine the number of occurrences of each emotion label as well as the number of co-occurrences of this emotion label given place/object pseudo-labels. If N i is the number of occurrences of the i th emotion in the dataset, then\n\nn is the probability of the i th label. Likewise, if N i,j is the co-occurrence number of the dataset's i th and j th labels,\n\nis a matrix, representing the joint probability of B i and A j . Therefore, we can obtain the conditional probability of one attribute given another, as shown in Eq. 6. Fig.  2 (b) shows the pseudo-colour plot of P + discovered on the BoLD training set, where the darker the blue, the lower the probability values.\n\nwhere p j is a high-order posterior probability expressing a correlation between the place (A 1 ) and object (A 2 ) streams given the i th emotion, as calculated by Eq. 7.\n\nBecause these high-order posterior probabilities cannot be calculated precisely, we approximate p j using local max-pooling over streams and apply local max-pooling over P + i,j (see Eq. 8).\n\nThe prediction result of the auxiliary streams, which are extracted from the input image using empirical knowledge about places/objects, has a significant impact on the accuracy of Eq. 8. Because these attributes are correlated, it is difficult for the streams to adequately separate them, resulting in a biased estimation of the conditional probability vector using Eq. 8. In addition, the columnar elements of Pr(B i ∩ A j ) are extremely small, resulting in relatively small max-pooling values. On the other hand, the unavailability of a meta-information in an input image may lead to the availability of another meta-information or vice versa. For instance, it is impossible to imagine people playing soccer in a pool full of water. For this reason, we also use conditional probability to measure the probability of the anticipated nonavailable meta-information (P -) by Eq. 9.\n\nWe then use P + and P -matrices to calculate the joint probability P as in Eq. 10, which is the output of the proposed pooling scheme.\n\nwhere 1 ∈ R 1×dy is a full one vector. By adding the proposed pooling scheme to the rest of the architecture, the assembled network H can predict bodily expressions of emotion ỹ given the input image x by Eq. 11.\n\nwhere λ is used to regulate the prediction results of the streams.\n\nIn order to minimise the loss function (ℓ) in Eq. 12, we use the stochastic gradient descent algorithm combined with the backpropagation strategy to update the network parameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Architecture Details",
      "text": "Experiments were performed on the NVIDIA RTX 2080 GPU with 8 GB of memory in MATLAB 2020a using the Deep Learning toolbox. The parameters for the proposed architecture are as follows:\n\n-Scene descriptor stream: In (H place ), we set weights and parameters of layers to the identical ones in Places-CNN, which was trained with Places2  [45]  database. To prevent layers of this stream from being overfitted during the training of H emotion on the BoLD database and force the stream to determine the probability of place categories, we set the learning rates to zero.\n\n-Object detector stream: In H object , we replace pooling layers with 16 × 16 and 32 × 32 strides, respectively. We use 7 anchor boxes (width, height) ∈ {(10, 13),  (16, 30) ,  (33, 23)  To set the weights and parameters of H object , we first built a deep neural network that was the serial connection of H base and H object . We trained this network in a separate scenario with the Microsoft COCO database  [23] . Throughout the training, we use a batch size of 8, a momentum of 0.9, and a decay of 5 × 10 -3 . The learning rate is set to 10 -2 , 10 -3 and 10 -4 for the first 75 epochs, the next 30 epochs, and the final 30 epochs, respectively. This policy is used to prevent the model from diverging due to unstable gradients. After training, we transferred the weights and parameters of the layers that had the same architecture as H object to the proposed architecture. Finally, we set the learning rates in these layers to zero in order to prevent these layers from being overfitted during the training of H emotion on the BoLD database.\n\n-Emotion stream: In H emotion , we train the network using stochastic gradient descent with momentum 0.9. The initial learning rate is set to 10 The proposed multi-stream architecture has two hyperparameters κ and λ that are used to align the meta-information dimensionality in the calculation of y place and y object and to regulate the prediction results of the streams in Eq. 11. To find the appropriate value for κ, we apply the pre-trained YOLO object detector and Places-CNN to the training set. By applying Places-CNN, the input image (x i ) maps into a 365-dimensional vector (z place i\n\n) containing the probability of place tags. However, this map cannot be straightforwardly built for the YOLO object detector.\n\nThe list of potential objects for each input image (x i ) may contain a different number of elements. Also, this list may contain multiple instances from one object with different confidence scores. To unify the codomain to which the input image is mapped, we define a vector with 80 entries (z object i\n\n), each of which corresponds to one object tag. We then assign the normalised cumulative confidence score of the detected objects to the corresponding entries of the object list (z object ) and set the rest of the entries to zero. For example, 2 instances of 'person', 5 instances of 'tie' and 1 instance of 'remote' were detected by the object detector in Fig.  1(c ). The normalised cumulative confidence scores of 0.19, 0.12 and 0.01 are assigned to the corresponding entries for these objects in the output list, and the rest of the entries are set to 0.\n\nThe maximum value of κ is equal to the minimum dimensionality of H place and H object outputs, i.e., κ = 80. We calculate the normalised sum of outputs for the place (z place = By applying the threshold of 0.01 to z place and z object , we found that 27 place and 14 object tags can meet the threshold. Therefore, we set the minimum value of κ to 14 and test its impact by changing the value from 14 to 80 with the step of 6.\n\nThe parameter κ enables f place and f object filters in Eq. 1 and 2 to select a subset of relevant predictions of H place and H object streams that contribute the most to the calculation of Eq. 5-10. The trainable filter kernel also enables the architecture to learn the correspondences of the feature maps for metainformation to minimise the loss function in Eq 12. Another hyper-parameter is λ that we evaluate its impact by changing the value from 0 to 0.5 with a 0.1 step. We trained the proposed architecture as a function of (κ, λ) and plotted the emotion recognition score (ERS) to find the best trade-off between these two hyper-parameters (see Fig.  4 ). We obtained an ERS value of 83.64 at κ = 56 and λ = 0.2 and used these values in the rest of the experiments. From Fig.  4  and the analysis of prediction errors, we inferred that larger values of (κ, λ) not only increase computational complexity but also make the result (ỹ) more inclined to the metainformation estimation of P + and P -.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Body Language Database",
      "text": "We performed our experiments on the Body Language database (BoLD)  [24] , which contains by far the most data for AIBEE. The videos in BoLD were chosen from the publicly available AVA database  [17] , which includes a collection of YouTube movie IDs. There are 9,876 video clips of humans expressing emotion, mainly through body gestures. The crowd-sourcing platform was employed to annotate the database with two widely accepted emotional categorisations.\n\nThere are 26 labels for categorical emotions, including {Peace, Affection, Esteem, Anticipation, Engagement, Confidence, Happiness, Pleasure, Excitement, Surprise, Sympathy, Doubt/confusion, Disconnection, Fatigue, Embarrassment, Yearning, Disapproval, Aversion, Annoyance, Anger, Sensitivity, Sadness, Disquietment, Fear, Pain, Suffering}. The continuous emotional dimensions are Valence, Arousal, and Dominance. It should be noted that, while the gathered videos are annotated based on body language, the movies with a close-up of the face rather than the entire or partially-occluded body remain unlabelled.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics And Experimental Protocols",
      "text": "We use the mean R 2 score (Eq. 13) to evaluate the proposed regression model. The R 2 metric calculates the ratio of explained variance (y) to measure how well the unseen samples are likely to be predicted by the model's independent variables.\n\nwhere ỹi is the is the predicted value of the i-th sample, and y i is the corresponding true value.\n\nSince we formulated the AIBEE as a regression problem, we applied the max threshold to convert the predicted quantity of the regression model into discrete buckets for emotions. The use of arg max lets us evaluate the efficiency of the proposed regression model using Precision, Recall and F 1, where F 1 is a weighted average of the precision and recall metrics. We report the average precision (mAP ), the mean area under the receiver's operating characteristic curve (mRA), and the mean of F 1 for the assessment. For ease of comparison, the Emotion Recognition Score (ERS) is also defined in Eq. 14.\n\nIn the experiments, we did not use data augmentation techniques to prevent unrealistic changes in colour, angle and position that could alter our hypothesis about the relationship between the representation of emotions and the environment and the objects involved. We partitioned the database into train, test and validation sets containing 60%, 30% and 10% of samples, respectively. The members of each set were chosen at random in such a way that the distribution of BoLD for each set was observed (see Fig.  3(a) ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "Table  1  shows the performance of the proposed architecture along with competitive methods  [7, 24, 40]  to verify the effectiveness of the BEE-NET. Following  [24] , we used a random method based on priors (referred to as 'Chance') as a basis for comparison. Laban Movement Analysis (LMA)  [39]  was originally developed to characterize dance movements by a set of structural and physical characteristics through representing body, effort, form and space. Because of the proximity of body language to this representation, Luo et al.  [24]  used LMA to identify the bodily expression of emotions. They used the method proposed by Cao et al.  [6]  to detect 2D poses in still images to use in LMA and reported promising results on the BoLD database for AIBEE.\n\nWe also compared the proposed architecture with two CNN architectures that were considered as state-of-the-art in action recognition. To use the Temporal Segment Networks (TSN)  [40]  in AIBEE, we split each video into 2 segments. During the training stage, one frame is randomly sampled for each segment, and the classification result is averaged over all the sampled frames. We set the learning rate and batch size to 10 -3 and 16, respectively. Other training requirements are similar to the original version. The twostream inflated 3D CNN  [7]  uses 3D convolution to learn Spatiotemporal features in an end-to-end way. However, we replaced 3D convolution with 2D convolution to perform experiments on still images that were randomly sampled from each video. In our experiment, we set the learning rate and batch size to 10 -2 and 16, respectively. We preserved other training details, as stated in the original version. Figure  5  provides comprehensive metric comparisons of all methods of each categorical and dimensional emotion. From Table  1  and Fig.  5 , it could be said that our hypothesis regarding the influence of the environment and the object involved in the disclosure of human emotions is valid. Moreover, it can be seen in Fig.  5  that the {Engagement, Happiness, Pleasure, Anticipation, Sadness} categories comprise the top-5 predictions. Indeed, the proposed architecture could appropriately address the bias problem (see Fig.  3(a) ) towards {Engagement, Anticipation, Confidence, Peace, Doubt} in the BoLD database. In the Ablation study, we will demonstrate that the formulation of the pooling scheme, where the probability of both available and anticipated non-available items being considered, could contribute to this achievement.\n\nIn the assessment of continuous emotions, all methods show a greater performance of arousal regression than valence and dominance. However, compared to the subjective test reported in  [24] , humans showed better valence-recognition performance than arousal. This distinction between human and model output indicates that domain knowledge and experience in other contexts allow humans to make better decisions in completely new situations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In the ablation study, we conducted four sets of experiments to better understand the efficacy of the BEE-NET for the AIBEE task. We therefore examine: -Contribution of pre-trained model weights: In this experiment, instead of assigning random weights to the H base + H emotion network filters, we used GoogLeNet filters' weight that was trained with ImageNet  [35] , Places2 and Microsoft COCO. We reduced the training steps to 45 iterations and retained all the other parameters as described in Section 4. The findings in Table  2  show that the initialisation of filters with random weights leads to a marginally better performance of AIBEE.  It can be inferred from Table  3  that the impact of H place is greater than that of H object . This effect is also evident in Table  2 , where the use of pre-trained model filters with the Microsoft COCO database, among other databases, resulted in lower performance. Furthermore, the frames were primarily sourced from older movies within the BoLD database, which inherently possess lower quality and smaller sizes. Therefore, locating the right and appropriate objects in the scene is met with an error that later propagates to the architecture. For example, a \"remote\" object in Fig.  1(c ) was detected by the object detector, which is an incorrect and unrelated object to the context. Moreover, considering the intent of the BoLD database, 'Person' is the dominant object in all scenes. Therefore, the majority of z object entries have a value of zero that, despite applying the f object kernel, the sparsity propagates to y object feature vector. In this way, the classifier must deal with the sparse representation in the latent space that reduces its performance. However, the influence of H object in the proposed architecture is undeniable as its combination with H place could improve the state-of-the-art in identifying in-thewild bodily expressions of emotions by 2.07%.\n\n-Contribution of face to AIBEE: To examine the impact of the face on BEE-NET performance, we filled the face area with black pixels in all database frames. Then, we trained the proposed architecture with new images, where the network parameters are retained as described in Section 4. Although masking the face had little effect on 'Person' detection due to the presence of H object stream, the ERS metric decreased to 62.35%. This remarkable decrease emphasises how facial expression can support the bodily expression of emotions.\n\n-Contribution of fusion strategies: Incorporating fusion strategies into deep learning models that deal with multi-modal input or extract features using multi-stream architectures can improve accuracy and performance. These fusion strategies are typically categorised into early, intermediate, and late fusion categories. This ablation study aims to compare the efficacy of the proposed probabilistic pooling-based late fusion strategy with other fusion strategies to demonstrate how leveraging meta-information can outperform conventional fusion strategies. To do this, we have modified the proposed architecture in the following ways and present the results in Table  4 .\n\n1) The early fusion strategy involves merging and processing all input data at the beginning of the neural network before performing feature extraction. Although an early fusion strategy can be useful for simple tasks, it may lead to overfitting. In this ablation study, we cannot apply the early fusion strategy as we feed the BEE-NET with unimodal data.\n\n2) The intermediate fusion strategy in multi-stream deep models combines features from multiple streams at an intermediate layer before performing the classification or regression. This fusion strategy usually uses concatenation, element-wise addition, or element-wise multiplication. In this ablation study, we replaced the proposed probabilistic pooling-based fusion strategy with an intermediate fusion strategy in BEE-NET. Specifically, we concatenated the output features of the place (H place ) and object (H object ) streams with the features of the initial network stem (H base ). We then passed the concatenated features through a fully connected layer to obtain a fused feature vector. Finally, we fed the emotion stream (H emotion ) with the fused features and continued the forward pass.\n\n3) The late fusion strategy in multi-stream deep neural networks involves combining the outputs of multiple streams at a later stage in the network architecture, typically after the individual streams have been processed by their own set of layers. In this ablation study, we conducted two experiments to assess the proposed fusion strategy. In the first experiment, we removed the probability of anticipated non-available meta-information by eliminating the P -and P terms from Eq. 10 and trained the BEE-NET with P = QP + . In the second experiment, we substituted the proposed fusion strategy with the one proposed by Kendall et al.  [19]  and proceeded with the forward pass.\n\nTable  4  reveals that the intermediate fusion strategy exhibits a marginal improvement over both late fusion strategies regarding the evaluation metrics. However, intriguingly, the late fusion strategy from which the probability of anticipated non-available meta-information is excluded shows reduced uncertainty in terms of Entropy (see Eq. 15) and Mutual Information (see Eq. 16) 1 .\n\n1. Entropy quantifies the uncertainty of a single random variable, while Mutual Information measures the shared information between two random variables. In our experiment, we employ kernel density estimation with a Gaussian kernel (i.e., N (µ, σ) = N (0, 1)) to estimate the probability density functions for predicted and ground-truth values. Subsequently, we derive the joint and marginal probability density functions from the estimated ones. In our experiment, we set the bandwidth value h to 1.06σn -\n\nwhere P (Y, Ỹ) is the joint probability density function of Y and Ỹ, and P Y and P Ỹ are the marginal probability density functions of Y (i.e., ground-truth values) and Ỹ, respectively. These findings support our hypothesis that the proposed probabilistic pooling-based late fusion strategy effectively utilises the meta-information provided by the place and object streams to confidently identify bodily expressions of emotions. Our results also highlight that the intermediate fusion strategy tends to dilute individual modalities' strengths and combine the individual models' uncertainties. This can lead to an overall higher level of uncertainty and inferior performance compared to our proposed fusion strategy. This observation aligns with the results presented in Table  3 , where we showed that if the object stream detects incorrect, unrelated, and dominant objects concerning the nature of the database, it can increase the sparsity, noise, and outlier in the latent space. These defects can be propagated throughout the model when the fusion strategy fails to leverage the correlation between the outputs from different streams.\n\nFinally, it is essential to note that the decision between late and intermediate fusion strategies should be carefully evaluated based on the specific requirements of the task, the characteristics of the data, and the available computational resources. While late fusion strategies may offer advantages in specific scenarios, intermediate fusion strategies can provide opportunities for more efficient and effective integration of multi-modal information. It is imperative to consider the unique contributions of each modality and the potential impact of noise when designing multi-modal deep learning architectures for optimal performance.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "Humans rely on emotional expressions to create meaningful interpersonal relationships. To enable computers to recognise, perceive, interpret, and simulate emotions as humans do, they must be equipped with the ability to understand and simulate human affects. Recent research has attempted to integrate bodily expressions of emotions into affective computing, as bodily expressions can convey emotional states and are sometimes the only modality that can accurately disambiguate the corresponding facial expression.\n\nThe present study investigated how environmental and object factors may influence the perception of in-the-wild bodily expressions of emotions. We proposed a novel multi-stream convolutional neural network (BEE-NET), which integrates pre-trained place and object recognition networks to represent contextual information. To incorporate this information, we formulated a derivable pooling scheme based on Bayes' theorem, which fuses the extracted uncertain information with the predicted imagebased emotional states. This allows for end-to-end model training and the acquisition of a priori information on the joint probability of emotions and both available and anticipated non-available places/objects, driving the emotion learning process during training.\n\nOur experimental results, obtained using the Body Language Database (BoLD), the largest database available for identifying in-the-wild bodily expressions of emotions, demonstrate that our proposed method outperforms the state-of-the-art in identifying categorical (discrete) and continuous in-the-wild bodily expressions of emotions. Specifically, we validated our hypothesis that explicitly incorporating the co-occurrences of available and anticipated non-available places/objects into the fusion strategy can simplify and guide the learning process, removing the need for the network to automatically discover the impact of these relationships on the decision. Overall, our proposed method, BEE-NET, provides an efficient and effective approach to incorporating contextual information into the emotion recognition process, which can lead to improved performance in real-world applications.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a)). We assume that each image consists of three",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) A sample from the BoLD database that mainly represents",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) BEE-NET architecture for the identification of in-the-wild bodily expression of emotions. Place and Object streams have a shade of grey",
      "page": 3
    },
    {
      "caption": "Figure 2: (a), the lower layers (F)",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) shows the pseudo-colour plot of P+ discovered",
      "page": 4
    },
    {
      "caption": "Figure 3: Cumulative probability of (a) labels in BoLD database, (b) pseudo-tag provided by applying place-CNN to BoLD database, and (c) pseudo-tag",
      "page": 5
    },
    {
      "caption": "Figure 1: (c). The normalised cumulative confidence scores",
      "page": 5
    },
    {
      "caption": "Figure 4: ). We obtained an ERS value of",
      "page": 5
    },
    {
      "caption": "Figure 4: The emotion recognition score for the proposed architecture",
      "page": 5
    },
    {
      "caption": "Figure 4: and the analysis of prediction errors, we inferred",
      "page": 5
    },
    {
      "caption": "Figure 5: provides comprehensive metric comparisons of all",
      "page": 6
    },
    {
      "caption": "Figure 5: , it could be said that our hypothesis regard-",
      "page": 6
    },
    {
      "caption": "Figure 5: that the {Engagement, Happiness, Pleasure,",
      "page": 6
    },
    {
      "caption": "Figure 3: (a)) towards {Engagement, Anticipation,",
      "page": 6
    },
    {
      "caption": "Figure 5: Classification performance for discrete emotions is reported based on the average precision (AP) in the [first row] and area under the",
      "page": 7
    },
    {
      "caption": "Figure 1: (c) was detected by the object detector, which is an incorrect",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the performance of the proposed architecture along",
      "page": 6
    },
    {
      "caption": "Table 1: The performance of BEE-NET in the test set. The mean measurement",
      "page": 6
    },
    {
      "caption": "Table 1: and Fig. 5, it could be said that our hypothesis regard-",
      "page": 6
    },
    {
      "caption": "Table 2: show that the initialisation of filters with random weights leads to",
      "page": 7
    },
    {
      "caption": "Table 2: Ablation study on the effect of pre-trained models.",
      "page": 7
    },
    {
      "caption": "Table 3: Ablation study on the effect of Hplace and Hobject streams.",
      "page": 7
    },
    {
      "caption": "Table 3: that the impact of Hplace is",
      "page": 7
    },
    {
      "caption": "Table 4: reveals that the intermediate fusion strategy exhibits a",
      "page": 8
    },
    {
      "caption": "Table 3: , where we showed that if the object stream detects",
      "page": 8
    },
    {
      "caption": "Table 4: Ablation study results on fusion strategies’ contribution, measured by mutual information (MI) and entropy (E) metrics. MI increases with accuracy",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Social interaction context shapes emotion recognition through body language, not facial expressions",
      "authors": [
        "L Abramson",
        "R Petranker",
        "I Marom",
        "H Aviezer"
      ],
      "year": "2020",
      "venue": "Emotion"
    },
    {
      "citation_id": "2",
      "title": "Clustering persian viseme using phoneme subspace for developing visual speech application",
      "authors": [
        "M Aghaahmadi",
        "M Dehshibi",
        "A Bastanfard",
        "M Fazlali"
      ],
      "year": "2013",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "3",
      "title": "A Multi-Stream Convolutional Neural Network for Classification of Progressive MCI in Alzheimer's Disease Using Structural MRI Images",
      "authors": [
        "Mona Ashtari-Majlan",
        "Abbas Seifi",
        "Mohammad Mahdi"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "4",
      "title": "Body Cues, Not Facial Expressions, Discriminate Between Intense Positive and Negative Emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "5",
      "title": "Depression: Clinical, experimental, and theoretical aspects",
      "authors": [
        "A Beck"
      ],
      "year": "1967",
      "venue": "Hoeber Medical Division"
    },
    {
      "citation_id": "6",
      "title": "OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields",
      "authors": [
        "Z Cao",
        "G Hidalgo",
        "T Simon",
        "S Wei",
        "Y Sheikh"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "7",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset"
    },
    {
      "citation_id": "8",
      "title": "A Deep Multimodal Learning Approach to Perceive Basic Needs of Humans From Instagram Profile",
      "authors": [
        "M Dehshibi",
        "Bita Baiani",
        "Gerard Pons",
        "David Masip"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect"
    },
    {
      "citation_id": "9",
      "title": "A new algorithm for age recognition from facial images",
      "authors": [
        "M Dehshibi",
        "A Bastanfard"
      ],
      "year": "2010",
      "venue": "Signal Process"
    },
    {
      "citation_id": "10",
      "title": "Pain Level and Pain-Related Behaviour Classification Using GRU-Based Sparsely-Connected RNNs",
      "authors": [
        "M Dehshibi",
        "T Olugbade",
        "F Diaz-De Maria",
        "N Bianchi-Berthouze",
        "A Tajadura-Jiménez"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Cubic norm and kernelbased bi-directional PCA: toward age-aware facial kinship verification",
      "authors": [
        "M Dehshibi",
        "J Shanbehzadeh"
      ],
      "year": "2019",
      "venue": "Vis. Comput"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "13",
      "title": "Toward Artificial Emotional Intelligence for Cooperative Social Human-Machine Interaction",
      "authors": [
        "B Erol",
        "A Majumdar",
        "P Benavidez",
        "P Rad",
        "K Choo",
        "M Jamshidi"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Comput. Soc. Syst"
    },
    {
      "citation_id": "14",
      "title": "SlowFast Networks for Video Recognition",
      "authors": [
        "C Feichtenhofer",
        "H Fan",
        "J Malik",
        "K He"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach",
      "authors": [
        "B Gholami",
        "P Sahu",
        "O Rudovic",
        "K Bousmalis",
        "V Pavlovic"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "16",
      "title": "Video Action Transformer Network",
      "authors": [
        "R Girdhar",
        "J Carreira",
        "C Doersch",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "17",
      "title": "AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions",
      "authors": [
        "C Gu",
        "C Sun",
        "D Ross",
        "C Vondrick",
        "C Pantofaru",
        "Y Li",
        "S Vijayanarasimhan",
        "G Toderici",
        "S Ricco",
        "R Sukthankar",
        "C Schmid",
        "J Malik"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Timeception for Complex Action Recognition",
      "authors": [
        "N Hussein",
        "E Gavves",
        "A Smeulders"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
      "authors": [
        "A Kendall",
        "Y Gal",
        "R Cipolla"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Context Based Emotion Recognition Using EMOTIC Dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "21",
      "title": "Noisy Student Training Using Body Language Dataset Improves Facial Expression Recognition",
      "authors": [
        "V Kumar",
        "S Rao",
        "L Yu"
      ],
      "year": "2020",
      "venue": "ECCV Workshops"
    },
    {
      "citation_id": "22",
      "title": "Spatio-Temporal Graph Routing for Skeleton-Based Action Recognition",
      "authors": [
        "B Li",
        "X Li",
        "Z Zhang",
        "F Wu"
      ],
      "year": "2019",
      "venue": "AAAI-19"
    },
    {
      "citation_id": "23",
      "title": "Microsoft COCO: Common Objects in Context",
      "authors": [
        "T Lin",
        "M Maire",
        "S Belongie",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "P Dollár",
        "C Zitnick"
      ],
      "year": "2014",
      "venue": "ECCV"
    },
    {
      "citation_id": "24",
      "title": "ARBEE: Towards Automated Recognition of Bodily Expression of Emotion in the Wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning",
      "authors": [
        "D Luvizon",
        "D Picard",
        "H Tabia"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "26",
      "title": "COSTA: Co-Occurrence Statistics for Zero-Shot Classification",
      "authors": [
        "T Mensink",
        "E Gavves",
        "C Snoek"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "In WACV"
    },
    {
      "citation_id": "28",
      "title": "Survey on Emotional Body Gesture Recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "29",
      "title": "Multitask, multilabel, and multidomain learning with convolutional networks for emotion recognition",
      "authors": [
        "G Pons",
        "D Masip"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "30",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "31",
      "title": "You Only Look Once: Unified, Real-Time Object Detection",
      "authors": [
        "J Redmon",
        "S Divvala",
        "R Girshick",
        "A Farhadi"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Recognizing emotions expressed by body pose: A biologically inspired neural model",
      "authors": [
        "K Schindler",
        "L Van Gool",
        "B Gelder"
      ],
      "year": "2008",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "33",
      "title": "Density Estimation for Statistics and Data Analysis",
      "authors": [
        "Bernard Silverman"
      ],
      "year": "1998",
      "venue": "Density Estimation for Statistics and Data Analysis"
    },
    {
      "citation_id": "34",
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "NIPS"
    },
    {
      "citation_id": "35",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "36",
      "title": "Video action detection by learning graph-based spatio-temporal interactions",
      "authors": [
        "M Tomei",
        "L Baraldi",
        "S Calderara",
        "S Bronzin",
        "R Cucchiara"
      ],
      "year": "2021",
      "venue": "Comput. Vis. Image Underst"
    },
    {
      "citation_id": "37",
      "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "38",
      "title": "Actor Conditioned Attention Maps for Video Action Detection",
      "authors": [
        "O Ulutan",
        "S Rallapalli",
        "M Srivatsa",
        "C Torres",
        "B Manjunath"
      ],
      "year": "2020",
      "venue": "WACV"
    },
    {
      "citation_id": "39",
      "title": "Choreutics. Macdonald and Evans",
      "authors": [
        "Von Laban"
      ],
      "year": "1966",
      "venue": "Choreutics. Macdonald and Evans"
    },
    {
      "citation_id": "40",
      "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "D Lin",
        "X Tang",
        "L Van Gool"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "41",
      "title": "Non-local neural networks",
      "authors": [
        "X Wang",
        "R Girshick",
        "A Gupta",
        "K He"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "Microexpression Identification and Categorization Using a Facial Dynamics Map",
      "authors": [
        "F Xu",
        "J Zhang",
        "J Wang"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "43",
      "title": "A review of multimodal human activity recognition with special emphasis on classification, applications, challenges and future directions",
      "authors": [
        "S Yadav",
        "K Tiwari",
        "H Pandey",
        "S Akbar"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "44",
      "title": "Unsupervised Person Re-Identification by Soft Multilabel Learning",
      "authors": [
        "H Yu",
        "W Zheng",
        "A Wu",
        "X Guo",
        "S Gong",
        "J Lai"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "45",
      "title": "Places: A 10 Million Image Database for Scene Recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    }
  ]
}