{
  "paper_id": "2510.06072v1",
  "title": "Emohrnet: High-Resolution Neural Network Based Speech Emotion Recognition",
  "published": "2025-10-07T15:59:40Z",
  "authors": [
    "Akshay Muppidi",
    "Martin Radfar"
  ],
  "keywords": [
    "Speech emotion recognition",
    "High Resolution Network",
    "Frequency Masking",
    "Time Masking"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is pivotal for enhancing human-machine interactions. This paper introduces \"EmoHRNet\", a novel adaptation of High-Resolution Networks (HRNet) tailored for SER. The HRNet structure is designed to maintain high-resolution representations from the initial to the final layers. By transforming audio samples into spectrograms, EmoHRNet leverages the HRNet architecture to extract high-level features. EmoHRNet's unique architecture maintains high-resolution representations throughout, capturing both granular and overarching emotional cues from speech signals. The model outperforms leading models, achieving accuracies of 92.45% on RAVDESS, 80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new benchmark in the SER domain.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has emerged as a pivotal domain, instrumental in advancing robot intelligence and human-machine interactions  [1] . Recognizing emotions from speech signals can substantially enhance the communication quality between humans and machines. However, discerning emotions from speech signals remains intricate due to factors like background noise, individual-specific accentuation, weak representation of grammatical and semantic knowledge, and the unique temporal and spectral attributes of speech signals  [2] .\n\nRecent literature has spotlighted the potential of High-Resolution Networks (HRNet) for tasks demanding highresolution inputs, especially in image analysis  [3] . HRNet's design, with its multi-resolution strategy that simultaneously extracts features from varying scales, allows it to assimilate both granular and overarching information, offering an edge in accuracy and speed over other models  [4] . In this context, we introduce \"EmoHRNet\", a novel adaptation of HRNet tailored for SER. We transform audio samples into spectrograms and employ the HRNet architecture to glean high-level features from these visual representations. Moreover, we use data augmentation techniques to capitalize on the intrinsic link between emotions in speech and variations in pitch, tone, and temporal patterns. Our experimental findings underscore that the HRNet-based SER model surpasses other leading models in unweighted accuracy. Specifically, our model achieves unweighted accuracies of 92.45% on RAVDESS, 80.06% on IEMOCAP, and 92.77% on EMOVO.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Relation To Prior Work",
      "text": "A myriad of techniques have been proposed to tackle the challenges of SER. With the advent of deep learning, newer models like deep neural networks combined with extreme learning machines  [5] , bi-directional Long Short-Term Memory (LSTM)  [6] , Recurrent Neural Networks (RNN)  [7] , Capsule Neural Networks  [8]    [9] , and Quaternion based CNNs  [10]  have shown promise in capturing high-level representations from pitch-based features and other speech attributes.\n\nAttention-based SER models, such as those employing multi-head attention  [11]  and attention pooling  [12] , have been increasingly studied for their potential in extracting high-level emotional information. However, many of these models, despite their advanced capabilities, are often laden with a large number of parameters, making them less suitable for real-time applications and environments constrained by computational resources.\n\nFurthermore, while models like the dual-level LSTM  [13] , which harnesses temporal information from different time-frequency resolutions, and the integrated spatiotemporal feature learners  [14] , have shown potential, they often face challenges. One of the primary limitations is their inability to consistently capture long-range dependencies essential for context modeling in SER. Emotions in speech are intrinsically context-dependent, and a model's failure to grasp these dependencies can lead to inaccuracies. Additionally, many of these models do not dynamically adjust their receptive fields, which can limit their adaptability and generalization to unfamiliar data or diverse corpora. While recent advancements like the Capsule neural network-based CNN  [15] , the Gated multi-scale temporal convolutional network  [16] , temporal modeling  [17] , and multi-resolution feature extraction methods  [18]  have shown potential, there remains a gap in consistently achieving high accuracies across diverse datasets and real-world scenarios.\n\nIn light of these challenges and limitations, High-Resolution Networks (HRNet) emerges as a promising solution. HRNet's unique architecture, which maintains high-resolution representations through parallel multi-resolution convolutions, allows it to capture both fine-grained and coarse contextual information simultaneously. This multi-resolution strategy is particularly advantageous for SER, where capturing nuances at different scales is crucial. Unlike many models that downsample and then upsample, HRNet's consistent highresolution processing ensures that no critical emotional cues are lost. Moreover, design inherently addresses the limitation of models that struggle with long-range dependencies, as HRNet can assimilate both granular and overarching information seamlessly. Our adaptation of HRNet, \"EmoHRNet\", further tailors this architecture for SER, achieving superior performance metrics across benchmark datasets. To the best of our knowledge, this is the first time that HRNet is being applied to the domain of SER. Notably, EmoHRNet outperforms the aforementioned state-of-the-art methods in accuracy, including attention-based models, making it an optimal choice for real-world SER applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Hrnet Structure",
      "text": "The HRNet architecture is meticulously designed to maintain high-resolution representations from the initial to the final layers. This consistent high-resolution processing is crucial for tasks like SER, where the detailed nuances in Mel spectrogram inputs are essential for accurate emotion recognition.\n\nHigh-Resolution Input Module (HRIM): At the outset, the HRIM processes the Mel spectrogram. It employs a 3x3 convolution to extract preliminary features, setting the stage for the deeper layers of the network. This initial processing ensures that the network starts with a rich set of features derived from the input.\n\nHigh-Resolution Stages (HRS): As the architecture deepens, it doesn't compromise on resolution. Instead, it introduces parallel branches that operate at varying resolutions. These branches are not isolated; they exchange information through a mechanism that allows multi-resolution fusions. This design ensures that the network captures and integrates features across multiple scales, preserving both granular details and broader patterns.\n\nFuse Layer (FL): Serving as a unifying layer, the FL takes the multi-resolution feature maps from the various stages and fuses them. It employs 1x1 convolutions to consolidate these maps into a singular high-resolution feature map. This fusion process ensures that the final output is a comprehensive representation that has benefited from multiscale processing. To counteract potential challenges like the vanishing gradient problem inherent in deep networks, residual connections are strategically placed throughout the network.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Connecting Layers",
      "text": "The output multiresolution feature map F F L from the Fuse Layer (FL) is directed to the connecting layers for the classification task. These layers comprise a global average pooling layer  [19] , which averages each feature map across its spatial dimensions, producing a fixed-size feature vector. This vector is then passed to a fully connected layer, which employs a softmax activation function  [8]  to generate a probability distribution over the emotion classes. The output from this layer is represented as y, given by:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "The proposed HRNet-based SER model is trained using the cross-entropy loss function, which measures the difference between the predicted probabilities and the ground-truth labels for each sample:\n\nIn this equation, N stands for the number of training samples, C is the number of emotion classes, y i,c is the true label of the ith sample for the cth emotion class, and p i,c is the model's predicted probability for the same.\n\nFor optimization, we employ the Adam optimizer with parameters: learning rate set to 0.001, beta1 at 0.9, and beta2 at 0.999. To mitigate overfitting, weight decay regularization is applied with a coefficient of 0.0001. The model is trained over 100 epochs with batches of 64 samples each. Model performance is periodically assessed on a validation set, and the iteration with the highest validation accuracy is selected as the final model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Materials",
      "text": "This study employs three benchmarked datasets for speech emotion recognition: RAVDESS  [5] , IEMOCAP  [20] , and EMOVO  [21] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ravdess",
      "text": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) comprises 7356 audio files from 24 professional actors, covering eight emotions in both speech and song formats. Each emotion is represented in two intensities: normal and strong.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iemocap",
      "text": "The Interactive Emotional Dyadic Motion Capture Database (IEMOCAP) offers 12 hours of audiovisual interactions between actors, capturing emotions like happiness, anger, sadness, frustration, and neutral.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emovo",
      "text": "EMOVO is a pioneering emotional corpus tailored for the Italian language. It comprises recordings from six actors who articulated  14",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "We assessed the performance of our proposed EmoHR-Net model on three renowned speech emotion recognition datasets: IEMOCAP, RAVDESS, and EMOVO. The results were juxtaposed with those of previously published state-of-the-art models, as shown in Table  1 . Notably, we compared the following state-of-the-art models: Separable Convolution  [22] , QCNN  [10]  , Light-SERNet  [23] , ACNN+SE  [24] , Tuncer et al  [25] , TIM-Net  [17] , TWATWF + BCNN  [18] , CTL-MTNet  [15] , Hybrid MFCCT + CNN  [26] , Transformer with Feature Fusion  [27] , Two-Stage feature selection  [28] , and statistical feature extraction  [29] .\n\nFrom Table  1 , it is evident that EmoHRNet consistently outperforms other leading models across all datasets. Specifically, on the RAVDESS dataset, EmoHRNet achieved an accuracy of 92.45%, for the IEMOCAP dataset, EmoHRNet's accuracy of 80.06% stands out, and for the EMOVO dataset, it achieves an accuracy of 92.77%.\n\nThe superior performance of EmoHRNet can be attributed to several factors. Primarily, the HRNet architecture's ability to maintain high-resolution representations throughout its depth allows for the extraction and preservation of intricate emotional features from the speech spectrograms. An interesting discussion point is that while the TWATWF + BCNN model employs a multi-branch network structure to capture features across different time and frequency dimensions, EmoHRNet offers a more holistic structured method. By seamlessly integrating multi-resolution features in a hierarchical manner, EmoHRNet ensures robust and adaptive feature extraction, guaranteeing resilience and high performance across diverse scenarios. This may be why it performed similarly, but still better than, TWATWF + BCNN.\n\nThe results underscore the efficacy of EmoHRNet in speech emotion recognition tasks, setting a new benchmark for future research in this domain.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced EmoHRNet, a novel model for speech emotion recognition (SER) that leverages the strengths of the HRNet architecture. Our approach emphasizes the importance of maintaining high-resolution representations throughout the network's depth, ensuring the extraction and preservation of intricate emotional features from speech spectrograms. The results, as demonstrated on three renowned SER datasets-IEMOCAP, RAVDESS, and EMOVO-highlight the model's superior performance, setting a new benchmark in the domain.\n\nFuture research could delve into the selection of different features, particularly focusing on the extraction of prosodic, phonetic, and articulatory features, which have been shown to carry significant emotional information. Combining EmoHR-Net with other models and methods discussed in this paper could potentially lead to even more robust and accurate SER systems. Moreover, experimenting with other data augmentation techniques, beyond the ones employed in this study, might further improve the model's generalization.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Original Mel-Spectrogram, The Distorted",
      "page": 2
    },
    {
      "caption": "Figure 2: EmoHRNet Model Architecture: Input, High Resolution Stages, Fuse Layer, and Fully Connected Layers.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results on EmoHRNet and state-of-the-art models for IEMOCAP, RAVDESS, and EMOVO",
      "page": 4
    },
    {
      "caption": "Table 1: , it is evident that EmoHRNet consistently",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "A review on emotion recognition using speech",
      "authors": [
        "Saikat Basu",
        "Jaybrata Chakraborty",
        "Arnab Bag",
        "Md Aftabuddin"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Inventive Communication and Computational Technologies"
    },
    {
      "citation_id": "4",
      "title": "Deep high-resolution representation learning for visual recognition",
      "authors": [
        "J Wang",
        "K Sun",
        "T Cheng",
        "B Jiang",
        "C Deng",
        "Y Zhao",
        "D Liu",
        "Y Mu",
        "M Tan",
        "X Wang",
        "W Liu",
        "B Xiao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Deep highresolution representation learning for human pose estimation",
      "authors": [
        "Ke Sun",
        "Bin Xiao",
        "Dong Liu",
        "Jingdong Wang"
      ],
      "year": "2019",
      "venue": "CVPR 2019"
    },
    {
      "citation_id": "6",
      "title": "Comparing of deep neural networks and extreme learning machines based on growing and pruning approach",
      "authors": [
        "Kemal Akyol"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition based on bi-directional lstm architecture and deep belief networks",
      "authors": [
        "N Senthilkumar",
        "S Karpakam",
        "M Gayathri",
        "R Devi",
        "P Balakumaresan",
        "Dhilipkumar"
      ],
      "year": "2022",
      "venue": "International Conference on Innovation and Application in Science and Technology"
    },
    {
      "citation_id": "8",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "Jinkyu Lee",
        "Ivan Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "The application of capsule neural network based cnn for speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Kun-Hong Liu",
        "Wei-Ming Zhang",
        "Kai Jiang"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using quaternion convolutional neural networks",
      "authors": [
        "Aneesh Muppidi",
        "Martin Radfar"
      ],
      "venue": "ICASSP 2021, 2021"
    },
    {
      "citation_id": "12",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "Anish Nediyanchath",
        "Periyasamy Paramasivam",
        "Promod Yenigalla"
      ],
      "venue": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition"
    },
    {
      "citation_id": "13",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Mcloughlin",
        "Wu Guo",
        "Lirong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with dual-sequence LSTM architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "ICASSP 2020"
    },
    {
      "citation_id": "15",
      "title": "Spatiotemporal and frequential cascaded attention networks for speech emotion recognition",
      "authors": [
        "Shuzhen Li",
        "Xiaofen Xing",
        "Weiquan Fan",
        "Bolun Cai",
        "Perry Fordson",
        "Xiangmin Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "16",
      "title": "Ctl-mtnet: A novel capsnet and transfer learning-based mixed task net for the single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Jia-Xin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "Ctl-mtnet: A novel capsnet and transfer learning-based mixed task net for the single-corpus and cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "GM-TCNet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "Xin-Cheng Jia-Xin Ye",
        "Xuan-Ze Wen",
        "Yong Wang",
        "Yan Xu",
        "Chang-Li Luo",
        "Li-Yan Wu",
        "Kun-Hong Chen",
        "Liu"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "18",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "venue": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition based on low-level auto-extracted time-frequency features",
      "authors": [
        "Ke Liu",
        "Jingzhao Hu",
        "Jun Feng"
      ],
      "year": "2023",
      "venue": "ICASSP 2023"
    },
    {
      "citation_id": "20",
      "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "SpecAugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "22",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "23",
      "title": "A lightweight model based on separable convolution for speech emotion recognition",
      "authors": [
        "Ying Zhong",
        "Ying Hu",
        "Hao Huang",
        "Wushour Silamu"
      ],
      "venue": "A lightweight model based on separable convolution for speech emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Light-sernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami",
        "Benoit Champagne"
      ],
      "venue": "Light-sernet: A lightweight fully convolutional neural network for speech emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks",
      "authors": [
        "Ke Liu",
        "Chen Wang",
        "Jiayue Chen",
        "Jun Feng"
      ],
      "venue": "Time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks"
    },
    {
      "citation_id": "26",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "Turker Tuncer",
        "Sengul Dogan",
        "U Rajendra"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition through hybrid features and convolutional neural network",
      "authors": [
        "Ala Alluhaidan",
        "Oumaima Saidani",
        "Rashid Jahangir"
      ],
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "28",
      "title": "Transformer-based multilingual speech emotion recognition using data augmentation and feature fusion",
      "authors": [
        "Badriyya Al-Onazi",
        "Muhammad Nauman",
        "Rashid Jahangir",
        "Muhammad Malik",
        "Eman Alkhammash",
        "Ahmed Elshewey"
      ],
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "29",
      "title": "Fusion-based speech emotion classification using two-stage feature selection",
      "authors": [
        "Jie Xie",
        "Mingying Zhu",
        "Kai Hu"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "30",
      "title": "A statistical feature extraction for deep speech emotion recognition in a bilingual scenario",
      "authors": [
        "Sara Sekkate",
        "Mohammed Khalil",
        "Abdellah Adib"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    }
  ]
}