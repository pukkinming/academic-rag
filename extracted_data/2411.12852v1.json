{
  "paper_id": "2411.12852v1",
  "title": "Enhanced Cross-Dataset Electroencephalogram-Based Emotion Recognition Using Unsupervised Domain Adaptation",
  "published": "2024-11-19T20:48:12Z",
  "authors": [
    "Md Niaz Imtiaz",
    "Naimul Khan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition holds great promise in healthcare and in the development of affect-sensitive systems such as brain-computer interfaces (BCIs). However, the high cost of labeled data and significant differences in electroencephalogram (EEG) signals among individuals limit the crossdomain application of EEG-based emotion recognition models. Addressing cross-dataset scenarios poses greater challenges due to changes in subject demographics, recording devices, and stimuli presented. To tackle these challenges, we propose an improved method for classifying EEG-based emotions across domains with different distributions. We propose a Gradual Proximity-guided Target Data Selection (GPTDS) technique, which gradually selects reliable target domain samples for training based on their proximity to the source clusters and the model's confidence in predicting them. This approach avoids negative transfer caused by diverse and unreliable samples. Additionally, we introduce a cost-effective test-time augmentation (TTA) technique named Prediction Confidence-aware Test-Time Augmentation (PC-TTA). Traditional TTA methods often face substantial computational burden, limiting their practical utility. By applying TTA only when necessary, based on the model's predictive confidence, our approach improves the model's performance during inference while minimizing computational costs compared to traditional TTA approaches. Experiments on the DEAP and SEED datasets demonstrate that our method outperforms state-ofthe-art approaches, achieving accuracies of 67.44% when trained on DEAP and tested on SEED, and 59.68% vice versa, with improvements of 7.09% and 6.07% over the baseline. It excels in detecting both positive and negative emotions, highlighting its effectiveness for practical emotion recognition in healthcare applications. Moreover, our proposed PC-TTA technique reduces computational time by a factor of 15 compared to traditional full TTA approaches. Code available at https://github.com/RyersonMultimediaLab/EmotionRecognitionUDA",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are crucial to the human experience, affecting behavior, mental well-being, relationships, and interactions with technology  [1] . Emotion recognition, a topic of growing interest, has great potential in various areas, including human-computer interaction, mood disorder management, and interactive storytelling. The implementation of accurate emotion recognition systems could lead to more natural and empathetic interactions with artificial intelligence, thereby advancing human-computer interaction.\n\nUsing physiological signals to recognize emotions is superior to relying on facial expressions, gestures, and voices, as physiological signals are less susceptible to manipulation and external influences  [2] . While multimodal approaches combining signals such as electroencephalogram (EEG), electromyography (EMG), electrocardiogram (ECG), and respiratory signals have gained interest, unimodal approaches are often preferred for their lower computational costs and simpler data collection. EEG, in particular, has proven to be a dependable and promising indicator of an individual's mental state, as it directly captures brain activity that is challenging to manipulate  [3] .\n\nAnalyzing EEG signals is a time-and labor-intensive process, making the use of existing labeled data crucial. However, substantial variations among individuals and domain shifts caused by differences in demographics, sensor technologies, and recording environments challenge traditional machine learning-based emotion recognition models, which assume identical training and test distributions. Hence, domain adaptation is a crucial task in EEG-based emotion recognition. Domain adaptation is a machine learning approach that improves a model's performance on a target domain by leveraging information from a related source domain. It aligns feature spaces or refines the model's focus to enhance generalization to new, unlabeled data.\n\nWhile considerable research has focused on domain adaptation for emotion classification using EEG, most studies have concentrated on adapting between subjects and sessions within the same dataset. Challenges persist in cross-domain scenarios, where domain adaptation across datasets is more complex due to variations in subjects, EEG collection devices, and stimuli. Limited studies on cross-dataset emotion recognition have faced performance issues largely due to these substantial differences. Our work introduces a domain-adaptive model designed to address these challenges, including domain discrepancies and the lack of labeled data in the target domain.\n\nThis method represents an advancement over our previous model  [4] , which was designed for predicting arrhythmia across different datasets and sessions. It consists of four stages: pre-training, cluster computation, domain adaptation, and inference. While the pre-training and cluster computation stages follow our earlier work  [4] , we introduce significant modifications in the domain adaptation stage with a novel technique called Gradual Proximity-guided Target Data Selection (GPTDS). Additionally, we propose a new cost-effective test-time augmentation technique, Prediction Confidence-aware Test-Time Augmentation (PC-TTA), for the inference stage.\n\nIn the pre-training stage, the model learns from labeled source samples to acquire the necessary information for emotion recognition. The cluster computation stage calculates clearly separable clusters and their centroids and other properties for the source based on true labels and for the target based on confident predictions. The adaptation stage reduces the distributional gap between the source and target domains using objective functions. In unsupervised domain adaptation, pseudo labels are used, but large distributional differences or unreliable pseudo labels can cause negative transfer  [5, 6] . Our GPTDS approach gradually incorporates reliable target samples based on their proximity to source clusters and prediction confidence, avoiding unreliable samples that could lead to negative transfer. As training progresses and the discrepancy decreases, samples that were initially avoided due to their difficulty become eligible for selection in later stages.\n\nTest-time augmentation (TTA) has recently gained attention for improving a model's ability to handle unseen variations and enhance classification accuracy. It works by generating multiple augmented input versions and combining their predictions. However, the computational cost of TTA is a significant concern, as it involves applying multiple transformations and performing numerous prediction operations, which can be resource-intensive. Balancing these costs with the need for high classification accuracy presents a challenge. Our PC-TTA method addresses this by quantifying predictive confidence and applying TTA only when confidence is low, reducing computational costs. While TTA has mostly been used in image classification and segmentation  [7, 8, 9] , to the best of our knowledge, this is the first application of TTA in classifying EEG signals.\n\nOur method has been evaluated on two widely used public datasets for emotion recognition: DEAP  [10]  and SEED  [11] . This evaluation involves training on one dataset and testing on the other, in both directions. Only two previous cross-dataset studies  [12, 13]  have used the same training and testing settings as ours, and we compare our approach with theirs. We also contrast it with the baseline method  [4]  and six recent high-performing domain-adaptive methods  [14, 15, 16, 17, 18, 19] , all under identical experimental conditions. Our approach surpasses all other methods, achieving an overall accuracy of 59.68% (SEED → DEAP) and 67.44% (DEAP → SEED).\n\nThe key contributions of this article are as follows:\n\n(1) An unsupervised domain-adaptive model is proposed for emotion recognition, specifically designed to address substantial distribution differences between training and test datasets. The effectiveness of the proposed method is demonstrated through experiments, outperforming state-of-the-art approaches in cross-domain scenarios.\n\n(2) To mitigate negative transfer caused by diverse and unreliable samples, a novel technique called Gradual Proximityguided Target Data Selection (GPTDS) is introduced. This method gradually selects reliable target domain samples for training by considering their proximity to source clusters and the model's confidence in predicting them.\n\n(3) A new, cost-effective TTA technique called Prediction Confidence-aware Test-Time Augmentation (PC-TTA) is proposed, which applies augmentation only when necessary. Experimental results show that PC-TTA significantly enhances model performance during inference and reduces the high computational costs associated with traditional TTA.\n\nThe rest of this paper is organized as follows. Section 2 discusses the existing literature relevant to our research. Section 3 provides a detailed description of the components of the proposed method, including the training and testing processes, as well as data preprocessing. Section 4 describes the datasets used and presents experiments and results for the analysis and validation of our method. In conclusion, Section 5 summarizes our work.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Machine learning (ML) has become essential across various domains, from human action recognition to sentiment and emotion analysis, due to its ability to handle complex, high-dimensional data  [20, 21, 22] . In EEG-based emotion recognition, support vector machines (SVMs)  [23, 24, 25]  are widely used. Other popular techniques include K-Nearest Neighbor (KNN)  [26, 27] , Decision Tree (DT)  [28] , and Linear Discriminant Analysis (LDA)  [29] . Shallow approaches often face challenges in effectively modeling the complex temporal relationships present in EEG signals, and they may not generalize well to new and unseen data. In the realm of deep learning, Autoencoders (AEs)  [30]  and Graph Neural Networks (GNNs)  [31]  have been researched. Additionally, Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks are commonly used  [32, 33, 34, 35] . These models have generally demonstrated strong performance, particularly in subject-dependent analyses.\n\nUnsupervised domain adaptation (UDA) is an effective approach for addressing distributional disparities between source and target domain data, especially when acquiring labeled data in the target domain is costly and timeconsuming. It minimizes the gap by learning a mapping from source to target, as explored in research  [16, 17, 18] . Table  1  summarizes key technical insights from the literature on domain adaptation.\n\nThere has been extensive research on domain adaptation for classifying EEG emotions in recent years, primarily focusing on cross-subject and cross-session adaptation  [14, 19, 36] . Many studies emphasize feature selection to identify effective subsets from high-dimensional EEG data  [37, 38] , aiming to find common features across individuals. Adversarial learning is also popular, training models to acquire domain-invariant features using a domain discriminator to differentiate between source and target features  [15, 39] .\n\nMuch of the research on domain adaptation for EEG emotion recognition has focused on adapting between subjects and sessions within the same dataset. Only a few studies have explored cross-dataset scenarios  [12, 13, 40, 41, 42] . Among these, Lan et al.  [40]  compared existing methods on the DEAP and SEED datasets in cross-dataset settings, but they used methods originally designed for other domains and validated their model on only 3 trials from 14 subjects, rather than the full 40 trials and 32 subjects. In contrast, He et al.  [41]  and Rayatdoost et al.  [42]  used their own selfrecorded datasets. Therefore, comparisons with these methods are not feasible as they do not align with our train-test dataset settings. Instead, we compare our proposed method with the two remaining cross-dataset studies by Ni et al.  [13]  and Gu et al.  [12] , as they conducted cross-dataset experiments using the same training and testing settings as ours.\n\nDespite significant advancements in deep learning for EEG-based emotion recognition, challenges persist in crossdomain scenarios. While much recent research has focused on domain adaptation strategies for recognizing emotions across different subjects and sessions, limited attention has been given to the cross-dataset scenario. This scenario presents even greater challenges due to the substantial disparities between source and target domains, which arise not only from differences in subjects but also from variations in EEG recording settings and the stimuli presented. The limited studies on cross-dataset emotion recognition have struggled with performance issues, largely due to these substantial differences.\n\nThis study addresses the challenges of domain discrepancies in cross-dataset scenarios and the lack of labeled data in the target domain. We also tackle issues related to negative transfer from unreliable samples and the high computational cost of Test Time Augmentation (TTA). Although TTA has gained popularity for improving models' handling of unseen variations in tasks such as image segmentation  [7, 8] , image classification  [43, 9] , and anomaly detection  [44] , its application in the EEG domain remains unexplored. The main drawback of TTA is its high computational cost, which limits its practical use. Our PC-TTA approach mitigates this computational burden while maintaining high classification accuracy and is the first application of TTA in EEG classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Our proposed method comprises four stages: pre-training, cluster computation, domain adaptation, and inference. While the pre-training and cluster computation stages remain the same as in our previous model  [4] , we introduce modifications to the domain adaptation and inference stages. Figure  1  depicts the block diagram of the proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Rayatdoost Et Al. [42], 2021",
      "text": "Combines subject-invariant learning with an adversarial network and uses a gradient reversal layer to balance recognition and subject confusion.\n\nReduces subject-specific biases and integrates domain adaptation.\n\nMay not fully resolve generalization issues in some cases.\n\nImproves accuracy by reducing subject-specific biases; performs best with specific hyperparameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Huang Et Al. [17], 2020",
      "text": "Introduces Representation Self-Challenging (RSC) to iteratively discard dominant features and use less dominant ones.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Improves Cross-Domain Generalization; Compatible With Various Architectures",
      "text": "Longer training time; requires careful hyperparameter tuning.\n\nEffective across domains; minimal increase in model size while enhancing performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Li Et Al. [15], 2019",
      "text": "Uses neural networks with adversarial training to adapt marginal distributions and reinforce conditional distributions.\n\nEffective in cross-subject and cross-session scenarios; parameter-efficient.\n\nPerformance varies with feature types; requires careful tuning. Surpasses conventional methods; better cross-session transfer.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Sagawa Et Al. [16], 2019",
      "text": "Uses Distributionally Robust Optimization (DRO) with strong regularization and a new stochastic optimizer to enhance worst-group accuracy.\n\nPrevents spurious correlations; works with imperfect group specifications.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Requires Strong Regularization And A New Optimizer.",
      "text": "Improves worst-group accuracy by 10-40 percentage points while maintaining high average accuracy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Framework",
      "text": "We use source domain to indicate the dataset utilized for training and target domain to indicate the dataset utilized for testing. In the source domain, we have N s labeled samples, X s = {x i s } Ns i=1 , along with their corresponding class labels, Y s = {y i s } Ns i=1 . Conversely, in the target domain, we have N t unlabeled samples, X t = {x i t } Nt i=1 . Here, x i s and x i t represent the features of the i th EEG segment from the source and target domains, respectively (we use Power Spectral Density (PSD) as features). The marginal probability distributions of the source and target domains are P s (X s ) and P t (X t ), respectively, where P s (X s ) = P t (X t ). Our goal is to learn a function f that minimizes the gap in marginal distributions between P s (X s ) and P t (X t ), thereby enabling accurate prediction of labels for target samples.\n\nThe network architecture includes a feature extractor (F ) followed by two parallel classifiers (C 1 and C 2 )  [4] . Previous studies have shown that a simple feed-forward network with only fully connected layers in the feature extractor performs effectively on EEG features for emotion recognition  [19, 15] . Therefore, to keep the network architecture simple, our new feature extractor is designed with two fully connected layers, replacing the complex residual blocks used in the previous model. The batch normalization layer is incorporated to standardize features after each fully connected layer. In experiments where we used the residual block-based feature extractor from our previous model, Figure  1 : Block diagram illustrating the overall approach, with the two bold rectangles highlighting the new additions from this research compared to our previous model.\n\nwe did not observe any performance improvements over the simpler network. Moreover, the simpler network helps mitigate the overfitting issues often encountered with more complex architectures, particularly given the relatively small datasets used in this study. We maintain two parallel classifiers following the feature extractor, as in our previous model. Having two classifiers addresses scenarios in which a single classifier may make incorrect predictions, even when the feature extractor generates distinct features. Additionally, we leverage the difference between the two classifiers to detect confident predictions in the target domain and determine the necessity of applying TTA in our PC-TTA approach. Each classifier consists of three fully connected layers. The predicted emotion category is derived by averaging the outputs from the two classifiers.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Pre-Training",
      "text": "In the pre-training stage, the model is trained with labeled source samples (X s , Y s ) to obtain essential information for recognizing emotions. During pre-training, we utilize a group distributionally robust optimization (DRO) technique  [16] . The objective of this approach is to train models that are not reliant on misleading correlations, which can lead to poor performance on certain data groups. Instead, our goal is to train models to minimize the highest potential loss across all groups in the training data.\n\nThe loss function during the pre-training stage is the weighted sum of the classifier discrepancy loss (L dis ), along with the classification loss (L cls ) (1). The classification loss is computed by applying group DRO to the weighted crossentropy loss. The classifier discrepancy loss is calculated by evaluating the Euclidean distance between the outputs produced by the two classifiers.\n\nwhere α denotes a hyperparameter.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cluster Computation",
      "text": "After the pre-training stage comes the cluster computation stage. Here, we start by calculating the source domain cluster centroids (CC spre ) for each emotion category in the source domain through the averaging of the feature extractor's output. Next, the model undergoes training with a pair of weighted loss functions: the cluster-separating loss (L sep ) (2) and the cluster-compacting loss (L comp ) (3), in addition to the classification loss specified in equation  (4) .\n\nwhere n k s represents the total sample count in the k th emotion category, K represents the number of emotion categories, D denotes the Euclidean distance, T m represents a large pre-defined threshold, and γ 1 and γ 2 denote hyperparameters.\n\nOptimizing the cluster-compacting loss reduces the intra-cluster distance, while optimizing the cluster-separating loss increases the inter-cluster distance. After training, the clusters in the source domain become well-organized. Subsequently, we recalculate the centroids (CC s ) of these clusters. Since the target domain is unlabeled, we only consider confident predictions when computing the target domain clusters. We compute the mean intra-cluster distance M ctr (the mean distance between the samples and their corresponding cluster centroids)  (5) , their standard deviation σ (6), and the mean classifier discrepancy M dis (the mean disparity between the outputs of the two classifiers)  (7)  in the source domain. We select confident predictions (CP t ) that meet all of the following criteria: the softmax value is greater than 0.99, the distance from the corresponding source cluster centroid is less than M ctr , and the classifier discrepancy is less than M dis . Afterward, we calculate the centroids (CC t ) of the clusters for the target domain using these selections.\n\nwhere N s denotes the total sample count in the source domain and M k ctr represents the mean intra-cluster distance for the k th emotion category.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Domain Adaptation",
      "text": "In this proposed method for domain adaptation, we introduce improvements to our previous model. Specifically, we propose the Gradual Proximity-guided Target Data Selection (GPTDS) technique, which enhances domain adaptation compared to our previous approach. GPTDS aims to improve the selection of reliable target domain samples for training through an iterative process. In our earlier method, we selected target samples from each training batch based on their proximity to the source cluster and the model's confidence. However, samples that were distant from the source cluster and therefore excluded from training in the initial stages might still be valuable for training in later iterations as the model is trained to minimize the distributional discrepancy between the source and target domains in the domain adaptation stage. Our previous method did not account for this, leading to a low number of samples included in the training process, overlooking potentially eligible candidates for later stages. GPTDS addresses this issue.\n\nWe first calculate the feature maps F (X t ) by inputting the target domain samples X t into the pre-trained network. Next, we determine the similarity of the target domain samples to the source samples by calculating the distance between the feature map of a target sample and the centroid of the corresponding source cluster. We select candidates (CX t ) for training from the target domain that are more similar to the source samples, based on the following condition:\n\nwhere CC k s denotes the cluster centroid, M k ctr represents the mean intra-cluster distance, and σ k represents the standard deviation for the k th emotion category. Next, from the candidates (CX t ), we further refine our selection and choose samples (SX t ) with confident predictions, based on softmax values and classifier discrepancies as outlined in the confident prediction conditions in the cluster computation stage. The goal is to select training samples from the target domain that have low distributional differences from the source and are reliable. This is important because using samples that are highly dissimilar from the source or unreliable can lead to negative transfer issues. Subsequently, we perform epoch training using both source domain samples and the selected target domain samples. This process is repeated iteratively until there are no selected samples from the target domain for training or a certain number of interactions have occurred. In each iteration, we check for target samples for training among the excluded samples from the previous iteration. As the iterations progress and the discrepancy between the source and target domains decreases, samples that were initially excluded due to their distance from the source cluster may be selected in later iterations (Figure  2 ). While GPTDS prioritizes target samples based on their proximity to source clusters and model confidence, it also captures subtle differences between the datasets by gradually broadening the selection as training progresses. This strategy allows the model to incorporate a diverse range of target domain samples, including those that may have initially posed challenges, ultimately leading to a more robust and generalized adaptation.\n\nDuring epoch training, we minimize the weighted sum of four loss functions: the cluster-separating loss (L sep ), the cluster-compacting loss (L comp ), the inter-domain cluster discrepancy loss (L cd ) (9), and the running combined loss (L cmd )  (10) , along with the classification loss stated in equation  (12) , as in our previous model  [4] .\n\nwhere N b represents the batch size and\n\nwhere β 1 , β 2 , β 3 , and β 4 denote hyperparameters.\n\nThe inter-domain cluster discrepancy loss aims to decrease the distance between clusters in the source and target domains, while the running combined loss aims to minimize the distance between the global average cluster centroids (the mean of the cluster centroids from the source and target domains, calculated in the cluster computation stage) and the current average cluster centroids for the current training batch.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Inference",
      "text": "During the inference stage, we introduce a Prediction Confidence-aware Test-Time Augmentation (PC-TTA) technique to enhance the model's performance on test data from the target domain. TTA enhances the model's ability to handle unseen variations by applying data augmentation techniques to the test data and combining the predictions. The primary downside of TTA is that applying multiple transformations and performing predictions can be computationally intensive. To address this issue, instead of applying TTA to all test samples, we quantify the model's prediction confidence and determine the necessity of applying TTA.\n\nFigure  3  illustrates our PC-TTA process. We obtain the softmax response for each input test sample x t ∈ X t using our trained model f . For a test sample x t , the softmax response using f is denoted as p(0) = f (x t ). Next, we assess the uncertainty of the prediction by using entropy as the measure, as follows:\n\nwhere K represents the total number of emotion categories, k refers to a specific category within this set, and p(y = k|x t ) denotes the probability of category k for the test sample x t .\n\nIf u(p (0) ; f ) is high, it indicates high uncertainty and reflects the model's poor confidence. When the model's prediction confidence for a test sample is low to some extent, we apply TTA to that sample; otherwise, we accept the model's prediction for that sample as the final output. In our model, when both u(p (0) ; f ) and the difference between the two classifiers are high for a test sample, we can infer that the model's confidence in predicting that sample is low. If u(p (0) ; f ) is greater than or equal to a certain threshold τ , and the classifier discrepancy D(C 1 , C 2 ) is greater than the mean classifier discrepancy M dis , then we perform TTA on that sample. Otherwise, we consider the model's prediction as confident, and the predicted label ŷ(0) based on the softmax response p(0) is considered the final prediction ŷ.\n\nFor the samples that require TTA after this filtering stage, we first extract corresponding EEG signal segment s t from the input sample x t . Subsequently, we apply augmentation techniques to s t , specifically utilizing Gaussian noise addition and resampling in this experiment. Let the number of transformations be denoted as N , resulting in transformed EEG signal segments S = {s (1) , . . . , s(N) }. From these signal segments, we extract PSD features X = {x (1) , . . . , x(N) }. Next, we feed these augmented features into the pre-trained model f to obtain predicted labels {ŷ (1) , . . . , ŷ(N) }, based on the softmax responses {p (1) , . . . , p(N) } from f . To determine the final prediction, we consider all N+1 predicted labels Ŷ = {ŷ (0) , ŷ(1) , . . . , ŷ(N) }. Here, ŷ(0) corresponds to the softmax response p(0) for the original input sample x t . We conduct a vote among these predicted labels, selecting the most frequently occurring class as the final prediction ŷ.\n\nAlgorithm 1 outlines the complete procedure of our proposed approach.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Preprocessing And Construction Of Model Inputs",
      "text": "Power Spectral Density (PSD)  [45, 42, 15]  and Differential Entropy (DE)  [19, 38, 37]  features are widely employed in EEG-based emotion recognition and have demonstrated superior performance compared to other EEG features in previous studies  [13, 3] . In this study, we explore both PSD and DE; however, our experimental results indicate that PSD outperforms DE. As a result, we incorporate PSD into our proposed method. The formula for computing the PSD is as follows:\n\nwhere, P SD(f ) represents the PSD at frequency f , x(n) represents the signal segment, and N corresponds to the number of samples in x(n).\n\nwhere, P SD band represents the PSD in the frequency band [f low , f high ].\n\nFigure  4  illustrates the process of constructing the temporal input for our model. To ensure a uniform input size for the model, we extract EEG signals from 32 common EEG channels present in both the DEAP and SEED datasets. Before feature extraction, we divide each EEG trial into multiple segments. For segmentation, we use a window size of 2 seconds and a step size of 1 second, similar to  [46] , resulting in a 1-second overlap between segments. Next, we extract the PSD features from each segment in each channel at frequency bands: delta δ (1-3 Hz), theta θ (4-7 Hz), alpha α (8-13 Hz), beta β (14-30 Hz), and gamma γ (31-50 Hz). We then create a 1-dimensional feature vector X ∈ R n * i by concatenating the PSD features. Here, n=32 represents the number of channels, and i is set to 5, corresponding to the frequency bands δ, θ, α, β, and γ. Therefore, the size of the 1-dimensional input feature vector is 5 × 32 = 160. Finally, we normalize the features to fall within the range of [-1, 1]. To address potential discrepancies between feature spaces, we ensure uniform preprocessing and normalization across domains, aligning data transformations and reducing the risk of errors.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Datasets And Experimental Setup",
      "text": "Our proposed model has been evaluated through experiments using the DEAP and SEED datasets, which are widely utilized for emotion recognition tasks and are publicly available. Our experiments include cross-dataset testing, where the model is trained on one dataset and evaluated on another.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Deap Dataset [10]",
      "text": "The stimuli consisted of 40 one-minute music videos. The experiment included 32 healthy participants. Each subject underwent 40 trials, each lasting 63 seconds. This included a 3-second pre-trial period and 60 seconds of watching one-minute videos. After viewing each video, participants rated their arousal, valence, dominance, and liking using a continuous scale ranging from 1 to 9. EEG signals were recorded using 32 electrodes at a sampling frequency of 512 Hz. The signals were downsampled to 128 Hz during preprocessing and further filtered using a band-pass filter between 4 Hz and 45 Hz to reduce noise and artifacts.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Seed Dataset [11]",
      "text": "A total of 15 Chinese movie clips, each carefully selected and approximately 4 minutes long, were used as stimuli.\n\nFifteen Chinese subjects participated in the study, comprising 7 females and 8 males. Each participant attended 3 sessions on different days, during which they watched the same set of 15 movie clips. The movie clips were intended to elicit three emotions: positive, neutral, and negative. EEG signals were captured using 62-channel electrode caps at a sampling frequency of 1000 Hz. The recorded data underwent preprocessing, including downsampling to 200 Hz and filtering with a bandpass of 0-75 Hz.\n\nIn our experiments, we utilize data samples from all subjects and trials in both the SEED and DEAP datasets. Since the first 3 seconds of each trial in the DEAP dataset consist of pre-trial period data, we exclude this initial 3-second segment from each trial. We filter the signals from the SEED dataset using a bandpass filter with a passband of 0.3 Hz to 50 Hz to remove noise and artifacts, following  [15, 47] . Since the DEAP dataset signals are already filtered between 4 Hz and 45 Hz, we do not perform additional filtering. This study focuses on two emotion categories: positive and negative. In the DEAP dataset, valence values above 4.5 are considered positive, and those below 4.5 are considered negative based on manual classification guidelines  [13, 48] . For the SEED dataset, we exclude neutrallabeled samples and retain only those labeled as positive or negative for analysis. After preprocessing, the DEAP dataset contains 74,240 samples, while the SEED dataset contains 99,630 samples, each with a vector size of 160. All samples from the source domain dataset are used for training, and all samples from the target domain dataset are used for testing.\n\nAll experiments are conducted on a Linux platform using Python (version 3.10.12) and the PyTorch library (version 2.3.1+cu121), running on an NVIDIA Tesla T4 GPU with 12GB of memory. The learning rate is set to 0.001, and weight decay is set to 0.0005, following our previous work  [4] . The batch size is set to 64, the Rectified Linear Unit (ReLU) is used as the activation function, and model optimization is performed using the Adam optimizer. The hyperparameters α, γ 1 , γ 2 , β 1 , β 2 , β 3 , and β 4 are set to 0.5, 0.1, 0.1, 0.1, 0.1, 0.5, and 0.1, respectively, consistent with our previous method  [4] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Results And Discussion",
      "text": "Our proposed method is evaluated in a cross-dataset setting, where the source domain (SD) and target domain (TD) are from different datasets. We test our model by training on one dataset (SD) and testing on the other (TD), in both directions. Specifically, we train on the DEAP dataset and test on the SEED dataset, and vice versa. For comparison with existing approaches, we shortlisted the existing works according to two categories: 1) existing domain adaptive emotion recognition methods that provide experimental results on the same datasets with identical training and testing settings, and 2) existing state-of-the-art general domain adaptive approaches that are open-source, so that we can easily re-implement for comparison.\n\nFor category 1, to the best of our knowledge, only two prior studies on EEG-based emotion recognition  [12, 13]  have used the same training and testing settings as ours. Table  2  presents a comparison between our proposed method and the prior approaches, demonstrating our method's superior performance by a significant margin. Specifically, our method achieves 59.68% accuracy when tested on DEAP (trained on SEED), outperforming the previous best of 53.67% by Gu et al.  [12] . Similarly, our method achieves 67.44% accuracy when tested on SEED (trained on DEAP), surpassing the previous best of 64.97% by Ni et al.  [13] .\n\nFor category 2 (open source projects), we compare our proposed approach against six state-of-the-art domain-adaptive approaches  [14, 15, 16, 17, 18, 19] . To ensure a fair comparison, we re-implemented these approaches using their open-source repositories and adopted identical network architecture and experimental setup as our proposed approach. Table  2  demonstrates the superior performance of our proposed method compared to the domain-adaptive approaches in terms of overall accuracy. Our method outperforms the second-best accuracy achieved by Jiménez-Guarneros and Fuentes-Pineda  [19]  by 6.20% for SEED → DEAP (trained on SEED, tested on DEAP). Similarly, our method surpasses the second-best accuracy achieved by Chen et al.  [14]  by 2.72% for DEAP → SEED. Furthermore, we Table  2 : Comparison of overall accuracy values between our proposed method and other state-of-the-art domainadaptive methods. The results for the top two methods  [13, 12]  are directly drawn from the papers due to identical training-test settings, while the remaining methods are implemented and evaluated using the same network architecture and experimental setup as ours. Symbols indicate differences in accuracies (paired-sample t-test: ∼ nonsignificant, *p < 0.05, **p < 0.01).\n\nAccuracy (%) SEED  [11] →DEAP  [10]  DEAP  [10] →SEED  [11]  Ni et al.  [13]  53.54 64.97 Gu et al.  [12]  53.67 64.67 Li et al.  [15]  53.44** 63.88* Chen et al.  [14]  51.93** 64.72* Sagawa et al.  [16]  48.91** 58.39** Huang et al.  [17]  52.56** 59.20** Jiménez-Guarneros and Fuentes-Pineda  [19]  53. conduct a paired-sample t-test, using p-values, to determine the significance of the differences in emotion recognition performance between our proposed method and other approaches. The results demonstrate that the difference in accuracy between our method and all other approaches is highly significant (**) for SEED → DEAP. For DEAP → SEED, the difference is significant (*) compared to Chen et al.  [14] , Li et al.  [15] , and Jiménez-Guarneros and Fuentes-Pineda  [19] , while the difference is highly significant compared to other approaches.\n\nFigure  5  displays the accuracy distributions of our proposed method and six domain-adaptive methods using boxplots.\n\nOur method stands out by achieving the highest median accuracy among all methods. When trained on SEED and tested on DEAP, Sagawa et al.'s method exhibits relatively less variation, but its overall accuracy is low. In contrast, our method achieves the highest median accuracy of 58.03%, with the majority of prediction accuracies exceeding this value. In the DEAP → SEED scenario, our method surpasses all others, achieving the highest median accuracy (68.87%) with low variations in accuracies compared to alternative methods. Table  3  compares the sensitivity, positive predictive value (PPV), and F1 score of our proposed method with six domain-adaptive approaches and our previous model. While Li et al.'s method yields the highest F1 score of 73.06% (0.98% higher than ours) in recognizing positive emotions, our proposed approach excels in recognizing negative emotions when tested on SEED. In contrast, when tested on DEAP, all methods face challenges in recognizing negative emotions. Nonetheless, our proposed method outperforms other methods, including our previous model, in recognizing both negative and positive emotions. The lower performance in recognizing negative emotions on DEAP may be attributed to the smaller number of samples with negative emotion (36.87%) compared to those with positive emotion (63.13%). In contrast, the SEED dataset features a balanced distribution of samples (negative-50.36%, positive-49.64%).   [15]  40,453 Chen et al.  [14]  122,963 Sagawa et al.  [16]  23,555,098* Huang et al.  [17]  34,583,605* Jiménez-Guarneros and Fuentes-Pineda  [19]  1,088,387 Zhi et al.  [18]  23,792,099* Imtiaz and Khan  [4]  357,392 Proposed method 34,288\n\nTable  4  compares model sizes, measured by the number of parameters, between our proposed method and other approaches. We report average model sizes for the other methods, as they employed different architectures for various datasets. The model sizes for Ni et al.  [13]  and Gu et al.  [12]  are not included due to the unavailability of this information. Our model is the lightest among those compared, significantly lighter than the complex, heavyweight models used by others, while still achieving superior performance. This reduction in model size leads to lower computational complexity in terms of both time and memory. Specifically, our efficient architecture reduces the number of parameters to 34,288, enabling faster training and inference times with lower memory usage.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "We perform an ablation study to determine how each component of our proposed method affects the results. We systematically remove the main components of our proposed method, one at a time, which constitute the primary contributions of this research. We then assess the model and observe the effect of removing each component. Table 5, Table  6 , and Figure  6  present the results of the ablation analysis. We create four models by excluding one component at a time while leaving all other components unchanged. Model A is created by removing all domain adaptation components from the proposed model, retaining only the pre-training stage. Model B represents our previously proposed method  [4] , referred to as the baseline model. Model C excludes the Gradual Proximity-guided Target Data Selection (GPTDS) component from the proposed method. Model D is constructed by excluding the Prediction Confidence-aware Test-Time Augmentation (PC-TTA) component while keeping all other components unchanged.\n\nIn addition to evaluating the models using Power Spectral Density (PSD), we also assess them using Differential Entropy (DE) features. Table  5  displays the accuracy of all models for both PSD and DE features. Each component influences performance to some extent. Overall, we achieve better performance when using PSD features, although DE works slightly better for Model C in SEED → DEAP and for Model B in DEAP → SEED. In most cases, PSD performs well. Therefore, we choose PSD for our proposed approach and focus on comparing others using PSD from here on. Model A performs the worst, even lower than the random probability for SEED → DEAP, as it lacks any knowledge about the test (target domain) data. The baseline model achieves an accuracy of 53.61% for SEED → DEAP and 60.35% for DEAP → SEED. Both GPTDS and PC-TTA significantly impact the performance of emotion classification. PC-TTA improves the accuracy compared to the baseline model by 3.09% for SEED → DEAP and by 4.02% for DEAP → SEED. GPTDS improves the accuracy compared to the baseline model by 3.36% for SEED → DEAP and by 3.54% for DEAP → SEED.\n\nGPTDS and PC-TTA components enhance the model's performance in classifying both positive and negative emotions, as demonstrated in Table  6  and Figure  6 . The confusion matrices (Figure  6 ) show that PC-TTA significantly improves the identification of negative emotions for both datasets, while GPTDS significantly enhances the identification of positive emotions for both datasets compared to the baseline model. Table  6  indicates that for classifying negative   emotions, PC-TTA achieves the highest F1 score, surpassing even the proposed method, with scores of 35.76% and 61.05% for SEED → DEAP and DEAP → SEED, respectively. The combined use of GPTDS and PC-TTA substantially boosts performance over the baseline model for identifying both positive and negative emotions.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Analysis Of Pc-Tta Threshold (Τ )",
      "text": "To select the optimal value of threshold τ , which is used to determine whether to perform TTA, we evaluate the model by varying its value from 0.1 to 0.99. Figure  7  illustrates the overall accuracy and average execution time per subject during the inference stage for different values of τ . After careful consideration of both emotion recognition accuracy and computational cost, we have selected τ = 0.9 for our proposed method. The model achieves its highest accuracy when tested on the DEAP dataset with τ = 0.9. However, when tested on SEED, although the model achieves slightly better accuracy with lower τ , the associated high computational time becomes a significant concern.\n\n(a) (b) Table  7  compares the performance of our PC-TTA approach with two other scenarios: full TTA, where TTA is applied to all test samples, and no TTA. In both cases, applying TTA (PC-TTA and full TTA) significantly improves the emotion recognition accuracy. However, it is crucial to consider the significant computational cost associated with TTA. This cost arises from a series of operations executed each time TTA is applied to a test sample, including extracting the corresponding signal segment, performing a number of augmentations, extracting PSD features from the transformed signal segments, and obtaining the model prediction by feeding them into the model. The execution time of our proposed PC-TTA approach is 2.86 and 2.43 times higher than when no TTA is applied, for the DEAP and SEED test datasets, respectively. This increase is significantly greater when applying TTA to all test samples, with execution times 43.77 and 37.77 times higher when tested on DEAP and SEED, respectively. While full TTA achieves slightly better accuracy (0.34% higher than our proposed PC-TTA) when tested on SEED, our PC-TTA performs better than full TTA when tested on DEAP. Therefore, our PC-TTA approach effectively balances computational cost while maintaining high classification accuracy.  To assess the impact of each frequency band on emotion recognition, we conduct experiments for each individual frequency band. We adjusted the input layer of the feature extractor (F ) to accommodate the change in input features from 5 bands to 1 band. Notably, the gamma and beta bands contribute more significantly to emotion recognition compared to the other frequency bands, as illustrated in Figure  8 . However, the model's performance using a single frequency band is notably lower than when using all five frequency bands.\n\nThe experimental results demonstrate the effectiveness of our proposed method across different datasets. Despite facing challenges due to significant distributional disparities between the training and test data, our method performs significantly better compared to other cutting-edge methods.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "This study proposes an efficient, unsupervised deep domain adaptation approach for recognizing emotions from EEG signals, addressing challenges such as limited labeled training data and differences in data distributions among datasets. Our proposal introduces the Gradual Proximity-guided Target Data Selection (GPTDS) technique, which gradually selects reliable target domain samples for training by considering their proximity to the source clusters and the model's confidence in predicting them. This approach prevents negative transfer resulting from the inclusion of diverse and unreliable samples from the target domain during training. We also propose a cost-effective test-time augmentation technique called Prediction Confidence-aware Test-Time Augmentation (PC-TTA). This technique applies TTA when necessary to improve the model's performance on test data while minimizing the computational burden posed by traditional TTA approaches. The experimental results across datasets demonstrate that the proposed approach yields more reasonable results in emotion recognition without using target domain labels during training, compared to existing state-of-the-art methods. The proposed approach has substantial industrial potential, offering enhanced human-computer interaction through more empathetic AI responses and providing robust solutions for monitoring and managing mood disorders in healthcare. Its adaptability across different platforms and reduced computational costs make it highly suitable for diverse, resource-constrained environments.\n\nAlthough our method demonstrates significant improvements, it may struggle with extreme dataset variations and challenging cross-dataset adaptation scenarios. Future work will focus on generalizing our model to support multiple target domains by incorporating advanced domain alignment techniques to handle diverse and heterogeneous datasets. We will integrate functional connectivity between EEG electrodes by developing methods to capture and leverage dynamic relationships between brain regions. Additionally, we will address overfitting through advanced data augmentation  [49]  and regularization  [50, 51]  techniques, including dropout and dynamic learning rate adjustments. While our PC-TTA method enhances computational efficiency and reduces the need for extensive test-time augmentations, we still need to evaluate its real-time performance and latency to ensure its suitability for latency-sensitive applications.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts the block diagram of the proposed",
      "page": 3
    },
    {
      "caption": "Figure 1: Block diagram illustrating the overall approach, with the two bold rectangles highlighting the new additions",
      "page": 5
    },
    {
      "caption": "Figure 2: Gradual selection of target domain samples for training. As the iterations progress and the discrepancy",
      "page": 7
    },
    {
      "caption": "Figure 2: ). While GPTDS prioritizes target samples based",
      "page": 7
    },
    {
      "caption": "Figure 3: Proposed Prediction Conﬁdence-aware Test-Time Augmentation (PC-TTA) technique.",
      "page": 8
    },
    {
      "caption": "Figure 3: illustrates our PC-TTA process. We obtain the softmax response for each input test sample xt ∈Xt using",
      "page": 8
    },
    {
      "caption": "Figure 4: Genaration of 1D input features.",
      "page": 10
    },
    {
      "caption": "Figure 4: illustrates the process of constructing the temporal input for our model. To ensure a uniform input size for",
      "page": 10
    },
    {
      "caption": "Figure 5: displays the accuracy distributions of our proposed method and six domain-adaptive methods using boxplots.",
      "page": 12
    },
    {
      "caption": "Figure 5: Boxplots showing the distribution of emotion recognition accuracies for our proposed method and other",
      "page": 12
    },
    {
      "caption": "Figure 6: present the results of the ablation analysis. We create four models by excluding one",
      "page": 13
    },
    {
      "caption": "Figure 6: The confusion matrices (Figure 6) show that PC-TTA signiﬁcantly improves",
      "page": 13
    },
    {
      "caption": "Figure 6: Confusion matrices from the ablation study for (a) SEED[11] →DEAP[10] and (b) DEAP[10] →SEED[11].",
      "page": 14
    },
    {
      "caption": "Figure 7: illustrates the overall accuracy and average execution time per subject",
      "page": 15
    },
    {
      "caption": "Figure 7: Model performance for different values of τ. (a) SEED[11] →DEAP[10] (b) DEAP[10] →SEED[11].",
      "page": 15
    },
    {
      "caption": "Figure 8: Contribution of frequency bands to emotion recognition.",
      "page": 16
    },
    {
      "caption": "Figure 8: However, the model’s performance using a single",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes key technical insights from the literature on domain adaptation.",
      "page": 3
    },
    {
      "caption": "Table 1: Technical insights from reviewed literature.",
      "page": 4
    },
    {
      "caption": "Table 2: presents a comparison between our proposed method",
      "page": 11
    },
    {
      "caption": "Table 2: demonstrates the superior performance of our proposed method compared to the domain-adaptive approaches",
      "page": 11
    },
    {
      "caption": "Table 2: Comparison of overall accuracy values between our proposed method and other state-of-the-art domain-",
      "page": 12
    },
    {
      "caption": "Table 3: compares the sensitivity, positive predictive value (PPV), and F1 score of our proposed method with six",
      "page": 12
    },
    {
      "caption": "Table 3: Performance comparison of our proposed method and other approaches for identifying positive and negative",
      "page": 13
    },
    {
      "caption": "Table 4: Comparison of model sizes between our proposed method and other approaches (approximate values indicated",
      "page": 13
    },
    {
      "caption": "Table 4: compares model sizes, measured by the number of parameters, between our proposed method and other ap-",
      "page": 13
    },
    {
      "caption": "Table 6: , and Figure 6 present the results of the ablation analysis. We create four models by excluding one",
      "page": 13
    },
    {
      "caption": "Table 5: displays the accuracy of all models for both PSD and DE features. Each component",
      "page": 13
    },
    {
      "caption": "Table 6: and Figure 6. The confusion matrices (Figure 6) show that PC-TTA signiﬁcantly improves",
      "page": 13
    },
    {
      "caption": "Table 6: indicates that for classifying negative",
      "page": 13
    },
    {
      "caption": "Table 5: Overall accuracy comparisons on both datasets through the ablation study.",
      "page": 15
    },
    {
      "caption": "Table 6: Performance of different components in the proposed method for identifying positive and negative emotions.",
      "page": 15
    },
    {
      "caption": "Table 7: compares the performance of our PC-TTA approach with two other scenarios: full TTA, where TTA is applied",
      "page": 15
    },
    {
      "caption": "Table 7: Comparison of PC-TTA (proposed), Full TTA, and No TTA.",
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition and its applications. Human-computer systems interaction: Backgrounds and applications 3",
      "authors": [
        "Agata Kołakowska",
        "Agnieszka Landowska",
        "Mariusz Szwoch",
        "Wioleta Szwoch",
        "Michal Wrobel"
      ],
      "year": "2014",
      "venue": "Emotion recognition and its applications. Human-computer systems interaction: Backgrounds and applications 3"
    },
    {
      "citation_id": "2",
      "title": "A comparative analysis of machine learning methods for emotion recognition using eeg and peripheral physiological signals",
      "authors": [
        "Vikrant Doma",
        "Matin Pirouz"
      ],
      "year": "2020",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "3",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "4",
      "title": "Cross-database and cross-channel electrocardiogram arrhythmia heartbeat classification based on unsupervised domain adaptation",
      "authors": [
        "Md Niaz",
        "Naimul Khan"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "Exploring uncertainty in pseudo-label guided unsupervised domain adaptation",
      "authors": [
        "Jian Liang",
        "Ran He",
        "Zhenan Sun",
        "Tieniu Tan"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "A survey on negative transfer",
      "authors": [
        "Wen Zhang",
        "Lingfei Deng",
        "Lei Zhang",
        "Dongrui Wu"
      ],
      "year": "2022",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "7",
      "title": "Test-time augmentation for deep learning-based cell segmentation on microscopy images",
      "authors": [
        "Nikita Moshkov",
        "Botond Mathe",
        "Attila Kertesz-Farkas",
        "Reka Hollandi",
        "Peter Horvath"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "8",
      "title": "Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks",
      "authors": [
        "Guotai Wang",
        "Wenqi Li",
        "Michael Aertsen",
        "Jan Deprest",
        "Sébastien Ourselin",
        "Tom Vercauteren"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Efficientnet-based model with test time augmentation for cancer detection",
      "authors": [
        "Yang Jiang",
        "Rao Huang",
        "Jiacheng Shi"
      ],
      "year": "2021",
      "venue": "2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)"
    },
    {
      "citation_id": "10",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "12",
      "title": "Multi-source domain transfer discriminative dictionary learning modeling for electroencephalogram-based emotion recognition",
      "authors": [
        "Xiaoqing Gu",
        "Weiwei Cai",
        "Ming Gao",
        "Yizhang Jiang",
        "Xin Ning",
        "Pengjiang Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "13",
      "title": "A domain adaptation sparse representation classifier for cross-domain electroencephalogram-based emotion classification",
      "authors": [
        "Tongguang Ni",
        "Yuyao Ni",
        "Jing Xue"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "14",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Ming Hao Chen",
        "Zhunan Jin",
        "Cunhang Li",
        "Jinpeng Fan",
        "Huiguang Li",
        "He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Changde Du",
        "Yixin Wang",
        "Huiguang He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "16",
      "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "authors": [
        "Shiori Sagawa",
        "Pang Wei Koh",
        "Tatsunori Hashimoto",
        "Percy Liang"
      ],
      "year": "2019",
      "venue": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "arxiv": "arXiv:1911.08731"
    },
    {
      "citation_id": "17",
      "title": "Self-challenging improves cross-domain generalization",
      "authors": [
        "Zeyi Huang",
        "Haohan Wang",
        "Eric Xing",
        "Dong Huang"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "18",
      "title": "Confusing pair correction based on category prototype for domain adaptation under noisy environments",
      "authors": [
        "Churan Zhi",
        "Junbao Zhuo",
        "Shuhui Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Learning a robust unified domain adaptation framework for cross-subject EEG-based emotion recognition",
      "authors": [
        "Magdiel Jiménez",
        "Gibran Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "Multilayer neural network based speech emotion recognition for smart assistance",
      "authors": [
        "Sandeep Kumar",
        "Mohd Anul Haq",
        "Arpit Jain",
        "Andy Jason",
        "Nageswara Rao Moparthi",
        "Nitin Mittal",
        "Zamil Alzamil"
      ],
      "year": "2023",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "21",
      "title": "Machine vision-based human action recognition using spatio-temporal motion features (STMF) with difference intensity distance group pattern (DIDGP)",
      "authors": [
        "Jawaharlalnehru Arunnehru",
        "Sambandham Thalapathiraj",
        "Ravikumar Dhanasekar",
        "Loganathan Vijayaraja",
        "Raju Kannadasan"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "22",
      "title": "Sentiment analysis using machine learning: Progress in the machine intelligence for data science",
      "authors": [
        "Revathy",
        "Saleh A Alghamdi",
        "M Sultan",
        "Alahmari",
        "Anil Saud R Yonbawi",
        "Mohd Anul Kumar",
        "Haq"
      ],
      "year": "2022",
      "venue": "Sustainable Energy Technologies and Assessments"
    },
    {
      "citation_id": "23",
      "title": "EEG-based emotion recognition using multiple kernel learning",
      "authors": [
        "Qian Cai",
        "Guo-Chong Cui",
        "Hai-Xian Wang"
      ],
      "year": "2022",
      "venue": "Machine Intelligence Research"
    },
    {
      "citation_id": "24",
      "title": "An analytical approach of eeg analysis for emotion recognition",
      "authors": [
        "Indronil Mazumder"
      ],
      "year": "2019",
      "venue": "Devices for Integrated Circuit (DevIC)"
    },
    {
      "citation_id": "25",
      "title": "Recognition of emotional states using eeg signals based on time-frequency analysis and svm classifier",
      "authors": [
        "Fabian Parsia",
        "Istiaque Shaikat"
      ],
      "year": "2019",
      "venue": "Prommy Sultana Ferdawoos, Mohammad Zavid Parvez, and Jia Uddin"
    },
    {
      "citation_id": "26",
      "title": "Human emotion recognition based on eeg signal using fast fourier transform and k-nearest neighbor",
      "authors": [
        "Anton Yudhana",
        "Akbar Muslim",
        "Dewi Wati",
        "Intan Puspitasari",
        "Ahmad Azhari",
        "Murein Miksa"
      ],
      "year": "2020",
      "venue": "Adv. Sci. Technol. Eng. Syst. J"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition from multichannel eeg signals using k-nearest neighbor classification",
      "authors": [
        "Mi Li",
        "Hongpei Xu",
        "Xingwang Liu",
        "Shengfu Lu"
      ],
      "year": "2018",
      "venue": "Technology and health care"
    },
    {
      "citation_id": "28",
      "title": "Cross-subject emotion recognition with a decision tree classifier based on sequential backward selection",
      "authors": [
        "Wenge Jiang",
        "Guangyuan Liu",
        "Xingcong Zhao",
        "Fu Yang"
      ],
      "year": "2019",
      "venue": "2019 11th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)"
    },
    {
      "citation_id": "29",
      "title": "A feature extraction method based on differential entropy and linear discriminant analysis for emotion recognition",
      "authors": [
        "Dong-Wei Chen",
        "Rui Miao",
        "Wei-Qi Yang",
        "Yong Liang",
        "Hao-Heng Chen",
        "Lan Huang",
        "Chun-Jian Deng",
        "Na Han"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "30",
      "title": "Expression-eeg based collaborative multimodal emotion recognition using deep autoencoder",
      "authors": [
        "Hongli Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "EEG emotion recognition using improved graph neural network with channel selection",
      "authors": [
        "Xuefen Lin",
        "Jielin Chen",
        "Weifeng Ma",
        "Wei Tang",
        "Yuchen Wang"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "32",
      "title": "Customized 2d cnn model for the automatic emotion recognition based on eeg signals",
      "authors": [
        "Farzad Baradaran",
        "Ali Farzan",
        "Sebelan Danishvar",
        "Sobhan Sheykhivand"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "33",
      "title": "Emotion Recognition from Electroencephalogram (EEG) Signals Using a Multiple Column Convolutional Neural Network Model",
      "authors": [
        "Sonu Kumar",
        "Somaraju Suvvari",
        "Mukesh Kumar"
      ],
      "year": "2024",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "34",
      "title": "Fused cnn-lstm deep learning emotion recognition model using electroencephalography signals",
      "authors": [
        "Munaza Ramzan",
        "Suma Dawn"
      ],
      "year": "2023",
      "venue": "International Journal of Neuroscience"
    },
    {
      "citation_id": "35",
      "title": "Icaps-reslstm: Improved capsule network and residual lstm for eeg emotion recognition",
      "authors": [
        "Cunhang Fan",
        "Heng Xie",
        "Jianhua Tao",
        "Yongwei Li",
        "Guanxiong Pei",
        "Taihao Li",
        "Zhao Lv"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "36",
      "title": "Multi-source domain adaptation with spatio-temporal feature extractor for eeg emotion recognition",
      "authors": [
        "Wenhui Guo",
        "Guixun Xu",
        "Yanjiang Wang"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "37",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu",
        "Dan Zhang",
        "Sen Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Cross-subject eeg emotion recognition using multi-source domain manifold feature selection",
      "authors": [
        "Qingshan She",
        "Xinsheng Shi",
        "Feng Fang",
        "Yuliang Ma",
        "Yingchun Zhang"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "39",
      "title": "Generator-based domain adaptation method with knowledge free for cross-subject eeg emotion recognition",
      "authors": [
        "Dongmin Huang",
        "Sijin Zhou",
        "Dazhi Jiang"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "40",
      "title": "Domain adaptation techniques for EEG-based emotion recognition: A comparative study on two public datasets",
      "authors": [
        "Zirui Lan",
        "Olga Sourina",
        "Lipo Wang",
        "Reinhold Scherer",
        "Gernot Müller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "41",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Zhipeng He",
        "Yongshi Zhong",
        "Jiahui Pan"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "42",
      "title": "Subject-invariant eeg representation learning for emotion recognition",
      "authors": [
        "Soheil Rayatdoost",
        "Yufeng Yin",
        "David Rudrauf",
        "Mohammad Soleymani"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "43",
      "title": "Improving convolutional neural networks performance for image classification using test time augmentation: a case study using mura dataset. Health information science and systems",
      "authors": [
        "Ibrahem Kandel",
        "Mauro Castelli"
      ],
      "year": "2021",
      "venue": "Improving convolutional neural networks performance for image classification using test time augmentation: a case study using mura dataset. Health information science and systems"
    },
    {
      "citation_id": "44",
      "title": "Boosting anomaly detection using unsupervised diverse test-time augmentation",
      "authors": [
        "Seffi Cohen",
        "Niv Goldshlager",
        "Lior Rokach",
        "Bracha Shapira"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "45",
      "title": "Tmlp+ srdann: A domain adaptation method for eeg-based emotion recognition",
      "authors": [
        "Wei Li",
        "Bowen Hou",
        "Xiaoyu Li",
        "Ziming Qiu",
        "Bo Peng",
        "Ye Tian"
      ],
      "year": "2023",
      "venue": "Measurement"
    },
    {
      "citation_id": "46",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Yixin Wang",
        "Shuang Qiu",
        "Xuelin Ma",
        "Huiguang He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Zhunan Li",
        "Enwei Zhu",
        "Ming Jin",
        "Cunhang Fan",
        "Huiguang He",
        "Ting Cai",
        "Jinpeng Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition from eeg signals using empirical mode decomposition and second-order difference plot",
      "authors": [
        "Nilima Salankar",
        "Pratikshya Mishra",
        "Lalit Garg"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "49",
      "title": "Anul Haq, and others. Brain tumor identification using data augmentation and transfer learning approach",
      "authors": [
        "K Kumar",
        "P Dinesh",
        "P Rayavel",
        "L Vijayaraja",
        "R Dhanasekar",
        "R Kesavan",
        "K Raju",
        "A Ahmad Khan",
        "C Wechtaisong"
      ],
      "venue": "Computer Systems Science & Engineering"
    },
    {
      "citation_id": "50",
      "title": "DCNNBT: A novel deep convolution neural network-based brain tumor classification model",
      "authors": [
        "M Haq",
        "I Khan",
        "A Ahmed",
        "S Eldin",
        "A Alshehri",
        "N Ghamry"
      ],
      "year": "2023",
      "venue": "Fractals"
    },
    {
      "citation_id": "51",
      "title": "U-Net-based models towards optimal MR brain image segmentation",
      "authors": [
        "R Yousef",
        "S Khan",
        "G Gupta",
        "T Siddiqui",
        "B Albahlal",
        "S Alajlan",
        "M Haq"
      ],
      "year": "2023",
      "venue": "Diagnostics"
    }
  ]
}