{
  "paper_id": "2404.02589v1",
  "title": "Affective-Nli: Towards Accurate And Interpretable Personality Recognition In Conversation",
  "published": "2024-04-03T09:14:24Z",
  "authors": [
    "Zhiyuan Wen",
    "Jiannong Cao",
    "Yu Yang",
    "Ruosong Yang",
    "Shuaiqi Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as emotions that reflect the speakers' personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective Natural Language Inference (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for utterances. For interpretability of recognition results, we formulate personality recognition as an NLI problem by determining whether the textual description of personality labels is entailed by the dialog content. Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-theart approaches. Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker's personality in the early stages of conversations by surpassing stateof-the-art methods with 22%-34% 1 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "The advancement of large language models (e.g., ChatGPT) has led to the emergence of numerous applications within Human-Computer Interaction (HCI), including AI-based mental therapy  [1] , educational robots  [2] , and companion robots designed for the elderly  [3] . In these application scenarios, fast and accurately recognizing the user's personality from the dialog content is crucial, as it enables a better understanding of the user's personalized needs and requirements, ultimately leading to the provision of high-quality services  [4] -  [6] , as shown in Figure  1 .\n\nAlthough important, addressing personality recognition in conversation (PRC) also faces a significant challenge. PRC strives to classify the specified speaker's personality into predefined personality classes (e.g., the big five personalities  [7]  shown in Table  I ) by analyzing the dialog content. However, personalities are relatively permanent traits that describe the consistency and individuality of a person's behaviors  [8] . 1 Our source code and data is at https://github.com/preke/Affective-NLI. I am sorry to hear that. Try to be positive...",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Response Without Knowing Mrs.Thompson'Personality:",
      "text": "Response with personality recognition: Fig.  1 . When the robot lacks knowledge about Mrs. Thompson's personality, its comfort is generic and straightforward. However, if the robot deduces that Mrs. Thompson exhibits Neuroticism based on the dialog content, it can tailor its suggestions to offer more personalized and comforting advice.\n\nAccurate personality assessments typically involve obtaining detailed personal information or long-term historical data, such as self-report essays, questionnaires, or longitudinal records from social media, which are always unavailable in daily conversation scenarios.\n\nRecent studies solve the PRC problem utilizing the advanced semantic comprehension abilities of deep neural networks to analyze the dialog content. Specifically, researchers either design dialog encoding models  [9]  or fine-tune pretrained language models  [10] -  [12]  for PRC. However, these methods primarily concentrate on the explicit dialog content yet overlook two major concerns that hinder their recognition performance. First, semantic information contained in short utterances within conversations is inherently limited. These methods ignore crucial implicit factors contained in conversation, such as emotions and attitudes that reflect the speakers' personalities, so that hinders their recognition accuracies. Second, only focusing on the input dialog content disregards the semantic understanding of personality class labels, which loses sight of additional discriminative cues and reduces the interpretability of the results.\n\nTo fill in the research gap, we propose Affective-Natural Language Inference (Affective-NLI), aiming for accurate and interpretable personality recognition in conversation with affective annotations (i.e., discrete emotion category labels) of dialog content and the natural language descriptions of personality labels. Psychology findings suggest that there are strong correlations between personalities and affective expressions in conversations  [13] ,  [14] . However, accurately obtaining realtime affective annotations of dialog content for personality recognition is not a trivial task. We fine-tuned a pre-trained language model specifically for emotion recognition in conversations, facilitating real-time affective annotations for the utterances. Besides, the semantics behind personality labels contain descriptions of individual behavioral characteristics and tendencies in expressing affectivities, offering additional grounds for personality recognition. To utilize them for our problem, we adopt text descriptions of personality labels from psychology findings  [15] ,  [16]  and formulate personality recognition as a natural language inference (NLI) problem. Specifically, the objective is to determine whether the personality description of the speaker (as the hypothesis) is correct or not, given the dialog content (as the premise). We solve this NLI problem by constructing a natural language prompt, including the premise and the hypothesis above, to fine-tune the pre-trained language models.\n\nCompared to existing approaches, Affective-NLI leverages auxiliary affective information to enhance the limited semantics in dialog content. Then, Affective-NLI correlates the language and behavior of speakers within conversations with descriptions of personality traits, yielding more interpretable recognition results. Moreover, Affective-NLI mainly manipulates the input dialog content, making it compatible with large pre-trained language models in various architectures.\n\nWe conducted extensive experiments on two daily conversation datasets annotated with personality labels. In the standard PRC setting, where the full dialog input is provided, Affective-NLI demonstrated a significant improvement (6%-7%) in the accuracy of personality recognition compared to state-of-the-art methods. Additionally, we designed a Flow experiment to assess whether Affective-NLI can instantly recognize the speaker's personality during the early stages of a conversation. The results showed that with just one or two utterances from the speaker under analysis, Affective-NLI achieved an accuracy of around 0.5-0.6 in personality recognition, surpassing state-of-the-art methods by 22%-34%. These findings suggest that Affective-NLI can be effectively employed for personality recognition in real-world humancomputer interaction (HCI) applications. To demonstrate the interpretability of Affective-NLI, we conduct a case study to illustrate how our methods obtain the personality recognition results in a conversation example.\n\nTo summarize, our contributions are listed as follows:\n\n• We introduce Affective-Natural Language Inference (Affective-NLI) to facilitate personality recognition in conversation with affective annotations of dialog content and the text descriptions of personality labels. • Affective-NLI is capable of providing interpretable recognition results by correlating the language and behavior of speakers within conversations with descriptions of personality traits.\n\n• Extensive experiments conducted on two conversational datasets verify the effectiveness of Affective-NLI. Additionally, Affective-NLI has been validated for early-stage personality recognition during conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "In this section, we review existing research related to personality recognition in conversation.\n\nAccurately recognizing users' personalities in conversation is crucial for various applications, like building user trust in conversational agents  [17] , providing considerate elderly companions  [4] , or delivering personalized customer service  [5] . So, this task has long been a critical research issue and explored by many researchers.\n\nResearch in early stages focus on finding statistical features for recognizing personality. The statistical word usage patterns and social behavior habits are highly correlated to the personality traits  [18] . Informed by these findings,  [19]  tracked 96 participants using the Electronically Activated Recorder (EAR) to examine the expression of personality in daily conversations. Besides,  [20]  firstly presented several non-linear statistical models to rank utterances on the Big Five personality traits with linguistic features. However, although the shallow features are efficient in providing statistical differences for personality recognition, they fail when the personalities are identified by deep understanding the dialog content.\n\nWith the development of deep learning, neural network models are widely applied to recognizing personality through understanding the semantics of the conversation.  [9]  proposed a classification model based on capsule neural networks to extract meaningful hidden patterns from conversations and use them to assess the personality of individuals. Subsequently,  [10]  used the pre-trained language models to encode the dialog content in different forms for personality classification. In  [11] , the authors inputted the full conversation text into BERT  [21]  and five independent SeNets  [22]  as the feature fusion layer of local personality features of different conversations. Some researchers also investigate prompt-tuning pre-trained language models (PLMs) for personality recognition. DesPrompt  [12]  Fig.  2 . An overview of Affective-NLI with the first four utterances in Figure  1 . The upper-left part illustrates the affective dialog content construction. The lower-left part shows the positive and negative label descriptions of Neuroticism. The right part shows (1) how we construct NLI prompting samples with the affective dialog content and both positive and negative personality label descriptions and (2) how we combine both NLI results for the final personality recognition. Noting that the two different inputs to the same pre-trained language model. utilizes a personality-descriptive prompt to encapsulate the input content, hence reformulates personality recognition as a cloze-style word-filling task to align with the pre-training Masked Language Modeling (MLM) task in PLMs.\n\nHowever, in conversation scenarios, the utterances used for personality recognition are short sentences with limited quantity. Only focusing on semantic information in utterances fails to provide sufficient information to analyze personality, which hinders the recognition accuracy. Besides, concentrating on the input dialog content neglects the semantic comprehension of personality itself, thereby overlooking valuable discriminative cues and diminishing the interpretability of the results.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Affective-Nli",
      "text": "In this section, we introduce the proposed Affective-NLI (as shown in Figure  2 ). We begin by presenting the formulation of the PRC problem in Section III-A. Subsequently, we delve into the construction of affective dialog content by Affective-NLI in Section III-B, followed by the introduction of the personality label descriptions utilized in Section III-C. Lastly, we introduce how we construct NLI samples for fine-tuning and inference in pre-trained language models in Section III-D.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "We study the Personality Recognition in Conversation (PRC) problem as stated below:\n\nGiven the conversation content X = {u 1 , u 2 , ..., u m } with m text utterances u i among a speaker s and other speakers, the objective is to recognize the personality traits of s, denoted as y. y is represented as a 5-d binary vector [y a , y c , y e , y n , y o ] indicating the binary values of each trait referring to the bigfive personality traits theory (shown in Table  I , shortened as AGR, CON, EXT, NEU, and OPN, respectively). The bigfive trait theory offers a categorical classification of personality derived from both the trait theory and the lexical hypothesis in psychology. It is widely used in analysis for social media content  [23] -  [26]  and conversations  [20] .\n\nWe propose Affective-Natural Language Inference (Affective-NLI) to formulate the PRC problem as the NLI problem between the affective dialog content (premise) and the descriptions of personality labels (hypothesis). Natural Language Inference (NLI) determines whether a hypothesis is true (entailment), false (contradiction), or undetermined (neutral) given a premise. In the scenario of PRC, Affective-NLI determines whether the hypothesis of the personality label description for the speaker s is correct or not, given the affective dialog content as the premise.\n\nFormally, to recognize each specific personality trait y p ∈ [y a , y c , y e , y o , y n ] for the speaker s given input dialog content, Affective-NLI maximizes the sum of the two probabilities:\n\nwhere the X af = {(u 1 , a 1 ), (u 2 , a 2 ), ..., (u m , a m )} is the affective dialog content obtained by attaching an affective annotation a i to each utterance u i . H pos (p) and H neg (p) are the natural language descriptions of people with and without the specific personality trait p, respectively. Note that H pos (p) and H neg (p) are opposite descriptions to a same personality trait p. The principle of maximizing both P pos and P neg is to affirm the personality description that aligns with the speaker s and simultaneously negate the description that contradicts s's personality.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Affective Dialog Content Construction",
      "text": "We first introduce how we construct the affective dialog content X af from the original dialog content X.\n\nIt is been pointed out that there are strong relations between the Extraversion and Conscientiousness traits and the positive affects such as Joy and Surprise; and between Neuroticism and disagreeableness and various negative affects such as Anger, Sadness, or Fear  [27] . By observing the dialogues in daily conversation, we also found that the utterance-level emotional interaction among the speakers (e.g., in what emotion the speaker s talks to others or how others respond to s) is essential in analyzing the personality traits of s.\n\nAlthough affectivity is essential for personality recognition, in conversation scenarios, especially in real-time interaction between users and conversational agents, it is unrealistic to always have accurate affective annotations for utterances. Therefore, we fine-tuned a pre-trained language model M ERC especially for Emotion Recognition in Conversations to obtain the emotion label e i of each u i ∈ X in an offline manner: e1, e2, ..., em = MERC (u1, u2, ..., um)\n\nwhere each e i is one of the six basic emotions  [28]  (i.e., Anger, Disgust, Fear, Joy, Sadness, Surprise) and Neutral. This emotion categories are widely employed in studies on personality analysis  [29] ,  [30] .\n\nAlthough existing research  [31] ,  [32]  has validated the affective processing capabilities of language models for text, the interaction among speakers in conversational scenarios provides additional contextual cues for emotion recognition. Inspired by previous work on modeling dialogue flows  [33] -  [35] , we incorporate the speaker's identity into the utterance sequence during the encoding process to achieve more accurate emotion recognition, where the speaker's identity refers to binary indicators showing whether the current utterance is from the speaker s.\n\nAfter we get each emotion label e i , we wrap it into a handcraft template to form the affective description a i for the corresponding utterance u i . The detailed process is depicted in Algorithm 1, where the input consists of the dialog content accompanied by an emotion label sequence, and the output yields the dialog content enveloped by affective descriptions. We tried various templates and found that different template contents had little impact on the final effect, so we adopted the current illustrated template content.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Personality Description Construction",
      "text": "The class labels of the big five personality traits (such as Conscientiousness and Neuroticism) are not commonly used in everyday language. So, language models trained on general corpora may struggle to understand these terms, making personality identification challenging in conversational contexts.\n\nThe lexical hypothesis of personality traits states that (1) the most distinctive, significant, and widespread phenotypic attributes tend to become encoded as words in the conceptual reservoir of language  [36] . Therefore, we adopt and summarize descriptions of the big five personality traits in both positive and negative classes (H pos , H neg ) from psychology findings  [15] ,  [16] , as shown in Figure II. These descriptions contain rich semantic meanings of personality traits in daily used languages that outline affective and behavioral tendencies of people with different personality traits. In Affective-NLI, these descriptions establish a correlation between the language and behavior of speakers during conversations and the personality traits, resulting in more understandable recognition results.\n\nOne possible concern is that these label descriptions may not be comprehensive or subjective in their wording. However, considering that these descriptions will be encoded in pre-trained models, benefiting from the powerful semantic understanding capabilities of language models, the semantics of each word within the descriptions, including its synonyms and similar expressions, will also be captured and encoded, to provide the auxiliary basis for personality recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Nli Training And Inference",
      "text": "To maximize the probability in Formula 1, we construct NLI samples for fine-tuning the pre-trained language model M in a prompt tuning manner.\n\nWe first wrap the original dialog content X into the following two NLI input samples when recognizing each single personality trait p:\n\nThe model M is fine-tuned to fill in the [MASK] with \"yes\" and \"no\".\n\nDuring training, we construct the NLI samples as follows:\n\n{Tpos(X, p), yes}, {Tneg(X, p), no}; if yp = 1 {Tpos(X, p), no}, {Tneg(X, p), yes}; if yp = 0\n\nwhere \"yes\" and \"no\" are the ground truth for NLI samples.\n\nAccording to the NLI samples, we denote the four probabilities as follows: The training objective is fine-tuning the parameters θ in M to maximum the sum of the probabilities\n\nIV. EXPERIMENT SETTINGS",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "Recording daily conversations for analysis, especially multiperson conversations, has potential privacy concerns. According to the book Television Dialogue: The sitcom Friends vs natural conversation  [37] , television conversations and natural conversations are basically the same in terms of linguistic features. Therefore, we use two datasets analyzing the scripts of TV Series: FriendsPersona constructed by  [10]  and the Chinese Personalized and Emotional Dialogue (CPED) from  [11]  to evaluate Affective-NLI.\n\nFriendsPersona is a dialog script dataset with personality annotations in 711 different dialogues, including 8,157 utterances. In each dialogue, The personality of one analyzed speaker is represented as 5-d binary vectors for the big-five traits. Following the problem settings in similar works  [9] ,  [10]  of personality analysis in conversation, we conduct binary classification tasks over the five personality traits respectively to evaluate our method.\n\nCPED is a large-scale Chinese personalized and emotional dialogue dataset, which contains around 12K dialogues of 392 speakers from 40 popular Chinese TV shows. To be consistent with the affective prompts and the personality descriptions in constructing input samples, we translate the dialogues in CPED into English with Google Translation 2  and manual verification. The detailed statistics are shown in Table  III .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Methods In Comparison",
      "text": "Affective-NLI mostly manipulates the input content, making it compatible with various large pre-trained language   [38] , encoder-decoder: T5  [39] , and decoder-only: Llama  [40] ). We also compare Affective-NLI with two state-of-the-art methods  [10] ,  [12]  in fine-tuning and prompt-tuning PLMs for personality recognition in conversation. Details of the methods are introduced as below.\n\nAffective-NLI (RoBERTa): Here, we use the pre-trained Roberta-base model for Affective-NLI. RoBERTa  [38]  is a famous pre-trained language model whose pre-training task is Masked-Language-Modeling (filling the masked tokens in sentences). It's natural to adopt it to conduct the mask-filling task in Affective-NLI.\n\nAffective-NLI (T5): Here, we use the pre-trained T5-base model as the backbone model of Affective-NLI. T5  [39]  is a unified pre-trained language model to convert all text-based language problems into a sequence-2-sequence format. In affective-NLI, we use the token generated by the t5-decoder to fill in the [MASK] in affective-NLI.\n\nAffective-NLI (Llama): Llama  [40]  is one of the most popular large generative language models, comparable with ChatGPT 3 in some tasks 4 . We adopt a parameter-efficient fine-tuning approach (i.e. P-tuning  [41] ) to only fine-tune a small number of extra parameters (∼0.6% of the whole model) while freezing most parameters of the pre-trained Llama-7b with around 7 billion parameters. Specifical to Affective-NLI, we use the last token generated by Llama to fill in the [MASK].\n\nFT-RoBERTa: FT-RoBERTa  [10]  uses RoBERTa-base as the dialog encoder with three different kinds of input for personality recognition. FT-RoBERTa(S) only uses the utterances from the analyzed speaker s as the input; FT-3 https://openai.com/blog/chatgpt 4 We didn't implement Affective-NLI on ChatGPT as it is not open-sourced.\n\nRoBERTa (S+C) concatenates utterances from the analyzed speaker and other utterances as context together as the input, while FT-RoBERTa (F) inputs all the utterances within the whole dialog flow in their natural order for classification.\n\nTo better illustrate the three inputs: if a dialogue flow is\n\n..] with the original sentence order. DesPrompt: DesPrompt  [12]  generates personality-descriptive prompts and fine-tunes pre-trained language models (i.e., here we use RoBERTa-base) for efficient personality recognition. It reformulates personality recognition as a cloze-style wordfilling task and aligns with the pre-training Masked Language Modeling (MLM) task in most PLMs, hence enhancing the utilization of the pre-trained parameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Evaluation Tasks And Metrics",
      "text": "We split both datasets into train, validation, and test sets with a portion around 8:1:1. Models are trained on the train and validation sets and the results on test sets are reported.\n\nTo comprehensively evaluate the performance of Affective-NLI and other models, we design two personality recognition tasks in different settings specific to the conversation scenario: Overall and Flow. The Overall is the regular task that takes the full length of dialog content for personality recognition. The Flow examines the personality recognition performance at first 25%, 50%, 75%, and the whole dialog flow. This is to show whether Affective-NLI can be applied to real conversation services to instantly recognize the personality of the user in the early stages of conversations.\n\nFor both tasks, we use the binary classification accuracy to evaluate the personality recognition performance on each trait  and the averaged result.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Implementation Details",
      "text": "During implementation, we pad all the utterances with [PAD] to the MAX LEN of 256. Each dialog flow is padded to a Dialogue MAX LEN of 20 according to the dataset statistics. The dialog flows are fed into the models in batches with a size of 32. As for the pre-trained Emotion Recognition in Conversation model M ERC , we classify 1000 randomly selected utterances in both datasets (in balanced emotion labels) by M ERC . The accuracies are 0.792 for FriendsPersona and 0.815 for CPED, respectively, which is considerably accurate for the classification of seven classes. We use the Adam  [42]  as the optimization algorithm in training. The learning rate for each model is selected to refer to the best performance on the validation sets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results Analysis",
      "text": "Based on the aforementioned settings, we perform comprehensive experiments and conduct both quantitative and qualitative analyses of the obtained results. The quantitative analysis aims to address the following three Research Questions (RQs), while the qualitative analysis utilizes a case study to showcase how Affective-NLI can provide interpretable personality recognition results.\n\n• RQ 1: Can Affective-NLI outperform state-of-the-art approaches on personality recognition in conversation? • RQ 2: How much do different modules in Affective-NLI influence the performance in personality recognition? • RQ 3: How early can Affective-NLI recognize the personality in a dialog flow?\n\nA. Can Affective-NLI outperform state-of-the-art approaches on personality recognition in conversation (PRC)?\n\nTo answer this research question, we analyze the PRC results of the methods introduced in Section IV-B on FriendsPersona and CPED are shown in Table  IV . These results are obtained from the Overall experiment setting.\n\nOn both datasets, Affective NLI achieves the best accuracies (including single personality trait recognition and overall performances) compared with other methods, which validates its effectiveness in personality recognition in conversations. Specifically, the best Affective-NLI method (i.e., Affective-NLI (T5)) outperforms the best baseline methods on both datasets by around 6-7%.\n\nDue to different distributions, the models' performance in recognizing the same personality trait varies across the two datasets. However, we can still observe that Affective-NLI exhibits the highest improvement over other methods in terms of AGR and NEU. We hypothesize that this may be attributed to a greater disparity in affective information expressed by positive and negative speakers in conversations pertaining to these two personality types.\n\nWithin the three implementations, Affective-NLI (T5) achieved the highest accuracy in recognizing seven out of the ten personality traits. This resulted in Affective-NLI (T5) having the best average results on both datasets. This suggests that compared to other types of pre-trained language models, the encoder-decoder architecture of the T5 model performs better in this form of NLI task. In addition, Affective-NLI (RoBERTa) and Affective-NLI (Llama) performed similarly, indicating that the parameter size of the pre-trained models does not play a decisive role in the results of the current PRC task. Therefore, Affective-NLI can be implemented on slightly smaller pre-trained language models without sacrificing accuracy, which validates the usability of Affective-NLI.\n\nAlthough our work has achieved state-of-the-art performance compared to most existing models, we are aware that the current performance still has some distance from realworld applications. Addressing this issue more effectively will require further efforts from interdisciplinary work in the future.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. How Much Do Different Modules In Affective-Nli Influence The Performance In Personality Recognition?",
      "text": "To figure out the answer, we design an ablation study to show the results of sub-models without different modules in Affective-NLI, respectively. We selected the Affective-NLI (T5) model with the best overall performance and designed the following sub-models: w.o. affective: we remove the affective annotations, replacing X af with the original X. However, we retained the personality description in the input. This is to evaluate the effect of affective information on personality recognition. w.o. personality: we keep the affective annotations and the NLI task format in the input but replace the personality label descriptions as directly the personality label name. This is to investigate the influence of semantic information contained in the personality descriptions on the PRC task. only Pos: in this sub-model, we maintained the input of Affective-NLI but only used positive personality descriptions during training and inference. This is to examine whether combining two different descriptions of the same personality enhances performance.\n\nWe conduct the personality recognition in conversation on FriendsPersona and CPED with all the sub-models, their accuracies are shown in Figure  3 . These results are also obtained from the Overall experiment setting. In general, removing any module in Affective-NLI results in a decrease in model performance, which validates our model design. Specifically, when removing the personality description (w.o. personality), the performance of Affective-NLI drops significantly on both datasets, highlighting the importance of enabling the model to understand the semantics of personality for personality recognition. Besides, removing the affective annotations (w.o. affective), also weakens the model to a considerable extent, particularly evident in the FriendsPersona dataset. By referring to Table  III , it can be observed that utterance length in FriendsPersona is apparently shorter. This suggests that providing affective information can be more helpful for personality recognition when there is less conversation content available. When using only the positive descriptions of each personality trait in Affective-NLI (only Pos), the model also shows a certain degree of performance decline, with a more pronounced decrease in FriendsPersona. Therefore, supervising the model to learn semantic information from different categories of personality traits can provide auxiliary benefits.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. How Early Can Affective-Nli Recognize The Personality In A Dialog Flow?",
      "text": "To evaluate whether Affective-NLI can be applied to conversation scenarios for instant personality recognition, we design an experiment Flow by feeding the first 25%, 50%, 75%, and 100% of each dialog flow in Test sets to the personality recognition models. According to the different lengths of dialog flows, we calculate the average numbers of utterances from the analyzed speaker s are 0.52, 1.48, 2.63, and 4.09, respectively. So, we make an additional rule that each dialog flow fed into the models must contain at least one utterance from the analyzed speaker s. Based on the aforementioned data  settings, the average accuracy of all models for personality recognition of the five personality traits on both datasets is shown in Figure  4 .\n\nWhen the dialog flow is at 25%, Affective-NLI models significantly outperform other methods. It suggests that in the early stage of a dialogue (only one of two sentences from the analyzed speaker), the affective information and label semantics provide additional evidence to analyze the personality effectively. DesPrompt also performs relatively well as it provides an ample amount of descriptive label words.\n\nWhen feeding 50%-75% of the whole dialog flow, Affective-NLI models also outperform the rest of the models. However, fine-tuning RoBERTa models (FT-RoBERTa (S), FT-RoBERTa (S+C), and FT-RoBERTa (F)) improve dramatically, and even outperform DesPrompt on FriendsPersona. This demonstrates the importance of rich semantic content in dialogues for traditional classification methods. Therefore, Affective-NLI effectively addresses the challenge of insufficient semantic information in early-stage conversations, which is a limitation of traditional classification methods.\n\nThe improvement rate of the model's performance slows down from feeding 75% to the entire dialog flow. This suggests that the relationship between semantic information and effectiveness in personality recognition during a conversation is not simply linearly correlated.\n\nTo summarize, the strength of integrating affective information and label semantics in Affective-NLI is reflected in the early stage of conversations. With only one or two utterances from the analyzed speaker, Affective-NLI can instantly recognize the personality with around 0.5-0.6 in accuracy.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Case Study",
      "text": "We use a case study to illustrate the usage of Affective-NLI in real HCI application scenarios, as shown in Table  V .\n\nAffective-NLI is designed for personality recognition in conversations. Therefore, the main experiments intend to verify the personality recognition performance with existing dialog pieces. However, in real HCI applications, we still need to design conversation agents to select appropriate, emotional, and personalized dialogue strategies based on the recognized personality. The case study here demonstrates how Affective-NLI supports such dialogue strategies during interactions. In this example, we pretend to be Mrs. Thompson and engage in a conversation with the Agent. The probability of personality in the case study is obtained from Affective-NLI, and the Agent's responses are generated by ChatGPT based on the identification results from Affective-NLI.\n\nIn Table  V , the dialog content with affective annotations is sequentially inputted into Affective-NLI to recognize the personality trait of Mrs. Thompson. Specifically, when Mrs. Thompson responds to the Agent, Affective-NLI calculates the probabilities (confidences of recognition results) of Mrs. Thompson exhibiting all the big five personality traits displayed on the right side of the Table . To facilitate decisionmaking, we can set a probability threshold, denoted as δ (e.g., 60%). When the probability of Mrs. Thompson possessing a certain personality trait (1) exceeds δ and (2) is the highest among the five personality traits, the Agent can generate a response with a specific dialogue policy tailored to that personality trait.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vi. Discussion",
      "text": "In this section, we discuss three practical issues of utilizing Affective-NLI in real applications.\n\nFirstly, the recognition results of Affective-NLI cannot be directly used as diagnostic evidence for psychological therapy. Personality is a multifaceted concept that is investigated and discussed in various disciplines, including physiology, linguistics, and cognitive science. Therefore, rigorous personality assessment still relies on professional approaches. The purpose and advantage of Affective-NLI lie in its ability to provide preliminary evaluations of users' personalities in conversational settings, facilitating a better conversational experience.\n\nSecondly, Affective-NLI can only recognize the personality manifested during conversations, which may not reflect the users' actual personalities. The content of conversations is constrained by different contexts, and individuals may not fully express their personalities due to politeness, cultural factors, and other considerations in specific situations. However, this limitation is inherent to the problem of personality recognition in conversation itself.\n\nThirdly, both datasets used to evaluate Affective-NLI consist of scripts from TV series rather than real-life conversations. Collecting conversations for personality analysis should be cautiously approached due to potential ethical and privacy concerns. To provide a stronger rationale for future data collection and usage, we initially validate our concept using dialog scripts from TV series. However, since the scripts we use are derived from TV series in various cultures and contexts, our model can still be either directly applied or fine-tuned with a small amount of real data to adapt to real application scenarios.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion, Limitations And Future Work",
      "text": "This paper presents Affective-NLI, a simple yet effective approach for enhancing dialog content with affective information and personality label descriptions, enabling accurate personality recognition in conversations. Affective-NLI provides interpretable recognition results and is easy to implement with various architectures of pre-trained language models. Through extensive experiments, we validate the effectiveness of Affective-NLI in personality recognition. Furthermore, Affective-NLI demonstrates its suitability for early-stage personality recognition in conversations.\n\nAlthough the effectiveness of Affective-NLI has been validated, we identify some limitations that can be improved and expanded upon in future work. First, Affective-NLI follows the setting in existing studies on personality recognition in conversation, where personality recognition is performed through independent binary classification of each trait. However, this setting overlooks the correlations among different personality traits, which have been explored in psychology findings  [43] ,  [44] . In future research, we will also investigate how to incorporate these correlations into Affective-NLI for more accurate personality recognition. Second, Affective-NLI focuses solely on the textual modality. Personality can be manifested in non-verbal factors such as facial expressions  [45]  and face gestures  [46] , or physiological signals like Electrocardiogram (ECG) and heart rate  [47] . In future work, we will also investigate transforming signals collected from pervasive systems into textual descriptions and achieving multimodal personality recognition.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Although important, addressing personality recognition in",
      "page": 1
    },
    {
      "caption": "Figure 1: When the robot lacks knowledge about Mrs. Thompson’s personality,",
      "page": 1
    },
    {
      "caption": "Figure 2: An overview of Affective-NLI with the first four utterances in Figure 1. The upper-left part illustrates the affective dialog content construction. The",
      "page": 3
    },
    {
      "caption": "Figure 2: ). We begin by presenting the formulation",
      "page": 3
    },
    {
      "caption": "Figure 3: Personality recognition accuracy comparison among the sub-models",
      "page": 7
    },
    {
      "caption": "Figure 3: These results are also",
      "page": 7
    },
    {
      "caption": "Figure 4: The personality recognition accuracy comparison in the Flow",
      "page": 8
    },
    {
      "caption": "Figure 4: When the dialog flow is at 25%, Affective-NLI models",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Statistics": "# Dialogues",
          "FriendsPersona": "711",
          "CPED": "11,835"
        },
        {
          "Statistics": "# Uttrs per dialogue",
          "FriendsPersona": "11.8",
          "CPED": "11.2"
        },
        {
          "Statistics": "# Unique Uttrs",
          "FriendsPersona": "8,157",
          "CPED": "109,455"
        },
        {
          "Statistics": "Uttr Length",
          "FriendsPersona": "16.3",
          "CPED": "28.9"
        },
        {
          "Statistics": "Label\nDistribution\n(Positive:Negative)",
          "FriendsPersona": "AGR(.43:.57)\nCON(.46:.54)\nEXT(.44:.56)\nOPN(.35:.65)\nNEU(.47:.53)",
          "CPED": "AGR(.58:.42)\nCON(.67:.33)\nEXT(.65:.35)\nOPN(.50:.50)\nNEU(.59:.41)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "FriendsPersona",
          "Method": "FT-RoBERTa (S)\nFT-RoBERTa (S+C)\nFT-RoBERTa (F)\nDesPrompt",
          "AGR\nCON\nEXT\nOPN\nNEU\nAvg": "0.579±0.056\n0.564±0.062\n0.567±0.065\n0.621±0.042\n0.525±0.051\n0.571\n0.579±0.033\n0.568±0.065\n0.568±0.065\n0.621±0.042\n0.531±0.049\n0.573\n0.579±0.056\n0.565±0.065\n0.568±0.065\n0.621±0.042\n0.529±0.048\n0.572\n0.521±0.076\n0.535±0.048\n0.549±0.045\n0.649±0.015\n0.495±0.039\n0.550"
        },
        {
          "Dataset": "",
          "Method": "Affective-NLI\n(RoBERTa)\nAffective-NLI\n(T5)\nAffective-NLI\n(Llama)",
          "AGR\nCON\nEXT\nOPN\nNEU\nAvg": "0.597±0.041\n0.569±0.051\n0.597±0.036\n0.639±0.044\n0.583±0.069\n0.597\n0.613\n0.625±0.034\n0.592±0.039\n0.614±0.056\n0.655±0.076\n0.581±0.073\n0.595±0.021\n0.566±0.004\n0.612±0.011\n0.645±0.030\n0.568±0.008\n0.597"
        },
        {
          "Dataset": "CPED",
          "Method": "FT-RoBERTa (S)\nFT-RoBERTa (S+C)\nFT-RoBERTa (F)\nDesPrompt",
          "AGR\nCON\nEXT\nOPN\nNEU\nAvg": "0.602±0.012\n0.654±0.011\n0.660±0.017\n0.529±0.012\n0.595±0.014\n0.608\n0.599±0.016\n0.656±0.012\n0.659±0.017\n0.529±0.014\n0.600±0.016\n0.608\n0.600±0.010\n0.653±0.008\n0.662±0.013\n0.532±0.020\n0.600±0.011\n0.610\n0.611±0.003\n0.660±0.032\n0.671±0.022\n0.516±0.027\n0.608±0.020\n0.613"
        },
        {
          "Dataset": "",
          "Method": "Affective-NLI\n(RoBERTa)\nAffective-NLI\n(T5)\nAffective-NLI\n(Llama)",
          "AGR\nCON\nEXT\nOPN\nNEU\nAvg": "0.637±0.007\n0.662±0.021\n0.676±0.016\n0.534±0.014\n0.612±0.041\n0.624\n0.648\n0.677±0.014\n0.656±0.013\n0.689±0.027\n0.569±0.018\n0.647±0.018\n0.645±0.033\n0.668±0.004\n0.677±0.009\n0.542±0.021\n0.605±0.043\n0.627"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ai in mental health",
      "authors": [
        "S Alfonso"
      ],
      "year": "2020",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "2",
      "title": "Hybrid physical education teaching and curriculum design based on a voice interactive artificial intelligence educational robot",
      "authors": [
        "D Yang",
        "E.-S Oh",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Sustainability"
    },
    {
      "citation_id": "3",
      "title": "Chatgpt, public health communication and 'intelligent patient companionship",
      "authors": [
        "J Kahambing"
      ],
      "year": "2023",
      "venue": "Journal of public health"
    },
    {
      "citation_id": "4",
      "title": "Stereotypes or golden rules? exploring likable voice traits of social robots as active aging companions for tech-savvy baby boomers in taiwan",
      "authors": [
        "-S Chang",
        "H.-P Lu",
        "P Yang"
      ],
      "year": "2018",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "5",
      "title": "A development of personality recognition model from conversation voice in call center context",
      "authors": [
        "N Srinarong",
        "J Mongkolnavin"
      ],
      "year": "2021",
      "venue": "The 12th International Conference on Advances in Information Technology"
    },
    {
      "citation_id": "6",
      "title": "A survey of personality and learning styles models applied in virtual environments with emphasis on e-learning environments",
      "authors": [
        "S Fatahi",
        "H Moradi",
        "L Kashani-Vahid"
      ],
      "year": "2016",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "7",
      "title": "Normal personality assessment in clinical practice: The neo personality inventory",
      "authors": [
        "P Costa",
        "R Mccrae"
      ],
      "year": "1992",
      "venue": "Psychological assessment"
    },
    {
      "citation_id": "8",
      "title": "theories of personality",
      "authors": [
        "J Feist",
        "G Feist"
      ],
      "year": "2008",
      "venue": "J"
    },
    {
      "citation_id": "9",
      "title": "Personality recognition in conversations using capsule neural networks",
      "authors": [
        "E Rissola",
        "S Bahrainian",
        "F Crestani"
      ],
      "year": "2019",
      "venue": "IEEE/WIC/ACM International Conference on Web Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Automatic text-based personality recognition on monologues and multiparty dialogues using attentive networks and contextual embeddings (student abstract)",
      "authors": [
        "H Jiang",
        "X Zhang",
        "J Choi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Cped: A large-scale chinese personalized and emotional dialogue dataset for conversational ai",
      "authors": [
        "Y Chen",
        "W Fan",
        "X Xing",
        "J Pang",
        "M Huang",
        "W Han",
        "Q Tie",
        "X Xu"
      ],
      "year": "2022",
      "venue": "Cped: A large-scale chinese personalized and emotional dialogue dataset for conversational ai",
      "arxiv": "arXiv:2205.14727"
    },
    {
      "citation_id": "12",
      "title": "Desprompt: Personality-descriptive prompt tuning for few-shot personality recognition",
      "authors": [
        "Z Wen",
        "J Cao",
        "Y Yang",
        "H Wang",
        "R Yang",
        "S Liu"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "13",
      "title": "Relationships among three general approaches to personality description",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1995",
      "venue": "The journal of Psychology"
    },
    {
      "citation_id": "14",
      "title": "Analysis of the big-five personality factors in terms of the pad temperament model",
      "year": "1996",
      "venue": "Australian journal of Psychology"
    },
    {
      "citation_id": "15",
      "title": "Evidence for the big five in analyses of familiar english personality adjectives",
      "authors": [
        "G Saucier",
        "L Goldberg"
      ],
      "year": "1996",
      "venue": "European journal of Personality"
    },
    {
      "citation_id": "16",
      "title": "The big five personality factors and personal values",
      "authors": [
        "S Roccas",
        "L Sagiv",
        "S Schwartz",
        "A Knafo"
      ],
      "year": "2002",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "17",
      "title": "Relational agents: a model and implementation of building user trust",
      "authors": [
        "T Bickmore",
        "J Cassell"
      ],
      "year": "2001",
      "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems"
    },
    {
      "citation_id": "18",
      "title": "Linguistic inquiry and word count: Liwc",
      "authors": [
        "J Pennebaker",
        "M Francis",
        "R Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc"
    },
    {
      "citation_id": "19",
      "title": "Personality in its natural habitat: Manifestations and implicit folk theories of personality in daily life",
      "authors": [
        "M Mehl",
        "S Gosling",
        "J Pennebaker"
      ],
      "year": "2006",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "20",
      "title": "Automatic recognition of personality in conversation",
      "authors": [
        "F Mairesse",
        "M Walker"
      ],
      "year": "2006",
      "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers"
    },
    {
      "citation_id": "21",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "22",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "Large scale personality classification of bloggers",
      "authors": [
        "F Iacobelli",
        "A Gill",
        "S Nowson",
        "J Oberlander"
      ],
      "year": "2011",
      "venue": "international conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "24",
      "title": "Personality classification based on profiles of social networks' users and the five-factor model of personality",
      "authors": [
        "A Souri",
        "S Hosseinpour",
        "A Rahmani"
      ],
      "year": "2018",
      "venue": "Human-centric Computing and Information Sciences"
    },
    {
      "citation_id": "25",
      "title": "Is chatgpt a good personality recognizer? a preliminary study",
      "authors": [
        "Y Ji",
        "W Wu",
        "H Zheng",
        "Y Hu",
        "X Chen",
        "L He"
      ],
      "year": "2023",
      "venue": "Is chatgpt a good personality recognizer? a preliminary study"
    },
    {
      "citation_id": "26",
      "title": "Can chatgpt assess human personalities? a general evaluation framework",
      "authors": [
        "H Rao",
        "C Leung",
        "C Miao"
      ],
      "year": "2023",
      "venue": "Can chatgpt assess human personalities? a general evaluation framework"
    },
    {
      "citation_id": "27",
      "title": "On traits and temperament: General and specific factors of emotional experience and their relation to the fivefactor model",
      "authors": [
        "D Watson",
        "L Clark"
      ],
      "year": "1992",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "28",
      "title": "The nature of emotion: Fundamental questions",
      "authors": [
        "P Ekman",
        "R Davidson"
      ],
      "year": "1994",
      "venue": "The nature of emotion: Fundamental questions"
    },
    {
      "citation_id": "29",
      "title": "Robotic emotional expression generation based on mood transition and personality model",
      "authors": [
        "M.-J Han",
        "C.-H Lin",
        "K.-T Song"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "30",
      "title": "Automatically select emotion for response via personality-affected emotion transition",
      "authors": [
        "Z Wen",
        "J Cao",
        "R Yang",
        "S Liu",
        "J Shen"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "31",
      "title": "Fine-grained affective processing capabilities emerging from large language models",
      "authors": [
        "J Broekens",
        "B Hilpert",
        "S Verberne",
        "K Baraka",
        "P Gebhard",
        "A Plaat"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "32",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A wide evaluation of chatgpt on affective computing tasks"
    },
    {
      "citation_id": "33",
      "title": "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
      "authors": [
        "X Gu",
        "K Yoo",
        "J.-W Ha"
      ],
      "year": "2020",
      "venue": "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
      "arxiv": "arXiv:2012.01775"
    },
    {
      "citation_id": "34",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "35",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "36",
      "title": "So what do you propose we use instead? a reply to block",
      "authors": [
        "L Goldberg",
        "G Saucier"
      ],
      "year": "1995",
      "venue": "So what do you propose we use instead? a reply to block"
    },
    {
      "citation_id": "37",
      "title": "Television dialogue: The sitcom friends vs. natural conversation. amsterdam, nl",
      "authors": [
        "P Quaglio"
      ],
      "year": "2009",
      "venue": "Television dialogue: The sitcom friends vs. natural conversation. amsterdam, nl"
    },
    {
      "citation_id": "38",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "39",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "40",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "41",
      "title": "Ptuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "43",
      "title": "Validation of the five-factor model of personality across instruments and observers",
      "authors": [
        "R Mccrae",
        "P Costa"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "44",
      "title": "American psychologist",
      "authors": [
        "L Goldberg"
      ],
      "year": "1993",
      "venue": "The structure of phenotypic personality traits"
    },
    {
      "citation_id": "45",
      "title": "Personality judgments based on physical appearance",
      "authors": [
        "L Naumann",
        "S Vazire",
        "P Rentfrow",
        "S Gosling"
      ],
      "year": "2009",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "46",
      "title": "By face gesture validating the personality of a person",
      "authors": [
        "M Latha",
        "V Chaitanya",
        "M Devi"
      ],
      "venue": "By face gesture validating the personality of a person"
    },
    {
      "citation_id": "47",
      "title": "Personality in daily life: Multi-situational physiological signals reflect big-five personality traits",
      "authors": [
        "X Shui",
        "Y Chen",
        "X Hu",
        "F Wang",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    }
  ]
}