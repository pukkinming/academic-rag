{
  "paper_id": "2311.09378v1",
  "title": "What Predicts Interpersonal Affect? Preliminary Analyses From Retrospective Evaluations",
  "published": "2023-11-15T21:11:24Z",
  "authors": [
    "Maria Teresa Parreira",
    "Michael J. Sack",
    "Malte Jung"
  ],
  "keywords": [
    "affective computing",
    "annotation tool",
    "sentiment analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While the field of affective computing has contributed to greatly improving the seamlessness of human-robot interactions, the focus has primarily been on the emotional processing of the self, rather than the perception of the other. To address this gap, in a user study with 30 participant dyads, we collected the users' retrospective ratings of the interpersonal perception of the other interactant, after a short interaction. We made use of CORAE, a novel web-based open-source tool for COntinuous Retrospective Affect Evaluation. In this work, we analyze how these interpersonal ratings correlate with different aspects of the interaction, namely personality traits, participation balance, and sentiment analysis. Notably, we discovered that conversational imbalance has a significant effect on the retrospective ratings, among other findings. By employing these analyses and methodologies, we lay the groundwork for enhanced human-robot interactions, wherein affect is understood as a highly dynamic and a context-dependent outcome of interaction history.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Background",
      "text": "Research in human-robot interaction is often focused on measuring users' external outcomes (e.g., improve group task performance  [30] ) or internal individual states (e.g., sensing student engagement  [2] ). However, for robots to both understand and shape interactions among humans, they require an understanding of internal interpersonal states -that is, the perception of the other through the user's perspective  [19] .\n\nObservable behavior and subjective experience are highly dynamic  [20] . In interactions, these aspects co-evolve over time for all the interactants  [3] , interacting in ways that are yet to be fully explored. To study these dynamics, continuous representations of affective states have been popularized  [14, 23] , allowing for an understanding of how humans aggregate affect information across time and unveiling regions of \"emotional saliency\", which may be pivotal to assessing the emotional experience  [23] .\n\nA popular approach for collecting continuous affect data is retrospective analysis  [7, 8, 22, 21] , which relies on the phenomenon that individuals often re-experience emotions when reliving a situation  [11] . In prior work  [27] , we introduced CORAE, an intuitive tool for COntinuous Retrospective Affect Evaluation. This tool enables researchers to collect continuous affect data about interpersonal perceptions. Participants retrospectively rank how another interactant came across immediately following an interaction, thus allowing us to capture interpersonal affective perceptions rather than feelings or affective state inferences. In other words, our system allows us to capture data about how people perceive each other emotionally continuously over time.\n\nWhile affect data has been collected across subjective experience  [26, 9, 13] , and observable behavior  [18, 6] , we still lack continuous data about how affective perceptions of others develop dynamically over time. Prior work identified relational emotion metrics that take temporal patterns of emotional expressions in interactions into account and that focus on relationally relevant dimensions of emotion expression.  [17, 18] . However, these metrics often rely on manual coding of emotional expressions, while automated approaches for relationally relevant emotion metrics remain to be developed.\n\nWe ran an online user study where 30 participant dyads interacted while completing a task. Following their discussion, participants retrospectively annotated the interaction, evaluating how the other participant came across. In the present work, we explored how different interaction variables -personality of the interactants, conversational balance, emotional content of participants' speech -affected how users perceived each other.\n\nOur perceptions of robots are yet to be fully understood  [29] , but authors have recognized the value of robotic systems that account for the dynamic nature of affect  [5] . This prelimi-arXiv:2311.09378v1 [cs.HC] 15 Nov 2023 nary analysis sheds light on aspects of interactions that impact our interpersonal states, with the potential to better inform the development of these systems.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Corae",
      "text": "We provide below a brief description of CORAE, to facilitate the understanding of the interpersonal perception data used in this study. The CORAE platform (related to the Latin word for \"heart\")  [27]  enables individuals to intuitively evaluate how a person's behavior is interpreted emotionally during interactions. It is an open-source tool that can be found in corae.org. Design: CORAE is intuitive and visually minimal (see Figure  1 ). The central focus is a video of the other interactant. Brief instructions above the video player describe the controls of the annotation dashboard (Spacebar to toggle playback and Left and Right Arrows to control the slider), as well as a brief description of the terms used for measuring interpersonal perception, which can be personalized to the platform's use case.\n\nA progress bar is displayed below the video player to inform participants what proportion remains of their evaluation. Finally, below the video player is displayed the annotation slider. The annotation bar is bounded and discretized (a total of 15 points, from -7 -Disagreeable -to +7 -Agreeable). Participants may only change their rating during video playback and are constrained by the platform to do it \"continuously\" (i.e., they cannot instantaneously change the rating from Neutral (0) to Agreeable  (7) , but rather adjust to each value in sequence). Data Logging: Data is logged for a session in two ways: (1) by default, the mode for data logging is set to predetermined intervals of one second; and (2) whenever a change in the rating occurs. Associated data points are the slider position (rating), time code, and video frame (in the format \"Slider-NumericalPosition\": \"Hours:Minutes:Seconds:VideoFrame\"), which are logged in a JSON file.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. User Study",
      "text": "We carried out a user study to collect dyadic interaction data and retrospective interpersonal ratings through CORAE. A more detailed description can be found in Sack et al.  [27] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Procedure",
      "text": "Participants were recruited through Prolific 1  . Before scheduling their slot, each participant read and signed a consent form. The study took place fully online, with interactions facilitated through Zencastr 2  , a video call platform that allows for the recording of each video and audio stream separately. Participants read task instructions, including a description of the discussion topic (Reasons for Poverty task  [28] , detailed below). After this, participants were recorded while interacting to solve the task. When they reached an agreement, or after 10 minutes of discussion, participants were asked to stop discussing and fill out a survey. This survey collected demographic data, as well as measures of interpersonal affect. Each participant was then distributed a URL that opened an instance of CORAE's annotation platform in their browser. Participants were each presented with a video of their discussion partner, and the audio of both, and were asked to continuously rate how their partner came across, moment-by-moment. Once finished, participants completed an exit survey with open comments. Participants were compensated for their participation with US$14, through Prolific. Reasons for Poverty task: We used a modified version of the Reasons for Poverty task  [28]  as a discussion prompt. The task requires participants to agree on selecting 5 items from a list of \"reasons for poverty\", and rank them according to their \"accuracy\". Some options in the 10-item list include \"Poor people lack the ability to manage money.\", or \"The society lacks justice\". We aimed to elicit an emotionally engaging interaction.\n\nParticipants were given a maximum of 10 minutes to discuss, to prevent individuals from getting disengaged when reviewing their discussion on CORAE.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Research Questions",
      "text": "In this preliminary analysis, we wanted to investigate if aspects of the users' identity, or conversational aspects such as the balance of the participation of the users were impactful for how participants perceived each other. We focused on the following research questions:\n\n• RQ1: Do personality traits or demographic aspects (age, gender) of a user impact how they come across to the other interactant? • RQ2: Do dynamics of the user's conversation (imbalance, duration of conversational turns) impact the users' interpersonal perceptions of each other? • RQ3: Does the emotional content of the user's speech impact how they come across to the other interactant?",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Measures",
      "text": "To answer the research questions laid out above, we defined a set of measures from the data collected during the interaction sessions. Interpersonal measures: We evaluated the perception of the other through two measures. The Interpersonal Agreeableness measure was operationalized by asking participants \"How did the other participant come across?\" on a 7-point Likert scale (from disagreeable to agreeable) in the post-interaction survey. Additionally, Interpersonal Perception (IP) was extracted from the continuous interpersonal rating data collected via CORAE. To calculate the IP, and in line with prior work  [18, 12] , for each participant, we took the cumulative sum of the ratings during the interaction and fitted a linear regression to that data. The Interpersonal Perception measure is given by the slope of that regression, providing an understanding of how the perception of the other interactant evolved over the interaction (see Figure  2 ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Demographic And Personality Measures:",
      "text": "In the postinteraction survey, we collected demographic information (age, gender, nationality, race/ethnicity) and personality traits through the short-version of the Big Five Inventory  [25] . Participants were also asked to rate their religiousness (not-at-all religious to very religious) and political leaning (very liberal to very conservative) with 7-point Likert scales. Conversational Balance measures: To assess RQ2, we measured the users' total number and average For duration of conversational turns, as well as participation imbalance  [30, 10] , which is given by:\n\nwhere s i is the amount of time that participant i has spoken over the total amount of speech. s is the mean of the relative speech time of the two participants, which in the case of a dyadic interaction is 0.5. This measures how much the participation of the users deviated from a perfectly balanced interaction. These measures were collected from Voice Activation Detection (VAD). Speech emotion content measures: We transcribed the speech of the participants using the speech-to-text model Whisper  [24] . Also through Whisper, we analyzed the emotional \"tone\" of each utterance of the participants in the dataset (either neutral, positive or negative). Because the task contained prompts that were negative, we extracted only the positive emotional tone ratio, i.e. the ratio of utterances that were evaluated as positive (over total number of utterances). Additionally, we collected the total number of agreement words within the speech of each user, to obtain a measure of affirmed agreement from each participant.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Participants",
      "text": "To potentially elicit disagreement during the interactions, participants were selected according to their political leaning (one conservative-and one liberal-leaning). Other recruitment criteria were proficiency in English and a computer device with a functioning camera and microphone.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "We collected data from 30 interaction sessions. Due to connection problems or data collection issues, we only used data from 27 interaction sessions (54 participants, a total of 3663 transcribed utterances). Participants' age ranged from 20 -87 years (M ± SD : 42.33 ± 15.78). Out of the 54 participants, 28 identified as female, 25 as male, one as non-binary. Race/ethnicity was mostly Caucasian/White (41), followed by Asian/Asian American (5), Hispanic/Latino (5), African/African American/Black (3), Middle Eastern/North African (1) and American Indian (1) (participants could select multiple). Most participants were native speakers of English (49), with 5 proficient users.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Personality And Demographics",
      "text": "To investigate RQ1, we evaluated whether personality or demographic traits impact how users are perceived with an ANCOVA. We examined the effects of each personality trait (agreeableness, conscientiousness, extroversion, neuroticism, openness) on both Interpersonal Perception (IP) and Interpersonal Agreeableness (IA), while controlling for gender and age. The effect of agreeableness on IP (the slope of the cumulative ratings curve, as evaluated by each discussion partner) was significant (F (1, 53) = 6.67, p = 0.01). Agreeableness was also a predictor of IA (how the participant came across during an interaction, reported via survey by their discussion partner), F (1, 53) = 4.48, p = 0.04, as well as neuroticism (F (1, 53) = 4.65, p = 0.04).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Conversational Balance",
      "text": "We considered how aspects of conversational balance can impact differences in the perception of the interaction. For this, we considered whether the difference in the IP values for the dyad (|IP 1 -IP 2 |), as well as the IA value difference, were impacted by the imbalance in the conversation. We also evaluated the effects of the difference in the total number and average length of turns for both participants. An ANCOVA revealed that conversational imbalance is a predictor of the IP difference (F (1, 26) = 4.79, p = 0.04). The interaction between these two variables can be seen in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Speech Emotional Content",
      "text": "Finally, to answer RQ3, we evaluated whether the content of the speech from each participant could impact how users are perceived. We ran an ANCOVA to evaluate the effects of the number of agreement words, as well as the positive emotional tone ratio, on IP and IA. Our results show that the ratio of positive tone utterances (in relation to all utterances from that participant) has a significant effect on the IP value (F (1, 54) = 8.55, p = 0.005).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Discussion",
      "text": "Our study contributes to the broader understanding of affective computing by emphasizing the significance of the perception of the other in shaping affective experiences. It expands the existing knowledge by highlighting the need to consider social distance as a key factor in human-robot interactions  [19, 4, 1] . By incorporating such considerations into the design and development of affective computing systems, we can create more empathetic and effective interactions between humans and robots.\n\nWe investigated if the personality traits of users impact how they are perceived in an interaction, to answer RQ1. Indeed, we found that the personality trait of agreeableness has an effect on how the participant is rated by their discussion partner, both on the \"static\" measure of the interpersonal dimension (Interpersonal Agreement), and on the continuous retrospective rating of the interaction (Interpersonal Perception). This indicates that the brief discussion task was sufficient to grant an alignment between the perception of the self and of the other, in spite of some literature reporting that the two may conflict  [15] .\n\nIn addition to the personality of each participant, we investigated if conversational dynamics impacted how users rated each other. To answer RQ2, we found that the imbalance in the speech time of each participant has an effect on how aligned their perceptions of each other are. We found that more imbalanced conversations lead to higher differences in the IP values of the participants. Further analyses are needed to understand how exactly the interpersonal perceptions get distorted by the amount of participation in the discussion.\n\nFinally, we answered RQ3 by investigating the emotional content of the speech of each participant. Interestingly, our results indicate that the ratio of utterances with positive emotional content impacts how the participant is rated by the other user. Agreeableness is associated with the expression of more positive emotions  [31] , which may explain why participants came across as generally more agreeable.\n\nIn addition to improving our knowledge of interpersonal affect, our study and follow-up work can also contribute to bettering human-robot interactions -agreeableness of users has been found to play a role in rapport building with robots  [16] . By computing affect as a highly dynamic and contextdependent phenomenon, we can develop strategies to improve the perceived social distance between humans and robots. Understanding how different aspects of the interaction influence affective perceptions allows for the design of more seamless and responsive systems, fostering a sense of connection and engagement.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Annotation dashboard for CORAE. Participants ret-",
      "page": 1
    },
    {
      "caption": "Figure 2: Dynamics of interpersonal ratings (IR) in one session",
      "page": 3
    },
    {
      "caption": "Figure 3: Conversational Imbalance impacts the alignment be-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cornell University": "Abstract—While\nthe\nfield\nof\naffective\ncomputing\nhas\ncon-"
        },
        {
          "Cornell University": "tributed to greatly improving the seamlessness of human-robot"
        },
        {
          "Cornell University": "interactions,\nthe\nfocus\nhas\nprimarily\nbeen\non\nthe\nemotional"
        },
        {
          "Cornell University": "processing of\nthe\nself,\nrather\nthan the perception of\nthe other."
        },
        {
          "Cornell University": "To address\nthis gap,\nin a user study with 30 participant dyads,"
        },
        {
          "Cornell University": "we collected the users’ retrospective ratings of\nthe interpersonal"
        },
        {
          "Cornell University": "perception of\nthe\nother\ninteractant,\nafter\na\nshort\ninteraction."
        },
        {
          "Cornell University": "We made use of CORAE, a novel web-based open-source\ntool"
        },
        {
          "Cornell University": "for COntinuous Retrospective Affect Evaluation. In this work, we"
        },
        {
          "Cornell University": "analyze how these interpersonal ratings correlate with different"
        },
        {
          "Cornell University": "aspects of the interaction, namely personality traits, participation"
        },
        {
          "Cornell University": "balance, and sentiment analysis. Notably, we discovered that con-"
        },
        {
          "Cornell University": "versational\nimbalance has a significant effect on the retrospective"
        },
        {
          "Cornell University": "ratings, among other findings. By employing these analyses and"
        },
        {
          "Cornell University": "methodologies, we lay the groundwork for enhanced human-robot"
        },
        {
          "Cornell University": "interactions, wherein affect\nis understood as a highly dynamic"
        },
        {
          "Cornell University": "and a context-dependent outcome of\ninteraction history."
        },
        {
          "Cornell University": "Index Terms—affective computing, annotation tool,\nsentiment"
        },
        {
          "Cornell University": "analysis"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "and a context-dependent outcome of\ninteraction history."
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "Index Terms—affective computing, annotation tool,\nsentiment"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "analysis"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "I. BACKGROUND"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "Research in human-robot\ninteraction is often focused on"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "measuring users’ external outcomes (e.g.,\nimprove group task"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "performance [30]) or\ninternal\nindividual\nstates\n(e.g.,\nsensing"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "student engagement\n[2]). However,\nfor\nrobots\nto both under-"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "stand and shape interactions among humans,\nthey require an"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "understanding\nof\ninternal\ninterpersonal\nstates\n–\nthat\nis,\nthe"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "perception of\nthe other\nthrough the user’s perspective [19]."
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "Observable behavior and subjective experience are highly"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "dynamic\n[20].\nIn\ninteractions,\nthese\naspects\nco-evolve\nover"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "time for all the interactants [3], interacting in ways that are yet"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "to be fully explored. To study these dynamics, continuous rep-"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "resentations of affective states have been popularized [14, 23],"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "allowing for an understanding of how humans aggregate affect"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "information across\ntime and unveiling regions of “emotional"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "saliency”, which may be pivotal\nto assessing the\nemotional"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": ""
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "experience [23]."
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "A popular\napproach for\ncollecting continuous\naffect data"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "is\nretrospective\nanalysis\n[7, 8, 22, 21], which relies on the"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "phenomenon\nthat\nindividuals\noften\nre-experience\nemotions"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "when reliving a situation [11].\nIn prior work [27], we intro-"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "duced CORAE, an intuitive tool for COntinuous Retrospective"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "Affect Evaluation. This\ntool\nenables\nresearchers\nto\ncollect"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "continuous\naffect data\nabout\ninterpersonal perceptions. Par-"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "ticipants\nretrospectively\nrank\nhow another\ninteractant\ncame"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "across\nimmediately\nfollowing\nan\ninteraction,\nthus\nallowing"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "us\nto capture\ninterpersonal\naffective perceptions\nrather\nthan"
        },
        {
          "interactions, wherein affect\nis understood as a highly dynamic": "feelings\nor\naffective\nstate\ninferences.\nIn\nother words,\nour"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "our interpersonal states, with the potential\nto better inform the",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "discussing and fill out a survey. This survey collected demo-"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "development of\nthese systems.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "graphic data, as well as measures of interpersonal affect. Each"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "participant was then distributed a URL that opened an instance"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "II. CORAE",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "of CORAE’s annotation platform in their browser. Participants"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "We provide below a brief description of CORAE,\nto facili-",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "were each presented with a video of\ntheir discussion partner,"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "tate the understanding of the interpersonal perception data used",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "and the audio of both, and were asked to continuously rate how"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "in this study. The CORAE platform (related to the Latin word",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "their partner came across, moment-by-moment. Once finished,"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "for\n“heart”)\n[27]\nenables\nindividuals\nto\nintuitively\nevaluate",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "participants\ncompleted an exit\nsurvey with open comments."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "how a\nperson’s\nbehavior\nis\ninterpreted\nemotionally\nduring",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "Participants were\ncompensated\nfor\ntheir\nparticipation with"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "interactions.\nIt\nis\nan open-source\ntool\nthat\ncan be\nfound in",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "US$14,\nthrough Prolific."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "corae.org.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "Reasons for Poverty task: We used a modified version of"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Design: CORAE is\nintuitive and visually minimal\n(see Fig-",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "the Reasons for Poverty task [28] as a discussion prompt. The"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "ure 1). The central\nfocus\nis a video of\nthe other\ninteractant.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "task requires participants to agree on selecting 5 items from a"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Brief instructions above the video player describe the controls",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "list of “reasons for poverty”, and rank them according to their"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "of the annotation dashboard (Spacebar to toggle playback and",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "“accuracy”. Some options\nin the 10-item list\ninclude “Poor"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Left\nand Right Arrows\nto\ncontrol\nthe\nslider),\nas well\nas\na",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "people\nlack\nthe ability\nsociety\nto manage money.”, or “The"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "brief description of the terms used for measuring interpersonal",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "lacks\njustice”. We\naimed to elicit\nan emotionally engaging"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "perception, which can be personalized to the platform’s use",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "interaction."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "case.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "Participants were given a maximum of 10 minutes to discuss,"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "A progress\nbar\nis\ndisplayed\nbelow the\nvideo\nplayer\nto\nin-",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "to prevent individuals from getting disengaged when reviewing"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "form participants what proportion remains of their evaluation.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "their discussion on CORAE."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Finally, below the video player\nis displayed the\nannotation",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "B. Research Questions"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "slider.\nThe\nannotation\nbar\nis\nbounded\nand\ndiscretized\n(a",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "In\nthis\npreliminary\nanalysis, we wanted\nto\ninvestigate\nif"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "total of 15 points,\nfrom −7 — Disagreeable — to +7 —",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "aspects of\nthe users’\nidentity, or conversational aspects\nsuch"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Agreeable). Participants may only change their\nrating during",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "as the balance of the participation of the users were impactful"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "video playback and are constrained by the platform to do it",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "for how participants perceived each other. We focused on the"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "“continuously”\n(i.e.,\nthey cannot\ninstantaneously change\nthe",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "following research questions:"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "rating from Neutral\n(0)\nto Agreeable (7), but\nrather adjust\nto",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "• RQ1: Do personality traits or demographic aspects (age,"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "each value in sequence).",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "gender) of a user\nimpact how they come across\nto the"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Data Logging: Data is logged for a session in two ways: (1)",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "other\ninteractant?"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "by default,\nthe mode for data logging is set\nto predetermined",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "• RQ2: Do dynamics of\nthe user’s\nconversation (imbal-"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "intervals of one\nsecond;\nand (2) whenever\na\nchange\nin the",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "ance, duration of conversational\nturns)\nimpact\nthe users’"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "rating occurs. Associated data points\nare\nthe\nslider position",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "interpersonal perceptions of each other?"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "(rating),\ntime code, and video frame (in the format “Slider-",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "• RQ3: Does\nthe emotional content of\nthe user’s\nspeech"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "NumericalPosition”: “Hours:Minutes:Seconds:VideoFrame”),",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "impact how they come across to the other\ninteractant?"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "which are logged in a JSON file.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "C. Measures"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "III. USER STUDY",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "To answer the research questions laid out above, we defined"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "We\ncarried out\na user\nstudy to collect dyadic\ninteraction",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "a set of measures from the data collected during the interaction"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "data and retrospective interpersonal\nratings through CORAE.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "sessions."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "A more detailed description can be found in Sack et al.\n[27].",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "Interpersonal measures: We evaluated the perception of\nthe"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "A. Experimental Procedure",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "other through two measures. The Interpersonal Agreeableness"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "measure was operationalized by asking participants “How did"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Participants\nwere\nrecruited\nthrough\nProlific1.\nBefore",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "the other participant come across?” on a 7-point Likert scale"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "scheduling their slot, each participant\nread and signed a con-",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "(from disagreeable to agreeable) in the post-interaction survey."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "sent form. The study took place fully online, with interactions",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "Additionally, Interpersonal Perception (IP) was extracted from"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "facilitated through Zencastr2, a video call platform that allows",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "the continuous interpersonal rating data collected via CORAE."
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "for\nthe recording of each video and audio stream separately.",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "To calculate the IP, and in line with prior work[18, 12],\nfor"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "Participants\nread task instructions,\nincluding a description of",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "each participant, we took the cumulative sum of\nthe ratings"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "the discussion topic (Reasons\nfor Poverty task [28], detailed",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "during\nthe\ninteraction\nand fitted\na\nlinear\nregression\nto that"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "below). After this, participants were recorded while interacting",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "data. The\nInterpersonal Perception measure\nis given by the"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "to solve the task. When they reached an agreement, or after",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": ""
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "slope of that regression, providing an understanding of how the"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "1https://www.prolific.co/",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "perception of the other interactant evolved over the interaction"
        },
        {
          "nary analysis sheds light on aspects of interactions that\nimpact": "2https://zencastr.com/",
          "10 minutes\nof\ndiscussion,\nparticipants were\nasked\nto\nstop": "(see Figure 2)."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "were evaluated as positive (over\ntotal number of utterances)."
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "agreement\nAdditionally, we\ncollected\nthe\ntotal\nnumber\nof"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "words within the speech of each user,\nto obtain a measure of"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "affirmed agreement\nfrom each participant."
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "D. Participants"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "To potentially elicit disagreement during the\ninteractions,"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "participants were selected according to their political\nleaning"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "(one conservative- and one liberal-leaning). Other recruitment"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "criteria were proficiency in English and a\ncomputer device"
        },
        {
          "positive emotional\ntone ratio,\ni.e.\nthe ratio of utterances\nthat": "with a functioning camera and microphone."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "connection problems or data collection issues, we only used"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "data from 27 interaction sessions\n(54 participants, a total of"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "3663\ntranscribed\nutterances). Participants’\nage\nranged\nfrom"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "20 − 87\nyears\n(M ± SD :\n42.33 ± 15.78). Out\nof\nthe"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "54 participants, 28 identified as\nfemale, 25 as male, one as"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "non-binary. Race/ethnicity was mostly Caucasian/White (41),"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "followed by Asian/Asian American (5), Hispanic/Latino (5),"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "African/African American/Black\n(3), Middle Eastern/North"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "African (1) and American Indian (1) (participants could select"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "multiple). Most participants were native speakers of English"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "(49), with 5 proficient users."
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "A. Personality and demographics"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "To investigate RQ1, we\nevaluated whether personality or"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "demographic\ntraits\nimpact how users\nare perceived with an"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "ANCOVA. We examined the effects of each personality trait"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": ""
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "(agreeableness,\nconscientiousness,\nextroversion, neuroticism,"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "openness) on both Interpersonal Perception (IP) and Inter-"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "personal Agreeableness\n(IA), while controlling for gender"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "agreeableness"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "and\nage. The\neffect\nof\non\nIP (the\nslope\nof"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "the\ncumulative\nratings\ncurve,\nas\nevaluated\nby\neach\ndiscus-"
        },
        {
          "We\ncollected\ndata\nfrom 30\ninteraction\nsessions. Due\nto": "sion partner) was\nsignificant\n(F (1, 53) = 6.67, p = 0.01)."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "ratio of positive tone utterances\n(in relation to all utterances"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "from that participant) has a significant effect on the IP value"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "(F (1, 54) = 8.55, p = 0.005)."
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "V. DISCUSSION"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "Our\nstudy\ncontributes\nto\nthe\nbroader\nunderstanding\nof"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "affective\ncomputing by emphasizing the\nsignificance of\nthe"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "perception of\nthe other\nin shaping affective\nexperiences.\nIt"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "expands\nthe existing knowledge by highlighting the need to"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "consider social distance as a key factor in human-robot interac-"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "tions [19, 4, 1]. By incorporating such considerations into the"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "design and development of affective computing systems, we"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "can create more empathetic and effective interactions between"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "humans and robots."
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "We investigated if the personality traits of users impact how"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "they are perceived in an interaction, to answer RQ1. Indeed, we"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "found that\nthe personality trait of agreeableness has an effect"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "on how the participant\nis\nrated by their discussion partner,"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "both on the “static” measure of\nthe interpersonal dimension"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "(Interpersonal Agreement), and on the continuous\nretrospec-"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "tive rating of\nthe interaction (Interpersonal Perception). This"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "indicates that\nthe brief discussion task was sufficient\nto grant"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "an alignment between the perception of\nthe\nself\nand of\nthe"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "other,\nin spite of\nsome literature reporting that\nthe two may"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "conflict\n[15]."
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "In addition to the personality of each participant, we inves-"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "tigated if conversational dynamics\nimpacted how users\nrated"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "each other. To answer RQ2, we found that\nthe imbalance in"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": ""
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "the\nspeech\ntime\nof\neach\nparticipant\nhas\nan\neffect\non\nhow"
        },
        {
          "emotional tone ratio, on IP and IA. Our results show that the": "aligned their perceptions of\neach other\nare. We\nfound that"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "more imbalanced conversations\nlead to higher differences\nin": "the IP values of\nthe participants. Further analyses are needed"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "to understand how exactly the\ninterpersonal perceptions get"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "distorted by the amount of participation in the discussion."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "Finally, we answered RQ3 by investigating the emotional"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "content of\nthe\nspeech of\neach participant.\nInterestingly, our"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "results indicate that\nthe ratio of utterances with positive emo-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "tional content impacts how the participant is rated by the other"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "user. Agreeableness is associated with the expression of more"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "positive emotions\n[31], which may explain why participants"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "came across as generally more agreeable."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "In addition to improving our knowledge of\ninterpersonal"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "affect, our\nstudy and follow-up work can also contribute to"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "bettering human-robot\ninteractions – agreeableness of users"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "has been found to play a role in rapport building with robots"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[16]. By computing affect as a highly dynamic and context-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "dependent phenomenon, we can develop strategies to improve"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "the perceived social distance between humans and robots. Un-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "derstanding how different aspects of\nthe interaction influence"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "affective perceptions allows\nfor\nthe design of more seamless"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "and responsive systems,\nfostering a sense of connection and"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "engagement."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "ACKNOWLEDGMENT"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "We\nthank Nawid Jamali\nand Hifza\nJaved for\nthe\ncollab-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "oration on CORAE and user\nstudy design. This work was"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "supported by Honda Research Institute USA,\nInc.."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "REFERENCES"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[1]\nP. A. Andersen\nand L. K. Guerrero.\nPrinciples\nof\ncommu-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "nication and emotion in social\ninteraction.\nIn Handbook of"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "communication and emotion, pages 49–96. Elsevier, 1996."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[2] M.-L. Bourguet, Y.\nJin, Y. Shi, Y. Chen, L. Rincon-Ardila,"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "and G. Venture.\nSocial\nrobots\nthat\ncan\nsense\nand\nimprove"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "student engagement.\nIn 2020 IEEE International Conference"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "on Teaching, Assessment, and Learning for Engineering (TALE),"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "pages 127–134, 2020. doi: 10.1109/TALE48869.2020.9368438."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[3] E. A. Butler and A. K. Randall. Emotional coregulation in close"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "relationships. Emotion Review, 5(2):202–210, 2013."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[4] C. Caffi and R. W.\nJanney.\nToward a pragmatics of emotive"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "communication. Journal of pragmatics, 22(3-4):325–373, 1994."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[5] N. Churamani, S. Kalkan, and H. Gunes.\nContinual\nlearning"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "2020\n29th\nfor\naffective\nrobotics: Why, what\nand\nhow?\nIn"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "IEEE International Conference\non Robot\nand Human\nInter-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "active Communication (RO-MAN), pages 425–431, 2020.\ndoi:"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "10.1109/RO-MAN47096.2020.9223564."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[6]\nJ. A. Coan and J. M. Gottman. The specific affect coding system"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "(spaff). Handbook of emotion elicitation and assessment, 267:"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "285, 2007."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[7] R. Cowie,\nE. Douglas-Cowie,\nS.\nSavvidou,\nE. McMahon,"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "M. Sawey,\nand M. Schr¨oder.\n’feeltrace’: An instrument\nfor"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "recording perceived emotion in real\ntime. 01 2000."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[8] R. Cowie, M.\nSawey, C. Doherty,\nJ.\nJaimovich, C.\nFyans,"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "and P. Stapleton.\nGtrace: General\ntrace program compatible"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "with emotionml.\nIn 2013 Humaine Association Conference on"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "Affective Computing and Intelligent Interaction, pages 709–710,"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "2013. doi: 10.1109/ACII.2013.126."
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "[9] M. Csikszentmihalyi, M. Csikszentmihalyi, and R. Larson. Va-"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": ""
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "lidity and reliability of\nthe experience-sampling method. Flow"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "and the foundations of positive psychology: The collected works"
        },
        {
          "more imbalanced conversations\nlead to higher differences\nin": "of Mihaly Csikszentmihalyi, pages 35–54, 2014."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "I. Leite.\nRobot gaze\ncan mediate participation imbalance\nin",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Current\n[20] P. Kuppens\nand\nP. Verduyn.\nEmotion\ndynamics."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "the 2021\ngroups with different\nskill\nlevels.\nIn Proceedings of",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Opinion in Psychology, 17:22–26, 2017."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "ACM/IEEE International Conference on Human-Robot\nInter-",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[21] P. Lopes, G. N. Yannakakis, and A. Liapis. Ranktrace: Relative"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "action, HRI\n’21, page 303–311, New York, NY, USA, 2021.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "and unbounded affect annotation.\nIn 2017 Seventh International"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Association for Computing Machinery.\nISBN 9781450382892.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Conference on Affective Computing and Intelligent\nInteraction"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "doi: 10.1145/3434073.3444670.\nURL https://doi.org/10.1145/",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "(ACII), pages 158–163, 2017. doi: 10.1109/ACII.2017.8273594."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "3434073.3444670.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[22] D. Melhart, A. Liapis,\nand G. N. Yannakakis.\nPagan: Video"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[11]\nJ. M. Gottman and R. W. Levenson.\nA valid procedure\nfor",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "affect\nannotation made\neasy.\npages 130–136.\nIEEE, 9 2019."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "obtaining self-report of affect\nin marital\ninteraction. Journal of",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "ISBN 978-1-7281-3888-6.\ndoi:\n10.1109/ACII.2019.8925434."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "consulting and clinical psychology, 53(2):151, 1985.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "URL https://ieeexplore.ieee.org/document/8925434/."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[12]\nJ. M. Gottman and R. W. Levenson. Marital processes predictive",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[23] A. Metallinou and S. Narayanan. Annotation and processing of"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Journal\nof\nlater dissolution: behavior, physiology, and health.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "continuous emotional attributes: Challenges and opportunities."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "of personality and social psychology, 63(2):221, 1992.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "In 2013 10th IEEE International Conference and Workshops"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[13]\nJ. M. Gottman and R. W. Levenson.\nThe timing of divorce:",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "on Automatic Face and Gesture Recognition (FG), pages 1–8,"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Predicting when a couple will divorce over a 14-year period.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "2013. doi: 10.1109/FG.2013.6553804."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Journal of Marriage and Family, 62(3):737–745, 2000.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[14] H. Gunes\nand B. Schuller.\nCategorical\nand dimensional\naf-",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "I. Sutskever.\nRobust\nspeech recognition via large-scale weak"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "fect\nanalysis\nin\ncontinuous\ninput: Current\ntrends\nand\nfuture",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "supervision, 2022."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "directions.\nImage and Vision Computing, 31(2):120–136, 2013.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[25] B. Rammstedt and O. P.\nJohn. Measuring personality in one"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "ISSN 0262-8856.\ndoi:\nhttps://doi.org/10.1016/j.imavis.2012.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "minute or less: A 10-item short version of the big five inventory"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "06.016. URL https://www.sciencedirect.com/science/article/pii/",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "in english and german.\nJournal of Research in Personality,"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "S0262885612001084. Affect Analysis In Continuous Input.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "41(1):203–212, 2007.\nISSN 0092-6566.\ndoi: https://doi.org/"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[15] L. G. Herringer\nand S. C. Haws.\nPerception of personality",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "10.1016/j.jrp.2006.02.001. URL https://www.sciencedirect.com/"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "traits\nin oneself and others.\nThe Journal of Psychology, 125",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "science/article/pii/S0092656606000195."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "(1):33–43, 1991. doi: 10.1080/00223980.1991.10543267. URL",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[26] A. M. Ruef\nand R. W. Levenson.\nContinuous measurement"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "https://doi.org/10.1080/00223980.1991.10543267.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "of emotion. Handbook of emotion elicitation and assessment,"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[16]\nS.\nJeong,\nL. Aymerich-Franch, K. Arias,\nS. Alghowinem,",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "pages 286–297, 2007."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "A. Lapedriza, R. Picard, H. W. Park, and C. Breazeal. Deploying",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[27] M.\nJ.\nSack, M.\nT.\nParreira,\nJ.\nFu, A.\nLipman, H.\nJaved,"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "a robotic positive psychology coach to improve college students’",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "N.\nJamali,\nand M.\nJung.\nCorae: A tool\nfor\nintuitive\nand"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "User Modeling and User-Adapted\npsychological well-being.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "continuous retrospective evaluation of\ninteractions. 2023."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Interaction, 33(2):571–615, Apr 2023.\nISSN 1573-1391.\ndoi:",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[28] D. T. L. Shek. Chinese adolescent’ explanations of poverty:\nthe"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "10.1007/s11257-022-09337-8.\nURL\nhttps://doi.org/10.1007/",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "perceived causes of poverty scale.\nAdolescence, 37:789–804,"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "s11257-022-09337-8.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "12 2002.\nISSN 00018449."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[17] M. Jung, J. Chong, and L. Leifer. Group hedonic balance and",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[29] R. Stock-Homburg. Survey of emotions in human–robot\ninter-"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "pair programming performance: Affective interaction dynamics",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "actions: Perspectives\nfrom robotic psychology on 20 years of"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "the SIGCHI\nas\nindicators of performance.\nIn Proceedings of",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "research.\nInternational Journal of Social Robotics, 14, 06 2021."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Conference\non Human Factors\nin Computing\nSystems, CHI",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "doi: 10.1007/s12369-021-00778-6."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "’12, page 829–838, New York, NY, USA, 2012. Association",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[30] H. Tennent, S. Shen, and M. Jung. Micbot: A Peripheral Robotic"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "for Computing Machinery.\nISBN 9781450310154.\ndoi: 10.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Object\nto Shape Conversational Dynamics\nand Team Perfor-"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "1145/2207676.2208523. URL https://doi.org/10.1145/2207676.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "mance. ACM/IEEE International Conference on Human-Robot"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "2208523.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Interaction, 2019-March:133–142, 2019.\nISSN 21672148. doi:"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[18] M. F. Jung. Coupling interactions and performance: Predicting",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "10.1109/HRI.2019.8673013."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "ACM Trans.\nteam performance\nfrom thin slices of\nconflict.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "[31]\nS. Whittaker, Y. Rogers, E. Petrovskaya, and H. Zhuang. De-"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Comput.-Hum. Interact., 23(3), jun 2016. ISSN 1073-0516. doi:",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "signing personas for expressive robots: Personality in the new"
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "10.1145/2753767. URL https://doi.org/10.1145/2753767.",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "breed of moving, speaking, and colorful social home robots. J."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "Hum.-Robot\nInteract., 10(1),\nfeb 2021.\ndoi: 10.1145/3424153."
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "[19] M. F. Jung. Affective grounding in human-robot\ninteraction.\nIn",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": ""
        },
        {
          "[10]\nS. Gillet, R. Cumbal, A. Pereira,\nJ. Lopes, O. Engwall,\nand": "Proceedings of\nthe 2017 ACM/IEEE International Conference",
          "on Human-Robot\nInteraction, pages 263–273, 2017.": "URL https://doi.org/10.1145/3424153."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Principles of communication and emotion in social interaction",
      "authors": [
        "P Andersen",
        "L Guerrero"
      ],
      "year": "1996",
      "venue": "Handbook of communication and emotion"
    },
    {
      "citation_id": "2",
      "title": "Social robots that can sense and improve student engagement",
      "authors": [
        "M.-L Bourguet",
        "Y Jin",
        "Y Shi",
        "Y Chen",
        "L Rincon-Ardila",
        "G Venture"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)",
      "doi": "10.1109/TALE48869.2020.9368438"
    },
    {
      "citation_id": "3",
      "title": "Emotional coregulation in close relationships",
      "authors": [
        "E Butler",
        "A Randall"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "4",
      "title": "Toward a pragmatics of emotive communication",
      "authors": [
        "C Caffi",
        "R Janney"
      ],
      "year": "1994",
      "venue": "Journal of pragmatics"
    },
    {
      "citation_id": "5",
      "title": "Continual learning for affective robotics: Why, what and how",
      "authors": [
        "N Churamani",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
      "doi": "10.1109/RO-MAN47096.2020.9223564"
    },
    {
      "citation_id": "6",
      "title": "The specific affect coding system (spaff). Handbook of emotion elicitation and assessment",
      "authors": [
        "J Coan",
        "J Gottman"
      ],
      "year": "2007",
      "venue": "The specific affect coding system (spaff). Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "7",
      "title": "'feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "'feeltrace': An instrument for recording perceived emotion in real time"
    },
    {
      "citation_id": "8",
      "title": "Gtrace: General trace program compatible with emotionml",
      "authors": [
        "R Cowie",
        "M Sawey",
        "C Doherty",
        "J Jaimovich",
        "C Fyans",
        "P Stapleton"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2013.126"
    },
    {
      "citation_id": "9",
      "title": "Validity and reliability of the experience-sampling method. Flow and the foundations of positive psychology: The collected works of Mihaly Csikszentmihalyi",
      "authors": [
        "M Csikszentmihalyi",
        "M Csikszentmihalyi",
        "R Larson"
      ],
      "year": "2014",
      "venue": "Validity and reliability of the experience-sampling method. Flow and the foundations of positive psychology: The collected works of Mihaly Csikszentmihalyi"
    },
    {
      "citation_id": "10",
      "title": "Robot gaze can mediate participation imbalance in groups with different skill levels",
      "authors": [
        "S Gillet",
        "R Cumbal",
        "A Pereira",
        "J Lopes",
        "O Engwall",
        "I Leite"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction, HRI '21",
      "doi": "10.1145/3434073.3444670"
    },
    {
      "citation_id": "11",
      "title": "A valid procedure for obtaining self-report of affect in marital interaction",
      "authors": [
        "J Gottman",
        "R Levenson"
      ],
      "year": "1985",
      "venue": "Journal of consulting and clinical psychology"
    },
    {
      "citation_id": "12",
      "title": "Marital processes predictive of later dissolution: behavior, physiology, and health",
      "authors": [
        "J Gottman",
        "R Levenson"
      ],
      "year": "1992",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "13",
      "title": "The timing of divorce: Predicting when a couple will divorce over a 14-year period",
      "authors": [
        "J Gottman",
        "R Levenson"
      ],
      "year": "2000",
      "venue": "Journal of Marriage and Family"
    },
    {
      "citation_id": "14",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/j.imavis.2012.06.016"
    },
    {
      "citation_id": "15",
      "title": "Perception of personality traits in oneself and others",
      "authors": [
        "L Herringer",
        "S Haws"
      ],
      "year": "1991",
      "venue": "The Journal of Psychology",
      "doi": "10.1080/00223980.1991.10543267"
    },
    {
      "citation_id": "16",
      "title": "Deploying a robotic positive psychology coach to improve college students' psychological well-being",
      "authors": [
        "S Jeong",
        "L Aymerich-Franch",
        "K Arias",
        "S Alghowinem",
        "A Lapedriza",
        "R Picard",
        "H Park",
        "C Breazeal"
      ],
      "year": "2023",
      "venue": "User Modeling and User-Adapted Interaction",
      "doi": "10.1007/s11257-022-09337-8"
    },
    {
      "citation_id": "17",
      "title": "Group hedonic balance and pair programming performance: Affective interaction dynamics as indicators of performance",
      "authors": [
        "M Jung",
        "J Chong",
        "L Leifer"
      ],
      "year": "2012",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '12",
      "doi": "10.1145/2207676.2208523"
    },
    {
      "citation_id": "18",
      "title": "Coupling interactions and performance: Predicting team performance from thin slices of conflict",
      "authors": [
        "M Jung"
      ],
      "year": "2016",
      "venue": "ACM Trans. Comput.-Hum. Interact",
      "doi": "10.1145/2753767"
    },
    {
      "citation_id": "19",
      "title": "Affective grounding in human-robot interaction",
      "authors": [
        "M Jung"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "20",
      "title": "Emotion dynamics. Current Opinion in Psychology",
      "authors": [
        "P Kuppens",
        "P Verduyn"
      ],
      "year": "2017",
      "venue": "Emotion dynamics. Current Opinion in Psychology"
    },
    {
      "citation_id": "21",
      "title": "Ranktrace: Relative and unbounded affect annotation",
      "authors": [
        "P Lopes",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2017.8273594"
    },
    {
      "citation_id": "22",
      "title": "Pagan: Video affect annotation made easy",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "IEEE",
      "doi": "10.1109/ACII.2019.8925434"
    },
    {
      "citation_id": "23",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG.2013.6553804"
    },
    {
      "citation_id": "24",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "25",
      "title": "Measuring personality in one minute or less: A 10-item short version of the big five inventory in english and german",
      "authors": [
        "B Rammstedt",
        "O John"
      ],
      "year": "2007",
      "venue": "Journal of Research in Personality",
      "doi": "10.1016/j.jrp.2006.02.001"
    },
    {
      "citation_id": "26",
      "title": "Continuous measurement of emotion. Handbook of emotion elicitation and assessment",
      "authors": [
        "A Ruef",
        "R Levenson"
      ],
      "year": "2007",
      "venue": "Continuous measurement of emotion. Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "27",
      "title": "Corae: A tool for intuitive and continuous retrospective evaluation of interactions",
      "authors": [
        "M Sack",
        "M Parreira",
        "J Fu",
        "A Lipman",
        "H Javed",
        "N Jamali",
        "M Jung"
      ],
      "year": "2023",
      "venue": "Corae: A tool for intuitive and continuous retrospective evaluation of interactions"
    },
    {
      "citation_id": "28",
      "title": "Chinese adolescent' explanations of poverty: the perceived causes of poverty scale",
      "authors": [
        "D Shek"
      ],
      "year": "2002",
      "venue": "Adolescence"
    },
    {
      "citation_id": "29",
      "title": "Survey of emotions in human-robot interactions: Perspectives from robotic psychology on 20 years of research",
      "authors": [
        "R Stock-Homburg"
      ],
      "venue": "International Journal of Social Robotics",
      "doi": "10.1007/s12369-021-00778-6"
    },
    {
      "citation_id": "30",
      "title": "Micbot: A Peripheral Robotic Object to Shape Conversational Dynamics and Team Performance",
      "authors": [
        "H Tennent",
        "S Shen",
        "M Jung"
      ],
      "year": "2019",
      "venue": "ACM/IEEE International Conference on Human-Robot Interaction",
      "doi": "10.1109/HRI.2019.8673013"
    },
    {
      "citation_id": "31",
      "title": "Designing personas for expressive robots: Personality in the new breed of moving, speaking, and colorful social home robots",
      "authors": [
        "S Whittaker",
        "Y Rogers",
        "E Petrovskaya",
        "H Zhuang"
      ],
      "year": "2021",
      "venue": "J. Hum.-Robot Interact",
      "doi": "10.1145/3424153"
    }
  ]
}