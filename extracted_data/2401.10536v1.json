{
  "paper_id": "2401.10536v1",
  "title": "Speech Swin-Transformer: Exploring A Hierarchical Transformer With Shifted Windows For Speech Emotion Recognition",
  "published": "2024-01-19T07:30:57Z",
  "authors": [
    "Yong Wang",
    "Cheng Lu",
    "Hailun Lian",
    "Yan Zhao",
    "Björn Schuller",
    "Yuan Zong",
    "Wenming Zheng"
  ],
  "keywords": [
    "speech emotion recognition",
    "hierarchical speech features",
    "Transformer",
    "shifted window"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segmentlevel emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A research hotspot of affective computing is speech emotion recognition (SER), which aims to use computers to analyze and recognize emotion categories in human speech  [1] . The core step of SER is to extract speech features with strong emotional discrimination  [2] .\n\nEarly SER work mainly combined low-level descriptor (LLD) features  [3] , e. g., fundamental frequency, Mel-frequency-Cepstral coefficients (MFCC), and frame energy, with traditional machine learning models  [4] , e. g., Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), and Support Vector Machine (SVM), to classify speech emotions. With the rise of deep learning, a series of Deep Neural Network (DNN) methods have achieved promise performance for SER  [5] ,  [6] ,  [7] , e. g., Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer. Especially, Transformer  [8]  have drew wide attention in SER tasks. For instance, Wang et al.  [9]  first used CNN and LSTM to extract temporal emotional information, and then adopted stacked Transformer layers to enhance the emotional feature extraction. Lu et al.  [10]  nested the frame-level Transformer in segment-level Transformer to fuse the local and global emotion features. Wang et al.  [11]  designed Transformer to extract local emotional feature in time-frequency domain, respectively, and then aggregated them for the global emotional representations through multiple Transformers.\n\nCurrent speech Transformers are expert in capturing the longrange dependencies within speech features by its Multihead Self-Attention (MSA), e. g., across frames or segments. They focuses on the intra-correlations of a sequence while ignoring the aggregation of inter-relationship across patches, especially the boundary information of different patches is neglected and cannot be united, which are key clues for speech emotion representation. Similarly, in computer vision, Swin Transformer  [12]  have proposed to address the patch feature aggregation under different-level by introducing the idea of receptive field expending in CNN into Transformer.\n\nInspired by Swin Transformer, we propose a novel hierarchical Transformer method for SER, which first uses the Transformer in a fixed local windows to extract emotional information, and then uses the Transformer in shifted windows to model the emotional correlation between local windows. Finally, the features are fused through a down-sampling operation. The above process constitutes a stage of our model, and we have greatly improved the emotional discrimination of features through multiple stages.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "Our proposed Speech Swin-Transformer is shown in Figure  1 , which consists of four stages. Each stage mainly includes three modules: local windows Transformer, shifted windows Transformer, and patch merging module. This section will introduce their design details and functions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Local Windows Transformer",
      "text": "The main role of this module is to calculate the emotional correlation in each time-domain window through MSA mechanism in Transformer  [8] , thereby increasing the model's attention to emotional related fragments.  The main difference between the local windows Transformer and the traditional Transformer is to use a window of size [f, t] to divide the input features x′ , get M time-domain blocks, and then calculate MSA in each window, i. e., M = d/(t × N), f and t are the frequency-domain and time-domain length of the window, instead of a traditional Transformer that directly performs MSA on the input features. After calculating the MSA, we concatenate the windows in the original order into feature s ∈ R b×h×e with the same dimension as the input feature x′ . The process is as follows:\n\nwhere LN (•) and W P (•) are layer normalization and window partition, respectively. The M SA(•), W M (•), and M LP (•) represent MSA, window merging, and the fully connected layer operation, respectively. Besides, l and m are the outputs of the window partitioning and window merging modules within the Transformer, respectively. The obtained s serves as the input for the shifted windows Transformer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Shifted Windows Transformer",
      "text": "Since the local windows Transformer module only performs MSA in each window, the connection between windows is ignored. We de-sign a Transformer with moving windows to focus on cross-window connections. The main difference between this module and the local windows Transformer is that there is an additional shift operation before the window partition, and the moving step is set to half length of the window's time-domain. The process can be represented as\n\nwhere SF (•) represents window movement similar to Swin Transformer  [12] . The l and m are the outputs of the window partitioning and merging modules within Transformer, respectively. The final output ŝ ∈ R b×h×e of Transformer module will be used as input for the patch merging module.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Patch Merging Module",
      "text": "This module reduces the resolution of features and adjusts the number of channels through downsampling operations, which can reduce computational complexity and form a hierarchical structure  [12] . Similar to Swin transformer, the specific approach is to select elements at intervals of one point in the time-domain and frequencydomain of the features, and then concatenate them together as a whole tensor. The channel dimension will become four times the original size, and then be adjusted to twice the original size through a fully connected layer. The operations can be represented as\n\nwhere P M (•) represents patch merging. The output feature ŝ′ ∈ R b×(h/4)×2e will be used as the input for the local windows Transformer in the next stage. The output of the shift windows Transformer of the last stage will be normalized, average pooled, and flattened to obtain feature y ∈ R b×8e , without going through the patch merging module. We pass the obtained feature y through the fully connected layer to calculate the predicted emotional probability; the operation is as follows:\n\nwhere F C(•) represents a fully connected layer. We use the Softmax function  [13]  to make the emotion with the highest prediction probability among the fully connected layer output features ŷ ∈ R b×k as the predicted emotion. Then, we optimize the model by reducing the cross-entropy loss Loss between predicted emotion labels and true emotion labels z ∈ R b×k , where k is the number of categories of emotions. We can represent the process as\n\nwhere Sof tmax(•) and CrossEntropyLoss(•) are the Softamx function and Cross-Entropy loss function  [14] , respectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Databases",
      "text": "Two widely used databases, i. e., IEMOCAP  [15]  and CASIA  [16]  are adopted to verify the effectiveness of the proposed method. IEMOCAP is a multimodal dataset that includes data such as video, voice, and text transcription, released by the SAIL Laboratory of the University of Southern California. It contains multimodal data of 5 male actors and 5 actresses in the process of emotional binary interaction. Our experiments select 4 types of emotional samples (angry, happy, neutral, sad) in impromptu scenes and a total of 2280 audio data. CASIA is a Chinese emotional speech database established by the Institute of Automation, Chinese Academy of Sciences. Under the fixed script content, 4 volunteers (2 men and 2 women) perform 6 emotions (angry, happy, fear, sad, neutral, and surprise) in this database, and the sampling rate of the samples is 16 000 Hz. In this paper, 200 samples of each emotion are selected, and a total of 1200 samples of audio data are used for our experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Protocol",
      "text": "We utilize the same Leave-One-Speaker-Out (LOSO) cross-validation protocol as previous studies  [10] ,  [17] ,  [11]  to evaluate model performance. In CASIA, one speaker's sample is used for testing, and the remaining three speaker's samples are used for training. For IEMOCAP, the samples of one speaker are used as the test set, and the samples of other 9 speakers are used as the training set. Moreover, the weighted average recall (WAR) and unweighted average recall (UAR)  [18]  are recorded as the evaluation metrics for the comparison experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In order to expand the number of samples and maintain the integrity of speech emotions, we sliced the audio samples into 128 frames (20 ms per frame) of fragments. Then, we pre-emphasize speech segments with a coefficient of 0.97. We use Short-Time Fourier Transform (STFT) to extract the log-Mel-spectrogram of speech, which uses a 20 ms Hamming window with a frame shift of 10 ms. Besides, the number of points of Fast Fourier Transform (FFT) and bands of Mel-filter are 512 and 32, respectively. Finally, we obtain the feature dimension of b=64, c=1, f =32, and d=128 as the input features for the proposed model. The model parameters in this paper are set as follows: N=4, t=4, e=96. The number of Transformer in the 4 stages of our model is 2, 2, 4, 2 respectively. The proposed model is implemented by the Pytorch framework  [19]  winth NVIDIA A10 GPUs. And its parameters are updating by a Adam  [20]  optimizer with a learning rate of 0.0001 and the training epoch is set to 100. To compare the performance of our proposed Speech Swin-Transformer, we selected some state-of-the-art algorithms based on Transformer architecture for performance comparison, i. e., Vision Transformer (ViT)  [21] , Transformer in Transformer (TNT)  [22] , a hierarchical vision Transformer using shifted windows (Swin-Transformer)  [12] , local to global feature aggregation learning based on Transformers (LGFA)  [10] , as well as SER via an attentive time-frequency neural network (ATFNN)  [17] , and a novel time frequency joint learning method (TF-Transformer)  [11] . Note that ViT and TNT are implemented by ourselves.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "The experiments on IEMOCAP and CASIA are shown in Table 1. We can observe that our proposed method achieves the stateof-the-art results. In detail, compared with other methods under the Leave-One-Speaker-Out protocol, our Speech Swin-Transformer achieves the highest WAR (75.22%) and UAR (65.94%) on the IEMOCAP dataset. Furthermore, we also calculate the confusion matrix obtained on IEMOCAP for comprehensive comparison, shown in Fig.  2(a) . Through the confusion matrix on IEMOCAP, it is obvious that our model has a high recognition rate (over 75%) in most emotions (angry, neutral and sad), which verifies the effectiveness of the proposed Speech Swin-Transformer. Moreover, we can also find that the recognition rate of our model on happy is only 24%, and it is easily misclassified as neutral. The possible reason is that the number of samples in the IEMOCAP dataset is unbalanced. The number of happy samples is only 284, which is the smallest number of samples in IEMOCAP. This makes the model unable to learn the unique emotional characteristics of happy, making happy easy to be misclassified for neutral. In addition, it is believed that happy and neutral are both high valence emotions  [23] . They are similar in terms of valence dimensions, which may lead to misclassification.\n\nAccording to the experimental results in Table  1 , it is obvious that our method also achieves the best WAR (54.33%) and UAR (54.33%) on CASIA. Based on the experimental results, we can discover some interesting things. Firstly, the UAR and WAR of the CASIA dataset experimental results are the same because the sample size for each type of emotion is balanced. In addition, we can see that compared with other advanced methods, the performance of our model improves by at least 1.16% on WAR and UAR.\n\nFig.  2 (b) shows the confusion matrix of our Speech Swin-Transformer on CASIA, in which we can find that the recognition accuracy of our model on angry, neutral, and sad in CASIA exceeds 68%, which reflects the effectiveness of the proposed method.\n\nNote that the recognition of our proposed method for the three types of emotions (fear, happy and surprise) are all lower than 40%. Specifically, fear is easily misidentified as sad, which may be due to the fact that these two types of emotions are low valence emotions  [23] , and similar valences may lead to this experimental phenomenon. In addition, happy and surprise are both prone to being misclassified as angry, as these three types of emotions are high arousal emotions  [23] , and similar arousal levels may lead to misidentification of emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Hierarchical Feature Map Visualizations",
      "text": "To further investigate the hierarchical representation results of the Speech Swin-Transformer, we also visualized the feature mappings in the four stages, as shown in Figure  3 . From (a) to (e), we can observe that the network's shallow layers (Stage 1) focus on the general features of the spectrogram, e. g., color and shape. As the stages progress, the network gradually differentiates the parts of the spectrogram that contain semantic information from those that do not (as shown by the red box in (c)). In the deeper Stage 3 and 4, the network distinguishes the responses in the mid-low frequency range, as indicated by the red box, which are closely related to the expression of sad. This phenomenon is consistent with previous studies  [17] , that is low arousal emotions (e. g., sad) commonly exhibit activations in the mid-low frequency range, demonstrating that our method can indeed obtain hierarchical speech emotion representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a Speech Swin-Transformer for SER based on the hierarchical Transformer architecture, which can effectively aggravate the emotional features at different-levels. Our experiments on IEMOCAP and CASIA demonstrate the effectiveness of our method. Compared with other state-of-the-art architectures based on the Transformer model, our model has improved to varying degrees in both WAR and UAR. Future research will focus on the patch boundary modeling both on frequency domain and timefrequency domain.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of Speech Swin-Transformer for speech emotion recognition (SER).",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrices of Speech Swin-Transformer.",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). Through the confusion matrix on IEMOCAP,",
      "page": 3
    },
    {
      "caption": "Figure 3: Visualizations of hierarchical feature maps generated by the different stages of Speech Swin-Transformer.",
      "page": 4
    },
    {
      "caption": "Figure 2: (b) shows the confusion matrix of our Speech Swin-",
      "page": 4
    },
    {
      "caption": "Figure 3: From (a) to (e), we can",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison results on IEMOCAP and CASIA with the",
      "data": [
        {
          "ViT": "TNT",
          "70.22": "70.61",
          "58.58": "59.72",
          "42.83": "46.58"
        },
        {
          "ViT": "Swin-Transformer [12]",
          "70.22": "70.75",
          "58.58": "60.08",
          "42.83": "47.25"
        },
        {
          "ViT": "LGFA [10]",
          "70.22": "73.29",
          "58.58": "62.63",
          "42.83": "49.75"
        },
        {
          "ViT": "ATFNN [17]",
          "70.22": "73.81",
          "58.58": "64.48",
          "42.83": "48.75"
        },
        {
          "ViT": "TF-Transformer [11]",
          "70.22": "74.43",
          "58.58": "62.90",
          "42.83": "53.17"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "Daniel Babak Joze Abbaschian",
        "Adel Sierra-Sosa",
        "Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Wenming Zheng",
        "Yang Li",
        "Chuangao Tang",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "Taiba Majid Wani",
        "Teddy Surya Gunawan",
        "Syed Asif",
        "Ahmad Qadri",
        "Mira Kartiwi",
        "Eliathamby Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "5",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "Monorama Swain",
        "Aurobinda Routray",
        "Prithviraj Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "6",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Beer Youddha",
        "Shivani Singh",
        "Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition: a comprehensive survey",
      "authors": [
        "Mohammed Jawad",
        "Abbas Ebrahimi-Moghadam"
      ],
      "year": "2023",
      "venue": "Wireless Personal Communications"
    },
    {
      "citation_id": "8",
      "title": "Speechformer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Attention is all you need",
      "authors": [
        "Waswani",
        "N Shazeer",
        "J Parmar",
        "L Uszkoreit",
        "Jones",
        "L Gomez",
        "Kaiser",
        "Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "10",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "Xianfeng Wang",
        "Min Wang",
        "Wenbo Qi",
        "Wanqi Su",
        "Xiangqian Wang",
        "Huan Zhou"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Learning local to global feature aggregation for speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Hailun Lian",
        "Wenming Zheng",
        "Yuan Zong",
        "Yan Zhao",
        "Sunan Li"
      ],
      "year": "2023",
      "venue": "Learning local to global feature aggregation for speech emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "Time-frequency transformer: A novel time frequency joint learning method for speech emotion recognition",
      "authors": [
        "Yong Wang",
        "Cheng Lu",
        "Yuan Zong",
        "Hailun Lian",
        "Yan Zhao",
        "Sunan Li"
      ],
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "13",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "14",
      "title": "Activation functions in deep learning: A comprehensive survey and benchmark",
      "authors": [
        "Ram Shiv",
        "Satish Dubey",
        "Bidyut Singh",
        "Chaudhuri Baran"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "Neural collapse under cross-entropy loss",
      "authors": [
        "Jianfeng Lu",
        "Stefan Steinerberger"
      ],
      "year": "2022",
      "venue": "Applied and Computational Harmonic Analysis"
    },
    {
      "citation_id": "16",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "17",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Jtflm Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "Cheng Lu",
        "Wenming Zheng",
        "Hailun Lian",
        "Yuan Zong",
        "Chuangao Tang",
        "Sunan Li",
        "Yan Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "19",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "André Stuhlsatz",
        "Christine Meyer",
        "Florian Eyben",
        "Thomas Zielke",
        "Günter Meier",
        "Björn Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "20",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "A method for stochastic optimization",
      "authors": [
        "Jimmy Kinga",
        "Ba Adam"
      ],
      "year": "2015",
      "venue": "International conference on learning representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "Transformer in transformer",
      "authors": [
        "Kai Han",
        "An Xiao",
        "Enhua Wu",
        "Jianyuan Guo",
        "Chunjing Xu",
        "Yunhe Wang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}