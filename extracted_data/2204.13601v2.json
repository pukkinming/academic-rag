{
  "paper_id": "2204.13601v2",
  "title": "Emotion Recognition In Persian Speech Using Deep Neural Networks",
  "published": "2022-04-28T16:02:05Z",
  "authors": [
    "Ali Yazdani",
    "Hossein Simchi",
    "Yasser Shekofteh"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Feature Extraction",
    "Deep Learning",
    "Farsi Language",
    "ShEMO dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is of great importance in Human-Computer Interaction (HCI), as it provides a deeper understanding of the situation and results in better interaction. In recent years, various machine learning and Deep Learning (DL) algorithms have been developed to improve SER techniques. Recognition of the spoken emotions depends on the type of expression that varies between different languages. In this paper, to further study important factors in the Farsi language, we examine various DL techniques on a Farsi/Persian dataset, Sharif Emotional Speech Database (ShEMO), which was released in 2018. Using signal features in low-and high-level descriptions and different deep neural networks and machine learning techniques, Unweighted Accuracy (UA) of 65.20% and Weighted Accuracy (WA) of 78.29% are achieved.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotional state is an important factor in their interactions and affects most communication ways such as facial expressions, voice characteristics, and linguistic content of verbal communication  [1] -  [3] . Speech is one of the main ways to express emotions. To obtain a natural humancomputer interaction (HCI), it is highly important to recognize, interpret, and respond to the emotions expressed in the speech  [3] ,  [4] . Today, speech emotion recognition (SER) systems have several applications such as human-machine interactions naturally e.g. web videos, computer videos, and training programs, car driver safety, computer games, diagnostic tools to treat the disease, as a tool for an automatic translation system and mobile communications  [1] -  [3] .\n\nDeep learning (DL) has been considered an emerging research field in machine learning (ML) and has received more attention in recent years  [5] . DL techniques for SER have several advantages over traditional methods, including their ability to recognize complex structures and features without needing to manually extraction, to extract low-level features from raw data, and the ability to deal with unlabeled data  [2] ,  [6] . Our main goal in this paper is to investigate the different speech feature sets and their extraction methods, as well as the impact of using different deep neural network (DNN) architectures to detect the spoken emotions such as anger, surprise, happiness, sadness, and neutral state in the ShEMO dataset, which is a Farsi/Persian spoken speech dataset.\n\nThe rest of the structure of this paper is as follows: In section 2, we will review the related works. In section 3, we introduce our contribution which includes reviewing the ShEMO data set and two common methods of extracting features as LLDs (Low-Level Descriptors) and functionals from voice signals, as well as testing different DNN models on these features. In section 4 we will review and discuss the results and finally in section 5, we will have the conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "The SER approaches consist of two steps known as feature extraction (FE) and feature classification. At the first step of speech processing, researchers have acquired several characteristics including prosodic and vocal tract features. The second step is about using traditional classifiers such as Support Vector Machine (SVM) or neural networks. Deep neural networks (DNNs) and Convolutional Neural Networks (CNN) provide efficient results for signal processing. On the other hand, recursive networks such as Recursive Neural Networks (RNNs) and Long Short-Term Memory (LSTM) are very effective in SER.\n\nFirst of all, it should be said that the SER systems can be classified into two main classes, frame-based or utterancebased systems  [3] ,  [4] ,  [7] ,  [8] . Input speech signal, as an utterance, is divided into smaller segments, called frames, in many speech processing systems. Frame-based SER systems utilize FE of all of the frames, but utterance-based systems use a fixed size feature vector, known as global or Functional features, after post-processing of the raw features. Using a Bidirectional LSTM (BLSTM) network which effectively preserves the characteristics of temporal dynamics, in  [7] , on the IEMOCAP database shows that the SER system will perform better than DNN. They obtained low-level acoustic features as well as the global features using statistical functions and then applied them to the output and then the acoustic features are given to the Extreme Learning Machine (ELM) network. Finally, UA was achieved of 63.89% with WA of 62.85%.\n\nIn  [9] , the spectrogram was used directly to train a CNN-LSTM network. At the first step, utterances that are longer than 3 seconds are divided into small equal parts. Also, they calculated spectrogram for each frame by applying Hamming window to each signal with a frame size of 10 milliseconds, a window size of 20 milliseconds, and a Fourier series with a length of 800. In the CNN-LSTM architecture, it is assumed that CNN extracts specific patterns that contain emotional information in the utterance, and LSTM pays attention to the temporal behavior during the utterance. The system without having a separate feature extraction step was achieved WA of 67.3% and UA of 62.0%.\n\nIn  [8] , a BLSTM recursive neural network was used along with the attention mechanism which ignores the silent parts of the utterance as well as the parts that do not have emotional content. Both frame features and temporal aggregation can be learned over longer periods. Therefore, silent frames receive small weights and the rest of the frames receive appropriate weights based on the amount of emotional content. Finally, they achieved WA of 63.5% and UA of 58.8% on the IEMOCAP database. In  [4] , the FCN (Fully Convolutional Network) is introduced which works with variable length of speech as well as regardless of segmentation. In this paper, a network with convolutional layers with the attention mechanism is used and it works without needing speech segmentation. Using the attention mechanism, the weights of silent frames are completely reduced and are ignored to detect emotions. This model is achieved a WA of 70.4% and UA of 63.9%.\n\nIn  [10] , after converting the signal into spectrogram, the data is transmitted directly to the network. A CNN network is used to extract high-level features and then they have used a recursive network. They also used the vocal tract length perturbation (VTLP) data augmentation technique to overcome the problem of insufficient data. Finally, UA of 65.3% and WA of 66.9% were obtained on the IEMOCAP database.\n\nAlso, as we mentioned before, choosing the proper features is an important factor in the SER task. In  [11] , using the openSMILE tool and emobase2010 feature set, they have extracted a 1582-dimensional vector for each utterance. Using GAN (Generative Adversarial Network) architecture with mixup data augmentation technique, better results in a crosscorpus task have been obtained than previous studies. Also,  [12]  used the IS09 1 feature set which includes 384 features per utterance, along with SVM and logistic regression. MFCC features were also extracted from a maximum of 120 frames of utterances and a matrix (120, 13) was obtained for each utterance which was used in a stacked LSTM architecture.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "The ShEMO dataset in  [13] , contains 3,000 semi-natural speech files, equivalent to 3 hours and 25 minutes of speech samples collected from online radio broadcasts. These files are 1 The INTERSPEECH 2009 Emotion Challenge feature set.\n\nin .wav, 16-bit, 44.1 kHz, and single-channel formats. In this dataset, 87 people (including 31 women and 56 men), whose mother tongue is Persian used to examine the 5 main feelings of anger, fear, happiness, sadness, surprise, and neutral state without feeling. 12 persons, including 6 men and 6 women, tag these speech files as emotion labels and are used by voting to determine the final label.\n\nThe average duration of utterances is 4.11 seconds with a standard deviation of 3.41. It should be noted that due to the small number of files labeled with fear (a total of 38 files), these files have been removed from the experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Features",
      "text": "In general, there are two steps to extract features from audio files. Initially, the audio file is split into smaller speech segments, the frames, and the low-level descriptions (LLDs) are extracted from each frame. Then different statistical functions (such as mean, max, variance, linear regression coefficients, etc.) are applied to the LLDs to obtain a highlevel or Functional feature vector for each utterance. Different machine learning models such as SVM or decision tree can be used on Functional feature vectors or neural network models such as recursive or convolutional networks directly using LLDs. Fig.  1 , shows the proposed system in this paper, along with the various models we used.\n\nWe extracted various LLDs from speech files. For this purpose, we sampled each speech file at a sampling rate of 16KHz and we used it for 7.52 seconds (average total file duration plus standard deviation inspired). Also, we cropped longer files and padded shorter files with zero padding in 7.52 seconds. Then we divided the obtained time series into 32ms as well as 100ms frames, with 50% overlap. Finally, 52 features including various spectral features such as centroid, contrast, roll-off, and bandwidth along with features such as zero-crossing rate and MFFC features are extracted from each frame as Hand Crafted LLDs. In addition, we tested two different feature sets for extracting LLDs using the openSMILE toolkit  [14] . It should be noted that these features are extracted from 20ms long frames. The first feature set is called the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [15] , which extracts 25 LLD features (such as MFCC features, frequency-related features, bandwidth, etc.) from each frame. The second feature set is the Interspeech 2016 Computational Paralinguistics Challenge (ComParE_2016)  [16] , which extracts 65 LLDs per frame.\n\nThe openSMILE tool provides various features for extracting from speech files. To extract these features, it splits the speech files into 20ms frames with 10ms overlap, extracts the various LLDs from the frames, and then using several statistical functions on them to achieve a d-dimensional feature vector as Functional features for each utterance. We used 3 different feature sets for our experiments:\n\n• The extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS): In addition to the LLD features described in the previous section, this feature set includes 88 Functional features such as spectral, frequency, amplitude, and energy.\n\n• The INTERSPEECH 2010 Paralinguistic Challenge feature set (IS10_Paraling)  [17] : It uses 21 different statistical functions (such as standard deviation, arithmetic mean, skewness, kurtosis, etc.) to 34 LLD features (including MFCC features, logarithmic power of Mel-frequency bands, and the loudness as the normalized intensity, etc.) is obtained along with their delta. In addition, there are several pitch-related features that ultimately result in a 1582-dimensional feature vector for each utterance.\n\n• The large openSMILE emotion feature set (emo_large): It extracts the largest number of Functional features from each audio file, contains 6552 features obtained by applying more functions to more LLDs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Deep Learning Models",
      "text": "In recent years, deep learning has been used effectively by researchers due to its multi-layer structure and the presentation of efficient results in a variety of fields, including emotion recognition in speech  [1] -  [3] ,  [6] .\n\nDNNs are based on feed-forward structures, consisting of one or more hidden layers between input and output. CNNs are another type of deep learning technique that is used exclusively with forward-looking architecture for classification. CNNs are commonly used to identify patterns and provide better classification. RNNs are a branch of sequential information neural networks, in which outputs and inputs are interdependent, and their dependence is usually useful in predicting the next state of the input. RNNs, like CNNs, require memory to store general information obtained in a sequential deep learning modeling process, and usually only work efficiently for a few generations. The main problem that affects the overall performance of RNN is its sensitivity to the disappearance of gradients, which leads to forgetting the initial input. To prevent this, LSTM is used to create a block between frequent connections. These networks can also be used Bidirectional-LSTM  [2] ,  [5] .\n\nThe combination of CNN and LSTM networks has received much attention in recent years. In the SER task, it is assumed that CNN extracts specific patterns that contain emotional information in the utterance and LSTM pays attention to the temporal behavior during the utterance  [6] ,  [10] . Therefore, using the CNN-LSTM architecture can be effective in categorizing LLD features.\n\nThe use of attention mechanisms in neural networks has shown widespread success in a wide range of tasks, such as question answering, machine translation, natural language understanding, and speech recognition. The main idea of the attention mechanism is to focus on a few related parts while ignoring the rest. There are many variations on this mechanism (global vs. local, soft vs. hard) but its main use is to reinforce different LSTM models such as encoder-decoder architecture (for example in machine translation), avoiding the use of a fixed context vector as the only output of the decoder. Specifically, this is the last hidden LSTM layer that carries all the information extracted by the LSTM encoder. Thus, in the classical structure, all information is compressed into a context vector, which can act as a bottleneck, while all hidden middle layers of the encoder are ignored. This vector is then passed to subsequent layers such as the LSTM or dense decoder. In later steps, we rely only on this type of summary given by the encoder, and by increasing the length of the time sequence analyzed, the performance of the model can be reduced. To deal with this problem, the attention mechanism is very effective  [2] -  [4] ,  [8] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Experiments",
      "text": "We tested and evaluated various deep learning models on the ShEMO dataset. Also, we used various LLD as well as Functional acoustic features. The ShEMO dataset is an imbalanced dataset according to the number of files in each class. So, we used both the Unweighted Accuracy (UA) and the Weighted Accuracy (WA) as evaluation metrics. We implemented a 5-fold cross-validation for our experiments. In each fold, four sessions of the data were used to train the model, and the remaining session was used as a test set. The results of our experiments are divided into 3 general sections:\n\n1) Test Different Models On LLDs: We first compare the results of applying different deep learning models to the features of Hand Crafted LLDs. In this section, we extract Hand Crafted LLDs in high resolution (32ms frames) and low resolution (100ms frames). All models of deep learning in this section receive inputs (batch size, number of frames, number of features per frame) and after passing different layers, in the end, by the softmax function, the class that is most likely will be determined. The DNN used in the experiment is a Fully Connected Network consisting of dense layers with batch normalization and drop out to prevent overfitting. The use of BLSTM, and BLSTM networks along with the attention mechanism, are two other models that were tested, and finally, a CNN-BLSTM network that uses the attention mechanism was also tested. The architecture of this network is such that it consists of 1-dimensional convolution layers with 1dimensional max pooling, followed by a Bidirectional LSTM layer that uses the attention mechanism. The results show that the use of the LSTM recursive network on sequential LLD features has improved the model performance over DNN. It is also much more effective than LSTM alone in recognizing emotion in spoken utterances using the attention mechanism associated with BLSTM. Finally, using convolution layers before BLSTM attention, more prominent components in LLDs were extracted first, which improved the performance of the model. It should be noted that this network in low resolution (100ms frames) can not have the same performance as high resolution because, at low resolutions, convolutional layers cannot create effective representations of LLD properties. The results of this section as are given in Table  1 , show that using a CNN+Attetnion-BLSTM model can perform well with Hand Crafted LLDs obtained from 32ms frames. This model achieved a UA of 63.52% and WA of 75.32%.\n\n2) Test CNN+Attention-BLSTM Model On Different LLD Feature Sets: In previous experiments, we showed that a neural network with a CNN-BLSTM architecture with an attention mechanism can perform well on Hand Crafted LLDs (52 LLDs). In this section, we test this model on the eGeMAPS (25 LLDs) and ComParE_2016 (65 LLDs) feature lists extracted by openSMILE tool. As shown in Table  2 , this CNN+Attetnion-BLSTM model with a UA of 63.52% and WA of 75.32%, still performs better on Hand Crafted LLDs than other feature sets.\n\n3) Test Different Models On Different Functional Feature Sets: In this section, we test different models of deep learning on Functional features. The neural networks used in this section receive the inputs (batch size, number of features per utterance) and after passing through different layers, like the previous models, the softmax function of the class with the most Specifies the probability. A DNN network consisting of fully connected layers is a CNN network with 1-dimensional convolutional layers. In addition, an SVM model with Radial Basis Function (RBF) kernel was tested for comparison according to the settings used in the ShEMO paper. The results show that using deep neural as well as larger feature sets can have better performance than traditional machine learning models. Compare to the traditional SVM model used in ShEMO paper, +7.0% improvement of UA (from 58.20% to 65.20%) was achieved by using a simple CNN along with emo_large feature set. The feature sets used, the number of their Functional features, models, and the results are listed in Table  3 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we investigated different deep and machine learning techniques, as well as speech features to recognize emotions. Also, due to the novelty of using different deep learning techniques in Farsi, we introduced the best model for recognizing emotions in Farsi, and finally, we were able to obtain higher accuracy than traditional methods. We improved the UA reported in the ShEMO paper by 7%. In addition, we achieved a WA of 78.29%.\n\nThe results showed that the use of a Convolutional Neural Network followed by a BLSTM, performs better than a DNN or CNN-only network when using LLDs. Also, the use of the attention mechanism significantly improves the performance of the model in this case. In contrast, when using Functional features, using a machine learning model such as SVM can be a good choice, although when the number of features increases, a CNN network can perform better in terms of computational cost and accuracy.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , shows the proposed system in this paper, along",
      "page": 2
    },
    {
      "caption": "Figure 1: Two general approaches for developing an SER system: (a) An SER system that uses Low-Level Descriptors",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 3: end, by the softmax function, the class that is most likely will",
      "data": [
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "Method"
        },
        {
          "Frame \nLength": "100ms",
          "Hand Crafted LLDs": "BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "Attention-BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "CNN+Attention-BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "DNN"
        },
        {
          "Frame \nLength": "32ms",
          "Hand Crafted LLDs": "BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "Attention-BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "CNN+Attention-BLSTM"
        },
        {
          "Frame \nLength": "",
          "Hand Crafted LLDs": "DNN"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: end, by the softmax function, the class that is most likely will",
      "data": [
        {
          "Feature Set": "",
          "CNN + Attention-BLSTM": "UA"
        },
        {
          "Feature Set": "ComParE_2016",
          "CNN + Attention-BLSTM": "61.00"
        },
        {
          "Feature Set": "eGeMAPS",
          "CNN + Attention-BLSTM": "60.45"
        },
        {
          "Feature Set": "Hand Crafted Features",
          "CNN + Attention-BLSTM": "63.52"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: end, by the softmax function, the class that is most likely will",
      "data": [
        {
          "Feature Set": "",
          "Functionals": "# of features \nMethod \nUA \nWA"
        },
        {
          "Feature Set": "eGeMAPS",
          "Functionals": "SVM (baseline) \n58.20 \n73.90 \n88 \nDNN \n56.05 \n72.45"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Set": "",
          "Functionals": "# of features"
        },
        {
          "Feature Set": "",
          "Functionals": ""
        },
        {
          "Feature Set": "IS10_Paraling",
          "Functionals": "1582"
        },
        {
          "Feature Set": "",
          "Functionals": ""
        },
        {
          "Feature Set": "",
          "Functionals": ""
        },
        {
          "Feature Set": "emo_large",
          "Functionals": "6553"
        },
        {
          "Feature Set": "",
          "Functionals": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "M Swain",
        "A Routray",
        "P Kabisatpathy"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "2",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "Attention Based Fully Convolutional Network for Speech Emotion Recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "5",
      "title": "Learning Deep Architectures for AI",
      "authors": [
        "Y Bengio"
      ],
      "year": "2009",
      "venue": "Learning Deep Architectures for AI"
    },
    {
      "citation_id": "6",
      "title": "End-to-End Speech Emotion Recognition Using Deep Learning",
      "authors": [
        "A Jacob",
        "A Jacob",
        "A Mathew"
      ],
      "year": "2021",
      "venue": "International Journal of Research in Engineering"
    },
    {
      "citation_id": "7",
      "title": "High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition"
    },
    {
      "citation_id": "8",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms"
    },
    {
      "citation_id": "10",
      "title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "11",
      "title": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Augmenting Generative Adversarial Networks for Speech Emotion Recognition",
      "arxiv": "arXiv:2005.08447"
    },
    {
      "citation_id": "12",
      "title": "Cross Lingual Cross Corpus Speech Emotion Recognition",
      "authors": [
        "S Goel",
        "H Beigi"
      ],
      "year": "2020",
      "venue": "Cross Lingual Cross Corpus Speech Emotion Recognition",
      "arxiv": "arXiv:2003.07996"
    },
    {
      "citation_id": "13",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "14",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language",
      "authors": [
        "B Schuller"
      ],
      "year": "2016",
      "venue": "The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native Language"
    },
    {
      "citation_id": "17",
      "title": "The INTERSPEECH 2010 paralinguistic challenge",
      "authors": [
        "B Schuller"
      ],
      "year": "2010",
      "venue": "The INTERSPEECH 2010 paralinguistic challenge"
    }
  ]
}