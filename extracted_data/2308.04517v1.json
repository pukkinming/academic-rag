{
  "paper_id": "2308.04517v1",
  "title": "Capturing Spectral And Long-Term Contextual Information For Speech Emotion Recognition Using Deep Learning Techniques",
  "published": "2023-08-04T06:20:42Z",
  "authors": [
    "Samiul Islam",
    "Md. Maksudul Haque",
    "Abu Jobayer Md. Sadat"
  ],
  "keywords": [
    "Speech Emotion Recognition (SER)",
    "Score Level Fusion",
    "Self-supervised Learning",
    "Hidden Unit Bidirectional Encoder Representations from Transformers(HuBERT)",
    "Graph Convolution Network A.2.2 SER using Convolutional Neural Network . . . . . . . . . . A.2.3 SER using Recurrent Neural Network . . . . . . . . . . . . . A.3 SER using Evolution of Deep Learning Techniques . . . . . . . . . . A.3.1 SER using Graph Convolution Network(GCN) . . . . . . . . A.3.2 SER using Hidden Unit BERT (HuBERT) model . . . . . . A.3.3 Multimodal Speech Emotion Recognition and Classification Using Convolutional Neural Network Techniques . . . . . . . Bibliography RADVDESS HuBERT Training testing for 4 specific emotion classes 6.4 RADVDESS HuBERT Confusion Matrix for 4 specific emotion classes"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Longterm contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining GCN and Hu-BERT, our ensemble model can leverage the strengths of both approaches. This allows for the simultaneous analysis of multimodal data, and the fusion of these modalities enables the extraction of complementary information, enhancing the discriminative power of the emotion recognition system. The results indicate that the combined model can overcome the limitations of traditional methods, leading to enhanced accuracy in recognizing emotions from speech.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Declaration Of Authorship",
      "text": "This is to certify that the work presented in this thesis titled \"Capturing Spectral    [6]  . . . . . . . . . . . . . . . . 3.12 A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).  [7]  . . . . . . 3.13 SERAB evaluation pipeline. The (pre-trained/non-trainable)feature extractor is used to obtain utterance-level embeddings (X) from the input. X are used as input to the task-specific classifier F optimized for predicting the emotion Y expressed in the input.  [8]  . . . . . . . 3.  14  The architecture of their proposed model. The right part illustrates the multimodal transformer fusion with weights sharing; the left part illustrates the word-level speech representation and the attentionbased interaction mechanism.  [9]  . . . . . . . . . . . . . . . . . . . . 3.15 Overall architecture of the proposed MMFA-RNN model based SER.  [10]  28 3.  16  The IEmoNet framework where each intermediate system (ASR, SER, TER) can be chosen and optimized individually.  [11]  . . . . . . 3.17 Our proposed SER architecture.  [11]  . . . . . . . . . . . . . . . . . . 3.18 Detailed architecture of CNN-BiLSTM module in the previous figure  [11]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The task of automatically detecting the emotional content of spoken language is defined as speech emotion recognition (SER). The naturalness and human-likeness of virtual assistants can be improved, the user experience of social robots can be improved, and people with communication difficulties can get feedback courtesy of this. SER can be difficult for a variety of reasons. First of all, emotions are illogical and might differ from one individual to the next. Second, the same feeling may be communicated in a variety of ways based on the speaker's temperament, cultural background, and conversational context. Third, voice signals may be distorted and include extraneous data, such as background noise or the accent of the speaker.\n\nSeveral techniques have been placed forth for SER, including more established deep learning methods like convolutional neural networks (CNNs) and long short-term memory (LSTM) networks, as well as more established machine learning methods like support vector machines (SVMs) and hidden Markov models (HMMs). The precise properties of the data and the task's needs determine the SER strategy to use. In general, more complex models, like deep learning techniques, tend to be more accurate but may need more computer power and labeled data in larger amounts. Overall, SER is a dynamic field of study with a variety of difficulties and chances for growth and progress.\n\nRecognition of emotion from the human speech is a non-trivial process for machines. Humans can understand each other's feelings through speech very easily while machines it goes through a computational procedure. Deep learning approaches for automated speech emotion recognition have made it easier for people to interact with computers supporting the Human-Computer Interaction field.\n\nThere are a few techniques to extract emotional features from speech, including extraction of hand-crafted prosodic features and automatic feature learning methods. Automated feature-learning algorithms learn relevant features from the input data. Once important features are extracted, then it will be fed into the machine learning architecture to classify emotions accordingly.\n\nSpeech emotion recognition involves analyzing various acoustic and linguistic features present in the speech signal to discern emotions such as sadness, anger, happiness, neutrality, surprise, fear, and disgust. By capturing vocal cues, intonations, prosodic patterns, and language characteristics, speech emotion recognition aims to decode the underlying emotional information and provide valuable insights into human communication. This field has significant implications in diverse domains, including human-computer interaction, sentiment analysis, mental health monitoring, and personalized user experiences, where understanding and responding to human emotions are essential for effective communication and interaction.\n\nSpeech emotion recognition holds immense importance as it allows for a deeper understanding and analysis of human emotions conveyed through spoken language.\n\nEmotions play a pivotal role in communication, shaping the way messages are delivered, perceived, and interpreted. By accurately recognizing and comprehending emotions from speech, a wide range of applications and fields can benefit greatly.\n\nIn the domain of human-computer interaction, speech-emotion recognition enables more intuitive and natural interactions between humans and machines. By discerning the emotional states of users, virtual assistants, and chatbots can respond in a more personalized and empathetic manner, enhancing user experiences and satisfaction. This capability has the potential to revolutionize the way we interact with technology, making it more human-centric and emotionally aware. Furthermore, in the realm of sentiment analysis, speech emotion recognition allows for the automatic assessment of emotions expressed in audio content, such as customer reviews, social media posts, and recorded interviews. This enables businesses and organizations to gain insights into customer sentiment, evaluate brand perception, and tailor their marketing strategies accordingly. Sentiment analysis powered by speech emotion recognition also aids in understanding public opinion, tracking trends, and informing decision-making processes. Additionally, speech-emotion recognition plays a crucial role in mental health monitoring and assessment. By",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Motivation",
      "text": "The motivation behind this project is driven by the need to improve the accuracy and robustness of speech-emotion recognition systems. Emotions play a fundamental role in human communication and accurately recognizing and understanding them from speech can have significant implications in various domains. However, traditional approaches in speech emotion recognition have limitations in effectively capturing and integrating information from multimodal sources. There is a growing demand for novel techniques that can leverage both textual and audio modalities to enhance feature extraction and overcome the shortcomings of existing methods. By addressing these challenges, this project aims to advance the field of speech emotion recognition and contribute to the development of more sophisticated and reliable emotion recognition systems.\n\nThe scope of this project encompasses the development and evaluation of a novel approach for multimodal speech emotion recognition using Graph Convolutional Networks (GCN) and the HuBERT transformer. The project involves preprocessing and preparing two datasets, namely the IEMOCAP and RAVDESS datasets, for experimentation. The audio signals from these datasets will be transformed into text, and both the textual and audio data will be processed using the GCN and HuBERT models. The test results from these models will be stored and analyzed. The project aims to explore the benefits of combining GCN and HuBERT in capturing informative features and information from both speech and text modalities. The proposed approach will be compared against traditional methods, such as LSTM, CNN, RNN, SVM, and MLP, to demonstrate its superiority in speech emotion recognition. The project's scope also includes evaluating the performance of the approach in terms of accuracy, efficiency, and ability to handle multimodal data. Overall, the project seeks to contribute to the advancement of multimodal speech emotion recognition techniques and provide insights for further research in this domain.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Research Challenges",
      "text": "Preprocessing the IEMOCAP and RAVDESS datasets posed specific challenges.\n\nIn the case of IEMOCAP, the dataset contained instances of mixed emotions within each speech sample, making it difficult to create accurate emotion graphs for the We encountered a research gap regarding the application of the HuBERT transformer specifically for speech emotion recognition. While there have been advancements in transformer models for natural language processing tasks, their utilization in the domain of speech emotion recognition was limited. This lack of prior work presented a challenge in terms of adapting the HuBERT model and exploring its effectiveness in capturing the emotional features present in speech data. Extensive experimentation and fine-tuning were necessary to determine the optimal configuration and performance of HuBERT for this task. Evaluation on Benchmark Datasets: Our thesis includes the evaluation of the proposed models on the latest benchmark datasets, namely the RAVDESS and IEMOCAP datasets. By testing our models on these widely used datasets, we provide valuable insights into their performance and generalizability.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Research Contribution",
      "text": "GCN and HuBERT for Speech Emotion Recognition: As previously expressed, we tried to compress our architecture in a parallel HuBERT and GCN module reasoning to capture both spectral and Long-term Contextual information of speech so that recognition is more robust and emotion can be classified depending on both the speech information at the same time. Also going for a multimodal approach is always a win-win situation for downstream tasks like this. Hence our motivation from the given challenges is like this to go for as our research contribution.\n\nOverall, our thesis makes significant contributions by introducing a novel multimodal approach, highlighting the strengths of GCN and HuBERT models, extending the application of HuBERT to speech emotion recognition, and evaluating the models on relevant benchmark datasets. These contributions advance the field of speech emotion recognition and provide valuable insights for future research in multimodal emotion analysis.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Thesis Outline",
      "text": "In Chapter 2",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Background Study",
      "text": "The background study for this project encompasses various aspects related to speech emotion recognition. Understanding and addressing these various aspects is crucial for the successful development of accurate and robust speech-emotion recognition systems. Speech emotion recognition tasks typically involve analyzing various acoustic and linguistic features to determine the emotional content of speech. Here are some commonly used speech features for this task. data. These learned representations can then be fine-tuned and applied to specific emotion recognition tasks with limited labeled data.",
      "page_start": 21,
      "page_end": 23
    },
    {
      "section_name": "Features Used In Speech Emotion Recognition",
      "text": "",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Semantic Relationships",
      "text": "Semantic relationships in speech emotion recognition refer to the connections and associations between words and their meanings within the context of emotional expression. It involves understanding the relationships and dependencies between words and the emotions they convey, as well as how they contribute to the overall emotional content of the speech. Semantic relationships play a vital role in speech emotion recognition by providing contextual understanding, enabling fine-grained emotion recognition, interpreting figurative language, disambiguating polysemous words, and facilitating multimodal integration. By capturing these relationships, the model can better comprehend and interpret the emotional content expressed in speech, leading to improved performance in emotion recognition tasks.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Contextual Dependencies In Textual Representation Of Data",
      "text": "In the textual representation of speech emotion recognition task, contextual dependencies refer to the relationships and dependencies between words or textual elements within a given context or sentence. These dependencies capture the influence and meaning conveyed by the surrounding words and help in understanding the emotional content.\n\nContextual dependencies are important in textual representation for speech emotion recognition for several reasons. Firstly, emotions are often expressed and perceived through the choice of words, sentence structure, and the overall context.\n\nThe surrounding words provide contextual cues that contribute to the interpretation and understanding of emotions.\n\nSecondly, capturing contextual dependencies allows for a more comprehensive analysis of the emotional content. By considering the relationships between words, such as their semantic connections, syntactic structure, and co-occurrence patterns, the model can gain a better understanding of the overall emotional expression.\n\nAdditionally, contextual dependencies aid in disambiguating the meaning of words.\n\nWords may have multiple meanings or interpretations, but their context helps in discerning the intended emotional connotation. By considering the surrounding words, the model can better infer the emotional intent and nuances in the textual representation.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Long-Term Dependencies In Sequential Data",
      "text": "Long-term dependencies in sequential data refer to the relationships and dependencies that exist between distant elements or events in a sequence. In the context of speech emotion recognition, long-term dependencies are crucial for understanding the context and temporal dynamics of speech. They involve capturing how earlier segments of speech may influence the interpretation and understanding of subsequent segments.\n\nTraditional approaches like LSTMs and RNNs have limitations in effectively modeling and capturing these long-term dependencies. These models suffer from vanishing or exploding gradients, making it difficult to propagate information across long sequences. As a result, they may struggle to capture the contextual meaning and relationships between words or phonemes that are further apart in the speech sequence.\n\nTo address this challenge, Graph Convolutional Networks (GCNs) offer a novel approach. GCNs leverage graph-based representations of text, where words or phonemes are represented as nodes in a graph, and the relationships between them are captured through edges. This graph representation allows GCNs to capture long-term contextual dependencies and semantic relationships more effectively.\n\nBy considering the connections and interactions between different elements in the graph, GCNs can better model the dependencies in sequential data.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Machine Learning Models Used In Speech Emotion Recognition",
      "text": "The field of Speech Emotion Recognition (SER) has evolved significantly in terms of the models used for classification. Initially, traditional handcrafted featurebased models were employed, utilizing features such as pitch, energy, and spectral properties. However, with the advent of machine learning, more sophisticated approaches were developed, including Support Vector Machines (SVM), Hidden\n\nMarkov Models (HMM), and Gaussian Mixture Models (GMM). These models leveraged the power of statistical learning algorithms to improve SER performance.\n\nAs deep learning gained prominence, deep neural networks, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), were introduced, offering enhanced representation learning capabilities. These models demonstrated superior performance by automatically extracting meaningful features from raw speech data. More recently, with the rise of language models, the SER task has incorporated the use of pre-trained language models, such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), which can leverage contextual information and semantic relationships within the speech data, enabling more accurate and nuanced emotion recognition. This progression from traditional handcrafted features to machine learning models and now to language models showcases the continuous advancements in SER research, highlighting the impact of these models in capturing and understanding emotional expressions in speech.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Traditional Models",
      "text": "Traditional models used in Speech Emotion Recognition (SER) encompass various techniques, including Support Vector Machines (SVM), Hidden Markov Models (HMM), and Gaussian Mixture Models (GMM). A brief overview of some of the traditional models is given below:\n\n• SVM: SVM is a supervised learning model that separates different emotion classes using a hyperplane in a high-dimensional feature space. Its strength lies in its ability to handle high-dimensional feature vectors and handle nonlinear relationships. However, SVM's performance heavily relies on the choice of kernel function and hyperparameters, and it may struggle with imbalanced datasets.\n\n• HMM: HMM is a statistical model that captures temporal dependencies in speech signals by modeling the underlying emotional state transitions. It can effectively capture sequential information and has been widely used in speech and audio processing. However, HMM assumes that the emotion classes are generated from discrete states, which may limit its ability to capture complex emotions.\n\n• GMM: GMM is a probabilistic model that represents speech data as a mixture of Gaussian distributions. It is flexible, capable of capturing complex statistical relationships, and can model the variability within each emotion class. However, GMM may struggle with modeling long-term dependencies and can be computationally demanding.\n\nOver time, traditional models have evolved to incorporate advanced techniques and improvements. For example, SVM has been enhanced with kernel trick methods and ensemble learning. HMM has been combined with other models like GMM to create hybrid systems, and adaptations like the semi-continuous HMM have been developed. GMM models have seen advancements in the form of diagonal covariance matrices and the integration of feature transformations.\n\nWhile traditional models have provided valuable insights and achieved reasonable performance in SER, their reliance on handcrafted features and limited ability to capture complex relationships led to the emergence of more powerful models, such as deep learning and language models, which offer improved representation learning and contextual understanding of emotions in speech.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Deep Learning Models",
      "text": "Deep learning models have overcome the limitations of traditional models in Speech Emotion Recognition (SER) by leveraging their ability to automatically learn complex features from raw speech data. In summary, GCN overcomes the limitations of traditional deep learning models by explicitly incorporating graph structures, handling irregular data, capturing multi-scale dependencies, and adapting to varying relationships. These advantages make GCN a promising approach for tasks like speech emotion recognition, where capturing complex relationships and dependencies among speech features is crucial. successfully in SER, they rely heavily on manually engineered features and may struggle with capturing complex emotional expressions and long-term dependencies in speech.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Hubert Model",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Hubert (Hidden",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Ser Using Support Vector Machine",
      "text": "SVM is a supervised machine learning algorithm to classify data into different categories. In the paper,  [2]  a method is described by using a set of features, including spectral features (Mel-frequency cepstral coefficients or MFCCs), prosodic features (such as pitch and energy), and linguistic features (such as word count and type-token ratio) as input for the SVM classifier which is trained to predict the emotion of the speech signal. The author also achieved an accuracy of 75% on 5-class emotion recognition. SVM can actually capture relevant characteristics of speech signals if a set of features is suitable but it can't be always the case. For high-dimensional features, can cause difficult to apply SVM, as high-dimensionality may degrade the performance of the model). Also as SVMs are more pruned for linear decision boundaries, this may limit the ability for non-linear relationships between input features and output labels.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Ser Using Hidden Markov Model",
      "text": "HMM is basically a probabilistic model to determine the sequence in the speech signals and is helpful to represent the sequence of observations of the signals. In the work,  [13]  author was able to capture total accuracy of 72% with a 5-class classification task. But limited modeling capacity, as the model's current state, depends on the previous state. Computationally intensive, when the number of states of the model is large, the data is high-dimensional, causing difficulty to apply on real-time applications.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Ser Using Gaussian Mixture Model",
      "text": "This work,  [3]  is also similar to HMM with improvement in handcrafted acoustic feature analysis and probabilistic classification of emotion but also described the difference between male and female emotional expressions. The below figure explains the idea of Males having more recognition rate than females in 5 Gaussian components. The limitation of receiving contextual information yields the need for using more robust and efficient models though.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Speech Emotion Recognition Using Deep Learning Approaches",
      "text": "This review work  [1]  shows recent different approaches for SER using deep learn-",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Ser Using Generic Deep Neural Network[1]",
      "text": "Generic Deep learning approaches basically consist of a bunch of hidden layers, including input layer and output layers to correctly identify weights and features",
      "page_start": 74,
      "page_end": 74
    },
    {
      "section_name": "Ser Using Deep Autoencoder",
      "text": "The autoencoder  [14]  technique contributes to having an encoder and decoder in the network, where the encoder is used to capture hidden feature information of  input speech data, and the decoder is used to reduce the error percentage of the network.  Then pooling layer does different types of pooling mechanisms including average pooling, stochastic pooling, average pooling, etc. Recently, max pooling showed better performance, hence max pooling technique is now broadly used for the pooling layer due to its structural rigidity of slight variations in speech utterances.\n\nThen comes the fully connected layer after many convolutional and pooling layers.\n\nFully connected layers connected with classifiers take the master feature vector for classification then successfully classify emotional expressions.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Ser Using Recurrent Neural Network",
      "text": "Speech can be represented as a 2-D graph of frequency and time. That's why speech databases are also known as time series data. Sequential information is important hence. Temporal features thus can be captured by sequential representation understanding models like RNN  [15] . Different variations of RNN are now used frequently like Long Short Term Memory (LSTM) for capturing long-term temporal dependencies of speech. Though LSTM proves to provide better performance for speech, it eventually contributes to the complexity and exploding gradient problems.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Ser Using Evolution Of Deep Learning Techniques",
      "text": "Over the years, DL techniques  [6]  for SER improved hugely and it can be broadly categorized into three, Generative, Discriminative, and Hybrid. In recent times due to the complexity of Discriminative models, Hybrid models thus perform more effectively than others. Taxonomy is given below for the evolution of models of SER.",
      "page_start": 76,
      "page_end": 76
    },
    {
      "section_name": "Ser Using Different Benchmark Literatures Of Recent Times",
      "text": "This paper  [7]  explores the impact of word error rate (WER) on the performance of acoustic-linguistic speech emotion recognition models. The authors analyze the performance of deep learning models for speech emotion recognition on a dataset with different levels of WER and compare their results to those obtained using traditional machine learning approaches. They find that the performance of deep learning models is more robust to WER than that of traditional machine learning approaches and that these models are able to achieve good performance even when the WER is high. The authors also explore the impact of different types of errors on the performance of the models and find that recognition errors that affect the content of the speech signal has a larger impact on the performance of the models than errors that do not affect the content. Overall, this paper suggests that deep learning models are more robust to WER than traditional machine learning approaches for speech emotion recognition, and highlights the importance of considering the impact of WER on the performance of these models. It also suggests that errors that affect the content of the speech signal may have a larger impact on the performance of these models than other types of errors.\n\nThe features act as the inputs for Support Vector Machines, which form the basis for our SER analysis (SVMs). Since IEMOCAP is established and suitable for comparison with state-of-the-art approaches, it is the main reason we chose it.\n\nAdditionally, IEMOCAP contains transcriptions that make it easier to evaluate how the ASR's WER impacted the final emotion classification.\n\nThis paper  [8]  describes the creation of a multi-lingual benchmark dataset for speech emotion recognition (SER). The authors describe the process of collecting and annotating a large dataset of speech samples in five different languages (English, Spanish, French, German, and Italian) with a total of 7,659 utterances.\n\nThe dataset includes a range of emotional states, including neutral, happy, sad, angry, and fearful, and was annotated using multiple annotators to ensure highquality labels. The authors also describe the performance of several state-of-the-art SER models on the dataset and compare their results to those obtained on other datasets. They find that the models achieve good performance on the SERAB dataset and that the performance is similar across the different languages. Overall, this paper presents a new benchmark dataset for SER that includes a wide range of emotional states and languages and provides a useful resource for researchers working in this field. It also demonstrates the effectiveness of state-of-the-art SER models on this dataset and highlights the potential for cross-lingual generalization of these models.\n\nThis paper  [9]  presents a new approach for speech emotion recognition (SER) using a multimodal transformer model. The authors propose a multimodal transformer model that takes both speech and text as input and learns to extract features from both modalities. The model includes a mutual correlation module that captures the correlation between the speech and text modalities and allows the model to It demonstrates the effectiveness of the proposed approach and suggests that it could be a useful tool for SER in practical settings.\n\nThis paper  [10]  presents a new multimodal approach for speech emotion recogni-  both speech and text modalities and uses a multi-level attention mechanism to learn from the two modalities simultaneously. It demonstrates the effectiveness of the proposed approach and suggests that it could be a useful tool for SER in practical settings. This paper  [16]  presents a new approach for speech emotion recognition (SER) using pre-trained language models. The authors propose a bimodal SER model that takes both speech and text as input and uses pre-trained language models to extract features from each modality. The model includes a multimodal fusion layer that combines the features from the two modalities and feeds them into a classification layer to predict the emotional state of the speech signal. The authors evaluate the performance of the proposed model on a speech emotion recognition dataset and compare it to several baseline models. They find that the bimodal SER model outperforms the baselines and achieves good performance on the task.\n\nHowever, this paper provides a new approach for SER that takes advantage of both speech and text modalities and uses pre-trained language models to extract features from the two modalities.\n\nThis paper  [11]  presents a new end-to-end approach for speech emotion recognition (SER) using stacked transformer layers. The authors propose an SER model that consists of a series of stacked transformer layers that process the raw speech signal and learn to extract features from it. The model includes a self-attention mechanism that allows it to capture long-range dependencies in the speech signal and a multi-head attention mechanism that allows it to focus on different aspects of the input simultaneously. The authors evaluate the performance of the proposed model on a speech emotion recognition dataset and compare it to several baseline models. They find that the stacked transformer SER model outperforms the baselines and achieves good performance on the task. Moreover, this paper presents a new end-to-end approach for SER that uses stacked transformer layers to process the raw speech signal and learn to extract features from it. This paper  [5]  presents a new approach for speech emotion recognition (SER) using a parallel neural network that combines a residual convolutional neural network (ResNet-CNN) with a transformer network. In this paper, the authors propose a parallel SER model that takes the raw speech signal as input and processes it using both a ResNet-CNN and a transformer network. The output of the two networks is then combined using a fusion layer and fed into a classification layer to predict the emotional state of the speech signal. The authors evaluate the performance of the proposed model on a speech emotion recognition dataset and compare it to several baseline models. They find that the parallel SER model outperforms the baselines and achieves good performance on the task. Overall, this paper presents a new approach for SER that combines a ResNet-CNN and a transformer network to process the raw speech signal and learn to extract features from it. It demonstrates the effectiveness of the proposed approach and suggests that it could be a useful tool for SER in practical settings.\n\nIn this paper,  [17][18]  the authors propose two SER models that use different types of CNNs (1D and 2D) to extract features from the raw speech signal and an LSTM network to process the features over time. The models are trained endto-end to predict the emotional state of the speech signal. The authors evaluate the performance of the proposed models on a speech emotion recognition dataset and compare them to several baseline models. They find that the deep CNN LSTM SER models outperform the baselines and achieve good performance on the task. This paper provides two new approaches for SER that use deep CNN LSTM networks to process the raw speech signal and learn to extract features from it. It encapsulates the effectiveness of the suggested approaches and offers that they could be useful tools for SER in practical settings.\n\nThis paper  [12]  proposed a new approach for speech emotion recognition (SER) using a compact graph architecture. In this paper, the authors propose an SER model that takes the raw speech signal as input and processes it using a series of graph convolutional layers. The model includes a self-attention mechanism that allows it to capture long-range dependencies in the speech signal and a fusion layer that combines the features from the graph convolutional layers and feeds them into a classification layer to predict the emotional state of the speech signal.\n\nThe authors evaluate the performance of the proposed model on a speech emotion recognition dataset and compare it to several baseline models. They find that the compact graph SER model outperforms the baselines and achieves good performance on the task. Overall, this paper presents a new approach for SER that uses a compact graph architecture to process the raw speech signal and learn to extract features from it. It demonstrates the effectiveness of the proposed approach and suggests that it could be a useful tool for SER in practical settings. The paper \"Multimodal Speech Emotion Recognition and Classification Using Convolutional Neural Network Techniques\"  [19]  presents a method for recognizing emotions in speech using a multimodal approach, which combines information from multiple modalities such as audio, text, and facial expression. showed that the proposed method outperformed several state-of-the-art methods for speech emotion recognition. The results also showed that the multimodal approach improved the performance of the method compared to using only a single modality. The authors evaluate the performance of the proposed model on a speech emotion recognition dataset and compare it to several baseline models.\n\nThey find that the multimodal CNN SER model outperforms the baselines and achieves good performance on the task.\n\nChapter 4",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "Proposed Approach",
      "text": "The proposed methodology for our research involves the following steps. Firstly, we take the speech signal and create two separate modules: one for the spectral representation of the audio and another for the textual representation of the audio data. The textual module is processed using the Graph Convolution Network",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Dataset Preparation",
      "text": "We mainly used two popular datasets that are frequently used for emotion recognition tasks, IEMOCAP and RAVDESS. The below section describes more about these datasets.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Iemocap",
      "text": "The IEMOCAP  [20]     • Vocal channel (01 = speech, 02 = song).\n\n• Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n\n• Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n\n• Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n\n• Repetition (01 = 1st repetition, 02 = 2nd repetition).\n\n• Actor (01 to 24. Odd numbered actors are male, even-numbered actors are female).\n\nPreprocessing for the audio same goes likewise IEMOCAP. Basically, the pipeline is Audio preprocessing, Graph Construction, Node feature representation and graph convolution.\n\nExtract relevant audio features from the raw audio files in the RAVDESS dataset.\n\nCommonly used features for speech include Mel-frequency cepstral coefficients (MFCCs), pitch, energy, and spectrograms. These features capture the acoustic characteristics of the speech signals. These features are preprocessed by the internal encoder of HuBERT.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Gcn Module",
      "text": "• Building the graph:The graph building process involves the following steps:\n\n1. Preprocessing the text:Tokenize the sentence into individual words and apply any necessary text cleaning techniques such as removing punctuation, empty spaces, and converting words to lowercase.",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "2.",
      "text": "Creating word vectors: Use a pre-trained GloVe word embedding model to represent each word as a vector. This creates a numerical representation of the meaning or context of each word.\n\n3. Graph Construction: Each word vector becomes a node in the graph.\n\nConnect the nodes (words) with edges based on their relationships within the sentence. The relationships can be established through various techniques such as dependency parsing or syntactic analysis. For example, you can connect nodes that are adjacent to each other in the sentence, representing the sequential order of the words.\n\n-Nodes: Each word vector becomes a node in the graph. We have nodes for each word in the sentence.\n\n-Edges: Connect the nodes based on their relationships within the sentence. Let's assume we connect nodes that are adjacent to each other in the sentence.\n\n-Assigning weights to edges: We calculate the cosine similarity between word vectors to determine edge weights based on proximity.\n\nWe use values between 0 and 1, where a higher value indicates a stronger relationship.",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "Graph Representation:",
      "text": "The graph can be represented using an adjacency list or an adjacency matrix. The graph represents the relationships between individual words in the sentence, and the emotion labels assigned to the sentence can guide the analysis and interpretation of the graph to recognize the emotions expressed.\n\n• Graph convolution:The graph convolution operation and the pooling layer in graph convolutional networks work as below:\n\n1. Graph Convolution Operation:The graph convolution operation is defined using the formula:\n\n, where H hat represents the transformed node features.H is computed as\n\n, where U is the graph Fourier transform matrix.The propagation of graph convolution at the k-th layer is given by:\n\n. This formulation leverages the graph Fourier transform (GFT) and avoids computationally expensive eigen decomposition for arbitrary graph structures. The learnable parameters are the weights W in the convolution operation.\n\n2. Pooling Layer:The goal of the pooling layer is to obtain a graph-level representation from the node-level embeddings at the final layer. Common choices for graph pooling functions are mean, max, and sum pooling. Max and mean pooling may not preserve the underlying graph structure information effectively, while sum pooling has been shown to be a better alternative. The pooling operation aggregates the node-level embeddings to obtain a graph-level representation that can be further used for classification or downstream tasks. Overall, the graph convolution operation applies the learned convolution kernel to update node features based on their relationships in the graph, and the pooling layer aggregates node-level embeddings to obtain a graph-level representation for classification or other tasks.\n\n3. Classification:The pooling layer is followed by one fully-connected layer which produces the classification labels. Our GCN model is trained with cross-entropy loss.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Hubert Module",
      "text": "The fundamental principle is to convert speech data into a more \"language-like\" structure by identifying discrete hidden units (the Hu in the name). The words or tokens in a written sentence could be compared to these hidden units. We may use the same potent models, such BERT, that are available for natural language processing by representing speech as a series of discrete units.\n\nThe technique is based on the DeepCluster paper in computer vision, where images are classified into a predetermined number of clusters before being used again as \"pseudo-labels\" for self-supervised model training. HuBERT uses short audio segments (25 milliseconds) instead of images for its clustering, and the generated clusters serve as the hidden units that the model will be taught to predict.\n\nTraining process The two parts of the training process alternate between creating pseudo-targets by clustering and trying to forecast them at hidden locations through prediction.\n\n•\n\nStep 1: Discover \"hidden units\" targets through clustering\n\nThe concealed units (pseudo-targets) must first be extracted from the audio's raw waveform. Each 25-millisecond audio fragment is divided into K clusters using the K-means algorithm. All audio frames associated to each recognized cluster will subsequently be given the unit label for the hidden unit that was created for it. Then, each hidden unit is mapped to its appropriate embedding vector, which can be utilized to produce predictions in the subsequent phase.\n\nThe most crucial clustering decision is which characteristics to turn the waveform into. Mel-Frequency Cepstral Coefficients (MFCCs), which have been demonstrated to be somewhat effective for speech processing, are employed for the first clustering phase. However, representations from a middle layer of the HuBERT transformer encoder (from the prior iteration) are reused for further clustering phases.\n\nThe HuBERT BASE model is only trained for two iterations, and during the second iteration, clustering is performed using the sixth transformer layer.\n\nThe ninth transformer layer from the second iteration of the BASE model is also used to train HuBERT LARGE and X-LARGE for a third iteration.\n\nTo capture targets with variable granularity (for example, vowel/consonant vs. sub-phone states), we experiment with combining clustering of various sizes as well as multiple clustering with various numbers of clusters. They demonstrate that employing cluster ensembles can marginally enhance performance. The original publication contains additional information on this.",
      "page_start": 51,
      "page_end": 52
    },
    {
      "section_name": "• Step 2: Predict Noisy Targets From The Context",
      "text": "The second step involves training with the masked language modeling aim, just like with the original BERT. The model is requested to forecast the objectives at positions where about 50% of the transformer's encoder input features are hidden. Prediction logits are calculated for this by computing the cosine similarity between the transformer outputs (projected to a lower dimension) and each hidden unit embedding from all feasible hidden units.\n\nThen, incorrect predictions are punished using cross-entropy loss. Since it has been demonstrated to perform better when utilizing noisy labels, the loss is only applied to the spots that are masked. By attempting to forecast only masked targets, only unmasked targets, or both at once, the authors experimentally demonstrate this.",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "Score Level Fusion",
      "text": "In score-level fusion, the predicted scores from different modalities (in this case, GCN for textual representation and HuBERT for spectral representation) are combined to make a final prediction. The softmax function is commonly used in score-level fusion to convert the raw scores into probability distributions over the possible classes. Here's the step-by-step process of using softmax for score-level fusion:\n\n1. Obtain the predicted scores from the GCN and HuBERT models for each emotion class.\n\nExample:\n\nGCN predictions: [0. Finally, we make the final prediction based on the class with the highest probability.\n\nIn this case, the class with the highest probability is chosen as the final predicted emotion class. Using the softmax function in score-level fusion ensures that the predicted scores are transformed into meaningful probabilities, allowing for easier interpretation, decision-making, and model calibration.\n\nChapter 5",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Experimental Design",
      "text": "This chapter discusses all the experiments we did conduct throughout our research, like dataset selection, preprocessing, model architecture, training and evaluation, cross-validation and hyperparameter tuning, and comprehensive analysis. We designed to experiment on both the HuBERT and GCN models as HuBERT is a very powerful model to capture the spectral information designed for speech and GCN is proven to gather contextual information of text. We thus experimented on both of the model in a special setup for both of the datasets.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Selection Of Baseline Models",
      "text": "We conducted our experimental performance on two baseline models, GCN and HuBERT. This subchapter describes the reasoning behind choosing these two models. GsCN performs better for gathering textual information due to its benefit for learning knowledge graph, KG  [22]  from texts. And thus it can enhance the contextual power of gathering contextual information from speech.\n\nOn the other hand, choosing HuBERT to capture spectral information rather than going for traditional deep learning approaches like CNN and LSTM is that Hu-BERT is a self-supervised speech model, separately designed for automatic speech recognition downstream tasks not too long ago. And also it is seen that wav2vec models tend to perform better than any other models for the SER tasks. HuBERT is an updated version of the wav2vec2.0 model and thus it can actually overcome the downstream task very accurately. Also as different types of emotion hit for different types of speech information, so capturing both spectral and contextual seemed a suitable task for our research.",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Experiments",
      "text": "We did two types of experiments in our task, one is traditional deep learning approaches for SER and GCN and HuBERT-based models. The below subsection describes it more.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Motivaiton Of The Experiment",
      "text": "The motivation for fusing two different types of architecture, specifically the By combining both models, the system may take use of the wav2vec2.0 model's skills in voice signal analysis and feature extraction as well as the text2graph model's aptitude for identifying linkages and dependencies within textual data. A system for emotion recognition that is more complete and reliable can be created by combining these two methods.\n\nAdditionally, the integration of various architectures attempts to improve the system's overall effectiveness in recognizing emotions. The system can have a more comprehensive grasp of emotions communicated in speech and text by combining the advantages of the two models. This can result in more accurate and consistent outcomes by improving the ability to recognize and interpret emotions.\n\nTo improve the precision and efficiency of the emotion identification system, the wav2vec2.0 model and the text2graph model were combined. The combination of these architectures enables a thorough analysis of both speech signals and textual data, utilizing each architecture's advantages to raise the system's total performance.\n\nAnother motivation for our work to have two types of experiment is getting the multimodality of speech data, one is in textual form and another is in spectral form.\n\nMultimodal speech emotion recognition are proven to have better performance than the single modality approach.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Experiment Setting And Pre-Processing",
      "text": "Experiment I\n\nIn the first phase of the experiment, we did work with HuBERT pre-trained model for the downstream task of speech emotion recognition, Though it is a pre-trained model to manage with a huge sized dataset we had to take the help of the computational resources. The environment we worked on was having the below specification\n\n• CPU: Intel Core i9 12th gen\n\n• RAM: 64 GB DDR5\n\n• GPU: NVIDIA GeForce RTX 3090\n\n• VRAM: 24+24 GB",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Preprocessing I",
      "text": "Preprocessing of the pipeline contains two part for two datasets.\n\n• IEMOCAP dataset contains 5 sessions of acted speech and video.\n\n• We had to set the locations of each session and conduct our experiment for each of the sessions.\n\n• As direct audio data are streamed to the pipeline, 5 fold test was performed.\n\n• RAVDESS dataset doesn't contain multiple sessions unlike IEMOCAP. So the full dataset was serially streamed to the HuBERT pipeline. And also RAVDESS is quite smaller than the IEMOCAP, so no sessions were needed.",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Experiment Ii",
      "text": "Now in the second part of the experiment, we worked with the GCN pipeline to have the stream of text to graph data.\n\nAnd the setting environment is our personal laptop configuration for experimenting with the GCN model. We worked with this experiment earlier HuBERT so the setup was in our personal laptop.\n\n• CPU: Intel Core i5 7th gen\n\n• RAM: 16 GB DDR4\n\n• GPU: Nvidia GeForce GTX 1050\n\n• VRAM: 4 GB",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Preprocessing Ii",
      "text": "As GCN is there for capturing the contextual information from speech texts, we need not to had much computational power like processing speech data or so. For both of the experiments, we used CUDA to optimize our computational power. Now about the preprocessing task, we had to preprocess both of the datasets for GCN and HuBERT pipeline, as GCN is there to work with textual data, we had to prepare transcripts for that. Below is some of the sample of the textual transcripts for the IEMOCAP dataset.",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Evaluation Metrics And Parameters",
      "text": "In this subsection, we will discuss about the evaluation metrics we used for our\n\nHuBERT pipeline and number of parameters that are generated from our pipeline. The diversity of the individual models helps the parallel model since each one may identify various elements or patterns in the data. By reducing model bias and raising overall prediction accuracy, this variety helps. And so the parallel approach takes advantage of the inherent diversity among the ensemble members in order to introduce randomness and regularization as opposed to depending on dropout.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Accuracy Matrices",
      "text": "In the context of our work, some of the accuracy matrices that are frequently used in this domain are:\n\n• Recognition Accuracy: This statistic assesses how well the voice emotion identification system distinguishes and categorizes emotions from speech signals. Typically, it is calculated as the proportion of correctly categorized emotion samples among all samples.\n\n• Confusion Matrix: The count of predicted emotion labels in comparison to real emotion labels is displayed in the confusion matrix, which is a tabular form. By showing the number of true positives, true negatives, false positives, and false negatives for each emotion class, it gives precise information on the system's performance. Numerous accuracy metrics, including as precision, recall, and F1-score for each emotion category, can be generated from the confusion matrix. discriminating between various emotion groups is indicated by a higher AUC.\n\n• Mean Accuracy: Measuring the mean of the recognition accuracies achieved for each individual emotion class yields the average accuracy. It gives a general assessment of the system's effectiveness across many emotion categories.\n\n• Precision, Recall, and F1-Score: These metrics are frequently employed in multi-class classification tasks to assess how well each emotion class is performing. Out of all cases projected as positive for a given emotion class, precision is the percentage of correctly predicted positive instances. The percentage of accurately predicted positive instances out of all actual positive instances is measured by the recall, also known as sensitivity. The F1-score, which provides a balanced assessment of the model's performance on a certain emotion class, is the harmonic mean of precision and recall. For our work, we used the below F-1 score to measure our accuracy for the proposed model, F 1 = 2 precision.recall precision+recall .",
      "page_start": 59,
      "page_end": 60
    },
    {
      "section_name": "Number Of Parameters",
      "text": "The number of parameters depends on the size of both of the models and how the input features are augmented with the models. For our GCN architecture, the parameter size was 30K in our experiment. For HuBERT model as this is a pre-trained model the parameter size is unknown to us but we used the base pre-trained model rather than the large model to mitigate the large parameter size and make the model a little bit lightweight.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Ser Models",
      "text": "Accuracy (ACC%) Attn-BLSTM 2016  [23]  59.33 RNN 2017  [24]  63.50 CRNN 2018  [25]  63.98 LSTM 2019  [26]  58.72 CNN-LSTM 2019  [26]  59.23 Ours 62.58",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Ser Models",
      "text": "Accuracy (ACC%) SVM  [27]  79.10 AlexNet 2021  [28]  61.67 CNN-14 2021  [28]  76.58 TIM-Net 2022  [29]  92.08 Wav2Vec2.0 2021  [30]  81.82 Ours 80.34 Surprisingly in the existing literatures, Wav2Vec2.0 model showed better performance among all the models, for our case HuBERT also showed comparative performance for RAVDESS.\n\nFor the IEMOCAP dataset in the existing literature CRNN showed the best performance, for our case HuBERT performed better than GCN.\n\nNonetheless, increasing accuracy was not the main concern of our work rather capturing both long-term contextual and spectral information at the same time to make our pipeline more robust in the SER field was the actual contribution. In this figure, we can see that the epoch level reaching to 2, then the accuracy took a turn and went for less increasing way than before, so we had to early stop to get a comparable accuracy. That's why 3 epochs seemed right to us.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Gcn-Iemocap Experiment",
      "text": "Our conducted experiment on IEMOCAP dataset for GCN gave us the following results having compared with the existing work.\n\n• epoch: 45 • We experimented and chose to stay on 45 epochs as given more epochs our model was ovcerfitting.\n\n• As the IEMOCAP dataset was 5 fold splitting considering the 5 sessions, we had to also split and use the first 4 sessions as our training sessions and the last session as our testing session.\n\n• We had to use an early stopping mechanism having a counter of 10, which means that our model was not improving past 10.\n\n• The average training testing accuracy of the model gives an idea of how our model performs in general.",
      "page_start": 68,
      "page_end": 68
    },
    {
      "section_name": "Chapter 7",
      "text": "Conclusion and Future Work",
      "page_start": 69,
      "page_end": 69
    },
    {
      "section_name": "Conclusion",
      "text": "Techniques for multimodal speech emotion detection that accept input from both speech and text usually outperform those that employ only one modality because they can make use of the complimentary information each modality offers. Both speech and writing may offer a variety of clues about the speaker's emotional state, making them both valuable sources of information about a speaker's state of mind.\n\nFor instance, the text has semantic and grammatical indicators that can also give hints about the speaker's emotional state, whereas speech signals contain prosodic cues like pitch, intensity, and length that might transmit emotional information.\n\nMultimodal techniques frequently perform better than single-modality approaches because they may leverage the advantages of both modalities' information by integrating them.\n\nMultimodal speech emotion recognition methodologies may be applied in a variety of ways. One approach is to employ a single model that learns to extract characteristics from both voice and text through the utilization of input from both modalities. Using different models for voice and text and fusing their output is an alternative strategy.\n\nOverall, compared to single-modality approaches, multimodal speech emotion identification algorithms have the potential to produce more reliable and accurate findings, especially when one modality is noisy or absent. It's crucial to keep in mind, too, that multimodal approaches can also increase complexity and call for more data and processing power to develop and implement.\n\nTo imply our work, it can be used for our case combining the output of speech and textual data to increase the integrity of classification but that might be a complex process as have to process both speech and textual data and build different architectures to support those.\n\nTo conclude the idea of using a parallel GCN and HuBERT architecture is that first of all, we will be able to represent contextual features conveniently and robustly while processing with GCN, and at the same time, we will be using the fundamental theory of spectral Graph Convolutional Network, working with the Fourier domain to extract important emotional features. And the second idea of using HuBERT is its ability to capture long-range spectral dependencies in the speech signal by passing MFCC spectrograms to the network. One key idea to using parallelism of both of the networks is influenced by GoogleNet due to the \"inception modules,\" which are designed to capture both local and global features from the input image.\n\nThe inception modules consist of a series of convolutional and pooling layers that are applied in parallel, with the output of each layer being concatenated and fed into the next layer. This allows the network to learn from different types of features simultaneously and to capture more fine-grained information from the input image like MFCC spectrograms. Though there hasn't been much work on graph architecture for SER, we are hoping to implement some experiments toward our understanding and make some results.",
      "page_start": 69,
      "page_end": 70
    },
    {
      "section_name": "Future Work",
      "text": "In future research, there is an opportunity to further explore the potential of  Then pooling layer does different types of pooling mechanisms including average pooling, stochastic pooling, average pooling, etc. Recently, max pooling showed better performance, hence max pooling technique is now broadly used for the pooling layer due to its structural rigidity of slight variations in speech utterances.\n\nThen comes the fully connected layer after many convolutional and pooling layers.\n\nFully connected layers connected with classifiers take the master feature vector for classification then successfully classify emotional expressions.",
      "page_start": 70,
      "page_end": 75
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: 1: Proposed method for Speech Emotion Recognition using SVM[2]",
      "page": 31
    },
    {
      "caption": "Figure 3: 2: Accurate identification rate of different genders[3]",
      "page": 32
    },
    {
      "caption": "Figure 3: 3: Different Deep Learning Approaches for SER[1]",
      "page": 33
    },
    {
      "caption": "Figure 3: 4: Generic Deep Neural Network Architecture[1]",
      "page": 33
    },
    {
      "caption": "Figure 3: 5: Restricted Boltzmann Machine Architecture[1]",
      "page": 34
    },
    {
      "caption": "Figure 3: 6: Deep Belief Network Architecture[1]",
      "page": 34
    },
    {
      "caption": "Figure 3: 7: Deep Boltzmann Machine Architecture[1]",
      "page": 34
    },
    {
      "caption": "Figure 3: 8: Deep Autoencoder Architecture[1]",
      "page": 35
    },
    {
      "caption": "Figure 3: 9: Convolutional Neural Network Architecture[4]",
      "page": 35
    },
    {
      "caption": "Figure 3: 10: Recurrent Neural Network Architecture[5]",
      "page": 36
    },
    {
      "caption": "Figure 3: 11: Taxonomy of DL techniques for SER[6]",
      "page": 37
    },
    {
      "caption": "Figure 3: 12: A general overview of our emotion recognition by multi-modal fusion of",
      "page": 38
    },
    {
      "caption": "Figure 3: 13: SERAB evaluation pipeline. The (pre-trained/non-trainable)feature ex-",
      "page": 39
    },
    {
      "caption": "Figure 3: 14: The architecture of their proposed model. The right part illustrates the",
      "page": 40
    },
    {
      "caption": "Figure 3: 15: Overall architecture of the proposed MMFA-RNN model based SER.[10]",
      "page": 40
    },
    {
      "caption": "Figure 3: 16: The IEmoNet framework where each intermediate system (ASR, SER,",
      "page": 41
    },
    {
      "caption": "Figure 3: 17: Our proposed SER architecture.[11]",
      "page": 42
    },
    {
      "caption": "Figure 3: 18: Detailed architecture of CNN-BiLSTM module in the previous figure[11]",
      "page": 42
    },
    {
      "caption": "Figure 3: 19: Architecture of ResNet-CNN model.[5]",
      "page": 43
    },
    {
      "caption": "Figure 3: 20: Proposed graph-based architecture for SER consists of two graph con-",
      "page": 44
    },
    {
      "caption": "Figure 3: 21: Graph construction from the speech input. (a) LLDs are extracted as",
      "page": 44
    },
    {
      "caption": "Figure 4: 1: Our Proposed Architecture",
      "page": 46
    },
    {
      "caption": "Figure 4: 2: Our proposed GCN architecture",
      "page": 51
    },
    {
      "caption": "Figure 4: 3: Our Proposed HuBERT Architecture",
      "page": 53
    },
    {
      "caption": "Figure 5: 1: IEMCOAP text feature generation from speech",
      "page": 58
    },
    {
      "caption": "Figure 5: 2: IEMOCAP transcripts for each sessions",
      "page": 58
    },
    {
      "caption": "Figure 6: 1: RADVDESS HuBERT iteration for 4 specific emotion classes",
      "page": 64
    },
    {
      "caption": "Figure 6: 2: RADVDESS HuBERT ROC curve",
      "page": 65
    },
    {
      "caption": "Figure 6: 3: RADVDESS HuBERT Training testing for 4 specific emotion classes",
      "page": 66
    },
    {
      "caption": "Figure 6: 4: RADVDESS HuBERT Confusion Matrix for 4 specific emotion classes",
      "page": 67
    }
  ],
  "tables": [
    {
      "caption": "Table 4: 1: IEMOCAP Dataset Description",
      "data": [
        {
          "Column": "’session’",
          "Type": "int",
          "Values": "1, 2, 3, 4, 5",
          "Descriptions": "dialogue sessions in the\ndatabase"
        },
        {
          "Column": "’method’",
          "Type": "str",
          "Values": "’script’,\n’impro’",
          "Descriptions": "the method of emotion\nelicitation"
        },
        {
          "Column": "’gender’",
          "Type": "str",
          "Values": "’M’,\n’F’",
          "Descriptions": "gender of the speaker"
        },
        {
          "Column": "’emotion’",
          "Type": "str",
          "Values": "’neu’,\n’fru’,\n’sad’,\n’sur’,\n’ang’,\n’hap’,\n’exc’,\n’fea’,\n’dis’,’oth’",
          "Descriptions": "annotated emotion"
        },
        {
          "Column": "’n annotators’",
          "Type": "int",
          "Values": "-",
          "Descriptions": "number of annotators"
        },
        {
          "Column": "’agreement’",
          "Type": "int",
          "Values": "2, 3, 4",
          "Descriptions": "number of annotators who\nagree to this label"
        },
        {
          "Column": "’path’",
          "Type": "str",
          "Values": "’path/to/file/’",
          "Descriptions": "path to the .wav file, relative to\n”IEMOCAP full release/” directory"
        }
      ],
      "page": 47
    },
    {
      "caption": "Table 6: 1: AccuracyTable;FeatureLevelFusion(FLF)andScoreLevelFusion(SLF)",
      "data": [
        {
          "Model/Dataset": "GCN",
          "IEMOCAP": "60.47%",
          "RAVDESS": "80.34%",
          "Inference Time": "∼35, ∼25 mins"
        },
        {
          "Model/Dataset": "HuBERT",
          "IEMOCAP": "62.58%",
          "RAVDESS": "78.01%",
          "Inference Time": "∼43, ∼23 mins"
        },
        {
          "Model/Dataset": "Score (Ours)",
          "IEMOCAP": "FLF\nSLF(max)",
          "RAVDESS": "FLF\nSLF(max)",
          "Inference Time": "—"
        },
        {
          "Model/Dataset": "",
          "IEMOCAP": "-\n62.58%",
          "RAVDESS": "-\n80.34%",
          "Inference Time": ""
        }
      ],
      "page": 62
    },
    {
      "caption": "Table 6: 2: SER results and comparison on the IEMOCAP dataset",
      "data": [
        {
          "SER Models": "Attn-BLSTM 2016[23]",
          "Accuracy (ACC%)": "59.33"
        },
        {
          "SER Models": "RNN 2017[24]",
          "Accuracy (ACC%)": "63.50"
        },
        {
          "SER Models": "CRNN 2018[25]",
          "Accuracy (ACC%)": "63.98"
        },
        {
          "SER Models": "LSTM 2019[26]",
          "Accuracy (ACC%)": "58.72"
        },
        {
          "SER Models": "CNN-LSTM 2019[26]",
          "Accuracy (ACC%)": "59.23"
        },
        {
          "SER Models": "Ours",
          "Accuracy (ACC%)": "62.58"
        }
      ],
      "page": 63
    },
    {
      "caption": "Table 6: 2: SER results and comparison on the IEMOCAP dataset",
      "data": [
        {
          "SER Models": "SVM[27]",
          "Accuracy (ACC%)": "79.10"
        },
        {
          "SER Models": "AlexNet 2021[28]",
          "Accuracy (ACC%)": "61.67"
        },
        {
          "SER Models": "CNN-14 2021[28]",
          "Accuracy (ACC%)": "76.58"
        },
        {
          "SER Models": "TIM-Net 2022[29]",
          "Accuracy (ACC%)": "92.08"
        },
        {
          "SER Models": "Wav2Vec2.0 2021[30]",
          "Accuracy (ACC%)": "81.82"
        },
        {
          "SER Models": "Ours",
          "Accuracy (ACC%)": "80.34"
        }
      ],
      "page": 63
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning approaches for speech emotion recognition: State of the art and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "F Hanif",
        "G Mujtaba"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "M Jain",
        "S Narayan",
        "P Balaji",
        "A Bhowmick",
        "R Muthu"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition using support vector machine",
      "arxiv": "arXiv:2002.07590"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using gaussian mixture model",
      "authors": [
        "X Cheng",
        "Q Duan"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 International Conference on Computer Application and System Modeling (ICCASM 2012)",
      "doi": "10.2991/iccasm.2012.311"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using convolution neural networks",
      "authors": [
        "K Chauhan",
        "K Sharma",
        "T Varma"
      ],
      "year": "2021",
      "venue": "2021 international conference on artificial intelligence and smart systems (ICAIS)"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition with a resnet-cnntransformer parallel neural network",
      "authors": [
        "S Han",
        "F Leng",
        "Z Jin"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "7",
      "title": "On the impact of word error rate on acoustic-linguistic speech emotion recognition: an update for the deep learning era",
      "authors": [
        "S Amiriparian",
        "A Sokolov",
        "I Aslan",
        "L Christ",
        "M Gerczuk",
        "T Hübner",
        "D Lamanov",
        "M Milling",
        "S Ottl",
        "I Poduremennykh"
      ],
      "year": "2021",
      "venue": "On the impact of word error rate on acoustic-linguistic speech emotion recognition: an update for the deep learning era",
      "arxiv": "arXiv:2104.10121"
    },
    {
      "citation_id": "8",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "Learning mutual correlation in multimodal transformer for speech emotion recognition",
      "authors": [
        "Y Wang",
        "G Shen",
        "Y Xu",
        "J Li",
        "Z Zhao"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "10",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "A novel end-toend speech emotion recognition network with stacked transformer layers",
      "authors": [
        "X Wang",
        "M Wang",
        "W Qi",
        "W Su",
        "X Wang",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Compact graph architecture for speech emotion recognition",
      "authors": [
        "A Shirian",
        "T Guha"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "14",
      "title": "Deep learning approaches for speech emotion recognition",
      "authors": [
        "A Bhavan",
        "M Sharma",
        "M Piplani",
        "P Chauhan",
        "R Hitkul",
        "Shah"
      ],
      "year": "2020",
      "venue": "Deep learning approaches for speech emotion recognition"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using recurrent neural networks with directional self-attention",
      "authors": [
        "D Li",
        "J Liu",
        "Z Yang",
        "L Sun",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Multimodal speech emotion recognition and classification using convolutional neural network techniques",
      "authors": [
        "A Christy",
        "S Vaithyasubramanian",
        "A Jesudoss",
        "M Praveena"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "22",
      "title": "Knowledge graph and knowledge reasoning: A systematic review",
      "authors": [
        "L Tian",
        "X Zhou",
        "Y.-P Wu",
        "W.-T Zhou",
        "J.-H Zhang",
        "T.-S Zhang"
      ],
      "year": "2022",
      "venue": "Journal of Electronic Science and Technology"
    },
    {
      "citation_id": "23",
      "title": "Attention assisted discovery of subutterance structure in speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech",
      "arxiv": "arXiv:1904.03833"
    },
    {
      "citation_id": "27",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion recognition on ravdess dataset using transfer learning",
      "authors": [
        "C Luna-Jiménez",
        "D Griol",
        "Z Callejas",
        "R Kleinlein",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "C Luna-Jiménez",
        "R Kleinlein",
        "D Griol",
        "Z Callejas",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    }
  ]
}