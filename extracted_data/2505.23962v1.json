{
  "paper_id": "2505.23962v1",
  "title": "Can Emotion Fool Anti-Spoofing?",
  "published": "2025-05-29T19:32:57Z",
  "authors": [
    "Aurosweta Mahapatra",
    "Ismail Rasim Ulgen",
    "Abinay Reddy Naini",
    "Carlos Busso",
    "Berrak Sisman"
  ],
  "keywords": [
    "Anti-Spoofing",
    "Speech Deepfake",
    "Text-to-Speech",
    "Ensemble Modeling",
    "Emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditional anti-spoofing focuses on models and datasets built on synthetic speech with mostly neutral state, neglecting diverse emotional variations. As a result, their robustness against high-quality, emotionally expressive synthetic speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of emotional text-to-speech samples. Our analysis shows existing anti-spoofing models struggle with emotional synthetic speech, exposing risks of emotion-targeted attacks. Even trained on emotional data, the models underperform due to limited focus on emotional aspect and show performance disparities across emotions. This highlights the need for emotion-focused anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated ensemble of emotionspecialized models with a speech emotion recognition gating network. GEM performs effectively across all emotions and neutral state, improving defenses against spoofing attacks. We release the EmoSpoof-TTS Dataset 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Anti-spoofing in the speech domain focuses on detecting spoofed speech produced through replay techniques, speech synthesis, and voice conversion  [1] . Among these, the misuse of synthetic speech  [1, 2]  has become a growing concern, especially with advancements in emotionally expressive text-tospeech (TTS) models.\n\nRecent TTS models can effectively capture a range of emotions, making synthetic speech increasingly realistic  [3] [4] [5] [6] [7] [8] . Furthermore, zero-shot TTS models  [9] [10] [11]  have demonstrated the ability to produce realistic synthetic speech from just a few seconds of reference audio while effectively capturing human emotions. While state-of-the-art (SOTA) anti-spoofing models  [12, 13]  perform relatively well in detecting neutral synthetic speech, their effectiveness against these emotional TTS models remains uncertain. This necessitates robust countermeasures against emotion-targeted attacks, where an attacker exploits the model's sensitivity to a particular emotion.\n\nVarious challenges, such as the Automatic Speaker Verification and Spoofing Countermeasures Challenge (ASVspoof)  [14]  and the Audio Deep Synthesis Detection (ADD) Challenge  [15, 16] , have been introduced to raise awareness about anti-spoofing. These initiatives have led to the development of large-scale datasets, including ASVspoof 2019  [17] , ASVspoof 2021  [18] , and ASVspoof 2024  [19] , which serve as benchmarks in this field. However, these datasets primarily contain 1 EmoSpoof-TTS Dataset: https://emospoof-tts.github.io/Dataset/ speech in a neutral state and do not specifically address the emotional aspects of speech, leaving a gap in the study of emotiondriven spoofing attacks.\n\nThe ASVspoof and ADD challenges, along with large-scale datasets, have significantly advanced the development of antispoofing models. Traditional models  [20] [21] [22]  rely on handcrafted features for spoof speech detection. However, more recent end-to-end models like RawNet2  [12]  and AASIST  [13]  process raw audio waveforms directly, achieving highly promising results by eliminating the need for manually engineered features. However, even these SOTA models have primarily been trained and evaluated on traditional datasets  [17] [18] [19] , which focus on neutral speech and lack diverse emotional variations. As a result, current anti-spoofing models are not explicitly designed to account for the impact of emotion on spoof detection, making them vulnerable to emotionally expressive synthetic speech.\n\nIn this work, we focus on the neglected emotional aspect of anti-spoofing by exploring whether the emotions can fool anti-spoofing systems. We reveal the limitations of current antispoofing paradigm in terms of lacking both suitable datasets and emotion tailored methods. To address this limitations, we build an emotionally rich anti-spoofing dataset and propose an anti-spoofing method specifically designed to tackle emotional speech to mitigate the risks associated with the emotiontargeted attacks.\n\nWe introduce EmoSpoof-TTS, a diverse dataset containing over 29 hours of emotionally expressive synthetic speech generated using recent TTS models. Our findings reveal that current anti-spoofing models are unreliable in scenarios involving emotional speech and exhibit performance disparities across different emotions. In addition, to improve the performance on emotionally expressive synthetic speech, we propose the Gated Ensemble Method (GEM). GEM leverages multiple emotion-specialized anti-spoofing models with a Speech Emotion Recognition (SER) model, which acts as a gating mechanism to enhance its performance in detecting high-quality, emotionally expressive speech while also addressing performance disparities across emotions. Our key contributions can be summarized as:\n\n• We identify and highlight the significant limitations of existing anti-spoofing models on emotionally expressive synthetic speech and expose a new risk of emotion-targeted attacks. • We introduce and release EmoSpoof-TTS, a corpus of emotionally expressive synthetic speech generated using recent TTS models. • We propose the Gated Ensemble Method (GEM), which significantly improves emotional synthetic speech detection, highlighting the need to prioritize emotion in anti-spoofing design.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "State-of-the-art anti-spoofing models, such as RawNet2  [12]  and AASIST  [13] , are well known for their strong benchmark performance. These models employ an end-to-end architecture, eliminating the need for hand-crafted feature extraction and simplifying implementation. In this study, we select RawNet2 as our baseline anti-spoofing model due to its robust performance. As illustrated in Figure  1 , RawNet2 comprises a Sinc-Net layer, residual blocks, a GRU layer, fully connected layers, and an output layer. Most prior studies utilize the publicly available pre-trained RawNet2 model, trained on the ASVspoof 2019 dataset  [17] . We note that as TTS continues to advance, the ASVspoof datasets-despite their scale and popularity-lack emotional speech samples. This limitation raises concerns about whether high-performing anti-spoofing models remain effective against increasingly emotional synthetic speech. Recently, EmoFake  [23]  was introduced as an emotional fake dataset generated via Emotional Voice Conversion (EVC) techniques. While valuable, EmoFake focuses on voice conversion and does not encompass emotional text-to-speech (TTS) synthesis. To bridge this gap, we introduce a new dataset, EmoSpoof-TTS, and utilize it to assess and enhance antispoofing robustness against advanced synthesis techniques and emotionally expressive speech. Further details are provided in Section 3.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Anti-Spoofing Vs Emotion",
      "text": "The state-of-the-art anti-spoofing models are primarily evaluated on traditional datasets, which lack emotionally expressive speech and do not account for recent advancements in speech synthesis. As a result, they are unreliable with emotional speech, making them vulnerable to emotion-targeted attacks. This section introduces EmoSpoof-TTS and compares anti-spoofing model performance on traditional datasets versus EmoSpoof-TTS.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emospoof-Tts",
      "text": "We propose EmoSpoof-TTS to explore the impact of emotions in anti-spoofing. This dataset focuses on spoofed speech generated by zero-shot TTS models-StyleTTS2  [9] , F5-TTS  [10] , and CosyVoice  [11] -known for producing high-quality, realistic speech even with a short reference audio. These TTS models requires emotional reference speech in order to synthesize emotional speech. In this work, we have utilized The Emotional Speech Database (ESD)  [24]  for this purpose. ESD dataset consists of 10 speakers expressing five emotions (Happiness, Anger, Sadness, Neutral, etc) with 350 utterances per emotion. For speech synthesis, we separate 50 utterances per speaker and emotion as reference speech. Using these reference speech, we synthesize 300 emotional speech samples which have the same content, speaker identity and emotion with the 300 bona-fide speech samples in ESD per speaker and emotion category.\n\nWe consider four basic emotions: Happiness, Anger, Sadness, and Neutral state. EmoSpoof-TTS contains a total of 36,000 synthesized speech samples from 4 emotions, 10 (5 male, 5 female) speakers and 3 TTS models  [9] [10] [11]  which is parallel to 12,000 bona-fide samples from ESD. EmoSpoof-TTS featuring multiple emotions, speakers, and TTS models, provides a valuable resource for studying emotional speech in antispoofing. We will publicly release the dataset to foster research in this area.  We then evaluated the pre-trained RawNet2 model on EmoSpoof-TTS, using spoof samples from four speakers while maintaining speaker consistency across all three TTS models. The results, shown in Table  2 , are reported for individual emotions and a combined category (Happiness, Anger, and Sadness, referred to as HAS). The HAS category allows us to assess the model's performance on emotionally expressive speech compared to neutral speech, while individual emotions highlight detection performance across different emotional states.\n\nThe evaluation presented in Table  2  provides insights into the vulnerabilities of the pre-trained RawNet2 model when exposed to high-quality, emotionally expressive synthetic speech. The model struggles to detect fake samples from EmoSpoof-TTS, with EER values higher than those for the ASVspoof 2024 dataset across all TTS models. The EER for HAS across all three models is consistently higher than for Neutral. For instance, the EER for HAS in the case of StyleTTS2 is 44.03%, whereas for Neutral, it is 39%. This indicates lower confidence of the anti-spoofing model in distinguishing between spoofed and bona-fide speech when it is exposed to fake emotional speech. These results reinforce our claim that SOTA antispoofing models, despite performing well on traditional datasets like ASVspoof 2019 and ASVspoof 2021, struggle with speech samples from recent emotionally expressive TTS models, especially when handling emotional speech, making them unreliable in such scenarios. Additionally, our analysis reveals that EER values vary across emotion categories; for instance StyleTTS2, the EER for Happiness is 45.00%, for Sadness is 46.75%, and for Anger is 35.17%.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Finetuning Rawnet2 On Emospoof-Tts",
      "text": "Given the fact that, the pre-trained anti-spoofing method struggles to generalize to other datasets, we explored the performance when the model is trained with EmoSpoof-TTS. For this purpose, we fine-tuned the pretrained RawNet2 on the EmoSpoof-TTS using the train, validation and test subsets, which are described in Section 5.1. We structured the TTS models and speaker splits to ensure the training, validation, and test sets have no overlap, ensuring a fair evaluation. Results in Table  3  indicate a significant improvement in Emo-RawNet2's performance over the pre-trained RawNet2 model. For instance, with StyleTTS2, the EER for HAS in the pre-trained RawNet2 is 44.03%, which drops to 10.36% for Emo-RawNet2. However, performance disparities across emotions persist, as reflected in the EER values: 8.00% for Happiness, 13.25% for Sadness, and 5.83% for the Neutral state. This uneven performance across different emotions suggests that attackers could exploit these inconsistencies by targeting emotions to which the model is more sensitive, increasing the likelihood of successful attacks. Moreover, the results in Table  3  highlights that while fine-tuning an existing model improves overall performance, it fails to resolve the disparity across emotions. Therefore, defending against the vulnerability of anti-spoofing models to emotionally expressive speech does not only require a useful dataset but also anti-spoofing paradigms specifically tailored for detecting emotional speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "As discussed in Section 3, the performance of traditional antispoofing on emotional synthetic speech degrades significantly. Additionally, the uneven performance of anti-spoofing models across different emotions presents a security risk, as attackers may exploit the model's vulnerability to specific emotional categories. To mitigate these vulnerabilities and enhance robustness against emotion-targeted attacks, this section introduces a novel approach: the Gated Ensemble Method (GEM).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion-Specialized Anti-Spoofing Models",
      "text": "To build emotion-specialized anti-spoofing models, we use Emo-RawNet2, our general-purpose model described in section 3.2. We fine-tune Emo-RawNet2 on emotion-specific subsets of the EmoSpoof-TTS dataset, resulting in four emotionspecialized models: Model-H (Happiness), Model-A (Anger), Model-S (Sadness), and Model-N (Neutral state).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gated Ensemble Method (Gem)",
      "text": "During inference, we lack prior knowledge of the emotion in the input speech, so we must automatically select the corresponding specialized anti-spoofing model. We chose not to use a hard selection of models, as we believe that emotions are inherently interconnected, rather than orthogonal  [25] . To account for the relationships between different emotions, we propose incorporating contributions from multiple emotion-specialized models. This approach ensures a more holistic analysis, improving the model's ability to detect whether a sample is fake. Therefore, we introduce an ensemble of specialized models with a soft gating mechanism to achieve the optimal solution. The core idea of GEM is to ensemble predictions from specialized models in a way that gives the highest weight to the model associated with the emotion of the input signal x during final decision-making. Soft gating enables this by allowing the most relevant model to make the final judgment while still incorporating the contributions from the other models.\n\nFor a given input x, each specialized model Mi produces a score Si:\n\nwhere Si is the model's spoof prediction score for x.\n\nWe employ a recent the SER system  [26]  to determine the emotion probabilities of x, as illustrated in Figure  1(b) . The input x is fed to SER, which outputs the probabilities (Ei) for each emotion category:\n\nThe final decision score ys is computed as a weighted average of model scores, where weights are the emotion probabilities:\n\nWe control the softness of this weights by applying softmax with temperature T to the output logits of the SER model. Based on ys, the system determines whether the input signal is spoofed (y = 0) or authentic (y = 1).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "For the experiments in this paper, we partitioned the dataset into separate training, validation, and test sets based on speakers and TTS models to ensure a fair and unbiased evaluation: To obtain Emo-RawNet2, we fine-tune the pre-trained RawNet2 on the training and validation sets of EmoSpoof-TTS. For the emotion-specialized models, we extracted emotion-specific subsets from these sets and fine-tuned Emo-RawNet2 separately for each emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "We utilize the pre-trained RawNet2 with default parameters  [12] . During the fine-tuning process to obtain the generalpurpose model Emo-RawNet2, we used a batch size of 32 and trained the model for 50 epochs. For emotion-specific finetuning, the batch size was reduced to 8, and training was extended to 100 epochs. Learning rate was set to 1e -4 in both the fine-tuning process. SER  [26]  is trained to classify 4 emotion categories (Neutral, Happiness, Anger, Sadness) on the MSP-Podcast dataset  [27] , using the default parameters and recipe provided. To ensure soft gating in the GEM, we set the temperature of the SER to 1.5.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "As a baseline, we consider the general-purpose model, Emo-RawNet2, which was fine-tuned with the full set of emotions from the EmoSpoof-TTS dataset, as described in Section 3.2. This allows us to analyze the effectiveness of our proposed method, which ensembles specialized models using a soft gating mechanism. Results in Table  4  presents the performance of our emotion-specialized models Model-N, Model-H, Model-A, Model-S. Each specialized model not only enhances detection for its respective target emotions but also demonstrates improvements in detecting other emotions. These results indicate the non-orthogonality  [25]  of emotions, as improving the model's performance on a particular emotion affects others.\n\nThe results presented in Table  4  demonstrate that GEM not only enhances the overall performance of the anti-spoofing model compared to Emo-RawNet2 but also mitigates the imbalance in spoof detection across different emotions. Its performance is notably more consistent across emotions compared to Emo-RawNet2 and individual emotion-specialized models. The results shows that proposed method (GEM) comes very close to the optimal performance of each specialzed model for its respective emotion category leading to a much improved overall performance and more balanced performance across different emotions. Another key strength of GEM is that it enhances spoof detection in emotional speech without compromising performance on neutral speech. The above results demonstrate that handling spoof detection for all emotions together is a complex task for a single anti-spoofing model, resulting in suboptimal performance. In contrast, using multiple models specialized for each emotion not only improves emotion understanding but also captures inter-emotional relationships, leading to better overall anti-spoofing performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we explored the effect of emotion on anti-spoofing, revealing emotion-related risks and the need of emotional aspect for this task in terms of both dataset and methodology. Our analysis showed current anti-spoofing paradigm with datasets and methods without emotional focus struggle with emotion-targeted attacks. To facilitate research in this domain, firstly we introduced EmoSpoof-TTS, a corpus created using recent text-to-speech (TTS) models to benchmark and enhance anti-spoofing performance on emotional synthetic speech. Furthermore, we proposed an anti-spoofing method specifically tailored for tackling emotional aspect of anti-spoofing. The proposed method, the Gated Ensemble Method (GEM), which leverages emotion-specialized models and a soft gating mechanism, improved anti-spoofing performance on emotional speech, validating the need for such methods. For future work, we aim to enhance anti-spoofing resilience to emotion-targeted attacks across diverse emotional states and TTS models while expanding EmoSpoof-TTS.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgment",
      "text": "This work is supported by NSF CAREER award IIS-2338979.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , RawNet2 comprises a Sinc-",
      "page": 2
    },
    {
      "caption": "Figure 1: Architecture of RawNet2",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed Gated Ensemble Method (GEM)",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 4: presents the performance",
      "data": [
        {
          "EmoSpoof-TTS (test set)": "HAS"
        },
        {
          "EmoSpoof-TTS (test set)": "7.89\n6.03\n11.36\n7.89"
        },
        {
          "EmoSpoof-TTS (test set)": "6.22"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Battling voice spoofing: a review, comparative analysis, and generalizability evaluation of state-of-the-art voice spoofing counter measures",
      "authors": [
        "A Khan",
        "K Malik",
        "J Ryan",
        "M Saravanan"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "3",
      "title": "Deepfakes generation and detection: State-of-the-art, open challenges, countermeasures, and way forward",
      "authors": [
        "M Masood",
        "M Nawaz",
        "K Malik",
        "A Javed",
        "A Irtaza",
        "H Malik"
      ],
      "year": "2023",
      "venue": "Applied intelligence"
    },
    {
      "citation_id": "4",
      "title": "Emodiff: Intensity controllable emotional text-to-speech with soft-label guidance",
      "authors": [
        "Y Guo",
        "C Du",
        "X Chen",
        "K Yu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "N Cheng",
        "J Xiao",
        "J Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Prompttts++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions",
      "authors": [
        "R Shimizu",
        "R Yamamoto",
        "M Kawamura",
        "Y Shirahata",
        "H Doi",
        "T Komatsu",
        "K Tachibana"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Hierspeech: Bridging the gap between text and speech by hierarchical variational inference using self-supervised representations for speech synthesis",
      "authors": [
        "S.-H Lee",
        "S.-B Kim",
        "J.-H Lee",
        "E Song",
        "M.-J Hwang",
        "S.-W Lee"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Voicebox: Text-guided multilingual universal speech generation at scale",
      "authors": [
        "M Le",
        "A Vyas",
        "B Shi",
        "B Karrer",
        "L Sari",
        "R Moritz",
        "M Williamson",
        "V Manohar",
        "Y Adi",
        "J Mahadeokar",
        "W.-N Hsu"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "S Chen",
        "C Wang",
        "Y Wu",
        "Z Zhang",
        "L Zhou",
        "S Liu",
        "Z Chen",
        "Y Liu",
        "H Wang",
        "J Li",
        "L He",
        "S Zhao",
        "F Wei"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models",
      "authors": [
        "Y Li",
        "C Han",
        "V Raghavan",
        "G Mischler",
        "N Mesgarani"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "F5-tts: A fairytaler that fakes fluent and faithful speech with flow matching",
      "authors": [
        "Y Chen",
        "Z Niu",
        "Z Ma",
        "K Deng",
        "C Wang",
        "J Zhao",
        "K Yu",
        "X Chen"
      ],
      "year": "2024",
      "venue": "F5-tts: A fairytaler that fakes fluent and faithful speech with flow matching",
      "arxiv": "arXiv:2410.06885"
    },
    {
      "citation_id": "12",
      "title": "Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens",
      "authors": [
        "Z Du",
        "Q Chen",
        "S Zhang",
        "K Hu",
        "H Lu",
        "Y Yang",
        "H Hu",
        "S Zheng",
        "Y Gu",
        "Z Ma"
      ],
      "year": "2024",
      "venue": "Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens",
      "arxiv": "arXiv:2407.05407"
    },
    {
      "citation_id": "13",
      "title": "End-to-end anti-spoofing with rawnet2",
      "authors": [
        "H Tak",
        "J Patino",
        "M Todisco",
        "A Nautsch",
        "N Evans",
        "A Larcher"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Aasist: Audio anti-spoofing using integrated spectro-temporal graph attention networks",
      "authors": [
        "J.-W Jung",
        "H.-S Heo",
        "H Tak",
        "H -J. Shim",
        "J Chung",
        "B.-J Lee",
        "H.-J Yu",
        "N Evans"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Automatic speaker verification spoofing and countermeasures challenge evaluation plan",
      "authors": [
        "Z Wu",
        "T Kinnunen",
        "N Evans",
        "J Yamagishi"
      ],
      "year": "2014",
      "venue": "Training"
    },
    {
      "citation_id": "16",
      "title": "Add 2022: the first audio deep synthesis detection challenge",
      "authors": [
        "J Yi",
        "R Fu",
        "J Tao",
        "S Nie",
        "H Ma",
        "C Wang",
        "T Wang",
        "Z Tian",
        "Y Bai",
        "C Fan",
        "S Liang",
        "S Wang",
        "S Zhang",
        "X Yan",
        "L Xu",
        "Z Wen",
        "H Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Add 2023: the second audio deepfake detection challenge",
      "authors": [
        "J Yi",
        "J Tao",
        "R Fu",
        "X Yan",
        "C Wang",
        "T Wang",
        "C Zhang",
        "X Zhang",
        "Y Zhao",
        "Y Ren"
      ],
      "year": "2023",
      "venue": "Add 2023: the second audio deepfake detection challenge",
      "arxiv": "arXiv:2305.13774"
    },
    {
      "citation_id": "18",
      "title": "Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech",
      "authors": [
        "X Wang",
        "J Yamagishi",
        "M Todisco",
        "H Delgado",
        "A Nautsch",
        "N Evans",
        "M Sahidullah",
        "V Vestman",
        "T Kinnunen",
        "K Lee",
        "L Juvela",
        "P Alku",
        "Y.-H Peng",
        "H.-T Hwang",
        "Y Tsao",
        "H.-M Wang",
        "S Maguer",
        "M Becker",
        "F Henderson",
        "R Clark",
        "Y Zhang",
        "Q Wang",
        "Y Jia",
        "K Onuma",
        "K Mushika",
        "T Kaneda",
        "Y Jiang",
        "L.-J Liu",
        "Y.-C Wu",
        "W.-C Huang",
        "T Toda",
        "K Tanaka",
        "H Kameoka",
        "I Steiner",
        "D Matrouf",
        "J.-F Bonastre",
        "A Govender",
        "S Ronanki",
        "J.-X Zhang",
        "Z.-H Ling"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "19",
      "title": "Asvspoof 2021: Towards spoofed and deepfake speech detection in the wild",
      "authors": [
        "X Liu",
        "X Wang",
        "M Sahidullah",
        "J Patino",
        "H Delgado",
        "T Kinnunen",
        "M Todisco",
        "J Yamagishi",
        "N Evans",
        "A Nautsch",
        "K Lee"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Asvspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale",
      "authors": [
        "X Wang",
        "H Delgado",
        "H Tak",
        "J.-W Jung",
        "H -J. Shim",
        "M Todisco",
        "I Kukanov",
        "X Liu",
        "M Sahidullah",
        "T Kinnunen"
      ],
      "year": "2024",
      "venue": "Asvspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale",
      "arxiv": "arXiv:2408.08739"
    },
    {
      "citation_id": "21",
      "title": "Light convolutional neural network with feature genuinization for detection of synthetic speech attacks",
      "authors": [
        "Z Wu",
        "R Das",
        "J Yang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Light convolutional neural network with feature genuinization for detection of synthetic speech attacks",
      "arxiv": "arXiv:2009.09637"
    },
    {
      "citation_id": "22",
      "title": "Assert: Antispoofing with squeeze-excitation and residual networks",
      "authors": [
        "C.-I Lai",
        "N Chen",
        "J Villalba",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Assert: Antispoofing with squeeze-excitation and residual networks",
      "arxiv": "arXiv:1904.01120"
    },
    {
      "citation_id": "23",
      "title": "Spoof detection using voice contribution on lfcc features and resnet-34",
      "authors": [
        "K Mon",
        "K Galajit",
        "C Mawalim",
        "J Karnjana",
        "T Isshiki",
        "P Aimmanee"
      ],
      "year": "2023",
      "venue": "2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Emofake: An initial dataset for emotion fake audio detection",
      "authors": [
        "Y Zhao",
        "J Yi",
        "J Tao",
        "C Wang",
        "Y Dong"
      ],
      "year": "2024",
      "venue": "China National Conference on Chinese Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
      "authors": [
        "H.-C Chou",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier"
    },
    {
      "citation_id": "27",
      "title": "Odyssey 2024-speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Development"
    },
    {
      "citation_id": "28",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}