{
  "paper_id": "2101.01356v2",
  "title": "Fixed-Maml For Few Shot Classification In Multilingual Speech Emotion Recognition",
  "published": "2021-01-05T05:51:50Z",
  "authors": [
    "Anugunj Naman",
    "Chetan Sinha",
    "Liliana Mancini"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we analyze the feasibility of applying few-shot learning to speech emotion recognition task (SER). The current speech emotion recognition models work exceptionally well but fail when then input is multilingual. Moreover, when training such models, the models' performance is suitable only when the training corpus is vast. This availability of a big training corpus is a significant problem when choosing a language that is not much popular or obscure. We attempt to solve this challenge of multilingualism and lack of available data by turning this problem into a few-shot learning problem. We suggest relaxing the assumption that all N classes in an N-way K-shot problem be new and define an N+F way problem where N and F are the number of emotion classes and predefined fixed emotion classes, respectively. We propose this modification to the Model-Agnostic Meta Learning (MAML) algorithm to solve the problem and call this new model F-MAML. This modification performs better than the original MAML and outperforms on EmoFilm dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a significant role in many intelligent interfaces  [1] . Even with the recent advances in machine learning, this is still a challenging task. The main reason behind this is that most publicly available annotated datasets in this domain are small in scale, which makes DL models prone to over-fitting. Another essential feature of emotion recognition is the inherent multi-modality in expressing emotions  [2] . Emotional information can be captured by studying many modalities, including facial expressions, body postures, and EEG  [3] . Of these, arguably, speech is the most accessible. In addition to accessibility, speech signals contain many other emotional cues  [4] . We, therefore, use speech signals as a base to predict the emotion.\n\nGenerally, in speech emotion recognition (SER) task, conventional supervised learning solves the problem efficiently given sufficient training data. Several studies on SER for different single corpora have been conducted using the language-dependent optimal acoustic sets over several decades. Such systems can be analyzed in monolingual scenarios; changing the source corpus requires re-selecting the optimal acoustic features and re-training the system. Human-emotion perception, however, has proved to be cross-lingual, even without the understanding of the language used  [5] . An SER system is expected to recognize emotions as such.\n\nHowever, for an automatic SER system to recognize emotion, there are two significant problems. First, the training corpus available for many different languages is very limited. Second, it is not clear which standard features are efficient in detecting emotions across different cultures. Commonalities and differences in human-emotion perception across languages in the valence-activation (V-A) space have recently been studied  [5] . It was revealed that direction and distance from neutral to other emotions are similar across languages, and languages' neutral positions are language-dependent. In this paper, motivated by the above challenges, we want to simulate a scenario where one can provide few labeled speech samples in any language and train a model on that language for few iterations to get a robust SER system. This proposed scenario removes the requirement for a large amount of data and identifies the standard features efficient in detecting emotions about that culture and fine-tune to it accordingly.\n\nSupervised learning has been extremely successful in computer vision, speech, or machine translation tasks, thanks to improvements in optimization technology, larger datasets, and streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this learning setup does not cover many aspects where learning is possible and desirable. One such instance is learning from very few examples in the so-called few-shot learning tasks  [6] . Rather than depending on regularization to compensate for the lack of training data, researchers have explored ways to leverage the distribution of similar tasks, inspired by human learning  [7] . A lot of useful solutions have been developed, and the most popular solution right now uses meta-learning.\n\nMeanwhile, most of the studies on few-shot learning are conducted on image tasks. We here attempt to apply those meta-learning solutions to SER systems. We formulate the problem mentioned above as a few-shot learning problem and analyze the performance state-of-art model-level few-shot learning algorithms.\n\nMeta-learning, also known as 'learning to learn,' aims to make quick adaptation to new tasks with only a few examples. Recently many different meta-learning solutions have been proposed to solve the few-shot learning problems. All these solutions differ in the form of learning a shared metric  [8] [9][10]  [11] , a generic inference network  [12] [13], a shared optimization algorithm  [14]   [15] , or a shared initialization for the model parameters  [16] [17]  [18] . In this paper, we use the Model-Agnostic Meta-Learning (MAML) approach  [16]  because of the following reasons:\n\n1. It is a model-agnostic general framework that can be easily used on a new task. 2. It achieves state-of-the-art performance in existing few-shot learning tasks.\n\nFew-shot learning is often defined as an N-way, K-shot problem where N is the number of class labels in the target task, and K is the number of examples of each class. In most previous studies, it is assumed that all the N classes or labels are new. However, in real-life applications, these classes or labels are not necessary to be all new. Thus, we further define an N+F-way, K-shot problem where N and F are the numbers of new classes or labels and fixed classes, respectively. In this new devised task, the model has to classify among new classes and fixed classes. We propose this modification to the original MAML algorithm to solve this problem and call this new model F-MAML.\n\nWe conduct our experiment on EmoFilm dataset  [19]  to simulate a scenario in SER. We compare our approach with two baseline approaches: the conventional supervised learning approach  [20]  and the MAML  [21]  approach. Experimental results show that F-MAML lead to obvious improvement over the supervised learning approach and even performing better than MAML. Our contributions in this paper are summarized here:\n\n1. We analyze the feasibility of few-shot learning for training SER models. 2. We propose an efficient way compared to MAML (F-MAML) to train future SER models for any language with few training examples.\n\nThe rest of the paper is presented in the following manner: In section 2, we discuss our work's background. In section 3, we discuss our proposed method. In section 4, experiments performed in detail, and results are mentioned. In section 5, we finally give a conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "In this section, we first briefly introduce MAML, the base, and our solution's motivation.\n\nModel-Agnostic Meta-Learning (MAML) is one of the most popular meta-learning algorithms that aim to solve the few-shot learning problem. The main goal of MAML is to train a model initializer that can adapt to any new task using very few labeled examples and training iterations  [16] . The model is trained across several tasks to reach this goal, and it treats the entire task as a training example. The model is required to face different tasks to get used to adapting to new tasks. In this section, we describe the MAML training framework. As is shown in Figure  -1 , the optimization procedure consists of two stages. A meta-learning stage on the training data and a fine-tuning stage on the testing tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Meta-Learning Stage",
      "text": "Given that the target evaluation task is an N-way, K-shot task, the model is trained across a set of task T where each task T i is also an N-way, K-shot task. In each iteration, a learning task, i.e., the meta-task T i is sampled according to a distribution over tasks p(T ). Each T i consist of a support set S i and a query set Q i .\n\nConsider a model represented by a parametrized function f θ with parameters θ. θ i is computed from θ through the adaptation to task T i . A loss function L Si (f θ ), which is cross-entropy loss over support set examples, is defined to guide the computation of θ i :\n\nA one-step gradient update is as below:\n\nHere, α is the learning rate, which can be a fixed hyperparameter or learned like the Meta-SGD  [17] . The gradient here is updated for multiple steps.\n\nAfter this, the model parameters are optimized on the performance of f θ i evaluated by the query set Q i with respect to θ. L Qi (f θ i\n\n) is another cross entropy loss over query set examples:\n\nBroadly speaking, MAML aims to optimize the model parameters such that few gradient steps on a new task will ultimately lead to a maximally effective behavior on that new task. At the end of each training iteration, the parameters θ are updated as below:\n\nHere, β is the learning rate of the meta learner. To increase the stability of training , instead of only one task a batch of tasks is sampled in each iteration. The optimization is performed by averaging the loss across the tasks. Thus, equation (  4 ) can be generalized to:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Fine-Tuning Stage",
      "text": "A fine-tuning is performed before the evaluation. In an N-way, K-shot task, K examples from each of the N class labels are available at this stage in the target task's support set. The model trained above in the meta-learning stage will now be fine-tuned according to equation (2) for a few iterations. The updated model will then be evaluated on the remaining unlabeled examples (the target task's query set).\n\nIn the original MAML, it is assumed that all class labels in the target task are new class labels. However, these class labels do not necessarily need to be all new. In real-life applications, some of the class labels are known so that more examples of these class labels can be used in the meta-learning stage. This paper will call them fixed classes as we later fix their output positions in the neural network classifier. We call this task, which has to classify among new classes and fixed classes, an N+F-way, K-shot problem where N, F, K are the number of emotion class labels, fixed class labels, and examples of each class for fine-tuning respectively. This problem of simultaneously classifying unseen and seen class labels has not been investigated in the original MAML. In our solution, we try to tackle the problem by proposing modifications to the MAML training framework. We believe that the N+F way, K-shot problem is more realistic and our modification to MAML applies to various tasks. We now describe our methodology for a few-shot SER task.\n\nFig.  2 . Framework of our F-MAML approach for few-shot SER.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology: F-Maml",
      "text": "Although the N+F-way, K-shot problem can be regarded as a specific form of the normal N-way, K-shot problem, solving it with the original MAML framework will lead to a performance degradation. Using the prior information of the F fixed classes, we modify the MAML framework in the following aspects:\n\n1. We fix the output positions, i.e., the output at the end of classification for a random sample for the fixed classes in the neural network classifier. 2. These fixed classes occur in every meta-task T i in the meta-learning stage. 3. The adaptation of fixed classes is not needed in the fine-tuning stage as they have already been learned in the meta-learning stage.\n\nThe above three modification to the original MAML makes the proposed framework more effectively to real applications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "We formulate a scenario for SER as N+F-way, K-shot classification task. N is the number of emotions that one wishes to recognize, and one should provide K speech audio samples for each such emotion. Fixed labels here are silence and neutral.\n\nFigure  2  illustrates the framework of F-MAML approach. The target data contains audio samples from one language, not in source data, while source data contain audio examples from all other languages. The fixed classes are the same in target and source data. In the meta-learning stage, several N+2-way, K-shot meta-task are sampled from source data for each language. Each meta-tasks is similar to the target task. We expect to learn a model initializer that can adapt to the target task using the provided speech samples and emotion labels. We exclude the fixed class labels from the support set in both meta-learning and the fine-tuning stages. As we can assume the availability of more training examples of fixed classes, we can keep them in the meta-tasks' query set in the meta-learning stage. Moreover, it can be seen that the positions of silence and the neutral classes are fixed to the last of network output (the orange area). Thus, we force our model to \"recall\" the fixed classes without the need for adaptation.\n\nAlgorithm 1 summarizes the details of our approach. The algorithm described here is based on the work of  [16]  but is different in terms of how sampling is done for the support set and the query set during the meta-learning stage, which is introduced in section 3.1.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimentation",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "We conduct our experiments on EmoFilm dataset  [19] . It consists of 1115 clips with a mean length of 3.5 seconds, resulting in 341 English audio clips, with an average of 34.3 utterances per emotion; 410 Italian audio clips with an average of 41.3 utterances per emotion; and 356 Spanish clips, with an average of 35.9 utterances per emotion (std 9). The higher number of Italian clips might be due to Italian being a more 'emotionally expressive' language; this could also relate to the pre-test made by Italian listeners, who may be better at perceiving emotions in their language  [19] . The dataset is categorized into five emotion labels: happiness, sadness, anger, fear, and disgust. We formulate three 5-way, K-shot tasks using the same setup as the audio recognition tutorial in official Py-Torch documentation. Table-1 gives information about total samples for each emotion in each language. We perform three experiments here. for all Ti do 5:\n\nSample a support set Si ∈ X 6:\n\nCompute the gradient LS i (f θ ) using Si and X as show in equation (1). 7:\n\nUpdate base model parameters with gradient descent: θ i = θ -α∇ θ LS i (f θ ). (Step 6-7 can be repeated several times.) 8:\n\nSample a query set Qi from the union [X, S il , Neu] \\ Si. (Selected emotion label from X in Qi and Si within Ti are the same). 9:\n\nCompute the loss LQ i (f θ i ) using Qi and the updated model f θ . 10:\n\nend for 11:\n\nUpdate the parameters for θ using each Qi and LQ i (f θ ): θ ← θ -β∇ θ i LQ i (f θ i ). 12: end while 1. The first experiment is SER in the English language, where we use the English language as a testing set while Spanish and Italian are used in training. 2. The second experiment is SER in the Italian language, where we use the Italian language as a testing set while English and Spanish are used in training. 3. The third experiment is SER in the Spanish language, where we use the Spanish language as a testing set while English and Italian are used in training.\n\nThe testing language is unseen to the meta-learning stage, and only K labeled examples of each label are available in the fine-tuning stage. The initialized model is fine-tuned on the labeled examples and evaluated on the unlabeled examples. The samples for silence class and neutral class were self-generated with a mean length of 3.5 seconds. In this paper, we simulated a scenario of SER as a few-shot learning problem. We define it as an N+F-way, K-shot problem and propose a modification to the Model-Agnostic Meta-Learning (MAML) algorithm where we kept F fixed to solve the problem. Experiments conducted on the EmoFilm dataset show that our approach performs the best compared to the baselines. In the future, we will attempt to test the feasibility of the approach on Indic languages, and mandarin derived languages since these languages differ vastly from each other. We also look for using image and text as well to make a multimodal system as well.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The MAML algorithm learns a good parameter initializer θ∗by training across various",
      "page": 3
    },
    {
      "caption": "Figure 2: Framework of our F-MAML approach for few-shot SER.",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the framework of F-MAML approach. The target data contains",
      "page": 6
    },
    {
      "caption": "Figure 3: Convergence Comparison MAML (MetaSER) vs F-MAML",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Dataset Details\nLanguage Total Samples": "341",
          "Samples per Emotion": "72 - Fear, 50 - Disgust,\n69 - Happiness, 76 - Anger,\n74 - Sadness"
        },
        {
          "Table 1. Dataset Details\nLanguage Total Samples": "410",
          "Samples per Emotion": "83 - Fear, 68 - Disgust,\n93 - Happiness, 73 - Anger,\n93 - Sadness"
        },
        {
          "Table 1. Dataset Details\nLanguage Total Samples": "356",
          "Samples per Emotion": "63 - Fear, 50 - Disgust,\n76 - Happiness, 82 - Anger,\n85 - Sadness"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Attentive to individual: A multimodal emotion recognition network with personalized attention profile",
      "authors": [
        "J Li",
        "C Lee"
      ],
      "year": "2019",
      "venue": "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "3",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "SPIE"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from human speech using temporal information and deep learning",
      "authors": [
        "J Kim",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Multilingual speech emotion recognition system based on a three-layer model",
      "authors": [
        "X Li",
        "M Akagi"
      ],
      "year": "2016",
      "venue": "Multilingual speech emotion recognition system based on a three-layer model"
    },
    {
      "citation_id": "6",
      "title": "Few-shot learning with graph neural networks",
      "authors": [
        "V Satorras",
        "J Estrach"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "Human-level concept learning through probabilistic program induction",
      "authors": [
        "B Lake",
        "R Salakhutdinov",
        "J Tenenbaum"
      ],
      "year": "2015",
      "venue": "Science"
    },
    {
      "citation_id": "8",
      "title": "Matching networks for one shot learning",
      "authors": [
        "O Vinyals",
        "C Blundell",
        "T Lillicrap",
        "K Kavukcuoglu",
        "D Wierstra"
      ],
      "year": "2016",
      "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS'16"
    },
    {
      "citation_id": "9",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17. Red Hook"
    },
    {
      "citation_id": "10",
      "title": "Learning to compare: Relation network for few-shot learning",
      "authors": [
        "F Sung",
        "Y Yang",
        "L Zhang",
        "T Xiang",
        "P Torr",
        "T Hospedales"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Prototypical networks for small footprint text-independent speaker verification",
      "authors": [
        "T Ko",
        "Y Chen",
        "Q Li"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Meta-learning with memory-augmented neural networks",
      "authors": [
        "A Santoro",
        "S Bartunov",
        "M Botvinick",
        "D Wierstra",
        "T Lillicrap"
      ],
      "year": "2016",
      "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning, ser. ICML'16"
    },
    {
      "citation_id": "13",
      "title": "A simple neural attentive metalearner",
      "authors": [
        "N Mishra",
        "M Rohaninejad",
        "X Chen",
        "P Abbeel"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "14",
      "title": "Meta networks",
      "authors": [
        "T Munkhdalai",
        "H Yu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34rd International Conference on International Conference on Machine Learning, ser. ICML'17"
    },
    {
      "citation_id": "15",
      "title": "Optimization as a model for few-shot learning",
      "authors": [
        "S Ravi",
        "H Larochelle"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ser. ICML'17"
    },
    {
      "citation_id": "17",
      "title": "Meta-sgd: Learning to learn quickly for few-shot learning",
      "authors": [
        "Z Li",
        "F Zhou",
        "F Chen",
        "H Li"
      ],
      "year": "2017",
      "venue": "Meta-sgd: Learning to learn quickly for few-shot learning"
    },
    {
      "citation_id": "18",
      "title": "On first-order meta-learning algorithms",
      "authors": [
        "A Nichol",
        "J Achiam",
        "J Schulman"
      ],
      "year": "2018",
      "venue": "On first-order meta-learning algorithms"
    },
    {
      "citation_id": "19",
      "title": "Categorical vs dimensional perception of italian emotional speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "21",
      "title": "Meta-learning for low-resource speech emotion recognition",
      "authors": [
        "S Chopra",
        "P Mathur",
        "R Sawhney",
        "R Shah"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning"
    }
  ]
}