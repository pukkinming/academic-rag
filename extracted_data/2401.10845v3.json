{
  "paper_id": "2401.10845v3",
  "title": "Emotion Classification In Software Engineering Texts: A Comparative Analysis Of Pre-Trained Transformers Language Models",
  "published": "2024-01-19T17:43:38Z",
  "authors": [
    "Mia Mohammad Imran"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in software engineering texts is critical for understanding developer expressions and improving collaboration. This paper presents a comparative analysis of state-of-the-art Pretrained Language Models (PTMs) for fine-grained emotion classification on two benchmark datasets from GitHub and Stack Overflow. We evaluate six transformer models -BERT, RoBERTa, ALBERT, DeBERTa, CodeBERT and GraphCodeBERT against the current best-performing tool SEntiMoji. Our analysis reveals consistent improvements ranging from 1.17% to 16.79% in terms of macroaveraged and micro-averaged F1 scores, with general domain models outperforming specialized ones. To further enhance PTMs, we incorporate polarity features in attention layer during training, demonstrating additional average gains of 1.0% to 10.23% over baseline PTMs approaches. Our work provides strong evidence for the advancements afforded by PTMs in recognizing nuanced emotions like Anger, Love, Fear, Joy, Sadness, and Surprise in software engineering contexts. Through comprehensive benchmarking and error analysis, we also outline scope for improvements to address contextual gaps.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The exploration of emotion classification in software engineering (SE) texts has garnered considerable attention in recent years  [1, 2] , leading to the development of various emotion classification tools by researchers. Notable among these are ESEM-E by Murgia et al.  [3] , EMTk by Calefato et al.  [4] , and SEntiMoji by Chen et al.  [5] . Chen et al. 's comparative study, utilizing emotion datasets extracted from Stack Overflow and JIRA  [6, 7] , affirmed the superior performance of SEntiMoji compared to EMTk and ESEM-E, solidifying its prominence in the domain. However, compared to open-domain, Pre-Trained Language Models (PTMs) rarely have been used in software engineering text for emotion classification.\n\nA recent study by Li et al.  [8]  introduced the use of BERT  [9]  as part of a two-stage framework for ambiguity detection, surpassing the performance of SEntiMoji in a GitHub emotion dataset curated by Imran et al.  [10] . Another recent study by Bleyl et al.  [11]  found that BERT outperforms EMTk on Stack Overflow dataset curated by Novielli et al.  [7] . This shift towards leveraging PTMs prompts a critical inquiry into the overall efficacy of such models in the nuanced task of emotion classification within software engineering contexts. Notably, Pre-trained Transformer Language Models like BERT  [9]  and RoBERTa  [12]  have already demonstrated state-of-theart results across various software engineering domains, including code completion, code review, bug localization, sentiment analysis, and toxicity detection  [13] [14] [15] [16] [17] [18] [19] .\n\nIn light of these developments, this paper embarks on a comprehensive exploration of Pre-trained Transformers Language Models in emotion classification for software engineering texts. Through rigorous evaluation and comparison, we aim to shed light on the strengths, limitations, and potential implications of leveraging PTMs in this nuanced domain.\n\nOur study utilizes two distinct datasets sourced from GitHub by Imran et al.  [10]  and Stack Overflow by Novielli et al.  [7] , encompassing a diverse range of emotions expressed in software engineering contexts. We assess the effectiveness of six state-of-the-art PTMs, including four general domain models (BERT, RoBERTa, ALBERT and DeBERTa) and two specifically tailored for software engineering (CodeBERT and GraphCodeBERT). This evaluation is conducted against the benchmark set by the SEntiMoji model. EMTk and ESEM-E are excluded from this comparative analysis, aligning with findings reported by Chen et al.  [5]  indicating the superior performance of SEntiMoji.\n\nOur investigation is guided by two primary research questions, each addressing specific facets of PTMs for emotion classification: RQ1: How accurately can Pre-Trained Language Models classify emotions in software engineering texts compared to state-of-the-art model? To address this question, we evaluate the performance of the six aforementioned state-of-the-art PTMs against the SEntiMoji model. Utilizing two distinct datasets, the GitHub dataset by Imran et al.  [10]  and the Stack Overflow dataset by Novielli et al.  [7] , both annotated using the comprehensive model by Shaver et al.  [20] , we scrutinize the models' efficacy in capturing the nuances of emotions expressed in software engineering contexts. Our results suggest that the PTMs generally outperform state-of-the-art, average f1-score improvement ranges from 1.17% to 16.79%.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Rq2: Can Integrating Polarity Features In The Training Improve Pre-Trained Language Models' Ability For Emotion Classification?",
      "text": "Building upon observations by Chen et al.  [5]  and Imran et al.  [10]  regarding the impact of sentiment polarity on emotion classification errors, we focus on the potential benefits of incorporating polarity features during transformer model training. Specifically, we examine the extent to which positive and negative polarity features enhance the contextual understanding of PTMs for emotion classification in software engineering texts. We incorporate polarity features in attention layer of the PTMs. Our results suggest that they, indeed, improve the baseline PTMs further 1.0% to 10.23% in average f1-score metric.\n\nThe main contributions of this work are as follows:\n\n‚Ä¢ To our best knowledge, this is the first study which leverages various PTMs for a large-scale comparative study against existing state-of-the-art tool for emotion classification in software engineering text. ‚Ä¢ This is the first study which leverages polarity features to enhance PTMs for emotion classification in software engineering text.\n\nWe release the source code at URL: https://anonymous.4open. science/status/SE_Emotion_PTM-3589.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup 2.1 State-Of-The-Art Models",
      "text": "SEntiMoji: SEntiMoji is a deep learning-based model and pretrained on GitHub data. The model is proposed by Chen et al.  [5]  and built on top of the DeepMoji  [21]  model. This flexible model can identify different emotion categorization schemes as well as sentiment classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Compared Ptms",
      "text": "We compare 6 encoder-based PTMs: BERT, RoBERTa, DeBERTa, AL-BERT, CodeBERT and GraphCodeBERT. Previous research shows that BERT, RoBERTa and ALBERT work well in SE affect analysis  [17, 22] , while DeBERTa shows promising results in sentiment analysis  [23] . In addition to these four models, we compare against 2 domain-specific models that are widely used and proposed by Microsoft: CodeBERT and GraphCodeBERT -which showed promising results in various SE tasks  [24, 25] . The models are explained below. BERT  [9] : Introduced by Google in 2018, BERT, pre-trained on a diverse corpus, including Wikipedia and Book Corpus, revolutionized natural language processing NLP with its bidirectional transformer architecture. This design allows BERT to capture context from both left and right contexts of a given word, making it highly effective for a diverse range of NLP tasks. RoBERTa  [12] : Developed by Meta AI, RoBERTa represents a refinement in transformer-based models. By modifying key hyperparameters and leveraging an extensive training dataset, RoBERTa enhances its performance on benchmark NLP tasks, showcasing improved language representation understanding. DeBERTa  [26] : An evolution of BERT and developed by Microsoft Research, DeBERTa focuses on enhancing the decoding process in language understanding tasks. It incorporates directional masks ALBERT  [27] : Designed by Google Research, ALBERT introduces efficiency improvements to the BERT architecture without compromising representational power. This is achieved through parameter reduction techniques during training, showcasing resource-efficient yet highly effective language representations. CodeBERT  [28] : Tailored for programming languages and code understanding, CodeBERT, developed by Microsoft Research, is pre-trained on a large-scale dataset of programming tasks. This specialization allows CodeBERT to excel in source code-related tasks such as code summarization and variable naming. GraphCodeBERT  [29] : Developed by Microsoft Research, Graph-CodeBERT is a transformer-based model designed for comprehending programming languages and code. Pre-trained on a vast dataset of programming tasks, it effectively captures the complex structures and semantics of code, serving as a proficient solution for source code-related tasks.\n\nWe use the publicly available versions of each model in Hugging Face  [30] . The model versions are shown in Table  1 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We conduct experiment using two existing SE-emotion datasets. Both datasets are annotated using Shaver's emotion model  [20]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Metrics",
      "text": "We evaluate using F1-score -which is commmonly used for this evaluation  [5, 10] . F1-score is the harmonic mean of Precision and Recall: F1-score = 2 * ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ * ùëÖùëíùëêùëéùëôùëô ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëÖùëíùëêùëéùëôùëô . Precision measures the ratio of true positives to all positive predictions, while Recall calculates the ratio of true positives to all actual positives. Additionally, micro-averaged F1 considers the overall performance by aggregating individual class scores, treating each instance equally, whereas macro-averaged F1 computes the average across emotion classes, providing equal weight to each class regardless of their size.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Classification Setup",
      "text": "As both datasets are multi-label, we employ one-vs-all settings for all models. A one-vs.-all solution consists of N (here, N = 6) separate binary classifiers, each answering a separate classification question during training  [31] .\n\nWe divide the datasets into stratified train (80%) and test (20%) splits based on emotions using random sampling  [32] . For all 6 models, we will use the same train and test sets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments 3.1 Rq1: How Accurately Can Pre-Trained",
      "text": "Language Models classify emotions in software engineering texts compared to state-of-the-art model?\n\nThe results for emotion analysis on GitHub and Stack Overflow datasets, utilizing the PTMs are presented in Error Analysis: To identify model mistakes, we conduct an error analysis. Due to space constraints, we focus exclusively on the GitHub benchmark which has gone prior error analysis  [10] . To understand where the PTMs commonly make mistake, we examine 67 cases where all six models agreed, yet the ground truth differed (9 false positives, 58 false negatives), distributed across emotions.\n\nFor assessment, we employ Novielli et al. 's  [33]  error categorization, using a thematic approach. An author initially mapped errors, and a senior undergraduate student reviewed and resolved disagreements through discussion. Echoing Imran et al. 's observations  [10] , the prevalent categories are 'General Error' and 'Implicit Sentiment Polarity.' General errors, occurring 29/67 times, manifest when models misinterpret or struggle to comprehend lexical cues conveying emotions. For instance, the text \"Nice, this is more slick thumbs-up\" is annotated as Joy. Another example, \"i'm actually surprised this didn't get flagged by the analyzer..., \" annotated as Surprise, remains mispredicted by all models. The majority of Surprise (10/15), Joy (8/14), and Love (4/5) errors fall into the general errors category.\n\nImplicit sentiment polarity errors occur 18/67 times. An example -\"Patiently waiting for any updates. [...]\" -is annotated as Sadness, with none of the models predicting it correctly. This category is particularly noticeable in Joy (6/14) and Sadness (5/11).\n\nThe third major error category is 'Figurative Language' (9/67), which occurs when users use humor, sarcasm, metaphors, etc to convey emotion. This category is noticeable among negative emotions (Anger 5/13 and Fear 3/9). For example, the following utterance \"[...] I understand what you mean by \"takeover\" however it doesn't hurt to be a little more explicit.\" -annotated as Anger but none of the models were able to detect it correctly.\n\nOf note, in 13/67 cases, the utterances contain emojis which contribute in expressing emotions. The models possibly fail to capture them. For instance, \"And yes, there should be tests facescreaming-in-fear face-screaming-in-fear face-screaming-in-fear\"annotated as Fear, none of the models predict it accurately.\n\nRQ1 Takeaway: The transformer-based PTMs consistently outperform the state-of-the-art SEntiMoji model across all emotions for both datasets. DeBERTa and RoBERTa emerge as top performers. General PTMs outshine domain-specific ones in this context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Rq2: Can Integrating Polarity Features In The Training Improve Pre-Trained Language Models' Ability For Emotion Classification?",
      "text": "In open-domain research, leveraging word polarity has proven effective for sentiment analysis  [34] [35] [36] . We aim to explore its applicability to emotion classification within the SE domain.  to extract word polarity. This involves a series of steps such as tokenization, part-of-speech tagging, and identification of words with discernible sentiment, resulting in a concise list of polarity words for each utterance. Subsequently, the model architecture is fine-tuned at the attention level, with a focus on polarity words. Attention weights are adjusted to assign greater significance to tokens linked with polarity words. This involves a strategic blending of attention weights related to the primary input text and those corresponding to polarity words. The adjustment ensures a heightened emphasis on the embeddings of polarity words, achieved by multiplying attention weights for primary input text's hidden states by 0.75 and those for polarity words' hidden states by 0.25, followed by concatenation. This modification enhances the model's sensitivity to sentiment-carrying terms during training, allowing for improved discernment of subtle variations in sentiment expression. Refer to Figure  1  for a visual representation of the token-level attention adjustment procedure.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": ". The results of emotion classification using polarity-enhanced PTMs are presented in Table  3 , along with the percentage difference in performance against a state-of-theart model and baseline PTMs. The findings show that polarityenhanced PTMs outperform the state-of-the-art SEntiMoji model and baseline PTMs. GitHub Dataset: SEntiMoji baseline: DeBERTa achieves the highest micro-averaged F1-score of 0.620, showing a 17.09% improvement. On the macro-averaged F1-score metric, BERT achieves the highest score of 0.621, reflecting a 19.28% improvement. Analyzing individual emotions, polarity-enhanced PTMs outperform SEnti-Moji 33 out of 36 times, with improvements in Anger, Love, Fear, and Surprise for all six models, Joy for five models, and Sadness for four models. Anger achieves the highest F1-score of 0.588 by DeBERTa, marking a 27.88% improvement from SEntiMoji, Love F1-score 0.787 by RoBERTa (+22.63%), Fear F1-score 0.583 by BERT (+54.58%), Joy F1-score 0.633 by BERT (+13.85%), Sadness F1-score 0.674 by BERT (+7.07%), and Surprise F1-score 0.661 by BERT (+44.19%). PTM baseline: The improvement on PTM baseline ranges from 2.52% to 9.16% on micro-averaged F1-score and from 1.04% to 8.37% on macro-averaged F1-score. The most improved PTM is ALBERT in both micro and macro-averaged F1-score. enhancement is observed consistently across individual emotions, with Joy and Sadness improving for all six models, Anger and Fear for five models, Love for four models, and Surprise for four models. Noteworthy improvements include a 26.61% enhancement in Anger by CodeBERT, a 5.80% improvement in Love by CodeBERT, a 25.37% improvement in Fear by ALBERT, a 31.19% improvement in Joy by ALBERT, an 8.47% improvement in Sadness by GraphCodeBERT, and a 17.04% improvement in Surprise by RoBERTa. Stack Overflow Dataset: SEntiMoji baseline: RoBERTa achieves the highest micro-averaged F1-score of 0.766, showing a 7.36% improvement; as well as and the highest-averaged F1-score of 0.646, reflecting a 21.93% improvement. Analyzing individual emotions, polarity-enhanced PTMs outperform SEntiMoji 33 out of 36 times, with improvements in Love, Fear, Joy, and Sadness for all six models, Anger for five models, and Surprise for four models. Anger achieved the highest F1-score of 0.777 by RoBERTa and ALBERT, marking a 2.44% improvement from SEntiMoji, Love F1-score 0.880 by RoBERTa (+7.47%), Fear F1-score 0.667 by ALBERT (+55.56%), Joy F1-score 0.643 by DeBERTa (+47.74%), Sadness F1-score 0.664 by BERT (+16.0%), and Surprise F1-score 0.400 by BERT (+120.0%).\n\nPTM baseline: The improvement on PTM baseline ranges from 1.0% to 1.91% on micro-averaged F1-score and from 3.11% to 10.23% on macro-averaged F1-score. The most improved PTM is ALBERT in macro-averaged F1-score metric and CodeBERT in macro-averaged F1-score metric. While the improvement in Stack Overflow dataset against baseline PTMs is less compared to the GitHub dataset, this enhancement is observed consistently across individual emotions, with Surprise improving for all six models, Love, Fear and Joy for five models, Sadness for four models, and Anger for four models. Noteworthy improvements include a 2.04% enhancement in Anger by ALBERT, a 1.47% improvement in Love by GraphCodeBERT, a 15.15% improvement in Fear by ALBERT, a 6.53% improvement in Joy by DeBERTa, an 18.15% improvement in Sadness by ALBERT, and a 140.0% improvement in Surprise by RoBERTa.\n\nThe average PTM improvement in both dataset are similar to the general domain sentiment analysis results  [34, 35] .\n\nError Analysis: Similar to RQ1, we look into GitHub dataset for this case as well. In RQ1 error analysis, we observe that in 67 cases all models predict incorrectly. Out of those 67 cases, we find that 40 cases still produce erroneous results. However, in rest 27 cases, at least one model predict correctly. The most prominent resolved error categories are: 13/29 (44.82%) general errors, 9/18 (50%) implicit sentiment polarity, and 2/3 politeness. For example, \"i'm actually surprised this didn't get flagged by the analyzer..., \" -this utterance is now correctly predicted by CodeBERT.\n\nThe least improved error category is 'Pragmatics'. Pragmatics is the error category when the classifiers deal with context information. That is often human annotators consider external facts for annotation. example, consider the following utterance, \"[...] This change makes it the same as the line above and I don't see any reason to have two lines that are showing the same thing. \" -is annotated as Anger as the commentator is annoyed/disagreed with the change in code. 6/7 (85.71%) Pragmatics related errors remains unresolved. Another least improved error category is 'Figurative Language' -6/9 (66%) utterances predictions remained unchanged. For example, this following utterance, \"[...] We need to add this test coverage. It's just not 'urgent'. neutral-face\" -which expresses sarcasm and annotated as Anger. The models predict it incorrectly.\n\nWe observe that there still remains a considerable amount of utterances (10/13) that are misclassified contain emojis. For example, the previously mentioned utterance, \"And yes, there should be tests face-screaming-in-fear face-screaming-in-fear face-screamingin-fear\" -all models still predict incorrectly.\n\nRQ2 Takeaway: Polarity-enhanced PTMs consistently outperform the SEntiMoji model and baseline PTMs in both datasets. Notable improvements are seen in micro-and macro-averaged along with substantial enhancements for individual metric across various models. While the findings indicate that integrating sentiment polarity improves PTM performance, it do not always help to capture context-dependent emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "In this section, based on our experiments, we summarize the key lessons learned and outline promising directions for future work related to emotion classification in SE.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Lessons Learned",
      "text": "Our study yields several key insights into the application of PTMs for emotion classification in SE texts. Firstly, general domain PTMs such as BERT, RoBERTa, and DeBERTa consistently outperform specialized models. This suggests that the pre-training objectives and textual domains carry greater relevance than task-specific tuning in the emotion classification task within SE text.\n\nSecondly, the incorporation of polarity features during finetuning consistently enhances performance, emphasizing the value of sentiment awareness. However, challenges persist, especially with negative emotions like Anger and Fear, which pose difficulties in context understanding.\n\nThirdly, no single model excels across all emotions in all metrics, necessitating careful benchmarking and tradeoff analysis. Common error categories, such as implicit polarity, figurative language, and pragmatics, underscore the challenges in understanding contextual gaps in nuanced emotion recognition. Addressing these complexities requires further customization. For example, Imran et al.  [37]  addressed the issue of understanding figurative language in SE text using contrastive learning.\n\nLastly, persistent challenges arise in handling emojis, as models struggle to interpret emotive signals. Incorporating dedicated approaches to handle emojis may offer mitigation strategies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Future Directions",
      "text": "Building on lessons learned, future work can focus on establishing more benchmark datasets, particularly emphasizing implicit and context-dependent emotions.\n\nInvestigating hierarchical emotion classification and joint modeling of related polarity dimensions merits investigation. Recognizing that emotion classification often treats each category separately, a hierarchical framework may enhance performance by first identifying broad emotional valence (positive, negative, or neutral) before classifying specific categories, as demonstrated by Li et al.  [8] .\n\nIn SE-specific models, exploring pre-training with polarity-word masking, emojis, and emoticons atop general word masking is warranted, given promising results in the general domain for sentiment analysis  [38] . Research also suggests that aspect-based sentiment analysis (ABSA)-enhanced PTMs outperform baseline PTMs  [35, 39] . As polarity-enhanced PTMs have already shown superiority over baseline PTMs, exploring ABSA-enhanced PTMs may further improve classifiers.\n\nAdditionally, investigating the multi-modal fusion of text and emojis cues during the pre-training/fine-tuning step could enhance the interpretation of emotive expressions. Exploring generative language models like GPT-4 and LLaMA for emotion detection using zero-shot and few-shot learning presents an intriguing direction. Recent advancements in these models enable synthesizing natural language responses, facilitating data augmentation and prompting techniques for improved classification  [40] [41] [42] [43] . Focusing on detecting the most relevant emotions that may harm productivity, such as Frustration and Disappointment, holds merit  [44] . For example, a recent case study delved into understanding Frustration in OSS text using generative models in zero-shot learning  [45] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Related Work",
      "text": "On a large scale the related work can be divided into two parts: emotions in SE texts and PTMs in SE tasks.   [10] . Tulli et al.  [56]  did a literature review on burnout in software engineering.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotions In",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ptms In Software Engineering Tasks",
      "text": "The utilization of PTMs has gained significant attention in software engineering text, offering a novel approach to address various challenges and enhance the performance of diverse tasks. One notable area of research involves the application of PTMs in coderelated tasks. Researchers have explored the effectiveness of models like in understanding and generating code snippets. These studies aim to improve code completion, summarization, comprehension and documentation  [14, [57] [58] [59] [60] . Lanaguage models have shown promise in tasks related to software bugs and assessing code quality  [13, 24, [61] [62] [63] [64] . There also has been study on understanding code attentions in BERT  [65] . Researchers also conduct studies on benchmarking PTMs on tag recommendation in Stack Overflow text, sentiment analysis in SE text and software aspect-based API review text  [17, 66, 67] .\n\nIn this study, we do comprehensive benchmark analysis of emotion classification task in SE text using PTMs and we also apply approaches in changing attention layer for further improving the performances.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Threats To Validy",
      "text": "Several limitations could affect the accurate interpretation of our results. We outline and detail each of these limitations below. Construct validity. Construct validity assesses how well the study measures the concepts and constructs it aims to evaluate. While potential issues may arise from manually annotating the error analysis in RQ1, we mitigated this by employing thematic analysis to address any possible problems. Internal validity. Internal validity focuses on the extent to which the study's findings can be attributed to the manipulation of the independent variable. Threats may arise from mistakes in our experiments, but we have provided a replication package and outputs for independent verifications. Another potential threat is the absence of cross-validation on smaller datasets, which we addressed by using stratified sampling for representativeness and an 80%-20% train-test split. There's also a potential threat related to data leakage in SEntiMoji models against the GitHub benchmark dataset, but this is unlikely as SEntiMoji was pre-trained before 2019  [5] , and data points of the GitHub dataset came afterward  [10] . However, verification of this is beyond the scope of our study given that not all pre-trained data is released by the authors  [5] . External validity. External validity concerns the generalization of our study's findings to other settings and contexts. Our results may not extend beyond the specific models, datasets, and domains of GitHub and Stack Overflow. However, we utilized diverse pretrained models and the most comprehensive datasets available in this field. Further investigation is necessary to validate our results beyond the datasets and pre-trained models used in our study.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presented a comprehensive comparative analysis of stateof-the-art Pre-trained Transformer Language Models for emotion classification in software engineering texts. Through evaluation on two datasets, sourced from GitHub and Stack Overflow and annotated using Shaver's emotion model, we address two key research questions. First, we examined the accuracy of PTMs in classifying emotions compared to SEntiMoji, we found consistent enhancements with models like BERT, DeBERTa and RoBERTa achieving upto 16.79% improvements in terms of macro and micro-averaged F1 scores. The results showcase the prowess of general domain PTMs, with domain specific CodeBERT and GraphCodeBERT underperforming. Analysis of common errors revealed struggles in handling general comprehension, implicit polarity, figurative language and contextual pragmatics. Next, we incorporated polarity features during PTM finetuning and demonstrated additional gains over both SEntiMoji and baseline PTMs. This validates the benefit of augmenting with explicit polarity information for nuanced emotion classification. However, errors linked to figurative language and pragmatics persisted. In conclusion, this study provides a robust benchmark of diverse PTMs in a complex affective analysis task within software engineering, showcasing their effectiveness not only in sentiment analysis but also in fine-grained emotion recognition. These findings underscore the potential of PTMs in advancing empathetic software intelligence, with opportunities for further refinement to address contextual gaps.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Fine-tuning procedure using token-level attention",
      "page": 4
    },
    {
      "caption": "Figure 1: for a visual representation of the token-level attention",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "BERT",
          "Version": "bert-base-uncased"
        },
        {
          "Model": "RoBERTa",
          "Version": "roberta-base"
        },
        {
          "Model": "ALBERT",
          "Version": "albert-base-v2"
        },
        {
          "Model": "DeBERTa",
          "Version": "microsoft/deberta-v3-base"
        },
        {
          "Model": "CodebERT",
          "Version": "microsoft/codebert-base"
        },
        {
          "Model": "GraphCodeBERT",
          "Version": "microsoft/graphcodebert-base"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GitHub": "",
          "SEntiMoji": "BERT\n(+/-)",
          "0.460": "0.426\n-7.49%",
          "0.642": "0.731\n+13.83%",
          "0.377": "0.545\n+44.55%",
          "0.556": "0.597\n+7.29%",
          "0.629": "0.609\n-3.26%",
          "0.458": "0.639\n+39.49%",
          "0.530": "0.585\n+10.37%",
          "0.521": "0.591\n+13.56%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "RoBERTa\n(+/-)",
          "0.460": "0.517\n+12.44%",
          "0.642": "0.774\n+20.60%",
          "0.377": "0.561\n+48.66%",
          "0.556": "0.521\n-6.37%",
          "0.629": "0.653\n+3.85%",
          "0.458": "0.511\n+11.55%",
          "0.530": "0.575\n+8.45%",
          "0.521": "0.590\n+13.28%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "ALBERT\n(+/-)",
          "0.460": "0.446\n-2.98%",
          "0.642": "0.753\n+17.29%",
          "0.377": "0.357\n-5.36%",
          "0.556": "0.447\n-19.61%",
          "0.629": "0.631\n+0.23%",
          "0.458": "0.602\n+31.30%",
          "0.530": "0.538\n+1.52%",
          "0.521": "0.539\n+3.61%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "DeBERTa\n(+/-)",
          "0.460": "0.578\n+25.72%",
          "0.642": "0.736\n+14.59%",
          "0.377": "0.476\n+26.19%",
          "0.556": "0.605\n+8.69%",
          "0.629": "0.642\n+1.95%",
          "0.458": "0.611\n+33.33%",
          "0.530": "0.610\n+15.07%",
          "0.521": "0.608\n+16.79%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "CodeBERT\n(+/-)",
          "0.460": "0.446\n-3.01%",
          "0.642": "0.653\n+1.73%",
          "0.377": "0.548\n+45.21%",
          "0.556": "0.518\n-6.86%",
          "0.629": "0.591\n-6.09%",
          "0.458": "0.574\n+25.19%",
          "0.530": "0.545\n+2.95%",
          "0.521": "0.555\n+6.62%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "GraphCodeBERT\n(+/-)",
          "0.460": "0.476\n+3.52%",
          "0.642": "0.632\n-1.62%",
          "0.377": "0.507\n+34.27%",
          "0.556": "0.552\n-0.74%",
          "0.629": "0.551\n-12.43%",
          "0.458": "0.578\n+26.14%",
          "0.530": "0.549\n+3.60%",
          "0.521": "0.549\n+5.53%"
        },
        {
          "GitHub": "Stack Overflow",
          "SEntiMoji": "SEntiMoji",
          "0.460": "0.759",
          "0.642": "0.819",
          "0.377": "0.429",
          "0.556": "0.435",
          "0.629": "0.556",
          "0.458": "0.182",
          "0.530": "0.714",
          "0.521": "0.530"
        },
        {
          "GitHub": "",
          "SEntiMoji": "BERT\n(+/-)",
          "0.460": "0.769\n+1.40%",
          "0.642": "0.851\n+3.95%",
          "0.377": "0.545\n+27.27%",
          "0.556": "0.597\n+37.05%",
          "0.629": "0.600\n+8.00%",
          "0.458": "0.167\n-8.33%",
          "0.530": "0.754\n+5.62%",
          "0.521": "0.588\n+11.02%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "RoBERTa\n(+/-)",
          "0.460": "0.786\n+3.60%",
          "0.642": "0.872\n+6.49%",
          "0.377": "0.581\n+35.48%",
          "0.556": "0.591\n+35.77%",
          "0.629": "0.600\n+8.00%",
          "0.458": "0.1667\n-8.33%",
          "0.530": "0.758\n+6.23%",
          "0.521": "0.599\n+13.13%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "ALBERT\n(+/-)",
          "0.460": "0.762\n+0.40%",
          "0.642": "0.845\n+3.13%",
          "0.377": "0.579\n+35.09%",
          "0.556": "0.640\n+47.00%",
          "0.629": "0.545\n-1.82%",
          "0.458": "0.133\n-26.67%",
          "0.530": "0.747\n+4.65%",
          "0.521": "0.584\n+10.23%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "DeBERTa\n(+/-)",
          "0.460": "0.777\n+2.49%",
          "0.642": "0.860\n+5.04%",
          "0.377": "0.591\n+37.88%",
          "0.556": "0.604\n+38.68%",
          "0.629": "0.598\n+7.59%",
          "0.458": "0.211\n+15.79%",
          "0.530": "0.756\n+5.90%",
          "0.521": "0.607\n+14.52%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "CodeBERT\n(+/-)",
          "0.460": "0.772\n+1.79%",
          "0.642": "0.854\n+4.29%",
          "0.377": "0.556\n+29.63%",
          "0.556": "0.519\n+19.17%",
          "0.629": "0.537\n-3.41%",
          "0.458": "0.167\n-8.33%",
          "0.530": "0.728\n+2.04%",
          "0.521": "0.567\n+7.08%"
        },
        {
          "GitHub": "",
          "SEntiMoji": "GraphCodeBERT\n(+/-)",
          "0.460": "0.727\n-4.13%",
          "0.642": "0.836\n+2.06%",
          "0.377": "0.514\n+20.00%",
          "0.556": "0.559\n+28.32%",
          "0.629": "0.521\n-6.30%",
          "0.458": "0.154\n-15.38%",
          "0.530": "0.722\n+1.17%",
          "0.521": "0.552\n+4.14%"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "RoBERTa+Polarity\nSEntiMoji (+/-)\nRoBERTa (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.475\n+3.31%\n-8.12%",
          "0.733\n+14.13%\n+0.26%": "0.787\n+22.63%\n+1.68%",
          "0.583\n+54.58%\n+6.94%": "0.538\n+42.47%\n-4.16%",
          "0.629\n+13.11%\n+5.42%": "0.583\n+4.76%\n+11.89%",
          "0.636\n+1.00%\n+4.41%": "0.654\n+3.91%\n+0.06%",
          "0.661\n+44.19%\n+3.37%": "0.598\n+30.57%\n+17.04%",
          "619\n+16.98%\n+5.99%": "0.603\n+13.81%\n+4.94%",
          "0.621\n+19.28%\n+5.04%": "0.606\n+16.39%\n+2.75%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "ALBERT+Polarity\nSEntiMoji (+/-)\nALBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.471\n+2.30%\n+5.45%",
          "0.733\n+14.13%\n+0.26%": "0.744\n+15.92%\n-1.16%",
          "0.583\n+54.58%\n+6.94%": "0.448\n+18.66%\n+25.37%",
          "0.629\n+13.11%\n+5.42%": "0.587\n+5.46%\n+31.19%",
          "0.636\n+1.00%\n+4.41%": "0.674\n+7.07%\n+6.83%",
          "0.661\n+44.19%\n+3.37%": "0.561\n+22.34%\n-6.82%",
          "619\n+16.98%\n+5.99%": "0.580\n+9.49%\n+7.86%",
          "0.621\n+19.28%\n+5.04%": "0.581\n+11.54%\n+7.65%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "DeBERTa+Polarity\nSEntiMoji (+/-)\nDeBERTa (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.588\n+27.88%\n+1.72%",
          "0.733\n+14.13%\n+0.26%": "0.680\n+5.99%\n-7.51%",
          "0.583\n+54.58%\n+6.94%": "0.507\n+34.37%\n+6.48%",
          "0.629\n+13.11%\n+5.42%": "0.633\n+13.85%\n+4.74%",
          "0.636\n+1.00%\n+4.41%": "0.654\n+3.91%\n+1.92%",
          "0.661\n+44.19%\n+3.37%": "0.623\n+35.85%\n+1.89%",
          "619\n+16.98%\n+5.99%": "0.620\n+17.09\n+1.75%",
          "0.621\n+19.28%\n+5.04%": "0.614\n+18.01\n+1.04%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "CodeBERT+Polarity\nSEntiMoji (+/-)\nCodeBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.565\n+22.80%\n+26.61%",
          "0.733\n+14.13%\n+0.26%": "0.691\n+7.62%\n+5.80%",
          "0.583\n+54.58%\n+6.94%": "0.576\n+52.58%\n+5.08%",
          "0.629\n+13.11%\n+5.42%": "0.530\n-4.66%\n+2.36%",
          "0.636\n+1.00%\n+4.41%": "0.607\n-3.57%\n+2.68%",
          "0.661\n+44.19%\n+3.37%": "0.640\n+39.64%\n+11.54%",
          "619\n+16.98%\n+5.99%": "0.595\n+12.38%\n+9.16%",
          "0.621\n+19.28%\n+5.04%": "0.601\n+15.55%\n+8.37%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "GraphCodeBERT+Polarity\nSEntiMoji (+/-)\nGraphCodeBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.514\n+11.63%\n+7.84%",
          "0.733\n+14.13%\n+0.26%": "0.654\n+1.91%\n+3.58%",
          "0.583\n+54.58%\n+6.94%": "0.551\n+45.94%\n+8.70%",
          "0.629\n+13.11%\n+5.42%": "0.570\n+2.42%\n+3.19%",
          "0.636\n+1.00%\n+4.41%": "0.598\n-5.01%\n+8.47%",
          "0.661\n+44.19%\n+3.37%": "0.521\n+13.70%\n-9.86%",
          "619\n+16.98%\n+5.99%": "0.563\n+6.22%\n+2.52%",
          "0.621\n+19.28%\n+5.04%": "0.568\n+9.09%\n+3.38%"
        },
        {
          "GitHub": "Stack\nOverflow",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.785\n+3.44%\n+2.02%",
          "0.733\n+14.13%\n+0.26%": "0.855\n+4.36%\n+0.40%",
          "0.583\n+54.58%\n+6.94%": "0.611\n+42.59%\n+12.04%",
          "0.629\n+13.11%\n+5.42%": "0.601\n+38.05%\n+0.73%",
          "0.636\n+1.00%\n+4.41%": "0.590\n+6.15%\n-1.71%",
          "0.661\n+44.19%\n+3.37%": "0.200\n+10.0%\n+20.0%",
          "619\n+16.98%\n+5.99%": "0.762\n+6.68%\n+1.0%",
          "0.621\n+19.28%\n+5.04%": "0.607\n+14.55%\n+3.17%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "RoBERTa+Polarity\nSEntiMoji (+/-)\nRoBERTa (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.777\n+2.45%\n-1.11%",
          "0.733\n+14.13%\n+0.26%": "0.880\n+7.47%\n+0.92%",
          "0.583\n+54.58%\n+6.94%": "0.650\n+51.67%\n+11.94%",
          "0.629\n+13.11%\n+5.42%": "0.594\n+36.45%\n+0.50%",
          "0.636\n+1.00%\n+4.41%": "0.575\n+3.45%\n-4.21%",
          "0.661\n+44.19%\n+3.37%": "0.400\n+120.0%\n+140.0%",
          "619\n+16.98%\n+5.99%": "0.767\n+7.51%\n+1.20%",
          "0.621\n+19.28%\n+5.04%": "0.646\n+21.93%\n+7.78%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "ALBERT\nSEntiMoji (+/-)\nALBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.777\n+2.44%\n+2.04%",
          "0.733\n+14.13%\n+0.26%": "0.844\n+3.09%\n-0.04%",
          "0.583\n+54.58%\n+6.94%": "0.667\n+55.56%\n+15.15%",
          "0.629\n+13.11%\n+5.42%": "0.598\n+37.31%\n-6.59%",
          "0.636\n+1.00%\n+4.41%": "0.644\n+16.0%\n+18.15%",
          "0.661\n+44.19%\n+3.37%": "0.167\n-8.33%\n+25.0%",
          "619\n+16.98%\n+5.99%": "0.757\n+6.08%\n+1.36%",
          "0.621\n+19.28%\n+5.04%": "0.616\n+16.30%\n+10.23%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "DeBERTa+Polarity\nSEntiMoji (+/-)\nDeBERTa (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.776\n+2.29%\n-0.20%",
          "0.733\n+14.13%\n+0.26%": "0.862\n+5.27%\n+0.22%",
          "0.583\n+54.58%\n+6.94%": "0.619\n+44.44%\n+4.76%",
          "0.629\n+13.11%\n+5.42%": "0.643\n+47.74%\n+6.53%",
          "0.636\n+1.00%\n+4.41%": "0.623\n+12.21%\n+4.30%",
          "0.661\n+44.19%\n+3.37%": "0.222\n+22.22%\n+5.56%",
          "619\n+16.98%\n+5.99%": "0.766\n+7.36%\n+1.37%",
          "0.621\n+19.28%\n+5.04%": "0.624\n+17.84%\n+2.89%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "CodeBERT+Polarity\nSEntiMoji (+/-)\nCodeBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.766\n+0.95%\n-0.83%",
          "0.733\n+14.13%\n+0.26%": "0.866\n+5.70%\n+1.35%",
          "0.583\n+54.58%\n+6.94%": "0.550\n+28.33%\n-1.0%",
          "0.629\n+13.11%\n+5.42%": "0.536\n+23.0%\n+3.22%",
          "0.636\n+1.00%\n+4.41%": "0.565\n+1.65%\n+5.42%",
          "0.661\n+44.19%\n+3.37%": "0.235\n+29.41%\n+41.18%",
          "619\n+16.98%\n+5.99%": "0.742\n+3.99%\n+1.91%",
          "0.621\n+19.28%\n+5.04%": "0.586\n+10.64%\n+3.32%"
        },
        {
          "GitHub": "",
          "BERT+Polarity\nSEntiMoji (+/-)\nBERT (+/-)": "GraphCodeBERT+Polarity\nSEntiMoji (+/-)\nGraphCodeBERT (+/-)",
          "0.484\n+5.19%\n+13.71%": "0.727\n-4.20%\n-0.07%",
          "0.733\n+14.13%\n+0.26%": "0.848\n+3.56%\n+1.47%",
          "0.583\n+54.58%\n+6.94%": "0.524\n+22.22%\n+1.85%",
          "0.629\n+13.11%\n+5.42%": "0.583\n+33.98%\n+4.42%",
          "0.636\n+1.00%\n+4.41%": "0.565\n+1.65%\n+8.48%",
          "0.661\n+44.19%\n+3.37%": "0.167\n-8.33%\n+8.33%",
          "619\n+16.98%\n+5.99%": "0.732\n+2.48%\n+1.29%",
          "0.621\n+19.28%\n+5.04%": "0.569\n+7.38%\n+3.11%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion analysis in software ecosystems",
      "authors": [
        "N Novielli",
        "A Serebrenik"
      ],
      "year": "2023",
      "venue": "Software Ecosystems: Tooling and Analytics"
    },
    {
      "citation_id": "2",
      "title": "Opinion mining for software development: a systematic literature review",
      "authors": [
        "B Lin",
        "N Cassee",
        "A Serebrenik",
        "G Bavota",
        "N Novielli",
        "M Lanza"
      ],
      "year": "2022",
      "venue": "ACM TOSEM"
    },
    {
      "citation_id": "3",
      "title": "An exploratory qualitative and quantitative analysis of emotions in issue report comments of open source systems",
      "authors": [
        "A Murgia",
        "M Ortu",
        "P Tourani",
        "B Adams",
        "S Demeyer"
      ],
      "year": "2018",
      "venue": "Empirical Software Engineering"
    },
    {
      "citation_id": "4",
      "title": "Emtk-the emotion mining toolkit",
      "authors": [
        "F Calefato",
        "F Lanubile",
        "N Novielli",
        "L Quaranta"
      ],
      "year": "2019",
      "venue": "IEEE/ACM 4th International Workshop on SEmotion. IEEE"
    },
    {
      "citation_id": "5",
      "title": "Emoji-powered sentiment and emotion detection from software developers' communication data",
      "authors": [
        "Z Chen",
        "Y Cao",
        "H Yao",
        "X Lu",
        "X Peng",
        "H Mei",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Emoji-powered sentiment and emotion detection from software developers' communication data"
    },
    {
      "citation_id": "6",
      "title": "The emotional side of software developers in jira",
      "authors": [
        "M Ortu",
        "A Murgia",
        "G Destefanis",
        "P Tourani",
        "R Tonelli",
        "M Marchesi",
        "B Adams"
      ],
      "year": "2016",
      "venue": "Proceedings of the 13th MSR"
    },
    {
      "citation_id": "7",
      "title": "A gold standard for emotion annotation in stack overflow",
      "authors": [
        "N Novielli",
        "F Calefato",
        "F Lanubile"
      ],
      "year": "2018",
      "venue": "IEEE/ACM 15th MSR"
    },
    {
      "citation_id": "8",
      "title": "A two-stage framework for ambiguous classification in software engineering",
      "authors": [
        "J Li",
        "Y Lei",
        "S Li",
        "H Zhou",
        "Y Yu",
        "Z Jia",
        "Y Ma",
        "T Wang"
      ],
      "year": "2023",
      "venue": "A two-stage framework for ambiguous classification in software engineering"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "10",
      "title": "Data augmentation for improving emotion recognition in software engineering communication",
      "authors": [
        "M Imran",
        "Y Jain",
        "P Chatterjee",
        "K Damevski"
      ],
      "year": "2022",
      "venue": "Proceedings of the 37th ASE"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition on stackoverflow posts using bert",
      "authors": [
        "D Bleyl",
        "E Buxton"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Big Data"
    },
    {
      "citation_id": "12",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach"
    },
    {
      "citation_id": "13",
      "title": "Fast changeset-based bug localization with bert",
      "authors": [
        "A Ciborowska",
        "K Damevski"
      ],
      "year": "2022",
      "venue": "Proceedings of the 44th ICSE"
    },
    {
      "citation_id": "14",
      "title": "An empirical study on the usage of bert models for code completion",
      "authors": [
        "M Ciniselli",
        "N Cooper",
        "L Pascarella",
        "D Poshyvanyk",
        "M Penta",
        "G Bavota"
      ],
      "year": "2021",
      "venue": "2021 IEEE/ACM 18th MSR"
    },
    {
      "citation_id": "15",
      "title": "Sentiment analysis for software engineering: How far can we go",
      "authors": [
        "B Lin",
        "F Zampetti",
        "G Bavota",
        "M Di Penta",
        "M Lanza",
        "R Oliveto"
      ],
      "year": "2018",
      "venue": "Proceedings of the 40th ICSE"
    },
    {
      "citation_id": "16",
      "title": "A multi-step learning approach to assist code review",
      "authors": [
        "O Sghaier",
        "H Sahraoui"
      ],
      "year": "2023",
      "venue": "2023 IEEE SANER"
    },
    {
      "citation_id": "17",
      "title": "Sentiment analysis for software engineering: How far can pre-trained transformer models go?",
      "authors": [
        "T Zhang",
        "B Xu",
        "F Thung",
        "S Haryono",
        "D Lo",
        "L Jiang"
      ],
      "venue": "Sentiment analysis for software engineering: How far can pre-trained transformer models go?"
    },
    {
      "citation_id": "18",
      "title": "A benchmark study of the contemporary toxicity detectors on software engineering interactions",
      "authors": [
        "J Sarker",
        "A Turzo",
        "A Bosu"
      ],
      "year": "2020",
      "venue": "A benchmark study of the contemporary toxicity detectors on software engineering interactions"
    },
    {
      "citation_id": "19",
      "title": "Sentiment in software engineering: detection and application",
      "authors": [
        "N Cassee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"
    },
    {
      "citation_id": "20",
      "title": "Emotion knowledge: further exploration of a prototype approach",
      "authors": [
        "P Shaver",
        "J Schwartz",
        "D Kirson",
        "C O'connor"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "21",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "B Felbo",
        "A Mislove",
        "A S√∏gaard",
        "I Rahwan",
        "S Lehmann"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on EMNLP"
    },
    {
      "citation_id": "22",
      "title": "Bert-based sentiment analysis: A software engineering perspective",
      "authors": [
        "H Batra",
        "N Punn",
        "S Sonbhadra",
        "S Agarwal"
      ],
      "year": "2021",
      "venue": "Database and Expert Systems Applications: 32nd International Conference, DEXA 2021"
    },
    {
      "citation_id": "23",
      "title": "Auto-absa: automatic detection of aspects in aspect-based sentiment analysis",
      "authors": [
        "T Wang",
        "B Sun",
        "Y Tong"
      ],
      "year": "2022",
      "venue": "Auto-absa: automatic detection of aspects in aspect-based sentiment analysis",
      "arxiv": "arXiv:2202.00484"
    },
    {
      "citation_id": "24",
      "title": "Assessing generalizability of codebert",
      "authors": [
        "X Zhou",
        "D Han",
        "D Lo"
      ],
      "year": "2021",
      "venue": "2021 IEEE ICSME"
    },
    {
      "citation_id": "25",
      "title": "What do pre-trained code models know about code?",
      "authors": [
        "A Karmakar",
        "R Robbes"
      ],
      "year": "2021",
      "venue": "2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
    },
    {
      "citation_id": "26",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "P He",
        "X Liu",
        "J Gao",
        "W Chen"
      ],
      "year": "2020",
      "venue": "arXiv"
    },
    {
      "citation_id": "27",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for self-supervised learning of language representations"
    },
    {
      "citation_id": "28",
      "title": "Codebert: A pre-trained model for programming and natural languages",
      "authors": [
        "Z Feng",
        "D Guo",
        "D Tang",
        "N Duan",
        "X Feng",
        "M Gong",
        "L Shou",
        "B Qin",
        "T Liu",
        "D Jiang"
      ],
      "venue": "Codebert: A pre-trained model for programming and natural languages"
    },
    {
      "citation_id": "29",
      "title": "Graphcodebert: Pre-training code representations with data flow",
      "authors": [
        "D Guo",
        "S Ren",
        "S Lu",
        "Z Feng",
        "D Tang",
        "S Liu",
        "L Zhou",
        "N Duan",
        "A Svyatkovskiy",
        "S Fu"
      ],
      "year": "2020",
      "venue": "Graphcodebert: Pre-training code representations with data flow"
    },
    {
      "citation_id": "30",
      "title": "Hugging face",
      "year": "2023",
      "venue": "Hugging face"
    },
    {
      "citation_id": "31",
      "title": "One vs all",
      "year": "2023",
      "venue": "One vs all"
    },
    {
      "citation_id": "32",
      "title": "Variance reduction",
      "authors": [
        "Z Botev",
        "A Ridder"
      ],
      "year": "2017",
      "venue": "Wiley statsRef: Statistics reference online"
    },
    {
      "citation_id": "33",
      "title": "A benchmark study on sentiment analysis for software engineering research",
      "authors": [
        "N Novielli",
        "D Girardi",
        "F Lanubile"
      ],
      "year": "2018",
      "venue": "Proceedings of the 15th MSR"
    },
    {
      "citation_id": "34",
      "title": "Skep: Sentiment knowledge enhanced pre-training for sentiment analysis",
      "authors": [
        "H Tian",
        "C Gao",
        "X Xiao",
        "H Liu",
        "B He",
        "H Wu",
        "H Wang",
        "F Wu"
      ],
      "year": "2020",
      "venue": "Skep: Sentiment knowledge enhanced pre-training for sentiment analysis"
    },
    {
      "citation_id": "35",
      "title": "An empirical study of sentiment-enhanced pre-training for aspect-based sentiment analysis",
      "authors": [
        "Y Zhang",
        "Y Yang",
        "B Liang",
        "S Chen",
        "B Qin",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Findings of the ACL 2023"
    },
    {
      "citation_id": "36",
      "title": "Sentilare: Sentiment-aware language representation learning with linguistic knowledge",
      "authors": [
        "P Ke",
        "H Ji",
        "S Liu",
        "X Zhu",
        "M Huang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on EMNLP"
    },
    {
      "citation_id": "37",
      "title": "Shedding light on software engineering-specific metaphors and idioms",
      "authors": [
        "M Imran",
        "P Chatterjee",
        "K Damevski"
      ],
      "year": "2023",
      "venue": "Shedding light on software engineering-specific metaphors and idioms",
      "arxiv": "arXiv:2312.10297"
    },
    {
      "citation_id": "38",
      "title": "Sentix: A sentiment-aware pre-trained model for cross-domain sentiment analysis",
      "authors": [
        "J Zhou",
        "J Tian",
        "R Wang",
        "Y Wu",
        "W Xiao",
        "L He"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "39",
      "title": "Pyabsa: A modularized framework for reproducible aspect-based sentiment analysis",
      "authors": [
        "H Yang",
        "C Zhang",
        "K Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM CIKM"
    },
    {
      "citation_id": "40",
      "title": "A survey on data augmentation for text classification",
      "authors": [
        "M Bayer",
        "M.-A Kaufhold",
        "C Reuter"
      ],
      "year": "2022",
      "venue": "A survey on data augmentation for text classification"
    },
    {
      "citation_id": "41",
      "title": "Chatgpt: Jack of all trades, master of none",
      "authors": [
        "J Kocon",
        "I Cichecki",
        "O Kaszyca",
        "M Kochanek",
        "D Szyd≈Ço",
        "J Baran",
        "J Bielaniewicz",
        "M Gruza",
        "A Janz",
        "K Kanclerz"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "42",
      "title": "Clarin-emo: Training emotion recognition models using human annotation and chatgpt",
      "authors": [
        "B Koptyra",
        "A Ngo",
        "≈Å Radli≈Ñski",
        "J Koco≈Ñ"
      ],
      "year": "2023",
      "venue": "International Conference on Computational Science"
    },
    {
      "citation_id": "43",
      "title": "Generative pretrained transformers for emotion detection in a code-switching setting",
      "authors": [
        "A Nedilko"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
      "citation_id": "44",
      "title": "Emotions in the software development process",
      "authors": [
        "M Wrobel"
      ],
      "year": "2013",
      "venue": "2013 6th International Conference on Human System Interactions (HSI)"
    },
    {
      "citation_id": "45",
      "title": "Uncovering the causes of emotions in software developer communication using zero-shot llms",
      "authors": [
        "M Imran",
        "P Chatterjee",
        "K Damevski"
      ],
      "year": "2023",
      "venue": "Uncovering the causes of emotions in software developer communication using zero-shot llms",
      "arxiv": "arXiv:2312.09731"
    },
    {
      "citation_id": "46",
      "title": "Security and emotion: sentiment analysis of security discussions on github",
      "authors": [
        "D Pletea",
        "B Vasilescu",
        "A Serebrenik"
      ],
      "year": "2014",
      "venue": "Proceedings of the 11th working conference on mining software repositories"
    },
    {
      "citation_id": "47",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "48",
      "title": "Mining valence, arousal, and dominance: possibilities for detecting burnout and productivity",
      "authors": [
        "M Mantyla",
        "B Adams",
        "G Destefanis",
        "D Graziotin",
        "M Ortu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 13th MSR"
    },
    {
      "citation_id": "49",
      "title": "Marvalous: Machine learning based detection of emotions in the valence-arousal space in software engineering text",
      "authors": [
        "M Islam",
        "M Ahmmed",
        "M Zibran"
      ],
      "year": "2019",
      "venue": "Proceedings of the 34th ACM/SIGAPP SAC"
    },
    {
      "citation_id": "50",
      "title": "Meme: toward a method for emotions extraction from github",
      "authors": [
        "K Werder",
        "S Brinkkemper"
      ],
      "year": "2018",
      "venue": "Proceedings of the 3rd International Workshop on SEmotion"
    },
    {
      "citation_id": "51",
      "title": "On measuring affects of github issues' commenters",
      "authors": [
        "G Destefanis",
        "M Ortu",
        "D Bowes",
        "M Marchesi",
        "R Tonelli"
      ],
      "year": "2018",
      "venue": "Proceedings of the 3rd International Workshop on SEmotion"
    },
    {
      "citation_id": "52",
      "title": "Mining communication patterns in software development: A github analysis",
      "authors": [
        "M Ortu",
        "T Hall",
        "M Marchesi",
        "R Tonelli",
        "D Bowes",
        "G Destefanis"
      ],
      "year": "2018",
      "venue": "Proceedings of the 14th PROMISE"
    },
    {
      "citation_id": "53",
      "title": "Understanding emotions of developer community towards software documentation",
      "authors": [
        "A Venigalla",
        "S Chimalakonda"
      ],
      "year": "2021",
      "venue": "2021 IEEE/ACM 43rd ICSE: Software Engineering in Society (ICSE-SEIS)"
    },
    {
      "citation_id": "54",
      "title": "Emod: An end-to-end approach for investigating emotion dynamics in software development",
      "authors": [
        "K Neupane",
        "K Cheung",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE ICSME"
    },
    {
      "citation_id": "55",
      "title": "An empirical study of emoji use in software development communication",
      "authors": [
        "S Rong",
        "W Wang",
        "U Mannan",
        "E De Almeida",
        "S Zhou",
        "I Ahmed"
      ],
      "year": "2022",
      "venue": "Information and Software Technology"
    },
    {
      "citation_id": "56",
      "title": "Burnout in software engineering: A systematic mapping study",
      "authors": [
        "T Tulili",
        "A Capiluppi",
        "A Rastogi"
      ],
      "year": "2023",
      "venue": "Information and Software Technology"
    },
    {
      "citation_id": "57",
      "title": "Code structure-guided transformer for source code summarization",
      "authors": [
        "S Gao",
        "C Gao",
        "Y He",
        "J Zeng",
        "L Nie",
        "X Xia",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Software Engineering and Methodology"
    },
    {
      "citation_id": "58",
      "title": "Semantic similarity metrics for evaluating source code summarization",
      "authors": [
        "S Haque",
        "Z Eberhart",
        "A Bansal",
        "C Mcmillan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension"
    },
    {
      "citation_id": "59",
      "title": "Experiences from using code explanations generated by large language models in a web software development e-book",
      "authors": [
        "S Macneil",
        "A Tran",
        "A Hellas",
        "J Kim",
        "S Sarsa",
        "P Denny",
        "S Bernstein",
        "J Leinonen"
      ],
      "year": "2023",
      "venue": "Proc. 54th ACM Tech. Symp. on CS Education V. 1"
    },
    {
      "citation_id": "60",
      "title": "A review on source code documentation",
      "authors": [
        "S Rai",
        "R Belwal",
        "A Gupta"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "61",
      "title": "Using bert to predict bug-fixing time",
      "authors": [
        "P Ardimento",
        "C Mele"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)"
    },
    {
      "citation_id": "62",
      "title": "Clebpi: Contrastive learning for bug priority inference",
      "authors": [
        "W.-Y Wang",
        "C.-H Wu",
        "J He"
      ],
      "year": "2023",
      "venue": "IST"
    },
    {
      "citation_id": "63",
      "title": "Bert based severity prediction of bug reports for the maintenance of mobile applications",
      "authors": [
        "A Ali",
        "Y Xia",
        "Q Umer",
        "M Osman"
      ],
      "year": "2023",
      "venue": "JSS"
    },
    {
      "citation_id": "64",
      "title": "Does bert understand code?an exploratory study on the detection of architectural tactics in code",
      "authors": [
        "J Keim",
        "A Kaplan",
        "A Koziolek",
        "M Mirakhorli"
      ],
      "year": "2020",
      "venue": "European Conference on Software Architecture"
    },
    {
      "citation_id": "65",
      "title": "An exploratory study on code attention in bert",
      "authors": [
        "R Sharma",
        "F Chen",
        "F Fard",
        "D Lo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th IEEE/ACM ICPC"
    },
    {
      "citation_id": "66",
      "title": "Aspect-based api review classification: How far can pre-trained transformer model go?",
      "authors": [
        "C Yang",
        "B Xu",
        "J Khan",
        "G Uddin",
        "D Han",
        "Z Yang",
        "D Lo"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on SANER"
    },
    {
      "citation_id": "67",
      "title": "Ptm4tag: sharpening tag recommendation of stack overflow posts with pre-trained models",
      "authors": [
        "J He",
        "B Xu",
        "Z Yang",
        "D Han",
        "C Yang",
        "D Lo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th IEEE/ACM ICPC"
    }
  ]
}