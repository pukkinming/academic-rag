{
  "paper_id": "2001.01456v2",
  "title": "A Robust System For Facial Emotions Recognition Using Convolutional Neural Network",
  "published": "2020-01-06T09:43:06Z",
  "authors": [
    "Faisal Ghaffar"
  ],
  "keywords": [
    "Facial Emotions",
    "Prediction",
    "Detection",
    "CNN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "I. INTRODUCTION UMAN facial expressions have a significant role in involvement, engaging, or communicating with each other. A machine with high accuracy and powerful expression recognition intelligence will better interpret human emotions and interact more naturally  [1, 2] . A face detection system is used to detect faces in an image/video and then processes those images to obtain visible facial features, while the emotion recognition system detects faces and recognizes the respective emotion  [3] . Many researchers have studied and conducted experiments to produce better results and, ultimately, a system that recognizes emotions. In real-world applications such as commercial call centers, the system of emotions screening during the interview, screening behavior of a student in a class or MOOCs, and affect-aware game development will significantly benefit from such a system. Thus, developing an efficient, low complexity, and robust system that can provide a probabilistic score for each emotion class is crucial. Also, developing an application for facial emotion detection from live video and still images is essential for many real-world scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. The Proposed System",
      "text": "Figure  1  shows the block diagram of the proposed systems. First, in a given image, the face is detected, and then it is extracted. The extracted face goes under extensive preprocessing and is then fed to the convolution neural network (CNN), assigning emotion probability to each emotion. The image with the highest probability is given to the respective emotion class. These are discussed in detail in the following.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Face Detection",
      "text": "Many algorithms are proposed for face detection, like, Haar cascade, HOG +, Support Vector Machine, or deep learning models. Our system utilized the algorithm proposed in  [20] , which is faster and can work in real-time. This algorithm finds the locations of 68 (x, y) coordinates.\n\nEach of these represents a location of the region of interest on the face. Those 68 points are shown in Figure  2 . This algorithm uses a training set of labeled facial points on an image, which is manually labeled. Those labels are (x, y) coordinates that specify regions of the face. Points 1-17 define the jawline, 18-22 specify the left eyebrow, 23-27 specify the right eyebrow, 37-42 encircle the left eye region, 43-48 encircle the right eyes region, 28-36 specify the region of the nose, 49-60 specify the outer lip area, while 60-68 specifies inner lip structure. This data is then used for the training of a model of regression trees to find these facial landmark points directly from the pixel intensities. There is no feature extraction in this method; hence the detection is very fast, i.e., in real-time and with high accuracy and quality. We do not need ear regions and regions above mid of the forehead because emotion mainly deals with the eyes, nose, eyebrows regions, and some facial regions. So, this face detection algorithm gives us the exact facial areas we are interested in. Figure  3  shows some results of our detected faces and areas according to the 68 points model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Face Extraction",
      "text": "After detecting the faces, the next step is to extract that face region from the original image and drop the rest. We first extracted 1-27 points locations from 68 points facial landmarks. These 1-27 points cover the overall face, which deals with facial expressions. To find the minimum enclosed structure of a set, we use a convex hull. The convex hull finds the minimum possible enclosed structure of the set of points. Thus, we applied a convex hull to those 27 points to create an enclosed structure boundary of the face. Figure  4  (Left) shows a convex hull of 0-27 points. To make a mask to extract the face, we fill",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Preprocessing",
      "text": "The mask was then applied to images to extract the face area. Figure  5  shows the extracted face images. Since deep learning learns from data, thus, it is crucial to feed proper data to the model. Thus, we apply a series of preprocessing before feeding the data into CNN. First, we used histogram equalization on the cropped face images for intensity normalization and contrast enhancement. We then used the bilateral filter to remove noise while preserving the edges. A bilateral filter combines both domain and range filtering. Figure  6  shows how the bilateral filter removes noise and preserves high-frequency edges.\n\nThen we apply convolutional 2D filter with a kernel of (\n\nWe then resized all images to 80*100. An array of training and test images were made to pass it into the CNN model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Cnn Architecture",
      "text": "The proposed CCN model has five convolutional layers, one max pooling layer, two average pooling layers, and three dense layers. Max pooling chooses the maximum response, while the average filter calculates the average of the responses. The dense layers contain a  dropout of 20%. This dropout in connected layers prevents the model from overfitting. We have train data of shape (14210,100,80) and test data of shape (1580,100,80). The shape tells us that there are 14210 training instances and 1580 test instances, each of which has a dimension of 80*100, the size of our image. The input layer accepts 80*100-dimension images and passes the data of the same dimension to the first convolutional layer. The convolutional layer has 64 filters and a kernel of size  (5, 5)  and has the activation function ReLu. The layer gives an output (76,96,64), which is passed to the max pooling layer with a pool size of (5,5) and strides of (2,2). A result of size (36,46,64) is obtained, then passed to two convolutional layers in series. Both have 64 filters, kernel size (3,3), and a relu activation function. The output of the second convolutional layer in the series has a size of 32,42,64. An average pooling of pool size (3,3) and strides (2,2) is connected next in the series. The output size of this average pooling layer is  (15, 20, 64) . It is further associated with two consecutive convolutional layers with 128 filters and kernel size  (3, 3)  and has the activation function relu. The output of these last convolutional layers has the size of  (11, 16, 128) . To this output, average pooling with the same pool size and stride as the earlier one is applied. The output of this layer has a size (5,7,128), which is then flattened 5*7*128 = 4480, passed to three fully connected layers with 1024 filters each.\n\nThe dropout rate of each layer is 0.2. Finally, the layer is dense with the activation function SoftMax, which gives the output of size 7. These seven instances represent the probability of each class. In the convolutional layer activation function, relu is used instead of sigmoid because it reduces the likelihood of gradient becoming too small and almost disappearing, and also relu results in sparse representation while sigmoid results in dense representation. The earlier is much better than the latter. The proposed CNN architecture is shown in Figure  7 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Evaluation",
      "text": "This section presents the dataset details and evaluates our proposed systems.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "We have combined two datasets Japanese Female Facial Expression dataset, and Karolinska Directed Emotional Faces Dataset  [21, 22] . The Japanese Female Expression dataset contains 213 static images of 10 models. The images are grayscale, and the resolution is 256*256. All the images are posted. The dataset includes 4900 pictures from different models. All images are grayscale with resolution 572*762. The dataset contains images of 70 models. There is an equal distribution of males and females. The images are taken in the same lighting conditions with no makeup, glass, or earrings.\n\nEach model is photographed from 5 different views (frontal, entire left, half left, full right, and half right). The mouth and eyes are positioned on a grid at a specific place, and then images are cropped at a specified resolution. The age of the model is in between 20 to 30 years. Both datasets contain seven facial expressions (angry, disgust, fear, happy, sad, surprise, neutral), which we labeled as (0, 1,\n\n, respectively. We have combined both datasets according to each emotion label. After preprocessing and duplicating data in",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Results",
      "text": "We trained the network with a batch size of 100 and 100 epochs for layers in {1, 2, 3, 4, 5, 6, 7, 8} Epochs are one complete pass-over training set instance. We varied the number of CNN layers from 1 to 8 to find the best accuracy. We found that the best accuracy in our setting is obtained on five layers. The execution time increased with the number of layers without improving the accuracy. Figure  9  shows the trend of accuracy against the number of layers. The accuracy of 70.2% was achieved.\n\nWe further optimize the model by varying the number of epochs in {25, 50, 100, 200, 300, 400, 500}. We find that the accuracy increases with the number of epochs, as shown in Figure  10 . However, we observe that the accuracy change is minor after the epochs increase from 300 to 500. The accuracy change is minor compared to the cost of the number of epochs. Thus, we selected the 300 epochs with an accuracy of 78.1 for comparison with other models.\n\nTable  1  shows the Precision, Recall, F1-score, and Support for all the seven emotion types and the average value. The labels 0 represent angry, 1 express disgust, 2 represent fear, 3 represent happy, 4 represent sad, 5 represent surprise, and 6 represent neutral. We observe that the proposed model performs better for each emotion. We trained and reported the results of the other models for the same number of epochs and layers, i.e., five layers and 300 epochs. We tested state-of-the-art models like AlextNet, VGG, GoogleNet, and ResNet, as shown in Table  2 . In terms of accuracy, the proposed CNN models outperform existing standard networks. All networks' accuracy is slightly lower because the dataset is small and challenging. We anticipate that the proposed system will have improved accuracy under large datasets training. However, our aim was to design a system with good accuracy under small datasets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Graphical User Interface",
      "text": "The trained model was then turned into an application. The application developed works in dual mode, which works on still images and live video feeds. The user has to select between each mood. Preprocessing and labeling each image takes about 20-30ms.\n\nThe interface shows the video's original image or frame and the cropped face image. Two labels have been included in the interface.\n\nOne label shows the probabilities of all the labels, while another shows the predicted emotion. The average probability of the related feelings of all the images is calculated to increase the chances of proper label selection, i.e., accuracy.   The probabilities in all three images are displayed at the bottom.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this article, we proposed an efficient system for facial emotion recognition using a convolution neural network, which predicts and assigns probabilities to each facial emotion. Moreover, our proposed system processes each image with diverse preprocessing steps for better prediction, as deep learning models learn from data. The results show that the proposed system performs better compared to other models. We then designed an application of the proposed system with a graphical user interface that classifies emotions in realtime. The results and the live display show that the system has quite a promising performance. Complex network leads to heavy models, which perform poorly during live video. We also observed that although the preprocessing stage is essential, unnecessary preprocessing causes the loss of many features and may lead to poor results. In the future, we aim to further improve the system prediction performance and deploy the proposed system to emotion scanning in a live interview system.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the block diagram of the proposed systems. First, in a",
      "page": 2
    },
    {
      "caption": "Figure 2: This algorithm uses a",
      "page": 2
    },
    {
      "caption": "Figure 3: shows some results of our detected faces and areas according to the",
      "page": 2
    },
    {
      "caption": "Figure 4: (Left) shows a",
      "page": 2
    },
    {
      "caption": "Figure 1: The block diagram of the proposed system.",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualizing the 68 facial landmark coordinates [20].",
      "page": 2
    },
    {
      "caption": "Figure 3: Face detection and face area estimation",
      "page": 2
    },
    {
      "caption": "Figure 4: (Right) shows the mask obtained after",
      "page": 3
    },
    {
      "caption": "Figure 5: shows the extracted face images. Since deep learning learns from",
      "page": 3
    },
    {
      "caption": "Figure 6: shows how",
      "page": 3
    },
    {
      "caption": "Figure 5: Face extraction process",
      "page": 3
    },
    {
      "caption": "Figure 4: (Left) Convex hull of 0-27 facial landmarks, (Right) Mask",
      "page": 3
    },
    {
      "caption": "Figure 7: The proposed CNN architecture.",
      "page": 3
    },
    {
      "caption": "Figure 6: Bilateral filter noise removal and edge-preserving",
      "page": 3
    },
    {
      "caption": "Figure 7: III. EVALUATION",
      "page": 4
    },
    {
      "caption": "Figure 8: The emotion dataset with (a) training and (d) testing data",
      "page": 4
    },
    {
      "caption": "Figure 9: Accuracy vs. different number of layers.",
      "page": 4
    },
    {
      "caption": "Figure 10: Accuracy vs. diverse number of epochs",
      "page": 4
    },
    {
      "caption": "Figure 8: B. Results",
      "page": 5
    },
    {
      "caption": "Figure 10: However, we",
      "page": 5
    },
    {
      "caption": "Figure 11: (Top) shows a happy",
      "page": 5
    },
    {
      "caption": "Figure 11: (Middle) is a continuation frame in which the emotion",
      "page": 5
    },
    {
      "caption": "Figure 11: (Bottom) shows a",
      "page": 5
    },
    {
      "caption": "Figure 11: Images display in the graphical user interface. (Top): a happy",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Facial expressions vary from person to person, and the brightness, contrast, and resolution of every random image\nare different. This is why recognizing facial expressions is very difficult. This article proposes an efficient system for\nfacial emotion recognition for the seven basic human emotions (angry, disgust, fear, happy, sad, surprise, and\nneutral), using a convolution neural network (CNN), which predicts and assigns probabilities to each emotion. Since\ndeep learning models learn from data, thus, our proposed system processes each image with various pre-processing\nsteps for better prediction. Every image was first passed through the face detection algorithm to include in the\ntraining dataset. As CNN requires a large amount of data, we duplicated our data using various filters on each\nimage. Pre-processed images of size 80*100 are passed as input to the first layer of CNN. Three convolutional layers\nwere used, followed by a pooling layer and three dense layers. The dropout rate for the dense layer was 20%. The\nmodel was trained by combining two publicly available datasets, JAFFE and KDEF. 90% of the data was used for\ntraining, while 10% was used for testing. We achieved maximum accuracy of 78.1 % using the combined dataset.\nMoreover, we designed an application of the proposed system with a graphical user interface that classifies\nemotions in real-time."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "KEYWORDS": "Facial Emotions,\nPrediction, Detection,\nCNN"
        }
      ],
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Ensemble methods for spoken emotion recognition in call-centers",
      "authors": [
        "D Morrison",
        "De Silva"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "2",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Facial Action Coding System, A Human Face",
      "authors": [
        "P Ekman",
        "W Friesen",
        "J Hager"
      ],
      "year": "2002",
      "venue": "ETC What is the ETC"
    },
    {
      "citation_id": "4",
      "title": "Original approach for the localisation of objects in images",
      "authors": [
        "R Vaillant",
        "C Monrocq",
        "Le Cun"
      ],
      "year": "1994",
      "venue": "IEE Proceedings-Vision, Image and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Neural network-based face detection",
      "authors": [
        "H Rowley",
        "Baluja Kanade"
      ],
      "year": "1998",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "6",
      "title": "Fddb: A benchmark for face detection in unconstrained settings",
      "authors": [
        "V Jain",
        "E Learned-Miller"
      ],
      "year": "2010",
      "venue": "UMass Amherst technical report"
    },
    {
      "citation_id": "7",
      "title": "Face detection, pose estimation, and landmark localization in the wild",
      "authors": [
        "X Zhu",
        "D Ramanan"
      ],
      "venue": "Face detection, pose estimation, and landmark localization in the wild"
    },
    {
      "citation_id": "8",
      "title": "Face detection by structural models",
      "authors": [
        "J Yan",
        "X Zhang",
        "Z Lei"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "9",
      "title": "Joint cascade face detection and alignment",
      "authors": [
        "D Chen",
        "Ren Wei",
        "Y Cao"
      ],
      "year": "2014",
      "venue": "Joint cascade face detection and alignment"
    },
    {
      "citation_id": "10",
      "title": "High-performance rotation invariant multiview face detection",
      "authors": [
        "C Huang",
        "Ai Li",
        "Y Lao"
      ],
      "year": "2007",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "11",
      "title": "A deep pyramid deformable part model for face detection",
      "authors": [
        "R Ranjan",
        "V Patel",
        "R Chellappa"
      ],
      "year": "2015",
      "venue": "2015 IEEE 7th international conference on biometrics theory, applications and systems (BTAS)"
    },
    {
      "citation_id": "12",
      "title": "Convolutional channel features",
      "authors": [
        "B Yang",
        "Yan Lei"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "13",
      "title": "Aggregate channel features for multi-view face detection",
      "authors": [
        "B Yang",
        "Yan Lei"
      ],
      "year": "2014",
      "venue": "IEEE international joint conference on biometrics"
    },
    {
      "citation_id": "14",
      "title": "Multi-view face detection using deep convolutional neural networks",
      "authors": [
        "S Farfade",
        "M Saberian"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "15",
      "title": "A convolutional neural network cascade for face detection",
      "authors": [
        "H Li",
        "Lin Shen",
        "X Brandt",
        "J Hua"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "16",
      "title": "Fully automatic facial action recognition in spontaneous behavior",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "M Frank",
        "C Lainscsek",
        "I Fasel"
      ],
      "year": "2006",
      "venue": "7th International Conference on Automatic Face and Gesture Recognition (FGR06)"
    },
    {
      "citation_id": "17",
      "title": "Facial action recognition for facial expression analysis from static face images",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "18",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H.-W Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "20",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "21",
      "title": "Japanese female facial expressions (JAFFE)",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1997",
      "venue": "Database of digital images"
    },
    {
      "citation_id": "22",
      "title": "Karolinska directed emotional faces",
      "authors": [
        "D Lundqvist",
        "A Flykt"
      ],
      "year": "1998",
      "venue": "Cognition and Emotion"
    }
  ]
}