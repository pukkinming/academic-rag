{
  "paper_id": "2405.03960v1",
  "title": "Esihgnn: Event-State Interactions Infused Heterogeneous Graph Neural Network For Conversational Emotion Recognition",
  "published": "2024-05-07T02:46:11Z",
  "authors": [
    "Xupeng Zha",
    "Huan Zhao",
    "Zixing Zhang"
  ],
  "keywords": [
    "Conversational Emotion Recognition",
    "Event-State Interactions",
    "Heterogeneous Knowledge Graph"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conversational Emotion Recognition (CER) aims to predict the emotion expressed by an utterance (referred to as an \"event\") during a conversation. Existing graph-based methods mainly focus on event interactions to comprehend the conversational context, while overlooking the direct influence of the speaker's emotional state on the events. In addition, real-time modeling of the conversation is crucial for real-world applications but is rarely considered. Toward this end, we propose a novel graph-based approach, namely Event-State Interactions infused Heterogeneous Graph Neural Network (ESIHGNN), which incorporates the speaker's emotional state and constructs a heterogeneous event-state interaction graph to model the conversation. Specifically, a heterogeneous directed acyclic graph neural network is employed to dynamically update and enhance the representations of events and emotional states at each turn, thereby improving conversational coherence and consistency. Furthermore, to further improve the performance of CER, we enrich the graph's edges with external knowledge. Experimental results on four publicly available CER datasets show the superiority of our approach and the effectiveness of the introduced heterogeneous event-state interaction graph.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Evidence from psychology suggests that human actions are influenced by numerous factors, including environmental events and emotional ambiance  [1, 2] . Understanding the decision-making process underlying human actions is crucial, as it can reflect individuals' subjective evaluations of these factors. Recognizing emotions in conversations is an important foundation for this endeavor  [3, 4] .\n\nConversational Emotion Recognition (CER) is a crucial cognitive task that aims to identify the emotions conveyed by speakers through their utterances (referred to as \"events\") during a conversation. CER has attracted significant attention in multiple research fields, as it serves as the foundation for developing conversational agents with emotional intelligence. These agents find applications in domains such as virtual reality therapy  [5] , social robotics  [6] , and smart home systems  [7] . In reality, the flow of a conversation is influenced by both previous events and the emotional states of the participants, which can trigger new events and update their emotions. Therefore, accurately modeling the real-time conversation driven by previous events and emotional states is crucial for a successful CER.\n\nRecent studies have focused on integrating speaker information into conversation modeling. Two approaches have been researched: recurrence-based methods and graph-based methods. Recurrence-based methods encode an event flow and a parallel state flow, and build interactions between them to dynamically model the conversation. For example, DialogueRNN  [8]  tracks the state of each speaker along the event flow using different Gated Recurrent Units (GRUs). However, it has limitations in terms of scalability and capacity to efficiently capture the interactions between events and speakers' states. To tackle scalability, DialogueCRN  [9] , CoMPM  [10] , and DialogueINAB  [11]  propose a unified speaker-level emotion tracking module. To enhance natural interactions between events and states, COSMIC  [12]  extends DialogueRNN with external knowledge. However, it still remains challenging to balance long-and short-term memory within each utterance. Moreover, these methods often require bidirectional context modeling, which sacrifices real-time conversational abilities while improving performance.\n\nOn the other hand, graph-based methods construct a conversation structure graph constrained by speaker identity to model the conversation using Graph Neural Networks (GNNs). Dia-logueGCN  [13] , for example, represents events as nodes and connections between speakers as edges to gather contextual information from neighboring nodes within a specific window. RGAT  [14]  expands upon DialogueGCN by including position encodings to consider sequential information. Meanwhile, DAG-ERC  [15]  constructs a directed acyclic graph inspired by the Directed Acyclic Graph Neural Network (DAGNN)  [16]  to sequentially model context. Similar to COSMIC, SKAIG  [17]  enhances event interactions with external knowledge, while knowledge also plays an essential role in both KET  [18]  and KI-Net  [19] . In summary, graph-based methods effectively model conversations by constructing interaction networks. However, these methods overlook the emotional states of speakers during a conversation and focus only on event interactions.\n\nAccording to the aforementioned observations, it is important to develop an event-state interaction graph-based approach that can capture effective interactions between events and the emotions of participants for real-time conversation modeling. This paper proposes a solution named ESIHGNN (Event-State Interactions infused Heterogeneous Graph Neural Network) to meet these requirements. Specifically, we construct a heterogeneous event-state graph, where the event node is initialized with the semantic feature of the utterance and the accompanying state node representing the speaker's emotional state is initialized with the speaker identity. To support real-time conversation modeling and enhance natural interactions between the event nodes and state nodes, we define eight preliminary logical edge relations based on speaker identity, represented by external knowledge or trainable vectors. The resulting graph is then fed into an introduced HDAGNN (Heterogeneous Directed Acyclic Graph Neural Network) that recurrently updates each event and state node in a single layer by gathering information from previous events and emotional states. Additionally, we use GRUs to enhance the representations of event and state nodes at each turn, in order to improve conversational consistency. Our con-tributions are summarized as follows:  (1)  We propose an ESIHGNN, a novel approach that first treats a conversation as a heterogeneous event-state interaction graph for the CER task.  (2)  We introduce an HDAGNN, a heterogeneous directed acyclic graph neural network that dynamically models interactions between events and speakers' emotional states during real-time conversations. (3) Extensive experiments conducted on four benchmark datasets validate the effectiveness and superiority of the proposed ESIHGNN and confirm the advantages of the heterogeneous event-state interaction graph.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Our ESIHGNN comprises three primary modules: graph construction, HDAGNN for feature transformation, and emotion prediction. Figure  1  illustrates the ESIHGNN framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Task Definition",
      "text": "In a conversation, a series of N sequential utterances are encountered, denoted as [(u1, pu 1 ), (u2, pu 2 ), . . . , (uN , pu N )]. Here, ui represents the utterance of the i-th turn, and pu i indicates the speaker identity for ui. Note that pu i and pu j may belong to the same speaker, but there must be at least two speakers participating in the conversation. The task of the CER is to predict the emotion category for each utterance. For a target utterance ui, its prediction relies on real-time sequence pairs consisting of utterances and accompanying speaker identities: {(u1, pu 1 ), (u2, pu 2 ), . . . , (ui, pu i )}.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Construction",
      "text": "A new methodology for modeling conversation structure is presented, emphasizing the interactions between events and speakers' emotional states. This methodology constructs a heterogeneous event-state interaction graph, denoted as G = (V, E, R, A), where V represents the node set consisting of event nodes Ve and state nodes Vs. In this graph, an edge eij = (vi, r, vj) ∈ E signifies a connection from a node vi ∈ V to its neighboring node vj ∈ V under relation type r ∈ R, with its representation denoted by aij,r ∈ A. Nodes: For sequential utterances in a conversation, each utterance ui is regarded as an event node ve i , while the emotional state of ui' speaker is represented as a state node vs i . Clearly, both ve i and vs i correspond to the same speaker and turn. Consistent with COSMIC  [12]  and SKAIG  [17] , we employ the fine-tuned RoBERTa model  [20]  to encode the utterance ui as the initial event node feature he i . The state node feature hs i is initialized using a one-hot vector that indicates the speaker identity pu i . The context-independence of these node features is essential for modeling real-time conversations. Relations: To simulate a real-time conversation driven by previous events and speakers' emotional states, we suggest eight preliminary event-state interaction relations to capture the historical context.\n\nEvent Node Updating:\n\n• xWant: Intra-speaker event-to-event interaction: Event ei passes the speaker's action guidance to the subsequent event ej. • oWant: Inter-speaker event-to-event interaction: Event ei can coordinate and motivate the subsequent event ej triggered by another speaker. • xDrive: Intra-speaker state-to-event interaction: The speaker's emotional state si internally influences the trend of their subsequent event ej. • oDrive: Inter-speaker state-to-event interaction: The speaker's emotional state si, resulting from contagion and empathy, externally influences the trend of the subsequent event ej triggered by another speaker.\n\nState Node Updating:\n\n• xReact: Intra-speaker event-to-state interaction: When event ei is executed by the speaker, their subsequent state sj is influenced by their reaction to this event ei. • oReact: Inter-speaker event-to-state interaction: The reaction of another speaker to an event ei influences their subsequent state sj. To harness this potential, we employ COMET  [21] , an inferential, knowledge-based transformer model, to generate edge representations based on the input format from COSMIC  [12]  and SKAIG  [17] . For each edge eij = (vi, r, vj), COMET takes the concatenated predecessor vi and relation r as input, and extracts the hidden state of the relation token to serve as the edge representation aij,r. Note that COMET was trained on commonsense knowledge data consisting of explicit event prompts, enabling it to generate edge representations for predecessor event nodes. However, COMET does not generate edge representations for predecessor state nodes associated with implicit header entities, including xDrive, oDrive, xDepend, and oDepend. To address this limitation, we propose using 300-dimensional trainable random vectors, which allow the model to learn these implicit relations during training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hdagnn Layers",
      "text": "We will now describe the methodology for feature transformation using HDAGNN. In addition to establishing event-state interactions across different turns for conversational coherence (see Subsection 2.2), it is equally crucial to consider the event-state interactions within the same turn to improve conversational consistency. To achieve this, we propose establishing two implicit, bidirectional paths connecting the event and its accompanying emotional state.\n\nInter-Turn Event-State Interactions: To model real-time conversations, we use a dynamic and forward feature propagation strategy that enables current nodes to only receive information from previous nodes. For each node vi, the weight coefficients are calculated by normalizing the attention scores between the hidden feature of vi at the (l-1)-th layer and those of its neighborhood Ni at the l-th layer:\n\nwhere W l , W l r,v , W l r,a , and W l p are trainable parameters, and pj,r signifies the positional information of node vj using two dimensions. The first dimension of pj,r indicates the absolute position, providing Emotion Representations Labels Once obtained, the normalized coefficients are used to compute a linear combination of the features of vi's neighborhood Ni, producing aggregated information (referred to as \"message\"):",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Event-State Interactions",
      "text": "In HDAGNN, two GRUs are used to address the node heterogeneity and capture the sequential dependencies among nodes in an event-state interaction graph. The GRU takes the past feature h l-1 i of the node vi as its input and the message M l i as its hidden state.\n\nIntra-Turn Event-State Interactions: Within a turn, there are two event-state interaction paths. The first path involves the speaker being influenced by the emotional context when executing events. For the emotional context of the i-th turn, we specify M l s i instead of the updated hl s i that may have forgotten some emotional information.\n\nwhere M l s , h l-1 e , and hl e are the input, hidden state, and output of the GRU, respectively.\n\nThe second path involves the emotional state of the speaker at the i-th turn being influenced by the context of the event, M l e i :\n\nTo ensure conversational coherence and consistency, the hidden feature of the node vi at the l-th layer is the addition of hl i and hl i :",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Prediction",
      "text": "To preserve the original semantics of each node vi, we employ a concatenation operation ∥ to combine its hidden features from each layer l in HDAGNN, resulting in the final node representation Hv i . Additionally, considering node consistency within a conversation turn i, we take the sum of the event representation He i and the state representation Hs i as the emotion representation Hi of the utterance ui, and input it into a fully connected layer to predict the emotion label:\n\n3. EXPERIMENTS",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Setup",
      "text": "Datesets: Our ESIHGNN is evaluated on four widely-used benchmark datasets: IEMOCAP  [22] , MELD  [23] , EmoryNLP  [24] , and DailyDialog  [25] . The dataset statistics are presented in Table  1 . To assess performance, we apply the evaluation metrics utilized in previous studies  [14, 26] . This entails calculating micro-averaged F1 for DailyDialog and weighted-average F1 for the other datasets.\n\nBaselines: We compare our proposed ESIHGNN with state-ofthe-art baseline methods, including recurrence-based methods: Di-alogueRNN  [8] , COSMIC  [12] , DialogueCRN  [9] , BiERU  [27] , MVN  [28] , CoMPM  [10] , and DialogueINAB  [11] ; and graph-based methods: DialogueGCN  [13] , KET  [18] , KI-Net  [19] , RGAT  [14] , SKAIG  [17] , DAG-ERC  [15] , CoG-BART  [29] , and MM-DFN  [30] . Implementation Details: We use the PyTorch framework and the AdamW optimizer on two RTX 3090 GPUs for code implementation. Our approach involves performing hyperparameter searches for the learning rate, dropout rate, batch size, and number of HDAGNN layers. We set ω = 1 as the default setting for overall performance comparisons, but in Subsection 3.3 we present ablation results for ω ranging from 1 to 3. The initial dimensions for the node features and edge features are set to 1024 and 768, respectively. These dimensions are consistent across all hidden layers, remaining at 300. The reported results of our approach are the average of five test runs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "The results of the four datasets are summarized in",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "In our ablation study, we analyze the contributions of different components in our ESIHGNN: edge construction, edge representation, IntraESI (Intra-turn Event-State Interactions), and window size ω. The edge construction module distinguishes our work from previous approaches like DAG-ERC  [15]  and SKAIG  [17] , as it allows the analysis of relations at a coarser scale. For example, -{event-to-event} denotes the removal of the xWant and oWant relations. To evaluate the effect of different edge representations, we use \"trainable\", which replaces all edge representations with trainable embeddings, and \"0/1\", which limits the relation types to the set R = {0, 1}. and speakers' emotional states. (2) Removing knowledge-based edges results in a more significant performance decline compared to removing trainable embedding-based edges. Additionally, both \"trainable\" and \"0/1\" show poor performance without encoding external knowledge in edge representations. These findings suggest that external knowledge can enhance the information interaction between nodes. (3) Ablating the IntraESI module noticeably decreases the performance of ESIHGNN, suggesting that there is an interplay between the speaker's events and emotional states. (4) Increasing ω may not significantly affect the performance of ESIHGNN, indicating that the event-state interactions are localized.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes ESIHGNN, a novel approach that incorporates the speaker's emotional state into conversational context modeling for conversational emotion recognition, based on a heterogeneous directed acyclic graph neural network. The experimental results on four benchmark datasets demonstrate that our approach achieves competitive or state-of-the-art performance when compared with baselines. Moreover, our ablation studies confirm the effectiveness of event-state interactions and emphasize the superiority of knowledge-enriched edge representations. In future work, we plan to contribute a knowledge graph where emotion serves as the head entity, filling gaps in both state-to-state and state-to-event relations.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the ESIHGNN framework.",
      "page": 2
    },
    {
      "caption": "Figure 1: The introduced ESIHGNN framework, where we present the interactions between nodes of the 3rd turn and their predecessors and",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "based methods encode an event flow and a parallel state flow, and"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "build interactions between them to dynamically model the conversa-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "tion. For example, DialogueRNN [8] tracks the state of each speaker"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "along the event flow using different Gated Recurrent Units (GRUs)."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "However,\nit has limitations in terms of scalability and capacity to"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "efficiently capture\nthe\ninteractions between events\nand speakers’"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "states. To tackle scalability, DialogueCRN [9], CoMPM [10], and"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "DialogueINAB [11] propose a unified speaker-level emotion track-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "ing module.\nTo enhance natural\ninteractions between events and"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "states, COSMIC [12] extends DialogueRNN with external knowl-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "edge.\nHowever,\nit\nstill\nremains challenging to balance long- and"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "short-term memory within each utterance. Moreover,\nthese meth-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "ods often require bidirectional context modeling, which sacrifices"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "real-time conversational abilities while improving performance."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "On the other hand, graph-based methods construct a conver-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "sation\nstructure\ngraph\nconstrained\nby\nspeaker\nidentity\nto model"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "the\nconversation\nusing Graph Neural Networks\n(GNNs).\nDia-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "logueGCN [13],\nfor example,\nrepresents events as nodes and con-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "nections between speakers as edges to gather contextual information"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "from neighboring nodes within a\nspecific window.\nRGAT [14]"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "expands upon DialogueGCN by including position encodings\nto"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "consider sequential\ninformation. Meanwhile, DAG-ERC [15] con-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "structs a directed acyclic graph inspired by the Directed Acyclic"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "Graph Neural Network (DAGNN)\n[16]\nto sequentially model con-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "text. Similar to COSMIC, SKAIG [17] enhances event\ninteractions"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "with external knowledge, while knowledge also plays an essential"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "role in both KET [18] and KI-Net\n[19].\nIn summary, graph-based"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "methods effectively model conversations by constructing interaction"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "networks. However, these methods overlook the emotional states of"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "speakers during a conversation and focus only on event interactions."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "According to the aforementioned observations,\nit\nis important"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "to\ndevelop\nan\nevent-state\ninteraction\ngraph-based\napproach\nthat"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "can capture effective interactions between events and the emotions"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "of participants\nfor\nreal-time\nconversation modeling.\nThis paper"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "proposes\na\nsolution named ESIHGNN (Event-State\nInteractions"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "infused Heterogeneous Graph Neural Network)\nto meet\nthese re-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "quirements.\nSpecifically, we construct a heterogeneous event-state"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "graph, where the event node is initialized with the semantic feature"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "of\nthe utterance and the accompanying state node representing the"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "speaker’s\nemotional\nstate\nis\ninitialized with the\nspeaker\nidentity."
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "To support\nreal-time\nconversation modeling and enhance natural"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "interactions between the event nodes and state nodes, we define"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "eight preliminary logical edge relations based on speaker\nidentity,"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "represented by external knowledge or trainable vectors. The result-"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "ing graph is then fed into an introduced HDAGNN (Heterogeneous"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "Directed Acyclic Graph Neural Network)\nthat\nrecurrently updates"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": ""
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "each event and state node in a single layer by gathering information"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "from previous events and emotional\nstates.\nAdditionally, we use"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "GRUs\nto enhance the representations of event and state nodes at"
        },
        {
          "College of Computer Science and Electronic Engineering, Hunan University, China": "each turn,\nin order to improve conversational consistency. Our con-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "across different\nturns for conversational coherence (see Subsection"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "2.2),\nit\nis\nequally crucial\nto consider\nthe\nevent-state\ninteractions"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "within the\nsame\nturn to improve\nconversational\nconsistency.\nTo"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "achieve\nthis, we propose\nestablishing two implicit,\nbidirectional"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "paths connecting the event and its accompanying emotional state."
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "Inter-Turn Event-State Interactions: To model real-time conver-"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "sations, we use a dynamic and forward feature propagation strategy"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "that enables current nodes to only receive information from previous"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "the weight coefficients are calculated by\nnodes. For each node vi,"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "normalizing the attention scores between the hidden feature of vi at"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "the (l−1)-th layer and those of its neighborhood Ni at the l-th layer:"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "αl\n(W l[W l\n+ W l\nij = Softmax\nr,vhl−1\nr,vhl\nj + W l\nr,aaji,r + W l\nppj,r]),"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": ""
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "j∈Ni,r∈R"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "(1)"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "where W l, W l\nr,v, W l\nr,a, and W l\np are trainable parameters, and pj,r"
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "signifies the positional information of node vj using two dimensions."
        },
        {
          "using HDAGNN. In addition to establishing event-state interactions": "The first dimension of pj,r indicates the absolute position, providing"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "a novel approach that first\ntreats a conversation as a heterogeneous",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "• xReact:\nIntra-speaker event-to-state interaction: When"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "event-state interaction graph for the CER task.\n(2) We introduce an",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "is executed by the speaker,\nevent ei\ntheir subsequent state sj"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "HDAGNN, a heterogeneous directed acyclic graph neural network",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "is influenced by their reaction to this event ei."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "that dynamically models\ninteractions between events\nand speak-",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "• oReact:\nInter-speaker event-to-state interaction: The re-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "ers’ emotional states during real-time conversations.\n(3) Extensive",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "influences their subse-\naction of another speaker to an event ei"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "experiments conducted on four benchmark datasets validate the ef-",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "quent state sj."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "fectiveness and superiority of the proposed ESIHGNN and confirm",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "state-to-state\n• xDepend:\nIntra-speaker\ninteraction:\nThe"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "the advantages of the heterogeneous event-state interaction graph.",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "speaker’s\nemotional\nat\na given moment has\nan in-\nstate si"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "fluence\non\ntheir\nsubsequent\nemotional\nstate\nindicating\nsj,"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "2. METHODOLOGY",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "self-dependency."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "Our ESIHGNN comprises three primary modules: graph construc-",
          "State Node Updating:": "state-to-state\n• oDepend:\nInter-speaker\ninteraction:\nThe"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "tion, HDAGNN for feature transformation, and emotion prediction.",
          "State Node Updating:": "emotional\nstates of different\nspeakers\ninfluence and interact"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "Figure 1 illustrates the ESIHGNN framework.",
          "State Node Updating:": "with each other, indicating inter-speaker dependency."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "Edges: Given the\nset of\nrelation types, R = {xWant, oWant,"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "2.1. Task Definition",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "xDrive, oDrive, xReact, oReact, xDepend, oDepend},"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "In a conversation, a series of N sequential utterances are encoun-",
          "State Node Updating:": "information can only flow from previous event/state nodes\nto the"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "tered, denoted as [(u1, pu1 ), (u2, pu2 ), . . . , (uN , puN )]. Here, ui",
          "State Node Updating:": "current ones along the edges E, and not in the opposite direction. To"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "indicates the speaker\nrepresents the utterance of the i-th turn, and pui",
          "State Node Updating:": "study the effect of the node connection range on the performance of"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "identity for ui.\nNote that pui\nand puj may belong to the same",
          "State Node Updating:": "CER, we introduce a window parameter ω that restricts the connec-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "speaker, but\nthere must be at\nleast\ntwo speakers participating in the",
          "State Node Updating:": "tions between the target node and previous nodes of each speaker"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "conversation. The task of the CER is to predict the emotion category",
          "State Node Updating:": "within a window of size ω. With ω = 1, only the most recent event"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "its prediction relies on\nfor each utterance. For a target utterance ui,",
          "State Node Updating:": "and state nodes are considered predecessors for the target node."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "real-time sequence pairs consisting of utterances and accompanying",
          "State Node Updating:": "Edge Representations:\nExternal knowledge plays a crucial\nrole"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "speaker identities: {(u1, pu1 ), (u2, pu2 ), . . . , (ui, pui )}.",
          "State Node Updating:": "in modeling conversations with fluency, coherence, and emotional"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "contagion. To harness this potential, we employ COMET [21], an"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "2.2. Graph Construction",
          "State Node Updating:": "inferential, knowledge-based transformer model,\nto generate edge"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "representations based on the input\nformat\nfrom COSMIC [12] and"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "A new methodology for modeling conversation structure\nis pre-",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "SKAIG [17].\nFor each edge eij = (vi, r, vj), COMET takes the"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "sented, emphasizing the interactions between events and speakers’",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "concatenated predecessor vi and relation r as input, and extracts the"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "emotional\nstates.\nThis methodology constructs\na heterogeneous",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "hidden state of\nthe relation token to serve as the edge representa-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "event-state interaction graph, denoted as G = (V, E, R, A), where",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "tion aij,r. Note that COMET was trained on commonsense knowl-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "V represents\nand state\nthe node set consisting of event nodes Ve",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "edge data consisting of explicit event prompts, enabling it\nto gen-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "nodes Vs.\nIn this graph, an edge eij = (vi, r, vj) ∈ E signifies a",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "erate edge representations for predecessor event nodes. However,"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "connection from a node vi ∈ V to its neighboring node vj ∈ V under",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "COMET does not generate edge representations for predecessor state"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "relation type r ∈ R, with its representation denoted by aij,r ∈ A.",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "nodes associated with implicit header entities,\nincluding xDrive,"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "Nodes: For sequential utterances in a conversation, each utterance",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "oDrive, xDepend, and oDepend. To address this limitation, we"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "ui\nis regarded as an event node vei , while the emotional state of",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "propose using 300-dimensional\ntrainable random vectors, which al-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "speaker\nis\nui’\nrepresented as a state node vsi .\nClearly, both vei",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "low the model to learn these implicit relations during training."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "correspond to the same speaker and turn. Consistent with\nand vsi",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "COSMIC [12] and SKAIG [17], we employ the fine-tuned RoBERTa",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "model [20] to encode the utterance ui as the initial event node feature",
          "State Node Updating:": "2.3. HDAGNN Layers"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "is initialized using a one-hot vector\nhei . The state node feature hsi",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "We will now describe the methodology for\nfeature transformation"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "that indicates the speaker identity pui . The context-independence of",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "using HDAGNN. In addition to establishing event-state interactions"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "these node features is essential for modeling real-time conversations.",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "across different\nturns for conversational coherence (see Subsection"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "Relations: To simulate a real-time conversation driven by previous",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "2.2),\nit\nis\nequally crucial\nto consider\nthe\nevent-state\ninteractions"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "events and speakers’ emotional states, we suggest eight preliminary",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "within the\nsame\nturn to improve\nconversational\nconsistency.\nTo"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "event-state interaction relations to capture the historical context.",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "achieve\nthis, we propose\nestablishing two implicit,\nbidirectional"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "Event Node Updating:",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "paths connecting the event and its accompanying emotional state."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "• xWant:\nIntra-speaker event-to-event interaction: Event ei",
          "State Node Updating:": "Inter-Turn Event-State Interactions: To model real-time conver-"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "passes the speaker’s action guidance to the subsequent event ej.",
          "State Node Updating:": "sations, we use a dynamic and forward feature propagation strategy"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "• oWant:\nInter-speaker event-to-event interaction: Event ei",
          "State Node Updating:": "that enables current nodes to only receive information from previous"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "triggered\ncan coordinate and motivate the subsequent event ej",
          "State Node Updating:": "the weight coefficients are calculated by\nnodes. For each node vi,"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "by another speaker.",
          "State Node Updating:": "normalizing the attention scores between the hidden feature of vi at"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "state-to-event\n• xDrive:\nIntra-speaker\ninteraction:\nThe",
          "State Node Updating:": "the (l−1)-th layer and those of its neighborhood Ni at the l-th layer:"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "internally influences the trend of\nspeaker’s emotional state si",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "αl\n(W l[W l\n+ W l\nij = Softmax\nr,vhl−1\nr,vhl\nj + W l\nr,aaji,r + W l\nppj,r]),"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "their subsequent event ej.",
          "State Node Updating:": ""
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "",
          "State Node Updating:": "j∈Ni,r∈R"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "state-to-event\n• oDrive:\nInter-speaker\ninteraction:\nThe",
          "State Node Updating:": "(1)"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "speaker’s emotional state si, resulting from contagion and em-",
          "State Node Updating:": "where W l, W l\nr,v, W l\nr,a, and W l\np are trainable parameters, and pj,r"
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "pathy, externally influences the trend of\nthe subsequent event",
          "State Node Updating:": "signifies the positional information of node vj using two dimensions."
        },
        {
          "tributions are summarized as follows: (1) We propose an ESIHGNN,": "triggered by another speaker.\nej",
          "State Node Updating:": "The first dimension of pj,r indicates the absolute position, providing"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xReact\noReact": "xDepend\noDepend"
        },
        {
          "xReact\noReact": "intra-speaker"
        },
        {
          "xReact\noReact": "inter-speaker"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "1ˆy"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "2ˆy"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "Classifier\n3ˆy"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "4ˆy"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "5ˆy"
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": "  "
        },
        {
          "xReact\noReact": ""
        },
        {
          "xReact\noReact": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "being silly!": "    \nspeaker identity"
        },
        {
          "being silly!": "(state) features\ntime"
        },
        {
          "being silly!": "Utterances\nNode Embeddings\nEvent-State Interactions \nEmotion Representations\nLabels"
        },
        {
          "being silly!": "Fig. 1. The introduced ESIHGNN framework, where we present\nthe interactions between nodes of the 3rd turn and their predecessors and"
        },
        {
          "being silly!": "successors in a conversation example."
        },
        {
          "being silly!": "To ensure conversational coherence and consistency, the hidden\nglobal sequential\ninformation throughout a conversation, while the"
        },
        {
          "being silly!": "second dimension denotes the relative position within the relation r,\nfeature of the node vi at the l-th layer is the addition of ¯hl\ni and ˜hl\ni:"
        },
        {
          "being silly!": "giving local sequential information within that relation."
        },
        {
          "being silly!": "hl\n(7)\ni = ¯hl\ni + ˜hl\ni."
        },
        {
          "being silly!": "Once obtained, the normalized coefficients are used to compute"
        },
        {
          "being silly!": "a linear combination of the features of vi’s neighborhood Ni, pro-\n2.4. Emotion Prediction"
        },
        {
          "being silly!": "ducing aggregated information (referred to as “message”):"
        },
        {
          "being silly!": "To preserve the original semantics of each node vi, we employ a con-"
        },
        {
          "being silly!": "catenation operation ∥ to combine its hidden features from each layer\n(cid:88)"
        },
        {
          "being silly!": "M l\nαl\n(2)\ni =\nijW l\nr,vhl\nj."
        },
        {
          "being silly!": "j∈Ni\nl in HDAGNN, resulting in the final node representation Hvi . Addi-"
        },
        {
          "being silly!": "tionally, considering node consistency within a conversation turn i,"
        },
        {
          "being silly!": "In HDAGNN,\ntwo GRUs are used to address the node hetero-\nwe take the sum of the event representation Hei and the state repre-"
        },
        {
          "being silly!": "geneity and capture the sequential dependencies among nodes in an\nsentation Hsi as the emotion representation Hi of the utterance ui,"
        },
        {
          "being silly!": "event-state interaction graph. The GRU takes the past feature hl−1\nand input it into a fully connected layer to predict the emotion label:\ni"
        },
        {
          "being silly!": "of the node vi as its input and the message M l\ni as its hidden state."
        },
        {
          "being silly!": "(8)\nHi = ∥L\nl=0(hl\nsi ),\nei + hl"
        },
        {
          "being silly!": "hl\n, M l"
        },
        {
          "being silly!": "(3)\nei = GRUl\nei ),"
        },
        {
          "being silly!": "(9)\nyi = Softmax(WzHi + bz)."
        },
        {
          "being silly!": "hl\n, M l"
        },
        {
          "being silly!": "(4)\nsi = GRUl\nsi )."
        },
        {
          "being silly!": "3. EXPERIMENTS"
        },
        {
          "being silly!": "Intra-Turn Event-State Interactions: Within a turn,\nthere are two"
        },
        {
          "being silly!": "3.1.\nSetup\nevent-state interaction paths. The first path involves the speaker be-"
        },
        {
          "being silly!": "ing influenced by the emotional context when executing events. For"
        },
        {
          "being silly!": "Datesets: Our ESIHGNN is evaluated on four widely-used bench-"
        },
        {
          "being silly!": "the emotional context of the i-th turn, we specify M l\ninstead of the"
        },
        {
          "being silly!": "si\nmark datasets:\nIEMOCAP [22], MELD [23], EmoryNLP [24], and"
        },
        {
          "being silly!": "updated ¯hl\nthat may have forgotten some emotional information.\nsi\nDailyDialog [25]. The dataset statistics are presented in Table 1. To"
        },
        {
          "being silly!": "assess performance, we apply the evaluation metrics utilized in pre-"
        },
        {
          "being silly!": "hl"
        },
        {
          "being silly!": "),\n(5)\nei = GRUl\nsi , hl−1\nvious studies [14,26]. This entails calculating micro-averaged F1 for"
        },
        {
          "being silly!": "DailyDialog and weighted-average F1 for the other datasets."
        },
        {
          "being silly!": "where M l\n, and ˜hl\ne are the input, hidden state, and output of\ns, hl−1"
        },
        {
          "being silly!": "Baselines: We\ncompare our proposed ESIHGNN with state-of-"
        },
        {
          "being silly!": "the GRU, respectively."
        },
        {
          "being silly!": "the-art baseline methods,\nincluding recurrence-based methods: Di-"
        },
        {
          "being silly!": "alogueRNN [8], COSMIC [12], DialogueCRN [9], BiERU [27],\nThe second path involves the emotional state of the speaker at"
        },
        {
          "being silly!": "MVN [28], CoMPM [10], and DialogueINAB [11]; and graph-based\nthe i-th turn being influenced by the context of the event, M l"
        },
        {
          "being silly!": "ei :"
        },
        {
          "being silly!": "methods: DialogueGCN [13], KET [18], KI-Net [19], RGAT [14],"
        },
        {
          "being silly!": "hl"
        },
        {
          "being silly!": ").\nSKAIG [17], DAG-ERC [15], CoG-BART [29], and MM-DFN [30].\n(6)\nsi = GRUl\nei , hl−1"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "erage number of utterances and speakers per dialogue, respectively.",
          "Table 2. Results of our method and state-of-the-art baselines.": "and “b” denote the incorporation of external information and the dis-",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "closure of future utterance information, respectively.",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "Datasets",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "IEMOCAP",
          "“a”": "DailyDialog"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "Recurrence-based methods",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "IEMOCAP",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "64.76",
          "“a”": "57.32"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "MELD",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "65.28",
          "“a”": "58.48"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "EmoryNLP",
          "Table 2. Results of our method and state-of-the-art baselines.": "66.20",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "DailyDialog",
          "Table 2. Results of our method and state-of-the-art baselines.": "65.22",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "65.44",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "65.79",
          "“a”": "59.63"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "Implementation Details: We use the PyTorch framework and the",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.22",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "AdamW optimizer on two RTX 3090 GPUs for code implementa-",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "Graph-based methods",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "tion. Our approach involves performing hyperparameter searches for",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "64.18",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "the learning rate, dropout rate, batch size, and number of HDAGNN",
          "Table 2. Results of our method and state-of-the-art baselines.": "59.56",
          "“a”": "53.37"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "layers. We set ω = 1 as the default setting for overall performance",
          "Table 2. Results of our method and state-of-the-art baselines.": "66.98",
          "“a”": "57.30"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "comparisons, but in Subsection 3.3 we present ablation results for ω",
          "Table 2. Results of our method and state-of-the-art baselines.": "65.22",
          "“a”": "54.31"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "66.96",
          "“a”": "59.75"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "ranging from 1 to 3. The initial dimensions for the node features and",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.03",
          "“a”": "59.33"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "edge features are set",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "66.18",
          "“a”": "56.29"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "sions are consistent across all hidden layers, remaining at 300. The",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.18",
          "“a”": "-"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "reported results of our approach are the average of five test runs.",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.53",
          "“a”": "59.78"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "3.2. Results and Analysis",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "The results of the four datasets are summarized in Table 2. It can be",
          "Table 2. Results of our method and state-of-the-art baselines.": "Table 3. Results of ablation studies on the four datasets.",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "observed that:",
          "Table 2. Results of our method and state-of-the-art baselines.": "IEMOCAP",
          "“a”": "DailyDialog"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "art performance, except",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.53",
          "“a”": "59.78"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "probably attributed to the complex and ambiguous contexts present",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.22",
          "“a”": "59.56"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "in short conversations with multiple speakers in MELD, which limit",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.17",
          "“a”": "59.69"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "ESIHGNN’s",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.70",
          "“a”": "59.52"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "tions.",
          "Table 2. Results of our method and state-of-the-art baselines.": "68.03",
          "“a”": "59.68"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "DialogueRNN when external knowledge is incorporated, while our",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.56",
          "“a”": "59.22"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "method surpasses DAG-ERC among graph-based methods.",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.60",
          "“a”": "59.47"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "findings highlight",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.92",
          "“a”": "59.48"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "edge.",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.68",
          "“a”": "59.50"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "not consider future utterances on all datasets, confirming the supe-",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "",
          "Table 2. Results of our method and state-of-the-art baselines.": "67.29",
          "“a”": "59.03"
        },
        {
          "Table 1. Statistics of the datasets. “A. L.” and “A. S.” denote the av-": "riority of the event-state connection structure and the effectiveness",
          "Table 2. Results of our method and state-of-the-art baselines.": "",
          "“a”": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "“Past, present, and future: Conversational emotion recognition"
        },
        {
          "5. REFERENCES": "[1]\nJoseph Paul Forgas, “Affect in social judgments and decisions:",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "through structural modeling of psychological knowledge,”\nin"
        },
        {
          "5. REFERENCES": "A multiprocess model,”\nin Advances in experimental social",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Proc. EMNLP, 2021, pp. 1204–1214."
        },
        {
          "5. REFERENCES": "psychology, vol. 25, pp. 227–275. 1992.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[18]\nPeixiang Zhong, Di Wang, and Chunyan Miao,\n“Knowledge-"
        },
        {
          "5. REFERENCES": "[2] Adam M Grant and Susan J Ashford, “The dynamics of proac-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "enriched transformer for emotion detection in textual conver-"
        },
        {
          "5. REFERENCES": "tivity at work,” Research in organizational behavior, vol. 28,",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "sations,” in Proc. EMNLP, 2019, pp. 165–176."
        },
        {
          "5. REFERENCES": "pp. 3–34, 2008.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[19] Yunhe Xie, Kailai Yang, Chengjie Sun, Bingquan Liu,\nand"
        },
        {
          "5. REFERENCES": "[3] Gene Ball and Jack Breese,\n“Emotion and personality in a",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Zhenzhou Ji,\n“Knowledge-interactive network with sentiment"
        },
        {
          "5. REFERENCES": "conversational agent,”\nEmbodied conversational agents, vol.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "polarity intensity-aware multi-task learning for emotion recog-"
        },
        {
          "5. REFERENCES": "189, pp. 189–219, 2000.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "nition in conversations,”\nin Proc. EMNLP, 2021, pp. 2879–"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "2889."
        },
        {
          "5. REFERENCES": "[4] Yu-Ping Ruan, Shu-Kai Zhen4g, Taihao Li, Fen Wang, and",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "Guanxiong Pei,\n“Hierarchical\nand multi-view dependency",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[20] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar"
        },
        {
          "5. REFERENCES": "modelling network for conversational emotion recognition,” in",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-"
        },
        {
          "5. REFERENCES": "Proc. ICASSP, 2022, pp. 7032–7036.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "moyer, and Veselin Stoyanov, “Roberta: A robustly optimized"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "bert pretraining approach,”\narXiv preprint arXiv:1907.11692,"
        },
        {
          "5. REFERENCES": "[5]\nPaul MG Emmelkamp and Katharina Meyerbr¨oker,\n“Virtual",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "Annual\nreview of clinical",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "2019."
        },
        {
          "5. REFERENCES": "reality therapy in mental health,”",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "psychology, vol. 17, pp. 495–519, 2021.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[21] Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Malaviya, Asli Celikyilmaz, and Yejin Choi,\n“COMET: com-"
        },
        {
          "5. REFERENCES": "[6] Alessandra Maria\nSabelli,\nTakayuki Kanda,\nand Norihiro",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "monsense transformers\nfor automatic knowledge graph con-"
        },
        {
          "5. REFERENCES": "Hagita,\n“A conversational robot\nin an elderly care center: An",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "struction,” in Proc. ACL, 2019, pp. 4762–4779."
        },
        {
          "5. REFERENCES": "ethnographic study,” in Proc. HRI, 2011, pp. 37–44.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "[7] Tom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou, Subham",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[22] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe)"
        },
        {
          "5. REFERENCES": "Biswas, and Minlie Huang, “Augmenting end-to-end dialogue",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Kazemzadeh,\nEmily Mower\nProvost,\nSamuel Kim,\nJean-"
        },
        {
          "5. REFERENCES": "systems with commonsense knowledge,” in Proc. AAAI, 2018,",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "nette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan,"
        },
        {
          "5. REFERENCES": "pp. 4970–4977.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "“Iemocap:\ninteractive\nemotional\ndyadic motion\ncapture"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "database,”\nLanguage Resources and Evaluation, vol. 42, pp."
        },
        {
          "5. REFERENCES": "[8] Navonil Majumder,\nSoujanya Poria, Devamanyu Hazarika,",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "335–359, 2008."
        },
        {
          "5. REFERENCES": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria, “Dia-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "loguernn: An attentive rnn for emotion detection in conversa-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[23]\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder,"
        },
        {
          "5. REFERENCES": "tions,” in Proc. AAAI, 2019, pp. 6818–6825.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Gautam Naik, Erik Cambria, and Rada Mihalcea,\n“MELD:"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "A multimodal multi-party dataset\nfor emotion recognition in"
        },
        {
          "5. REFERENCES": "[9] Dou Hu, Lingwei Wei, and Xiaoyong Huai,\n“Dialoguecrn:",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "conversations,” in Proc. ACL, 2019, pp. 527–536."
        },
        {
          "5. REFERENCES": "Contextual reasoning networks for emotion recognition in con-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "versations,” in Proc. ACL, 2021, pp. 7042–7052.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[24]\nSayyed M. Zahiri and Jinho D. Choi,\n“Emotion detection on"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "TV show transcripts with sequence-based convolutional neural"
        },
        {
          "5. REFERENCES": "[10]\nJoosung Lee and Wooin Lee, “Compm: Context modeling with",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "networks,” in Proc. AAAI, 2018, pp. 44–52."
        },
        {
          "5. REFERENCES": "speaker’s pre-trained memory tracking for emotion recognition",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "in conversation,” in Proc. NAACL, 2022, pp. 5669–5679.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[25] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and"
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Shuzi Niu,\n“Dailydialog: A manually labelled multi-turn dia-"
        },
        {
          "5. REFERENCES": "[11]\nJunyuan Ding, Xiaoliang Chen, Peng Lu, Zaiyan Yang, Xiany-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "logue dataset,” in Proc. IJCNLP, 2017, pp. 986–995."
        },
        {
          "5. REFERENCES": "ong Li, and Yajun Du,\n“Dialogueinab:\nan interaction neural",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "network based on attitudes and behaviors of\ninterlocutors for",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[26] Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhixian Xie,"
        },
        {
          "5. REFERENCES": "The Journal of Supercomput-\ndialogue emotion recognition,”",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "“Dialogxl: All-in-one xlnet for multi-party conversation emo-"
        },
        {
          "5. REFERENCES": "ing, pp. 1–34, 2023.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "tion recognition,” in Proc. AAAI, 2021, pp. 13789–13797."
        },
        {
          "5. REFERENCES": "[12] Deepanway Ghosal, Navonil Majumder, Alexander F. Gel-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[27] Wei Li, Wei Shao, Shaoxiong Ji, and Erik Cambria,\n“Bieru:"
        },
        {
          "5. REFERENCES": "bukh, Rada Mihalcea, and Soujanya Poria,\n“COSMIC: com-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Bidirectional emotional recurrent unit for conversational senti-"
        },
        {
          "5. REFERENCES": "monsense knowledge for emotion identification in conversa-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "ment analysis,” Neurocomputing, vol. 467, pp. 73–82, 2022."
        },
        {
          "5. REFERENCES": "tions,” in Proc. EMNLP, 2020, pp. 2470–2481.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[28] Hui Ma,\nJian Wang, Hongfei Lin, Xuejun Pan, Yijia Zhang,"
        },
        {
          "5. REFERENCES": "[13] Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niy-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "and Zhihao Yang, “A multi-view network for real-time emotion"
        },
        {
          "5. REFERENCES": "ati Chhaya, and Alexander F. Gelbukh, “Dialoguegcn: A graph",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "recognition in conversations,” Knowledge-Based Systems, vol."
        },
        {
          "5. REFERENCES": "convolutional neural network for emotion recognition in con-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "236, pp. 107751, 2022."
        },
        {
          "5. REFERENCES": "versation,” in Proc. EMNLP, 2019, pp. 154–164.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[29]\nShimin Li, Hang Yan, and Xipeng Qiu, “Contrast and genera-"
        },
        {
          "5. REFERENCES": "[14] Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun Goto,",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "tion make bart a good dialogue emotion recognizer,”\nin Proc."
        },
        {
          "5. REFERENCES": "“Relation-aware graph attention networks with relational posi-",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "AAAI, 2022, pp. 11002–11010."
        },
        {
          "5. REFERENCES": "tion encodings for emotion recognition in conversations,”\nin",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "[30] Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, and Yang"
        },
        {
          "5. REFERENCES": "Proc. EMNLP, 2020, pp. 7360–7370.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "Mo, “Mm-dfn: Multimodal dynamic fusion network for emo-"
        },
        {
          "5. REFERENCES": "[15] Weizhou Shen, Siyue Wu, Yunyi Yang,\nand Xiaojun Quan,",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "tion recognition in conversations,” in Proc. ICASSP, 2022, pp."
        },
        {
          "5. REFERENCES": "“Directed acyclic graph network for conversational emotion",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": "7037–7041."
        },
        {
          "5. REFERENCES": "recognition,” in Proc. ACL, 2021, pp. 1551–1560.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "[16] Veronika Thost and Jie Chen,\n“Directed acyclic graph neural",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        },
        {
          "5. REFERENCES": "networks,” in ICLR, 2021.",
          "[17]\nJiangnan\nLi,\nZheng\nLin,\nPeng\nFu,\nand Weiping Wang,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Affect in social judgments and decisions: A multiprocess model",
      "authors": [
        "Joseph Paul"
      ],
      "year": "1992",
      "venue": "Advances in experimental social psychology"
    },
    {
      "citation_id": "3",
      "title": "The dynamics of proactivity at work",
      "authors": [
        "M Adam",
        "Susan Grant",
        "Ashford"
      ],
      "year": "2008",
      "venue": "Research in organizational behavior"
    },
    {
      "citation_id": "4",
      "title": "Emotion and personality in a conversational agent",
      "authors": [
        "Gene Ball",
        "Jack Breese"
      ],
      "year": "2000",
      "venue": "Emotion and personality in a conversational agent"
    },
    {
      "citation_id": "5",
      "title": "Hierarchical and multi-view dependency modelling network for conversational emotion recognition",
      "authors": [
        "Yu-Ping Ruan",
        "Shu-Kai Zhen4g",
        "Taihao Li",
        "Fen Wang",
        "Guanxiong Pei"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Virtual reality therapy in mental health",
      "authors": [
        "M Paul",
        "Katharina Emmelkamp",
        "Meyerbröker"
      ],
      "year": "2021",
      "venue": "Annual review of clinical psychology"
    },
    {
      "citation_id": "7",
      "title": "A conversational robot in an elderly care center: An ethnographic study",
      "authors": [
        "Alessandra Maria Sabelli",
        "Takayuki Kanda",
        "Norihiro Hagita"
      ],
      "year": "2011",
      "venue": "Proc. HRI"
    },
    {
      "citation_id": "8",
      "title": "Augmenting end-to-end dialogue systems with commonsense knowledge",
      "authors": [
        "Tom Young",
        "Erik Cambria",
        "Iti Chaturvedi",
        "Hao Zhou",
        "Subham Biswas",
        "Minlie Huang"
      ],
      "year": "2018",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "9",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "10",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "venue": "Proc. ACL, 2021"
    },
    {
      "citation_id": "11",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proc. NAACL"
    },
    {
      "citation_id": "12",
      "title": "Dialogueinab: an interaction neural network based on attitudes and behaviors of interlocutors for dialogue emotion recognition",
      "authors": [
        "Junyuan Ding",
        "Xiaoliang Chen",
        "Peng Lu",
        "Zaiyan Yang",
        "Xianyong Li",
        "Yajun Du"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "13",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "F Alexander"
      ],
      "year": "2020",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "14",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "15",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "16",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proc. ACL, 2021"
    },
    {
      "citation_id": "17",
      "title": "Directed acyclic graph neural networks",
      "authors": [
        "Veronika Thost",
        "Jie Chen"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "18",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "venue": "Proc. EMNLP, 2021"
    },
    {
      "citation_id": "19",
      "title": "Knowledgeenriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "20",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
      "authors": [
        "Yunhe Xie",
        "Kailai Yang",
        "Chengjie Sun",
        "Bingquan Liu",
        "Zhenzhou Ji"
      ],
      "year": "2021",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "21",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "22",
      "title": "COMET: commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "23",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "25",
      "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "26",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proc. IJCNLP"
    },
    {
      "citation_id": "27",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "venue": "Proc. AAAI, 2021"
    },
    {
      "citation_id": "28",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Xuejun Pan",
        "Yijia Zhang",
        "Zhihao Yang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "30",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "31",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    }
  ]
}