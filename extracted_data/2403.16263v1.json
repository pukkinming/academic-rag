{
  "paper_id": "2403.16263v1",
  "title": "Emotion Recognition From The Perspective Of Activity Recognition",
  "published": "2024-03-24T18:53:57Z",
  "authors": [
    "Savinay Nagendra",
    "Prapti Panigrahi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition from using visual signals is one of the burgeoning research areas in the domain of Computer Vision. Applications of an efficient emotion recognition system can be found in several domains such as medicine, driver fatigue surveillance, social robotics and human-computer interaction. Appraising human emotional states, behaviors and reactions displayed in real world settings can be accomplished using latent continuous dimensions. Continuous dimensional models of human affect, such as those based on valence and arousal have been shown to be more accurate in describing a broad range of spontaneous everyday emotions than more traditional models of discrete stereotypical emotion categories (e.g. happiness, surprise). Most of the prior work on estimating valence and arousal consider laboratory settings and acted data. But, for emotion recognition systems to be deployed and integrated into real-world mobile and computing devices, we need to consider data collected in the wold. Action recognition is a domain of Computer Vision that involves capturing complementary information on appearance from still frames and motion between frames. In this paper, we treat emotion recognition from the perspective of action recognition by exploring the application of deep learning architectures specifically designed for action recognition, for continuous affect recognition. We propose a novel three stream end-to-end deep learning regression pipeline with attention mechanism, which is an ensemble design based on sub-modules of multiple state-of-the-art action recognition systems. The pipeline constitutes a novel data preprocessing approach with spatial self-attention mechanism to extract key frames. Optical flow of high attention regions of the face are extracted to capture temporal context. AFEW-VA in-the-wild dataset has been used to conduct comparative experiments. Quantitative analysis shows that the proposed model outperforms multiple standard baselines of both emotion recognition and action recognition models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The field of Automatic Emotion Recognition analysis has grown rapidly in recent years, with applications spread across a variety of fields, such as medicine  [59] ,  [60] , health monitoring  [27] , entertainment, lie-detection  [75] ,  [28] . Current research in automatic analysis of facial affect aims at developing systems, such as robots and virtual humans, that will interact with humans in a naturalistic way under real-world settings. Such systems should automatically sense and interpret visual signals relevant to emotions, appraisals and intentions. There are two major emotion computing models according to theories in psychology research  [34] : discrete and dimensional theory. When designing emotion recognition systems for real-world settings, where subjects operate in a diversity of contexts and environments, systems that perform automatic analysis of human behavior should be robust to the diversity of contexts, the timing of display and video recording conditions.\n\nThere are two major emotion computing models according to theories in psychology research  [34] : discrete and dimensional theory. Research in the past  [13, 31, [39] [40] [41] [42] [43] [44] [45] 50, 76]  has revolved around the recognition of the so-called six universal expressions -Happiness, Sadness, Fear, Disgust, Surprise and Anger.  [11] ,  [25] ,  [29] ,  [10] ,  [26] . These are intuitive and simple, but cannot express complex affective states. For many years, research in automatic analysis of facial behavior was mainly limited to posed behavior which was captured in highly controlled recording conditions  [32, 59, 61, 64] . Some representative datasets, which are still used in many recent works  [19] , are Cohn-Kanade database  [32, 61] , MMI database  [59, 64] , Multi-PIE database  [28]  and the BU-3D and BU-4D databases  [70, 74] .\n\nHowever, the fact that the facial expressions of naturalistic behaviors can be radically different from the posed ones has been widely accepted in the research commu- nity  [9, 54, 71] . Hence, efforts have been made in order to collect subjects displaying naturalistic behavior. Examples include the recently collected EmoPain  [3]  and UNBC-McMaster  [33]  databases for analysis of pain, the RU-FACS database of subjects participating in a false opinion scenario  [4]  and the SEMAINE corpus  [35]  which contains recordings of subjects interacting with a Sensitive Artificial Listener (SAL) in controlled conditions. All the above databases have been captured in well-controlled recording conditions and mainly under a strictly defined scenario eliciting pain.\n\nValence defines how negative or positive the experience is, and intensity of arousal defines how calming or exciting the experience is. The circumplex model of emotion shown in Figure  1   The motivation to explore this problem comes from the fact that the field of emotion recognition is nascent and not as advanced as the field of action recognition in the domain of Computer Vision. With this research we seek the answers to the following questions: (1) With the above mentioned similarities between the two tasks, is it feasible to explore emotion recognition from the perspective of Action Recognition? (2) Can the complex deep learning mechanisms of Action recognition be adopted for the task of emotion recognition? (3) Can we come up with a general model which can work for both tasks without invoking specifics of a particular task?\n\nIn this work, we aim to explore the problem of emotion recognition from the perspective of action recognition. We use a continuous affect recognition dataset collected in-thewild, called AFEW-VA which consists of highly accurate per-frame annotations of valence and arousal for 600 challenging, real-world video clips extracted from feature films Added to these are per-frame annotations of 68 facial landmarks. The dataset has been made publicly available.\n\nWe conduct a quantitative study of multiple standard and state-of-the-art emotion and action recognition algorithms applied to our dataset. We propose a novel three stream ensemble deep learning pipeline shown in Figure  2 . The pipeline is designed as a regression system to detect continuous affect.Illumination equalization is performed across all video clips using Contrast Limited Adaptive Histogram Equalization (CLAHE). This helps a uniform illumination over all subjects, hence increasing the performance of the system. Action recognition research  [68]  shows that not all video frames are necessary for video classification. Hence, a spatial self-attention mechanism is applied to compute the key frames which capture the same spatio-temporal information as that of the original video frames. The framewise Gaussian heatmaps provided by the attention network is used to identify the most discriminating regions of the face for efficient emotion recognition. Optical flow from eye and mouth regions, which get the highest score from attention maps, are extracted to capture temporal context in the dataset. Three streams of input, thus consists of RGB video frames, optical flows from eye and mouth. A Convo- The contributions of our work are as follows:\n\n1. A novel end-to-end three stream ensemble deep learning regression pipeline for continuous affect recognition.\n\n2. Key frame extraction and identification of the most discriminative features of the face using spatial selfattention.\n\n3. Using optical flow frames of eye and mouth regions to capture temporal context.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Incorporation Of Temporal",
      "text": "Gaussian Attention filters into the three stream pipeline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "5.",
      "text": "A quantitative comparative study of multiple standard and state-of-the-art action and emotion recognition algorithms applied to our chosen dataset .\n\nThe rest of the paper is organized as follows: Section 2 mentions a brief literature survey regarding the evolution of emotion and action recognition algorithms and the work that has been implemented for this paper. Section 3 provides information about the chosen dataset. Section 4 provides information about our pipeline. Section 5 provides information about the baseline algorithms that have been compared with our method and quantitative results of the comparative study. Section 6 provides a summary and discussion of future work. Finally, Section 7 provides acknowledgments for our work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "There has been significant work on audio-based valence and arousal estimation  [14] . Our paper focuses on videobased valence and arousal estimation. Both valence and arousal are defined as continuous emotional dimensions. Therefore, it seems suitable to study them directly in the continuous domain. Even though much of the early work considered coarse levels of valence and arousal (e.g. positive vs negative), and posed the problem as one of classification  [9] , more recent work casts the problem in the continuous domain  [14, 38, 55] .\n\nWith the onset of AVEC challenges in 2011, there has been tremendous progress in the field. Pioneering work was done with a subset of the Semaine dataset  [56] , which originally formulated the problem as one of classification, using binarized values (±1), before moving to continuous annotations in 2012. The best results obtained that year were an average Pearson Correlation Coefficient (PCC) of 0.456  [47] . The 2013 and 2014 editions of the challenge used the audio-visual depression language corpus  [63, 65] . The best results obtained on that corpus were lower than in the first year with a PCC of 0.1409 in 2013  [36]  but improved to 0.5946 for the best performer in 2014  [20] . RECOLA dataset was used from 2015 in AVEC challenges. The results were good even though RECOLA  [52]  is a challenging dataset. For instance, the best performer in the 2015 edition of the challenge obtained an average PCC of 0.685  [16] , while the best performer in 2016 obtained an average PCC of 0.731  [6] .\n\nThe most obvious approaches, which can also be treated as non-deep learning baselines to valence and arousal estimation are static regression, such as linear regression  [15, 62] , partial least squares  [36] , and Support Vector Machine for Regression (SVR)  [30, 46, 53] . In  [53] , SVR is combined with Canonical Correlation Analysis (CCA) to iteratively fuse predictions. In  [30] , SVR is used as a baseline to test the performance of multiple facial appearance and geometric features. SVR is also employed in the work of  [20]  but its use differs in that template trajectories for each emotional dimension are first built and then matched to a new testing sample using metadata as features. Metadata is also used in  [58]  where audio, video and contextual (meta-data) features are combined in a multimodal fuzzy inference system. More powerful kernel based methods can be used such as Nadaraya-Watson kernel regression  [47]  or Doubly Sparse Relevance Vector Machine  [22]  that can impose sparsity on both the kernels and the training samples.\n\nIn  [30] , a comparative study has been done on multiple appearance and geometric features. In appearance features, the paper experiments with Hybrid-SIFT  [1] , Block LBP  [73] , Hybrid LBP, Holistic-DCT  [37] , Block-DCT and Hybrid-DCT. Normalized features  [24]  are explored as geometric features. When dealing with several features, e.g. geometric and appearance features, or multiple modalities, e.g. audio and video, the question of how to fuse them arises. There are four main types of fusions: feature level (early fusion), decision level (late fusion), model-level and output-associative fusion  [48, 72] . In the early-fusion case, the features are combined most commonly by simply concatenating them and using the output for estimation(e.g.as is done in  [8]  for predicting valence). However, in general, this approach tends to create very high dimensional feature vectors and lead to overfitting. Late fusion is the process of first generating separate estimations from each modality before combining -fusing -them into one final estimation. This fusion can be achieved in numerous ways, from simple mapping, e.g. averaging, as in  [21] , to more complex methods such as linear and multi-linear regression  [15, 25, 26, 28, 30, 37, 38] , SVR  [30] , random forests  [7]  or Kalman filters based  [6] . This type of late fusion is akin to stacking, a classical ensemble learning technique.\n\nDeep learning approaches to solve the problem of continuous affect use blocks of convolutional neural networks as feature extractors with SVM/SVR as classifiers, or have an end-to-end strategy of training. A combination of convolutional and recurrent neural networks is also used to capture spatio-temporal context. The most commonly used metric is CCC (Concordance Correlation Coefficient) which is a Of these algorithms that use shallow hand-crafted features in (1), improved Dense Trajectories  [67]  (iDT) which uses densely sampled trajectory features was the state-ofthe-art. Simultaneously, 3D convolutions were used as is for action recognition without much help in 2013  [18] . Soon after this in 2014, two breakthrough research papers were released which form the backbone for all the papers we are going to discuss in this post. The major differences between them was the design choice around combining spatiotemporal information.\n\nThis  [23]  is on of the foremost attempts of video classification using deep learning. The paper explores multiple ways to fuse temporal information from consecutive frames using 2D pre-trained convolutions. the consecutive frames of the video are presented as input in all setups. Single frame uses single architecture that fuses information from all frames at the last stage. Late fusion uses two nets with shared parameters, spaced 15 frames apart, and also com-bines predictions at the end. Early fusion combines in the first layer by convolving over 10 frames. Slow fusion involves fusing at multiple stages, a balance between early and late fusion. For final predictions, multiple clips were sampled from entire video and prediction scores from them were averaged for final prediction. The slow fusion model, which performed the best according to the authors of this paper, is used as the first action recognition baseline in our work. In this work  [57] , the authors build on the failures of the previous work by Karpathy et al. Given the toughness of deep architectures to learn motion features, authors explicitly modeled motion features in the form of stacked optical flow vectors. So instead of single network for spatial context, this architecture has two separate networksone for spatial context (pre-trained), one for motion context. The input to the spatial net is a single frame of the video. Authors experimented with the input to the temporal net and found bi-directional optical flow stacked across 10 successive frames was performing best. The two streams were trained separately and combined using SVM. The final prediction was the same as the previous paper, i.e. averaging across sampled frames. This work is used as the second action recognition baseline in our paper. In this work  [12] , authors use the base two-stream architecture with two novel approaches and demonstrate performance increment without any significant increase in the size of parameters. The authors explore the efficacy of two major ideas: (1) Fusion of spatial and temporal streams (how and when) -For a task discriminating between brushing hair and brushing teeth -a spatial net can capture the spatial dependency in a video (if it's hair or teeth) while temporal net can capture the presence of periodic motion for each spatial location in the video. Hence it's important to map spatial feature maps about say a particular facial region to a temporal feature map for the corresponding region. To achieve the same, the nets need to be fused at an early level such that responses at the same pixel position are put in correspondence rather than fusing at the end (like in base two-stream architecture).\n\n(2) Combining temporal net output across time frames so that long-term dependency is also modeled. Our architecture is designed using this framework. Finally,  [69]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Afew-Va: Emotion Recognition Dataset Inthe-Wild",
      "text": "This section provides information about the dataset that has been used for all the experiments in this paper.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data",
      "text": "AFEW-VA consists of 600 videos extracted from features films. The videos are processed at 30fps and range from short (around 10 frames) to longer clips (more than 120 frames), and display various facial expressions. They are captured under challenging indoor and outdoor conditions such as complex cluttered backgrounds, poor illumination, large out-of-plane head rotations, variations in scale, and occlusions. In total, there are 30000 frames with perframe levels of valence and arousal intensities in the range of -10 to 10. Figure  3  shows the distribution of the values of valence and arousal present in the dataset. It matches the expected distribution and, as can be seen, there is a wide range of values for both valence and arousal. In some videos, we observe a significant signal change in valence and arousal across the frames. In some other videos, the temporal change of valence and arousal is negligible.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Annotations",
      "text": "Per-frame annotations of valence and arousal levels are provided for all frames within all video clips of our dataset. Annotations are created by two expert annotators, a male and a female, FACS AU coding certified. Both annotators have annotated all videos together, therefore discussing all disagreements and coming up with a unique solution. Hence, AFEW-VA annotations are detailed and highly accurate. The range of annotation levels for both valence and arousal is from -10 to 10, resulting in a total of 21 levels. Figure  4  shows a screenshot of the annotation tool used by the experts. Figure  5  shows the temporal evolution of the valence and arousal signals for an example video from our dataset, along with some representative frames. The dataset also provides accurate location for 68 landmarks, as shown in Figure  6 , including both interior and boundary points of the face. The facial annotations were generated in a semiautomatic way. First, the face was detected in the first frame of each video sequence using the tree-based deformable part model (DPM).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Our Approach",
      "text": "We propose a novel three stream ensemble deep learning pipeline for emotion recognition using concepts of action recognition mechanisms. In this section, the proposed pipeline shown in Figure  2  will be discussed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Key Frame Sub-Sampling",
      "text": "Each video clip has variable number of video frames, which makes it difficult to design a deep learning architecture. As described in Section 2,  [69]  tells us that we do not need all the frames of every video clip to get a high performing architecture. Instead of randomly sub-sampling video frames, we incorporate a spatial self-attention mechanism to sample equal number of key discriminative frames from each video clip. Every video clip, as shown in Figure  6  shows varying illumination and distance from camera perspective of subjects in every video clip. This poses a problem to capture spatio-temporal context for facial expression recognition efficiently. The reasons for not exploring body language are: (1) There was no publicly available in-thewild continuous affect dataset to perform experiments. (2) There were not enough deep learning baselines for a comparative study. (3) Emotions are highly correlated with the face.\n\nUsing the 68 facial landmarks that were provided, faces are extracted from every video clip by drawing a bounding box covering all the landmarks. Illumination of every video frame is equalized by using Contrast Limited Adaptive Histogram Equalization (CLAHE)  [77] . Adaptive histogram equalization (AHE) is a computer image processing technique used to improve contrast in images. It differs from ordinary histogram equalization in the respect that the adaptive method computes several histograms, each corresponding to a distinct section of the image, and uses them to redistribute the lightness values of the image. It is therefore suitable for improving the local contrast and enhancing the definitions of edges in each region of an image. However, AHE has a tendency to over-amplify noise in relatively homogeneous regions of an image. A variant of adaptive histogram equalization called contrast limited adaptive histogram equalization (CLAHE) prevents this by limiting the amplification.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Local Feature Extraction",
      "text": "The idea of key frame sub-sampling is taken from the paper  [2] . We have frame-by-frame annotation of our dataset. This enabled us to apply this approach to capture the most important frames for regression. Consider a video sample S i and its associated emotion y i ∈ R E , where we represent the video as a sequence of F frames [X 0,i , ...X F,i ], each of size W × H × 3. VGG-16, pre-trained with VGG-Face model  [49]  was used for extracting an independent description of a face on each frame in the video. For a given frame X of a video, we consider the feature map produced by the last convolutional layer of the network as representation. This feature map has spatial resolution of L = H/16 × W/16 and D channels. channels.We discard the spatial resolution and reshape the feature map as a matrix R composed of L D-dimensional local descriptors (row vectors).\n\nThese descriptors will be associated to a corresponding weight and used for the attention mechanism.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Spatial Attention",
      "text": "For the spatial attention we rely on the self-attention mechanism  [66] , which aggregates a set of local frame descriptors R into a single weighted sum v that summarizes the most important regions of a given video frame:\n\nwhere a is a row vector of dimension L, which defines the importance of each frame region. The weights a are generated by two layers fully connected network that associates each local feature (row of R) to a corresponding weight. This vector representation usually focuses on a specific region in the facial feature, like the mouth. However, it is possible that multiple regions of the face contain different type of information that can be combined to obtain a better idea of the person emotional state. Hence, we need multiple attention units that focus on different parts of the image.\n\nFor doing that, we transform w s2 into a matrix W s2 of size R × L, in which every row represents a different attention.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Temporal Pooling",
      "text": "After extracting the local features and aggregating them using the attention mechanism for each frame, we have to take into account frame features over the whole video. As the length of a video can be different for each example, we need an approach that supports different input lengths. The most commonly used approaches are average and max pooling; however, these techniques assume that every frame of the video has the same importance in the final decision (average pooling) or that only a single frame is considered as a general representation of the video (max pooling). In order to use the best of both techniques, we use an aggregation based on softmax, which can be considered a generalization of the average and max pooling. Given a video sample S, after feature extraction and spatial attention, we obtain a matrix V in which each row represents the features of a frame. The 21 labels of valence and arousal are discretized into 7 classes based on the 2D continuous emotion wheel for this process. These features are converted into class scores through a final fully connected layer O = W sm V. Here, O is a matrix in which an element o c,f is the score for class c of the frame f . We then transform the scores over frames and classes in probabilities with a softmax.\n\nIn this way, we obtain a joint probability on class c and frame f . This representation can be marginalized over classes p(f |S) = c p(c, f |S). This will give us information about the most important frames of a given video. The activations of each frame are converted to 2D Gaussian heatmaps, which will give us a qualitative evaluation of the most important regions of the face as shown in Figure  8 .\n\nThe figure shows all the steps of key frame sub-sampling. Using this method, we sample 10 key frames for each video clip. 10 key frames with highest classification scores are chosen.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Optical Flow",
      "text": "Key Frame sub-sampling shows us that eyes and mouth are the most important regions of the face, from which discriminative features can be extracted. Hence, as shown in Figure  2 , we use the extracted key frames and facial landmarks to automatically draw bounding boxes over the mouth and eyes of every frame. This way, the mouth and eyes are extracted from every key frame. Now, we have RGB images video frames and RGB frames of mouth and eyes.\n\nOptical flow  [17]  is the pattern of apparent motion of image objects between two consecutive frames caused by the movemement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Optical flow is explicitly used in Action Recognition algorithms to extract motion information. To incorporate action recognition mechanisms and to effectively capture temporal context, we extract optical frame information between every two consecutive frames. After this step, we have our input data for any action recognition mechanism that is rich in spatial and temporal context.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Temporal Gaussian Attention Filters",
      "text": "Many high-level activities are often composed of multiple temporal parts (e.g., sub-events) with different duration/speed, and our objective is to make the model explicitly learn such temporal structure using multiple attention filters and benefit from them. The idea of temporal filters is taken from  [51] . The filters are designed to be fully differentiable, allowing end-to-end training. We can learn a set of optimal static temporal attention filters to be shared across different videos. Hence, in our work, we incorporate this idea to capture global temporal attention between video clips.\n\nThe temporal attention filters are applied after we do convolutional feature extraction from segments of each video clip, as shown in Figure  2 . Each temporal filter learns three parameters: a center g, a stride δ and a width σ. These parameters determine where the filter is placed and the size of the interval focused on. A filter consists of N Gaussian filters separated by a astride of δ frames. The goal of this model is to learn where in the video the most useful features appear.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Network Architecture And Training",
      "text": "We propose a three stream convolutional feed-forward architecture for feature extraction. The inputs to the network are key facial RGB frames, optical flow frames of eyes and optical flow frames of mouth. All the inputs are of shape 96 X 96 X 3. Every block of the architecture consists of Conv -Batch Normalization -Max Pooling layers. The network has one spatial stream which takes RGB images as input and two temporal streams which take optical flow as inputs. The architecture is an encoder with decreasing resolution and increasing number of channels with depth. Dropout regularization is used to avoid over-fitting. Out of a total of 600 video clips, 500 are used for training and 100 are for test. The train and test video clips are chosen such that they have identical independent frame-wise valencearousal distributions as shown in Figure  9 . The features from the three streams are fused using average pooling after after 5 blocks. The fused features are given to temporal attention filters, after which there are two fully connected blocks. 21 levels of Arousal and Valence [-10, 10] are converted to values between 0,1 by using min-max normalization. All layers use ReLU activation, while the last layer uses sigmoid activation. 1 -(Average of Concordance Correlation Coefficient (CCC) or Arousal and Valence) is used as the Loss function. The network is trained using Nvidia RTX 2080 Ti GPU for 200 epochs with a batch size of 32. We use a Learning Rate Scheduler with an initial learning rate of 5e-5.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "In this section we are going to discuss quantitative results and the baselines used. Table  11  shows the quantitative scores of CCC and MSE. It can be observed that Our Approach performs the best on afew-va dataset over all other emotion and action recognition models. This is because our approach is an ensemble approach that includes submodules of multiple action recognition algorithms, such as optical flow streams, parallel feature extraction and temporal attention filters. On top of this, key frame extraction using spatial self-attention ensures the accuracy with discriminative frames with a low processing overhead.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "We use Support Vector Machine for Regression (SVR) as our baseline. Hybrid LBP features are extracted from Facial video frames. LBP is a visual descriptor used for texture classification. We use a simple linear transformation model to scale and translate all the images onto a common coordinate frame. We extract 27X27 patches around each of the 68 landmarks. Each patch is represented by a 59-D histogram. All histograms are concatenated to form a global descriptor of size 4012. SVR is trained on these features to establish",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Two evaluation metrics are used in this paper for quantitative comparative analysis:\n\n, where s x and s y are standard deviations and µ x and µ y are corresponding means. This is a widely used metric to measure the performance of dimensional emotion recognition methods. CCC evaluates the agreement between two timeseries data, in our case, video annotations and predictions.\n\nThis is a common evaluation metric used for regression problems.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper presents a novel three-stream action recognition-based emotion recognition pipeline with spatial key frame extraction mechanism and temporal Gaussian filters. This is an ensemble approach that combines multiple sub-modules of standard and state-of-the-art action recognition algorithms to get state-of-the-art accuracy on the continuous, in-the-wild affect dataset AFEW-VA. Further, our work is a proof of concept that the action recognition mechanisms can be extrapolated to the task of continuous emotion recognition. To the best of our knowledge, this is the first multi-stream pipeline with spatial self-attention, optical flow inputs of facial regions and temporal Gaussian Attention filters to be designed to solve the problem of continuous affect recognition. Further, the paper provides a de- For future work, the whole body could be considered for expression recognition by extracting bodily features. This would be a more challenging problem. The model could be fine-tuned further for better accuracy. Adversarial training and Semi-supervised training can be used for further improvement in accuracy.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The 2D Emotion wheel or the 2D Valence-Arousal Space",
      "page": 2
    },
    {
      "caption": "Figure 1: represents the 2D Emotion Wheel or",
      "page": 2
    },
    {
      "caption": "Figure 2: Pipeline of our Emotion Recognition System",
      "page": 3
    },
    {
      "caption": "Figure 3: Data Statistics of our dataset",
      "page": 4
    },
    {
      "caption": "Figure 4: Screenshot of the annotation tool developed and used to",
      "page": 5
    },
    {
      "caption": "Figure 5: Example of annotated valence and arousal levels for",
      "page": 5
    },
    {
      "caption": "Figure 6: Examples of tracked landmarks in the dataset",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the distribution of the values",
      "page": 6
    },
    {
      "caption": "Figure 4: shows a screenshot of the annotation tool used by",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the temporal evolution of the",
      "page": 6
    },
    {
      "caption": "Figure 6: , including both interior and boundary points of",
      "page": 6
    },
    {
      "caption": "Figure 2: will be discussed.",
      "page": 6
    },
    {
      "caption": "Figure 6: shows varying illumination and distance from camera per-",
      "page": 6
    },
    {
      "caption": "Figure 7: The CNN takes the video frame as its input and produces",
      "page": 6
    },
    {
      "caption": "Figure 8: Steps of Key Frame subsampling. Row 1: Raw face extracted images. Row2: Facial Landmarks for faces. Row3: Illumination",
      "page": 7
    },
    {
      "caption": "Figure 8: The figure shows all the steps of key frame sub-sampling.",
      "page": 8
    },
    {
      "caption": "Figure 2: , we use the extracted key frames and facial",
      "page": 8
    },
    {
      "caption": "Figure 2: Each temporal filter learns",
      "page": 8
    },
    {
      "caption": "Figure 9: The features",
      "page": 8
    },
    {
      "caption": "Figure 9: Train-Test Distributions of Arousal and Valence",
      "page": 9
    },
    {
      "caption": "Figure 10: An illustration of our temporal attention filter. The filter",
      "page": 9
    },
    {
      "caption": "Figure 11: Table of quantitative results",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 11: shows the quantitative",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition using a hybrid cnn-sift aggregator",
      "authors": [
        "Mundher Al-Shabi",
        "Ping Cheah",
        "Tee Connie"
      ],
      "year": "2016",
      "venue": "Facial expression recognition using a hybrid cnn-sift aggregator",
      "arxiv": "arXiv:1608.02833"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition with spatial attention and temporal softmax pooling. Image Analysis and Recognition",
      "authors": [
        "Masih Aminbeidokhti",
        "Marco Pedersoli",
        "Patrick Cardinal",
        "Eric Granger"
      ],
      "year": "2019",
      "venue": "Emotion recognition with spatial attention and temporal softmax pooling. Image Analysis and Recognition"
    },
    {
      "citation_id": "3",
      "title": "The automatic detection of chronic painrelated expression: requirements, challenges and the multimodal emopain dataset",
      "authors": [
        "Min Sh Aung",
        "Sebastian Kaltwang",
        "Bernardino Romera-Paredes",
        "Brais Martinez",
        "Aneesha Singh",
        "Matteo Cella",
        "Michel Valstar",
        "Hongying Meng",
        "Andrew Kemp",
        "Moshen Shafizadeh"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Fully automatic facial action recognition in spontaneous behavior",
      "authors": [
        "Marian Stewart",
        "Gwen Littlewort",
        "Mark Frank",
        "Claudia Lainscsek",
        "Ian Fasel",
        "Javier Movellan"
      ],
      "year": "2006",
      "venue": "7th International Conference on Automatic Face and Gesture Recognition (FGR06)"
    },
    {
      "citation_id": "5",
      "title": "Affwild net and aff-wild database",
      "authors": [
        "Alvertos Benroumpi",
        "Dimitrios Kollias"
      ],
      "year": "2019",
      "venue": "Affwild net and aff-wild database"
    },
    {
      "citation_id": "6",
      "title": "Multi-modal audio, video and physiological sensor learning for continuous emotion prediction",
      "authors": [
        "Kevin Brady",
        "Youngjune Gwon",
        "Pooya Khorrami",
        "Elizabeth Godoy",
        "William Campbell",
        "Charlie Dagli",
        "Thomas Huang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "7",
      "title": "Ets system for av+ ec 2015 challenge",
      "authors": [
        "Patrick Cardinal",
        "Najim Dehak",
        "Alessandro Koerich",
        "Jahangir Alam",
        "Patrice Boucher"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "8",
      "title": "Multi-modal dimensional emotion recognition using recurrent neural networks",
      "authors": [
        "Shizhe Chen",
        "Qin Jin"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "9",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu Simón",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "Roddy Cowie",
        "Randolph Cornelius"
      ],
      "year": "2003",
      "venue": "Describing the emotional states that are expressed in speech"
    },
    {
      "citation_id": "11",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "Tim Dalgleish",
        "Mick Power"
      ],
      "year": "2000",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "12",
      "title": "Convolutional two-stream network fusion for video action recognition",
      "authors": [
        "Christoph Feichtenhofer",
        "Axel Pinz",
        "Andrew Zisserman"
      ],
      "year": "2005",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "Learning dynamics from kinematics: Estimating 2d foot pressure maps from video frames",
      "authors": [
        "Christopher Funk",
        "Savinay Nagendra",
        "Jesse Scott",
        "Bharadwaj Ravichandran",
        "John Challis",
        "Robert Collins",
        "Yanxi Liu"
      ],
      "year": "2018",
      "venue": "Learning dynamics from kinematics: Estimating 2d foot pressure maps from video frames",
      "arxiv": "arXiv:1811.12607"
    },
    {
      "citation_id": "14",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "15",
      "title": "Multimodal prediction of affective dimensions and depression in humancomputer interactions",
      "authors": [
        "Rahul Gupta",
        "Nikolaos Malandrakis",
        "Bo Xiao",
        "Tanaya Guha",
        "Maarten Van Segbroeck",
        "Matthew Black",
        "Alexandros Potamianos",
        "Shrikanth Narayanan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "16",
      "title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "Lang He",
        "Dongmei Jiang",
        "Le Yang",
        "Ercheng Pei",
        "Peng Wu",
        "Hichem Sahli"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "17",
      "title": "Determining optical flow",
      "authors": [
        "K Berthold",
        "Brian Horn",
        "Schunck"
      ],
      "year": "1981",
      "venue": "Techniques and Applications of Image Understanding"
    },
    {
      "citation_id": "18",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "Shuiwang Ji",
        "Wei Xu",
        "Ming Yang",
        "Kai Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "Heechul Jung",
        "Sihaeng Lee",
        "Junho Yim",
        "Sunjeong Park",
        "Junmo Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "20",
      "title": "Inferring depression and affect from application dependent meta knowledge",
      "authors": [
        "Markus Kächele",
        "Martin Schels",
        "Friedhelm Schwenker"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "21",
      "title": "Ensemble methods for continuous affect recognition: Multi-modality, temporality, and challenges",
      "authors": [
        "Markus Kächele",
        "Patrick Thiam",
        "Günther Palm",
        "Friedhelm Schwenker",
        "Martin Schels"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "22",
      "title": "Doubly sparse relevance vector machine for continuous facial behavior estimation",
      "authors": [
        "Sebastian Kaltwang",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "Andrej Karpathy",
        "George Toderici",
        "Sanketh Shetty",
        "Thomas Leung",
        "Rahul Sukthankar",
        "Li Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Real-time normalization and feature extraction of 3d face data using curvature characteristics",
      "authors": [
        "Seok Tae Kyun Kim",
        "Sang Cheol Kee",
        "Kim"
      ],
      "year": "2001",
      "venue": "Proceedings 10th IEEE International Workshop on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "25",
      "title": "Interweaving deep learning and semantic techniques for emotion analysis in human-machine interaction",
      "authors": [
        "Dimitris Kollias",
        "George Marandianos",
        "Amaryllis Raouzaiou",
        "Andreas-Georgios Stafylopatis"
      ],
      "year": "2015",
      "venue": "2015 10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)"
    },
    {
      "citation_id": "26",
      "title": "On line emotion detection using retrainable deep neural networks",
      "authors": [
        "Dimitrios Kollias",
        "Athanasios Tagaris",
        "Andreas Stafylopatis"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "27",
      "title": "Deep neural architectures for prediction in healthcare",
      "authors": [
        "Dimitrios Kollias",
        "Athanasios Tagaris",
        "Andreas Stafylopatis",
        "Stefanos Kollias",
        "Georgios Tagaris"
      ],
      "year": "2018",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "28",
      "title": "Adaptation and contextualization of deep neural network models",
      "authors": [
        "Dimitrios Kollias",
        "Miao Yu",
        "Athanasios Tagaris",
        "Georgios Leontidis",
        "Andreas Stafylopatis",
        "Stefanos Kollias"
      ],
      "venue": "2017 IEEE symposium series on computational intelligence (SSCI)"
    },
    {
      "citation_id": "29",
      "title": "Training deep neural networks with different datasets in-the-wild: The emotion recognition paradigm",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "30",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "31",
      "title": "A new rainfall-induced deep learning strategy for landslide susceptibility prediction",
      "authors": [
        "Jiangtao Liu",
        "Chaopeng Shen",
        "Te Pei",
        "Kathryn Lawson",
        "Daniel Kifer",
        "Savinay Nagendra",
        "Srikanth Banagere"
      ],
      "year": "2021",
      "venue": "AGU Fall Meeting Abstracts"
    },
    {
      "citation_id": "32",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognitionworkshops"
    },
    {
      "citation_id": "33",
      "title": "Painful data: The unbcmcmaster shoulder pain expression archive database",
      "authors": [
        "Patrick Lucey",
        "Jeffrey F Cohn",
        "Patricia Kenneth M Prkachin",
        "Iain Solomon",
        "Matthews"
      ],
      "year": "2011",
      "venue": "Face and Gesture"
    },
    {
      "citation_id": "34",
      "title": "Computationally modeling human emotion",
      "authors": [
        "Stacy Marsella",
        "Jonathan Gratch"
      ],
      "year": "2014",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "35",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "36",
      "title": "Depression recognition based on dynamic facial and vocal expression features using partial least square regression",
      "authors": [
        "Hongying Meng",
        "Di Huang",
        "Heng Wang",
        "Hongyu Yang",
        "Mohammed Ai-Shuraifi",
        "Yunhong Wang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge"
    },
    {
      "citation_id": "37",
      "title": "Robust face recognition system in video using hybrid scale invariant feature transform",
      "authors": [
        "Vimalkumar Mohanraj",
        "M Vimalkumar",
        "M Mithila",
        "Vaidehi"
      ],
      "year": "2016",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "38",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "An efficient deep learning mechanism for cross-region generalization of landslide events",
      "authors": [
        "Savinay Nagendra",
        "S Banagere Manjunatha",
        "Chaopeng Shen",
        "Daniel Kifer",
        "Te Pei"
      ],
      "year": "2020",
      "venue": "AGU Fall Meeting Abstracts"
    },
    {
      "citation_id": "40",
      "title": "Patchrefinenet: Improving binary segmentation by incorporating signals from optimal patch-wise binarization",
      "authors": [
        "Savinay Nagendra",
        "Daniel Kifer"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Constructing a large-scale landslide database across heterogeneous environments using task-specific model updates",
      "authors": [
        "Savinay Nagendra",
        "Daniel Kifer",
        "Benjamin Mirus",
        "Te Pei",
        "Kathryn Lawson",
        "Srikanth Banagere Manjunatha",
        "Weixin Li",
        "Hien Nguyen",
        "Tong Qiu",
        "Sarah Tran"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "42",
      "title": "Cloud-based interactive database management suite integrated with deep learning-based annotation tool for landslide mapping",
      "authors": [
        "T Nagendra",
        "S Pei",
        "G Banagere Manjunatha",
        "T He",
        "D Qiu",
        "C Kifer",
        "Shen"
      ],
      "year": "2020",
      "venue": "AGU Fall Meeting Abstracts"
    },
    {
      "citation_id": "43",
      "title": "Comparison of reinforcement learning algorithms applied to the cart-pole problem",
      "authors": [
        "Savinay Nagendra",
        "Nikhil Podila",
        "Rashmi Ugarakhod",
        "Koshy George"
      ],
      "year": "2022",
      "venue": "2017 international conference on advances in computing, communications and informatics (ICACCI)",
      "arxiv": "arXiv:2211.06560"
    },
    {
      "citation_id": "44",
      "title": "Estimating uncertainty in landslide segmentation models",
      "authors": [
        "Savinay Nagendra",
        "Chaopeng Shen",
        "Daniel Kifer"
      ],
      "year": "2023",
      "venue": "Estimating uncertainty in landslide segmentation models",
      "arxiv": "arXiv:2311.11138"
    },
    {
      "citation_id": "45",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Robust continuous prediction of human emotions using multiscale dynamic cues",
      "authors": [
        "Jérémie Nicolle",
        "Vincent Rapp",
        "Kévin Bailly"
      ],
      "year": "2004",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "47",
      "title": "Toward an affectsensitive multimodal human-computer interaction",
      "authors": [
        "Maja Pantic"
      ],
      "year": "2003",
      "venue": "Toward an affectsensitive multimodal human-computer interaction"
    },
    {
      "citation_id": "48",
      "title": "Deep face recognition",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Deep face recognition"
    },
    {
      "citation_id": "49",
      "title": "Utilizing an interactive ai-empowered web portal for landslide labeling for establishing a landslide database in washington state, usa",
      "authors": [
        "Te Pei",
        "Savinay Nagendra",
        "Srikanth Banagere Manjunatha",
        "Guanlin He",
        "Daniel Kifer",
        "Tong Qiu",
        "Chaopeng Shen"
      ],
      "year": "2021",
      "venue": "EGU General Assembly Conference Abstracts"
    },
    {
      "citation_id": "50",
      "title": "Learning latent subevents in activity videos using temporal attention filters",
      "authors": [
        "Chenyou Aj Piergiovanni",
        "Michael Fan",
        "Ryoo"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Shashank Jaiswal",
        "Erik Marchi",
        "Denis Lalanne",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "52",
      "title": "Audiovisual three-level fusion for continuous estimation of russell's emotion circumplex",
      "authors": [
        "Enrique Sánchez-Lozano",
        "Paula Lopez-Otero",
        "Laura Docio-Fernandez",
        "Enrique Argones-Rúa",
        "José Luis"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge"
    },
    {
      "citation_id": "53",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "55",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "Björn Schuller",
        "Michel Valstar",
        "Florian Eyben",
        "Gary Mckeown",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "56",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "57",
      "title": "A multimodal fuzzy inference system using a continuous facial expression representation for emotion detection",
      "authors": [
        "Catherine Soladié",
        "Hanan Salam",
        "Catherine Pelachaud",
        "Nicolas Stoiber",
        "Renaud Séguier"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "58",
      "title": "Assessment of parkinson's disease based on deep neural networks",
      "authors": [
        "Athanasios Tagaris",
        "Dimitrios Kollias",
        "Andreas Stafylopatis"
      ],
      "year": "2017",
      "venue": "International Conference on Engineering Applications of Neural Networks"
    },
    {
      "citation_id": "59",
      "title": "Machine learning for neurodegenerative disorder diagnosis-survey of practices and launch of benchmark dataset",
      "authors": [
        "Athanasios Tagaris",
        "Dimitrios Kollias",
        "Andreas Stafylopatis",
        "Georgios Tagaris",
        "Stefanos Kollias"
      ],
      "year": "2018",
      "venue": "International Journal on Artificial Intelligence Tools"
    },
    {
      "citation_id": "60",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y-I Tian",
        "Takeo Kanade",
        "Jeffrey Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "61",
      "title": "End-toend multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "63",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "Michel Valstar",
        "Maja Pantic"
      ],
      "venue": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database"
    },
    {
      "citation_id": "64",
      "title": "Avec 2013: the continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "Michel Valstar",
        "Björn Schuller",
        "Kirsty Smith",
        "Florian Eyben",
        "Bihan Jiang",
        "Sanjay Bilakhia",
        "Sebastian Schnieder",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge"
    },
    {
      "citation_id": "65",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "66",
      "title": "Action recognition with improved trajectories",
      "authors": [
        "Heng Wang",
        "Cordelia Schmid"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "67",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "Limin Wang",
        "Yuanjun Xiong",
        "Zhe Wang",
        "Yu Qiao",
        "Dahua Lin",
        "Xiaoou Tang",
        "Luc Van Gool"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "68",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "Limin Wang",
        "Yuanjun Xiong",
        "Zhe Wang",
        "Yu Qiao",
        "Dahua Lin",
        "Xiaoou Tang",
        "Luc Van Gool"
      ],
      "year": "2016",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "69",
      "title": "A 3d facial expression database for facial behavior research",
      "authors": [
        "Lijun Yin",
        "Xiaozhou Wei",
        "Yi Sun",
        "Jun Wang",
        "Matthew Rosato"
      ],
      "year": "2006",
      "venue": "7th international conference on automatic face and gesture recognition (FGR06)"
    },
    {
      "citation_id": "70",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "71",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "72",
      "title": "Face detection based on multi-block lbp representation",
      "authors": [
        "Lun Zhang",
        "Rufeng Chu",
        "Shiming Xiang",
        "Shengcai Liao",
        "Stan Li"
      ],
      "year": "2007",
      "venue": "International conference on biometrics"
    },
    {
      "citation_id": "73",
      "title": "A high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Shaun Jeffrey F Cohn",
        "Michael Canavan",
        "Andy Reale",
        "Peng Horowitz",
        "Liu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "74",
      "title": "Lie detection from speech analysis based on k-svd deep belief network model",
      "authors": [
        "Yan Zhou",
        "Heming Zhao",
        "Xinyu Pan"
      ],
      "year": "2015",
      "venue": "International Conference on Intelligent Computing"
    },
    {
      "citation_id": "75",
      "title": "A rapid and realistic 3d stratigraphic model generator conditioned on reference well log data",
      "authors": [
        "Zhu",
        "S Tilke",
        "M Nagendra",
        "M Etchebes",
        "Lefranc"
      ],
      "year": "2022",
      "venue": "Second EAGE Digitalization Conference and Exhibition"
    },
    {
      "citation_id": "76",
      "title": "Contrast limited adaptive histogram equalization",
      "authors": [
        "Karel Zuiderveld"
      ],
      "year": "1994",
      "venue": "Graphics gems IV"
    }
  ]
}