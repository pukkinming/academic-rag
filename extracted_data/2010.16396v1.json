{
  "paper_id": "2010.16396v1",
  "title": "Emotion Understanding In Videos Through Body, Context, And Visual-Semantic Embedding Loss",
  "published": "2020-10-30T17:48:42Z",
  "authors": [
    "Panagiotis Paraskevas Filntisis",
    "Niki Efthymiou",
    "Gerasimos Potamianos",
    "Petros Maragos"
  ],
  "keywords": [
    "emotion",
    "body",
    "context",
    "visual-semantic",
    "BEEU challenge"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present our winning submission to the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Based on recent literature on the effect of context/environment on emotion, as well as visual representations with semantic meaning using word embeddings, we extend the framework of Temporal Segment Network to accommodate these. Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.",
      "page_start": 1,
      "page_end": 7
    },
    {
      "section_name": "Introduction",
      "text": "Automatic human affect recognition from visual cues is an important area of computer vision that has attracted increased interest over the last two decades, due to its many applications. Indeed, social robotics  [2] , psychiatric care  [13] , and edutainment  [10]  are all areas that can benefit from automatic recognition of emotion.\n\nMost past approaches to the problem have focused on facial expressions in order to determine the emotional state of the person of interest  [7, 18, 22] . This is reasonable due to the fact that facial expressions have been studied extensively in the psychology and emotion literature  [8] . For example, the Facial Action Coding System (FACS)  [9]  identifies the units of facial movements, based on facial muscle groups. Combinations of the so-called action units (AUs) have also been linked with emotional states with extensions of the basic FACS such as EMFACS (Emotion FACS)  [11] . On the other hand, there is no similar established coding system for body expressions, although some have been proposed  [4] .\n\nCompared to facial expression based approaches, recent works have sought alternative modalities and streams of information to detect emotion; one is bodily expressions since many have highlighted the fact that the emotional state is conveyed through bodily expressions as well, and in certain emotions it is the main modality  [5, 15, 26] , or can be used to correctly disambiguate the corresponding facial expression  [1] . Simultaneously, it is important to note that in cases and applications where the emotion needs to be identified, the human body is more frequently available than the face since the face can be occluded, hidden, or far in the distance. Another auxiliary stream of information besides the face and the body that can help in identifying emotions is the context and the surrounding environment of the person  [16, 21] . It is apparent that both the place, as well as objects and other humans can influence a person's emotions.\n\nWe should also note that inherently emotion recognition is a multi-label problem -the subject might be feeling two or more emotions. This is true, especially when considering an extended set of emotions, as in  [19] . The emotions in extended sets do not have the same \"semantic\" distance between them. For example, anger is more close to annoyance than to happiness. Considering that previous works have showed the superiority of methods that attempt to learn a joint embedding space that contains both word embeddings and visual representations  [6, 12, 24] , we believe that trying to attach a semantic meaning to the extracted visual feature is a natural way forward.\n\nIn this paper, based on the above, we describe the method of our team in the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Our method combines Temporal Segment Networks (TSNs)  [27]  focusing on the body, using the context in each video as an additional stream, and also uses an extra visual-semantic embedding loss, based on GloVE (Global Vectors)  [23]  word embedding representations. Our experiments in the validation set verify the better performance of our method compared to the traditional TSNs, while our emotion recognition score on the test set was 0.26235.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Related Work",
      "text": "While most past approaches in visual detection of affect have been focused on facial expressions  [5] , recent approaches have started taking into account the body language  [15]  of the person in question, as well as its surrounding context/environment.\n\nIn  [14] , Gunes and Piccardi introduced a bimodal architecture that takes into account both upper body and facial expressions, in order to detect affect in videos. In  [3] , Dael et al. analyzed and classified body emotional expressions using a body action and posture coding system which was proposed in  [4] . The 3D pose of children was also utilized in  [20]  by Marinoui et al. to detect emotions in continuous dimensions, while in  [10] , 2D pose was used and fused with facial expressions for child emotion recognition. Luo et al.  [19]  introduced a large scale video dataset (BoLD) annotated with categorical and continuous emotions, which is the one used in the BEEU challenge.\n\nRegarding the context modality, Kosti et al.  [16]  introduced a large scale dataset for emotion recognition (EMOTIC) in different contexts (e.g., other people, places, or objects) and a convolutional neural network (CNN) based two-stream architecture that focused on the body and context of the subjects.\n\nThe CAER video dataset for context-based emotion recognition was presented in  [17] , along with a two-stream architecture which employed adaptive-fusion to merge the two steams. In  [21] , Mittal et al. designed a deep architecture with several branches, focusing on different interpretations of the surrounding context (e.g., environment and interaction context) to significantly increase resulting predictions in the EMOTIC dataset.\n\nFinally, some recent works have also focused on extracting visual representations from images that present the semantic relations found in embeddings built from words. The DeViSE embedding model  [12]  extracted semanticallymeaningful visual representations by introducing a similarity loss between the feature vector extracted from a CNN and the word embedding from a skip-gram text model. Using a similar method, Wei et al.  [28]  built joint text and visual embeddings as emotion representation from web images, and in  [29] , Ye and Li built semantic embeddings for a multi-label classification problem.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The dataset used in the challenge is the BoLD (Body Language Dataset) corpus  [19]  consisting of 9,876 video clips of humans expressing emotion, primarily through body movements. Each clip can contain multiple characters, yielding a total of 13,239 annotations, split into a training, validation, and test set. The dataset has been annotated by crowdsourcing employing two widely accepted categorizations of emotion. The first one is the categorical annotation with a total of 26 labels first used in  [16] , by collecting and processing an extensive affective vocabulary. The second annotation regards the continuous emotional dimensions of the VAD (Valence -Arousal -Dominance) Emotional State Model  [25] . The methods in the challenge are evaluated using the following Emotion Recognition Score (ERS):\n\nwhere mR 2 is the mean coefficient of determination (R 2 ) score for the three dimensional emotions (VAD), and mAP and mRA is the mean Average Precision and the mean area under receiver operating characteristic curve (ROC AUC) of the multilabel categorical predictions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "Our model is based on the TSN architecture  [27] , which has been widely used in action recognition and can be seen in Fig.  1 . During training, K different segments are selected from the input video, and then N consecutive frames are selected from each segment. This is done to deal with the fact that consecutive frames have usually redundant information. Traditionally, two different modalities are used, one is the spatial (RGB) modality and the second one is the optical flow. TSNs have already been shown to achieve good results for the BoLD dataset in its introductory paper  [19] .\n\nIn our approach, we modify the original version of TSNs mainly in two directions:\n\nContext: We introduce one additional stream based on the context-environment surrounding the annotated human. For the RGB modality, we input the context in the network in the same way as in  [21] , by masking out the instance body (we set all pixels to 0). We call this stream RGB-c, and the body streams RGB-b and Flow-b. During training, the RGB-b and RGB-c streams are combined at the feature level (RGB-bc) and are trained jointly while the Flow-b TSN is trained independently.\n\nEmbedding Loss: Our second extension is the introduction of an embedding loss on the feature vector extracted by the Convolutional Neural Network (ConvNet). This is done to exploit the fact that some emotions are closer semantically to others. This is also revealed by examining the correlation matrix of the dataset labels in  [19] , where some labels occur more frequently in combination with others (e.g. Happiness and Pleasure, Annoyance and Anger, etc.). Due to this result, we try to attach a semantic meaning to the feature vector extracted by the backbone image network.\n\nTo implement this, we first obtain for each one of the 26 categorical labels of BoLD their 300-dimensional GloVE word embedding  [23] . A PCA-projection of the 26 embeddings is shown in Fig.  2 , where it is apparent that the distances between embeddings are indicative of their \"semantic\" distance. We then use Fig.  2 : PCA projection of the categorical emotions GloVE word embeddings. a fully connected layer to map the feature extracted from the image to a 300dimensional space and introduce the following mean-squared based loss:\n\nwhere f v (x) is the feature vector extracted by applying the convNet on the image x, W is a linear transformation from the space of the feature vector to the word embedding space, f w (y) is the word embedding of the label y, and K is the set of all positive labels for the image x. That is, we try to reduce the Euclidean distance between the projected image feature and the arithmetic mean of the GloVE embeddings of the positive labels for image/video.\n\nPredictions: Finally, after extracting for each sampled image its feature vector, we use two fully connected layers, one to classify to the 26 different categorical labels, and one to regress over the 3 different categorical emotions. The two TSNs are trained using the following loss:\n\nSpecifically, since the dataset does not provide explicitly the multilabel targets, but the crowdsourced scores between 0 and 1, we include two different losses for the classification part: L cls1 that is the binary cross-entropy between the predicted scores and the multilabel target (obtained after thresholding the multilabel scores at 0.5) and L cls2 that is the mean squared error between the predicted scores and the multilabel scores. We empirically found that the inclusion of L cls2 slightly boosted performance. For the regression part, L cont is the",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "We train each TSN for 50 epochs using Stochastic Gradient Descent (SGD), with initial learning rate 10 - 3  which drops by a factor of 10 at 20 epochs 3 . The backbone networks used is a residual network (ResNet) with 101 layers for the body convNets and a ResNet with 50 layers for the context convNet. We use the default hyperparameters of TSNs: 3 segments, 1 frame from each segment for the RGB streams, and 5 frames from each segment for the optical flow stream. The consensus used for segment fusion is averaging. For each network, we select the epoch with the best validation ERS. We have also found experimentally that the partialBN (Batch Normalization) technique used in  [27]  gives a nontrivial boost to the performance of the network. First, in Table  1  we present two ablation experiments regarding the addition of L emb . We can see that adding the embedding loss increases slightly the performance in the RGB-b stream, and gives a boost to the performance of the Flow-b stream.\n\nThen, in Table  2  we present our experimental results on the validation set of BoLD including the RGB context stream. From the results we can see that including the context along with the body in the RGB modality boosts the validation ERS of the architecture. We also experimented with including the context in the Flow network, but this resulted in worse performance. Our final submission for the test set was the model with the best validation score (0.2439 employing RGB-bc + Flow-b), using 25 segments instead of 3. The results of the different metrics on the test set can also be seen in Table  2 , while the final ERS is 0.26235, improving upon the previous best result of 0.2530  [19] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper we presented our method submitted at the BEEU challenge, winning first place. Our method extended the TSN framework to include a visualsemantic embedding loss, by utilizing GloVE word embeddings, and also included an additional context stream for the RGB modality. We verified the superiority of our extensions compared to the baseline on the validation set of the challenge, and submitted the best system which achieved 0.26235 Emotion Recognition Score on the BoLD test set, surpassing the previous best result of 0.2530.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: During training, K diﬀerent",
      "page": 3
    },
    {
      "caption": "Figure 1: TSN with two RGB spatial streams (body and context) and one optical",
      "page": 4
    },
    {
      "caption": "Figure 2: , where it is apparent that the distances",
      "page": 4
    },
    {
      "caption": "Figure 2: PCA projection of the categorical emotions GloVE word embeddings.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Ablation experiment by training with and without L .",
      "data": [
        {
          "without Lemb": "with Lemb",
          "RGB-b\nFlow-b\nRGB-b + Flow-b": "RGB-b\nFlow-b\nRGB-b + Flow-b",
          "0.1567\n0.1444\n0.1623": "0.1564\n0.1465\n0.1637",
          "0.6140\n0.5914\n0.6307": "0.6143\n0.5947\n0.6327",
          "0.0538\n0.0507\n0.078": "0.0546\n0.0579\n0.0874",
          "0.21955\n0.2093\n0.2375": "0.21997\n0.2142\n0.2428"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Body cues, not facial expressions, discriminate between intense positive and negative emotions",
      "authors": [
        "H Aviezer",
        "Y Trope",
        "A Todorov"
      ],
      "year": "2012",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "Emotion modelling for social robotics applications: a review",
      "authors": [
        "F Cavallo",
        "F Semeraro",
        "L Fiorini",
        "G Magyar",
        "P Sinčák",
        "P Dario"
      ],
      "year": "2018",
      "venue": "Journal of Bionic Engineering"
    },
    {
      "citation_id": "3",
      "title": "Emotion expression in body action and posture",
      "authors": [
        "N Dael",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "4",
      "title": "The body action and posture coding system (BAP): Development and reliability",
      "authors": [
        "N Dael",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "J. Nonverbal Behavior"
    },
    {
      "citation_id": "5",
      "title": "Why bodies? twelve reasons for including bodily expressions in affective neuroscience",
      "authors": [
        "B De Gelder"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society of London B"
    },
    {
      "citation_id": "6",
      "title": "Word2visualvec: Image and video to sentence matching by visual feature prediction",
      "authors": [
        "J Dong",
        "X Li",
        "C Snoek"
      ],
      "year": "2016",
      "venue": "Word2visualvec: Image and video to sentence matching by visual feature prediction",
      "arxiv": "arXiv:1604.06838"
    },
    {
      "citation_id": "7",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "8",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "P Ekman",
        "D Keltner"
      ],
      "year": "1997",
      "venue": "Nonverbal communication: Where nature meets culture"
    },
    {
      "citation_id": "9",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "10",
      "title": "Fusing body posture with facial expressions for joint recognition of affect in child-robot interaction",
      "authors": [
        "P Filntisis",
        "N Efthymiou",
        "P Koutras",
        "G Potamianos",
        "P Maragos"
      ],
      "year": "2019",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "11",
      "title": "Emfacs-7: Emotional facial action coding system",
      "authors": [
        "W Friesen",
        "P Ekman"
      ],
      "year": "1983",
      "venue": "Unpublished manuscript, University of California at San Francisco"
    },
    {
      "citation_id": "12",
      "title": "Devise: A deep visual-semantic embedding model",
      "authors": [
        "A Frome",
        "G Corrado",
        "J Shlens",
        "S Bengio",
        "J Dean",
        "M Ranzato",
        "T Mikolov"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Improving facial emotion recognition in schizophrenia: a controlled study comparing specific and attentional focused cognitive remediation",
      "authors": [
        "B Gaudelus",
        "J Virgile",
        "S Geliot",
        "N Franck",
        "M Dupuis",
        "C Hochard",
        "A Josserand",
        "A Koubichkine",
        "T Lambert",
        "M Perez"
      ],
      "year": "2016",
      "venue": "Frontiers in psychiatry"
    },
    {
      "citation_id": "14",
      "title": "A bimodal face and body gesture database for automatic analysis of human nonverbal affective behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "Proc. ICPR"
    },
    {
      "citation_id": "15",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2013",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "17",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proc. IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "Proc. IEEE computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "19",
      "title": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "3D human sensing, action and emotion recognition in robot assisted therapy of children with autism",
      "authors": [
        "E Marinoiu",
        "M Zanfir",
        "V Olaru",
        "C Sminchisescu"
      ],
      "year": "2018",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "21",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "GloVE: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Multiple instance visual-semantic embedding",
      "authors": [
        "Z Ren",
        "H Jin",
        "Z Lin",
        "C Fang",
        "A Yuille"
      ],
      "year": "2017",
      "venue": "Proc. BMVC"
    },
    {
      "citation_id": "25",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "26",
      "title": "Show your pride: Evidence for a discrete emotion expression",
      "authors": [
        "J Tracy",
        "R Robins"
      ],
      "year": "2004",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "27",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Van Gool"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Learning visual emotion representations from web data",
      "authors": [
        "Z Wei",
        "J Zhang",
        "Z Lin",
        "J Lee",
        "N Balasubramanian",
        "M Hoai",
        "D Samaras"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Multilabel deep visual-semantic embedding",
      "authors": [
        "M Yeh",
        "Y Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}