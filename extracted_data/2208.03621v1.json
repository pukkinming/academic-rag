{
  "paper_id": "2208.03621v1",
  "title": "Bias Reducing Multitask Learning On Mental Health Prediction",
  "published": "2022-08-07T02:28:32Z",
  "authors": [
    "Khadija Zanna",
    "Kusha Sridhar",
    "Han Yu",
    "Akane Sano"
  ],
  "keywords": [
    "bias",
    "epistemic uncertainty",
    "fairness metric",
    "Monte-Carlo dropout",
    "protected label",
    "multi-task learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There has been an increase in research in developing machine learning models for mental health detection or prediction in recent years due to increased mental health issues in society. Effective use of mental health prediction or detection models can help mental health practitioners re-define mental illnesses more objectively than currently done, and identify illnesses at an earlier stage when interventions may be more effective. However, there is still a lack of standard in evaluating bias in such machine learning models in the field, which leads to challenges in providing reliable predictions and in addressing disparities. This lack of standards persists due to factors such as technical difficulties, complexities of high dimensional clinical health data, etc., which are especially true for physiological signals. This along with prior evidence of relations between some physiological signals with certain demographic identities restates the importance of exploring bias in mental health prediction models that utilize physiological signals. In this work, we aim to perform a fairness analysis and implement a multi-task learning based bias mitigation method on anxiety prediction models using ECG data. Our method is based on the idea of epistemic uncertainty and its relationship with model weights and feature space representation. Our analysis showed that our anxiety prediction base model introduced some bias with regards to age, income, ethnicity, and whether a participant is born in the U.S. or not, and our bias mitigation method performed better at reducing the bias in the model, when compared to the reweighting mitigation technique. Our analysis on feature importance also helped identify relationships between heart rate variability and multiple demographic groupings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction And Background",
      "text": "Irrespective of the advancements that machine learning has made possible in several fields such as language technologies, computer vision and medical applications, negative bias is often embedded in the essence of machine learning algorithms. Negative bias is an erroneous assumption made by an algorithm, that is systemically prejudiced against certain groups of people. Negative biases can be encoded in algorithms due to a number of factors, the first being imbalance in the representation of different population categories in the training This work was supported by NSF #1840167 and #2047296.\n\ndata. If certain demographics are lacking from the sample data, models trained on this data often do not generalize when applied to new data that contains those missing demographics  [12] . The second factor that could introduce negative bias in machine learning algorithms is biased human labeling. This is due to the fact that data that are fed into models, especially supervised or semi-supervised models which are widely used in various jurisdictions, are manually labeled by humans who are inherently biased. These models ultimately reflect people's impressions and sustain or further magnify bias from the labeled data  [34] . Training and labeled data aside, there is still risk of introducing bias in the functional form of a model through features and modeling techniques  [12] .\n\nDue to the expanding popularity of machine learning and the inherent biases that come with it, there has been an increased focus on bias and fairness in the field. There are several works on how to accurately define and measure fairness in systems  [18] ,  [21] ,  [29] , how to analyze and mitigate bias using various techniques  [12] ,  [22] ,  [38] , and a few works that assess the trade-offs between fairness and accuracy in these models  [26] ,  [33] .\n\nMental health poses a significant challenge for an individual's well-being, and it is estimated that 792 million people lived with a mental health disorder in 2017  [27] . This is slightly more than one in ten people globally (10.7%). Rising statistics like this has led to an increase in research on mental health, including mental health and well-being prediction using physiological signals over the past couple of years. Several authors research on predicting stress levels, and various mental health conditions using data collected both in clinical settings and in the wild  [1] ,  [7] ,  [30] ,  [31] ,  [36] ,  [39] .\n\nWith the rise in popularity of this field of research and its widespread applications in psychiatry and psychology, the need for effective bias mitigation techniques has become apparent, especially with the sensitive nature of physiological data. Previous research that explored emotional responses captured via physiological signals found some relations between blood pressure, electrodermal activity and race, and blood pressure and gender  [40] . These findings re-iterate the importance of exploring possible bias in mental health prediction models that utilize physiological signals.\n\nDespite several relatively successful bias mitigation efforts in general machine learning literature, when it comes to the field of mental health prediction and emotion recognition, there is still a lack of standards in methods for reducing bias. This ultimately leads to many challenges in providing reliable predictions and in addressing disparities  [24] . This lack of standards persists due to factors such as technical difficulties in regard to data collection, complexities of high dimensional clinical health data, lack of knowledge of underlying causal structures, and challenges to algorithm evaluation  [17] ,  [25] .\n\nOnly a few works to date have explored methods to reduce bias in mental health prediction models, however there has been research on bias analysis and mitigation in emotion recognition models mostly on the facial recognition aspect of computer vision. Majority of these works state nonrepresentative data ratio as a major cause of bias in facial recognition models  [5] ,  [13] ,  [15] ,  [23] ,  [37] , and gender, age and skin-tone to be some of attributes data is unbalanced by  [8] ,  [15] ,  [19] . The methods these papers have introduced are often data and model-driven, making them non-transferable to emotion detection using other types of data  [37] . There have also been a few works that consider bias in emotion recognition using speech data  [12] ,  [14] . They mention gender as a leading demographic that is a source of bias in models  [12] . Other works conducted analyses on emotions in general healthcare procedures, and found that emotional responses in medical practices are heavily culturally mediated with both individual factors like gender, age, occupation, and social factors like food habits, availability, etc. coming into play  [2] .\n\nPark et al. explored the performance of different methods to reduce bias for clinical prediction algorithms for postpartum depression  [24] . They implemented three methods, reweighting, prejudice removal, and removal of the race label to logistic regression, random forest, and extreme gradient boosting models, to mitigate bias based on race. They found that reweighing improves their chosen fairness metrics without compromising accuracy, prejudice remover performed less reliably, and omitting the race label made no significant difference to the fairness metric. Although this study provided promising results, there are some limitations that come with it. First, the authors conducted their experiments based on data collected in a clinical setting, and these results might not hold true on data from the wild. They also implemented these methods on well-known interpretable, and often used models in fairness literature  [33] , and only analyzed bias based on race without exploring other factors. To counter some of these limitations, we will be testing our method on a Long Short term memory (LSTM)-based anxiety prediction model using physiological data collected in the wild. The reason we are using anxiety to assess fairness and our bias mitigation method is due to the fact that most scientific work addressing anxiety and its disorders has thus far been conducted among European Americans. This has created striking gaps of inequalities in anxiety disorder research and practice  [43] , and with this work, we aim to aid in closing some of this gap.\n\nOne area of interest in the field of fairness in general is the trade-off between accuracy and fairness in models  [22] ,  [33] . It is often the case that bias is first introduced to models from the data, and that means using sensitive information in the functional form of the model will improve prediction accuracy  [22] . However, it is well known that in some jurisdictions, using different models either explicitly or implicitly for different protected groups is not allowed  [22] . In this paper, we develop a technique that enables us to optimize accuracy and fairness while improving interpretability of the model, without explicitly using any of the sensitive information in the functional form of our model. We propose the use of Multitask Learning (MTL), which has been proven to improve interpretablity in models that use multidimensional health data  [42] , and has been used in previous research to optimize both accuracy and fairness  [22] , along with Monte Carlo dropout to utilize model uncertainty to improve fairness without sacrificing either computational complexity or accuracy.\n\nOur contribution can be summarized as (i) analyses of bias based on different demographic information on data collected in the wild for anxiety prediction, (ii) development of an MTL-based bias mitigation technique to optimize fairness while preserving accuracy and improving interpretability, (iii) evaluation of the proposed method on LSTM-based models and a large scale of public biobehavioral dataset with various demographics, and\n\n(iv) comparison of our method against a conventional method, reweighting introduced by  [24]  for both accuracy and fairness measures.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Data And Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset",
      "text": "In our experiments, we used a part of the TILES dataset, physiological data collected by Mundnich et al.  [20]  from over 200 hospital workers. We used 10 weeks worth of electrocardiogram (ECG) data collected with the OMSignal smart garment (15-second long ECG signal in 250 Hz every 5 minutes). We extracted 25 frequency and time-domain ECG features, such as the statistical characteristics of the peak-topeak intervals and the energy of the signal in various frequency bands. Table  I  shows the full list of the extracted tables, and the definition of each features can be referred to  [4] . We made use of self-reported anxiety levels which were measured using the State Trait Anxiety Inventory giving a value in the range 20 to 80 for each participant. We further binarized these scores using personalized z-score. The original released data contains anxiety labels that were reported in a 5-point scale by subjects. Following the setting in  [10] , we calculated the z-score of labels for each participant separately, and marked labels below the personalized average as negative (0) and labels above the mean as positive  (1) . We used 2 hours of the data (5 minutes x 24 steps) to infer upcoming anxiety labels. Under this scenario, we have in total 920 samples with 506 negative samples and 414 positive samples.\n\nWe used gender, age, race, income, shift, ethnicity, born in the US, English as a native language, and work hours as the demographic data (referred to as the 9 protected labels in this paper) to test our model fairness on. These data were collected via surveys administered to the participants during the study. We binary-encoded each of the protected labels to assign them as privileged and unprivileged classes to effectively analyze them using our chosen fairness metrics. We chose the class with the higher number of participants as the privileged class (denoted by 1), and the other with less participants as the unprivileged class (denoted by 0). Figure  1  below shows the distribution of the participants in our data based on the different demographic groups. We split the data into 75% training and 25% testing sets to fit into the model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Fairness Terminology",
      "text": "In this section, we introduce and define some of the concepts and terminologies generally used in algorithmic fairness research that are relevant to this paper.\n\n• Protected (sensitive) label: An attribute that partitions a population into groups whose outcomes should have parity (such as race, gender, income, etc.). It is the measure of how different outcomes are for different groups, based on the results of a model  [9] . In this paper, we assume an acceptable lower bound of 0.8, and a higher bound of 1.2, with 1 being the the ideal score.\n\n• Equalized odds: This fairness metric enforces that the model correctly identify the positive outcome at equal rates across groups, and misclassify the positive outcome at equal rates across groups (creating the same proportion of True Positives and False Positives across groups).\n\nIn the context of this paper, we choose to compare false negative instead of true positive rates between the privileged and unprivileged classes because a negative outcome (anxiety=0) is more desirable in our study.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Multi-Task Learning Based Bias Mitigation Method",
      "text": "Our proposed methodology is based on the premise of epistemic uncertainty in Bayesian uncertainty estimation. Epistemic uncertainty refers to uncertainty in the structure and parameters of a model caused by a lack of knowledge, which has also been proven to correlate with model weights  [16] ,  [35] . We hypothesize that when the model is most uncertain about the protected label, the weights of that model will lack knowledge at certain regions of the feature space related to the protected label, and therefore using these weights for anxiety prediction will minimize the influence of that protected label on the final anxiety prediction. Our proposed method will in turn minimize any bias that might be introduced to the model through imbalances in the data based on the protected label.\n\nFigure  2  presents a visual diagram of our proposed method. We utilize multi-task learning to predict anxiety and one of the protected labels (e.g gender). We save the model periodically in our experiments, and implement Monte Carlo dropout which allows us to get uncertainty estimations of these saved models. Modeling uncertainty with Monte Carlo dropout works by running multiple forward passes through the model with a different dropout mask every time. Given a trained neural network model with dropout f nn , to derive uncertainty for one sample x, we collect the predictions of T inferences with different dropout masks. Here, f di nn represents the model with dropout mask d i . So we obtain a sample of the possible model outputs for sample x as f d0 nn (x), ..., f d T nn (x). By computing the average and variance of this sample, we obtain an ensemble prediction, which is the mean of the models posterior distribution for this sample and an estimate of the uncertainty of the model regarding x.\n\npredictive posterior mean:\n\nuncertainty: More information on this can be found in the original publication by Gal et. al  [11] .\n\nFrom the distribution of Monte Carlo predictions obtained, we select the model prediction where the difference in uncertainties between anxiety and the protected label is highest, and then extract the parameters of the model at this desired point, and utilize them in our final anxiety prediction model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Analysis Of Base Model",
      "text": "To start off our experiments, we calculated the disparate impact ratio of the original dataset for a combination of every protected label and anxiety, to understand if the data itself was biased in terms of the anxiety labels. Next, we ran a basic LSTM model for anxiety prediction and analyzed our results for fairness by calculating the disparate impact ratio and equalized odds for our prediction against all 9 protected labels. We conducted these analyses to determine whether and where the model was introducing bias, and ensure that we only apply our bias mitigation method on aspects of the data that are actually biased.\n\nAfter performing fairness analysis on the base model, we selected the protected labels by which our model showed most bias, and tested our method on.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Method Implementation",
      "text": "To implement our method, we ran the MTL model with the structure shown in Figure  3  on a combination of anxiety plus each of the protected labels that showed bias in the base model, and we assigned loss weights of the ratio 4.5:0.5, 4.5 for anxiety and 0.5 for the demographic label. We used this ratio to ensure that anxiety prediction is prioritized by the model, making sure that its uncertainty score is kept lower than the demographic prediction. We arrived at this combination by experimenting with a few different combinations.\n\nWe ran these MTL networks for 100 epochs each, and saved the weights at every 5 epochs, making it a total of 20 weight combinations, and we determined these parameters using a grid search. With this, we used Monte Carlo dropout, and ran some analysis on the Monte Carlo outputs to calculate the uncertainties of each of the saved weight combinations. We computed the uncertainties by calculating the sample variance of the different forward passes using the equation 3.\n\nWe plotted the uncertainties and identified the model that showed the biggest difference in uncertainty scores between anxiety and the demographic prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Saliency Maps",
      "text": "To gain a better understanding of how the features influence our anxiety prediction and the different protected labels, and how our proposed method affects the models, we utilized a saliency map technique  [28]  to visualize the importance of model weights on our input time axis and features. According to the predicted class c, for example, the decision-making process of the model can be represented as S c (I) = w T c I +b c , where I, S c (I), w, b are the model input, output, weights and bias, respectively. The essential of this method is to calculate the model weights on the input layer by gradients, e.g., w = ∂Sc ∂I | I0 . The calculated w represents the model saliency regrading to the input layer. In this study, we fetched the saliency maps for all the test samples and calculated the average saliency map to develop the intuition of important features and time steps in general.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Initial Analysis",
      "text": "After analyzing the dataset before running any models, the TILES dataset appeared to be relatively balanced in terms of the disparate impact ratio. Figure  4  shows the distribution of these scores and their proximity to the ideal score of 1. For most of the protected labels, the scores for the training data were in the range of 0.8 to 1.2, with the exception of income and shift with scores of 1.96 and 1.3 respectively. As for the testing data, the scores for race, shift, ethnicity, bornUS, and lang were in the range, while scores for gender, age income are below, and hours were above the range. After running the base LSTM model, we obtained an accuracy score of 57.5% and an F1 score of 0.487. The baseline random performance for anxiety prediction was 49%, and the macro F1 score was 0.348.\n\nA fairness analysis of the predictions showed us that the model does introduce some bias for some of the labels, specifically age, income, ethnicity, and bornUS, as shown in Tables II to V. The scores for the other labels fall within the chosen range of 0.8 and 1.2.\n\nFigure  5  shows the saliency map of anxiety produced by the base LSTM model. The x-axis represents the different features in our data, and the y-axis represents the 24 timesteps fed into the LSTM model to infer upcoming anxiety labels. From this figure, we can see that feature importance is relatively evenly distributed, with the most importance given to sdsd and pnni 20, which are time-domain-based heart rate variability features. In general, heart rate features (mean hr, max hr, min hr, std hr) seem to have the least importance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Proposed Method",
      "text": "Next, we implemented our method to predict anxiety while mitigating bias caused by age, income, ethnicity, and bornUS protected labels.\n\nTables II to V show a comparison of performance and fairness scores for the base model, our method, and an implementation of the reweighting method on our data. Reweighting involves applying appropriate weights to different tuples in the training dataset to make it discrimination free with respect to the protected label  [24] . These tables show that our method was able to improve all the fairness scores, giving an ideal disparate impact ratio score of 1 for, and 0 for differences in False Positive and False Negative rates. This comes at a performance cost, as both our method and reweighting were not able to preserve the accuracy of the base model and F1 scores of the base model.\n\nFigures  6  and 7  show the saliency maps for the age and bornUS experiments. Darker red color indicates more importance. The first map on each figure shows the feature importance on the demographic prediction when running the MTL model, the second shows that of anxiety from the MTL model before proposed method was implemented, and the third one for anxiety after method was fully implemented. Our method shifted weights from features that are important to the protected label to those that are less important. This ultimately decreased the importance of features associated with the protected label in the final anxiety prediction. For example, in Figure  6 , the first map shows that features nni 20 and pnni 20 carried the most importance when it comes to the age label, and the second map shows that before our method implementation, they also appeared to be very important for anxiety prediction. After our method was implemented, it is shown in the third map that these features carried less weight and ultimately are less important for anxiety prediction, which according to our hypothesis, reduced the possibility of the model being biased based on age.    Seeing the ever growing effects of mental health on people's lives, and the gap between advances made in mental health prediction research, and development of effective bias analysis and mitigation methods, the goal of our study is to develop an effective technique to identify and mitigate bias in mental health prediction models. To do this, we analyzed bias in a physiological dataset, and an anxiety prediction model, and introduced a multi-task learning-based bias mitigation method where necessary. This method is based on the hypothesis that when a model is most uncertain about a particular protected label, the weights of that model will lack knowledge at certain regions of the feature space related to that label. Which means that using the weights of that model to predict anxiety will minimize the influence of the protected label on anxiety prediction. Our analysis found that the TILES dataset on its own was imbalanced by number of participants based on gender, age, race, income, shift, ethnicity, number of work hours, and whether or not the participant was born in the US (bornUS), but it was not biased by these demographics when analyzed using the disparate impact ratio as a fairness metric. After running an LSTM model to predict anxiety, and analyzing the fairness in the prediction results, we discovered that the model introduced some bias by age, income, ethnicity, and bornUS.\n\nWe implemented our method to mitigate bias by each of these protected labels, and compared our results to a standard reweighting bias mitigation technique. Our results show that our method overall did better than reweighting on the fairness metric scores, but was not able to preserve the anxiety model performance. There was an average of 7% drop in accuracy, across all four experiments (age, income, ethnicity, bornUS), with the experiment on ethnicity causing the most depreciation by 14%.\n\nTo better understand the effect of different features on prediction, and how they are related to anxiety, and the different protected labels, we utilized a saliency map technique to visualize the importance of model weights on different prediction tasks. We found feature importance to be relatively evenly distributed for anxiety prediction but most correlated with 2 time-domain-based heart rate variability features (sdsd and pnni 20). Heart rate variability is the fluctuation of heart period over time, commonly measured by ECG, and is an important marker of psychological well-being  [3] . One study found anxiety disorders to correspond with lower heart rate variability (HRV)  [3] , confirming why these features carry the most weight for anxiety prediction.\n\nThe saliency maps for the protected labels also gave us some interesting insight. We found that age correlated most with time domain-based HRV features (pnni 20, nni 20, sdsd, sdnn, cvi). Research has shown connections between HRV and a number of demographic factors such as age, race, ethnicity and gender or sex  [6] ,  [32] ,  [41] . This was confirmed by the high correlation between HRV features with bornUS and ethnicity as well. Income on the other hand, correlated with both frequency domain and time domain-based HRV features. Seeing as the considered protected labels and anxiety have high correlations with certain HRV features, it is easy to see how bias can be introduced to the model based on these protected labels.\n\nThe saliency maps also showed how our method shifted importance from features that highly correlate with a particular protected label, in order to reduce it's effect on the final anxiety prediction. Having stated that, we attribute our method's lack of ability to preserve accuracy and F1 scores to the fact that both anxiety and the protected labels have similar important features, which means that shifting weights from these features will undoubtedly have a significant effect on the model's ability to form a pattern for anxiety, ultimately affecting it's prediction. This method will benefit from further testing using datasets with more varying label-feature relationships.\n\nIn conclusion, it is important to acknowledge that data sources represent just one aspect of bias, which can be introduced through a model and certain technical metrics as well. The purpose of prediction algorithms is to influence the clinical decision-making process, and the biases of those developing and using them would have greater impact on whether they ultimately perpetuate inequality. For this reason, it is imperative to take into consideration, the impact of both human behavior and technical details when developing algorithms that make critical decisions such as those on mental health, especially in clinical settings.\n\nWorking with deep learning models for our prediction task, we have encountered a few limitations during the course of our study. First is the computational complexity of using monte carlo dropout. Using this method along with the aspect of saving and loading model weights results in a higher than normal computational cost. In the future, we aim to test our bias mitigation method on other datasets both physiological and non-physiological, and explore different ways to better preserve accuracy with it. We will explore developing a custom loss function that will utilize the relationship between uncertainty and weights to produce better performance.",
      "page_start": 5,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: below shows",
      "page": 3
    },
    {
      "caption": "Figure 1: Distributions of Demographic Data in the TILES dataset",
      "page": 3
    },
    {
      "caption": "Figure 2: presents a visual diagram of our proposed method.",
      "page": 3
    },
    {
      "caption": "Figure 2: Process Diagram",
      "page": 4
    },
    {
      "caption": "Figure 3: on a combination of anxiety",
      "page": 4
    },
    {
      "caption": "Figure 3: Structure of Multi-talk Model",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the distribution of",
      "page": 5
    },
    {
      "caption": "Figure 4: Disparate Impact Scores of Initial Data",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the saliency map of anxiety produced by the",
      "page": 5
    },
    {
      "caption": "Figure 5: Saliency Map of Base Anxiety Prediction (5 mins x 24 steps). The",
      "page": 5
    },
    {
      "caption": "Figure 6: , the ﬁrst map shows that features nni 20",
      "page": 5
    },
    {
      "caption": "Figure 6: Saliency Maps of Age Experiments",
      "page": 7
    },
    {
      "caption": "Figure 7: Saliency Maps of bornUS Experiments",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Category": "Time Domain",
          "Features": "mean nni, sdnn, sdsd, nni 50, pnni 50,\n20, pnni 20,rmssd, median nni,\nrange nni,\nnni\ncvsd, cvnni, mean hr, max\nhr, min\nhr, std hr"
        },
        {
          "Category": "Frequency Domain",
          "Features": "lf, hf,\nlf hf\nratio,\nlfnu, hfnu,\ntotal power, vlf"
        },
        {
          "Category": "Other",
          "Features": "cardiac sympathetic index, cardiac vagal\nindex"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric": "Accuracy",
          "Base Model": "0.575",
          "Reweighting": "0.549",
          "Proposed Method": "0.527"
        },
        {
          "Metric": "F1",
          "Base Model": "0.487",
          "Reweighting": "0.517",
          "Proposed Method": "0.418"
        },
        {
          "Metric": "DI Ratio",
          "Base Model": "0.682",
          "Reweighting": "0.648",
          "Proposed Method": "1"
        },
        {
          "Metric": "Diff\nin FN",
          "Base Model": "0.077",
          "Reweighting": "0.066",
          "Proposed Method": "0"
        },
        {
          "Metric": "Diff\nin FP",
          "Base Model": "-0.077",
          "Reweighting": "-0.059",
          "Proposed Method": "0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric": "Accuracy",
          "Base Model": "0.575",
          "Reweighting": "0.539",
          "Proposed Method": "0.544"
        },
        {
          "Metric": "F1",
          "Base Model": "0.487",
          "Reweighting": "0.517",
          "Proposed Method": "0.408"
        },
        {
          "Metric": "DI Ratio",
          "Base Model": "1.389",
          "Reweighting": "0.997",
          "Proposed Method": "1"
        },
        {
          "Metric": "Diff\nin FN",
          "Base Model": "-0.099",
          "Reweighting": "-0.153",
          "Proposed Method": "0"
        },
        {
          "Metric": "Diff\nin FP",
          "Base Model": "0.055",
          "Reweighting": "-0.114",
          "Proposed Method": "0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric": "Accuracy",
          "Base Model": "0.575",
          "Reweighting": "0.542",
          "Proposed Method": "0.434"
        },
        {
          "Metric": "F1",
          "Base Model": "0.487",
          "Reweighting": "0.450",
          "Proposed Method": "0.303"
        },
        {
          "Metric": "DI Ratio",
          "Base Model": "1.24",
          "Reweighting": "1.329",
          "Proposed Method": "1"
        },
        {
          "Metric": "Diff\nin FN",
          "Base Model": "0.072",
          "Reweighting": "-0.054",
          "Proposed Method": "0"
        },
        {
          "Metric": "Diff\nin FP",
          "Base Model": "0.128",
          "Reweighting": "0.054",
          "Proposed Method": "0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Metric": "Accuracy",
          "Base Model": "0.575",
          "Reweighting": "0.525",
          "Proposed Method": "0.504"
        },
        {
          "Metric": "F1",
          "Base Model": "0.487",
          "Reweighting": "0.490",
          "Proposed Method": "0.388"
        },
        {
          "Metric": "DI Ratio",
          "Base Model": "0.673",
          "Reweighting": "0.866",
          "Proposed Method": "1"
        },
        {
          "Metric": "Diff\nin FN",
          "Base Model": "0.110",
          "Reweighting": "0.094",
          "Proposed Method": "0"
        },
        {
          "Metric": "Diff\nin FP",
          "Base Model": "-0.058",
          "Reweighting": "0.016",
          "Proposed Method": "0"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Stress detection with deep learning approaches using physiological signals",
      "authors": [
        "F Albertetti",
        "A Simalastar",
        "A Rizzotti-Kaddouri"
      ],
      "year": "2020",
      "venue": "International Conference on IoT Technologies for HealthCare"
    },
    {
      "citation_id": "2",
      "title": "Biases in assigning emotions in patients due to multicultural issues",
      "authors": [
        "D Casacuberta",
        "J Vallverdú"
      ],
      "year": "2022",
      "venue": "Handbook of Artificial Intelligence in Healthcare"
    },
    {
      "citation_id": "3",
      "title": "Anxiety disorders are associated with reduced heart rate variability: a meta-analysis",
      "authors": [
        "J Chalmers",
        "D Quintana",
        "M Abbott",
        "A Kemp"
      ],
      "year": "2014",
      "venue": "Frontiers in psychiatry"
    },
    {
      "citation_id": "4",
      "title": "Heart rate variability analysis",
      "authors": [
        "R Champseix"
      ],
      "year": "2018",
      "venue": "Heart rate variability analysis"
    },
    {
      "citation_id": "5",
      "title": "Towards unbiased visual emotion recognition via causal intervention",
      "authors": [
        "Y Chen",
        "X Yang",
        "T.-J Cham",
        "J Cai"
      ],
      "year": "2021",
      "venue": "Towards unbiased visual emotion recognition via causal intervention",
      "arxiv": "arXiv:2107.12096"
    },
    {
      "citation_id": "6",
      "title": "Age and ethnicity differences in short-term heart-rate variability",
      "authors": [
        "J.-B Choi",
        "S Hong",
        "R Nelesen",
        "W Bardwell",
        "L Natarajan",
        "C Schubert",
        "J Dimsdale"
      ],
      "year": "2006",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "7",
      "title": "Comparing stress prediction models using smartwatch physiological signals and participant self-reports",
      "authors": [
        "R Dai",
        "C Lu",
        "L Yun",
        "E Lenze",
        "M Avidan",
        "T Kannampallil"
      ],
      "year": "2021",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "8",
      "title": "Responsible ai: Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "Responsible ai: Gender bias assessment in emotion recognition",
      "arxiv": "arXiv:2103.11436"
    },
    {
      "citation_id": "9",
      "title": "Certifying and removing disparate impact",
      "authors": [
        "M Feldman",
        "S Friedler",
        "J Moeller",
        "C Scheidegger",
        "S Venkatasubramanian"
      ],
      "year": "2015",
      "venue": "proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "10",
      "title": "Context-aware speech stress detection in hospital workers using bi-lstm classifiers",
      "authors": [
        "A Gaballah",
        "A Tiwari",
        "S Narayanan",
        "T Falk"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Y Gal",
        "Z Ghahramani"
      ],
      "year": "2016",
      "venue": "international conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "Gender de-biasing in speech emotion recognition",
      "authors": [
        "C Gorrostieta",
        "R Lotfian",
        "K Taylor",
        "R Brutti",
        "J Kane"
      ],
      "year": "2019",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "13",
      "title": "Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems",
      "authors": [
        "A Howard",
        "C Zhang",
        "E Horvitz"
      ],
      "year": "2017",
      "venue": "IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)"
    },
    {
      "citation_id": "14",
      "title": "Removing bias with residual mixture of multi-view attention for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain",
        "R Moore"
      ],
      "year": "2020",
      "venue": "Interspeech 2020"
    },
    {
      "citation_id": "15",
      "title": "Towards fair affective robotics: Continual learning for mitigating bias in facial expression and action unit recognition",
      "authors": [
        "O Kara",
        "N Churamani",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "Towards fair affective robotics: Continual learning for mitigating bias in facial expression and action unit recognition",
      "arxiv": "arXiv:2103.09233"
    },
    {
      "citation_id": "16",
      "title": "Deep bayesian gaussian processes for uncertainty estimation in electronic health records",
      "authors": [
        "Y Li",
        "S Rao",
        "A Hassaine",
        "R Ramakrishnan",
        "D Canoy",
        "G Salimi-Khorshidi",
        "M Mamouei",
        "T Lukasiewicz",
        "K Rahimi"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "17",
      "title": "Ethical limitations of algorithmic fairness solutions in health care machine learning",
      "authors": [
        "M Mccradden",
        "S Joshi",
        "M Mazwi",
        "J Anderson"
      ],
      "year": "2020",
      "venue": "The Lancet Digital Health"
    },
    {
      "citation_id": "18",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2019",
      "venue": "A survey on bias and fairness in machine learning",
      "arxiv": "arXiv:1908.09635"
    },
    {
      "citation_id": "19",
      "title": "Bias and fairness in face detection",
      "authors": [
        "H Menezes",
        "A Ferreira",
        "E Pereira",
        "H Gomes"
      ],
      "year": "2021",
      "venue": "2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "20",
      "title": "Tiles-2018, a longitudinal physiologic and behavioral data set of hospital workers",
      "authors": [
        "K Mundnich",
        "B Booth",
        "M Hommedieu",
        "T Feng",
        "B Girault",
        "J L'hommedieu",
        "M Wildman",
        "S Skaaden",
        "A Nadarajan",
        "J Villatte"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "21",
      "title": "Fairness in machine learning",
      "authors": [
        "L Oneto",
        "S Chiappa"
      ],
      "year": "2020",
      "venue": "Recent Trends in Learning From Data"
    },
    {
      "citation_id": "22",
      "title": "Taking advantage of multitask learning for fair classification",
      "authors": [
        "L Oneto",
        "M Doninini",
        "A Elders",
        "M Pontil"
      ],
      "year": "2019",
      "venue": "ACM Conference (AIES)"
    },
    {
      "citation_id": "23",
      "title": "Contemplating visual emotions: Understanding and overcoming dataset bias",
      "authors": [
        "R Panda",
        "J Zhang",
        "H Li",
        "J.-Y Lee",
        "X Lu",
        "A Roy-Chowdhury"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "24",
      "title": "Comparison of methods to reduce bias from clinical prediction models of postpartum depression",
      "authors": [
        "Y Park",
        "J Hu",
        "M Singh",
        "I Sylla",
        "I Dankwa-Mullan",
        "E Koski",
        "A Das"
      ],
      "year": "2021",
      "venue": "JAMA network open"
    },
    {
      "citation_id": "25",
      "title": "Ensuring fairness in machine learning to advance health equity",
      "authors": [
        "A Rajkomar",
        "M Hardt",
        "M Howell",
        "G Corrado",
        "M Chin"
      ],
      "year": "2018",
      "venue": "Annals of internal medicine"
    },
    {
      "citation_id": "26",
      "title": "Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy",
      "authors": [
        "K Rodolfa",
        "H Lamba",
        "R Ghani"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Mental health. Our World in Data",
      "authors": [
        "H Saloni Dattani",
        "M Roser"
      ],
      "year": "2021",
      "venue": "Mental health. Our World in Data"
    },
    {
      "citation_id": "28",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2013",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "arxiv": "arXiv:1312.6034"
    },
    {
      "citation_id": "29",
      "title": "Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning",
      "authors": [
        "M Srivastava",
        "H Heidari",
        "A Krause"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th acm sigkdd international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "30",
      "title": "Emotiono+: Physiological signals knowledge representation and emotion reasoning model for mental health monitoring",
      "authors": [
        "Y Su",
        "B Hu",
        "L Xu",
        "H Cai",
        "P Moore",
        "X Zhang",
        "J Chen"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "31",
      "title": "Early prediction of cognitive impairments using physiological signal for enhanced socioeconomic status",
      "authors": [
        "S Swati",
        "M Kumar",
        "S Namasudra"
      ],
      "year": "2022",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "32",
      "title": "Twenty-four hour time domain heart rate variability and heart rate: relations to age and gender over nine decades",
      "authors": [
        "K Umetani",
        "D Singer",
        "R Mccraty",
        "M Atkinson"
      ],
      "year": "1998",
      "venue": "Journal of the American College of Cardiology"
    },
    {
      "citation_id": "33",
      "title": "How fair can we go in machine learning? assessing the boundaries of accuracy and fairness",
      "authors": [
        "A Valdivia",
        "J Sánchez-Monedero",
        "J Casillas"
      ],
      "year": "2021",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "34",
      "title": "Artificial intelligence and public trust",
      "authors": [
        "S Vallor"
      ],
      "year": "2017",
      "venue": "Artificial intelligence and public trust"
    },
    {
      "citation_id": "35",
      "title": "A hybrid framework for improving uncertainty quantification in deep learning-based qsar regression modeling",
      "authors": [
        "D Wang",
        "J Yu",
        "L Chen",
        "X Li",
        "H Jiang",
        "K Chen",
        "M Zheng",
        "X Luo"
      ],
      "year": "2021",
      "venue": "Journal of cheminformatics"
    },
    {
      "citation_id": "36",
      "title": "A physiological signal-based method for early mental-stress detection",
      "authors": [
        "L Xia",
        "A Malik",
        "A Subhani"
      ],
      "year": "2018",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "37",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "T Xu",
        "J White",
        "S Kalkan",
        "H Gunes"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Mitigating biases in multimodal personality assessment",
      "authors": [
        "S Yan",
        "D Huang",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "39",
      "title": "Passive sensor data based future mood, health, and stress prediction: User adaptation using deep learning",
      "authors": [
        "H Yu",
        "A Sano"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "40",
      "title": "Clustering of physiological signals by emotional state, race, and sex",
      "authors": [
        "K Zanna",
        "T Neal",
        "S Canavan"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "41",
      "title": "Effect of age and sex on heart rate variability in healthy subjects",
      "authors": [
        "J Zhang"
      ],
      "year": "2007",
      "venue": "Journal of manipulative and physiological therapeutics"
    },
    {
      "citation_id": "42",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "43",
      "title": "Disparities in anxiety and its disorders",
      "authors": [
        "M Zvolensky",
        "L Garey",
        "J Bakhshaie"
      ],
      "year": "2017",
      "venue": "Disparities in anxiety and its disorders"
    }
  ]
}