{
  "paper_id": "2312.01335v1",
  "title": "Facial Emotion Recognition Under Mask Coverage Using A Data Augmentation Technique",
  "published": "2023-12-03T09:50:46Z",
  "authors": [
    "Aref Farhadipour",
    "Pouya Taghipour"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Convolutional Neural Network",
    "Data Augmentation",
    "Transfer Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Identifying human emotions using AI-based computer vision systems, when individuals wear face masks, presents a new challenge in the current Covid-19 pandemic. In this study, we propose a facial emotion recognition system capable of recognizing emotions from individuals wearing different face masks. A novel data augmentation technique was utilized to improve the performance of our model using four mask types for each face image. We evaluated the effectiveness of four convolutional neural networks, Alexnet, Squeezenet, Resnet50 and VGGFace2 that were trained using transfer learning. The experimental findings revealed that our model works effectively in multi-mask mode compared to single-mask mode. The VGGFace2 network achieved the highest accuracy rate, with 97.82% for the person-dependent mode and 74.21% for the person-independent mode using the JAFFE dataset. However, we evaluated our proposed model using the UIBVFED dataset. The Resnet50 has demonstrated superior performance, with accuracies of 73.68% for the person-dependent mode and 59.57% for the person-independent mode. Moreover, we employed metrics such as precision, sensitivity, specificity, AUC, F1 score, and confusion matrix to measure our system's efficiency in detail. Additionally, the LIME algorithm was used to visualize CNN's decision-making strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. I Ntroduction",
      "text": "The popularity of computer assistant systems is dependent on the proper interaction between humans and machines, regardless of real-world scenarios. Emotion recognition from facial expressions plays a crucial role in ensuring accurate communication between humans and machines. However, expressing emotions through the face involves variations in the form and position of facial organs. Additionally, real-world obstacles, such as a mask covering the face, may hinder the system from correctly interpreting a person's emotional state entirely.\n\nThe COVID-19 pandemic has resulted in the widespread use of masks in crowded environments, such as workplaces and hospitals. Despite the increased use of masks, individuals still wish to avail of services that rely on machine interaction. Unfortunately, the standard emotion recognition systems are unable to correctly monitor the person's entire face due to partial mask coverage.\n\nTo address this issue, we propose a new emotion recognition system that can perform effectively in the presence of masks. We first trained the system using datasets comprising images of individuals wearing masks. Convolutional Neural Networks (CNN) were used in four different architectures, and applied with the transfer learning approach. To prevent overfitting and address data scarcity challenges  [1] , we introduce an augmentation technique that adds different masks to the face to increase image variations. This approach involves using an automated algorithm to place mask models on the human face.\n\nTo the best of our understanding, only a few studies have been done on facial emotion recognition under masked conditions. Castellano et al.  [2]  focused on emotion recognition task solely from the eye area. This approach was assessed using the masked FER-2013 dataset containing seven emotions. In  [3] , a deep network with a two-stage attention mechanism was proposed to tackle the obstacles of face masks in emotion recognition. The network could recognize three emotions: positive, negative, and neutral.\n\nPaper  [4]  proposed face expression recognition system based on vision transformers. Based on this strategy in the first step, a face parsing model has been trained to recognize better covered part of the face from the naked. In the next step, they established a vision transformer extractor for face emotion recognition and in the best situation, 62% accuracy was reported on the masked FER-2023 dataset.\n\nIn  [5] , a variety of training schemes have been evaluated to better understand the changes in arusal and valence dusing the masked facial emotion expression. The best results on the masked AffectNet dataset were obtained, with 53% and 45% accuracy for arousal and valence, respectively. Magherini et al.  [6]  utilized Resnet50 and InceptionResnet to classify five-class emotion recognition for masked images. Their system achieved 96.92% accuracy on AffectNet dataset. Dinca et al.  [7]  proposed an autoencoder artichecture of CNN to recognize positive and negative emotions, yielding 95.4% for the AFEW-VA dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. E Xperimental S Etup",
      "text": "Datasets are a crucial requirement in the design of pattern recognition systems. In this paper, we used two datasets, namely JAFFE and UIBVFED. We will introduce these datasets, their preprocessing algorithms, and the convolutional neural networks utilized in this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Uibvfed Dataset",
      "text": "The dataset used in this study comprises avatar images, featuring 20 virtual characters of 10 men and 10 women of varying skin colors and age groups ranging from 20 to 80 years old. The dataset only includes one type of mask and unbalanced images of the seven basic emotions  [8] . Sample images of the UIBVFED are depicted in Fig.  1 .\n\nThe dataset provides only 20 images for neutral and surprise, whereas happiness has 280 images and sadness has 120 images. The asymmetry in the dataset is due to some of the basic emotion categories consisting of compound emotions. As a result, two emotions, neutral and surprise, were removed to train the networks in a balanced manner. For the emotion of happiness, four compound emotions, namely \"AbashedSmile\", \"DebauchedSmile\", \"EagerSmile\", and \"SlySmile\", were removed. Similarly, for the sadness category, two compound emotions, \"CryingClosedMouth\" and \"Miserable\" were removed. To summarize, the dataset includes 80 images for each of the five basic emotions, including anger, disgust, fear, happiness and sadness.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Jaffe Dataset",
      "text": "The JAFFE dataset contains 213 images of 10 different Japanese female subjects. Each subject presents seven facial emotions, including neutral, fear, happiness, anger, disgust, sadness, and surprise. Each person expressed each emotion almost three times, and the images were annotated by 60 annotators  [9] . Fig.  2  presents sample images from this dataset. Note that the JAFFE dataset contains unmasked facial emotions, which require occlusion by the mask.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C.",
      "text": "Proposed Data Augmentation Fig.  3  depicts the CNN-based face emotion recognition under the mask coverage. Our proposed data augmentation method was applied to the JAFFE dataset. This method generates new training data using different types of masks to cover the face. In this situation, we have a wide variety of images to train the classifier. We inspired this method by adding noise and music to audio signals in speech processing scenarios as an augmentation method. As the JAFFE dataset includes backgrounds that are not required, and doesn't come with a mask, it's necessary to mask the face and crop the image to only the face part before applying classification techniques. We used the MaskTheFace algorithm  [10]  to artificially cover the face with four types of masks: surgical, cloth, N95, and KN95. Once the mask was applied, the MTCNN algorithm  [11] , a deep learning-based method for detecting faces within images, was employed to remove the unwanted, non-face background regions. Both MaskTheFace and MTCNN are well-known for their effectiveness in deep learning-based approaches for computer vision tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D.",
      "text": "Transfer Learning In transfer learning, a pre-trained network is adapted to a new scenario by adjusting the network's weights. CNNs typically consist of two parts: feature extraction and classification. In transfer learning, the last layers of the network that perform the classification are replaced with new layers to learn the features of the new classes. In this paper, we utilize four pre-trained CNNs: Alexnet  [12] , Squeezenet  [13] , Resnet50  [14] , and VGGFace2  [15] . Alexnet consisted of 25 layers and was trained on 1000 classes from the Imagenet dataset  [16] . Squeezenet is inspired by the Network in Network (NiN) architecture  [17]  and was also trained on the Imagenet dataset for 1000 classes. Resnet50 contains 50 layers of depth, about 25M learnable parameters, and proposed residual connections. VGGFace2 included 180 layers in the Resnet50 architecture and was trained on 3.31 million images from the VGGFace2 dataset for face recognition. We employ this pre-trained CNN for its strong background knowledge of facial features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E.",
      "text": "Evaluation Metrics There are various metrics available to evaluate a system, and selecting appropriate parameters would showcase the system's efficiency from different perspectives and allow for comparison with other works. The proposed evaluation parameters include precision, specificity, sensitivity, accuracy, and F1 score. These parameters are based on four indicators: True Positive (TP), which indicates correctly accepted, True Negative (TN), which expresses correctly rejected, False Positive (FP), which In addition to these parameters, the ROC curve is drawn, and the AUC is also calculated for all proposed systems. Changing the value of the decision threshold makes the ROC diagram, and the AUC expresses the area under the ROC curve. Moreover, the confusion matrix is reported for each system.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. E Xperimental R Esult",
      "text": "This study conducted evaluations using two methods: Person-Dependent (PD) and Person-Independent (PI). In the PD method, the identities of the individuals in the testing and training phases were the same, while in the PI method, they were different. Two datasets were used for the investigations, namely the JAFFE and UIBVFED datasets.\n\nFor the JAFFE dataset, three individuals with IDs KM, NM, and YM were selected for testing in the PI mode. In the PD section, the third session of each person was considered as the test file, while the other two performances were used for training.\n\nIn the case of the UIBVFED dataset, five identities -Alicia, Jose, Ramon, Tomeu, and Wanda -were used for testing in the PI mode, while the remaining identities were used for training.\n\nIn the PD mode, 75% of the data was used for training and 25% for testing.\n\nIn addition to our proposed data generation method to manipulate the training data with adding different mask to the face, two common data augmentation techniques were used: rotation operators within the range of  [-20, 20]  degrees, and vertical and horizontal translation by a distance of  [5, 5]  pixels. Moreover, we employed four types of masks for the JAFFE dataset, resulting in a total of 213x4 images. If an original image with one type of mask was used for training, that same image with the other three masks would not be included in the test set. Notably, the learning rate was set to 0.001 for all experiments, and the PD mode had an epoch number of 25 while the PI mode had 60.\n\nIn the first experiment, we trained Alexnet using only the surgery mask to evaluate the system efficiency without the proposed augmentation method. This evaluation was repeated for both PD and PI modes, and the results are presented in Table  I . Confusion matrices for both modes are shown in Fig.  4 , providing a detailed view of the network's performance for each class.  The system trained in PD mode achieves an accuracy of 85.8%, while the PI mode acquires an accuracy of 60.9%. The confusion matrix reveals that the SU and NE classes are responsible for most of the errors in the PI mode. Due to the reduced identity changes in test and training images in PD mode, the system performs better in this mode than in PI mode.\n\nIn the second experiment, we evaluated three networks -Alexnet, Squeezenet, and VGGFace2 -using four types of masks in both PD and PI modes. Tables (II) and (III) present the results of the proposed systems on the JAFFE dataset in PD and PI modes, respectively. We also showed the confusion matrices for all six trained networks in Fig.  5 . Based on the multi-mask mode results, the VGGFace2 network achieved the highest performance in PD mode with a 97.82% accuracy rate. The sensitivity, specificity, precision, F1 score, and AUC were 0.978, 0.996, 0.978, 0.978, and 0.999, respectively. However, in PI mode, these evaluation parameters experienced a significant drop, resulting in an accuracy of 74.21%. Further examination of the confusion matrixes in PI mode enables detailed performance assessment for each network. In comparison with one of the recent works  [6] , which reported 96.92% accuracy, our proposed system in the best situation achieved 0.9% outperform.  In this evaluation, the Resnet50 network showed the best performance. In this evaluation, the Resnet50 network showed the best performance. In PD mode, the accuracy percentage, sensitivity, specificity, precision, F1 score, and AUC were 73.68%, 0.73, 0.73, 0.93, 0.73, and 0.93, respectively. In PI mode, the accuracy percentage, sensitivity, specificity, precision, F1 score, and AUC were 59.57%, 0.59, 0.9, 0.59, 059, and 0.8, respectively. Confusion matrices provide more detailed information about the results. To our knowledge, only the article  [8]  mentioned the accuracy of 65% for the UIBVFED dataset without providing further details.\n\nBased on the results, networks performed differently in various scenarios. To better understand the efficiency and shortcomings of the CNNs, the Local Interpretable Model-Agnostic Explanations (LIME) technique was used to visualize the parts of the image that the CNNs focused on when making decisions  [19] . The visualization for the VGGFace2 network showed that in successful decisions, the network focused on the eyes and upper part of the face and correctly recognized the expected task. However, as shown in Fig.  7 , when the network focused on unimportant parts of the face, it made mistakes and was unable to recognize the emotion correctly.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. C Onclusion",
      "text": "In this study, we tackled the challenging task of recognizing facial emotions of individuals wearing masks using CNNs. Our approach utilized four different CNN architectures -Alexnet, Squeezenet, ResNet50 and VGGFace2 -and we evaluated our model on two datasets -JAFFE and UIBVFED. We addressed the issue of limited data availability by proposing an augmentation strategy that introduced four types of masks, increasing the diversity of images in the dataset.\n\nOur results demonstrate that VGGFace2 provided the best performance, achieving 97.82% accuracy for PD and 74.2% for PI modes in the JAFFE dataset. For UIBVFED, the Resnet50 achieved accuracy rates of 73.68% and 59.57% for PD and PI modes, respectively. Notably, our method achieved appropriate accuracy rates while facing the additional challenge of limited data availability.\n\nOur study contributes to the ongoing efforts in the field of AI and image processing, demonstrating the potential of CNNs to accurately recognize facial emotions even when individuals wear masks. We hope our proposed approach can serve as a baseline for further research. We have made the source code of our study available at  [20] , and we encourage other researchers to replicate our experiments and build upon our work.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The dataset provides only 20 images for neutral and surprise,",
      "page": 2
    },
    {
      "caption": "Figure 2: Image samples of the",
      "page": 2
    },
    {
      "caption": "Figure 2: presents sample images from this dataset.",
      "page": 2
    },
    {
      "caption": "Figure 3: depicts the CNN-based face emotion recognition under",
      "page": 2
    },
    {
      "caption": "Figure 3: Blockdiagram of CNN based system with proposed data augmentation",
      "page": 2
    },
    {
      "caption": "Figure 4: Confusion matrixes for single-mask in PD and PI modes",
      "page": 3
    },
    {
      "caption": "Figure 5: TABLE II.",
      "page": 3
    },
    {
      "caption": "Figure 5: Confusion matrixes for three networks in two scenarios",
      "page": 4
    },
    {
      "caption": "Figure 6: also provides the confusion matrixes.",
      "page": 4
    },
    {
      "caption": "Figure 7: Heatmap visualization of CNN feature-level using LIME",
      "page": 4
    },
    {
      "caption": "Figure 6: Confusion matrixes for three networks in two modes",
      "page": 4
    },
    {
      "caption": "Figure 7: , when the network focused on",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "1"
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "1",
          "1": "3",
          "2": "2"
        },
        {
          "6": "",
          "1": "",
          "2": "3"
        },
        {
          "6": "",
          "1": "",
          "2": "1"
        },
        {
          "6": "",
          "1": "",
          "2": "5"
        },
        {
          "6": "",
          "1": "",
          "2": ""
        },
        {
          "6": "",
          "1": "",
          "2": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "40": "2"
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "40": "DI\n2",
          "8": "6"
        },
        {
          "40": "",
          "8": "3"
        },
        {
          "40": "",
          "8": "5"
        },
        {
          "40": "",
          "8": "44"
        },
        {
          "40": "",
          "8": "1"
        },
        {
          "40": "",
          "8": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "40": "4"
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "40": "8",
          "8": ""
        },
        {
          "40": "",
          "8": "9"
        },
        {
          "40": "",
          "8": "7"
        },
        {
          "40": "",
          "8": "46"
        },
        {
          "40": "",
          "8": "5"
        },
        {
          "40": "",
          "8": "3"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "40": "4"
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        },
        {
          "40": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "32": "DI\n13",
          "4": ""
        },
        {
          "32": "",
          "4": "1"
        },
        {
          "32": "",
          "4": "1"
        },
        {
          "32": "",
          "4": "35"
        },
        {
          "32": "3",
          "4": ""
        },
        {
          "32": "",
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "37": "DI\n10",
          "1": "29",
          "10": ""
        },
        {
          "37": "",
          "1": "",
          "10": "1"
        },
        {
          "37": "1",
          "1": "",
          "10": "2"
        },
        {
          "37": "",
          "1": "",
          "10": "31"
        },
        {
          "37": "1",
          "1": "",
          "10": "2"
        },
        {
          "37": "",
          "1": "",
          "10": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "39": "",
          "1": "1"
        },
        {
          "39": "",
          "1": "38"
        },
        {
          "39": "",
          "1": ""
        },
        {
          "39": "",
          "1": ""
        },
        {
          "39": "",
          "1": ""
        },
        {
          "39": "",
          "1": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "DI\n1",
          "5": "7",
          "1": "2"
        },
        {
          "14": "3",
          "5": "",
          "1": "6"
        },
        {
          "14": "2",
          "5": "15",
          "1": "2"
        },
        {
          "14": "3",
          "5": "4",
          "1": "10"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Correct": "Incorrect"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "C Shorten",
        "T Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "Journal of big data"
    },
    {
      "citation_id": "2",
      "title": "Automatic emotion recognition from facial expressions when wearing a mask",
      "authors": [
        "G Castellano",
        "B Carolis",
        "N Macchiarulo"
      ],
      "year": "2021",
      "venue": "CHItaly 2021: 14th Biannual Conference of the Italian SIGCHI Chapter"
    },
    {
      "citation_id": "3",
      "title": "Face mask aware robust facial expression recognition during the COVID-19 pandemic",
      "authors": [
        "B Yang",
        "W Jianming",
        "G Hattori"
      ],
      "year": "2021",
      "venue": "2021 IEEE International conference on image processing (ICIP)"
    },
    {
      "citation_id": "4",
      "title": "Face-mask-aware facial expression recognition based on face parsing and vision transformer",
      "authors": [
        "B Yang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "5",
      "title": "I only have eyes for you: The impact of masks on convolutional-based facial expression recognition",
      "authors": [
        "P Barros",
        "A Sciutti"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in the times of COVID19: Coping with face masks",
      "authors": [
        "R Magherini",
        "E Mussi",
        "M Servi",
        "Y Volpe"
      ],
      "year": "2022",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "7",
      "title": "Unleashing the Transferability Power of Unsupervised Pre-Training for Emotion Recognition in Masked and Unmasked Facial Images",
      "authors": [
        "C Beyan",
        "R Niewiadomski",
        "S Barattin",
        "N Sebe"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "UIBVFED-Mask: A Dataset for Comparing Facial Expressions with and without Face Masks",
      "authors": [
        "M Mascaró-Oliver",
        "R Mas-Sansó",
        "E Amengual-Alcover",
        "M Roig-Maimó"
      ],
      "year": "2023",
      "venue": "Data"
    },
    {
      "citation_id": "9",
      "title": "The images are provided at no cost for non-commercial scientific research only",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "The images are provided at no cost for non-commercial scientific research only"
    },
    {
      "citation_id": "10",
      "title": "Masked face recognition for secure authentication",
      "authors": [
        "A Anwar",
        "A Raychowdhury"
      ],
      "year": "2020",
      "venue": "Masked face recognition for secure authentication",
      "arxiv": "arXiv:2008.11104"
    },
    {
      "citation_id": "11",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "12",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size",
      "authors": [
        "F Iandola",
        "S Han",
        "M Moskewicz",
        "K Ashraf",
        "W Dally",
        "K Keutzer"
      ],
      "year": "2016",
      "venue": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size",
      "arxiv": "arXiv:1602.07360"
    },
    {
      "citation_id": "14",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi"
      ],
      "venue": ""
    },
    {
      "citation_id": "16",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "17",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "18",
      "title": "Network in network",
      "authors": [
        "M Lin",
        "Q Chen",
        "S Yan"
      ],
      "year": "2013",
      "venue": "Network in network",
      "arxiv": "arXiv:1312.4400"
    },
    {
      "citation_id": "19",
      "title": "Evaluating learning algorithms: a classification perspective",
      "authors": [
        "N Japkowicz",
        "M Shah"
      ],
      "year": "2011",
      "venue": "Evaluating learning algorithms: a classification perspective"
    },
    {
      "citation_id": "20",
      "title": "Explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "21",
      "title": "Masked Facial Emotion Recognition",
      "authors": [
        "A Farhadipour"
      ],
      "year": "2023",
      "venue": "Masked Facial Emotion Recognition"
    }
  ]
}