{
  "paper_id": "2308.07145v1",
  "title": "Integrating Emotion Recognition With Speech Recognition And Speaker Diarisation For Conversations",
  "published": "2023-08-14T13:50:47Z",
  "authors": [
    "Wen Wu",
    "Chao Zhang",
    "Philip C. Woodland"
  ],
  "keywords": [
    "Automatic emotion recognition",
    "automatic speech recognition",
    "speaker diarisation",
    "foundation model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There has been much research work in automatic emotion recognition (AER)  [1, 2, 3, 4, 5] . However, most AER systems operate on manually segmented utterances although manual segmentations are not generally available in practical use cases. Besides, automatic speech recognition systems trained on standard speech can give poor recognition performance on emotional speech  [4, 6, 7] .\n\nThis work proposes an integrated system for emotion recognition, speaker diarisation and speech recognition. Speaker diarisation is the process that detects speech regions of an audio recording and groups them into homogeneous segments according to the relative identity of the speaker. The outcome of speaker diarisation is referred to as segmentation in this paper. As illustrated in Figure  1 , the system takes the audio recording of a dialogue as input. It automatically diarises the dialogue into segments associated with different speakers, transcribes the audio segments into text, and predicts the speaker's emotion state.\n\nThe model contains four downstream heads for voiceactivity-detection (VAD), speaker indentity (SI) extraction, automatic speech recognition (ASR), automatic emotion recognition and an encoder shared by all downstream heads. Speaker Wen Wu is supported by Cambridge International Scholarship from the Cambridge Trust. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service funded by EPSRC Tier-2 capital grant EP/T022159/1. 1 Code available: https://github.com/W-Wu/sTEER diarisation is achieved by the VAD head and the SI head. The VAD head classifies each frame into speech or non-speech. The SI head learns speaker embeddings that capture the characteristics of each speaker. Based on the predicted segmentation, the ASR head converts each speech segment into text and the AER head predicts the emotion states of the corresponding speaker.\n\nA speech foundation model  [8]  (i.e. WavLM  [9] ) is used as the shared encoder which takes raw speech waveform as input. Foundation models are pre-trained on large amount of unlabelled data and can help handle the data sparsity issue in AER tasks. The downstream heads take the weighted sum of intermediate hidden states from the shared encoder as input. Each head has an individual set of weights, which are trained jointly with the shared encoder and the downstream heads. Furthermore, this work also proposes two metrics to evaluate the emotion recognition performance with automatic segmentation: the time-weighted emotion error rate (TEER) and the speaker-attributed time-weighted emotion error rate (sTEER). To the best of our knowledge, this is the first work that considers emotion recognition with automatic segmentation and integrates emotion recognition, speech recognition and speaker diarisation into a jointly-trained model.\n\nThe rest of the paper is organised as follows. Section 2 summarises related work. Section 3 introduces the proposed system and the TEER and sTEER metrics. Section 4 presents the experimental setup while the results and analysis are shown in Sections 5 and 6 respectively, followed by the conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multi-task training and transfer learning have been investigated to improve the AER performance with ASR transcriptions. Feng et al.  [10]  used an attention-based encoder-decoder model for ASR and combined the states of the ASR decoder with the acoustic features for AER. Li et al.  [11]  encode the raw waveform with the Wav2Vec 2.0 model  [12]  for ASR and separately as MFCCs for AER. The ASR outputs were fused into the pipeline for joint training with AER. Zhou et al.  [13]  fine-tuned an ASR model on emotional speech for emotion recognition. Cai et al.  [14]  proposed a multi-task learning framework that fed the output of a Wav2Vec 2.0 model to an ASR head and an AER head each containing one fully-connected layer. Two heads were simultaneously trained while only the AER head was kept during inference. Heusser et al.  [15]  trained ASR from audio, AER from audio and from text independently and finetuned the combined sub-models. Ghriss et al.  [16]  pre-trained the AER model by ASR which is trained jointly with a sentiment classifier.\n\nApart from combining AER and ASR, Velichko et al.  [17]  proposed a hierarchical framework to predict gender, emotion,  and deception in a cascaded way. So far, combining speaker diarisation with emotion recognition and evaluating emotion recognition when using automatic segmentation hasn't been widely studied.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "The structure of the proposed system is shown in Figure  2 , which consists of a shared encoder, an interface and four downstream heads.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Shared Encoder And Interface",
      "text": "The WavLM Base model  [9]  is used as the shared encoder in this paper, which takes the raw waveform as input. It contains a convolutional neural network (CNN) block as the feature extractor and 12 Transformer  [18]  encoder blocks. The output of the encoder is a frame sequence with a frame shift of 20 ms.\n\nAll the semantic and non-semantic information co-exist in the same speech signal. Research  [9, 19, 20]  has shown that intermediate representations of such foundation models contain different levels of information. The weighted sum of embeddings from the CNN block and each Transformer encoder block is used as the input to the downstream tasks to exploit this property. Each downstream head has its own set of weights which are trainable.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Downstream Heads",
      "text": "The VAD head consists of 3 fully-connected (FC) layers with a hidden dimension of 256 and leaky ReLU activation and an output FC layer with Softmax activation which performs framelevel speech/non-speech detection. The SI head consists of an X-vector speaker embedding model  [21] , which generates one speaker embedding for each input sequence. The speaker embedding is fed into an FC layer with Softmax activation for speaker classification during training. During testing, spectral clustering is conducted based on speaker embeddings to produce speaker diarisation. The ASR head consists of 4 Bi-LSTM layers  [22]  with dimension of 256, followed by an FC layer for token prediction. A vocabulary of 29 graphemes is used including 26 letters in English plus a few punctuation characters. The AER head consists of 2 Transformer encoder layers of dimension 256. The representations are average-pooled along the time axis before being fed into an FC layer with Softmax activation for emotion classification. Six emotion classes are used: \"happy\", \"sad\", \"angry\", \"neutral\", \"other\", \"no majority agreement (NMA)\". \"NMA\" denotes that the human annotators don't have a majority agreed emotion class label for this utterance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Training Loss",
      "text": "The ASR head is trained using the CTC loss  [23]  and the other three heads were trained using the cross entropy loss. The shared encoder and four downstream heads are jointly trained using a multi-task loss:\n\nwhere ϵVAD, ϵSV, ϵASR, ϵAER are coefficients that are set manually to keep the weighted loss of the three head in the same scale.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training And Testing Procedures",
      "text": "Teacher forcing is applied during training using reference segmentations. The system takes segmented utterances as input.\n\nThe encoder and downstream heads are trained jointly using the multi-task loss defined in Eqn 1. The segmented utterances can contain silence at the beginning, between words and at the end. The VAD head is trained based on the intra-utterance silence.\n\nDuring testing, dialogues are input into the system. The system can benefit from knowing the context. A sliding window of 3 s length and 1 s overlap is applied to the dialogue. VAD is performed on each window. To avoid overlapped regions being counted twice at the output, the results of the middle second was kept for each window. This is equivalent to taking previous 1 s and future 1 s as context when making a prediction on the current 1 s of audio data. Post-processing is applied to the VAD predictions, so speech/non-speech regions shorter than 0.25 s are removed. A smaller sliding window of 1 s length and 0.5 s overlap is then applied to the detected speech regions. The SI head extracts a single speaker embedding from each window. Spectral clustering is used based on the speaker embeddings which groups segments from the same speaker together, thus producing an automatic segmentation. Based on the automatic segmentation, the ASR and AER heads takes each segment as input and predict the text and emotion respectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Evaluating Emotion Classification Performance With Automatic Segmentation",
      "text": "The segments predicted by the system can have different start and end times to the reference segments. Therefore, the classification accuracy is no longer sufficient to evaluate the performance of emotion classification in this case since it cannot handle the alignment between segments. This paper, therefore, proposes the time-weighted emotion error rate (TEER) in order to evaluate the AER performance given non-oracle segmentations. The TEER is computed as follows:\n\nwhere missed speech (MS) is the duration of speech incorrectly classified as non-speech, false alarm speech (FA) is the duration of non-speech incorrectly classified as speech, confusion (CONFemo) is the audio duration where emotion is wrongly classified, and TOTAL is the sum of the reference speech duration for all utterances. Furthermore, the speaker-attributed TEER (sTEER) is proposed which expects the system to accurately predict both the speaker and their emotion. The sTEER is computed as follows:\n\nCONFemo in Eqn 2 is replaced by CONFemo+spk, which is the duration where either speaker or emotion is wrong. sTEER reflects the overall performance of both speaker diarisation and emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4. Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4.1. Dataset",
      "text": "The benchmark IEMOCAP dataset  [24]  was used which consists of approximately 12 hours of English speech including 5 dyadic conversational sessions. There are in total 151 dialogues including 10,039 utterances. IEMOCAP provides the time-stamp of each utterance in a dialogue as well as wordlevel alignments of each utterance. The alignments show that 40% frames in the segmented utterances are silence. Each utterance was annotated by three human annotators. Sentences that don't have majority agreed emotion label from the annotators accounts for a 25% of the dataset. AER in this paper uses a six-way classification setup. Emotion class \"excited\" is merged with \"happy\". All sentences with emotion label other than \" happy\", \"sad\", \"angry\", \"neutral\" are grouped into class \"others\". Sentences that don't have a majority agreed emotion label from the annotators are grouped into the sixth class \"NMA\". Speaker exclusive leave-one-session-out five-fold cross validation (CV) are performed and the average results are reported. Speakers in the test set are unseen in the training and validation set and it is ensured that utterances from the same dialogue are either all in the training set or all in the validation set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "The false alarm rate (FAR) and missed rate (MSR) were used to evaluate the VAD performance. FAR computes the ratio of the number of non-speech frames mispredicted as speech to the total number of speech frames. MSR computes the ratio of the number of speech frames mispredicted as non-speech to the total number of speech frames. Diarisation error rate (DER) was used to evaluate the performance of diarisation which maps the predicted relative speaker identity to the true speaker identity and measures the fraction of time not attributed correctly to a speaker or to non-speech. Overlapped speech was considered when computing DER. Since manual annotations cannot be precise at the audio sample level, it is common to remove from evaluation a forgiveness collar around each segment boundary. Unless otherwise mentioned, a collar of 0.25 s was applied when evaluating with automatic segmentation.\n\nWord error rate (WER) and classification accuracy (Accemo) were used to evaluate the performance of speech recognition and emotion classification respectively with oracle segmentation. With automatic segmentation, the concatenated minimumpermutation word error rate (cpWER)  [25]  was used to evaluate the ASR system performance which concatenates utterances of the same speaker and computes the WER. The sTEER and TEER were used to evaluate the AER system which have been defined in Section 3.5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Specification",
      "text": "The shared encoder was initialised with the publicly available WavLM Base+ model 2  . It was fine-tuned jointly with the downstream head while the CNN feature extractor of the WavLM model was frozen during fine-tuning. Speed perturbation was applied to the ASR and SI heads. For each epoch, the speed of each waveform was randomly adjusted to 0.95 or 1.05 of the original speech or remain unchanged. Speed perturbation was not applied to AER since speed is an important clue for emotion detection. The model was implemented using the SpeechBrain toolkit  [26] . The system was trained using Adadelta optimiser with the Newbob learning rate scheduler. Scaling coefficients ϵVAD and ϵSI were set to 1.2 while the other two were set to 1. For each fold in 5-CV, training took around 5 hours on an NVIDIA A100 GPU and five checkpoints with the lowest validation loss were averaged after training for testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "The performance of the SI, ASR and AER heads were first evaluated with oracle segmentations in Section 5.1. The complete system was then evaluated with automatic segmentation in Section 5.2. The proposed system was compared to two baseline models:\n\n• \"Baseline-ref\": Reference models which have been trained\n\non other large datasets. The reference ASR model was pretrained using 100 hours of LibriSpeech  [27]  training data, which has a WER of 5.64% on \"test-clean\" set and 12.15% on \"test-other\" set. The reference speaker embedding model 3  was pre-trained on Voxceleb 1.0  [28] . A VAD module  [29]  pre-trained on the augmented multi-party interaction (AMI) meeting corpus  [30]  was used as the reference baseline for VAD, which has 2.1% FAR and 4.7% MSR on the AMI eval set. No reference model was used for AER since we are evaluating on the emotion dataset. The reference system is a cascade of the reference models in the order of VAD, SI, ASR. • \"Baseline-frzn\": The shared encoder was frozen during train-Table  1 : Results with reference segmentation. Collar was set to 0 when computing DER since oracle segmentation was assumed. '↑' denotes the higher the better, '↓' denotes the lower the better. Best results of each column are shown in bold. ing. In this case, the four downstream heads were independent of each other and the multi-task loss becomes equivalent to training each head separately.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "%Accemo↑ %Wer↓ %Der↓",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Oracle Segmentation",
      "text": "In this section, utterances based on the reference segmentation were used as input to the SI, ASR and AER heads. The results are shown in Table  1 . The ASR head with a frozen encoder reduced the WER on IEMOCAP to 31.43% and the SI head with frozen encoder reduced the DER to 0.40%. The proposed system with shared encoder jointly fine-tuned with downstream heads further reduced the WER and DER to 24.61% and 0.30% respectively. The 6-way emotion classification accuracy increased from 44.44% to 49.49%. The proposed integrated system outperforms the baselines for all three heads. Finetuning the pre-trained encoder on emotion data helps to adapt it to the specific domain, while sharing the encoder between the four downstream heads helps to capture general information relevant to the domain and avoids overfitting to trivial patterns, especially given the scarcity of data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Automatic Segmentation",
      "text": "The VAD performance and diarisation results based on VAD predictions are summarised in Table  2 . The proposed system produced the best results on both VAD and diarisation. ASR and AER were conducted using the diarisation output. As shown in Table  3 , the proposed integrated system reduced cpWER by relative 12% compared to the baselines. sTEER is slightly higher than TEER as it takes speaker prediction error into account. The proposed system outperforms the single AER head in both emotion metrics, showing its superior performance for emotion recognition with automatic segmentation.  6. Discussion and analysis",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Trainable Weights Of The Interface",
      "text": "The trainable weights for the four downstream heads are plotted in Figure  3 (a). As can be seen, layer 0 and layer 4 are particularly useful for extracting speaker information. Layer 8-10 are more effective for AER and layer 11 contains most text information. This shows a similar pattern to previous findings  [9, 19]  that block-wise evolution of intermediate representations of a foundation model follows an acoustic-linguistic hierarchy, where the lower layers encode speaker-related information and higher layers encode phonetic/semantic information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Confusion Matrix Of 6-Way Emotion Classification",
      "text": "Based on the 6-way predictions, 4-way classification accuracy considering \"happy\", \"sad\", \"angry\", \"neutral\" is 73.96%, which is better than the results of Wav2Vec 2.0 Base (63.43%) and WavLM Base+ (68.65%) from the SUPERB leaderboards  [20] . The class \"NMA\" is relatively easily confused, as shown by the 6-way confusion matrix of in Figure  3 (b). For utterances classified as \"NMA\", the human annotators gave different emotion class labels and didn't reach majority agreement. These utterances may contain ambiguous emotions, mixed emotions, or emotions that tend to confuse the annotators. Among the other five classes, \"angry\" is the least likely to be confused with \"NMA\" probably because \"angry\" is relatively less ambiguous. By contrast, \"neutral\" is more likely to be wrongly predicted as \"NMA\", possibly because neutral emotions are relatively weak and human annotators are likely to disagree due to subjective perception.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "This paper proposes a system that integrates emotion recognition with speech recognition and speaker diarisation in a jointlytrained model. The system investigates emotion recognition with automatic segmentation to address the issue of lacking manual segmentation in practical applications. The system also improves recognition performance on emotional speech by 12% reduction in relative word error rate with automatic segmentation. Time-weighted emotion error rate and speaker-attributed time-weighted emotion error rate were proposed to evaluate emotion classification performance when segmentation is nonoracle. Although the benchmark dataset used in this paper contains only dyadic conversations, the proposed method can also be applied to multi-party conversations.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the system takes the audio recording",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the proposed integrated system. Taking a dialogue as input, the VAD and SI head perform automatic segmenta-",
      "page": 2
    },
    {
      "caption": "Figure 2: Structure of the proposed integrated system.",
      "page": 2
    },
    {
      "caption": "Figure 3: (a) Weights of the interface for different downstream",
      "page": 4
    },
    {
      "caption": "Figure 3: (a). As can be seen, layer 0 and layer 4 are",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "cz277@tsinghua.edu.cn"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "diarisation is achieved by the VAD head and the SI head. The"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "VAD head classifies each frame into speech or non-speech. The"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "SI head learns speaker embeddings that capture the characteris-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "tics of each speaker. Based on the predicted segmentation,\nthe"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "ASR head converts each speech segment into text and the AER"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "head predicts the emotion states of the corresponding speaker."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "A speech foundation model\n[8]\n(i.e. WavLM [9])\nis used as"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "the shared encoder which takes\nraw speech waveform as\nin-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "put. Foundation models are pre-trained on large amount of un-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "labelled data and can help handle the data sparsity issue in AER"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "tasks. The downstream heads take the weighted sum of inter-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "mediate hidden states from the shared encoder as input. Each"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "head has an individual set of weights, which are trained jointly"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "with the shared encoder and the downstream heads."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "Furthermore, this work also proposes two metrics to evalu-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "ate the emotion recognition performance with automatic seg-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "mentation:\nthe\ntime-weighted\nemotion\nerror\nrate\n(TEER)"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "and the\nspeaker-attributed time-weighted emotion error\nrate"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "(sTEER). To the best of our knowledge,\nthis is the first work"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "that considers emotion recognition with automatic segmenta-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "tion and integrates emotion recognition, speech recognition and"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "speaker diarisation into a jointly-trained model."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "The rest of\nthe paper\nis organised as\nfollows.\nSection 2"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "summarises related work.\nSection 3 introduces the proposed"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "system and the TEER and sTEER metrics. Section 4 presents"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "the experimental setup while the results and analysis are shown"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "in Sections 5 and 6 respectively, followed by the conclusions."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "2. Related work"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "Multi-task\ntraining\nand\ntransfer\nlearning\nhave\nbeen\ninvesti-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "gated to improve the AER performance with ASR transcrip-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "tions. Feng et al. [10] used an attention-based encoder-decoder"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "model\nfor ASR and combined the states of\nthe ASR decoder"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "with the acoustic features for AER. Li et al. [11] encode the raw"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "waveform with the Wav2Vec 2.0 model [12] for ASR and sepa-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "rately as MFCCs for AER. The ASR outputs were fused into the"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "pipeline for joint training with AER. Zhou et al. [13] fine-tuned"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "an ASR model on emotional speech for emotion recognition."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "Cai et al.\n[14] proposed a multi-task learning framework that"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "fed the output of a Wav2Vec 2.0 model\nto an ASR head and"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "an AER head each containing one fully-connected layer. Two"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "heads were simultaneously trained while only the AER head"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "was kept during inference. Heusser et al. [15] trained ASR from"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "audio, AER from audio and from text\nindependently and fine-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "tuned the combined sub-models. Ghriss et al. [16] pre-trained"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "the AER model by ASR which is trained jointly with a senti-"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "ment classifier."
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": ""
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "Apart from combining AER and ASR, Velichko et al. [17]"
        },
        {
          "2Department of Electrical Engineering, Tsinghua University, Beijing, China": "proposed a hierarchical framework to predict gender, emotion,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker A\nSpeaker B\nYou did.": "too long.\n(ASR) head\n(SI)  head\nget in this line?"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Transformer \nencoder 1"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Interface\nh1\n⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "encoder 12\nh12\n⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Actually \nSpeech recognition \nWho told you to \n⋅⋅⋅\nEmotion recognition"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Transcription\nYou did."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Emotion\ntoo long.\nSad\nAngry\nNeutral\n(ASR) head\nget in this line?"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Transformer \n(AER) head"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "⋅⋅⋅\nWavLM encoder"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "encoder 12\nh12"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "⋅⋅⋅\nEmotion recognition"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Sad\nFigure 1: Overview of the proposed integrated system. Taking a dialogue as input, the VAD and SI head perform automatic segmenta-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "WavLM encoder"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "tion. The ASR and AER head recognise the text and emotion based on the predicted segments."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "WavLM\nCNN\nEncoder \n⋅⋅⋅\nEncoder \n⋅⋅⋅\nEncoder \n12\ni\n1\nand deception in a cascaded way.\nSo far, combining speaker"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "CNN\nEncoder \n⋅⋅⋅\nEncoder \n⋅⋅⋅\nEncoder \n12\ndiarisation with emotion recognition and evaluating emotion\ni\n1"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "WavLM\nrecognition when using automatic\nsegmentation hasn’t been"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "widely studied.\nh0\nh12"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "h0\nh12"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Interface\n⋅⋅⋅ ⋅⋅⋅\n3. Proposed approach"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Interface\nAER\nVAD\nASR\nSI\n⋅⋅⋅ ⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "[T, 768]\n[T, 768]"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "AER\nVAD\nASR\nVAD\nw0\nSI\nAER w12\nASR w12\nw0"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "VAD\nw0\nSI\nAER w12\nASR w12\nw0"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "w12\nw12\nw0\nw0\nThe structure of\nthe proposed system is\nshown in Figure 2,"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "w12\nw12\nw0\nw0"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "which consists of a shared encoder, an interface and four down-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "⋅⋅⋅\n⋅⋅⋅\n⋅⋅⋅\n⋅⋅⋅\n⋅⋅⋅\n⋅⋅⋅\n⋅⋅⋅"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Transformer\nTransformer"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "stream heads."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "BiLSTM*4\nencoder *2\nencoder *2\nX-vector \nX-vector"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "FC*4\nFC*4"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "model\nmodel"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Avg pooling"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Avg pooling"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "FC"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "3.1.\nShared encoder and interface\nFC"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "FC"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Speech /\nSpeaker \nFC"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Speech /\nSpeaker \nTranscription"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Non speech\nembedding \nEmotion\nThe WavLM Base model\n[9]\nis used as the shared encoder\nin"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Transcription"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Non speech\nembedding \nEmotion"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "this paper, which takes the raw waveform as input.\nIt contains\n[T, 2]"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Figure 2: Structure of the proposed integrated system."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "a convolutional neural network (CNN) block as the feature ex-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "tractor and 12 Transformer [18] encoder blocks. The output of"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "the encoder is a frame sequence with a frame shift of 20 ms.\n3.3. Multi-task training loss"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "All\nthe semantic and non-semantic information co-exist\nin"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "The ASR head is trained using the CTC loss [23] and the other"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "the same speech signal. Research [9, 19, 20] has shown that"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "three heads were\ntrained using the\ncross\nentropy loss.\nThe"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "intermediate representations of such foundation models contain"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "shared encoder and four downstream heads are jointly trained"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "different\nlevels of\ninformation. The weighted sum of embed-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "using a multi-task loss:"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "dings from the CNN block and each Transformer encoder block"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "(1)\nLTotal = ϵVADLVAD + ϵSILSI + ϵASRLASR + ϵAERLAER\nis used as the input to the downstream tasks to exploit this prop-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "erty. Each downstream head has its own set of weights which"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "where ϵVAD, ϵSV, ϵASR, ϵAER are coefficients that are set manually"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "are trainable."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "to keep the weighted loss of the three head in the same scale."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "3.2. Downstream heads"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "3.4. Training and testing procedures"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "Teacher forcing is applied during training using reference seg-\nThe VAD head consists of 3 fully-connected (FC) layers with"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "mentations.\nThe system takes segmented utterances as input.\na hidden dimension of 256 and leaky ReLU activation and an"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "The encoder and downstream heads are trained jointly using the\noutput FC layer with Softmax activation which performs frame-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "multi-task loss defined in Eqn 1. The segmented utterances can\nlevel speech/non-speech detection. The SI head consists of an"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "contain silence at the beginning, between words and at the end.\nX-vector speaker embedding model [21], which generates one"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "The VAD head is trained based on the intra-utterance silence.\nspeaker embedding for each input sequence. The speaker em-"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "During testing, dialogues are input\ninto the system.\nThe\nbedding is\nfed into an FC layer with Softmax activation for"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "system can benefit from knowing the context. A sliding window\nspeaker classification during training. During testing, spectral"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "clustering is conducted based on speaker embeddings to pro-\nof 3 s length and 1 s overlap is applied to the dialogue. VAD is"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "duce speaker diarisation. The ASR head consists of 4 Bi-LSTM\nperformed on each window. To avoid overlapped regions being"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "layers [22] with dimension of 256, followed by an FC layer for\ncounted twice at\nthe output,\nthe results of\nthe middle second"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "token prediction.\nA vocabulary of 29 graphemes\nis used in-\nwas kept for each window. This is equivalent to taking previous"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "cluding 26 letters in English plus a few punctuation characters.\n1 s and future 1 s as context when making a prediction on the"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "The AER head consists of 2 Transformer encoder layers of di-\ncurrent 1 s of audio data. Post-processing is applied to the VAD"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "mension 256. The representations are average-pooled along the\npredictions,\nso speech/non-speech regions shorter\nthan 0.25 s"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "time axis before being fed into an FC layer with Softmax acti-\nare removed. A smaller sliding window of 1 s length and 0.5 s"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "vation for emotion classification. Six emotion classes are used:\noverlap is then applied to the detected speech regions. The SI"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "“happy”, “sad”, “angry”, “neutral”, “other”, “no majority agree-\nhead extracts a single speaker embedding from each window."
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "ment (NMA)”. “NMA” denotes that the human annotators don’t\nSpectral clustering is used based on the speaker embeddings"
        },
        {
          "Speaker A\nSpeaker B\nYou did.": "which groups segments from the same speaker\ntogether,\nthus\nhave a majority agreed emotion class label for this utterance."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "time-stamp of each utterance in a dialogue as well as word-": ""
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "level alignments of each utterance. The alignments show that"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": ""
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "40% frames in the segmented utterances are silence. Each ut-"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": ""
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "terance was annotated by three human annotators.\nSentences"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": ""
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "that don’t have majority agreed emotion label from the annota-"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "tors accounts for a 25% of the dataset."
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "AER in this paper uses a six-way classification setup. Emo-"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "tion class “excited” is merged with “happy”. All sentences with"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "emotion label other than “ happy”, “sad”, “angry”, “neutral” are"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "grouped into class “others”. Sentences that don’t have a major-"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "ity agreed emotion label from the annotators are grouped into"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "the sixth class “NMA”. Speaker exclusive leave-one-session-out"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "five-fold cross validation (CV) are performed and the average"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "results are reported. Speakers in the test set are unseen in the"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "training and validation set and it is ensured that utterances from"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "the same dialogue are either all\nin the training set or all\nin the"
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "validation set."
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": ""
        },
        {
          "time-stamp of each utterance in a dialogue as well as word-": "4.2. Evaluation metrics"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "producing an automatic segmentation. Based on the automatic": "segmentation,\nthe ASR and AER heads takes each segment as",
          "the number of non-speech frames mispredicted as\nspeech to": "the total number of speech frames. MSR computes the ratio"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "input and predict the text and emotion respectively.",
          "the number of non-speech frames mispredicted as\nspeech to": "of the number of speech frames mispredicted as non-speech to"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "the total number of speech frames. Diarisation error rate (DER)"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "3.5. Evaluating\nemotion\nclassification\nperformance with",
          "the number of non-speech frames mispredicted as\nspeech to": "was used to evaluate the performance of diarisation which maps"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "automatic segmentation",
          "the number of non-speech frames mispredicted as\nspeech to": "the predicted relative speaker identity to the true speaker iden-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "tity and measures the fraction of\ntime not attributed correctly"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "The segments predicted by the system can have different start",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "to a speaker or\nto non-speech.\nOverlapped speech was con-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "and end times to the reference segments. Therefore,\nthe clas-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "sidered when computing DER. Since manual annotations can-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "sification accuracy is no longer sufficient\nto evaluate the per-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "not be precise at\nthe audio sample level,\nit\nis common to re-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "formance of emotion classification in this case since it cannot",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "move from evaluation a forgiveness collar around each segment"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "handle the alignment between segments. This paper,\ntherefore,",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "boundary. Unless otherwise mentioned, a collar of 0.25 s was"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "proposes the time-weighted emotion error rate (TEER) in order",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "applied when evaluating with automatic segmentation."
        },
        {
          "producing an automatic segmentation. Based on the automatic": "to evaluate the AER performance given non-oracle segmenta-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "Word error rate (WER) and classification accuracy (Accemo)"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "tions. The TEER is computed as follows:",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "were used to evaluate the performance of speech recognition"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "and emotion classification respectively with oracle segmenta-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "MS + FA + CONFemo",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "TEER =\n(2)",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "tion. With automatic segmentation, the concatenated minimum-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "TOTAL",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "permutation word error rate (cpWER) [25] was used to evalu-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "where missed speech (MS) is the duration of speech incorrectly",
          "the number of non-speech frames mispredicted as\nspeech to": "ate the ASR system performance which concatenates utterances"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "classified as non-speech,\nfalse alarm speech (FA)\nis the dura-",
          "the number of non-speech frames mispredicted as\nspeech to": "of the same speaker and computes the WER. The sTEER and"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "tion of non-speech incorrectly classified as speech, confusion",
          "the number of non-speech frames mispredicted as\nspeech to": "TEER were used to evaluate the AER system which have been"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "is\nthe\naudio duration where\nemotion is wrongly\n(CONFemo)",
          "the number of non-speech frames mispredicted as\nspeech to": "defined in Section 3.5."
        },
        {
          "producing an automatic segmentation. Based on the automatic": "classified, and TOTAL is the sum of\nthe reference speech du-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "ration for all utterances.",
          "the number of non-speech frames mispredicted as\nspeech to": "4.3. Training specification"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "Furthermore, the speaker-attributed TEER (sTEER) is pro-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "The shared encoder was initialised with the publicly available"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "posed which expects the system to accurately predict both the",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "WavLM Base+ model2. It was fine-tuned jointly with the down-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "speaker and their emotion. The sTEER is computed as follows:",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "stream head while the CNN feature extractor of\nthe WavLM"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "MS + FA + CONFemo+spk",
          "the number of non-speech frames mispredicted as\nspeech to": "model was frozen during fine-tuning.\nSpeed perturbation was"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "sTEER =\n(3)",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "applied to the ASR and SI heads.\nFor each epoch,\nthe speed"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "TOTAL",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "of each waveform was randomly adjusted to 0.95 or 1.05 of the"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "CONFemo\nin Eqn 2 is replaced by CONFemo+spk, which is the",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "original speech or remain unchanged. Speed perturbation was"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "duration where either speaker or emotion is wrong.\nsTEER re-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "not applied to AER since speed is an important clue for emotion"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "flects the overall performance of both speaker diarisation and",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "detection. The model was implemented using the SpeechBrain"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "emotion classification.",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "toolkit [26]. The system was trained using Adadelta optimiser"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "with the Newbob learning rate scheduler. Scaling coefficients"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "4. Experimental setup",
          "the number of non-speech frames mispredicted as\nspeech to": "to 1.2 while the other\ntwo were set\nto\nϵVAD and ϵSI were set"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "1.\nFor each fold in 5-CV,\ntraining took around 5 hours on an"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "4.1. Dataset",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "NVIDIA A100 GPU and five checkpoints with the lowest vali-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "The benchmark IEMOCAP dataset\n[24] was used which con-",
          "the number of non-speech frames mispredicted as\nspeech to": "dation loss were averaged after training for testing."
        },
        {
          "producing an automatic segmentation. Based on the automatic": "sists of approximately 12 hours of English speech including",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "5 dyadic conversational sessions.\nThere are in total 151 dia-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "5. Results"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "logues\nincluding 10,039 utterances.\nIEMOCAP provides\nthe",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "The performance of the SI, ASR and AER heads were first eval-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "time-stamp of each utterance in a dialogue as well as word-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "uated with oracle segmentations in Section 5.1. The complete"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "level alignments of each utterance. The alignments show that",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "system was then evaluated with automatic segmentation in Sec-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "40% frames in the segmented utterances are silence. Each ut-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "tion 5.2. The proposed system was compared to two baseline"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "terance was annotated by three human annotators.\nSentences",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "models:"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "that don’t have majority agreed emotion label from the annota-",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "tors accounts for a 25% of the dataset.",
          "the number of non-speech frames mispredicted as\nspeech to": "•\n“Baseline-ref”: Reference models which have been trained"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "AER in this paper uses a six-way classification setup. Emo-",
          "the number of non-speech frames mispredicted as\nspeech to": "on other large datasets. The reference ASR model was pre-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "tion class “excited” is merged with “happy”. All sentences with",
          "the number of non-speech frames mispredicted as\nspeech to": "trained using 100 hours of LibriSpeech [27]\ntraining data,"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "emotion label other than “ happy”, “sad”, “angry”, “neutral” are",
          "the number of non-speech frames mispredicted as\nspeech to": "which has a WER of 5.64% on “test-clean” set and 12.15%"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "grouped into class “others”. Sentences that don’t have a major-",
          "the number of non-speech frames mispredicted as\nspeech to": "on “test-other” set. The reference speaker embedding model3"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "ity agreed emotion label from the annotators are grouped into",
          "the number of non-speech frames mispredicted as\nspeech to": "was pre-trained on Voxceleb 1.0 [28]. A VAD module [29]"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "the sixth class “NMA”. Speaker exclusive leave-one-session-out",
          "the number of non-speech frames mispredicted as\nspeech to": "pre-trained on the augmented multi-party interaction (AMI)"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "five-fold cross validation (CV) are performed and the average",
          "the number of non-speech frames mispredicted as\nspeech to": "meeting corpus [30] was used as the reference baseline for"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "results are reported. Speakers in the test set are unseen in the",
          "the number of non-speech frames mispredicted as\nspeech to": "VAD, which has 2.1% FAR and 4.7% MSR on the AMI eval"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "training and validation set and it is ensured that utterances from",
          "the number of non-speech frames mispredicted as\nspeech to": "set. No reference model was used for AER since we are eval-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "the same dialogue are either all\nin the training set or all\nin the",
          "the number of non-speech frames mispredicted as\nspeech to": "uating on the emotion dataset. The reference system is a cas-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "validation set.",
          "the number of non-speech frames mispredicted as\nspeech to": "cade of the reference models in the order of VAD, SI, ASR."
        },
        {
          "producing an automatic segmentation. Based on the automatic": "",
          "the number of non-speech frames mispredicted as\nspeech to": "•\n“Baseline-frzn”: The shared encoder was frozen during train-"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "4.2. Evaluation metrics",
          "the number of non-speech frames mispredicted as\nspeech to": ""
        },
        {
          "producing an automatic segmentation. Based on the automatic": "The false alarm rate (FAR) and missed rate (MSR) were used",
          "the number of non-speech frames mispredicted as\nspeech to": "2https://huggingface.co/microsoft/wavlm-base-plus"
        },
        {
          "producing an automatic segmentation. Based on the automatic": "to evaluate the VAD performance. FAR computes the ratio of",
          "the number of non-speech frames mispredicted as\nspeech to": "3https://huggingface.co/microsoft/wavlm-base-plus-sv"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Results with reference segmentation. Collar was set",
      "data": [
        {
          "Table 2: VAD and speaker diarisation results.": "%FAR↓\n%MSR↓\n%DER↓",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "(a) Weights of\nthe interface for different downstream\nFigure 3:"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "Baseline-ref\n5.15\n1.30\n8.20",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "heads. (b) Confusion matrix of 6-way emotion recognition."
        },
        {
          "Table 2: VAD and speaker diarisation results.": "Baseline-frzn\n3.14\n1.16\n7.04",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "2.91\n1.06\n6.87\nProposed",
          "(a)\n(b)": "6. Discussion and analysis"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "6.1. Trainable weights of the interface"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "ing.\nIn this case,\nthe four downstream heads were indepen-",
          "(a)\n(b)": "The trainable weights for the four downstream heads are plot-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "dent of each other and the multi-task loss becomes equivalent",
          "(a)\n(b)": "ted in Figure 3(a).\nAs can be seen,\nlayer 0 and layer 4 are"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "to training each head separately.",
          "(a)\n(b)": "particularly useful\nfor extracting speaker\ninformation.\nLayer"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "8-10 are more effective for AER and layer 11 contains most"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "text information. This shows a similar pattern to previous find-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "5.1. Oracle segmentation",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "ings [9, 19] that block-wise evolution of intermediate represen-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "In this section, utterances based on the reference segmentation",
          "(a)\n(b)": "tations of a foundation model follows an acoustic-linguistic hi-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "were used as input\nto the SI, ASR and AER heads.\nThe re-",
          "(a)\n(b)": "erarchy, where the lower layers encode speaker-related informa-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "sults are shown in Table 1.\nThe ASR head with a frozen en-",
          "(a)\n(b)": "tion and higher layers encode phonetic/semantic information."
        },
        {
          "Table 2: VAD and speaker diarisation results.": "coder\nreduced the WER on IEMOCAP to 31.43% and the SI",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "head with frozen encoder reduced the DER to 0.40%. The pro-",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "6.2. Confusion matrix of 6-way emotion classification"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "posed system with shared encoder jointly fine-tuned with down-",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "stream heads further reduced the WER and DER to 24.61% and",
          "(a)\n(b)": "Based\non\nthe\n6-way\npredictions,\n4-way\nclassification\naccu-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "0.30% respectively. The 6-way emotion classification accuracy",
          "(a)\n(b)": "racy considering “happy”, “sad”, “angry”, “neutral” is 73.96%,"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "increased from 44.44% to 49.49%.\nThe proposed integrated",
          "(a)\n(b)": "which is better than the results of Wav2Vec 2.0 Base (63.43%)"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "system outperforms\nthe baselines\nfor all\nthree heads.\nFine-",
          "(a)\n(b)": "and WavLM Base+\n(68.65%)\nfrom the\nSUPERB leader-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "tuning the pre-trained encoder on emotion data helps to adapt",
          "(a)\n(b)": "boards\n[20].\nThe class “NMA” is\nrelatively easily confused,"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "it\nto the specific domain, while sharing the encoder between",
          "(a)\n(b)": "as shown by the 6-way confusion matrix of in Figure 3(b). For"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "the four downstream heads helps to capture general information",
          "(a)\n(b)": "utterances classified as “NMA”, the human annotators gave dif-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "relevant to the domain and avoids overfitting to trivial patterns,",
          "(a)\n(b)": "ferent emotion class\nlabels and didn’t\nreach majority agree-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "especially given the scarcity of data.",
          "(a)\n(b)": "ment.\nThese\nutterances may\ncontain\nambiguous\nemotions,"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "mixed emotions, or emotions that\ntend to confuse the annota-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "tors. Among the other five classes, “angry” is the least\nlikely"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "5.2. Automatic segmentation",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "to be confused with “NMA” probably because “angry” is rel-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "The VAD performance and diarisation results based on VAD",
          "(a)\n(b)": "atively less ambiguous. By contrast, “neutral” is more likely"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "predictions are summarised in Table 2.\nThe proposed system",
          "(a)\n(b)": "to be wrongly predicted as “NMA”, possibly because neutral"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "produced the best results on both VAD and diarisation.",
          "(a)\n(b)": "emotions are relatively weak and human annotators are likely"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "ASR and AER were conducted using the diarisation output.",
          "(a)\n(b)": "to disagree due to subjective perception."
        },
        {
          "Table 2: VAD and speaker diarisation results.": "As shown in Table 3,\nthe proposed integrated system reduced",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "cpWER by relative 12% compared to the baselines.\nsTEER is",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "7. Conclusions"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "slightly higher\nthan TEER as it\ntakes speaker prediction error",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "into account. The proposed system outperforms the single AER",
          "(a)\n(b)": "This paper proposes a system that\nintegrates emotion recogni-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "head in both emotion metrics, showing its superior performance",
          "(a)\n(b)": "tion with speech recognition and speaker diarisation in a jointly-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "for emotion recognition with automatic segmentation.",
          "(a)\n(b)": "trained model.\nThe system investigates emotion recognition"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "with automatic segmentation to address\nthe issue of\nlacking"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "manual segmentation in practical applications. The system also"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "Table 3: Speaker-attributed ASR and AER performance under",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "improves recognition performance on emotional speech by 12%"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "automatic segmentation. Best results shown in bold.",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "reduction in relative word error rate with automatic segmenta-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "tion. Time-weighted emotion error rate and speaker-attributed"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "%cpWER↓\n%sTEER↓\n%TEER↓",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "time-weighted emotion error\nrate were proposed to evaluate"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "Baseline-ref\n43.76\n/\n/",
          "(a)\n(b)": "emotion classification performance when segmentation is non-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "Baseline-frzn\n41.19\n69.49\n68.70",
          "(a)\n(b)": "oracle. Although the benchmark dataset used in this paper con-"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "tains only dyadic conversations,\nthe proposed method can also"
        },
        {
          "Table 2: VAD and speaker diarisation results.": "36.20\n66.03\n65.17\nProposed",
          "(a)\n(b)": ""
        },
        {
          "Table 2: VAD and speaker diarisation results.": "",
          "(a)\n(b)": "be applied to multi-party conversations."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Y\n. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,"
        },
        {
          "8. References": "[1] Y. Kim, H. Lee, and E. M. Provost, “Deep learning for\nrobust",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "W.-C. Tseng, K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W."
        },
        {
          "8. References": "feature generation in audiovisual emotion recognition,” in Proc.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech"
        },
        {
          "8. References": "ICASSP, Vancouver, Canada, 2013.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Processing Universal PERformance Benchmark,” in Proc. Inter-"
        },
        {
          "8. References": "[2]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "speech, Brno, Czech, 2021."
        },
        {
          "8. References": "and A. Hussain, “Multimodal sentiment analysis: Addressing key",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[21] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-"
        },
        {
          "8. References": "issues and setting up the baselines,”\nIEEE Intelligent Systems,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "pur, “X-vectors: Robust DNN embeddings for speaker\nrecogni-"
        },
        {
          "8. References": "vol. 33, no. 6, pp. 17–25, 2018.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "tion,” in Proc. ICASSP, Hyderabad, India, 2018."
        },
        {
          "8. References": "[3]\nP. Liu, K. Li, and H. Meng, “Group gated fusion on attention-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[22] M. Schuster and K. Paliwal, “Bidirectional recurrent neural net-"
        },
        {
          "8. References": "based bidirectional alignment\nfor multimodal emotion recogni-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "works,”\nIEEE Transactions on Signal Processing, vol. 45, pp."
        },
        {
          "8. References": "tion,” in Proc. Interspeech, Shanghai, China, 2020.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "2673 – 2681, 12 1997."
        },
        {
          "8. References": "[4] W. Wu, C. Zhang, and P. C. Woodland, “Emotion recognition by",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[23] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-"
        },
        {
          "8. References": "fusing time synchronous and time asynchronous representations,”",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "nectionist\ntemporal\nclassification:\nlabelling\nunsegmented\nse-"
        },
        {
          "8. References": "in Proc. ICASSP, Toronto, Canada, 2021.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "quence data with recurrent neural networks,” in Proc. ICML, Pitts-"
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "burgh, US, 2006."
        },
        {
          "8. References": "[5] W. Wu, C. Zhang, and P. C. Woodland, “Distribution-based emo-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "tion recognition in conversation,”\nin Proc. SLT, Doha, Qatar,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[24] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Provost,"
        },
        {
          "8. References": "2022.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "S. Kim,\nJ. Chang, S. Lee, and S. Narayanan, “IEMOCAP:\nIn-"
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "teractive emotional dyadic motion capture database,” Language"
        },
        {
          "8. References": "[6]\nS. Sahu, V. Mitra, N. Seneviratne, and C. Y. Espy-Wilson, “Multi-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Resources and Evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "8. References": "modal\nlearning for speech emotion recognition: An analysis and",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[25]\nS. Watanabe, M. Mandel,\nJ. Barker,\nE. Vincent, A. Arora,"
        },
        {
          "8. References": "comparison of\nasr outputs with ground truth transcription.”\nin",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj,\nand"
        },
        {
          "8. References": "Proc. Interspeech, Graz, Austria, 2019.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "D. Snyder, “CHiME-6 challenge: Tackling multispeaker speech"
        },
        {
          "8. References": "[7] R. Fernandez, “A computational model for the automatic recogni-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "recognition for unsegmented recordings,” in Proc. CHiME, San"
        },
        {
          "8. References": "tion of affect\nin speech,” Ph.D. dissertation, Massachusetts Insti-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Francisco, US, 2020."
        },
        {
          "8. References": "tute of Technology, 2004.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[26] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell,"
        },
        {
          "8. References": "[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, and"
        },
        {
          "8. References": "S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "J. Chou, “SpeechBrain: A general-purpose speech toolkit,” arXiv"
        },
        {
          "8. References": "et al.,\n“On the opportunities and risks of\nfoundation models,”",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "preprint arXiv:2106.04624, 2021."
        },
        {
          "8. References": "arXiv preprint arXiv:2108.07258, 2021.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[27] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-"
        },
        {
          "8. References": "[9]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "riSpeech: An ASR corpus based on public domain audio books,”"
        },
        {
          "8. References": "N. Kanda, T. Yoshioka, X. Xiao et al., “WavLM: Large-scale self-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "in Proc. ICASSP, South Brisbane, Australia, 2015."
        },
        {
          "8. References": "supervised pre-training for\nfull\nstack speech processing,” IEEE",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[28] A. Nagrani,\nJ.\nS.\nChung,\nand A.\nZisserman,\n“VoxCeleb:"
        },
        {
          "8. References": "Journal of Selected Topics in Signal Processing, 2022.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "arXiv\npreprint\na\nlarge-scale\nspeaker\nidentification\ndataset,”"
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "arXiv:1706.08612, 2017."
        },
        {
          "8. References": "[10] H. Feng, S. Ueno, and T. Kawahara, “End-to-end speech emotion",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "recognition combined with acoustic-to-word asr model.” in Proc.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[29] G. Sun, C. Zhang, and P. C. Woodland, “Combination of deep"
        },
        {
          "8. References": "Interspeech, Shanghai, China, 2020.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "speaker embeddings for diarisation,” Neural Networks, vol. 141,"
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "pp. 372–384, 2021."
        },
        {
          "8. References": "[11] Y. Li, P. Bell, and C. Lai, “Fusing ASR outputs in joint training for",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "speech emotion recognition,” in Proc. ICASSP, Singapore, 2022.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "[30]\nJ. Carletta,\nS. Ashby,\nS. Bourban, M. Flynn, M. Guillemot,"
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, and"
        },
        {
          "8. References": "[12] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2Vec 2.0:",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "G. Lathoud, “The AMI meeting corpus: A pre-announcement,” in"
        },
        {
          "8. References": "A framework for self-supervised learning of speech representa-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "Proc. Machine Learning for Multimodal Interaction, Edinburgh,"
        },
        {
          "8. References": "tions,” in Proc. NeurIPS, Conference held virtually, 2020.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": "UK, 2005."
        },
        {
          "8. References": "[13]\nS. Zhou and H. Beigi,\n“A transfer\nlearning method for\nspeech",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "emotion recognition from automatic speech recognition,” arXiv",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "preprint arXiv:2008.02863, 2020.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[14] X. Cai,\nJ. Yuan, R. Zheng, L. Huang, and K. Church, “Speech",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "Emotion Recognition with Multi-Task Learning,” in Proc. Inter-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "speech, Brno, Czech, 2021.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[15] V. Heusser, N. Freymuth, S. Constantin, and A. Waibel, “Bimodal",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "speech emotion recognition using pre-trained language models,”",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "arXiv preprint arXiv:1912.02610, 2019.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[16] A. Ghriss, B. Yang, V. Rozgic,\nE. Shriberg,\nand C. Wang,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "“Sentiment-aware automatic speech recognition pre-training for",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "enhanced speech emotion recognition,” in Proc. ICASSP, Singa-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "pore, 2022.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[17] A. Velichko, M. Markitantov, H. Kaya, A. Karpov et al., “Com-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "plex paralinguistic analysis of speech:\nPredicting gender, emo-",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "Inter-\ntions and deception in a hierarchical\nframework,” Proc.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "speech, 2022.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "in Proc. NeurIPS, Long Beach, US, 2017.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "[19] A. Pasad,\nJ.-C. Chou, and K. Livescu, “Layer-wise analysis of",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "a self-supervised speech representation model,” in Proc. ASRU,",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        },
        {
          "8. References": "Cartagena, Colombia, 2021.",
          "[20]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "4",
      "title": "Group gated fusion on attentionbased bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Distribution-based emotion recognition in conversation",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2022",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "7",
      "title": "Multimodal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "A computational model for the automatic recognition of affect in speech",
      "authors": [
        "R Fernandez"
      ],
      "year": "2004",
      "venue": "A computational model for the automatic recognition of affect in speech"
    },
    {
      "citation_id": "9",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "10",
      "title": "WavLM: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "End-to-end speech emotion recognition combined with acoustic-to-word asr model",
      "authors": [
        "H Feng",
        "S Ueno",
        "T Kawahara"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Wav2Vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS, Conference held virtually"
    },
    {
      "citation_id": "14",
      "title": "A transfer learning method for speech emotion recognition from automatic speech recognition",
      "authors": [
        "S Zhou",
        "H Beigi"
      ],
      "year": "2020",
      "venue": "A transfer learning method for speech emotion recognition from automatic speech recognition",
      "arxiv": "arXiv:2008.02863"
    },
    {
      "citation_id": "15",
      "title": "Speech Emotion Recognition with Multi-Task Learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng",
        "L Huang",
        "K Church"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "17",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "A Ghriss",
        "B Yang",
        "V Rozgic",
        "E Shriberg",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP, Singapore"
    },
    {
      "citation_id": "18",
      "title": "Complex paralinguistic analysis of speech: Predicting gender, emotions and deception in a hierarchical framework",
      "authors": [
        "A Velichko",
        "M Markitantov",
        "H Kaya",
        "A Karpov"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "20",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2021",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "24",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "26",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "27",
      "title": "CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings",
      "authors": [
        "S Watanabe",
        "M Mandel",
        "J Barker",
        "E Vincent",
        "A Arora",
        "X Chang",
        "S Khudanpur",
        "V Manohar",
        "D Povey",
        "D Raj",
        "D Snyder"
      ],
      "year": "2020",
      "venue": "Proc. CHiME"
    },
    {
      "citation_id": "28",
      "title": "SpeechBrain: A general-purpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga",
        "A Rouhe",
        "S Cornell",
        "L Lugosch",
        "C Subakan",
        "N Dawalatabad",
        "A Heba",
        "J Zhong",
        "J Chou"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "29",
      "title": "Lib-riSpeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP, South"
    },
    {
      "citation_id": "30",
      "title": "VoxCeleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "VoxCeleb: a large-scale speaker identification dataset",
      "arxiv": "arXiv:1706.08612"
    },
    {
      "citation_id": "31",
      "title": "Combination of deep speaker embeddings for diarisation",
      "authors": [
        "G Sun",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "32",
      "title": "The AMI meeting corpus: A pre-announcement",
      "authors": [
        "J Carletta",
        "S Ashby",
        "S Bourban",
        "M Flynn",
        "M Guillemot",
        "T Hain",
        "J Kadlec",
        "V Karaiskos",
        "W Kraaij",
        "M Kronenthal",
        "G Lathoud"
      ],
      "year": "2005",
      "venue": "Proc. Machine Learning for Multimodal Interaction"
    }
  ]
}