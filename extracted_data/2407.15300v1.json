{
  "paper_id": "2407.15300v1",
  "title": "Selm: Enhancing Speech Emotion Recognition For Out-Of-Domain Scenarios",
  "published": "2024-07-22T00:01:20Z",
  "authors": [
    "Hazim Bukhari",
    "Soham Deshmukh",
    "Hira Dhamyal",
    "Bhiksha Raj",
    "Rita Singh"
  ],
  "keywords": [
    "speech emotion recognition",
    "out-of-distribution",
    "audio language models",
    "few-shot learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) has been traditionally formulated as a classification task. However, emotions are generally a spectrum whose distribution varies from situation to situation leading to poor Out-of-Domain (OOD) performance. We take inspiration from statistical formulation of Automatic Speech Recognition (ASR) and formulate the SER task as generating the most likely sequence of text tokens to infer emotion. The formulation breaks SER into predicting acoustic model features weighted by language model prediction. As an instance of this approach, we present SELM, an audio-conditioned language model for SER that predicts different emotion views. We train SELM on curated speech emotion corpus and test it on three OOD datasets (RAVDESS, CREMAD, IEMOCAP) not used in training. SELM achieves significant improvements over the stateof-the-art baselines, with 17% and 7% relative accuracy gains for RAVDESS and CREMA-D, respectively. Moreover, SELM can further boost its performance by Few-Shot Learning using a few annotated examples. The results highlight the effectiveness of our SER formulation, especially to improve performance in OOD scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding emotions from spoken language helps humans in decision-making and social interaction. Speech Emotion Recognition (SER) aims to enable machines to detect emotions from human speech. The emotions are commonly represented as either categorical emotions (Happy, Angry, Sad), as Sentiments (Positive, Neutral, Negative), or as continuous values of valence, arousal, and dominance ratings. Automatically detecting emotions from speech has multiple applications in various domains such as interactive voice assistants, caller-agent conversation analysis, health care, education, and entertainment.\n\nLiterature on SER formulates it as a classification task  [1] . The model architecture consists of pretrained speech encoder like Wav2vec2  [2] , HuBERT  [3] , and Whisper  [4]  followed by a classifier. The pretrained speech encoder is usually completely or partially frozen and the classifier is trained from scratch on the target data. This model architecture performs well on in-domain data  [5] . However, the performance drops drastically on out-ofdomain data. Moreover, the learned classifier is static and only predicts predefined emotions and hence needs to be retrained for every domain and target task. The recent works in audio-text learning  [6, 7, 8, 9, 10]  aim to avoid training classifiers for each target task and domain. To remove the fixed classifier stage, audio-text models use cosine similarity between audio embeddings and classes to determine predictions where the classes are represented by text embeddings. The popular instances of such approaches like CLAP  [7, 11, 12] , Pengi  [6] , LTU  [8, 9]  allow users to define emotion classes to predict at test-time. Moreover, as emotion is a spectrum, flexible class prediction enables users to define emotion as \"happy but little sad\" and \"initially happy tone but overall sad\" or \"happy initially and sad later\". Though audio-text approaches are trained on large-scale audio data, they still cannot generalize to OOD scenarios, where the speech data differs from the training data in terms of speakers, languages, accents, recording conditions, or emotional categories. For example, Happy emotion in call-center has a significantly different energy than Happy in business meetings, leading to misclassifications. This reduces the applicability of audio-text models for SER in-the-wild.\n\nFew-Shot Learning (FSL) approaches utilize a few annotated examples from the target domain to adapt the pre-trained model. This allows SER models to be adapted to different domains like call-center, and health care with few examples and minimal updates. Two popular FSL approaches for SER are LanSER  [13]  and AudioFlamingo  [14] . LaNSER utilises a pre-trained Large Language Model (LLM) through weakly-supervised learning and then uses 10% of data to improve OOD performance. Similarly, AudioFlamingo takes the approach of building a strong foundation audio model that can be later adapted to OOD dataset using as less as 4-shots. However, there exists a gap in in-domain performance and the OOD performance of AudioFlamingo and LanSER. Moreover, LaNSER requires large amount of annotated audio data to improve OOD performance. For example, 10% for CREMAD  [15]  is 744 audio files, which is costly to annotate.\n\nIn this work, we provide an alternate formulation for SER and propose SELM, a speech emotion recognition model that improves OOD performance and is adaptable to new test distribution with minimal or few-annotated examples. SELM uses an audio-conditioned language modeling approach for SER where the prediction of different emotion views is formulated as a text generation task. Our contributions are as follows: (1) An alternate formulation of the task of SER which breaks SER into Acoustic Model and Language Model. (2) Introduce SELM which is an instance of the above formulation and provides SoTA performance in Out-of-Domain scenarios. (3) Propose Few-Shot Learning approach for SELM (4) Extensively testing SELM in various setups: In-domain, OOD, and Few-Shot Learning on three public datasets (RAVDESS, CREMA-D and IEMOCAP), thus establishing a baseline for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Formulation",
      "text": "An ASR system produces the most likely word sequence for a speech signal. The statistical approach to ASR has been: where x represents the audio, w represents a word sequence and the optimal word decoding is w * . The p(w) is modeled by the language model and p(x|w) is calculated by the acoustic model. Traditionally speech emotion recognition has been formulated as a classification problem. However, one can formulate speech emotion recognition as generating emotion tokens (e), conditioned on audio, where the emotion tokens are actually text tokens describing emotion. Taking inspiration from the ASR formulation, we provide the statistical formulation for speech emotion recognition as :",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We realize the formulation (Section 2) as audio-conditioned language modeling for Speech Emotion Recognition. In this section, we describe SELM (Figure  1 ), which takes an audio recording and prompt as input and produces text as output.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Architecture",
      "text": "Audio encoder. The audio encoder extracts dense representations from audio. We use Wav2Vec2 as the choice of audio encoder due to its near SOTA performance for SER  [5] . The 4th layer of Wav2Vec2 contains the information relevant to the task of SER  [5] . Therefore, we extract the 4th layer of Wav2vec2 as our frozen audio feature extractor.\n\nText Embedder. The text embedder consists of a Language Model tokenizer followed by embedding lookup to convert language model tokens to continuous embeddings. We keep the tokenizer and embedding lookup same as the Language Model used, which in our case is GPT2  [16]  Audio Projection. The audio projection consists of two learnable linear layers with GeLU activation in between. This is similar to CLAP projection  [7] , and performs a non-linear transformation of the 4th layer of Wav2Vec2 hidden state. This transformed representation is passed to the Audio Mapper. Audio Mapper. The audio mapper consists of a sequence mapper and a transformer layer. The sequence mapper is a linear The Language Model generates a text (emotion of happy) conditioned on this latent prefix. We use GPT2  [16]  as the choice of Language Model and a prefix size of 10 for audio and text respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training",
      "text": "The model is trained using the next-token prediction objective.\n\nThen the Language Model conditioned on zi produces a sequence of output tokens ot i . The model is trained to predict the next text tokens ot i (t ∈ [0, l]) conditioned on zi in an autoregressive fashion. The loss function is Cross-Entropy:\n\nwhere θ denotes the model's trainable parameters which consist of Audio Projection, Audio Mapper, and Text Mapper. We keep the Wav2Vec2, Text embedder, and Language Model frozen.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Inference",
      "text": "The test audio and test input prompt are used to build prefix zi to prompt the Language Model. The Language Model predicts the next token based on beam search decoding with a beam size of 3. The output of the model is a text and can be directly used in any downstream pipeline For evaluating metrics, the free-form text answer from SELM has to be converted to 1 out of C classes for each dataset or user-specified classes. For example, SELM generates the emotion as frustration but the user wants classification into 4 classes of Happy, Sad, Angry, Neutral. Therefore, the generated text needs to be mapped into one of the C classes. For this, we encode the generated text and classes with CLAPS text encoder. Then use cosine similarity between text embedding of generated text and text embedding of classes to determine the most likely class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4. Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4.1. Datasets",
      "text": "Training Datasets: The training dataset for SELM consists of triplets of form (audio, prompt, output). Training on all emotion views is proven to improve performance  [17] . Hence, we train SELM to predict all three views of emotions: categorical, sentiment, and dimensional. For categorical data, we source audio and labels from MSP Podcast  [18] , CMU MOSI  [19] , CMU MOSEI  [20] , MELD  [21]  and use \"This person is\" as the prompt. For sentiment data, we convert categorical labels from MSP Podcast and MELD to sentiment with \"This sentiment is\" as a prompt. While, for dimensional scores, we utilize the labels available from the MSP Podcast and prompt with \"Describe emotion parameters\". In total, we train on 315k triplets. Evaluation Datasets: We use three datasets for evaluating SER performance: RAVDESS  [22] , CREMAD  [15] , IEMOCAP  [23] . These datasets differ in the number of emotional classes, the type of features (audio-only vs. multimodal), and their specific data collection setup (Table  1 ). The above datasets are not used in training SELM. This makes the three datasets ideal for simulating OOD setup and for benchmarking.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Setup",
      "text": "In-Domain. To check the in-distribution performance of SELM we use the in-domain setup. In this setup, the model is trained using the training subset of the Evaluation Datasets and evaluated on the testing subset of them.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "Unweighted accuracy is used as the primary evaluation metric. The experiments in this section are designed to test SELM in different scenarios, namely: In-domain (Section 5.1), Out-of-Domain (Section 5.2), Few-Shot (Section 5.3). The Section 5.4 contains ablation studies to determine which parameters to update for Few-Shot Learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "In-Domain",
      "text": "In this Section, we compare our formulation of SER with the traditional classification-based SER. In SELM, the audio encoder is Wav2Vec and is frozen i.e. not learned during training. We compare against literature baselines where the Wav2Vec2 is frozen followed by learning single or multiple linear layers. This experiment is called In-Domain setup as the model is trained on the train subset of the dataset followed by testing on the test subset. The results are shown in Table  2  for three datasets. We present SELM performance in the first row, and compare it to the methodology where Wav2Vec2 features are used followed by a simple neural network on top. We believe this is a fair comparison as the acoustic model architecture is the same in both models. SELM's better performance shows the benefit of decomposing the SER task into an Acoustic Model followed by a Language Model reweighting.\n\nIn Table  2 : In-domain performance of SELM on three datasets. Similar to SELM, the benchmark numbers also use wav2vec2base embeddings to extract acoustic features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Out-Of-Domain",
      "text": "In this Section, we compare SELM's Out-of-Domain (OOD) performance against benchmark models from Literature. The existing literature models are also not trained on these three datasets, making this a valid OOD comparison. Table  3  presents the OOD experimental results for RAVDESS, CREAM-D and IEMOCAP. The first half of the table presents performance on all emotion classes in the dataset, while the lower half (denoted with *) presents numbers on a subset of classes. The subset of classes are chosen to be the four primary emotions: happy, sad, angry, neutral. The results show SELM outperforms existing models on all three datasets for both all-class and 4-class setup. Moreover, some of the datasets contain emotions that SELM has not seen during training, and hence showcasing the generalization ability of the model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Few-Shot Learning",
      "text": "In this Section, we compare adaptation  [29]  like Few-Shot learning performance of SELM against literature benchmarks. The results are shown in  . The Au-dioFlamingo reports 4-shot and 8-shot numbers, however the testing strategy and split is not reported. For the 4 class emotion classification, we compare againts LanSER  [13] , which reports the performance when the model is fine-tuned using 10% of the data. We note that 10% data from target domain is significantly more than 4-shot. For example, 10% of data from CREMA-D is equivalent to 744 audio files and hence the setups are not directly comparable. Due to the lack of Few-Shot Learning approaches in SER, we compare SELM against LanSER The results shown in Table  4  lead to multiple conclusions. First, SELM performs better than prior work on CREMA-D and IEMOCAP. Second, 8-shot always leads to performance improvement over 4 shot. Similarly, 16-shot performs better than 8-shots, i.e. providing SELM more target domain data will lead to better performance. However, there SELM Few-Shot has some limitations. First, the AudioFlamingo performs better on RAVDESS in all-class settings than SELM. We believe that this is due to the specific training strategy of AudioFlamingo which enables use of In-Context Learning. Moreover, AudioFlamingo is trained on 6 million instances, which is 20 times higher than our settings. Second, in RAVDESS, the 4-shot leads to higher performance than 8-shot in the 4-class settings. This might be because RAVDESS has song and speech samples, while SELM has never encountered songs in training data. As songs have different audio distribution from speech samples, the parameter-efficient Few-Shot learning is not sufficient to improve performance and adapt to the song distribution in a few examples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation On Few-Shot Learning",
      "text": "To understand which parameter of SELM to update for Few-Shot Learning, we perform multiple ablation studies. The parameters under consideration are: linear layer that is part of the decoder (AL-Dec) or the linear layer part of the encoder (AL-Enc), or the Audio Mapper Transformer (AT) or the Text Mapper Transformer (TT). The results of the ablation study are shown in Figure  2 . We find that finetuning Text mapper from SELM leads to the best performance improvement when data distribution is similar. However, for completely different data distribution, adapting a parameter in audio mapper performs better. For example, RAVDESS contains songs which SELM has not seen in training data. To adapt to a widely different audio distribution,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Limitations",
      "text": "SELM achieves SoTA Out-of-Domain performance. Moreover, by using Few-Shot Learning, the OOD performance can be further improved. However, SELM has limitations. First, SELM is trained only on spoken English datasets and hence its speech emotion recognition will drop on other languages. Second, the Few-Shot Learning improves SELM's performance on OOD data. However, the improvements are minor when the audio distribution is significantly different.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we provide an alternate formulation for Speech Emotion Recognition, posing it as a text token generation task, consisting of both acoustic and language models. We build SELM based on this formulation and test on multiple SER datasets. The results show that SELM generalizes better than the literature models on Out-of-Domain and Few-Shot scenarios. In Out-of-Domain settings, SELM achieves 17% and 7% relative accuracy improvements in RAVDESS and CREAMA-D respectively. In Few-Shot setting (4-shot and 8-shot), on average SELM achieves better performance over baseline models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), which takes an audio",
      "page": 2
    },
    {
      "caption": "Figure 1: SELM: Speech Emotion Language Model. The model",
      "page": 2
    },
    {
      "caption": "Figure 2: We find that finetuning Text mapper from SELM leads",
      "page": 4
    },
    {
      "caption": "Figure 2: Ablation study on the parameters to update for Few-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Few-Shot Learning results of different models and",
      "data": [
        {
          "Dataset for Few-Shot Learning": "RAVD ↑\nCREMA ↑\nIEMOC ↑"
        },
        {
          "Dataset for Few-Shot Learning": "-\n30.47\n-\n35.20\n31.80\n-\n30.09\n30.10\n25.32\n32.27\n27.01\n31.81"
        },
        {
          "Dataset for Few-Shot Learning": "-\n35.50\n42.00\n-\n43.70\n50.00\n57.14\n43.93\n50.17\n46.02\n50.17\n55.77"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Few-Shot Learning results of different models and",
      "data": [
        {
          "Out-of-Domain Dataset": "RAVD ↑\nCREMA ↑\nIEMOC ↑"
        },
        {
          "Out-of-Domain Dataset": "12.50\n16.70\n-\n16.00\n17.80\n13.71\n20.32\n18.46\n-\n13.50\n17.20\n-\n15.30\n20.90\n-\n16.70\n19.90\n-\n15.10\n20.20\n-\n20.90\n26.50\n-\n24.51\n28.30\n21.42"
        },
        {
          "Out-of-Domain Dataset": "29.48\n31.05\n33.72\n-\n15.90\n30.90\n-\n23.50\n34.30\n38.46\n35.22\n-\n52.53\n42.79\n40.02"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Positional encoding for capturing modality specific cadence for emotion detection",
      "authors": [
        "H Dhamyal",
        "B Raj",
        "R Singh"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "6",
      "title": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "authors": [
        "A Saliba",
        "Y Li",
        "R Sanabria",
        "C Lai"
      ],
      "year": "2024",
      "venue": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "arxiv": "arXiv:2402.02617"
    },
    {
      "citation_id": "7",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "S Deshmukh",
        "B Elizalde",
        "R Singh",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Listen, think, and understand",
      "authors": [
        "Y Gong",
        "H Luo",
        "A Liu",
        "L Karlinsky",
        "J Glass"
      ],
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "Joint audio and speech understanding",
      "authors": [
        "Y Gong",
        "A Liu",
        "H Luo",
        "L Karlinsky",
        "J Glass"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "11",
      "title": "Audio Retrieval with WavText5K and CLAP Training",
      "authors": [
        "S Deshmukh",
        "B Elizalde",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "12",
      "title": "Natural language supervision for general-purpose audio representations",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "H Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang",
        "Y Hui",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Lanser: Language-model supported speech emotion recognition",
      "authors": [
        "T Gong",
        "J Belanich",
        "K Somandepalli",
        "A Nagrani",
        "B Eoff",
        "B Jou"
      ],
      "year": "2023",
      "venue": "Lanser: Language-model supported speech emotion recognition",
      "arxiv": "arXiv:2309.03978"
    },
    {
      "citation_id": "15",
      "title": "Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities",
      "authors": [
        "Z Kong",
        "A Goel",
        "R Badlani",
        "W Ping",
        "R Valle",
        "B Catanzaro"
      ],
      "year": "2024",
      "venue": "Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities"
    },
    {
      "citation_id": "16",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "18",
      "title": "Multi-view learning for speech emotion recognition with categorical emotion, categorical sentiment, and dimensional scores",
      "authors": [
        "D Tompkins",
        "D Emmanouilidou",
        "S Deshmukh",
        "B Elizalde"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "21",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "C Luna-Jiménez",
        "R Kleinlein",
        "D Griol",
        "Z Callejas",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "26",
      "title": "A comparative study of pre-trained speech and audio embeddings for speech emotion recognition",
      "authors": [
        "O Phukan",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2023",
      "venue": "A comparative study of pre-trained speech and audio embeddings for speech emotion recognition",
      "arxiv": "arXiv:2304.11472"
    },
    {
      "citation_id": "27",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Investigating the emergent audio classification ability of asr foundation models",
      "authors": [
        "R Ma",
        "A Liusie",
        "M Gales",
        "K Knill"
      ],
      "year": "2024",
      "venue": "Investigating the emergent audio classification ability of asr foundation models"
    },
    {
      "citation_id": "29",
      "title": "Describing emotions with acoustic property prompts for speech emotion recognition",
      "authors": [
        "H Dhamyal",
        "B Elizalde",
        "S Deshmukh",
        "H Wang",
        "B Raj",
        "R Singh"
      ],
      "year": "2022",
      "venue": "Describing emotions with acoustic property prompts for speech emotion recognition",
      "arxiv": "arXiv:2211.07737"
    },
    {
      "citation_id": "30",
      "title": "Domain adaptation for contrastive audio-language models",
      "authors": [
        "S Deshmukh",
        "R Singh",
        "B Raj"
      ],
      "year": "2024",
      "venue": "Domain adaptation for contrastive audio-language models",
      "arxiv": "arXiv:2402.09585"
    }
  ]
}