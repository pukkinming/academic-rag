{
  "paper_id": "2010.09895v4",
  "title": "Multi-Window Data Augmentation Approach For Speech Emotion Recognition",
  "published": "2020-10-19T22:15:03Z",
  "authors": [
    "Sarala Padi",
    "Dinesh Manocha",
    "Ram D. Sriram"
  ],
  "keywords": [
    "IEMOCAP",
    "Speech emotion recognition (SER)",
    "Speech augmentation",
    "Windowing effect",
    "RAVDESS",
    "SAVEE"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present a Multi-Window Data Augmentation (MWA-SER) approach for speech emotion recognition. MWA-SER is a unimodal approach that focuses on two key concepts; designing the speech augmentation method and building the deep learning model to recognize the underlying emotion of an audio signal. Our proposed multi-window augmentation approach generates additional data samples from the speech signal by employing multiple window sizes in the audio feature extraction process. We show that our augmentation method, combined with a deep learning model, improves speech emotion recognition performance. We evaluate the performance of our approach on three benchmark datasets: IEMOCAP, SAVEE, and RAVDESS. We show that the multi-window model improves the SER performance and outperforms a single-window model. The notion of finding the best window size is an essential step in audio feature extraction. We perform extensive experimental evaluations to find the best window choice and explore the windowing effect for SER analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In human-computer interactions, the emotion of humans plays a crucial role. It also plays a significant role in psychological and nano-physiological studies of human emotional expression, automatic tutoring systems, customer services, call center services, gaming, personal assistants  [1] . Thus, there is an increasing demand for automated methods to understand and recognize human emotions  [2, 3] . In real life, there are many ways to express human feelings. Speech is considered an easy and effective communication mechanism to convey human emotions  [4] . However, automatic speech emotion recognition (SER) is a challenging task  [5] , and machine learning models built for such analysis mainly depend on the features extracted from the speech signal. Most of the previous studies on SER obtain spectral and prosody-based features by selecting an appropriate window size  [6, 7, 8, 9, 10] . However, it is crucial to choose an optimal window size, and it depends purely on the type of input signal  [11, 12] . The features extracted from speech signals vary across humans because of speaking style, native language speaker status, speaking speed, language usage, context, location. Also, there are significant intercultural differences in understanding human emotions  [13] . Thus, it is hard to build speaker-independent models and quantify emotions for a given sentence.\n\nIn recent years, deep learning (DL) models have shown great success in recognizing the emotions from the manually obtained features or by directly extracting the speech characteristics from the raw waveform or spectrograms  [6, 14, 15, 16, 17, 18, 19, 9, 20, 8, 7, 21, 18, 22] . However, building such complex models requires large amounts of data to learn the millions of parameters. Also, overfitting is one of the challenges in building such models. In overcoming this issue, there are some techniques incorporated during training, such as making use of dropout layers, adding regularizers, normalizing each layer input by adding batch normalization, incorporating transfer learning techniques, and making use of pre-trained models  [23] . In contrast to these techniques, data augmentation is another approach that overcomes model overfitting by providing more generalized data during training. However, there are few methods explored, such as vocal tract length perturbations (VTLP), signal-based transformations like time stretch, pitch shift, and adding noise to the original utterance, Generative Adversarial Networks (GAN), and CycleGAN augmentation approaches for SER tasks  [24, 25, 19, 25, 26, 27] . The main disadvantage of signal-based transformations is that models tend to overfit due to similar samples in the training set, while random balance removes possibly valuable information. The downside with GAN and CycleGAN models is that the feature vectors generated from these models dependent on data used during training and may not generalize to other datasets. The other big concern with these models is that they are hard to train and optimize.\n\nWe propose a Multi-window Data Augmentation (MWA-SER) approach to address one of the challenges (overfitting) in building the DL models for SER analysis. The proposed method generates more data samples from the speech signal by employing multiple window sizes in the audio feature extraction process. The advantage of our approach is twofold: by applying the multi-window approach, we can obtain features considering smaller and longer utterances, which play a vital role in emotion analysis, and 2) address the lack of data to build a complex deep learning model by generating additional data samples from the speech signal. To show the benefit of our proposed approach, we consider three benchmark datasets: IEMOCAP, SAVEE, and RAVDESS, and show that our proposed strategy improves the emotion recognition performance and outperforms the singlewindow model. When we work with a multi-window approach, finding the best window size is an essential step in audio feature extraction. We also extend our study to explore the windowing effect and to find the best window choice for speech emotion recognition analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mwa-Ser: Methodology",
      "text": "The proposed methodology consists of two stages: 1) a multiwindow data augmentation and 2) a Convolutional Neural Network (CNN) model. The first stage generates additional data samples to augment the CNN model by employing multiple window sizes in the audio feature extraction process. In the second stage, the CNN model uses the extracted features for emotion analysis. As shown in Figure  1 , in the audio feature extraction process, a 25 ms window has densely placed filters, and a 200 ms window has filters that are farther apart. We can also see that windows are placed at different scales to extract features from smaller and longer utterances. A single-window",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Stage 1: Multi-Window Approach",
      "text": "In speech processing, features extracted using a windowed signal which is typically called a frame. In general, an audio signal is stationary for a short period, and the width of the windowing function has a direct impact on the feature values extracted from the speech signal  [28] . For example, if the selected window length 1 is too small, then we may not have enough data samples to get a reliable spectral estimate of the signal; if it is too large, then the signal changes too much throughout the frame. So, selecting the width of a windowing function is a crucial step, and it is hard to choose when no background information about the input signal is known  [11, 12] . Studies have shown that an optimal window size selection increases the correlation between the acoustic representation and human perception of a speech signal  [29, 30] .\n\nA window function is represented by three tuples: window width (milliseconds), window offset or overlap (milliseconds), and window shape. To extract part of a signal, we multiply the value of the signal at time \"t\", signal[t], with the value of the window (hamming or rectangular) at time \"t\", window[t] represented as:\n\nWindowed signal (frame) is used to compute features for emotion analysis. For SER, a typical window of size 25 ms is used to extract features with an overlap of 10 ms window  [6, 7, 8, 9] . On the other hand, other studies have shown that a bigger window size increases the emotion recognition performance  [31, 32] . There are other studies in evaluating the importance of step size (overlap window size), but a single-window is used for SER analysis  [7, 31] . Tarantino et al. demonstrated the effect of overlap window size for SER and showed that a small step size leads to a reduced test loss. Chernykh et al. experimented with different window sizes ranging from 30 ms to 200 ms and 1 The number of samples considered for short-time analysis selected a single window of size 200 ms for SER analysis. To the best of our knowledge, our approach is the first method to generate additional data samples to augment the DL models by employing multiple window sizes in the audio feature extraction process along with a CNN model for SER analysis.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Stage 2: Convolutional Neural Network Model (Cnn)",
      "text": "Motivated by the CNN model, which outperformed ML models for the SER task  [33] , we build a CNN model to recognize the underlying emotion of the speech signal. We use four convolutional layers in the CNN model with two fully-connected (FC) layers followed by a softmax layer. The number of kernels used in each of the four convolutional layers is 32, 64, 128, 256 followed by 128 and 32 hidden neurons at the FC layers.\n\nTo mitigate the overfitting issue, we use dropout layers with p = 0.23. Furthermore, after each convolutional layer, we used batch normalization and max-pooling layers to subsample the feature dimensions. We use kernels of different sizes to learn the representative features to a larger context. The typical kernel sizes that we use are 7 × 7, 5 × 5, 3 × 3, and 1 × 1. During training, we augment the CNN model by generating additional samples using multiple windows. All the extracted features are fed to the CNN model to learn the most representative features for SER analysis. Finally, the softmax layer is applied, to the vector generated by the FC layer, to predict the emotion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Features:",
      "text": "As detailed in  [31, 32] , before feature extraction, the speech signal is preprocessed with methods such as DC removal and then normalized with the maximum value of the signal. We extract 34-dimensional features:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets:",
      "text": "To evaluate and compare our proposed methodology with the baseline, we conduct the experiments on three benchmark datasets.\n\nIEMOCAP  [34] : Interactive emotional dyadic motion capture (IEMOCAP) dataset contains improvised and scripted multimodal dyadic conversations between actors of the opposite gender. It consists of 12 hours of speech data from 10 subjects. It includes nine categorical emotions and 3-dimensional labels. We consider categorical emotions, where at least two experts agree in the annotation. We conduct two experiments: 1) Exp 1 uses four classes of emotions: \"angry\", \"happy\", \"neutral\", \"sad\", and 2) Exp 2 uses the same categories as in Exp 1, replacing the \"happy\" emotion with \"excited\". RAVDESS  [35]  : The Ryerson Audio-Visual Data of Emotional Speech and Song (RAVDESS) is a standard scripted speech dataset containing eight categories of emotions. This dataset contains 60 spoken sentences and 40 sung sentences from 24 actors (12 male, 12 female). We consider only speech sentences from 24 actors for emotion analysis. We merged \"neutral\" and \"calm\" emotions into a single class called \"neutral\". We consider six categories of emotions:\"angry\", \"happy\", \"neutral\", \"sad\", \"fear\", and \"disgust\" for SER analysis.\n\nSAVEE  [36]  : The Surrey Audio-Visual Expressed Emotion (SAVEE) dataset has 120 spoken sentences from 4 native English male actors. We consider six categories of emotions:\"angry\", \"happy\", \"neutral\", \"sad\", \"fear\", and \"disgust\" for SER analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup And Training Details:",
      "text": "We use the scipy library 2 to extract speech-based features and the Keras deep learning framework with TensorFlow 2.0 as the backend to build the CNN model. We train the CNN model with an RMSprop optimizer to minimize the categorical crossentropy loss with a 0.0004 learning rate. We ran the model for 1000 epochs with a batch size of 32. In our evaluations, we use 80% of the data for training and 20% of data for testing purposes because performing cross-validation on deep learning models with varying window sizes is not feasible in terms of time and computational requirements. We present our findings by reporting Unweighted Accuracy (UA), Weighted Average Precision (WAP), and Weighted Average F1 (WAF1) measures.\n\nIn the experiments, the window sizes that we considered to evaluate the performance of single-window and multi-window approaches are 25 ms, 50 ms, 100 ms, and 200 ms with a 50% overlap. In the multi-window approach, we train the DL model on the data samples generated by multiple-window sizes. For example, in a three-window model with 25 ms, 50 ms and 100 ms window sizes, we used all three windows to generate 2 www.scipy.org data for training, and while testing, we considered the window that gives the best accuracy against the test data.\n\nTable  1 : Performance comparison between a single-window model and a multi-window methods on the IEMOCAP, SAVEE, and RAVDESS datasets. The single-window model with 25 ms window size is a baseline. For the multi-window approach, we reported the best window model performances. Abbreviations: A-Angry, H-Happy, N-Neutral, E-Excited.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results:",
      "text": "Table  1  shows an SER performance comparison of singlewindow and multi-window methods on the IEMOCAP, SAVEE and RAVDESS datasets. For the IEMOCAP dataset (Exp 1), our proposed MWA-SER approach improved the emotion recognition accuracy with an improvement of 6% accuracy, 9% in WAP, and 7% in WAF1 measure. We also extended our analysis by replacing \"happy\" emotion with \"excited\" and showed that the proposed model outperformed a single-window model with an improvement of 6% accuracy, a 7% in WAP, and a 5% in WAF1, respectively. We also investigate the performance of our proposed augmentation approach on two other benchmark datasets: SAVEE and RAVDESS, and Table  1  compare the performance of single-window and multi-window models on these two datasets. For the SAVEE dataset, there is an improvement of 14% accuracy, 7% in WAP, and 12% in WAF1 score. For the RAVDESS dataset, there is an improvement of 2% accuracy, WAP, and WAF1 scores.\n\nFrom experimental analysis, we observed that tuning CNN model parameters are hard to classify the \"happy\" class examples than the other three categories of emotions. The potential reason is some examples belonging to the \"happy\" category include overlapped speech. Further, the duration of the \"happy\" category examples is small compared to other emotional classes. It may not be accurate for all the examples of the \"happy\" emotion in the dataset.\n\nTo further analyze the misclassification error within and across emotional categories, we show the confusion matrices for the three benchmark datasets. Figure  2 (a) and (b) show the confusion matrices for IEMOCAP dataset (Exp 1 and 2). The \"happy\" and \"excited\" classes of emotions got confused with the \"neutral\" class of emotion and got the best accuracy for the other three categories of emotions. Similarly, for the SAVEE and RAVDESS datasets (Figure  2 (c) and (d)), \"sad\" and \"neutral\" emotions got confused and in fact these are the most confusable emotions across three datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Windowing Effect:",
      "text": "To further study the importance of window size, we extended our analysis by varying the window size and increasing the data to augment the DL model for SER analysis. Table  2  shows the performance comparison of a single-window and multi- window models. As shown in Table  2 , for IEMOCAP dataset the three-window model with window sizes 50, 100, 200 ms for Exp 1 and 25, 50, 100 ms for Exp 2 performs better than single-window model. It indicates that extraction of audio features at multiple scales is useful for emotion analysis. However, the optimal window size that we use in our experimental study may vary depending on the dataset because our methodology purely depends on the features extracted from the speech signal. These extracted features vary widely across datasets based on the type of audio signal, duration of an audio signal, etc.\n\nFor instance, we extended our experiments on two other benchmark datasets: SAVEE and RAVDESS. As shown in Table  2 , the SAVEE dataset, the two-window model with 100 ms and 200 ms window sizes, and for the RAVDESS dataset, a four window model with 25 ms, 50 ms, 100 ms, and 200 ms window sizes improved the SER performance. The performance of the proposed model and the optimal window choice depends on: the type of dataset, type of features extracted, type of emotions considered for analysis, window size used to compute the speech-based features, and the model architecture used for the SER analysis. However, this general idea can be adapted to speech processing applications when limited data are available to train complex DL models.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "We presented a multi-window data augmentation approach to improving speech emotion recognition performance. We addressed the overfitting issue of the deep learning model by generating additional data samples during training. We evaluated the performance of the proposed approach on three benchmark datasets. We showed that our proposed method improved the emotion recognition performance and consistently outperformed the single-window model across datasets. We also showed that window size plays a crucial role in speech emotion recognition, and the best window choice varies across datasets.\n\nIn the future, to further improve the emotion recognition performance, we plan to extend the multi-window approach with other augmentation techniques such as spectrogram and GAN-based methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Disclaimer:",
      "text": "The views and conclusions presented in this paper are those of the authors and should not be interpreted as the official findings, either expressed or implied, of NIST or the U.S. Government.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , in the audio feature",
      "page": 1
    },
    {
      "caption": "Figure 1: A multi-window data augmentation combined with a deep learning model for SER analysis. Wfs indicates ﬁxed window",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) and (b) show the",
      "page": 3
    },
    {
      "caption": "Figure 2: (c) and (d)), “sad” and “neu-",
      "page": 3
    },
    {
      "caption": "Figure 2: Confusion matrix of the proposed MWA-SER approach evaluated for IEMOCAP (Exp 1 and 2), SAVEE, and RAVDESS",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance comparison between a single-window",
      "data": [
        {
          "Dataset": "IEMOCAP: A, H, S, N\n(Exp 1)",
          "Model": "Single-window\nMWA-SER",
          "UA (%)": "60\n65",
          "WAP (%)": "65\n73",
          "WAF1 (%)": "61\n68"
        },
        {
          "Dataset": "IEMOCAP: A, E, S, N\n(Exp 2)",
          "Model": "Single window\nMWA-SER",
          "UA (%)": "60\n66",
          "WAP (%)": "64\n68",
          "WAF1 (%)": "61\n66"
        },
        {
          "Dataset": "SAVEE",
          "Model": "Single-window\nMWA-SER",
          "UA (%)": "56\n70",
          "WAP (%)": "67\n74",
          "WAF1 (%)": "59\n71"
        },
        {
          "Dataset": "RAVDESS",
          "Model": "Single-window\nMWA-SER",
          "UA (%)": "86\n88",
          "WAP (%)": "86\n88",
          "WAF1 (%)": "86\n88"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "",
          "Multi-window": "Two window"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "25(w1)",
          "Multi-window": "w12"
        },
        {
          "Dataset": "IEMOCAP (Exp 1)",
          "Metric": "Accuracy\nWAP\nWAF1",
          "Single window": "60",
          "Multi-window": "64"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "65",
          "Multi-window": "70"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "61",
          "Multi-window": "66"
        },
        {
          "Dataset": "IEMOCAP (Exp 2)",
          "Metric": "Accuracy\nWAP\nWAF1",
          "Single window": "60",
          "Multi-window": "65"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "64",
          "Multi-window": "67"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "61",
          "Multi-window": "66"
        },
        {
          "Dataset": "SAVEE",
          "Metric": "Accuracy\nWAP\nWAF1",
          "Single window": "56",
          "Multi-window": "64"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "67",
          "Multi-window": "69"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "59",
          "Multi-window": "65"
        },
        {
          "Dataset": "RAVDNESS",
          "Metric": "Accuracy\nWAP\nWAF1",
          "Single window": "86",
          "Multi-window": "84"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "86",
          "Multi-window": "86"
        },
        {
          "Dataset": "",
          "Metric": "",
          "Single window": "86",
          "Multi-window": "84"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wrobel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "Being Human: Human-Computer Interaction in The Year",
      "authors": [
        "H Richard",
        "R Tom",
        "R Yvonne",
        "S Abigail"
      ],
      "year": "2008",
      "venue": "Being Human: Human-Computer Interaction in The Year"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "6",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "8",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "10",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "arxiv": "arXiv:1706.00612"
    },
    {
      "citation_id": "11",
      "title": "Audiovisual emotion recognition in wild",
      "authors": [
        "E Avots",
        "T Sapinski",
        "M Bachmann",
        "D Kaminska"
      ],
      "year": "2018",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "12",
      "title": "Introduction to digital speech processing",
      "authors": [
        "L Rabiner",
        "R Schafer"
      ],
      "year": "2007",
      "venue": "Introduction to digital speech processing"
    },
    {
      "citation_id": "13",
      "title": "Windowed attention mechanisms for speech recognition",
      "authors": [
        "S Zhang",
        "E Loweimi",
        "P Bell",
        "S Renals"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "The influence of language and culture on the understanding of vocal emotions",
      "authors": [
        "R Altrov",
        "H Pajupuu"
      ],
      "year": "2015",
      "venue": "Journal of Estonian and Finno-Ugric Linguistics"
    },
    {
      "citation_id": "15",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Sixteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2018-1625"
    },
    {
      "citation_id": "18",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "E Kim",
        "J Shin"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "20",
      "title": "Emotion identification from raw speech signals using DNNs",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "21",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "22",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "23",
      "title": "Convolutional RNN: an enhanced model for extracting features from sequential data",
      "authors": [
        "G Keren",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. IEEE International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "24",
      "title": "A survey of transfer learning",
      "authors": [
        "K Weiss",
        "T Khoshgoftaar",
        "D Wang"
      ],
      "year": "2016",
      "venue": "Journal of Big data"
    },
    {
      "citation_id": "25",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "26",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "27",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition"
    },
    {
      "citation_id": "28",
      "title": "Modeling feature representations for affective speech using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "The effects of windowing on the calculation of mfccs for different types of speech sounds",
      "authors": [
        "A Kelly",
        "C Gobl"
      ],
      "year": "2011",
      "venue": "International Conference on Nonlinear Speech Processing"
    },
    {
      "citation_id": "30",
      "title": "An efficient adaptive window size selection method for improving spectrogram visualization",
      "authors": [
        "S Nisar",
        "O Khan",
        "M Tariq"
      ],
      "year": "2016",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "31",
      "title": "A comparison of spectral continuity measures as a join cost in concatenative speech synthesis",
      "authors": [
        "B Kirkpatrick",
        "D O'brien",
        "R Scaife"
      ],
      "year": "2006",
      "venue": "A comparison of spectral continuity measures as a join cost in concatenative speech synthesis"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "V Chernykh",
        "P Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "33",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "T Sarthak",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "35",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "36",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "37",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2011",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    }
  ]
}