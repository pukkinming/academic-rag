{
  "paper_id": "2211.05163v4",
  "title": "Multimodal Dyadic Impression Recognition Via Listener Adaptive Cross-Domain Fusion",
  "published": "2022-11-09T19:23:00Z",
  "authors": [
    "Yuanchao Li",
    "Peter Bell",
    "Catherine Lai"
  ],
  "keywords": [
    "dyadic interaction",
    "impression recognition",
    "listener adaptation",
    "cross-domain fusion",
    "multitask learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As a sub-branch of affective computing, impression recognition, e.g., perception of speaker characteristics such as warmth or competence, is potentially a critical part of both human-human conversations and spoken dialogue systems. Most research has studied impressions only from the behaviors expressed by the speaker or the response from the listener, yet ignored their latent connection. In this paper, we perform impression recognition using a proposed listener adaptive cross-domain architecture, which consists of a listener adaptation function to model the causality between speaker and listener behaviors and a cross-domain fusion function to strengthen their connection. The experimental evaluation on the dyadic IMPRESSION dataset verified the efficacy of our method, producing concordance correlation coefficients of 78.8% and 77.5% in the competence and warmth dimensions, outperforming previous studies. The proposed method is expected to be generalized to similar dyadic interaction scenarios in affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Besides planning spoken content, psychological research indicates that people also intentionally control their appearances and behaviors to leave different impressions on their interaction partners in scenarios such as selection, job performance, and leader-subordinate relationships. For example, people tend to express extraversion during job interviews and show agreeableness in dating  [1] . Social research has pointed out the importance of impressions in human interactions, where it is natural to perceive impressions from the partners via non-verbal behaviors such as eye gaze, body pose, speaking activity, and prosody variation  [2] . Impressions are often categorized in terms of the \"Big Five\" personality traits: extraversion, agreeableness, conscientiousness, neuroticism, and openness, or the formed opinions of social perception/cognition: warmth and competence. Warmth reflects intentions towards others and includes traits of being good-natured, trustworthy, tolerant, friendly, and sincere.\n\nCompetence reflects the ability to enact intentions and means being capable, skillful, intelligent, and confident  [3] .\n\nThe procedure of impression recognition is similar to that of emotion recognition, consisting of two steps: feature extraction and classification/regression. Researchers have used many approaches to predict impressions from human behaviors, such as facial, vocal, and bodily expressions.  [4]  used high-level features obtained from speaking activity, body and head motion, along with low-level features extracted from audio to predict the personality impressions.  [5]  extracted visual and vocal features (e.g., eye gaze, head pose, speaking activity, and prosody variation) to characterize the social interaction.  [6]  used Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic Skin Response (GSR), and facial activity data to recognize personality.\n\nUnlike emotions that almost rely on the subjective feelings of the speaker, impressions also rely on how the listener perceives the speaker's expressions. However, most previous work recognized impressions by using only the speaker or listener behaviors, ignoring the fact that impressions are formed by both of the dyadic partners. Therefore, in this paper, we propose a dyadic impression recognition architecture using listener adaptive cross-domain fusion to capture and strengthen causal and related information from the speaker and listener as there is a perception gap that separates them into two domains of signal sources (explained in Sec 2).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Dyadic impression recognition. The majority of impression research focuses on personality impressions, yet more and more studies on other impression dimensions have emerged. The Noxi corpus  [7]  was collected to investigate the relationship between observed non-verbal cues and the first impression formation of warmth and competence  [8] . AMIGOS  [9]  collected participants' multimodal behaviors expressed during watching videos for the assessment of various affective levels (e.g., valence, arousal, control, familiarity, liking). The Dyadic IMPRESSION dataset  [3]  contains audio, visual, and physiological signals from both the speaker and listener for the recognition of warmth and competence. A critical issue, however, has long existed in this field yet not been solved: arXiv:2211.05163v4 [cs.MM] 16 Feb 2023 previous work either performed impression recognition from only speaker or listener behaviors, or failed to deal with their combination properly. For example, 1) the studies on the SSPNet Speaker Personality Corpus  [10]  estimated speaker personalities using speaker audio and listener annotations but gave no consideration to listener responses, which were not collected; 2) AMIGOS provides only the participant (i.e., listener) signals without the videos (i.e., speaker), resulting in the situation that studies using this dataset for impression research can only obtain listener behaviors; 3)  [3]  extracted both speaker and listener features for fusion but did not consider that the speaker signals (video sources) are identical for all listeners, bringing about redundancy in training data. Therefore, we aim to incorporate both speaker and listener features and fuse them properly. Feature fusion. Another major issue of impression recognition is feature fusion -a general problem for affective computing tasks. Tensor fusion, which deals with features at the hidden-state level, is becoming dominant as it can better model synchrony, relatedness, and hierarchy of multimodal features. For example, attention-based and hierarchical tensor fusion methods have been investigated for features of different levels and proven useful in emotion recognition  [11, 12] . However, we regard the fusion problem as more complex in impression recognition because impressions also depend on how listeners perceive speaker behaviors, which means there is a perception gap between the listener and speaker. In particular, many studies on affective states are conducted in the scenarios of video watching or audio listening where there is no real-time interaction, making it difficult for the two parties (signal sources) to achieve consistency towards the target states  [13] . Thus, in this work, we investigate effective fusion methods for such a cross-domain problem.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Data Preparation",
      "text": "Dyadic IMPRESSION dataset. This dataset contains multimodal signals from both the speaker and listener  [3] . The dataset consists of 31 dyads, in total 1,890 minutes of synchronized recordings of face videos, speech clips, eye gaze data, and peripheral nervous system physiological signals (e.g., EEG, ECG, blood volume pulse). 40 participants (listeners) watched 13 Noxi stimuli (speakers) and annotated their formed impressions in warmth and competence dimensions in real time. The labels are represented in a step-wise continuous manner, which means the participants were allowed to increase/decrease the label value once they felt an impression change. Thus, there is no value range limitation. Feature preparation. The provided features contain four modalities: audio, eye, face, and physio, from both the speaker and listener. Since the sample numbers of each modality are different, we then conducted resampling to make them the same as the label's. Finally, the 13 speakers have a total of 44,923 samples with 412 feature dimensions, while each of the 40 listeners has 44,923 samples with 68 feature dimensions, as in previous work  [3, 14] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Architecture",
      "text": "To address the two issues stated in Sec 2, we propose a listener adaptive cross-domain architecture, shown in Fig 1 . We first denote the concatenated features of the speaker and listener as S and L, which are 412-and 68-dimensional feature sequences with the length of 44,923, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Listener Adaptation",
      "text": "Causality modeling. In dyadic impression recognition, the listener reacts according to the speaker's signals, e.g., with a faster heartbeat or a smile. This causal relationship is usually ignored in previous studies  [3] , leaving them unable to exploit the full potential of the features. To this end, we performed a novel causality modeling module consisting of segmental Projection-Weighted Canonical Correlation Analysis (PWCCA) and a causality gating process. Segmental PWCCA first splits both speaker and listener features into n segments (we used n = 450, which means the length of the last segment is 23 and that of the previous 449 is 100), then calculates causality using every segment pair. This practice is inspired by the fact that in human interaction, listeners only respond to the speaker behaviors that they have just received in a short period of time  [15] . For the causality calculation, we used PWCCA  [16] , a variant of CCA, which gives higher weights for directions accounting for a higher proportion of the input. This is useful for finding the salient parts in the speaker segments that have high correlations with the listener segments, which we regard as high-level causality. Next, causality gating assigns the calculated PWCCA causality as the weight to each speaker segment to form causality-weighted speaker features as S W = {s w1 1 , s w2 2 , • • • , s wn n }, where s wi i = w i s i (s i : the ith speaker segment; w i : the ith PWCCA causality). Listener modeling. Considering the annotation bias introduced when assessing the speaker recordings by different listeners, we propose a listener modeling module to incorporate listener IDs. The IDs were first transformed into one-hot embeddings by the one-hot encoder, followed by a linear layer to encode the listener information and reshape the embeddings to the same size as other listener features for concatenation.\n\nAfter causality modeling, we fed the speaker features to a Bi-directional Long Short-Term Memory (BLSTM) network. Meanwhile, the listener features were fed to another BLSTM network (except for the listener ID, as it has no temporal information) and then concatenated with the listener ID.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Domain Fusion",
      "text": "Cross-domain attention. Following the listener adaptation function, a structured cross-domain attention consisting of intra-attention and inter-attention networks, aggregated information from the BLSTM hidden states and produced four fixed-length encodings: H s , H l , H s→l , and H l→s . For both intra-and inter-attention, we used multi-head self-attention:\n\nwhere W O , W Q i , W K i , and W V i are trainable parameters. Q, K, and V represent the query, key, and value, respectively. For inter-attention, Q is from one domain (the speaker or listener), while K and V are from the other. For intra-attention, the three parameters are from the same domain. The value of m is 16, and H denotes the concatenated multi-head representations: H s , H l , H s→l , and H l→s . Next, we concatenated the four representations and passed them to a Fully-Connected (FC) network with a ReLU activation function to generate the recognition outputs C p and W p , which are competence and warmth predictions.\n\nWe used cross-domain attention because the speaker and listener signals can be regarded as different domains in this dataset. The impression labels have high correlations with listener features but low correlations with the speaker's  [3] , indicating that their relatedness is not obvious. This phenomenon is plausible as the listener responds to the speaker recordings without real interaction. Thus, we need inter-attention to find relevant features between the two domains.\n\nInter-attention has been adopted in affective computing over recent years  [11] . It exchanges key-value pairs in multi-head self-attention. As shown in Fig  1,   H s→l denotes speaker-attended listener features and H l→s is the reverse.\n\nHowever, there may be other useful information from individual modalities ignored by the inter-attention. We used intra-attention to resolve this issue. Intra-attention focuses on salient information in each respective signal domain towards impression recognition and generates H s and H l as the hidden representations. The four representations were then concatenated for the final non-linear combination. Cross-domain regularization. To reduce the discrepancy and further regulate the relatedness between the two different domains, we designed a cross-domain regularization that has a Knowledge Distillation (KD) loss L kd and a Similarity Enhancement (SE) loss L se . Knowledge distillation is a deep learning technique used for training a small network under the guidance of a trained network  [17] . Though this technique is widely used in model training, recent work has been inspired to apply it to transfer knowledge between hidden representations  [18] . Considering the fact that the multimodal signals from the speaker and listener have weak relatedness, we used a KD loss to transfer the information from the other domain. Unlike inter-attention, which directly attends one domain to the other, which we call \"hard relatedness\", the KD loss enables indirectly learning multimodal knowledge with minor changes in a \"soft\" way: H l and H s can absorb information from each other to some extent while still maintaining their independence. We calculated the Mean Squared Error (MSE) for the KD loss (note that the two MSE calculations are identical):\n\nTo ensure the inter-attention representations have successfully learned the information from the other domain, we applied an SE loss. For example, minimizing the distance between H l→s and H l to align the two representations means that the listener information has been attended to the speaker's by interattention. We used Kullback-Leibler (KL) divergence for this purpose (note that the variables have been converted to probability distributions using softmax):\n\nThe reasons why we chose MSE and KL divergence are: 1) MSE generally outperforms KL divergence in knowledge distillation  [19] ; 2) KL divergence is good at calculating the distance between two distributions in the same probability space and is popular for similarity measurement  [20] , so we expect it to enhance the similarity in the cross-domain situation. We also tried exchanging MSE and KL divergence for KD and SE but found a small decrease in the warmth dimension. Finally, the concatenated representations were fed to an FC network, which contains a linear layer with 16 neurons, followed by a ReLU activation function, a dropout layer with 0.5 probability, and two parallel linear layers to generate predictions using multitask learning. The prediction task was optimized by the following objective function:\n\nwhere C p and W p are the predictions of competence and warmth, and C g and W g are the corresponding groundtruth.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation",
      "text": "The model was built using Pytorch and optimized using the Adam method. The learning rate was set at 1e-3 and reduced by half every 20 epochs. The model was trained for 40 epochs by minimizing the overall loss:\n\nWe randomly used 80% of the data for training, 10% for validation, and 10% for testing. The performance was evaluated using the Concordance Correlation Coefficient (CCC).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "We compare our results with the only two studies on this dataset and present an ablation study where we removed each component. Table  1  shows that: 1) our proposed method achieves the best results by largely increasing the CCC; 2) the removal of each of the components causes a decrease in performance, which in turn proves their effectiveness, and causality modeling contributes the most as the decrease is the largest; 3) inter-attention works better than intra-attention, suggesting that the cross-domain relevancy is learned and contributes to the recognition. To verify if LA really modeled the latent relationship, we compare the contribution of crossdomain fusion (i.e., performance decrease) with and without W w/ 1.1 1.1 1.0 0.9 0.9 0.6 1.1 0.7 w/o 0.9 0.7 0.7 0.8 0.7 0.5 0.4 0.3 LA in Table  2 . It shows that cross-domain fusion contributes more with LA, indicating that with the help of LA modeling the latent causality and listener information, it becomes easier for cross-domain fusion to capture the speaker-listener relatedness. In particular, the contribution of SE loss changes the most, indicating the relatedness between H s and H l encoded from two respective domains has been modeled by LA.\n\nMoreover, in Fig 2 , we observe that both KD and SE losses show a decreasing trend. However, we saw an increasing trend when removed from back-propagation, which also demonstrates their usefulness (figure omitted for brevity).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a listener adaptive cross-domain architecture to address long-existing problems in dyadic impression recognition. This architecture consists of a listener adaptation function and a cross-domain fusion function to model the causality between speaker and listener behaviors and capture their relatedness. The experimental evaluation shows that both functions help and the fusion works better with the listener adaptation. We expect the proposed architecture can be generalized to similar dyadic interaction scenarios and will test it on other datasets in our future work.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed architecture using listener adaptive cross-domain fusion.",
      "page": 3
    },
    {
      "caption": "Figure 1: , Hs→l denotes",
      "page": 3
    },
    {
      "caption": "Figure 2: , we observe that both KD and SE",
      "page": 4
    },
    {
      "caption": "Figure 2: Trends of regularization losses.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "Competence reﬂects the ability to enact intentions and means"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "being capable, skillful, intelligent, and conﬁdent [3]."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "The procedure of impression recognition is similar to that"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "of emotion recognition, consisting of\ntwo steps:\nfeature ex-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "traction and classiﬁcation/regression. Researchers have used"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "many approaches to predict\nimpressions from human behav-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "iors, such as facial, vocal, and bodily expressions.\n[4] used"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "high-level features obtained from speaking activity, body and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "head motion, along with low-level features extracted from au-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "dio to predict\nthe personality impressions.\n[5] extracted vi-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "sual and vocal\nfeatures (e.g., eye gaze, head pose, speaking"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "activity, and prosody variation) to characterize the social\nin-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "teraction.\n[6] used Electroencephalogram (EEG), Electrocar-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "diogram (ECG), Galvanic Skin Response (GSR), and facial"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "activity data to recognize personality."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "Unlike emotions that almost\nrely on the subjective feel-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "ings of the speaker, impressions also rely on how the listener"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "perceives\nthe speaker’s expressions.\nHowever, most previ-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "ous work recognized impressions by using only the speaker"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "or\nlistener behaviors,\nignoring the fact\nthat\nimpressions are"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "formed by both of the dyadic partners. Therefore,\nin this pa-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "per, we propose a dyadic impression recognition architecture"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "using listener adaptive cross-domain fusion to capture and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "strengthen causal and related information from the speaker"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "and listener as there is a perception gap that separates them"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "into two domains of signal sources (explained in Sec 2)."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "2. RELATED WORK"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": ""
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "Dyadic impression recognition. The majority of impression"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "research focuses on personality impressions, yet more and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "more studies on other impression dimensions have emerged."
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "The Noxi corpus [7] was collected to investigate the relation-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "ship between observed non-verbal cues and the ﬁrst\nimpres-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "sion formation of warmth and competence [8]. AMIGOS [9]"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "collected participants’ multimodal behaviors expressed dur-"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "ing watching videos for\nthe assessment of various affective"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "levels (e.g., valence, arousal, control, familiarity, liking). The"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "Dyadic IMPRESSION dataset [3] contains audio, visual, and"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "physiological signals from both the speaker and listener\nfor"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "the recognition of warmth and competence. A critical\nissue,"
        },
        {
          "Centre for Speech Technology Research, University of Edinburgh, UK": "however, has long existed in this ﬁeld yet not been solved:"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "previous work either performed impression recognition from": "only speaker or listener behaviors, or failed to deal with their",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "feature dimensions, as in previous work [3, 14]."
        },
        {
          "previous work either performed impression recognition from": "combination properly.\nFor example, 1)\nthe studies on the",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "SSPNet Speaker Personality Corpus [10] estimated speaker",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "4. PROPOSED ARCHITECTURE"
        },
        {
          "previous work either performed impression recognition from": "personalities using speaker audio and listener annotations but",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "To address the two issues stated in Sec 2, we propose a lis-"
        },
        {
          "previous work either performed impression recognition from": "gave no consideration to listener\nresponses, which were not",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "tener adaptive cross-domain architecture, shown in Fig 1. We"
        },
        {
          "previous work either performed impression recognition from": "collected; 2) AMIGOS provides only the participant (i.e., lis-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ﬁrst denote the concatenated features of the speaker and lis-"
        },
        {
          "previous work either performed impression recognition from": "tener) signals without\nthe videos (i.e., speaker),\nresulting in",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "tener as S and L, which are 412- and 68-dimensional feature"
        },
        {
          "previous work either performed impression recognition from": "the situation that studies using this dataset for impression re-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "sequences with the length of 44,923, respectively."
        },
        {
          "previous work either performed impression recognition from": "search can only obtain listener behaviors; 3) [3] extracted both",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "speaker and listener\nfeatures for\nfusion but did not consider",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "4.1. Listener Adaptation"
        },
        {
          "previous work either performed impression recognition from": "that\nthe speaker signals (video sources) are identical\nfor all",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "listeners, bringing about redundancy in training data. There-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "Causality modeling.\nIn dyadic impression recognition,\nthe"
        },
        {
          "previous work either performed impression recognition from": "fore, we aim to incorporate both speaker and listener features",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "listener reacts according to the speaker’s signals, e.g., with a"
        },
        {
          "previous work either performed impression recognition from": "and fuse them properly.",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "faster heartbeat or a smile.\nThis causal\nrelationship is usu-"
        },
        {
          "previous work either performed impression recognition from": "Feature fusion. Another major\nissue of\nimpression recog-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ally ignored in previous studies [3],\nleaving them unable to"
        },
        {
          "previous work either performed impression recognition from": "nition is\nfeature\nfusion – a general problem for\naffective",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "exploit\nthe full potential of the features. To this end, we per-"
        },
        {
          "previous work either performed impression recognition from": "computing tasks. Tensor fusion, which deals with features at",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "formed a novel causality modeling module consisting of seg-"
        },
        {
          "previous work either performed impression recognition from": "the hidden-state level,\nis becoming dominant as it can better",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "mental Projection-Weighted Canonical Correlation Analysis"
        },
        {
          "previous work either performed impression recognition from": "model synchrony,\nrelatedness, and hierarchy of multimodal",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "(PWCCA) and a causality gating process. Segmental PWCCA"
        },
        {
          "previous work either performed impression recognition from": "features. For example, attention-based and hierarchical tensor",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ﬁrst splits both speaker and listener features into n segments"
        },
        {
          "previous work either performed impression recognition from": "fusion methods have been investigated for features of differ-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "(we used n = 450, which means the length of\nthe last seg-"
        },
        {
          "previous work either performed impression recognition from": "ent\nlevels and proven useful\nin emotion recognition [11, 12].",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ment is 23 and that of the previous 449 is 100), then calculates"
        },
        {
          "previous work either performed impression recognition from": "However, we regard the fusion problem as more complex in",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "causality using every segment pair. This practice is inspired"
        },
        {
          "previous work either performed impression recognition from": "impression recognition because impressions also depend on",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "by the fact\nthat\nin human interaction,\nlisteners only respond"
        },
        {
          "previous work either performed impression recognition from": "how listeners perceive speaker behaviors, which means there",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "to the speaker behaviors that they have just received in a short"
        },
        {
          "previous work either performed impression recognition from": "is a perception gap between the listener and speaker.\nIn par-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "period of\ntime [15].\nFor\nthe causality calculation, we used"
        },
        {
          "previous work either performed impression recognition from": "ticular, many studies on affective states are conducted in the",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "PWCCA [16], a variant of CCA, which gives higher weights"
        },
        {
          "previous work either performed impression recognition from": "scenarios of video watching or audio listening where there is",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "for directions accounting for a higher proportion of the input."
        },
        {
          "previous work either performed impression recognition from": "no real-time interaction, making it difﬁcult\nfor\nthe two par-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "This is useful for ﬁnding the salient parts in the speaker seg-"
        },
        {
          "previous work either performed impression recognition from": "ties (signal sources) to achieve consistency towards the target",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ments that have high correlations with the listener segments,"
        },
        {
          "previous work either performed impression recognition from": "states [13]. Thus, in this work, we investigate effective fusion",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "which we regard as high-level causality. Next, causality gat-"
        },
        {
          "previous work either performed impression recognition from": "methods for such a cross-domain problem.",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "ing assigns the calculated PWCCA causality as the weight to"
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "each speaker segment to form causality-weighted speaker fea-"
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ", sw2\n, · · ·\n, swn\ntures as SW = {sw1\n}, where swi\n= wisi\n(si:"
        },
        {
          "previous work either performed impression recognition from": "3. DATA PREPARATION",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "n\n1\n2\ni"
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "the ith PWCCA causality).\nthe ith speaker segment; wi:"
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "Listener modeling.\nConsidering the annotation bias intro-"
        },
        {
          "previous work either performed impression recognition from": "Dyadic IMPRESSION dataset. This dataset contains mul-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "duced when assessing the speaker recordings by different lis-"
        },
        {
          "previous work either performed impression recognition from": "timodal signals from both the speaker and listener\n[3]. The",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "teners, we propose a listener modeling module to incorporate"
        },
        {
          "previous work either performed impression recognition from": "dataset consists of 31 dyads,\nin total 1,890 minutes of syn-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "listener IDs. The IDs were ﬁrst transformed into one-hot em-"
        },
        {
          "previous work either performed impression recognition from": "chronized recordings of\nface videos, speech clips, eye gaze",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "beddings by the one-hot encoder, followed by a linear layer to"
        },
        {
          "previous work either performed impression recognition from": "data,\nand peripheral nervous\nsystem physiological\nsignals",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "encode the listener information and reshape the embeddings"
        },
        {
          "previous work either performed impression recognition from": "(e.g., EEG, ECG, blood volume pulse).\n40 participants (lis-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "to the same size as other listener features for concatenation."
        },
        {
          "previous work either performed impression recognition from": "teners) watched 13 Noxi\nstimuli\n(speakers)\nand annotated",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "After causality modeling, we fed the speaker features to a"
        },
        {
          "previous work either performed impression recognition from": "their formed impressions in warmth and competence dimen-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "Bi-directional Long Short-Term Memory (BLSTM) network."
        },
        {
          "previous work either performed impression recognition from": "sions in real\ntime. The labels are represented in a step-wise",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "Meanwhile, the listener features were fed to another BLSTM"
        },
        {
          "previous work either performed impression recognition from": "continuous manner, which means\nthe participants were al-",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "network (except for the listener ID, as it has no temporal\nin-"
        },
        {
          "previous work either performed impression recognition from": "lowed to increase/decrease the label value once they felt an",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "formation) and then concatenated with the listener ID."
        },
        {
          "previous work either performed impression recognition from": "impression change. Thus, there is no value range limitation.",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "Feature preparation.\nThe provided features contain four",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "4.2. Cross-Domain Fusion"
        },
        {
          "previous work either performed impression recognition from": "modalities:\naudio,\neye,\nface,\nand\nphysio,\nfrom both\nthe",
          "while each of\nthe 40 listeners has 44,923 samples with 68": ""
        },
        {
          "previous work either performed impression recognition from": "speaker\nand listener.\nSince\nthe\nsample numbers of\neach",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "Cross-domain attention.\nFollowing the listener adaptation"
        },
        {
          "previous work either performed impression recognition from": "modality\nare\ndifferent, we\nthen\nconducted\nresampling\nto",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "function,\na structured cross-domain attention consisting of"
        },
        {
          "previous work either performed impression recognition from": "make them the same as the label’s. Finally,\nthe 13 speakers",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "intra-attention and inter-attention networks, aggregated in-"
        },
        {
          "previous work either performed impression recognition from": "have a total of 44,923 samples with 412 feature dimensions,",
          "while each of\nthe 40 listeners has 44,923 samples with 68": "formation from the BLSTM hidden states and produced four"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "ﬁxed-length encodings: H s, H l, H s→l, and H l→s. For both"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "intra- and inter-attention, we used multi-head self-attention:"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "H = M ultiHead(Q, K, V )W O\n(1)"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "(2)\n= Concat(head1, ..., headm)"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ", KW K\n, V W V\n)\n(3)\nheadi = Attention(QW Q"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "i\ni"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "where W O, W Q\n, W K\n, and W V\nare trainable parameters. Q,"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "i\ni\ni"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "K, and V represent\nthe query, key, and value,\nrespectively."
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "For inter-attention, Q is from one domain (the speaker or lis-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "tener), while K and V are from the other. For intra-attention,"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "the three parameters are from the same domain.\nThe value"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "of m is 16, and H denotes the concatenated multi-head rep-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "resentations: H s, H l, H s→l, and H l→s. Next, we concate-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "nated the four\nrepresentations and passed them to a Fully-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "Connected (FC) network with a ReLU activation function to"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "generate the recognition outputs Cp and Wp, which are com-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "petence and warmth predictions."
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "We used cross-domain attention because the speaker and"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "listener signals can be regarded as different domains in this"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "dataset. The impression labels have high correlations with lis-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "tener features but low correlations with the speaker’s [3], indi-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "cating that their relatedness is not obvious. This phenomenon"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "is plausible as the listener responds to the speaker recordings"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "without real interaction. Thus, we need inter-attention to ﬁnd"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "relevant features between the two domains."
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "Inter-attention\nhas\nbeen\nadopted\nin\naffective\ncomput-"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": ""
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "ing over\nrecent years [11].\nIt exchanges key-value pairs in"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "multi-head self-attention. As shown in Fig 1, H s→l denotes"
        },
        {
          "Fig. 1. Proposed architecture using listener adaptive cross-domain fusion.": "speaker-attended listener\nfeatures and H l→s\nis\nthe reverse."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: shows that: 1) our proposed method In this paper, we propose a listener adaptive cross-domain",
      "data": [
        {
          "SE loss. For example, minimizing the distance between H l→s": "and H l\nto align the two representations means that\nthe lis-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Method\nCompetence\nWarmth"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "tener information has been attended to the speaker’s by inter-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Wang et al. [3]\n73.7\n75.1"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "attention. We used Kullback–Leibler (KL) divergence for this",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Li et al. [14]\n77.0\n74.8"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "purpose (note that the variables have been converted to prob-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Ours\n78.8\n77.5"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "ability distributions using softmax):",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) Causality modeling\n77.3 ↓1.5\n75.6 ↓1.9"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "(5)\nLse = KL(H s→l, H s) + KL(H l→s, H l)",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) Listener modeling\n77.9 ↓0.9\n76.8 ↓0.7"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) Inter-attention\n77.7 ↓1.1\n76.4 ↓1.1"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "The reasons why we chose MSE and KL divergence are: 1)",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) Intra-attention\n77.8 ↓1.0\n76.6 ↓0.9"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "MSE generally outperforms KL divergence in knowledge dis-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) KD loss\n77.9 ↓0.9\n76.9 ↓0.6"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "tillation [19]; 2) KL divergence is good at calculating the dis-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "(-) SE loss\n77.7 ↓1.1\n76.8 ↓0.7"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "tance between two distributions in the same probability space",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "and is popular for similarity measurement [20], so we expect",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Table 2. Performance decrease of removing cross-domain fu-"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "it to enhance the similarity in the cross-domain situation. We",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "sion components w/ and w/o Listener Adaptation (LA)."
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "also tried exchanging MSE and KL divergence for KD and",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "SE but found a small decrease in the warmth dimension.",
          "Table 1. Results. (-): removal of the component.": "Inter-attn\nIntra-attn\nKD loss\nSE loss"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "LA"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "Finally,\nthe concatenated representations were fed to an",
          "Table 1. Results. (-): removal of the component.": "C\nW\nC\nW\nC\nW\nC\nW"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "FC network, which contains a linear\nlayer with 16 neurons,",
          "Table 1. Results. (-): removal of the component.": "w/\n1.1\n1.1\n1.0\n0.9\n0.9\n0.6\n1.1\n0.7"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "followed by a ReLU activation function, a dropout layer with",
          "Table 1. Results. (-): removal of the component.": "w/o\n0.9\n0.7\n0.7\n0.8\n0.7\n0.5\n0.4\n0.3"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "0.5 probability, and two parallel linear layers to generate pre-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "dictions using multitask learning.\nThe prediction task was",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "LA in Table 2.\nIt shows that cross-domain fusion contributes"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "optimized by the following objective function:",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "more with LA,\nindicating that with the help of LA modeling"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "the latent causality and listener information, it becomes easier"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "(6)\nLpred = M SE(Cp, Cg) + M SE(Wp, Wg)",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "for cross-domain fusion to capture the speaker-listener relat-"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "are\nthe predictions of\ncompetence\nand\nwhere Cp\nand Wp",
          "Table 1. Results. (-): removal of the component.": "edness.\nIn particular,\nthe contribution of SE loss changes the"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "warmth, and Cg and Wg are the corresponding groundtruth.",
          "Table 1. Results. (-): removal of the component.": "most, indicating the relatedness between H s and H l encoded"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "from two respective domains has been modeled by LA."
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "5. EXPERIMENTAL EVALUATION",
          "Table 1. Results. (-): removal of the component.": "Moreover,\nin Fig 2, we observe that both KD and SE"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "losses show a decreasing trend. However, we saw an increas-"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "5.1.\nImplementation",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "ing trend when removed from back-propagation, which also"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "demonstrates their usefulness (ﬁgure omitted for brevity)."
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "The model was built using Pytorch and optimized using the",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "Adam method. The learning rate was set at 1e-3 and reduced",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "by half every 20 epochs. The model was trained for 40 epochs",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "by minimizing the overall loss:",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "(7)\nL = Lpred + Lkd + Lse",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "We randomly used 80% of the data for training, 10% for val-",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "idation, and 10% for testing. The performance was evaluated",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "using the Concordance Correlation Coefﬁcient (CCC).",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "Fig. 2. Trends of regularization losses."
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "5.2. Results and Discussion",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "We compare our\nresults with the only two studies on this",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "",
          "Table 1. Results. (-): removal of the component.": "6. CONCLUSIONS"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "dataset and present an ablation study where we removed each",
          "Table 1. Results. (-): removal of the component.": ""
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "component.\nTable 1 shows\nthat:\n1) our proposed method",
          "Table 1. Results. (-): removal of the component.": "In this paper, we propose a listener adaptive cross-domain"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "achieves the best\nresults by largely increasing the CCC; 2)",
          "Table 1. Results. (-): removal of the component.": "architecture to address long-existing problems in dyadic im-"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "the removal of each of the components causes a decrease in",
          "Table 1. Results. (-): removal of the component.": "pression recognition. This architecture consists of a listener"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "performance, which in turn proves\ntheir effectiveness,\nand",
          "Table 1. Results. (-): removal of the component.": "adaptation function and a cross-domain fusion function to"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "causality modeling contributes the most as the decrease is the",
          "Table 1. Results. (-): removal of the component.": "model\nthe causality between speaker and listener behaviors"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "largest;\n3)\ninter-attention works better\nthan intra-attention,",
          "Table 1. Results. (-): removal of the component.": "and capture their\nrelatedness.\nThe experimental evaluation"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "suggesting that\nthe\ncross-domain relevancy is\nlearned and",
          "Table 1. Results. (-): removal of the component.": "shows that both functions help and the fusion works better"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "contributes to the recognition. To verify if LA really modeled",
          "Table 1. Results. (-): removal of the component.": "with the listener adaptation. We expect the proposed architec-"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "the latent relationship, we compare the contribution of cross-",
          "Table 1. Results. (-): removal of the component.": "ture can be generalized to similar dyadic interaction scenarios"
        },
        {
          "SE loss. For example, minimizing the distance between H l→s": "domain fusion (i.e., performance decrease) with and without",
          "Table 1. Results. (-): removal of the component.": "and will test it on other datasets in our future work."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "tomatic personality perception:\nPrediction of\ntrait at-"
        },
        {
          "7. REFERENCES": "[1] Brian W Swider, T Brad Harris, and Qing Gong, “First",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "IEEE Transac-\ntribution based on prosodic features,”"
        },
        {
          "7. REFERENCES": "impression effects in organizational psychology.,” Jour-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "tions on Affective Computing, vol. 3, no. 3, pp. 273–284,"
        },
        {
          "7. REFERENCES": "nal of Applied Psychology, 2021.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "2012."
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[11] Yuanchao Li, Peter Bell, and Catherine Lai,\n“Fusing"
        },
        {
          "7. REFERENCES": "[2]\nJoan-Isaac Biel and Daniel Gatica-Perez, “The youtube",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "asr outputs in joint training for speech emotion recogni-"
        },
        {
          "7. REFERENCES": "lens: Crowdsourced personality impressions and audio-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "tion,”\nin ICASSP 2022-2022 IEEE International Con-"
        },
        {
          "7. REFERENCES": "IEEE Transactions on Multi-\nvisual analysis of vlogs,”",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "ference on Acoustics,\nSpeech and Signal Processing"
        },
        {
          "7. REFERENCES": "media, vol. 15, no. 1, pp. 41–55, 2012.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "(ICASSP). IEEE, 2022, pp. 7362–7366."
        },
        {
          "7. REFERENCES": "[3] Chen Wang and Guillaume Chanel,\n“An open dataset",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[12] Leimin Tian, Johanna Moore, and Catherine Lai, “Rec-"
        },
        {
          "7. REFERENCES": "for impression recognition from multimodal bodily re-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "ognizing emotions\nin spoken dialogue with hierarchi-"
        },
        {
          "7. REFERENCES": "sponses,”\nin 2021 9th International Conference on Af-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "cally fused acoustic and lexical features,” in 2016 IEEE"
        },
        {
          "7. REFERENCES": "fective Computing and Intelligent\nInteraction (ACII).",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Spoken Language Technology Workshop (SLT).\nIEEE,"
        },
        {
          "7. REFERENCES": "IEEE, 2021, pp. 1–8.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "2016, pp. 565–572."
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[13] Leimin Tian, Michal Muszynski, Catherine Lai,\nJo-"
        },
        {
          "7. REFERENCES": "[4] Oya Aran and Daniel Gatica-Perez,\n“One of a kind:",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "hanna D Moore, Theodoros Kostoulas, Patrizia Lom-"
        },
        {
          "7. REFERENCES": "Inferring personality impressions in meetings,”\nin Pro-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "bardo, Thierry Pun, and Guillaume Chanel, “Recogniz-"
        },
        {
          "7. REFERENCES": "ceedings of\nthe 15th ACM on International conference",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "ing induced emotions of movie audiences: Are induced"
        },
        {
          "7. REFERENCES": "on multimodal interaction, 2013, pp. 11–18.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "and perceived emotions the same?,” in 2017 Seventh In-"
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "ternational Conference on Affective Computing and In-"
        },
        {
          "7. REFERENCES": "[5] Timothy A Judge, Chad A Higgins, Carl\nJ Thoresen,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "telligent Interaction (ACII). IEEE, 2017, pp. 28–35."
        },
        {
          "7. REFERENCES": "and Murray R Barrick,\n“The big ﬁve personality traits,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "general mental ability, and career success across the life",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[14] Yuanchao Li\nand Catherine Lai,\n“A cross-domain"
        },
        {
          "7. REFERENCES": "span,”\nPersonnel psychology, vol. 52, no. 3, pp. 621–",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "approach for continuous\nimpression recognition from"
        },
        {
          "7. REFERENCES": "652, 1999.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "arXiv\npreprint\ndyadic\naudio-visual-physio\nsignals,”"
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "arXiv:2203.13932, 2022."
        },
        {
          "7. REFERENCES": "[6] Ramanathan\nSubramanian,\nJulia\nWache,\nMo-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "jtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[15] Mark L Knapp, Judith A Hall, and Terrence G Horgan,"
        },
        {
          "7. REFERENCES": "and Nicu Sebe,\n“Ascertain: Emotion and personality",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Nonverbal communication in human interaction, Cen-"
        },
        {
          "7. REFERENCES": "IEEE Trans-\nrecognition using commercial\nsensors,”",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "gage Learning, 2013."
        },
        {
          "7. REFERENCES": "actions\non Affective Computing,\nvol.\n9,\nno.\n2,\npp.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[16] Ari Morcos, Maithra Raghu, and Samy Bengio,\n“In-"
        },
        {
          "7. REFERENCES": "147–160, 2016.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "sights on representational similarity in neural networks"
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "with canonical correlation,” Advances in Neural Infor-"
        },
        {
          "7. REFERENCES": "[7] Angelo\nCafaro,\nJohannes Wagner,\nTobias\nBaur,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "mation Processing Systems, vol. 31, 2018."
        },
        {
          "7. REFERENCES": "Soumia Dermouche, Mercedes Torres Torres, Cather-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "ine Pelachaud, Elisabeth Andr´e,\nand Michel Valstar,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[17] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-"
        },
        {
          "7. REFERENCES": "“The NoXi database: multimodal\nrecordings of medi-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "hou, Antoine Chassang, Carlo Gatta, and Yoshua Ben-"
        },
        {
          "7. REFERENCES": "the\nated novice-expert\ninteractions,”\nin Proceedings of",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "arXiv preprint\ngio,\n“Fitnets: Hints for thin deep nets,”"
        },
        {
          "7. REFERENCES": "19th ACM International Conference on Multimodal In-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "arXiv:1412.6550, 2014."
        },
        {
          "7. REFERENCES": "teraction, 2017, pp. 350–359.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[18] Zhiqi Huang, Fenglin Liu, Xian Wu, Shen Ge, Helin"
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Wang, Wei Fan, and Yuexian Zou, “Audio-oriented mul-"
        },
        {
          "7. REFERENCES": "[8] Beatrice\nBiancardi,\nAngelo\nCafaro,\nand\nCatherine",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "timodal machine comprehension via dynamic inter- and"
        },
        {
          "7. REFERENCES": "Pelachaud, “Analyzing ﬁrst impressions of warmth and",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "intra-modality attention,” in AAAI, 2021."
        },
        {
          "7. REFERENCES": "competence from observable nonverbal cues in expert-",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "the 19th ACM\nnovice interactions,”\nin Proceedings of",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[19] Taehyeon Kim,\nJaehoon Oh, NakYil Kim, Sangwook"
        },
        {
          "7. REFERENCES": "International Conference\non Multimodal\nInteraction,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Cho, and Se-Young Yun, “Comparing Kullback-Leibler"
        },
        {
          "7. REFERENCES": "2017, pp. 341–349.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Divergence and Mean Squared Error Loss in Knowledge"
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Distillation,” arXiv preprint arXiv:210/5.08919, 2021."
        },
        {
          "7. REFERENCES": "[9]\nJuan Abdon Miranda Correa, Mojtaba Khomami Abadi,",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "[20]\nJacob Goldberger,\nShiri Gordon, Hayit Greenspan,"
        },
        {
          "7. REFERENCES": "Niculae Sebe, and Ioannis Patras,\n“Amigos: A dataset",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "et al., “An Efﬁcient Image Similarity Measure Based on"
        },
        {
          "7. REFERENCES": "for affect, personality and mood research on individuals",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "Approximations of KL-Divergence Between Two Gaus-"
        },
        {
          "7. REFERENCES": "IEEE Transactions on Affective Comput-\nand groups,”",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        },
        {
          "7. REFERENCES": "",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": "sian Mixtures,” in ICCV, 2003, vol. 3, pp. 487–493."
        },
        {
          "7. REFERENCES": "ing, 2018.",
          "[10] Gelareh Mohammadi and Alessandro Vinciarelli,\n“Au-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "First impression effects in organizational psychology",
      "authors": [
        "Brian Swider",
        "Brad Harris",
        "Qing Gong"
      ],
      "year": "2021",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "3",
      "title": "The youtube lens: Crowdsourced personality impressions and audiovisual analysis of vlogs",
      "authors": [
        "Joan-Isaac Biel",
        "Daniel Gatica-Perez"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "An open dataset for impression recognition from multimodal bodily responses",
      "authors": [
        "Chen Wang",
        "Guillaume Chanel"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "One of a kind: Inferring personality impressions in meetings",
      "authors": [
        "Oya Aran",
        "Daniel Gatica-Perez"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "6",
      "title": "The big five personality traits, general mental ability, and career success across the life span",
      "authors": [
        "Timothy Judge",
        "Chad Higgins",
        "Carl Thoresen",
        "Murray Barrick"
      ],
      "year": "1999",
      "venue": "Personnel psychology"
    },
    {
      "citation_id": "7",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "Ramanathan Subramanian",
        "Julia Wache",
        "Mojtaba Khomami Abadi",
        "L Radu",
        "Stefan Vieriu",
        "Nicu Winkler",
        "Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "The NoXi database: multimodal recordings of mediated novice-expert interactions",
      "authors": [
        "Angelo Cafaro",
        "Johannes Wagner",
        "Tobias Baur",
        "Soumia Dermouche",
        "Mercedes Torres",
        "Catherine Pelachaud",
        "Elisabeth André",
        "Michel Valstar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "9",
      "title": "Analyzing first impressions of warmth and competence from observable nonverbal cues in expertnovice interactions",
      "authors": [
        "Beatrice Biancardi",
        "Angelo Cafaro",
        "Catherine Pelachaud"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "Juan Abdon",
        "Miranda Correa",
        "Mojtaba Khomami Abadi",
        "Niculae Sebe",
        "Ioannis Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Automatic personality perception: Prediction of trait attribution based on prosodic features",
      "authors": [
        "Gelareh Mohammadi",
        "Alessandro Vinciarelli"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Recognizing emotions in spoken dialogue with hierarchically fused acoustic and lexical features",
      "authors": [
        "Leimin Tian",
        "Johanna Moore",
        "Catherine Lai"
      ],
      "year": "2016",
      "venue": "2016 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "14",
      "title": "Recognizing induced emotions of movie audiences: Are induced and perceived emotions the same?",
      "authors": [
        "Leimin Tian",
        "Michal Muszynski",
        "Catherine Lai",
        "Johanna Moore",
        "Theodoros Kostoulas",
        "Patrizia Lombardo",
        "Thierry Pun",
        "Guillaume Chanel"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "15",
      "title": "A cross-domain approach for continuous impression recognition from dyadic audio-visual-physio signals",
      "authors": [
        "Yuanchao Li",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "A cross-domain approach for continuous impression recognition from dyadic audio-visual-physio signals",
      "arxiv": "arXiv:2203.13932"
    },
    {
      "citation_id": "16",
      "title": "Nonverbal communication in human interaction",
      "authors": [
        "Judith Mark L Knapp",
        "Terrence Hall",
        "Horgan"
      ],
      "year": "2013",
      "venue": "Nonverbal communication in human interaction"
    },
    {
      "citation_id": "17",
      "title": "Insights on representational similarity in neural networks with canonical correlation",
      "authors": [
        "Ari Morcos",
        "Maithra Raghu",
        "Samy Bengio"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Fitnets: Hints for thin deep nets",
      "authors": [
        "Adriana Romero",
        "Nicolas Ballas",
        "Samira Kahou",
        "Antoine Chassang",
        "Carlo Gatta",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Fitnets: Hints for thin deep nets",
      "arxiv": "arXiv:1412.6550"
    },
    {
      "citation_id": "19",
      "title": "Audio-oriented multimodal machine comprehension via dynamic inter-and intra-modality attention",
      "authors": [
        "Zhiqi Huang",
        "Fenglin Liu",
        "Xian Wu",
        "Shen Ge",
        "Helin Wang",
        "Wei Fan",
        "Yuexian Zou"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "20",
      "title": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation",
      "authors": [
        "Taehyeon Kim",
        "Jaehoon Oh",
        "Nakyil Kim",
        "Sangwook Cho",
        "Se-Young Yun"
      ],
      "year": "2021",
      "venue": "Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation"
    },
    {
      "citation_id": "21",
      "title": "An Efficient Image Similarity Measure Based on Approximations of KL-Divergence Between Two Gaussian Mixtures",
      "authors": [
        "Jacob Goldberger",
        "Shiri Gordon",
        "Hayit Greenspan"
      ],
      "year": "2003",
      "venue": "ICCV"
    }
  ]
}