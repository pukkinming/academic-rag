{
  "paper_id": "2203.03428v1",
  "title": "Attention-Based Region Of Interest (Roi) Detection For Speech Emotion Recognition",
  "published": "2022-03-03T22:01:48Z",
  "authors": [
    "Jay Desai",
    "Houwei Cao",
    "Ravi Shah"
  ],
  "keywords": [
    "speech recognition",
    "LSTM",
    "ROI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition for real-life applications is a challenging task. Human emotion expressions are subtle, and can be conveyed by a combination of several emotions. In most existing emotion recognition studies, each audio utterance/video clip is labelled/classified in its entirety. However, utterance/clip-level labelling and classification can be too coarse to capture the subtle intra-utterance/clip temporal dynamics. For example, an utterance/video clip usually contains only a few emotion-salient regions and many emotionless regions. In this study, we propose to use attention mechanism in deep recurrent neural networks to detection the Regions-of-Interest (ROI) that are more emotionally salient in human emotional speech/video, and further estimate the temporal emotion dynamics by aggregating those emotionally salient regions-of-interest. We compare the ROI from audio and video and analyse them. We compare the performance of the proposed attention networks with the state-of-the-art LSTM models on multi-class classification task of recognizing six basic human emotions, and the proposed attention models exhibit significantly better performance. Furthermore, the attention weight distribution can be used to interpret how an utterance can be expressed as a mixture of possible emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotion plays an important role in everyday decision making. It impacts the way they communicate i.e. via body language, facial expressions, verbal communication, personality, etc  [1] ,  [2]  and also shows the characteristics of the person like  [3]  leadership, honesty, teamwork. In fact, in a book Emotional Intelligence 2.0 by Travis Bradberry, Ph.D. it was found that 90% of all the people they studied at work had higher EQs. Speech is the key aspect of expressing emotion and most basic way of human-human and human-machine interactions. In this study, we use the acoustic features of speech to detect the part/region of speech which impacts the most in detecting the emotional state of the subject.\n\nSpeech emotion recognition (SER) is a trending topic in the field of research with the rapid use of artificial intelligence and machine learning in our lives. While there are many techniques and features that has been proposed  [4] ,  [5]  but it is still not clear on which gives most information about emotions. There are two common types of feature extraction methods, Global features and Local features. Features extracted for whole audio file using full audio clip are usually known as global features while features extracted using multiple time frames at regular intervals usually 20-30ms with or without overlap are known as local features. Global features usually work well for general machine learning models whereas local features are better suited for deep neural networks. Features are the Low Level Descriptors (LLDs) which are believed to affect the most to emotions and High Level Statistical Functions (HSFs) which are applied to LLDs to extract variations and contours for temporal description of the data. While the majority of researchers have agreed that global features perform better than local features  [6] -  [9]  but other researchers also believe that global features only show better performance for high arousal emotions like anger, fear, joy  [10] . Here are some common LLDs and HSFs: In recent years, there has been advancements in the field of machine learning and artificial intelligence techniques and their applications. The authors in  [16] -  [20]  used ensemble of SVM instead of just one to better classify emotions using MFCC, total energy and f0 features from audio signal. The work done in  [21] ,  [22]  focused on using Deep Neural Networks and Recurrent Neural Network (RNNs) to learn short term acoustic features at frame level and then mapping them to sentence-level representation using extreme learning machines (ELM). The authors in  [23]  used various LSTM techniques and used logistic regression based general attention mechanism with weighted pooling for classification.\n\nThe authors in  [24]  used deep convolutional recurrent neural networks where they used a combination of convolutional neural network (CNN) layers and bi-directional LSTM layers. They derived a generalized attention mechanism method from the methods proposed in  [25] ,  [26]  and compare CNNs taskspecific spectral decorrelation with that of the discrete cosine transformation (DCT) in clean and noisy conditions. One issue that appears in most SER datasets is their labels are given at utterance level and in any speech, there are many silence periods and only a short time of utterance of few words that impact the overall emotion. The silent frames can be handled by labelling them as null, our approach handles it implicitly without any separate or explicit mechanisms to handle it.\n\nIn this paper, we used an LSTMs in an encoder -decoder based fashion proposed in  [27] ,  [28]  using LSTM. Different from the attention technique used in  [23]  where they used logistic regression and mean pooling, in our approach for attention, we train a separate small Neural Network to learn how much attention to pay on each frame. Our method works similar to human mechanism for translating the input signals in brain and using only contextually important parts to decode the input and return the results as emotions. This has been proven better in many language translation tasks where to translate one word, only a part of the sentence is important rather than whole sentence. In the following we discuss various approaches we used for classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Neural Network Model",
      "text": "In the below subsections data description describing which dataset is used and details about it, features extraction techniques describing all the features extracted tools or libraries used, testing strategy, various models have been described.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Description",
      "text": "The dataset used here is Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D)  [29] . It is a dataset of 7442 audio files from 91 different subjects -48 males and 43 females of various races such as African, Asian, American, Hispanic, Caucasian between the ages of 20 and 74. Subjects spoke 12 sentences in 6 different emotions i.e. Anger, Disgust, Fear, Happy, Neutral and Sad in four emotion levels -low, medium, high and unspecified. Categorical emotion labels and real-value intensity values for the perceived emotion were collected using crowd-sourcing from 2,443 raters. All 91 actors read 12 sentences in three to four emotion levels making total number of files to be 7442 clips. Each actor has an average of 82 audio clips. The problems with other datasets such as low recognition rates of human subjects  [10] , bad quality of recorded utterances is not found in this dataset. The human recognition rates for intended emotion for audio only data was 40%. Recognition rates were highest for neutral followed by happy, anger, disgust, fear and sad.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Feature Extraction",
      "text": "Every audio clip has been padded with zeros till maximum audio clip length to remove the inconsistencies between audio clip length. The first 13 Mel-frequency cepstral coefficients are extracted for every 20 milliseconds (frame length) with 50% or 10 milliseconds (frame step) of overlap between each frame for the data used in model 1,2,3 and 4.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Testing Strategy",
      "text": "We have used Leave One Subject Out (LOSO) strategy to train-test-split the data. In that for every 91 subjects that a model gets trained on, it gets tested on one subject. The total training files for one model comprised of 7360 training files and testing on one subject having 82 files. In total of 92 models were trained and tested and for the final result, mean from all the results from 91 models was taken and a confusion matrix was created as shown in Fig.  3 . It gives a better information on how the model will perform on real world scenario because it has never seen the subject before than other methods such as kfold cross validation or random train-test split where the model has already seen and trained on the audio clips of subject and can perform better on them in testing phase.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Model 1 -Lstm With Attention",
      "text": "We have used a Sequence to Sequence type model which has following parts:\n\n1) Encoder (Pre-LSTM) 2) Intermediate Attention Layer environment\n\n3) Decoder (Post-LSTM) 4) Dense (SoftMax) Layer The Pre-LSTM and Post-LSTM layers are CuDNNLSTM and are unidirectional. The output of Pre-LSTM is given to the attention mechanism layer explained below which returns attention weights. These attention weights are dotted with the output of Pre-LSTM and fed to Post-LSTM followed by dense or fully connected layer with softmax activation function which calculates the probability for 6 classes. Dropout is used to reduce overfitting for regularization.\n\nIn general, LSTMs have ability to learn from the sequence of data and also learn the dependencies between sequences and store them in the memory cell for future use. It stores the relevant information and forgets the irrelevant information using the forget gate and passes on this learnt context to next time steps. The context learnt here is the result of backpropagation and loss minimization to optimize the overall accuracy. The attention mechanism used in model 1 and model 2 is described and explained below.\n\nAttention Mechanism: The attention block shown in figure  1  shows the architecture of attention mechanism in the dotted line. Models without attention passes the output from dropout layer directly to post-LSTM layer. The similar attention mechanism is used in  [11] ,  [12]  for generating image captions while focusing on only parts of the image.\n\nThere are two types of LSTM layers used for pre-LSTM, one is uni-directional and other is bi-directional. The post-LSTM passes outputs o < t > and hidden cell state h < t > from one time step to next. The inputs to post-LSTM are s < t > , context and h < t >. The outputs of pre-LSTM are represented as p < t > for unidirectional and for bidirectional LSTM, the forward and backward direction, the outputs are concatenated p < t >= [p < t > (f orward), p < t > (backward)]. The repeatVector copies o < t -1 > for x times where x is the number of time frames used to extract the data and then Concatenate layer concatenates it with p < t > to compute e < t, t > which is then passed to dense layer and then softmax layer to output a < t, t > where the t represents the post-LSTM's time step and t represents the pre-LSTM's time step.\n\nAt any time step t, given the outputs of pre-LSTM [p < 1 >, p < 2 >, . . . , p < x >] and the previous output of post-LSTM o < t -1 >, the attention mechanism will compute attention vector or attention weights [a < t, 1 >, a < t, 2 > . . . a < t, x >] and output the context vector shown in (1)",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "E. Model 2 -Bidirectional Lstm With Attention",
      "text": "A basic LSTM cell has memory cell which preserves information for learning Long Term Dependencies. It learns the dependencies of the inputs that are passed through it and uses that for the next output. A Uni-Directional LSTM only preserves past information where the inputs are passed forward as they come whereas the bidirectional LSTM passes the inputs in both forward and backward directions. For instance, let's say on a high level the LSTM predicts the emotion for various time steps with inputs given in forward and inputs given in backward direction as shown below, Forward LSTM: Angry, Sad, Angry, Fear, Angry. . . . Backward LSTM: . . . . Fear, Fear, Fear, Fear, Fear, Fear, Fear.\n\nHere one can see that the past and future information both can make it easier for model to predict the emotion for next time step.\n\nThe hidden state in bi-directional LSTM is the concatenation of hidden states from forward run and backward run of the inputs. Bi-directional LSTM generally outperforms the unidirectional LSTM and it is becoming a new standard where the input sequence doesn't matter. In this model we have changed the type from uni-directional to bi-directional in Pre-LSTM layer of model 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "F. Model 3 -Lstm Without Attention",
      "text": "The model consists of 4 layers: Pre-LSTM (uni-directional), Dropout, Post-LSTM, Dense. The model is almost like model 1 but without the attention mechanism. We removed the attention mechanism in this model to compare the results with model 1 and check the impact attention makes on the results. The model architecture can be seen in Fig.  1  by removing the attention block. The output of Pre-LSTM is fed to the dropout layer followed by Post-LSTM. Then the final layer is dense or fully connected layer with softmax as activation function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "G. Model 4 -Bidirectional Lstm Without Attention",
      "text": "This model is same as model 3 except for the Pre-LSTM layer is replaced by Bidirectional Pre-LSTM layer whose outputs are combined and fed to next layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Evaluation And Result Analysis",
      "text": "We compare the results of model 1 and model 2 to analyse the attention values for unidirectional LSTM and bidirectional LSTM. For example, in a statement 'DFA' recorded by subject 1015 in 'ANG' anger emotion with unspecified emotion level, attention weights from model 1 and model 2 are shown with spectrogram and raw waveform above them. It can be observed that both the models can handle the silent areas very well and the values tend to zero. It can also be seen that Bi-Directional LSTM helps the model predict the correct emotion compared to Uni-Directional LSTM at the word 'JACKET' which spans from 30000th sample to 40000th sample which can be seen in Fig.  2 . Note that the values for attention are the output of a softmax function which gives the probability for every time frame and they have been expanded to fit the samples in the raw waveform and spectrogram shown in figure  3 .\n\nWe used confusion matrix to describe the performance of the classification model on test data for each model. For 91 models, 91 confusion matrices were generated, aggregated and normalized to generate one final confusion matrix which represents the overall model performance. Each item(I,j) in confusion matrix tells the number of items belonging to emotion I classified as emotion J.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).",
      "page": 2
    },
    {
      "caption": "Figure 3: It gives a better information on",
      "page": 2
    },
    {
      "caption": "Figure 1: by removing the",
      "page": 3
    },
    {
      "caption": "Figure 2: Note that the values for attention are the output of",
      "page": 3
    },
    {
      "caption": "Figure 2: For audio ﬁle 1015 DFA ANG XX.wav, RAW Waveform, Spec-",
      "page": 4
    },
    {
      "caption": "Figure 3: Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "Abstract—Automatic\nemotion recognition for\nreal-life\nappli-",
          "rshah79@nyit.edu": "at\nregular\nintervals usually 20-30ms with or without overlap"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "cations\nis\na\nchallenging\ntask. Human emotion expressions\nare",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "are known as local features. Global features usually work well"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "subtle, and can be conveyed by a combination of\nseveral emo-",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "for general machine\nlearning models whereas\nlocal\nfeatures"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "tions.\nIn most\nexisting emotion recognition studies,\neach audio",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "are better\nsuited for deep neural networks. Features\nare\nthe"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "utterance/video clip is labelled/classiﬁed in its entirety. However,",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "utterance/clip-level\nlabelling and classiﬁcation can be too coarse",
          "rshah79@nyit.edu": "Low Level Descriptors (LLDs) which are believed to affect the"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "to capture the subtle intra-utterance/clip temporal dynamics. For",
          "rshah79@nyit.edu": "most\nto emotions and High Level Statistical Functions (HSFs)"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "example,\nan\nutterance/video\nclip\nusually\ncontains\nonly\na\nfew",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "which are applied to LLDs to extract variations and contours"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "emotion-salient\nregions\nand many\nemotionless\nregions.\nIn this",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "for\ntemporal description of\nthe data. While\nthe majority of"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "study, we propose to use attention mechanism in deep recurrent",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "researchers\nhave\nagreed\nthat\nglobal\nfeatures\nperform better"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "neural networks\nto detection the Regions-of-Interest\n(ROI)\nthat",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "are more emotionally salient\nin human emotional\nspeech/video,",
          "rshah79@nyit.edu": "than local\nfeatures\n[6]–[9] but other\nresearchers also believe"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "and further estimate the temporal emotion dynamics by aggre-",
          "rshah79@nyit.edu": "that global\nfeatures only show better performance\nfor high"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "gating those emotionally salient regions-of-interest. We compare",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "arousal\nemotions\nlike\nanger,\nfear,\njoy\n[10]. Here\nare\nsome"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "the ROI\nfrom audio and video and analyse them. We compare",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "common LLDs and HSFs:"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "the performance\nof\nthe proposed attention networks with the",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "state-of-the-art LSTM models on multi-class classiﬁcation task of",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "recognizing six basic human emotions, and the proposed attention",
          "rshah79@nyit.edu": "TABLE I"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "COMMON LLDS AND HSFS [13]–[15]"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "models exhibit signiﬁcantly better performance. Furthermore, the",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "attention weight distribution can be used to interpret how an",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "LLDs\nHSFs"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "utterance can be expressed as a mixture of possible emotions.",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "pitch, MFCCS, voicing probability,\nmean, variance, min, max,\nrange,"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "Index Terms—speech recognition, LSTM, ROI.",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "energy,\nZero-Crossing\nrate,\nmedian,\nquartiles,\nskewness,\nkur-"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "formant\nlocations/\nbandwidths,\ntosis,\nlinear regression coefﬁcients,"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "harmonics-to-noise\nratio,\njitter,\nRMSenergy, SMA, PCM, Rﬁlters,"
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "I.\nINTRODUCTION",
          "rshah79@nyit.edu": ""
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "",
          "rshah79@nyit.edu": "etc.\nloudness, etc."
        },
        {
          "hcao02@nyit.edu\njdesai09@nyit.edu": "Human emotion plays\nan important\nrole\nin everyday de-",
          "rshah79@nyit.edu": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "read 12 sentences in three to four emotion levels making total"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "number of ﬁles\nto be 7442 clips. Each actor has an average"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "of 82 audio clips. The problems with other datasets\nsuch as"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "low recognition rates of human subjects\n[10], bad quality of"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "recorded utterances\nis not\nfound in this dataset. The human"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "recognition rates for intended emotion for audio only data was"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "40%. Recognition rates were highest\nfor neutral\nfollowed by"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "happy, anger, disgust,\nfear and sad."
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "B. Feature Extraction"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "Every audio clip has been padded with zeros till maximum"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "audio clip length to remove the inconsistencies between audio"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "clip length. The ﬁrst 13 Mel-frequency cepstral\ncoefﬁcients"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "are\nextracted for\nevery 20 milliseconds\n(frame\nlength) with"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "50% or 10 milliseconds (frame step) of overlap between each"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "frame for\nthe data used in model 1,2,3 and 4."
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "C. Testing Strategy"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "We have used Leave One Subject Out\n(LOSO) strategy to"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "train-test-split\nthe data.\nIn that\nfor\nevery 91 subjects\nthat\na"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "model gets trained on,\nit gets tested on one subject. The total"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "training ﬁles\nfor one model comprised of 7360 training ﬁles"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "and testing on one subject having 82 ﬁles. In total of 92 models"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "were trained and tested and for the ﬁnal result, mean from all"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "the results from 91 models was taken and a confusion matrix"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "was created as shown in Fig. 3. It gives a better information on"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "how the model will perform on real world scenario because it"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "has never seen the subject before than other methods such as k-"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "fold cross validation or random train-test split where the model"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "has already seen and trained on the audio clips of subject and"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "can perform better on them in testing phase."
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "D. Model 1 - LSTM with Attention"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": ""
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "We have used a Sequence to Sequence type model which"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "has following parts:"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "1) Encoder\n(Pre-LSTM)"
        },
        {
          "Fig. 1. Architecture for Model 1 LSTM with Attention (the dotted line shows the attention mechanism block).": "2)\nIntermediate Attention Layer environment"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3) Decoder\n(Post-LSTM)": "4) Dense (SoftMax) Layer",
          "inputs in both forward and backward directions. For instance,": "let’s\nsay on a high level\nthe LSTM predicts\nthe emotion for"
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "various\ntime\nsteps with inputs given in forward and inputs"
        },
        {
          "3) Decoder\n(Post-LSTM)": "The Pre-LSTM and Post-LSTM layers are CuDNNLSTM",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "given in backward direction as shown below,"
        },
        {
          "3) Decoder\n(Post-LSTM)": "and are unidirectional. The output of Pre-LSTM is given to",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "Forward LSTM: Angry, Sad, Angry, Fear, Angry. . . ."
        },
        {
          "3) Decoder\n(Post-LSTM)": "the attention mechanism layer explained below which returns",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "Backward LSTM:\n. . . . Fear, Fear, Fear, Fear, Fear, Fear, Fear."
        },
        {
          "3) Decoder\n(Post-LSTM)": "attention weights. These\nattention weights\nare\ndotted with",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "Here one can see that\nthe past and future information both"
        },
        {
          "3) Decoder\n(Post-LSTM)": "the output of Pre-LSTM and fed to Post-LSTM followed by",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "can make it easier\nfor model\nto predict\nthe emotion for next"
        },
        {
          "3) Decoder\n(Post-LSTM)": "dense or fully connected layer with softmax activation function",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "time step."
        },
        {
          "3) Decoder\n(Post-LSTM)": "which calculates the probability for 6 classes. Dropout\nis used",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "to reduce overﬁtting for\nregularization.",
          "inputs in both forward and backward directions. For instance,": "The hidden state in bi-directional LSTM is the concatena-"
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "tion of hidden states\nfrom forward run and backward run of"
        },
        {
          "3) Decoder\n(Post-LSTM)": "In general, LSTMs have ability to learn from the sequence",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "the inputs. Bi-directional LSTM generally outperforms the uni-"
        },
        {
          "3) Decoder\n(Post-LSTM)": "of data\nand also learn the dependencies between sequences",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "directional LSTM and it is becoming a new standard where the"
        },
        {
          "3) Decoder\n(Post-LSTM)": "and store\nthem in the memory cell\nfor\nfuture use.\nIt\nstores",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "input sequence doesn’t matter. In this model we have changed"
        },
        {
          "3) Decoder\n(Post-LSTM)": "the relevant\ninformation and forgets the irrelevant\ninformation",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "the\ntype\nfrom uni-directional\nto bi-directional\nin Pre-LSTM"
        },
        {
          "3) Decoder\n(Post-LSTM)": "using\nthe\nforget\ngate\nand\npasses\non\nthis\nlearnt\ncontext\nto",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "layer of model 1."
        },
        {
          "3) Decoder\n(Post-LSTM)": "next\ntime\nsteps. The\ncontext\nlearnt\nhere\nis\nthe\nresult\nof",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "backpropagation and loss minimization to optimize the overall",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "F\n. Model 3 - LSTM without attention"
        },
        {
          "3) Decoder\n(Post-LSTM)": "accuracy. The attention mechanism used in model 1 and model",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "The model consists of 4 layers: Pre-LSTM (uni-directional),"
        },
        {
          "3) Decoder\n(Post-LSTM)": "2 is described and explained below.",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "Dropout, Post-LSTM, Dense. The model\nis almost\nlike model"
        },
        {
          "3) Decoder\n(Post-LSTM)": "Attention Mechanism: The attention block shown in ﬁgure",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "1\nbut without\nthe\nattention mechanism. We\nremoved\nthe"
        },
        {
          "3) Decoder\n(Post-LSTM)": "1 shows the architecture of attention mechanism in the dotted",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "attention mechanism in this model\nto compare the results with"
        },
        {
          "3) Decoder\n(Post-LSTM)": "line. Models without attention passes the output from dropout",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "model 1 and check the impact attention makes on the results."
        },
        {
          "3) Decoder\n(Post-LSTM)": "layer directly to post-LSTM layer. The similar attention mech-",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "The model architecture can be seen in Fig. 1 by removing the"
        },
        {
          "3) Decoder\n(Post-LSTM)": "anism is used in [11], [12] for generating image captions while",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "attention block. The output of Pre-LSTM is fed to the dropout"
        },
        {
          "3) Decoder\n(Post-LSTM)": "focusing on only parts of\nthe image.",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "layer\nfollowed by Post-LSTM. Then the ﬁnal\nlayer\nis dense"
        },
        {
          "3) Decoder\n(Post-LSTM)": "There are two types of LSTM layers used for pre-LSTM,",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "or\nfully connected layer with softmax as activation function."
        },
        {
          "3) Decoder\n(Post-LSTM)": "one\nis uni-directional\nand other\nis bi-directional. The post-",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "LSTM passes outputs o < t > and hidden cell\nstate h <",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "G. Model 4 - Bidirectional LSTM without attention"
        },
        {
          "3) Decoder\n(Post-LSTM)": "t > from one\ntime\nstep to next. The\ninputs\nto post-LSTM",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "This model\nis\nsame as model 3 except\nfor\nthe Pre-LSTM"
        },
        {
          "3) Decoder\n(Post-LSTM)": "are\ns < t > ,\ncontext\nand h < t >. The outputs of pre-",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "layer\nis\nreplaced\nby Bidirectional\nPre-LSTM layer whose"
        },
        {
          "3) Decoder\n(Post-LSTM)": "LSTM are represented as p < t > for unidirectional and for",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "outputs are combined and fed to next\nlayer."
        },
        {
          "3) Decoder\n(Post-LSTM)": "bidirectional LSTM,\nthe forward and backward direction,\nthe",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "outputs are concatenated p < t >= [p < t > (f orward), p <",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "III. EVALUATION AND RESULT ANALYSIS"
        },
        {
          "3) Decoder\n(Post-LSTM)": "t > (backward)]. The repeatVector copies o < t − 1 > for x",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "We compare the results of model 1 and model 2 to analyse"
        },
        {
          "3) Decoder\n(Post-LSTM)": "times where x is the number of time frames used to extract the",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "the attention values for unidirectional LSTM and bidirectional"
        },
        {
          "3) Decoder\n(Post-LSTM)": "data and then Concatenate layer concatenates it with p < t >",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "LSTM. For example, in a statement ‘DFA’ recorded by subject"
        },
        {
          "3) Decoder\n(Post-LSTM)": "to compute e < t, t(cid:48) > which is then passed to dense layer and",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "1015 in ‘ANG’ anger emotion with unspeciﬁed emotion level,"
        },
        {
          "3) Decoder\n(Post-LSTM)": "then softmax layer to output a < t, t(cid:48) > where the t represents",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "attention weights from model 1 and model 2 are shown with"
        },
        {
          "3) Decoder\n(Post-LSTM)": "the post-LSTM’s time step and t(cid:48)\nrepresents the pre-LSTM’s",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "spectrogram and raw waveform above them. It can be observed"
        },
        {
          "3) Decoder\n(Post-LSTM)": "time step.",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "that both the models can handle the silent areas very well and"
        },
        {
          "3) Decoder\n(Post-LSTM)": "At any time step t, given the outputs of pre-LSTM [p <",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "the values tend to zero. It can also be seen that Bi-Directional"
        },
        {
          "3) Decoder\n(Post-LSTM)": "1 >, p < 2 >, . . . , p < x >] and the previous output of post-",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "LSTM helps the model predict\nthe correct emotion compared"
        },
        {
          "3) Decoder\n(Post-LSTM)": "LSTM o < t − 1 >,\nthe attention mechanism will compute",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "to Uni-Directional LSTM at\nthe word ‘JACKET’ which spans"
        },
        {
          "3) Decoder\n(Post-LSTM)": "[a < t, 1 >, a < t, 2 >\nattention vector or attention weights",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "from 30000th sample to 40000th sample which can be seen"
        },
        {
          "3) Decoder\n(Post-LSTM)": ". . . a < t, x >] and output\nthe context vector shown in (1)",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "in Fig. 2. Note that\nthe values for attention are the output of"
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "a softmax function which gives the probability for every time"
        },
        {
          "3) Decoder\n(Post-LSTM)": "x(cid:88) t\ncontext < t > =\na < t, t(cid:48) > p < t(cid:48) >\n(1)",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "frame and they have been expanded to ﬁt\nthe samples in the"
        },
        {
          "3) Decoder\n(Post-LSTM)": "(cid:48)",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "raw waveform and spectrogram shown in ﬁgure 3."
        },
        {
          "3) Decoder\n(Post-LSTM)": "E. Model 2 - Bidirectional LSTM with Attention",
          "inputs in both forward and backward directions. For instance,": ""
        },
        {
          "3) Decoder\n(Post-LSTM)": "",
          "inputs in both forward and backward directions. For instance,": "We used confusion matrix to describe the performance of"
        },
        {
          "3) Decoder\n(Post-LSTM)": "A basic\nLSTM cell\nhas memory\ncell which\npreserves",
          "inputs in both forward and backward directions. For instance,": "the classiﬁcation model on test data for each model. For 91"
        },
        {
          "3) Decoder\n(Post-LSTM)": "information for\nlearning Long Term Dependencies.\nIt\nlearns",
          "inputs in both forward and backward directions. For instance,": "models,\n91\nconfusion matrices were\ngenerated,\naggregated"
        },
        {
          "3) Decoder\n(Post-LSTM)": "the dependencies of\nthe inputs that are passed through it and",
          "inputs in both forward and backward directions. For instance,": "and normalized to generate one ﬁnal confusion matrix which"
        },
        {
          "3) Decoder\n(Post-LSTM)": "uses\nthat\nfor\nthe next output. A Uni-Directional LSTM only",
          "inputs in both forward and backward directions. For instance,": "represents\nthe overall model performance. Each item(I,j)\nin"
        },
        {
          "3) Decoder\n(Post-LSTM)": "preserves past information where the inputs are passed forward",
          "inputs in both forward and backward directions. For instance,": "confusion matrix\ntells\nthe\nnumber\nof\nitems\nbelonging\nto"
        },
        {
          "3) Decoder\n(Post-LSTM)": "as\nthey\ncome whereas\nthe\nbidirectional LSTM passes\nthe",
          "inputs in both forward and backward directions. For instance,": "emotion I classiﬁed as emotion J."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. Attention vs Non-Attention": "Comparing the unidirectional"
        },
        {
          "B. Attention vs Non-Attention": "3, model 1 with attention performs better"
        },
        {
          "B. Attention vs Non-Attention": "emotions: disgust, neutral, happy and signiﬁcantly better"
        },
        {
          "B. Attention vs Non-Attention": "classifying fear. Comparing the bidirectional"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "adding attention to it model 2 performs better"
        },
        {
          "B. Attention vs Non-Attention": "models in classifying fear emotion."
        },
        {
          "B. Attention vs Non-Attention": "Finally,\nout\nof\nall\nthe\nfour models, model"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "other model."
        },
        {
          "B. Attention vs Non-Attention": "IV. DISCUSSION"
        },
        {
          "B. Attention vs Non-Attention": "From the\nresults of\nall\nthe"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "attention mechanism improves\nthe\nperformance"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "However,\nincreasing the number of\ninput"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "13 mfccs such as pitch,"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "classiﬁers as\nseen in [30] can be combined as"
        },
        {
          "B. Attention vs Non-Attention": "[31]\nin hierarchical, serial and parallel\nfashion."
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "V. CONCLUSION"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "In future,"
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": ""
        },
        {
          "B. Attention vs Non-Attention": "features can be used to get more better\nresults."
        },
        {
          "B. Attention vs Non-Attention": "REFERENCES"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "Neutral\nModel 1 – 63.10\nModel 3 – 52.48"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "Sad\nModel 2 – 70.57\nModel 4 – 63.25"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "As seen in table II, model 2 best classiﬁes the anger, disgust,"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "fear and sad emotions of all the models. Model 1 best performs"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "on happy and neutral emotions and worst performs on anger"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "emotion of all the models. Model 3 worst performs on disgust,"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "fear, happy and neutral emotions of all\nthe model. Model 4"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "worst performs on sad emotion of all\nthe models. From this it"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "can be deduced that model 3 is the worst performer and model"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "4 is the best performer."
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "A. Uni-directional Vs Bi-directional Results"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "Comparing the\nresults of model 1 and model 2, we\ncan"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "see that model 2 classiﬁes every emotion better\nthan model"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "1 except\nfor “happy” emotion where model 1 has 62.6% true"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "positives vs 55.62% of model 2. Comparing the\nresults of"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "model 3 and model 4, we can see that model 4 outperforms"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "model 3 for\nall\nemotions\nexcept\nfor\nemotion “sad” having"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "68.67% true positives vs 63.25% true positives. This\nshows"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "that\nthe inputs passed to LSTM in both the directions help the"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "model\nto perform better at classifying compared to the inputs"
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": ""
        },
        {
          "Happy\nModel 1 – 62.7\nModel 3 – 43.71": "passed only in one direction."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "LSTM. NA: Non (without)\n-Attention model. AT: Attention."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[9] M.T. Shami, M.S. Kamel, Segment-based approach to the recognition of"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "emotions\nin speech,\nin:\nIEEE International Conference on Multimedia"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "and Expo, 2005.\nICME 2005, 2005, 4pp."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[10] R.W. Picard, E. Vyzas,\nJ. Healey, Toward machine\nemotional\nintel-"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "ligence:\nanalysis of\naffective physiological\nstate,\nIEEE Trans. Pattern"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Anal. Mach.\nIntell. 23 (10)\n(2001) 1175–1191."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[11] Xu, Kelvin & Ba,\nJimmy & Kiros, Ryan & Cho, Kyunghyun &"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Courville, Aaron & Salakhutdinov, Ruslan & Zemel, Richard & Bengio,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Y\n. (2015). Show, Attend and Tell: Neural Image Caption Generation with"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Visual Attention."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[12] Bahdanau, Dzmitry & Cho, Kyunghyun & Bengio, Y.\n(2014). Neural"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Machine Translation by Jointly Learning to Align and Translate. ArXiv."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "1409."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[13] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, S. Kollias, W. Fellenz,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "J. Taylor, Emotion recognition in human–computer\ninteraction,\nIEEE"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Signal Process. Mag. 18 (2001) 32–80."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[14] C. Busso, S. Lee, S. Narayanan, Analysis of emotionally salient aspects"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "of\nfundamental\nfrequency for\nemotion detection,\nIEEE Trans. Audio"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Speech Language Process. 17 (4)\n(2009) 582–596."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[15]\nL. Bosch, Emotions,\nspeech and the asr\nframework, Speech Commun."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "40 (2003) 213–225."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[16] Danisman T., Alpkocak A.\n(2008) Emotion Classiﬁcation\nof Audio"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Signals Using Ensemble of Support Vector Machines.\nIn: Andr´e E.,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Dybkjær L., Minker W., Neumann H., Pieraccini R., Weber M.\n(eds)"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Perception in Multimodal Dialogue Systems. PIT 2008. Lecture Notes"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "in Computer Science, vol 5078. Springer, Berlin, Heidelberg"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[17] B. Schuller, G. Rigoll, M. Lang, Speech emotion recognition combining"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "acoustic features and linguistic information in a hybrid support vector"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "machine-belief\nnetwork\narchitecture,\nin: Proceedings\nof\nthe\nICASSP"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "2004, vol. 1, 2004, pp. 577–580."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[18] C. Lee, S. Yildrim, M. Bulut, A. Kazemzadeh, C. Busso, Z. Deng, S."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Lee, S. Narayanan, Emotion recognition based on phoneme classes,\nin:"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Proceedings of\nICSLP, 2004, pp. 2193–2196."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[19] O. Kwon, K. Chan,\nJ. Hao, T. Lee, Emotion recognition by speech"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "signal,\nin: EUROSPEECH Geneva, 2003, pp. 125–128."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[20] O. Pierre-Yves, The production and recognition of emotions in speech:"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "features\nand\nalgorithms,\nInt.\nJ. Human–Computer\nStud.\n59\n(2003)"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "157–183."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[21] Kun Han, Dong Yu, and Ivan Tashev, “Speech emotion recognition using"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "deep neural network and extreme\nlearning machine.,”\nin Interspeech,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "2014, pp. 223–227."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[22]\nJinkyu Lee\nand Ivan Tashev,\n“High-level\nfeature\nrepresentation using"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "recurrent neural network for speech emotion recognition,” in Interspeech,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "2015."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[23]\nSeyedmahdad Mirsamadi, Emad Barsoum and Cha Zhang, “Automatic"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "speech emotion recognition using recurrent neural networks with local"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "attention” in IEEE International Conference on Acoustics, Speech and"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Signal Processing (ICASSP) 2017."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[24]\nEyben, Florian & W¨ollmer, Martin & Schuller, Bj¨orn.\n(2010). openS-"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "MILE – The Munich Versatile\nand Fast Open-Source Audio Feature"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Extractor. MM’10 - Proceedings of the ACM Multimedia 2010 Interna-"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "tional Conference. 1459-1462. 10.1145/1873951.1874246."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[25] Huang, C., Narayanan, S.S. (2016) Attention Assisted Discovery of Sub-"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Utterance Structure in Speech Emotion Recognition. Proc.\nInterspeech"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "2016, 1387-1391."
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "[26] Chao, Linlin & Tao,\nJianhua & Yang, Minghao & Li, Ya & Wen,"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Zhengqi.\n(2016). Audio Visual Emotion Recognition with Temporal"
        },
        {
          "Fig. 3. Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional": "Alignment and Perception Attention."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "English intonation: its form and function",
      "authors": [
        "M Schubiger"
      ],
      "year": "1958",
      "venue": "English intonation: its form and function"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "3",
      "title": "Vocal correlates of emotional states, Speech Evaluation in Psychiatry, Grune and Stratton",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1981",
      "venue": "Vocal correlates of emotional states, Speech Evaluation in Psychiatry, Grune and Stratton"
    },
    {
      "citation_id": "4",
      "title": "Towards a small set of robust acoustic features for emotion recognition: challenges",
      "authors": [
        "Marie Tahon",
        "Laurence Devillers"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "The relevance of feature type for the automatic classification of emotional user states: low level descriptors and functionals",
      "authors": [
        "Anton Bj Orn Schuller",
        "Dino Batliner",
        "Stefan Seppi",
        "Thurid Steidl",
        "Johannes Vogt",
        "Laurence Wagner",
        "Laurence Devillers",
        "Noam Vidrascu",
        "Loic Amir",
        "Kessous"
      ],
      "year": "2007",
      "venue": "The relevance of feature type for the automatic classification of emotional user states: low level descriptors and functionals"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "7",
      "title": "Emotional speech classification using Gaussian mixture models and the sequential floating forward selection algorithm",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "8",
      "title": "Fusion of global statistical and segmental spectral features for speech emotion recognition",
      "authors": [
        "H Hu",
        "M.-X Xu",
        "W Wu"
      ],
      "year": "2007",
      "venue": "International Speech Communication Association-8th Annual Conference of the International Speech Communication Association, Interspeech 2007"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "Fig"
      ],
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional LSTM. NA: Non (without) -Attention model. AT: Attention",
      "venue": "Mean normalized confusion matrices of 91 subjects tested on 91 models trained using LOSO strategy. UNI: Uni-directional LSTM. BI: Bi-directional LSTM. NA: Non (without) -Attention model. AT: Attention"
    },
    {
      "citation_id": "11",
      "title": "Segment-based approach to the recognition of emotions in speech",
      "authors": [
        "M Shami",
        "M Kamel"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "12",
      "title": "Toward machine emotional intelligence: analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "13",
      "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhutdinov",
        "Richard Zemel",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
    },
    {
      "citation_id": "14",
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural Machine Translation by Jointly Learning to Align and Translate"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "16",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Audio Speech Language Process"
    },
    {
      "citation_id": "17",
      "title": "Emotions, speech and the asr framework",
      "authors": [
        "L Bosch"
      ],
      "year": "2003",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "18",
      "title": "Emotion Classification of Audio Signals Using Ensemble of Support Vector Machines",
      "authors": [
        "T Danisman",
        "A Alpkocak"
      ],
      "year": "2008",
      "venue": "Perception in Multimodal Dialogue Systems. PIT 2008. Lecture Notes in Computer Science"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "Proceedings of the ICASSP 2004"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition based on phoneme classes",
      "authors": [
        "C Lee",
        "S Yildrim",
        "M Bulut",
        "A Kazemzadeh",
        "C Busso",
        "Z Deng",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of ICSLP"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition by speech signal",
      "authors": [
        "O Kwon",
        "K Chan",
        "J Hao",
        "T Lee"
      ],
      "year": "2003",
      "venue": "Emotion recognition by speech signal"
    },
    {
      "citation_id": "22",
      "title": "The production and recognition of emotions in speech: features and algorithms",
      "authors": [
        "O Pierre-Yves"
      ],
      "year": "2003",
      "venue": "Int. J. Human-Computer Stud"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "24",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "Jinkyu Lee",
        "Ivan Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "openS-MILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor. MM'10 -Proceedings of the ACM Multimedia 2010 International Conference",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "openS-MILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor. MM'10 -Proceedings of the ACM Multimedia 2010 International Conference",
      "doi": "1459-1462.10.1145/1873951.1874246"
    },
    {
      "citation_id": "27",
      "title": "Attention Assisted Discovery of Sub-Utterance Structure in Speech Emotion Recognition",
      "authors": [
        "C Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention",
      "authors": [
        "Linlin Chao",
        "Tao",
        "& Jianhua",
        "Minghao Yang",
        "Ya Li",
        "Zhengqi Wen"
      ],
      "year": "2016",
      "venue": "Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention"
    },
    {
      "citation_id": "29",
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer",
        "Gulcehre",
        "& Caglar",
        "Bougares",
        "& Fethi",
        "Schwenk",
        "Holger",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "doi": "10.3115/v1/D14-1179"
    },
    {
      "citation_id": "30",
      "title": "Sequence to Sequence Learning with Neural Networks",
      "authors": [
        "Ilya Sutskever",
        "Vinyals",
        "& Oriol",
        "Quoc Le"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans Affect Comput"
    },
    {
      "citation_id": "32",
      "title": "Psychological motivated multi-stage emotion classification exploiting voice quality features",
      "authors": [
        "M Lugger",
        "B Yang"
      ],
      "year": "2008",
      "venue": "Speech Recognition"
    },
    {
      "citation_id": "33",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recognition",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recognition",
      "doi": "44.572-587.10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "34",
      "title": "Combining Pattern Classifiers: Methods and Algorithms",
      "authors": [
        "L Kuncheva"
      ],
      "year": "2004",
      "venue": "Combining Pattern Classifiers: Methods and Algorithms"
    }
  ]
}