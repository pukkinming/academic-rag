{
  "paper_id": "2202.05400v2",
  "title": "Parse: Pairwise Alignment Of Representations In Semi-Supervised Eeg Learning For Emotion Recognition",
  "published": "2022-02-11T01:10:17Z",
  "authors": [
    "Guangyi Zhang",
    "Vandad Davoodnia",
    "Ali Etemad"
  ],
  "keywords": [
    "Semi-supervised learning",
    "EEG representations learning",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose PARSE, a novel semi-supervised architecture for learning reliable EEG representations for emotion recognition. To reduce the potential distribution mismatch between large amounts of unlabeled data and a limited number of labeled data, PARSE uses pairwise representation alignment. First, our model performs data augmentation followed by label guessing for large amounts of original and augmented unlabeled data. The model is then followed by sharpening the guessed labels and convex combinations of the unlabeled and labeled data. Finally, it performs representation alignment and emotion classification. To rigorously test our model, we compare PARSE to several state-of-the-art semi-supervised approaches, which we implement and adapt for EEG learning. We perform these experiments on four public EEG-based emotion recognition datasets, SEED, SEED-IV, SEED-V and AMIGOS (valence and arousal). The experiments show that our proposed framework achieves the overall best results with varying amounts of limited labeled samples in SEED, SEED-IV and AMIGOS (valence), while approaching the overall best result (reaching the second-best) in SEED-V and AMIGOS (arousal). The analysis shows that our pairwise representation alignment considerably improves the performance by performing the distribution alignment between unlabeled and labeled data, especially when only",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotions are encountered and experienced by humans on a daily basis, and significantly influence our behaviors, interactions, and perceptions of the world. It is, therefore, critical to develop algorithms capable of recognizing different human emotions from behavioural measures and physiological signals to assist computers in better responding to human needs and interactions. As a result, affective computing, a discipline that aims to develop data-driven computational models capable of recognizing human emotional states  [1] , has become a popular field of research in recent years.\n\nEmotions can be detected and quantified using a variety of different human-generated signals, including facial expressions  [2] , speech  [3] , and bio-signals such as electrocardiogram (ECG)  [4] , electrodermal activity  [5] , photoplethysmogram  [6] , Electroencephalogram (EEG)  [7] , and others. While each modality comes with a unique set of advantages and disadvantages (e.g., wearability, cost, accuracy, etc.), EEG is highly regarded as an informative and viable option for affective computing given its direct relation to the central nervous system.\n\nDeep learning models have become increasingly popular in recent years due to the significant advancements in deep neural networks, computational power, and abundance of collected data. Such methods have shown great promise in dealing with various challenges encountered in EEG, including high dimensionality, non-stationarity, and high susceptibility to noise and the linear mixing effect  [8] ,  [9] . However, majority of these methods are fully-supervised and rely heavily on large amounts of 'labeled' training samples. On the other hand, labeling EEG signals is challenging, time-consuming, expensive, and often requires annotation experts. For example, many EEG-based works in the area require diverse emotion annotating methods such as prestimulation self-assessment, post-experiment self-assessment, and numerous expert evaluations to acquire accurate labels  [10] -  [13] . When only a very small portion of the training samples are labeled, most existing fully-supervised learning solutions suffer from performance degradation. To address this challenge, we propose a novel Semi-Supervised Learning (SSL) pipeline by efficiently leveraging large amounts of unlabeled EEG samples and very few labeled ones. Problem statement. (1) The field of SSL has witnessed interesting progress in recent years, most notably in the computer vision domain  [14] -  [19] . Nonetheless, very few works have adopted these state-of-the-art methods in the field of EEG representation learning, and fewer so have proposed novel SSL frameworks to learn EEG for emotion recognition or otherwise.  (2)  In most recent SSL methods, the distribution of unlabeled samples plays a critical role in the performance of the model. In particular, when pseudo-labels are generated (guessed) for unlabeled samples, existing methods often consider the confidence of the model on the estimated pseudo-labels to accept or reject unlabeled samples for which low-confidence pseudo-labels cannot be guessed  [18] ,  [19] . While this approach is logically viable and practically effective, the confidence threshold set is often dataset-specific and requires trial and error. In particular, this would be a significant challenge given the distribution differences between different datasets in the context of EEG.  (3)  In addition to the internal distribution of unlabeled data, the closeness of the distribution of unlabeled samples with respect to labeled ones plays a key role in SSL methods as well. Should the distribution of labeled and unlabeled samples be far from each other, the quality of the generated pseudo-labels for the unlabeled samples might get compromised due to the lack of generalizability in the model. While most SSL methods have not addressed this issue  [17] -  [19] , we believe that this problem needs more attention, especially in the context of semi-supervised EEG representation learning, given the vast distribution differences often found within different datasets. Contributions. In this paper, we propose a novel semi-supervised EEG learning framework for emotion recognition. Our model, entitled PARSE (Pairwise Alignment of Representations for Semi-Supervised EEG Learning), first augments the labeled and unlabeled EEG data. Then, we use a classifier to make predictions on the original, weakly-augmented, and strongly-augmented unlabeled data. Next, we average these three predictions as the guessed label for each unlabeled sample. Afterward, we compute convex combinations of labeled and unlabeled data followed by the representation alignment using a domain discriminator trained on the interpolated set. Simultaneously, a classifier is also trained to perform emotion recognition. An overview of our method is presented in Figure  1 . First, EEG is collected from the subject while a stimuli is provided (in the case of our study to induce different emotions). Predominant features are then extracted from the pre-processed EEG recordings. Following, an encoder is used to extract new representations. After that, to allow the classifier to predict the labels of unseen data with higher confidence, we enforce the distributions of labeled and unlabeled samples to become close to each other by aligning their representations in a pairwise manner. We test PARSE on four publicly available datasets, namely SEED  [10] , SEED-IV  [11] , SEED-V  [12] , and AMIGOS  [13] , considering the following criteria: i) including discrete (e.g., sad, happy, neutral) and continuous emotions (e.g., arousal); (ii) including binary and multi-class classification tasks (2, 3, 4, or 5 classes); (iii) including both balanced and imbalanced datasets.\n\nTo fully evaluate our method, we implement and adopt several SSL methods from other domains (e.g., computer vision) to compare with our method. In particular, we implement and compare PARSE with varying amounts of few-labeled samples (1, 3, 5, 7, 10, 25 labeled samples per class) against three cuttingedge methods, MixMatch  [17] , FixMatch  [18] , and AdaMatch  [19] , in addition to five classical SSL methods, Π-model  [14] , temporal ensembling  [14] , mean teacher  [15] , convolutional autoencoders  [20] , and pseudo-labeling  [21] . Our experiments show that PARSE achieves overall best results in SEED, SEED-IV and AMIGOS (valence), and approaches the best results in SEED-V (0.3% difference) and AMIGOS (arousal). We also perform an analysis on the impact of our pairwise representation alignment module, showing that the alignment consistently improves our method's performance over a varying number of labeled samples across all datasets. Our analysis also shows that the alignment helps to reduce the distance between the distributions of labeled and unlabeled data, particularly, when only one sample per class is available. This finding is further demonstrated by the effective performance of our model.\n\nIn summary, our contributions in this paper are as follows:\n\n(1) We propose a novel SSL approach for EEG representation learning. Our method performs pairwise representation alignment between labeled and unlabeled samples and generalizes well across a varying number of scenarios with few labeled samples across four large public datasets. (2) We perform extensive experiments and compare our method to several recent and clas- sical SSL methods. The study shows that our approach obtains strong results, outperforming other methods in the majority of experimental conditions. We also carry out a detailed ablation study to demonstrate the impact of our pairwise representation alignment component. (  3 ) To contribute to the field and enable reproducibility, we make our code publicly available at https://github.com/guangyizhangbci/PARSE. The rest of this paper is organized as follows. Section 2 discusses the related work in emotion recognition with EEG, semisupervised learning, as well as semi-supervised EEG representation learning for emotion recognition. Section 3 describes our proposed semi-supervised solution for EEG learning. Section 4 presents the dataset, EEG preprocessing, and feature extraction. We also describe the evaluation protocol, implementation details, and SSL benchmarks. In section 5, we present and analyze the experiments and results, including the ablation study performed to investigate the impact of the pairwise representation alignment. Additionally, we further analyze the effect of various hyperparameter settings on the model's performance. Finally, we summarize our work and provide a future direction and the concluding remarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition With Eeg",
      "text": "The general approach to emotion recognition with EEG consists of preprocessing EEG recordings, followed by feature extraction, and finally a classifier to learn the extracted features  [9] ,  [22] . In the preprocessing step, many approaches such as frequency filtering and blind signal separation are used to remove noise and artefacts from EEG data  [10] ,  [11] ,  [13] . Following, well-known features such as logarithm Power Spectral Density (PSD) and Differential Entropy (DE) are extracted from the decomposed signals in several dominant EEG frequency bands (e.g., alpha, beta, gamma)  [10] -  [13] . Successive to feature extraction, various classification algorithms for emotion recognition have been developed  [10] -  [13] . For instance, K-nearest neighbor  [10] , support vector machines  [10] -  [13] , logistic regression  [10] , random forest  [23] , and naive Bayes  [13]  have been employed to learn the non-linear output information from the extracted features.\n\nRecently, various deep learning techniques have been used to improve recognition performance due to their capabilities in learning more task-relevant and dominant information from extracted features or automatically learned representations. For example, deep belief networks  [10]  and deep neural networks  [11]  have been used to learn higher level features using multiple hidden layers. Recurrent deep neural architectures such as long shortterm memory networks  [24]  and spatial-temporal recurrent neural networks  [25]  have been used to exploit dependencies among features extracted from sequential data. A convolutional neural network framework was employed to discover spatial-temporal information  [26]  and graph neural networks were used to learn topological structure information of EEG channels through graph connections  [27] ,  [28] . All of the deep learning-based frameworks listed above outperform conventional machine learning classifiers, many of them achieving state-of-the-art performance in various emotion recognition tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Semi-Supervised Learning",
      "text": "When very few labeled data are used for training, deep learning networks may not perform well due to overfitting, slow convergence, and the random initialization of the neural networks  [29] .\n\nTo tackle this problem, unsupervised pre-training, for instance using Stacked Auto-Encoders (SAE) and Deep Belief Networks (DBN), has been proposed  [30] . In  [29] ,  [30] , the unsupervised pre-training stage was claimed as a regularization and was shown to be capable of guiding the model toward a better local minima in the fine-tuning stage. This unsupervised pre-training strategy has been successfully used to improve performance on small labeled training sets in computer vision, natural language processing, and EEG learning  [29] ,  [31] -  [33] . Recently, contrastive learning has been used for unsupervised pre-training by increasing the similarity between augmented samples of the same EEG data while decreasing that of the different ones  [34] -  [36] .\n\nPseudo-labeling  [21]  is another semi-supervised approach that encourages the model to obtain lower entropy predictions on the unlabeled data. In this method, the network (trained on labeled data) is used to generate pseudo-labels for unlabeled data, and the network is then retrained with both labeled and pseudo-labeled data. This efficient semi-supervised paradigm outperformed many supervised techniques with small amounts of labeled training samples  [21] .\n\nConsistency regularization claims that a label should remain consistent even after a perturbation or augmentation is applied on its data, which has been widely used in semi-supervised learning  [14] -  [16] . The most common regularization approaches that can be applied on both unlabeled and labeled data are stochastic augmentations (e.g., Gaussian noise) imposed on inputs and a dropout layer applied in the network. Π-model  [14]  encourages the network to give consistent outputs for two augmentations on the same input. Temporal ensembling  [14]  uses a similar consistency regularization technique and aggregates the network predictions from previous training epochs. Mean teacher  [15]  is also built based on the Π-model and works by aggregating the network weights from previous training batches.\n\nRecently, a few complex pipelines, also known as 'holistic' approaches, have been proposed by combining various elements of the SSL ideas described above. For example, MixMatch  [17]  utilizes consistency regularization and label-entropy minimization based on several techniques such as MixUp  [37]  and prediction sharpening. Compared to MixMatch, FixMatch  [18]  provides a simplified solution by applying a customized threshold for choosing high quality pseudo-labels. AdaMatch  [19]  encourages the label distribution of predictions on unlabeled data to become closer to that of labeled data. It also uses a dynamic threshold based on the network's confidence on predictions of labeled data to choose high quality pseudo-labels for unlabeled data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Semi-Supervised Eeg Emotion Recognition",
      "text": "Semi-supervised learning has been very rarely studied in the context of EEG-based emotion recognition. Recently, in  [38] , we proposed a deep semi-supervised architecture with an attention-based recurrent autoencoder for EEG learning. We also re-implemented several popular SSL pipelines equipped with deep learning techniques, namely unsupervised pre-training  [29] , pseudo-labeling  [21] , Π-model  [14] , temporal ensembling  [14] , and mean teacher  [15] , which were all originally proposed in the field of computer vision. We compared our framework to the aforementioned SSL methods on a large-scale EEG emotion dataset, SEED. Very lately, in  [39] , we adapted and implemented three recent stateof-the-art holistic SSL methods, MixMatch  [17] , FixMatch  [18] , and AdaMatch  [19]  for EEG representation learning. We also compared these three holistic SSL techniques to the other five classical SSL methods, Π-model  [14] , temporal ensembling  [14] , mean teacher  [15] , convolutional autoencoder  [20] , and pseudolabeling  [21]  on two popular emotion recognition datasets, SEED and SEED-IV. Our study demonstrated the great potential for semi-supervised EEG-based emotion recognition, and motivated this study to develop novel semi-supervised methods designed for the field of EEG representation learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Solution",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Setup",
      "text": "For a classification problem with k emotion categories, let us denote Our goal is to propose a robust pipeline to improve the model's performance on emotion recognition by leveraging large amounts of D u when limited D l are available. We are interested in multiple few-labeled scenarios when M N , including a barely supervised scenario, where only one sample per class is labeled (m = 1).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Solution Overview",
      "text": "We aim to propose a novel SSL architecture for EEG-based emotion recognition, while reducing the distribution mismatch between large unlabeled and small labeled data. To achieve this, we first apply two types of augmentations (strong and weak) on both labeled and unlabeled EEG data. Following that, we compute the model's averaged predictions on both the original and augmented unlabeled data. To reduce the entropy of the averaged predictions, we further employ a sharpening process and use the sharpened prediction as guessed labels for all the unlabeled data. Next, we use convex combinations of pairwise labeled and unlabeled data to form a new set. After that, we simultaneously perform emotion recognition and pairwise representation alignment by training an emotion classifier and a domain discriminator (labeled vs. unlabeled) on the new interpolated set. The overview of our proposed solution is shown in Figure  2 . In the following subsections, we describe each step for our proposed method in details.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Our Approach",
      "text": "In order to efficiently train our model using the entire large set of unlabeled data (D u ) together with the small amounts of labeled data (D l ), we first replicate D l by N/M times to make it the same size as D u . In each training iteration, we randomly select batches of samples (x l b , y l b ) ∈ D l from the labeled dataset and (x u b ) ∈ D u from the unlabeled set, respectively. Next, we perform data augmentation on each data batch as described below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation",
      "text": "Data augmentation is an important step in semi-supervised learning. In fundamental SSL literature  [17] -  [19] ,  [40] , data augmentation methods generally include (1) geometric transformations, and (2) noise addition. Geometric transformations, such as flipping, rotation, and scaling, are not suitable for time-series data such as EEG  [41] . Possible types of added noise include salt-andpepper, Poisson, and Gaussian  [41] . Salt-and-pepper and Poisson noise are not recommended for EEG data augmentation since they might alter the EEG's intrinsic features  [41] ,  [42] . Therefore, we adopt additive Gaussian noise which has been widely used for data augmentation in both recent EEG studies  [34] ,  [41] -  [44]  and other domains  [17] ,  [40] .\n\nWe first apply strong and weak augmentations on both labeled and unlabeled data. For each x l b and x u b in the training set of D l and D u , we generate the augmented data as A s/w (x b ) = x b + N (µ, σ), where x b ∼ [0, 1] is the normalized input, and N is a Gaussian distribution with µ = 0.5 which is consistent with the mean value of the normalized input. The strength of the augmentation can be tuned by changing σ. We choose 0.8 and 0.2 as σ in the additive Gaussian noise for strong (A s ) and weak (A w ) augmentations respectively, as suggested in  [26] ,  [45] . Following data augmentation, label-guessing is required to obtain pseudolabels for both the original and augmented unlabeled data, which we describe below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label-Guessing",
      "text": "After data augmentation, we enlarge the unlabeled set of data by concatenating the original (x u b ), weakly (A w (x u b )), and strongly\n\n) augmented unlabeled samples. We then pass-forward the unlabeled data to an encoder (G) and a classifier (C) to generate pseudo-labels (p b ) by averaging the predictions as:\n\nwhere x u,r b for r = 1, 2, 3, denotes x u b , A w (x u b ), and A s (x u b ), respectively. Here, p m represents the model prediction, and θ G and θ C denote the model's parameters for the encoder and classifier, respectively. Then, to minimize the entropy of the pseudo-labels, we apply a sharpening operation by:\n\nwhere T = 1 is the temperature hyper-parameter used to adjust the entropy level of the pseudo-labels (p b ), as suggested in  [17] .\n\nIn addition to emotion pseudo-labels, we also assign binary domain (labeled Vs. unlabeled) labels (z u ) to the unlabeled set (G u ) as:\n\nwhere , denotes the concatenation of two or more sets. Similarly, we enlarge the labeled data with the concatenation of the original (x l b ), weakly (A w (x l b )), and strongly (A s (x l b )) augmented labeled samples. Since the augmentation should not alter the labels of the data, we employ the ground truth of the original labeled data as labels for the weakly-and stronglyaugmented labeled data. Moreover, similar to the unlabeled set (G u ), we also assign binary domain labels (z l ) to the labeled set (G l ) as:\n\nwhere\n\n| denotes the size of a set). The data and corresponding labels from both G l and G u are then used for calculating pairwise convex combinations, which will be discussed in the following.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Mixup",
      "text": "The generalization performance of a model could degrade due to the adversarial examples whose model predictions could be easily altered by small perturbations (e.g., Gaussian noise used for our data augmentations)  [46] ,  [47] . To tackle this problem, we encourage using convex combinations between pairs of samples, as inspired by a powerful method called MixUp  [37] . Specifically, we generate a new training set by using the convex combinations of the labeled and unlabeled set (G l , G u ), according to:\n\nwhere B is the Beta distribution with α, β of 0.25, as suggested in  [17] ,  [37] ,  [48] , and λ ∈ [0, 1]. This matching is performed between randomly selected pairs of the labeled and unlabeled samples. We determine the binary domain label through a rounding operation applied on Eq. 8 as zb . Finally, a new combined set containing triplets of (x b , ỹb , zb ) will be used in our pairwise representation alignment step.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pairwise Representation Alignment",
      "text": "Following MixUp, we align the distribution of the pairwise labeled and unlabeled embeddings, that are obtained by training the encoder using the newly interpolated triplet of (x b , ỹb , zb ).\n\nTo measure the distribution divergence between the labeled and unlabeled data, we use:\n\nbased on  [48] ,  [49] , in which x i ∈ xb and z i ∈ zb . θ D is the model parameters for the discriminator (D). The min() operation is applied on the discriminator's prediction error (labeled vs. unlabeled) so that distribution divergence would be larger when the prediction error is smaller. Parameters θ G and θ D need to be optimized during this operation. We minimize this distribution divergence in order to encourage the encoder (G) to align the EEG representations of labeled and unlabeled data as:\n\nAs suggested in  [48] ,  [49] , we optimize the max-min problem by adding a Gradient Reverse Layer (GRL) before the D in order to reverse the gradient in G, as shown in Figure  2 . Specifically, GRL is used to reverse the sign of the gradients during backpropagation so that the discriminator loss can be directly minimized using the existing optimization algorithms (e.g., Adam  [50] )  [51] . Finally, we train both the emotion classifier and the domain discriminator by minimizing the emotion classification loss:\n\nand domain discriminator loss:\n\nwhere H(p, q) = -p(x) log q(x) represents cross-entropy of p and q. We use an adversarial training strategy to minimize the distance between the labeled and unlabeled representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Total Loss Function",
      "text": "Our total loss function comprises three parts. The first term is a supervised loss\n\nthat has been commonly used in many SSL literature. We adopt the unsupervised loss L u used in  [18] , as the second term as follows:\n\nwhere\n\n. Consequently, we update the total loss as:\n\nwhere δ = 1.0. Instead of using a pre-defined threshold τ  [18]  that may need to be tuned for each dataset individually, we apply a warm-up function η on the unsupervised loss, similar to  [19] , as follows:\n\nwhere t and T are the current and the maximum iterations. The warm-up function is used to slowly increase the weight of the unsupervised loss when the model is being trained further as well as being more confident on its predictions. The third term is the sum of the classification and discriminator losses trained on the interpolated set as mentioned earlier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Architecture Details",
      "text": "Here we describe the architecture details of the different modules, namely encoder, classifier, and discriminator, used in our proposed method, as shown in Table  1 . The encoder consists of two 1-D convolutional blocks, where each block contains a 1-D convolutional layer followed by a 1-D batch normalization layer and a LeakyReLU activation. The classifier and discriminator share the same architecture containing two fully connected layers with a dropout rate of 0.5. The encoder is used to transform EEG inputs into a learned embedding, while the classifier is used for identifying emotion categories, and the discriminator is used to determine whether the input data are labeled or unlabeled. In Table  1 , s denotes the total number of EEG features, and k is the number of emotion categories.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup And Settings",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We use the following four datasets in our study. In all the dataset mentioned below, the EEG electrodes were placed using 10 -20 system.   [11] . As stimuli, 72 film clips with four emotions (sad, fear, happy and neutral) were selected. The experiments were completed by 15 individuals, consisting of 8 females and 7 males. Each participant repeated the experiment three times, with completely different stimuli each time. Each experiment has a total of 24 trials (6 trials for each emotion), where each trial has three stages: 5 seconds of start hint, 2 minutes of film clip, followed by 45 seconds of self-assessment. 62 EEG recordings were collected at a sampling frequency of 1000Hz.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Seed-V",
      "text": "The SEED-V dataset was collected by Liu et al.  [12] . 45 short videos with five emotions (happy, fear, neutral, sad, and disgust) were chosen as stimuli. The studies were completed by a total of 16 participants, 10 females and 6 males. Each participant repeated the experiment three times, with completely different stimuli each time. Each experiment includes 15 trials (3 trials for each emotion), where each trial has three stages: 15 seconds of start hint, 2 -4 minutes of film clip, and 15 or 30 seconds of self-assessment. In total, 62 EEG recordings were collected with a sampling frequency of 1000Hz.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Amigos",
      "text": "The AMIGOS dataset was developed by Correa et al.  [13] .   [10] . In SEED-V, principle component analysis was used to remove ocular artefacts from EEG by  [12] ,  [53] . Similarly in AMIGOS, blind source separation was employed to remove the eye artefact  [13] . At last, a band-pass filter was applied to filter the noise and artefacts outside the dominant EEG frequency range. Specifically, the frequency ranges of the bandpass filter are 0.3-50Hz, 1-75Hz, 1-75Hz, and 4-45Hz for SEED, SEED-IV, SEED-V, and AMIGOS datasets, respectively  [10] -  [13] . We use the preprocessed signals made public by the original publications  [10] -  [13]  and do not perform further preprocessing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "Following preprocessing, we use pre-defined EEG segments proposed in the respective dataset publications [10]-  [13] , for feature extraction. In each of the SEED-series datasets, EEG data were divided into segments of the same length with no overlap between adjacent segments. The lengths of EEG segments were set to 1 second for SEED and 4 seconds for SEED-IV and SEED-V, respectively  [10] -  [12] . For AMIGOS, EEG signals corresponding to each video clip were separated into 20-second segments. The first and last segments were selected from the EEG data corresponding to the initial and final 20 seconds of the video clips.\n\nNext, following the first 5 seconds of video clips, EEG data were divided into non-overlapping 20-second segments, yielding 340 EEG segments for each participant  [13] . It should be noted that the extracted features (described below) are reshaped from 2-D to 1-D feature vectors (e.g., [62 channels, 5 features/channel] reshaped to [310 × 1 features]) and further scaled into the range of [0, 1] with min-max normalization before being fed to our proposed framework. The 1-D feature vector begins with features extracted from each frequency band (e.g., delta, theta, alpha, beta, and gamma) of the first EEG channel, and continues with features extracted in the same order from the other EEG channels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Differential Entropy",
      "text": "In SEED-series datasets, DE features were extracted from five EEG bands, notably delta (1 -4Hz), theta (4 -8Hz), alpha (8 -14Hz), beta (14 -31Hz), and gamma (31 -50Hz). Consequently, a total of 62 × 5 = 310 features were extracted. We assume that the signals have a Gaussian distribution, and thus DE is calculated as follows:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Power Spectral Density",
      "text": "In AMIGOS dataset, the logarithm of PSD features were computed from five EEG bands, namely theta (3 -7Hz), slow alpha (8 -10Hz), alpha (8 -13Hz), beta (14 -29Hz), and gamma (30 -47Hz). In addition, the logarithm of PSD asymmetry between 7 symmetric pairs of EEG channels (e.g., F7 and F8) were also extracted as features. As a result, a total of (14 + 7) × 5 = 105 features were extracted. The PSD is calculated as:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Protocols",
      "text": "To evaluate our proposed pipeline, we strictly follow the same evaluation protocols used for emotion recognition in the original articles that published the datasets [10]-  [13] . In SEED, we use the first 9 trials as training data (each emotion class with three trials) and the rest 6 trials as the testing data in each experiment, as defined in  [10] . In SEED-IV, we employ the pre-defined first 16 trials (each emotion class with four trials) as the training set, and the remaining 8 trials as the testing set. In SEED-V, 15 trials were split into three pre-defined groups, each containing 5 trials with all 5 emotions. We concatenate the first group from each of the three experiments to form a new fold of 15 trials. Similarly, we form the other two folds by concatenating the second and third groups from each of the three experiments. Following that, we perform a 3-fold cross-validation as performed in  [12] . Since the class distributions are almost balanced in the SEED-series datasets, accuracy is selected as the evaluation metric [10]-  [12] . In AMIGOS, we adopt the same leave-one-participant-out protocol for training and testing data splits, as used in  [13] . The ratios of negative/positive classes are 0.721/0.279 and 0.805/0.195 for valence and arousal, respectively  [13] ,  [54] . As the class distribution is very imbalanced, we use the F1-score (mean F1score for both classes) as the evaluation metric as suggested in  [13] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "We use a batch size of 8 for the datasets with subject-dependent evaluation (SEED, SEED-IV and SEED-V) and 64 for the dataset with subject-independent evaluation (AMIGOS). We adopt the Adam algorithm  [50]  with a default learning rate of 1e -3 for optimization. We train our model for a total of 30 epochs for all the experiments. We implemented our experiments using PyTorch on a pair of NVIDIA GeForce RTX 2080 Ti GPUs. Similar to other semi-supervised studies  [17] -  [19] , we evaluate PARSE with a varying number of labeled samples per class (m), where m ∈ {1, 3, 5, 7, 10, 25}. For the benchmark methods using a dataset-specific hyper-parameter τ , we perform a smart search in the range of [0.0 -1.0] with a step size of 0.1 on the validation set (not the test set) to find the optimum hyperparameter (τ ). Specifically, in FixMatch, we set τ = 0.9 for SEED and SEED-IV, τ = 0.7 for SEED-V and τ = 0.6 for AMIGOS. In AdaMatch, we set τ = 0.6 for SEED and AMIGOS, τ = 0.5 for SEED-IV, as well as τ = 0.9 for SEED-V.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ssl Benchmarks",
      "text": "For comparison and evaluation of our work, we adapt, implement, and necessary modify three cutting-edge SSL methods, MixMatch  [17] , FixMatch  [18] , and AdaMatch  [19] , in addition to five classical SSL methods, Π-model  [14] , temporal ensembling  [14] , mean teacher  [15] , convolutional autoencoders  [20] , and pseudolabeling  [21] , for EEG representation learning. We implement the aforementioned Π-model, temporal ensembling, mean teacher, pseudo-labeling, and convolutional autoencoder with the same algorithm settings (e.g., loss function, unsupervised loss coefficient, etc.) used in  [38] . Lastly for MixMatch, FixMatch, and AdaMatch, we follow the implementation and hyper-parameter details presented in  [39] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Π-Model",
      "text": "Π-model, as proposed in  [14] , first applies two different augmentations on the same input. It then trains the network with dropout on the two augmented inputs, and enforces an unsupervised consistency by reducing the distance between the two corresponding outputs of the network. The supervised loss is simply the crossentropy calculated between labeled data and the ground truth. Π-model outperformed pseudo-labeling in multiple datasets with small amounts of labeled data  [16] . Π-model, however, trains slowly because it trains on each input twice. Furthermore, because of the single evaluation in each training epoch, the network outputs may be noisy or unreliable  [14] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Temporal Ensembling",
      "text": "Temporal ensembling was proposed as an improved version of the Π-model by training the network only once on each input, and aggregating the predictions from earlier training epochs  [14] . As a result, compared to the Π-model, temporal ensembling requires less training time. Moreover, consistency regularization is applied between the current network output and the aggregated output, which is more reliable  [14] ,  [16] . However, due to the slow update from aggregating the network predictions (once per training epoch), temporal ensembling may not be able to provide promising results when the dataset is too large  [15] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Mean Teacher",
      "text": "In  [15] , a more enhanced version of temporal ensembling, called mean teacher, was proposed. Mean teacher method updates more frequently by aggregating the model weights from each previous training batch instead of ensembling the model predictions from past training epochs. Afterward, the consistency regularization is applied by minimizing the distance between the two outputs of the network with and without the ensembled model weights. Mean teacher method has obtained better results than the Π-model and temporal ensembling in a few datasets  [15] ,  [16] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pseudo-Labeling",
      "text": "In order to generate pseudo-labels, first the model is trained using labeled data. Then, the trained model is used to obtain confident predictions on the unlabeled samples (also called pseudo-labels). Finally, the model is retrained using the entire data with true labels and pseudo-labels together  [21] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Convolutional Autoencoder",
      "text": "We employ the same convolutional encoder and classifier in these benchmarks as in the one used in our proposed method. For the decoder component of the convolutional autoencoder benchmark, we use two transposed convolutional 1-D blocks. In each block, a 1-D transposed convolutional layer is followed by a 1-D batch normalization layer and ReLU activation. In the pre-training stage, the convolutional autoencoder is trained on the unlabeled data to update the weights of each layer and capture a better latent representation of the inputs while minimizing the unsupervised loss. Then, the pre-trained network (e.g., encoder) followed by a classifier is fine-tuned using labeled data in a supervised manner  [30] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mixmatch",
      "text": "MixMatch  [17]  has been built upon two SSL core concepts, namely consistency regularization applied in  [14] -  [16]  and label entropy minimization used in pseudo-labeling  [21] . Specifically, in the first step, data augmentations are applied to both labeled and unlabeled data. In the second step, the augmented unlabeled data are fed forward to a model to obtain the predictions, with no gradients propagated during this step. Following this, the predictions are averaged to obtain the guessed labels. The entropy of the guessed labels' distributions is then minimized using a sharpening method. In the third step, a powerful data augmentation method, named MixUp  [37] , is employed to combine the augmented labeled data with ground truth and the augmented unlabeled data with the guessed labels into a new set  [17] . In the last step, the model is trained by minimizing the cross-entropy loss between the predictions on the labeled data and ground truth, as well as minimizing the L2 loss between the predictions on the unlabeled data and the guessed labels of the new mixed set. MixMatch outperformed Π-model, mean-teacher, and pseudolabeling in multiple datasets given a few labeled examples  [17] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Fixmatch",
      "text": "FixMatch, a more accurate SSL technique, was proposed in  [18] . In comparison to MixMatch  [17] , FixMatch combines consistency regularization and pseudo-labeling in a much simpler manner. Specifically, FixMatch first applies weak and strong augmentations to unlabeled data, and a weak augmentation to labeled data. Then, the weakly-augmented unlabeled data are fed to a model to obtain pseudo-labels. Next, the model is trained on strongly-augmented unlabeled data by minimizing cross-entropy loss between the model's prediction and the pseudo-labels, rather than reducing their squared difference which was used in the previous SSL methods  [14] -  [17]  as the unsupervised loss. During training, a user-defined threshold is employed to ensure that pseudo-labels are only used for unsupervised loss update when the model is confident in its predictions on the unlabeled data. Additionally, the cross-entropy loss between model's predictions on weakly-augmented labeled data and the ground truth is used for the supervised term. FixMatch achieved better performance than the related works discussed above when only a small number of samples were labeled  [18] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Adamatch",
      "text": "AdaMatch, a very recent cutting-edge SSL framework, was proposed in  [19]  to tackle the issue of SSL performance degradation when the class distribution differed between the labeled and unlabeled sets. This problem could ideally be solved by having the same class distribution between the pseudo-labels of the unlabeled data and the actual unlabeled data. However, because the real class distribution of unlabeled data is often unknown, AdaMatch estimates it using the class distribution of labeled data. To do so, AdaMatch first acquires the class distribution of a model's predictions on weakly-augmented labeled data. Similarly, it obtains the predicted class distribution of the model's predictions on weaklyaugmented unlabeled data. Following that, it calculates the ratio of labeled data class distribution to the expected unlabeled data class distribution. This ratio is then used to modify the model's prediction on unlabeled data so that the adjusted class distribution of the unlabeled data would follow the labeled data class distribution. AdaMatch proposes a relative confidence threshold that is not only based on a user-defined value suggested in FixMatch  [18] , but also depends on the model's confidence in its predictions on weakly labeled data. Although, the unsupervised term of AdaMatch is quite similar to the one used in FixMatch  [18] , AdaMatch employs an additional supervised cross-entropy loss between the model's prediction on strongly-augmented labeled data and ground truth. AdaMatch outperformed other approaches, including FixMatch, on various computer vision datasets and achieved state-of-the-art results  [19] .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results And Discussions",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "We compare our proposed framework to other existing methods when only few training samples per class are labeled (m ∈ {1, 3, 5, 7, 10, 25}). This experimental protocol is in accordance to recent state-of-the-art studies on SSL  [17] -  [19] . Each method is evaluated five times for all 6 few-labeled scenarios, each time with a different random seed for the selection of D l . Tables 2, 3, 4, and 5 show the average and standard deviation of the results for SEED, SEED-IV, SEED-V, and AMIGOS (valence and arousal) across 5 different random seeds, respectively. Following, we provide the detailed results and discussions on each dataset.\n\nSEED. As shown in Table  2 , classical SSL methods generally produce inferior performance in all 6 designated few-labeled scenarios, except that convolutional autoencoder consistently outperforms other classical approaches and slightly outperforms Fix-Match. In all of the few-labeled scenarios, AdaMatch consistently  outperforms FixMatch and all classical SSL approaches by achieving the second-best result (shown with underline). Moreover, AdaMatch outperforms MixMatch only with the exception of 5 labeled samples-per-class scenario. In the meantime, the convolutional autoencoder and MixMatch achieve the third-best results for m ∈ {1, 25} and m ∈ {3, 7, 10}, respectively. Our proposed method, PARSE, consistently achieves the best (shown in bold) performance, outperforming the second-best methods by more than 3.0% across all the few-labeled scenarios, demonstrating its superiority in the absence of sufficient labeled samples.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Seed-Iv.",
      "text": "As shown in Table  3 , traditional SSL methods perform worse than other more recent solutions. Temporal ensembling and convolutional autoencoder have comparable performances and produce the best results among classical approaches. MixMatch, FixMatch, and AdaMatch nearly outperform all of the classical methods across all 6 few-labeled scenarios. In particular, when only one sample per class is labeled, PARSE achieves the second-best result, approaching the best method obtained by AdaMatch with only 0.43% difference. Our PARSE achieves the best results when more labeled samples are given (m ∈ {3, 5}). Especially in the case of 3 labeled samples per class, our method outperforms the second-best result (AdaMatch) by 2%. When even more labeled samples are provided, PARSE approaches the highest performance with a very small difference (0.15%, 0.16% and 0.88% for m ∈ {7, 10, 25}).  4 , the classical methods are generally outperformed by other recent techniques, except that temporal ensembling achieves the second-best result when m = 1, and convolutional autoencoder closely approaches the third-best result in case of m = 10. Furthermore, FixMatch and AdaMatch have similar performance across all the few-labeled scenarios. Our proposed architecture achieves the best results when 1 and 3 labeled samples per class are provided, respectively. In cases with more than 3 labeled samples, MixMatch obtains the best results despite its poor performance in the barely supervised scenario (m = 1) while PARSE consistently obtains the second-best results.  5  (Valence), among the classical approaches, mean teacher method consistently achieves the highest performance, followed by temporal ensembling. Mean teacher also outperforms all other approaches in the barely supervised scenario, followed by our proposed method. MixMatch achieves the best result when m = 5 and the secondbest ones when more samples are labeled (m ∈ {7, 10}). Our  3. Average performance of PARSE in comparison to other SSL methods as well as supervised learning method for SEED, SEED-IV, SEED-V, and AMIGOS (valence and arousal), across all 6 few-labeled scenarios (m ∈ {1, 3, 5, 7, 10, 25}).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Seed-V. As Shown In Table",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Amigos (Valence). As Shown In Table",
      "text": "PARSE achieves the best performance when m ∈ {3, 7, 10, 25} and approaches the best result at m = 5 with less than 1% difference.\n\nAMIGOS (arousal). As displayed in Table  5  (Arousal), similar to valence, mean teacher consistently achieves the highest results, followed by temporal ensembling among all of the classical SSL methods. Especially, mean teacher achieves the best result in the barely supervised scenario and obtains the secondbest results among all other methods in cases of more labeled samples (m ∈ {3, 5, 7}). MixMatch achieves the second-best result in the barely supervised setting and achieves the best results in the remaining few-labeled scenarios. It can be observed that for estimating arousal in the AMIGOS, our method does not perform the best in any of the cases, yet consistently obtains competitive performances all around.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Comparison",
      "text": "In Figure  3 , we compare the average performance (across m ∈ {1, 3, 5, 7, 10, 25}) for all the SSL methods as well as a baseline supervised-only method. In this experiment, we evaluate a supervised method including an encoder and classifier with the same architecture used in the SSL methods and trained only on labeled data (without any unlabeled data).\n\nAmong the classical SSL methods with consistency regularization technique, temporal ensembling outperforms the Π-model on all of the datasets. This is mainly due to the more reliable outputs being generated by temporal ensembling methods (as described in Section 4.6.2). Meanwhile, it is interesting to note that mean teacher method performs worse than Π-model and temporal ensembling on SEED-series datasets with balanced class distributions while outperforming both on AMIGOS with highly unbalanced label distributions. We also observed that the pseudolabeling method has moderate performance among all classical SSL methods on SEED-series datasets but performs poorly on AMIGOS (valence and arousal). The large difference in label distribution between labeled and unlabeled data in AMIGOS is likely to causes the network (which was initially trained on labeled data) to generate pseudo-labels with the same class distribution as in the labeled set. Similarly, FixMatch performs well on the datasets with balanced class distributions but poorly on those with unbalanced ones. This is mainly because FixMatch ignores the class distribution mismatch between labeled and unlabeled sets (as described in Section 4.6.7). AdaMatch, unlike FixMatch, takes into account class distribution alignment (as described in Section 4.6.8), delivering good results across all the datasets. Compared to  Here, the size of labeled and unlabeled sets are the same due to the replication process applied to the labeled set to increase their size (see Section 3.3). We observe stronger overlaps between the labeled and unlabeled samples when representation alignment is applied in PARSE, which will eventually result in more confident guessed labels for the unlabeled samples.\n\nFixMatch and AdaMatch, MixMatch does not consider the distribution alignment but encourages convex combinations of labeled and unlabeled samples (as described in Section 4.6.6), leading to more consistent and higher performances across datasets.\n\nCompared to all these cutting-edge semi-supervised solutions, our proposed PARSE considers both the confidence of pseudolabels and the distribution alignment of labeled and unlabeled data. For the confidence of pseudo-labels, we use a warm-up function to slowly increase the unsupervised term's weight so that lowconfidence pseudo-labels are rejected. For pairwise distribution alignment, our proposed module reduces the distribution mismatch between labeled and unlabeled data. As shown in Figure  3 , our novel solution enables the model to perform better than MixMatch on SEED, SEED-IV, and AMIGOS (valance), as well as FixMatch and AdaMatch on all the datasets. Although MixMatch performs better than PARSE on AMIGOS (arousal), our model is much more stable, as evidenced by the considerably larger standard deviation of MixMatch on this dataset (see Table  5",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "(Arousal)).",
      "text": "As shown in Figure  3 , on all the datasets, our proposed method outperforms the supervised-only model by considerable margins. It should be pointed out that unlike PARSE, not all SSL methods consistently outperform the supervised-only approaches. Moreover, a general observation is that the performance of most SSL methods is quite dataset-dependant, and while our performance also varies across different datasets, PARSE achieves more consistent results. This demonstrates the superiority of our method when faced with a scarcity of labeled samples across different",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ablation And Analysis",
      "text": "We perform an ablation study to investigate the impact of the pairwise representation alignment step in our method. Figure  4  shows the performance (mean and standard error) of our proposed method with and without pairwise representation alignment for SEED, SEED-IV, SEED-V, and AMIGOS (valence and arousal).   Over all of the datasets and across all the 6 few-labeled scenarios, we find our model performing consistently better with pairwise representation alignment than without it. Specifically, when the class distribution of a dataset is balanced, the alignment improves PARSE, for example, 5.0%, 4.8%, and 3.0% in SEED, SEED-IV, and SEED-V, respectively. Moreover, when the class distribution is imbalanced, the representation alignment boosts the performance by around 3.5% and 3.7% in AMIGOS (valence) and AMIGOS (arousal), respectively.\n\nFollowing, we explore the learned EEG embeddings for both labeled and unlabeled data to better understand the impact of the representation alignment step in our model. To this end, we use t-distributed Stochastic Neighbor Embedding (t-SNE) to visualize the learned EEG representations. Figure  5  shows a comparison between the representations with and without alignment for the different numbers of labeled samples in SEED-V (containing 5 emotion classes) as examples. We observe in Figures  5 (A ) and (G) that when only 1 labeled sample per class is available, the learned embeddings of the unlabeled data are far apart from the labeled data in the absence of pairwise representation alignment. However, we observe that when alignment is performed, these embeddings (unlabeled and labeled) are brought closer to each other. This is a desired property as it means that the output class information for the labeled samples can confidently be used to guess the labels for the unlabeled samples. We observe a similar, but less significant improvement, when more labeled samples are available (m ∈ {3, 5, 7, 10, 25}), (see Figures  5 (B ) through (F) vs. (H) through (L)). To provide quantitative evidence for this experiment, we employ the Maximum Mean Discrepancy (MMD) to measure the distance between the learned EEG representation of labeled and unlabeled sets. MMD values with and without pairwise representation alignment are compared across all 6 fewlabeled scenarios. As depicted in Figure  6 , MMD is 0.0138 and 0.5853 with and without alignment when only 1 labeled sample per class is available, highlighting the importance of representation alignment in the scarcely supervised situation. When more labeled samples are provided, however, MMD without alignment falls to 0.0726 at m = 3 but remains nearly constant thereafter (e.g., MMD of 0.0650 at m = 5). In all the few-labeled scenarios except for the barely supervised setting (m > 1), MMD values with alignment are consistently around 10× lower than MMD values without it. Our proposed method with pairwise representation alignment brings the labeled and unlabeled data closer to each other, particularly in the barely supervised scenario, resulting in consistent and improved performance.\n\nNext, we study the effect of data augmentation in our proposed framework. Figure  7  shows the performance (mean and standard error) of our proposed method with and without data augmentation by additive noise for SEED, SEED-IV, SEED-V, and AMIGOS (valence and arousal). We find that data augmentation improves the performance of our model in all the five datasets. In-line with previous research on semi-supervised learning  [14] ,  [15] ,  [17] -  [19] , this improvement is likely due to two reasons. First, data augmentation is a necessary component of reliable pseudo-label generation of unlabeled data (Eq. 1)  [17] . Second, we use model prediction of strongly augmented unlabeled data rather than that of the original unlabeled data for unsupervised consistency loss results for this experiment are depicted in Figure  10  (middle). Lastly, the experiment is repeated with SEED-V being selected as the labeled set, while SEED and SEED-IV are used without the labels for training, as evaluation is done on SEED-V. The results for this experiment are shown in Figure  10  (right). From these experiments, we conclude that PARSE performs very well and obtains competitive results to single-dataset tests, even when incorporating the set of unlabeled data from other datasets. Lastly, we perform further experiments to evaluate the effect of different hyper-parameter values on our model's performance. As described earlier (Eq. 15), we use a warm-up function to gradually increase the weight η applied to the unsupervised term. Here we evaluate the impact of this parameter in the final performance. Even though our warm-up function only selects η values in the range of 0 to 1, manually selecting higher values is possible. Accordingly, we conduct our experiment by selecting η = 0.1, 1.0, 5.0, and compare them to ours. As illustrated in Figure  11  (A), PARSE shows slight sensitivity to this parameter, and while small variations are observed by selecting different values, the warm-up function does provide the best results. Another parameter used in our model is δ, which is the weight applied on the pairwise representation alignment loss term (Eq. 14). To evaluate the impact of this parameter, we compare the performance of our method for different values of δ by setting it to 0.1, 0.5, 1.0, 5.0. As shown in Figure  11  (B), similar to η, δ does not have a significant impact on the performance of our model. Nonetheless, δ = 1.0 does achieve the best performance, which we selected for our model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this research, we propose a novel semi-supervised method for EEG-based emotion recognition. Our model relies on data augmentation, label guessing, convex combinations of unlabeled and labeled sets, and pairwise representation alignment between the distributions of unlabeled and labeled data. We conduct extensive experiments against a number of other methods, with only 1, 3, 5, 7, 10, and 25 labeled samples per class, and evaluate the performance on four large publicly available datasets, SEED, SEED-IV, SEED-V, and AMIGOS (valence and arousal). Our proposed framework achieves average best results across all 6 labeled scenarios in SEED, SEED-IV, and AMIGOS (valence), and closely approaches the best result with only a 0.3% difference in SEED-V. Our method also reaches the second-best performance on AMIGOS (arousal). In addition, we show the impact of the pairwise representation alignment on our proposed method, with the varying number of labeled samples across all the datasets. We perform further analysis that shows our pairwise representation alignment considerably reduces the distance between labeled and unlabeled representations, especially in the barely supervised scenarios. The results also show that our framework consistently outperforms the supervised-only method, addressing the challenge of scarcity of labeled EEG data.\n\nIn-line with prior works, our analysis above shows that large amounts of labeled data are required for supervised EEG representation learning to achieve reliable performance. In practice, labeling each subject's EEG data is costly and time demanding. Moreover, the collected labels might be noisy and incomplete. Our proposed semi-supervised approach requires substantially less number of labeled samples to obtain a similar performance to fully-supervised models trained on large-scale labeled datasets. As a result, our proposed solution can alleviate the burden of future data annotation and labeling, either by self-reporting from the subjects or external professional annotators.\n\nFor future work, the challenges of potential label distribution mismatch and domain difference across datasets can be further explored and addressed. Moreover, the combination of partial label learning and semi-supervised learning can be investigated to alleviate the time-consuming process of assigning candidate labels to large amounts of training examples.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: First, EEG is collected from the subject",
      "page": 2
    },
    {
      "caption": "Figure 1: An overview of our proposed semi-supervised EEG learning",
      "page": 2
    },
    {
      "caption": "Figure 2: In the following",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of our proposed semi-supervised framework with pairwise representation alignment, PARSE, for emotion recognition.",
      "page": 4
    },
    {
      "caption": "Figure 2: Speciﬁcally,",
      "page": 5
    },
    {
      "caption": "Figure 3: Average performance of PARSE in comparison to other SSL methods as well as supervised learning method for SEED, SEED-IV, SEED-V,",
      "page": 10
    },
    {
      "caption": "Figure 3: , we compare the average performance (across m ∈",
      "page": 10
    },
    {
      "caption": "Figure 4: The performance of PARSE with and without representation alignment, across varying number of labeled samples per class (m ∈",
      "page": 11
    },
    {
      "caption": "Figure 5: Visual comparison, using t-SNE, between learned EEG embeddings without representation alignment (1st row) and with representation",
      "page": 11
    },
    {
      "caption": "Figure 3: , on all the datasets, our proposed",
      "page": 11
    },
    {
      "caption": "Figure 6: Quantitative evaluation of the distances between labeled and",
      "page": 11
    },
    {
      "caption": "Figure 4: shows the performance (mean and standard error) of our proposed",
      "page": 11
    },
    {
      "caption": "Figure 7: The performance of PARSE with and without data augmentation by additive Gaussian noise, across varying number of labeled samples per",
      "page": 12
    },
    {
      "caption": "Figure 8: The average performance of PARSE in comparison to other",
      "page": 12
    },
    {
      "caption": "Figure 5: shows a comparison",
      "page": 12
    },
    {
      "caption": "Figure 6: , MMD is 0.0138 and",
      "page": 12
    },
    {
      "caption": "Figure 7: shows the performance (mean and standard",
      "page": 12
    },
    {
      "caption": "Figure 9: The performance of PARSE in comparison to supervised-only method, across varying number of labeled samples per class (m ∈",
      "page": 13
    },
    {
      "caption": "Figure 8: shows the",
      "page": 13
    },
    {
      "caption": "Figure 7: ), we can further improve",
      "page": 13
    },
    {
      "caption": "Figure 8: ), validating the effectiveness of our",
      "page": 13
    },
    {
      "caption": "Figure 9: , where we observe that",
      "page": 13
    },
    {
      "caption": "Figure 10: Average performance of PARSE in cross-dataset experiments,",
      "page": 13
    },
    {
      "caption": "Figure 3: where despite",
      "page": 13
    },
    {
      "caption": "Figure 11: The impact of different hyperparameters, η (A) and δ (B), on the",
      "page": 14
    },
    {
      "caption": "Figure 10: (middle).",
      "page": 14
    },
    {
      "caption": "Figure 10: (right). From",
      "page": 14
    },
    {
      "caption": "Figure 11: (A), PARSE shows slight sensitivity to this parameter,",
      "page": 14
    },
    {
      "caption": "Figure 11: (B), similar to η,",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Module": "Input",
          "Layer details": "-",
          "Output shape": "(1, s)"
        },
        {
          "Module": "Encoder",
          "Layer details": " \n \nConv1D, kernel(3), output channels(5)\nBatchNorm1D, output channels(5)\nLeakyReLU, slope(0.3)",
          "Output shape": "(5, s − 2)"
        },
        {
          "Module": "",
          "Layer details": " \n \nConv1D, kernel(3), output channels(10)\nBatchNorm1D, output channels(10)\nLeakyReLU, slope(0.3)",
          "Output shape": "(10, s − 4)"
        },
        {
          "Module": "Embedding",
          "Layer details": "Flatten",
          "Output shape": "10 × (s − 4)"
        },
        {
          "Module": "Classiﬁer",
          "Layer details": " \n \nLinear, 64\nReLU, −\nDropout, 0.5\nLinear, k",
          "Output shape": "(k)"
        },
        {
          "Module": "Discriminator",
          "Layer details": " \n \nLinear, 64\nReLU, −\nDropout, 0.5\nLinear, 2",
          "Output shape": "(2)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: (Arousal) and Figure 3 where despite",
      "data": [
        {
          "PARSE\nSupervised-Only": "Fully-Supervised"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The accuracy (in %) of PARSE in comparison to other semi-supervised methods on SEED-V dataset. Method 1 label 3 labels 5 labels 7 labels 10 labels 25 labels Π-model",
      "venue": "The accuracy (in %) of PARSE in comparison to other semi-supervised methods on SEED-V dataset. Method 1 label 3 labels 5 labels 7 labels 10 labels 25 labels Π-model"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "R Picard",
        "Affective Computing"
      ],
      "year": "2000",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Facial emotion recognition using light field images with deep attention-based bidirectional lstm",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Knowing what to listen to: Early attention for deep speech representation learning",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Knowing what to listen to: Early attention for deep speech representation learning",
      "arxiv": "arXiv:2009.01822"
    },
    {
      "citation_id": "5",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Attentive cross-modal connections for deep multimodal wearable-based emotion recognition",
      "authors": [
        "A Bhatti",
        "B Behinaein",
        "D Rodenburg",
        "P Hungler",
        "A Etemad"
      ],
      "venue": "Attentive cross-modal connections for deep multimodal wearable-based emotion recognition",
      "arxiv": "arXiv:2108.02241,2021.1"
    },
    {
      "citation_id": "7",
      "title": "Human emotion recognition using deep belief network architecture",
      "authors": [
        "M Hassan",
        "M Alam",
        "M Uddin",
        "S Huda",
        "A Almogren",
        "G Fortino"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "Distilling eeg representations via capsules for affective computing",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Distilling eeg representations via capsules for affective computing",
      "arxiv": "arXiv:2105.00104"
    },
    {
      "citation_id": "9",
      "title": "Classification of hand movements from eeg using a deep attention-based lstm network",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Sepas-Moghaddam",
        "Y Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "10",
      "title": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "arxiv": "arXiv:2008.08633"
    },
    {
      "citation_id": "11",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "12",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "14",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Temporal ensembling for semi-supervised learning",
      "authors": [
        "L Samuli",
        "A Timo"
      ],
      "year": "2009",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "16",
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "authors": [
        "A Tarvainen",
        "H Valpola"
      ],
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "17",
      "title": "Realistic evaluation of deep semi-supervised learning algorithms",
      "authors": [
        "A Oliver",
        "A Odena",
        "C Raffel",
        "E Cubuk",
        "I Goodfellow"
      ],
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "18",
      "title": "Mixmatch: A holistic approach to semi-supervised learning",
      "authors": [
        "D Berthelot",
        "N Carlini",
        "I Goodfellow",
        "N Papernot",
        "A Oliver",
        "C Raffel"
      ],
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "19",
      "title": "Fixmatch: Simplifying semisupervised learning with consistency and confidence",
      "authors": [
        "K Sohn",
        "D Berthelot",
        "N Carlini",
        "Z Zhang",
        "H Zhang",
        "C Raffel",
        "E Cubuk",
        "A Kurakin",
        "C.-L Li"
      ],
      "year": "2009",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "20",
      "title": "Adamatch: A unified approach to semi-supervised learning and domain adaptation",
      "authors": [
        "D Berthelot",
        "R Roelofs",
        "K Sohn",
        "N Carlini",
        "A Kurakin"
      ],
      "year": "2009",
      "venue": "Adamatch: A unified approach to semi-supervised learning and domain adaptation",
      "arxiv": "arXiv:2106.04732"
    },
    {
      "citation_id": "21",
      "title": "Caesnet: Convolutional autoencoder based semi-supervised network for improving multiclass classification of endomicroscopic images",
      "authors": [
        "L Tong",
        "H Wu",
        "M Wang"
      ],
      "year": "2009",
      "venue": "Journal of the American Medical Informatics Association"
    },
    {
      "citation_id": "22",
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "authors": [
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Workshop on challenges in representation learning"
    },
    {
      "citation_id": "23",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "25",
      "title": "Continuous vigilance estimation using lstm neural networks",
      "authors": [
        "N Zhang",
        "W.-L Zheng",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "26",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "27",
      "title": "Capsule attention for multimodal eeg-eog representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "28",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Why does unsupervised pre-training help deep learning",
      "authors": [
        "D Erhan",
        "A Courville",
        "Y Bengio",
        "P Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "31",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "J Van Engelen",
        "H Hoos"
      ],
      "year": "2008",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised pre-training with seq2seq reconstruction loss for deep relation extraction models",
      "authors": [
        "Z Li",
        "L Qu",
        "Q Xu",
        "M Johnson"
      ],
      "year": "2016",
      "venue": "Proceedings of the Australasian Language Technology Association Workshop"
    },
    {
      "citation_id": "33",
      "title": "Unsupervised pretraining for sequence to sequence learning",
      "authors": [
        "P Ramachandran",
        "P Liu",
        "Q Le"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Affective states classification using eeg and semi-supervised deep learning approaches",
      "authors": [
        "H Xu",
        "K Plataniotis"
      ],
      "year": "2016",
      "venue": "IEEE 18th International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Contrastive representation learning for electroencephalogram classification",
      "authors": [
        "M Mohsenvand",
        "M Izadi",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Machine Learning for Health"
    },
    {
      "citation_id": "36",
      "title": "Bendr: using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data",
      "authors": [
        "D Kostas",
        "S Aroca-Ouellette",
        "F Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "37",
      "title": "Uncovering the structure of clinical eeg signals with self-supervised learning",
      "authors": [
        "H Banville",
        "O Chehab",
        "A Hyvärinen",
        "D.-A Engemann",
        "A Gramfort"
      ],
      "year": "2021",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "38",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "39",
      "title": "Deep recurrent semi-supervised eeg representation learning for emotion recognition",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Holistic semi-supervised approaches for eeg representation learning",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Holistic semi-supervised approaches for eeg representation learning",
      "arxiv": "arXiv:2109.11732"
    },
    {
      "citation_id": "41",
      "title": "On data-augmentation and consistencybased semi-supervised learning",
      "authors": [
        "A Ghosh",
        "A Thiery"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "42",
      "title": "Data augmentation for eeg-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "F Wang",
        "S -H. Zhong",
        "J Peng",
        "J Jiang",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "International conference on multimedia modeling"
    },
    {
      "citation_id": "43",
      "title": "Data augmentation for deeplearning-based electroencephalography",
      "authors": [
        "E Lashgari",
        "D Liang",
        "U Maoz"
      ],
      "year": "2020",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "44",
      "title": "A channel-projection mixed-scale convolutional neural network for motor imagery eeg decoding",
      "authors": [
        "Y Li",
        "X.-R Zhang",
        "B Zhang",
        "M.-Y Lei",
        "W.-G Cui",
        "Y.-Z Guo"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "45",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "46",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349"
    },
    {
      "citation_id": "47",
      "title": "Interpolation consistency training for semi-supervised learning",
      "authors": [
        "V Verma",
        "A Lamb",
        "J Kannala",
        "Y Bengio",
        "D Lopez-Paz"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "48",
      "title": "How does mixup help with robustness and generalization?",
      "authors": [
        "L Zhang",
        "Z Deng",
        "K Kawaguchi",
        "A Ghorbani",
        "J Zou"
      ],
      "year": "2020",
      "venue": "How does mixup help with robustness and generalization?",
      "arxiv": "arXiv:2010.04819"
    },
    {
      "citation_id": "49",
      "title": "Semi-supervised learning by augmented distribution alignment",
      "authors": [
        "Q Wang",
        "W Li",
        "L Gool"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "50",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "51",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "52",
      "title": "Collaborative and adversarial network for unsupervised domain adaptation",
      "authors": [
        "W Zhang",
        "W Ouyang",
        "W Li",
        "D Xu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "53",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "54",
      "title": "Classification of five emotions from eeg and eye movement signals: complementary representation properties",
      "authors": [
        "L.-M Zhao",
        "R Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Engineering"
    },
    {
      "citation_id": "55",
      "title": "Feature extraction and selection for emotion recognition from electrodermal activity",
      "authors": [
        "J Shukla",
        "M Barreda-Angeles",
        "J Oliver",
        "G Nandi",
        "D Puig"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}