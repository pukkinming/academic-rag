{
  "paper_id": "2501.08182v1",
  "title": "Cg-Mer: A Card Game-Based Multimodal Dataset For Emotion Recognition",
  "published": "2025-01-14T15:08:56Z",
  "authors": [
    "Nessrine Farhat",
    "Amine Bohi",
    "Leila Ben Letaifa",
    "Rim Slama"
  ],
  "keywords": [
    "Emotion recognition",
    "Spontaneous dataset",
    "Multimodal data",
    "Artificial intelligence",
    "Affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The field of affective computing has seen significant advancements in exploring the relationship between emotions and emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a comprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three primary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the dataset has the potential to incorporate additional modalities, such as Natural Language Processing (NLP) to expand the scope of emotion recognition research. The dataset was curated through engaging participants in card game sessions, where they were prompted to express a range of emotions while responding to diverse questions. The study included 10 sessions with 20 participants (9 females and 11 males). The dataset serves as a valuable resource for furthering research in emotion recognition and provides an avenue for exploring the intricate connections between human emotions and digital technologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions profoundly impact our daily experiences and decision-making processes. Recognizing and understanding emotions is crucial for improving interpersonal interactions and has diverse applications in fields such as humancomputer interaction  [1] , mental healthcare  [2] , market research  [3] , and education  [4] . These applications demonstrate the broad impact of emotion recognition in advancing various communities, including artificial intelligence, computer vision, speech processing, human sciences, and psychology.\n\nThe development and evaluation of emotion recognition models rely on the availability of diverse and high-quality unimodal datasets. In the realm of facial emotion recognition, widely used datasets include AffectNet  [5]  and Fer2013  [6] . For speech emotion recognition, datasets such as RAVDESS  [7]  and EmoDB  [8]  have been utilized. Moreover, we identified the ChaLearn Gesture Challenge (CGC)  [9]  dataset for gesture-based emotion recognition. These unimodal datasets have significantly contributed to advancing emotion recognition models within their respective modalities. However, it is important to acknowledge that unimodal datasets may not cover the complete spectrum of emotions expressed through multiple modalities  [10] . To overcome this limitation, using multimodal datasets has become crucial, as they provide a comprehensive understanding of emotions. These datasets combine audio, visual, physiological signals, and other modalities, offering a holistic representation and allowing for a more in-depth exploration of emotional cues.\n\nIn this paper, we introduce CG-MER, a Card Game-based Multimodal dataset for Emotion Recognition, constructed during sessions involving an emotional card game. Our primary contribution lies in the introduction of a new benchmark accompanied by multimodal annotations. Furthermore, we present an original protocol centered on the emotion game, wherein participants engage in spontaneous and convivial conversations, expressing various emotions while responding to questions that vary in intensity. This paper is organized into three main sections. Section 2 delves into the related work, where we explore human emotion recognition, covering both monomodal and multimodal approaches. In Section 3, we present our CG-MER dataset, providing comprehensive details on its setup, experimental procedure, data description, and annotation process. Finally, Section 4 concludes the paper, summarizing our findings and discussing the perspectives.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we will provide a concise overview of recent studies concerning the identification of human emotions. Our primary focus will be on the utilization of multimodal datasets, which have played a vital role in evaluating the effectiveness of deep learning models in the field of emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Human Emotion Recognition",
      "text": "Emotion recognition is a prominent research area with potential impacts on understanding human behavior and enhancing human-computer interactions. It involves multiple modalities such as facial expressions, speech, gestures, and physiological signals. In the domain of facial emotion recognition, extensive surveys have been conducted, providing comprehensive overviews of techniques and advancements. Notably, Zhang et al.  [11]  conducted an in-depth survey that explored various facial expression recognition techniques. Speech emotion recognition has also been extensively investigated, with surveys like the one conducted by Abbaschian et al.  [12]  delving into feature extraction, classification techniques, and relevant databases, offering valuable insights into the domain. Furthermore, Khalil et al.  [13]  provided an overview of deep learning techniques and their applications in speech-based emotion recognition. Gesture-based emotion recognition has also received considerable attention, and surveys, such as the one conducted by Noroozi et al.  [14] , have provided valuable insights into this modality. With the advent of multimodal approaches, surveys focusing on fusion techniques and challenges have emerged as well. For instance, Koromilas et al.  [15]  reviewed the state-of-the-art multimodal speech emotion recognition methodologies, with a particular emphasis on integrating audio, text, and visual information, another survey  [16]  has presented the basic research in the field and the recent advances into the emotion recognition from facial, voice, and physiological signals, where the different modalities are treated independently. These surveys contribute significant insights into the advancements and potential of multimodal emotion recognition, highlighting the benefits of leveraging multiple modalities to enhance accuracy and robustness in emotion recognition models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition Datasets",
      "text": "This subsection provides an overview of well-known multimodal datasets that have been widely utilized for multimodal emotion recognition in the literature.\n\nIn  [17] , the authors introduced the K-EmoCon dataset, which encompasses videos, vocal audio, and biosignals recorded during debates involving 32 individuals. This dataset covers 5 emotional classes. Another notable dataset, AMIGOS  [18] , focuses on group affect, personality, and mood. It comprises facial expressions, audio recordings, and physiological responses collected from 40 participants across 7 emotional classes. LUMED  [19] , a multimodal dataset capturing simultaneous audiovisual data from 13 participants. The dataset includes selected web-based video clips and covers 3 basic emotional classes. The MELD dataset was presented in  [20] , featuring dialogue excerpts from the sitcom Friends along with textual, audio, and video recordings. This dataset involves 7 participants and spans 7 emotional classes. IEMOCAP  [21]  introduced by Busso et al., offers audiovisual and motion data from dyadic sessions involving 10 actors with 5 emotional classes. Additionally, the EMOFBVP dataset, outlined in  [22] , captures the expressions of 10 actors through facial and vocal expressions, body gestures, and physiological signals, covering 23 different emotional classes. In addition, the MSP-IMPROV dataset, presented by Caridakis et al.  [23] , that serves to study non-verbal behavior across different cultures specifically, German, Greek, and Italian. The dataset comprises 51 participants who express emotions through gestures and speech while reading Velten sentences. Recently, a large audio-visual dataset denoted Empathic  [24]  has been developed. This dataset involves 250 speakers from three European countries namely Spain, France and Norway.\n\nAlthough the reviewed datasets have contributed significantly to the field of emotion recognition, they also exhibit certain limitations that warrant further improvements. Some datasets may lack enough participants to train deep learning models effectively, while others might not contain all the necessary modalities with annotated data for robust model training. Additionally, limited classes in some datasets might not cover the full spectrum of emotional states. Furthermore, the absence of a suitable French dataset with adequate simplicity and favorable conditions for usage is also a concern. In Table  1 , we present a comprehensive comparison between our dataset and existing ones, considering factors like participant count, modalities, experiments, annotation methods, and number of classes. Understanding these strengths and limitations allows us to recognize the unique contributions and potential of our dataset in advancing multimodal emotion recognition research.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Proposed Dataset: Cg-Mer",
      "text": "The challenge with existing emotion datasets lies in their limited applicability to real-world contexts, due to induced emotions in controlled environments. Unlike typical datasets that use specific stimuli, CG-MER dataset employs an emotional card game to explore genuine emotions between participants. Its goals are to investigate correlations between emotional stimuli and expressions, understand emotions in social contexts, and capture dynamic emotional experiences. The dataset adheres to ethical standards approved by GDPR (General Data Protection Regulation) in France, with participants providing written consent after reviewing detailed information about data collection, privacy, and data deletion options.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection Setup",
      "text": "The dataset comprises 10 sessions involving 20 participants, including 9 females and 11 males. The age range of the participants spans from 20 to 43 years old, ensuring a diverse representation within the dataset. To introduce the study and emphasize the importance of constructing a multimodal dataset for emotion recognition, we prepared an engaging animation campaign. This campaign served as an introduction, presenting the research topic and emphasizing the significance of developing a multimodal dataset for emotion recognition. To facilitate participant involvement, a structured protocol was implemented. Participants were asked to complete a form, allowing them to choose their preferred partner and select their desired participation time slot. A pre-arranged schedule facilitated the organization and coordination of the participant's involvement in the study. All data collection sessions took place in a dedicated room with the same temperature and illumination conditions. To ensure optimal data capture, two participants were seated across a table, maintaining a comfortable distance between them to facilitate communication (Figure  1 ). To capture facial expressions and movements, two Kinect cameras V1 were positioned at the center of the table, facing each participant.\n\nFigure  1 . The left image depicts a pair of participants engaged in the card game while seated at a table. Two Kinect cameras positioned in the middle of the table were used to capture the participants' facial expressions, upper body movements and voice, as illustrated on the right side in the sample screenshot of the recorded footage.\n\nTo configure the cameras, we installed KinectSDK-v1.8, which included the Kinect driver and runtime for the Windows Software Development Kit (SDK). This SDK enabled developers to create applications that support face, gesture, and voice recognition utilizing the Kinect sensor technology. Additionally, we utilized the Kinect for Windows Developer Toolkit, which provided valuable resources such as source code samples and tools, streamlining the development of Kinect for Windows applications. During the data collection sessions, we utilized OBS Studio (Version: 29.1.1) 1  to capture both color and depth videos for each participant, allowing us to capture facial expressions and gestures accurately. For audio capture, we employed the Audio Capture Row-Console application available in the Kinect developer kit for each participant.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Collection Procedure",
      "text": "The data collection procedure consisted of three steps to ensure a systematic and organized process. Two experimenters administered each session to configure the cameras and software, to explain the game to participants, and to manage the record during the session.\n\nStep 1: Consent and data usage agreement Upon arrival, each participant was provided with a consent form outlining the purpose of the study and the use of their data, including facial expressions, voice, and gestures, for research purposes. They were given ample time to review the form and provide their written consent for participating in the data collection process.\n\nStep 2: Game explanation and annotation instructions Following the consent process, the participants were briefed about the card game and its rules. They were given a detailed explanation of how the game would unfold and were encouraged to ask any questions for clarification. Additionally, they were introduced to the annotation form, which contained sections for self-annotation and partner annotation for the seven predefined emotions (happiness, anger, neutral, fear, sadness, surprise, and disgust). The participants were guided on how to effectively use the annotation form to record their own emotional experiences and observe their partner's emotions during the card game. Furthermore, the participants were directed to select two cards from each of the three categories (red, yellow, and green), comprising 30 different questions, with 10 questions in each card category (examples of the questions are provided in Table  2 ). Each category was associated with a specific color and score. The green cards, indicated by a score of +1, represented questions with lower intensity and less personal nature. These cards contained simpler and non-personal questions. The yellow cards, marked with a score of +3, had a moderate intensity level and included questions that were slightly more personal. Lastly, the red cards, carrying a score of +5, had the highest intensity and featured questions of a more personal nature. With their selected cards in hand, encompassing a variety of colors and corresponding question types, the participants were prepared to commence the game.\n\nStep 3: Gameplay and emotion annotation During this step, the participants actively engaged in the card game, creating a convivial and spontaneous atmosphere. Unlike a mechanized question-and-answer process, participants had the opportunity to ask additional questions to gain further insights into the given situation. After responding to each question, both participants were encouraged to annotate their own emotional states and provide observations regarding their partner's emotions using the provided annotation form. This interactive and dynamic process enabled the comprehensive collection of data on selfperceived emotions as well as the perceived emotions of their partner throughout the gameplay.\n\nBy following these three steps, we ensured a structured and standardized procedure for data collection during the card game sessions. This approach facilitated the systematic capture of multimodal data and enriched our dataset for further analysis and research on emotion recognition. Avez-vous déjà été confronté(e) à une phobie ou une peur irrationnelle?",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dataset Description",
      "text": "The CG-MER dataset comprises a comprehensive collection of multimodal data obtained from 10 distinct sessions involving an emotional card game, resulting in a total duration of approximately 10 hours of dyadic interactions. Each session was recorded in the form of two audiovisual recordings, saved as mp4 files. These recordings encompassed the RGB video and depth video of each participant, enabling a detailed analysis of facial expressions and body movements. To ensure focused analysis, the videos were carefully cropped using Adobe Premium software to extract the RGB and depth videos as separate entities. Furthermore, continuous annotations of emotions were collected from three distinct perspectives of the subject, the partner, and the external observers. This multimodal approach allowed for a comprehensive understanding of the emotional dynamics at play during card game interactions. In addition to the visual data, audio tracks capturing the participants' speeches were recorded, then saved as individual WAV files. This audio component provides valuable insights into the verbal expressions and vocal cues associated with different emotional states. To provide an overview of the dataset's contents and data collection outcomes, Table  3  summarizes the key details, including the number of sessions, duration, and the various data modalities captured. Table  3 . A summary of the data collection.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). To capture facial",
      "page": 4
    },
    {
      "caption": "Figure 1: The left image depicts a pair of participants engaged in the card game while seated at a table. Two Kinect cameras",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the emotion",
      "page": 6
    },
    {
      "caption": "Figure 2: Emotion distribution across participants in",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of existing multimodal datasets for emotion recognition with our CG-MER dataset: A comprehensive analysis",
      "data": [
        {
          "Context": "Type",
          "Participants": "Number",
          "Modalities": "Visual",
          "Data": "Experiment",
          "Annotation": "Method"
        },
        {
          "Context": "",
          "Participants": "",
          "Modalities": "",
          "Data": "",
          "Annotation": ""
        },
        {
          "Context": "D",
          "Participants": "32",
          "Modalities": "√",
          "Data": "Debate",
          "Annotation": "S,P, E"
        },
        {
          "Context": "I,G",
          "Participants": "40",
          "Modalities": "√",
          "Data": "Watching \nvideos",
          "Annotation": "S,E"
        },
        {
          "Context": "I",
          "Participants": "13",
          "Modalities": "√",
          "Data": "Watching \nvideos",
          "Annotation": "S"
        },
        {
          "Context": "D",
          "Participants": "10",
          "Modalities": "√",
          "Data": "Dyadic \nsessions",
          "Annotation": "S,E"
        },
        {
          "Context": "D,G",
          "Participants": "7",
          "Modalities": "√",
          "Data": "Collect \nvideos",
          "Annotation": "E"
        },
        {
          "Context": "I",
          "Participants": "10",
          "Modalities": "√",
          "Data": "Ask actors to \ndisplay \nemotions",
          "Annotation": "E"
        },
        {
          "Context": "I",
          "Participants": "11",
          "Modalities": "√",
          "Data": "Velten \nphrases",
          "Annotation": "E"
        },
        {
          "Context": "",
          "Participants": "21",
          "Modalities": "",
          "Data": "",
          "Annotation": ""
        },
        {
          "Context": "",
          "Participants": "19",
          "Modalities": "",
          "Data": "",
          "Annotation": ""
        },
        {
          "Context": "I",
          "Participants": "134",
          "Modalities": "√",
          "Data": "Coaching",
          "Annotation": "S,E"
        },
        {
          "Context": "",
          "Participants": "76",
          "Modalities": "",
          "Data": "",
          "Annotation": ""
        },
        {
          "Context": "",
          "Participants": "62",
          "Modalities": "",
          "Data": "",
          "Annotation": ""
        },
        {
          "Context": "D",
          "Participants": "20",
          "Modalities": "√",
          "Data": "Emotional \ncard game",
          "Annotation": "S, P, E"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: ). Each category was associated with a specific color",
      "data": [
        {
          "Card Type": "Green",
          "Question Score": "1",
          "Intensity": "Low",
          "Question": "What is your main center of interest, and why does it fascinate you? \nQuel est votre meilleur centre d’intérêt et pourquoi il vous passionne?"
        },
        {
          "Card Type": "Yellow",
          "Question Score": "3",
          "Intensity": "Medium",
          "Question": "How do you feel when you are alone and no one is talking to you? \nComment vous sentez-vous lorsque vous êtes seul(e) et que personne ne vous parle?"
        },
        {
          "Card Type": "Red",
          "Question Score": "5",
          "Intensity": "High",
          "Question": "Have you ever experienced a phobia or an irrational fear? \nAvez-vous déjà été confronté(e) à une phobie ou une peur irrationnelle?"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: ). Each category was associated with a specific color",
      "data": [
        {
          "Data": "Number of participants",
          "collection summary": "20 (11 males, 9 females)"
        },
        {
          "Data": "Participants age",
          "collection summary": "20 to 43 (mean = 31,5)"
        },
        {
          "Data": "Game sessions duration",
          "collection summary": "∼30 min"
        },
        {
          "Data": "Emotion annotation categories",
          "collection summary": "Surprise, Happiness, Neutral, Anger, Sadness, Fear, Disgust"
        },
        {
          "Data": "Game sessions videos",
          "collection summary": "10h 1min 24sec (of 20 participants)"
        },
        {
          "Data": "Game sessions audios",
          "collection summary": "5h 3min 45sec (of 10 sessions)"
        },
        {
          "Data": "E\nmotions annotations",
          "collection summary": "Self: per question (120 questions for 10 sessions) \nPartner: per question (120 questions for 10 sessions) \n3 external observers: per time range"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Example of an external observer’s annotation for the Figure 2. Emotion distribution across participants in",
      "data": [
        {
          "SESSION": "Participant",
          "Session 1": "P1",
          "Session 2": "P1",
          "Session 3": "P1",
          "total": "-"
        },
        {
          "SESSION": "Happiness",
          "Session 1": "5",
          "Session 2": "3",
          "Session 3": "2",
          "total": "18"
        },
        {
          "SESSION": "Surprise",
          "Session 1": "4",
          "Session 2": "4",
          "Session 3": "4",
          "total": "23"
        },
        {
          "SESSION": "Neutral",
          "Session 1": "13",
          "Session 2": "12",
          "Session 3": "10",
          "total": "74"
        },
        {
          "SESSION": "Anger",
          "Session 1": "-",
          "Session 2": "2",
          "Session 3": "1",
          "total": "16"
        },
        {
          "SESSION": "Sadness",
          "Session 1": "1",
          "Session 2": "2",
          "Session 3": "1",
          "total": "10"
        },
        {
          "SESSION": "Fear",
          "Session 1": "3",
          "Session 2": "-",
          "Session 3": "-",
          "total": "4"
        },
        {
          "SESSION": "Disgust",
          "Session 1": "-",
          "Session 2": "3",
          "Session 3": "-",
          "total": "6"
        },
        {
          "SESSION": "total",
          "Session 1": "26",
          "Session 2": "26",
          "Session 3": "18",
          "total": "151"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning-based facial emotion recognition for human-computer inter-action applications",
      "authors": [
        "M Kalpanan",
        "Chowdary"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "2",
      "title": "Artificial intelligence for mental health care: Clinical applications, barriers, facilitators, and artificial wisdom",
      "authors": [
        "E Lee",
        "J Torous",
        "M Choudhury",
        "C Depp",
        "S Graham",
        "H.-C Kim",
        "M Paulus",
        "J Krystal",
        "D Jeste"
      ],
      "year": "2021",
      "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging"
    },
    {
      "citation_id": "3",
      "title": "Hello marketing, what can artificial intelligence help you with?",
      "authors": [
        "N Wirth"
      ],
      "year": "2018",
      "venue": "International Journal of Market Research"
    },
    {
      "citation_id": "4",
      "title": "Artificial intelligence in education: A review",
      "authors": [
        "L Chen",
        "P Chen",
        "Z Lin"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Others"
      ],
      "year": "2013",
      "venue": "Neural Information Processing: 20th International Conference"
    },
    {
      "citation_id": "7",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "8",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss",
        "Others"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "9",
      "title": "Multi-modal gesture recognition challenge 2013: Dataset and results",
      "authors": [
        "S Escalera",
        "J Gonza`lez",
        "X Baro´",
        "M Reyes",
        "O Lopes",
        "I Guyon",
        "V Athitsos",
        "H Escalante"
      ],
      "year": "2013",
      "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction"
    },
    {
      "citation_id": "10",
      "title": "A survey on databases for multimodal emotion recognition and an introduction to the viri (visible and infrared image) database",
      "authors": [
        "M Siddiqui",
        "P Dhakal",
        "X Yang",
        "A Javaid"
      ],
      "year": "2022",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition based on deep learning: a survey",
      "authors": [
        "T Zhang"
      ],
      "year": "2018",
      "venue": "Advances in Intelligent Systems and Interactive Applications: Proceedings of the 2nd International Conference on Intelligent and Interactive Systems and Applications"
    },
    {
      "citation_id": "12",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamin´ska",
        "T Sapin´ski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "Deep multimodal emotion recognition on human speech: A review",
      "authors": [
        "P Koromilas",
        "T Giannakopoulos"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "16",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "17",
      "title": "Kemocon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "C Park",
        "N Cha",
        "S Kang",
        "A Kim",
        "A Khandoker",
        "L Hadjileontiadis",
        "A Oh",
        "Y Jeong",
        "U Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "18",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Cross-subject multimodal emotion recognition based on hybrid fusion",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu",
        "S Caglar-Ozhan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "2016 IEEE winter conference on applications of computer vision (WACV)"
    },
    {
      "citation_id": "23",
      "title": "A cross-cultural, multimodal, affective corpus for gesture expressivity analysis",
      "authors": [
        "G Caridakis",
        "J Wagner",
        "A Raouzaiou",
        "F Lingenfelser",
        "K Karpouzis",
        "E Andre"
      ],
      "year": "2013",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "24",
      "title": "Perceptual borderline for balancing multi-class spontaneous emotional data",
      "authors": [
        "L Letaifa",
        "M Torres"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "Blazepose: On-device realtime body pose tracking",
      "authors": [
        "V Bazarevsky",
        "I Grishchenko",
        "K Raveendran",
        "T Zhu",
        "F Zhang",
        "M Grundmann"
      ],
      "year": "2020",
      "venue": "Blazepose: On-device realtime body pose tracking",
      "arxiv": "arXiv:2006.10204"
    },
    {
      "citation_id": "26",
      "title": "Graph convolutional networks for assessment of physical rehabilitation exercises",
      "authors": [
        "S Deb",
        "M Islam",
        "S Rahman",
        "S Rahman"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "27",
      "title": "Recording affect in the field: Towards methods and metrics for improving ground truth labels",
      "authors": [
        "J Healey"
      ],
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011"
    },
    {
      "citation_id": "28",
      "title": "Automatic recognition of self-reported and perceived emotion: Does joint modeling help?",
      "authors": [
        "B Zhang",
        "G Essl",
        "E Provost"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    }
  ]
}