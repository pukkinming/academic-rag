{
  "paper_id": "2407.04712v1",
  "title": "International Journal Of Medical Informatics",
  "published": "2024-05-15T19:48:04Z",
  "authors": [
    "Oresti Banos",
    "Zhoe Comas-González",
    "Javier Medina",
    "Aurora Polo-Rodríguez",
    "David Gil",
    "Jesús Peral",
    "Sandra Amador",
    "Claudia Villalonga"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Background: Human Emotion Recognition (HER) has been a popular field of study in the past years. Despite the great progresses made so far, relatively little attention has been paid to the use of HER in autism. People with autism are known to face problems with daily social communication and the prototypical interpretation of emotional responses, which are most frequently exerted via facial expressions. This poses significant practical challenges to the application of regular HER systems, which are normally developed for and by neurotypical people. Objective: This study reviews the literature on the use of HER systems in autism, particularly with respect to sensing technologies and machine learning methods, as to identify existing barriers and possible future directions. Methods: We conducted a systematic review of articles published between January 2011 and June 2023 according to the 2020 PRISMA guidelines. Manuscripts were identified through searching Web of Science and Scopus databases. Manuscripts were included when related to emotion recognition, used sensors and machine learning techniques, and involved children with autism, young, or adults. Results: The search yielded 346 articles. A total of 65 publications met the eligibility criteria and were included in the review. Conclusions: Studies predominantly used facial expression techniques as the emotion recognition method. Consequently, video cameras were the most widely used devices across studies, although a growing trend in the use of physiological sensors was observed lately. Happiness, sadness, anger, fear, disgust, and surprise were most frequently addressed. Classical supervised machine learning techniques were primarily used at the expense of unsupervised approaches or more recent deep learning models. Studies focused on autism in a broad sense but limited efforts have been directed towards more specific disorders of the spectrum. Privacy or security issues were seldom addressed, and if so, at a rather insufficient level of detail.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by a deficit in communication, social interaction, and lack of understanding of emotions. It affects circa 1% of the population and can be detected in the first years of life  [1] . One of the key reasons for the emotional misunderstanding is the inability of people with autism to comprehend prototypical feelings and emotions, which directly af-generally accepted that existing approaches are not definitive. Many of these studies deal with biased data and recognition of emotions such as happiness or fear was only marginally impaired in autism as well as the generalizability of the findings from the currently available data remains unclear  [2, 3] . Furthermore, HER algorithms primarily rely on facial cues, overlooking other important aspects such as body language, vocal tone, and contextual and situational factors that would improve the accuracy of the algorithms  [4] .  [5]  and  [6]  describe new tools as well as computational model to assist people with autism in understanding and operating in the socioemotional world around them. Some findings reveal that children with autism spectrum condition have residual difficulties in this aspect of empathy. In the work of  [7]  authors concluded that relations between particular emotions and human body reactions have long been known, but there remain many uncertainties in selecting measurement and data analysis methods Moreover, it is also observed that a great number of the HER models used in autism are based on data collected from neurotypical people  [8] . Be that as it may, the use of general HER models in autism-related applications poses a number of challenges yet to be addressed and which demand special attention from the scientific community. While there exists a great bulk of systematic reviews addressing the technologies and methods used for emotion recognition in general  [7, [9] [10] [11] , very few focus specifically on its use in autism. In fact, existing systematic reviews in this direction are either centred on a specific technology such as eye-tracking  [12] , robots  [13] , and wearables  [14] , or particular methods like deep learning  [15] . Hence, a comprehensive systematic review focusing on the state of the art on emotion recognition sensing technologies and machine learning methods for autism emotion recognition is presented here. The results of this review will contribute to improve the current techniques for emotion recognition used in autism studies, encourage new research focusing on other conditions of the autism spectrum disorder that have been marginally investigated to date, and promote the use of physiological methods in addition to other traditional behavioural methods as potential emotion recognition modalities to be used in autism. The primary objective of this review was to determine the trends, advances, and challenges on sensing technologies and machine learning methods for emotion recognition in autism. To that end, this review aimed to answer the following research questions:  (1)  What type of sensor technology has been used for emotion recognition in autism?;  (2)  What type of machine learning techniques are most commonly used for emotion recognition in people with autism?; and (3) What are the main challenges in the use of emotion recognition technologies in people with autism? To the best of our knowledge, there are many reviews on autism, on HER, on machine learning methods but very little written about the whole of them and their complementation of these different areas. This is the main novelty of this review. Our study covers all age groups unlike most studies that focus on children. We raised a specific question to identify the main challenges in the use of emotion recognition technologies in autism. We also provide privacy and security aspects including the use of inform consents or approval by ethics committees. Furthermore, we offer a more recent view on the art as its search reaches up to June 2023.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "The PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines  [16]  were followed to perform a systematic review of the literature on sensing technologies and machine learning methods for emotion recognition in autism. The specific methodology followed is described in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eligibility Criteria",
      "text": "This review focused on studies that dealt with sensor technology and machine learning techniques for emotion recognition in children, young, and adults with autism. We did not restrict study location, sample size, gender, age, autism type, type of emotion, emotion recognition modality, devices and sensors, nor algorithms. Studies were eligible to be included in this review if they had three characteristics: 1) they were related to emotion recognition; 2) used sensors and machine learning techniques; and 3) involved children with autism, young or adults.\n\nOther eligibility criteria included: 1) published between January 2011 and June 2023; 2) written in English; 3) scientific article published in a journal or in conference proceedings; and 4) research domain related to computer science or engineering.\n\nStudies were ineligible if affective technology was used in therapy and treatment of patients with autism or in an educational environment. Therefore, we excluded studies related to: 1) robotic treatments or therapies; and 2) social interaction and education.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Information Sources",
      "text": "We conducted electronic searches for eligible studies within the reference databases of Scopus and Web of Science. The search was conducted from 1st January 2011 to 30th June 2023.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Search Strategy",
      "text": "\"Autism\" and \"emotion recognition\"/\"recognition of emotion\" were selected as primal concepts to be searched. In addition to them, synonyms of the \"autism\" term, namely \"autistic\", and the \"emotion\" term, namely \"mood\" and \"affect\", were also considered as they are quite often used interchangeably in this research area. Limits were also applied to the search strategy based on the eligibility criteria. We selected papers published between 2011-2023, published in English computer science or engineering journals or proceedings. The resulting queries eventually run on Scopus and Web of Science are shown below. )) AND (LIMIT-TO (LANGUAGE,\"English\")) AND (LIMIT-TO (DOCTYPE,\"ar\") OR LIMIT-TO (DOCTYPE,\"cp\")) AND (LIMIT-TO (SUB-JAREA,\"COMP\") OR LIMIT-TO (SUBJAREA,\"ENGI\")) AND (LIMIT-TO (SRCTYPE,\"p\") OR LIMIT-TO (SRCTYPE,\"j\")))",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Scopus: Title-Abs-Key(((Autism Or Autistic) And (\"Emotion Recognition\" Or \"Mood Recognition\" Or \"Affect Recognition\" Or \"Recognition Of Mood\" Or \"Affect Recognition\" Or \"Recognition Of Affect",
      "text": "Web of Science:\n\n(TS=((((autism OR autistic) AND (\"emotion recognition\" OR \"mood recognition\" OR \"affect recognition\" OR \"recognition of mood\" OR \"affect recognition\" OR \"recognition of affect\"))))) AND (PY==(\"2023\" OR \"2022\" OR \"2021\" OR \"2020\" OR \"2019\" OR \"2018\" OR \"2017\" OR \"2016\" OR \"2015\" OR \"2014\" OR \"2013\" OR \"2012\" OR \"2011\") AND DT==(\"ARTICLE\") AND SJ==(\"ENGINEERING\" OR \"COMPUTER SCIENCE\") AND LA==(\"ENGLISH\"))",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Selection Process",
      "text": "The records retrieved from the databases and hand search were imported to the Mendeley Web Library, which was used as a primary tool to navigate through both records and reports. Duplicate records were manually identified by cross-checking title and abstract and then removed by three reviewers (ZC, OB, CV). These reviewers also screened each record and each report retrieved, assessed their eligibility, and eventually selected the final set of studies to be included in the review after reaching a majority consensus.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Collection Process",
      "text": "All reviewers (ZC, OB, JM, AP, DG, JP, SA, CV) participated in the review and assessment of the included studies. The studies were evenly distributed among three groups of reviewers according to their affiliation.\n\nWe used a cloud-based collaborative spreadsheet (Google Spreadsheet) to collect data from the included studies. The document consisted of a state-of-the-art matrix where each row represented a study and the columns indicated the data items to be analyzed. Each group of reviewers had to full screen and analyze the papers that were assigned to them and fill the information in the corresponding columns of the matrix. Periodic meetings were held in order to harmonise terminology and overcome potential discrepancies in the assessment process. Reviewers worked independently to extract the information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Items",
      "text": "The columns defined in the collaborative spreadsheet corresponded to the outcomes for which data were sought. The specific columns defined were: study name, year of publication, type of article, research goals, subject condition (autism type), emotion recognition modality, dataset (collection or use of), description of the dataset (if applicable), emotions sensed, devices used for the data collection, machine learning techniques, validation methods, study sample (size, type), study length, performance results, study outcomes, privacy and security, and challenges and future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Study Selection",
      "text": "A sample of 371 records were identified from the literature search. Namely, the search in Scopus yielded 206 records, while 165 records were obtained for Web of Science. 71 duplicate records were removed before screening. After deduplication, 300 records remained and were screened based on title and abstract. 112 records were excluded and 188 reports were sought for retrieval. 13 reports could not be retrieved and the remaining 175 reports were assessed for eligibility. 110 reports were excluded according to the eligibility criteria and the remaining 65 reports were included for the analysis. The workflow with the detailed process is shown in Fig.  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Research Goal",
      "text": "The objectives for investigating emotion recognition in autism vary across studies. However, certain common goals are shared among some of these studies. Thus, for example, 29%  (19/65)  of the studies propose and analyze algorithms and machine learning techniques to automatically recognize emotions in people with autism  [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] . Around 18% (12/65) of the studies propose the development and application of video games and apps to help children with autism understand and express emotions  [8, [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] . Fewer than 5% (3/65) of the studies explore the use of physiological signals for the automated identification of emotions  [19, 46, 47]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Autism Type",
      "text": "The type of autism varies across studies (Fig.  2 ). For example, 70% (46/65) of the studies address \"autism\" in general  [8, [17] [18] [19] [20] [21] [22] [23] [28] [29] [30] [31] [32] [33] [34] [35] [38] [39] [40] 42, 44, 46, , while 12% (8/65) refer to the full spectrum as \"all kind of autism\"  [24] [25] [26] [27] 37, [72] [73] [74] . Studies referring to \"autism\" tend to address broad aspects that apply to the overarching spectrum of ASD. In contrast, when some studies specifically mention \"all kind of autism\", they appear to suggest a deliberate effort to encompass the diverse manifestations and subtypes within the autism spectrum, recognizing and considering the heterogeneity of the condition. Nonetheless, most often both terms are used interchangeably. In some studies specific subtypes of the disorder are addressed. For example, 8% (5/65) correspond to high-functioning autism  [45, 61, [75] [76] [77] . Less than 2% (1/65) to mild autism  [78]  and 2% (1/65) to attention-deficit hyperactivity disorder  [79] . 6% (4/65) address combinations of parts of the spectrum, namely middle and moderate autism  [47] , all kinds of autism and Asperger  [36] , and high-functioning autism and Asperger  [41, 80] . While there exist a varied distribution of research focus across subtypes, the contributions in this regard are comparatively low. The limited focus on these subtypes may suggest that the majority of research in ASD aims to address broader aspects of the spectrum rather than delving into detailed examinations of specific subtypes. In Table A.2, the subtype of autism addressed in each selected paper is listed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotional Expressions",
      "text": "The selected studies focus on diverse emotional responses, expressions and sensed body regions or signals (Fig.  3 ). The most common emotion recognition modality is based on facial expressions, accounting for 63% (41/65) of the studies  [8, 20, 22, 23, 26, [28] [29] [30] [31] [32] [34] [35] [36] [37] [38] [40] [41] [42] [43] [44] [45] [46] 49, [51] [52] [53] [54] [55] [56] [58] [59] [60] [61] 64, 67, 68, 72, 75, 76, 79, 80] . 15% (10/65) are based on speech aspects  [25, 27, 63, 65, 66, 69, 71, 73, 74, 77] , followed by 5% (3/65) exploiting body movement  [21, 39, 48] , 2% (1/65) based on daily activities  [17] , 2% (1/65) measuring brain activity  [19] , and 2% (1/65) focusing on eye activity  [78] . 12% (8/65) of the studies consist of a multimodal approach which combine some of the above  [18, 24, 33, 47, 50, 57, 62, 70] . This distribution underscores the prevalent reliance on facial expressions while recognizing the significance of speech-related aspects in understanding and studying emotions within ASD.\n\nIn relation to the sensed body regions or signals, the majority of studies 71% (46/65) use physical data, i.e. sensed from the external parts of the body, mostly the face. 20% (13/65) of the studies exploit the inner body, including physiological signals such as electroencephalography (EEG) and electromyography (EMG), or psychoacoustic signals  [19, 24, 25, 27, 47, 63, 65, 66, 69, 71, 73, 74, 77] . The neurophysiological approaches provide valuable insights into the neural and muscular correlates of emotional states. As for the rationale behind considering psychoacoustics lies in the fundamental role of voice in the recognition of emotions within human interactions. By delving into the nuances of voice expression, researchers aim to deepen their understanding of how emotions are conveyed and perceived through auditory cues, con- tributing to a more comprehensive exploration of emotional recognition within the context of ASD. Around 8% (5/65) combine both physical and physiological signals  [33, 35, 50, 62, 70] . Less than 2% (1/65) of the studies did not provide enough information to this respect  [17]  (Table A.3).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Study Characteristics",
      "text": "The average number of participants was 49, calculated from the 48 studies indicating the number of participants (74% of the studies)  [8, 18, 19, [21] [22] [23] [24] [25] [26] [27] 29, 30, 32, 33, [36] [37] [38] [39] [40] [41] [44] [45] [46] [47] [48] [50] [51] [52] [53] [54] [55] 57, 58, [60] [61] [62] 65, 67, 70, [72] [73] [74] [75] [76] [77] [78] [79] [80] . The minimum sample size was four subjects  [67]  and the maximum 500  [22] . This diversity in sample sizes underscores the variability in research approaches within the field, with some studies opting for smaller, more focused samples, while others involve larger cohorts. Seventeen papers did not specify this number  [17, 20, 28, 31, 34, 35, 42, 43, 49, 56, 59, 63, 64, 66, 68, 69, 71] . The absence of participant count details in a significant number of papers highlights the need for increased transparency and reporting consistency in research methodologies. The distribution of the studies based on the sample size is shown in Fig.  4 .\n\nOne day was the minimum study duration  [19]  and 140 days the maximum duration  [48] . Yet, it must be noted that no additional information is provided in the rest of studies to this respect. The absence of duration details in the rest of the studies emphasizes the need for improved reporting standards to ensure a comprehensive understanding of the temporal aspects of HER research in autism.\n\nAs for the neurodevelopmental disorder distribution, 51% (33/65) of the studies involved people with autism  [8, 18, 21, 22, 25, 27, 29, 32, 33, [37] [38] [39] [40] [41] [44] [45] [46] [47] 51, 54, 57, 58, 61, 62, 65, 67, 70, 72, [75] [76] [77] [78] 80] . Almost 28% (18/65) included both people with and without autism  [8, 18, 25, 27, 29, 38, 41, 44, 45, 47, 54, 57, 58, [74] [75] [76] [77] [78] [79] . This approach allows researchers to explore and understand the unique features associated with ASD by contrasting them with individuals without the disorder. Around 9% (6/65) of the studies considered only people without autism  [30, 50, 52, 53, 55, 81] . Although it is not always clearly stated, the reasons for only considering neurotypical individuals are either the desire to establish baseline characteristics or more often the lack of access to people with autism. In 32% (21/65) of the studies, the disorder is not precisely described  [17, 19, 20, 23, 24, 26, 28, 31, 35, 36, 42, 43, 48, 52, 56, 60, 63, 64, 66, 68, 73] . In 23% (15/65) an existing dataset was used to test the proposed solution  [19, 28, [30] [31] [32] 34, 40, 42, 43, 49, 63, 66, 68, 69, 71] .\n\nLess than 28% (18/65) of the studies involved subjects of both genders  [18, 23, 24    [46, 77, 79]  while 6% (4/65) only included females  [26, 52, 53, 55] . The remaining 59% (38/65) of the studies did not specify the gender of the participants  [8, 17, [19] [20] [21] [22] 25, 28, 35, 38, [47] [48] [49] 51, 56, [58] [59] [60] [61] [62] 75, 78, 80] . Inadequate reporting of participant gender in these studies hinders the interpretability of outcomes and, more significantly, impedes a thorough analysis of gender-related effects and potential learning biases.\n\nRegarding the age of the involved subjects, 55% (36/65) of the studies provided this value  [8, 18, 21, [23] [24] [25] 27, 29, 30, [36] [37] [38] [39] [40] [41] 44, 46, 47, 51, 52, 54, 57, 58, 60, 62, 65, 67, 70, [72] [73] [74] [75] [76] [77] [78] [79] [80]  resulting in an average age of 18±10 years old. This suggests a relatively diverse age range among the participants, which could have implications for the generalizability of the proposed HER systems across different developmental stages. The other 45% (29/65) of the studies did not mention any age details  [17, 19, 20, 22, 26, 28, [31] [32] [33] [34] [35] 42, 43, 45, [48] [49] [50] 53, 55, 56, 59, 61, 63, 64, 66, 68, 69, 71] . All this information is summarized in Table A.4.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Types Of Emotions",
      "text": "The set of emotions analyzed in the selected studies are broadly based on the six universal emotional expressions, i.e. \"anger\", \"sadness\", \"happiness\", \"disgust\", \"surprise\", and \"fear\"  [82] . 43% (28/65) of the studies focused on these six basic emotions  [18, 22, 23, 26, 27, 30, 32, 35, 37, 38, [47] [48] [49] [50] [56] [57] [58] [59] [60] [61] [73] [74] [75] [76] [77] [78] [79] [80] . Two studies  [20, 68]  used these very six emotions but replacing \"disgust\" with the \"neutral\" emotional state. Another study only uses four basic emotions adding \"delight\" and \"joy\" emotions  [34] . The remaining 33 studies (51%)  [8, 17, 19, 21, 24, 25, 29, 31, 33, 36, [39] [40] [41] [42] [43] [44] [45] [46] [50] [51] [52] [53] [54] [55] [62] [63] [64] [65] [66] [67] 69, 71, 72]  used a number of emotions ranging from two to nine primitives. The emotions considered in addition to the six basic ones were \"neutral\", \"calm\", \"nervous\", \"scared\", \"curious\", \"excited\", \"sleepy\", \"contempt\", \"joy\", \"interested\", \"positive\", \"positive and talking\", \"odd positive\", \"negative\", \"boredom\", and \"contentment\".\n\nTable  A .5, Fig.  5  and Fig.  6  show the emotions used in all the analyzed studies for training/validation and test respectively. According to the listed results, approximately half of the studies leverage the six universal emotions, often relying on or producing publicly available datasets accessible to the scientific community. This choice facilitates meaningful comparisons between these studies, given their shared use of a standardized set of emotions. In contrast, the remaining studies opt for or create \"ad-hoc\" specific datasets, employing a set of emotions distinct from the universal ones. As a result, conducting comparisons be-  tween these approaches becomes more intricate due to the varied and specialized nature of the emotional categories used in these datasets.\n\nAlthough some studies did not mention the emotions used in the training and validation of the HER models  [28, 29, 36, 37, 44, 45, 67, 70, 75, 78] , a prevailing trend is the consistent use of the same set of emotions across training, validation, and test phases in most studies. Exceptions to this are  [25] [26] [27] 43, 47, [58] [59] [60] 73, 74, 76, 77, 80] , which used different sets of emotions for training and validation than for test, representing 20% (13/65) of the studies. Employing a different set of emotions for test introduces valuable diversity, reflecting the model's adaptability to recognize a broader spectrum of emotional expressions beyond its training data. This approach enhances the robustness and real-world applicability of HER models by challenging them with unseen emotion data instances during evaluation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Devices And Sensors",
      "text": "Two generations of devices and sensors are identified for the time frame considered for this review, which is related to the periods 2011-2014 and 2015-2023, respectively. Around 11% (7/65) of the studies correspond to the period 2011-2014, which is characterized by using images and audio as the primary data source. 57% (4/7) of these studies use a so-called first generation of devices consisting of webcams, headphones, and microphones  [36, 73, 74, 77] . To facilitate the labelling of the user's data, some controls were incorporated into the systems in 43% (3/7) of the aforementioned studies, including control knobs  [74, 77]  or numeric keypads  [79] , which were easily handled by users with autism. This period marked the initial steps in using technology for autism research, establishing a foundation for future studies.\n\nCirca 89% (58/65) of the studies represent the period 2015-2023, which is distinguished by a second generation of more advanced and ubiquitous devices and sensors. The technological advancement introduced a wide range of versatile devices, including mobile phones, tablets, 3D cameras, infrared cameras, and more, expanding the capabilities for data collection. Thus, for example, handheld devices are included in 16% (9/58) of these studies, including mobile phones  [30, 40, 50] , tablets  [37, 78] , and other handheld devices  [58] . Some of these devices were used to support gamification apps  [8]  or to exploit the mobile camera sensor for recognition purposes  [30, 35] . In particular, the new generation of cameras was used in 7% (4/58) of the studies, including IP cameras  [29, 56] , 3D cameras  [51] , and infrared cameras  [76] . However, classic vision and audio devices and sensors continued to be used in 33% (19/58) of the studies  [20, 21, 25, [38] [39] [40] 43, 54, 59, 60, [62] [63] [64] [65] [66] [67] 70, 71] , most likely due to the prevalence of the study of physical cues in autism research.\n\nThe advent of facial and body tracking technologies was also leveraged in this field. Such technologies were used in 16% (9/58) of the studies of the so-defined second generation. Devices like Kinect and Intel RealSense enabled improved facial and body tracking, enhancing the interaction and analysis of autistic behaviours. Kinect devices were incorporated into various works  [17, 45, 48, 56]  due to the availability of an RGB camera, a depth sensor, and a microphone of-the-shelf. Recently, some works have started to use the Intel RealSense device, which has characteristics similar to Kinect  [41] . In the same way, Tobii devices were proposed for eye tracking  [57]  or for head and eye tracking  [18] . Standard cameras were also used to record images for eye tracking  [75]  and pose tracking  [48] .\n\nPhysiological sensors such as EEG were incorporated in 5% (3/58) of the second generation studies. In  [19, 47]  EEG data is collected using a headset with electrodes placed on the participants' scalps. In  [46] , the authors use a commercial EEG device (Emotiv) to collect data from the frontal, temporal, and posterior brain regions. The use of wearable devices was rare. Only 3% (2/58) of the studies included these devices. Microsoft Band 2 was used in  [56] , while shimmer sensors were used in  [47] . In an attempt to incorporate augmented reality features, Microsoft Hololens and Google Glasses were also used in  [52, 54] , respectively. Full details are provided in Table  A .  6  and Fig.  7 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Models And Performance",
      "text": "This subsection summarises the findings concerning machine learning techniques, performance and metrics, validation methods, and the number of data samples.\n\nThe reviewed studies make primary use of supervised learning techniques. Support vector machines (SVM) stand out as the most widely used technique, namely in 29%  (19/65)   The use of Deep Learning is currently confined to recent works, constituting a 31% (20/65) of the studies  [20, 25, [28] [29] [30] [31] [32] [33] [34] [40] [41] [42] [43] [63] [64] [65] [66] 68, 69, 71] . This limitation suggests an untapped opportunity, as earlier research may not have fully harnessed the capabilities of deep learning for complex pattern recognition tasks in emotion recognition in autism. Notably, some of the most recent works leverage Deep Learning techniques, including convolutional neural networks, highlighting the emerging potential for improved performance in emotion recognition. However, it is crucial to acknowledge a potential bias towards supervised learning, indicating a potential gap in exploring unsupervised or semi-supervised methods. These alternative approaches could offer valuable insights, especially in scenarios where labelled data is scarce or challenging to acquire. Exploring a broader spectrum of deep learning methodologies could enhance the versatility and effectiveness of emotion recognition models.\n\nThe performance of emotion recognition models varies significantly among the studies, attributed to differences in target emotions, sensor data types, machine learning techniques, and dataset instances. This variation suggests challenges in directly comparing study outcomes and establishing standardized benchmarks. Studies can be categorized into three groups according to performance levels. First, 28% (18/65) of the studies are ranked as of high performance (i.e. accuracies greater than 90%), most usually developing an offline evaluation based on datasets collected under controlled conditions  [8, 21, 23   , with more ambitious and challenging solutions based on emerging sensor technologies, leading to performances below 80%. The remaining studies have not sufficiently described their performance results and could not be classified into any group.\n\nThe studies make use of a variety of metrics to evaluate model performance, including accuracy, sensitivity, and specificity. This range of metrics provides a fair understanding of model performance, especially those handling dataset imbalances. Concerning the metrics used to estimate model performance, around 57% (37/65) of the studies have chosen the use of accuracy  [8,17-24,28-30,32,33,35,38-43,46,48-50, 52-55,62-64,66-70] . Other studies have used unweighted average recall to deal with data set imbalance more effectively  [27, 30, 65, 74] . Sensitivity has also been used in some studies  [51, 59, 72] , although they still need to evaluate the prominence of true negatives by avoiding the use of specificity. Few studies  [18, 22, 41]  rely on the use of accuracy, sensitivity, and specificity to more fully reflect the performance of their recognition system.\n\nThe studies adopted different cross-validation techniques (e.g., ten-fold, five-fold, leave-one-out) which provide a rigorous approach to model validation, ensuring the reliability of the findings. Crossvalidation is developed in 22% (14/65) of the studies: ten-fold crossvalidation is used in  [19, 21, 29, 49, 53, 54, 60] , five-fold cross-validation  [31, 33, 46] , eight-fold cross-validation  [46]  and one-leave-out crossvalidation  [18, 28, 74] . More exceptionally, other approaches such as random split leave one subject out  [48] , random split cross validation  [32, 52] , and split train-test or hold out  [26, 38, 42, 43, 63, 68, 69]  are used. A more detailed description can be found in Table  A .  7  and Fig.  8 . This figure illustrates the variety of machine learning methods employed in the reviewed papers, emphasizing the growing prevalence of deep learning due to its robust yet intricate models. However, a noteworthy 12% (8/65) of the papers lack information on the techniques utilized. Encouragingly, there is an expectation that this trend will shift, leading to more papers sharing their models' code in repositories for enhanced scientific community knowledge.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Information Privacy And Security",
      "text": "Despite the relevance of ensuring privacy and security policies in this field, only 22% (14/65) of the studies acknowledge these sufficiently  [18, 23, 24, 29, 36, 39, 41, 47, 54, 58, 61, 67, 72, 79] . Three of these studies  [23, 72, 79]  followed the 1964 Declaration of Helsinki, a formal statement of ethical principles published by the World Medical Association (WMA) to guide the protection of human participants in medical research  [83] . The other 11 studies mentioned that they either had the consent of the relatives of the people with autism or their work was approved by the ethics committee of the given universities or other institutions. All details are provided in Table  A .  8 .\n\nUpon analyzing the obtained results, the majority of studies that indicated privacy and security aspects had obtained consent from family members, or the research was approved by ethical committees of universities/institutions; very few studies adhered to the Declaration of Helsinki. However, it is evident that there is insufficient consideration of privacy and security aspects in the majority of studies. Studies lacking pertinent details may not adhere to ethical protocols, thereby generating concerns about the protection of participants.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Findings",
      "text": "The great majority of the studies analyzed referred to the autism spectrum disorder in different ways. Namely, it was noticed the use of two terminologies \"autism\" and \"all kinds of autism\" to refer to this condition interchangeably. This shows a lack of unification on the use of this terminology by the scientific community of the HER field. More importantly, more research needs to be placed towards mild and highfunctioning autism, as well as other conditions of the autism spectrum like Asperger, which according to the results are just marginally considered.\n\nWhile a majority of studies indicate the type of autism considered in their research, a significant number did not. Omitting information on the type of autism considered in studies can hinder accurate interpretation and reduce the applicability of findings, leading to potential misinterpretations and limiting the generalizability of research outcomes. Additionally, the absence of this specification may impede meaningful comparisons across studies, hindering the overall advancement of knowledge in the field of emotion recognition in autism.\n\nThe lack of proper specification of gender aspects in over half of the reviewed studies can have several consequences. It may lead to an incomplete understanding of how gender influences the outcomes of the research, potentially masking gender-related patterns or differences in emotional recognition within the context of ASD. Additionally, it hinders the generalizability of findings, as the impact of gender on emotion recognition might be relevant.\n\nThe number of participants involved in the studies varies remarkably, thus limiting the comparability of the results. While it is generally encouraged in the area to include as many participants as possible, the number of involved individuals should be fairly supported via an appropriate statistical power analysis. At least, it should be attempted to guarantee a sufficient number of participants matching the average number of the art.\n\nThe emotion recognition modality most predominantly used is the one based on facial expressions, followed by speech. The reason for favouring the measurement of physical variables over physiological might be related to the fact that emotions are socially expressed and perceived via physical cues, such as facial and visual expressions and O. Banos, Z. Comas-González, J. Medina et al.\n\nthe voice tone. ASD is however sometimes characterized by a lack of expressiveness. Hence, it might be a good choice to observe and to analyze physiological behaviour in this population in addition to the physical one.\n\nA major part of the studies used the six basic universal emotions (anger, sadness, happiness, disgust, surprise, and fear) considered as a standard for HER systems. The reasons may have to do with the fact that such emotions represent the most common set expressed by people in their daily life  [82] . Moreover, using emotions similar to the ones used in prior works facilitate cross comparison and reproducibility, so that better conclusions can be drawn. Several studies used combinations of the six basic emotions by adding very specific emotions (neutral, calm, nervous, scared, curious, excited, sleepy, contempt, joy, and contentment). From this set of emotions, \"neutral\" stands out as the most frequent one, possibly due to its prevalence in the daily life.\n\nThe majority of devices and sensors employed in the period 2015-2023 are seen to be particularly advances with respect to the ones used in the period 2011-2014. IP and infrared cameras, face or body tracking sensors, and partially wearable sensors or robots are used in the second half of the decade while more traditional systems such as webcams and microphones were used during the first half. From our analysis we can conclude that most works use a single technology to assess emotions in autism. The main reason for considering a sole device could be to simplify the sensor setup and lessen the intrusiveness sometimes felt by users when using these technologies. Combining multiple technologies to assess emotions may potentially lead to more accurate and robust decisions, as shown in the literature for neurotypical populations  [84] . However, as we found out in a former study of ours  [85] , people with autism (adolescents) show reluctance to using multiple devices, and in particular to some specific ones such as infrared cameras.\n\nAll the algorithms used are of the supervised kind, which was expected since most of the reviewed studies are aimed at diagnosing autism. A limitation observed for such approaches is that they tend to be learned on general-purpose emotion recognition datasets, most likely due to a clear lack of existing autism-specific datasets. General-purpose datasets could serve well for boosting some machine learning models, however they are of limited use when it comes to recognising the emotions expressed by people with autism. One goal for the community could then be the creation of new relevant datasets particularly devised for autism applications. Moreover, we did not find any study exploring the use of unsupervised methods. The use of these methods allows for the creation of clusters, which could help identify people with similar patterns within a similar spectrum. This is found of much interest specially when it comes to a disorder like autism, which is quite diverse per se.\n\nThe performance results have been shown to vary among studies. High performances are obtained for a number of studies, however, it is observed that most of such studies do not describe in sufficient detail the evaluation method, thus hindering the validity of the reported results. Cross-validation and accuracy metrics are most widely used for evaluating the emotion recognition models performance  [86] . A minority of the studies characterize the performance of their system more comprehensively using other metrics such as sensitivity and specificity. In order to avoid the effects of data bias, future research in this area is encouraged to consider using more robust metrics such as the F-score  [87] . The number of data samples is also found key to determine the relevance of the reported results, and according to this review, only a minority of the studies appear to use a relevant sample set. This limits somewhat the validity of some of the results reported in the reviewed studies.\n\nPrivacy and security aspects have been partially addressed and only by a minority of the studies. It should be noted that this kind of studies work with sensitive data, and it is imperative to guarantee the protection of participants in medical research, especially when it comes to people with neurodevelopmental disorders. Presumably, the studies that did not give details on this information may not have followed any ethical protocol or perhaps simply forgot to report it in the manuscript. While the former is a more serious issue than the latter, we think this is an aspect that must be improved considerably in the future and information privacy and security should be compulsorily addressed in all studies of this nature.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Challenges And Opportunities",
      "text": "From the previous findings a number of important challenges and opportunities are identified which should be considered in our opinion while designing, developing, and using emotion recognition technologies for people with autism.\n\nOne such challenge has to do with the heterogeneity of the autism spectrum disorder. The fact that autism is a spectrum disorder entails that people with autism have a wide range of abilities, difficulties, and preferences. Regular emotion recognition technologies may not account for this heterogeneity, as they often rely on standardized models and assumptions about emotional expressions of neurotypical persons. Hence, we consider important to build systems devised for each specific subtype of the disorder, and whether possible, favour the development of personalized approaches that consider the unique characteristics of each person with autism. A way to materialize this idea could be to use transfer learning approaches, in which an existing emotion recognition model trained on a large dataset from a heterogeneous cohort of individuals is used to learn a personalized model by tuning the former with new data of a particular individual or group of subjects pertaining to a specific autism subtype.\n\nAnother important challenge relates to the nonverbal communication variability linked to the disorder. People with autism may have atypical patterns in facial expressions, body language, and vocal tone. This review underscores that current emotion recognition technologies, predominantly reliant on visual and auditory cues, may struggle to accurately interpret these atypical communication styles. To overcome this, the creation of new algorithms tailored to the specific nonverbal communication of individuals with autism is proposed, potentially involving the development of expert-validated datasets that capture this variability  [88, 89] . While engaging individuals with autism in this process is ideal, an alternative involves using actors to mimic these cues based on expert instructions.\n\nMany people with autism have sensory sensitivities, which can affect their tolerance for certain stimuli, such as bright lights, loud sounds, or touch. As a result, the use of emotion recognition technologies that involve a sensory input, like vivid displays, loudspeakers, or tactile sensors, may potentially cause discomfort or distress for some individuals with autism. Hence, it is important to make sure that the technologies are adaptable to the sensory needs and particularly to the preferences of the user. Emotion recognition systems should be designed to be flexible and compatible enough to operate with the sensor modalities chosen by the user. One way to achieve this is to develop ensemble learning models combining multiple individual models each running on data from different sensor modalities. By following this approach, the resulting emotion recognition model can easily adapt to the absence of one or various sensor modalities and still operate, although the accuracy of the system would be normally lower.\n\nOther relevant challenge emphasizes the contextual nature of emotions, particularly pertinent in the case of autism where social nuances may pose difficulties for solely facial or vocal emotion recognition systems. To address this, integrating alternate sensing options, such as physiological cues, is suggested. However, the broader context, including situational cues, personal history, and individual preferences, is crucial for enhancing recognition accuracy. To capture this intricate information, incorporating virtual agents or chatbots in regular interactions with individuals with autism is proposed as a means to gather comprehensive data for more effective emotion recognition models.\n\nAs previously highlighted, emotion recognition technologies introduce significant ethical considerations concerning privacy and data security. The collection and storage of sensitive emotional data carry potential implications for an individual's privacy and autonomy. Therefore, it is paramount to establish rigorous privacy measures, secure informed consent, and guarantee that people with autism retain control over their personal information  [90] . Given that many emotion recognition models operate on sensitive data, including video and audio, it becomes particularly imperative to transparently communicate the purpose, methodology, and safeguards associated with data collection. Ensuring these systems adhere to and assure full compliance with relevant regulations is essential. Consequently, the incorporation of emotion recognition technologies utilizing sensory inputs, such as vivid visual displays, loudspeakers, or tactile sensors, has the potential to induce discomfort or distress for some individuals with autism Addressing these challenges requires interdisciplinary collaboration between researchers, technologists, and the autism community. It is crucial to involve individuals with autism and their families in the design and development of these technologies to ensure that they are respectful, inclusive, and beneficial for the target population.\n\nA summary of the principal findings described previously is provided in Fig.  9 . The diagram shows the strengths and weaknesses of the reviewed studies on sensing technologies and machine learning methods for emotion recognition in autism as well as the challenges and recommendations for the research community. Strengths are the aspects that these studies have performed well on and could be reproduced in future investigations. Weaknesses are matters that went wrong in these studies and could be improved in future research. Challenges are the elements that the scientific community needs to address successfully to boost the investigation of this topic. Recommendations are the suggestions for the research community working on this field.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "As for any other review, and despite having used a rather broad search strategy, it is certainly possible that some interesting studies may have left out from our analysis. Namely, the search areas of this systematic review were circumscribed to computer science and engineering respectively, as they are quite large areas and most relevant for the scope of this study. Nonetheless, it is also possible that some relevant studies indexed in other related categories may have been filtered out. We conducted a preliminary check for other domains such as psychology, behavioural sciences, or pediatrics and we did not find relevant studies that would meet the defined criteria. Another possible limitation of this review refers to the reference management software used to process both records and reports. We decided to use Mendeley since all reviewers were quite familiarised with the tool and all three contributing institutions supports access to it. One of the major advantages in deciding to use Mendeley is that it allows the creation of academic research communities through collaborative research  [91] . However, other free and open-source reference management software, such as Zotero, could be more appropriate when it comes to pursuing open science principles. Finally, the protocol systematic review conducted here could have been pre-registered, for example via PROSPERO, however O. Banos, Z. Comas-González, J. Medina et al.\n\nthe researchers were not aware of this option when the work started. Nonetheless, we recently searched for similar pre-registered protocols and none resulted from the search, so we presume that no overlap exists between our work and other on-going reviews in the field.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "Automatic emotion recognition constitutes a fairly consolidated research domain in the affective computing field. However, as it is shown in this review, its application to autism is limited and insufficiently validated. Thus, for example, new research should explore the design and development of models that account for the particular characteristics of people with autism, rather than pushing to the limits the generalisation of existing models trained on data collected from neurotypical people. In this regard, collecting and sharing publicly new datasets involving people with autism is found of paramount importance as these are practically nonexistent to date. More efforts should also be put towards describing in greater detail the characteristics of the samples subject to study. Gender, age, and autism type are not consistently reported thus making it difficult to assess the relevance of the proposed models and hindering the replicability of the studies. In view of the diverse nature of the autism spectrum, it seems also quite reasonable to explore in future studies the use of holistic sensing approaches. Indeed, facial expression recognition is ahead of other solutions, also in this domain, however, the disparity and lack of expressiveness among people with autism make it necessary to consider measuring multiple physical and physiological signals. Accomplishing these challenges demands interdisciplinary collaboration teams and the appropriate funding of governments and institutions to design, develop, and validate the required technologies from an autism-centric perspective and in realistic settings. We truly hope that reflecting on the positive contributions made by researchers in this field particularly on the ample room for improvement can spark great interest from other colleagues from the affective computing field to devote time and effort to boost this important domain.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Summary Table",
      "text": "• Automatic emotion recognition constitutes a consolidated research domain in the affective computing field. Nevertheless, as shown in this review, its application to autism is limited and not sufficiently validated. • New research should explore the design and development of models that take into account the unique characteristics of individuals with autism, rather than generalising existing models trained on data collected from neurotypical individuals. • The collection and public sharing of new datasets that include individuals with autism is therefore considered of utmost importance, as they are virtually non-existent to date and should include more detailed characteristics of the samples under study. • Considering the diverse nature of the autistic spectrum, it also seems quite reasonable to explore the use of holistic detection approaches in future studies. • Facial expression recognition is ahead of other solutions, also in this area; nevertheless, the disparity and sometimes lack of expressiveness among individuals with autism makes it necessary to consider the measurement of multiple physical and physiological signals.\n\n• Meeting these challenges requires interdisciplinary collaborative teams and adequate funding from governments and institutions to design, develop and validate the necessary technologies from an autism-centred perspective and in realistic settings. • The overall aim is that reflection on the positive contributions made by researchers in this field, in particular on the vast room for improvement, may inspire great interest in other colleagues in the field of affective computing to dedicate time and effort to furthering this important field.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Credit Authorship Contribution Statement",
      "text": "Oresti Banos: Writing -review & editing, Writing -original draft, Supervision, Methodology, Funding acquisition, Formal analysis, Conceptualization. Zhoe Comas-González: Visualization, Methodology, Formal analysis, Data curation. Javier Medina: Writing -review & editing, Writing -original draft, Visualization, Methodology, Funding acquisition, Formal analysis, Conceptualization. Aurora Polo-Rodríguez: Writing -original draft, Methodology, Investigation, Data curation. David Gil: Writing -review & editing, Writing -original draft, Methodology, Funding acquisition, Formal analysis, Conceptualization. Jesús Peral: Writing -review & editing, Writing -original draft, Supervision, Methodology, Funding acquisition, Formal analysis, Conceptualization. Sandra Amador: Writing -original draft, Visualization, Methodology, Data curation. Claudia Villalonga: Writing -original draft, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Declaration Of Competing Interest",
      "text": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.   [8]  Create a game to help children with autism cope with their emotional difficulties  Leo et al. (2015)    [49]  Integrate automatic emotion recognition capabilities in a robot-children interaction tool for autism treatment Postawka et al. (2019)  [17]  Develop emotion recognition methods for behaviour model estimation based on body position Jiang et al. (2019)  [18]  Identify subjects with/without autism by using facial emotion recognition and eye tracking  Gao et al. (2015)    [19]  Classify emotions through electroencephalography signals  Heni et al. (2016)    [35]  Design an app to recognize both emotions and voice Jeon et al. (2015)  [50]  Examine how children with autism and neurotypical children understand and interpret emotions Fan et al. (2017)  [46]  Explore the feasibility of using electroencephalography signals to analyze the facial affect recognition process of individuals with autism Joseph et al. (2018)  [20]  Propose a new algorithm to detect primary emotions of children with autism in real time using deep learning Spicker et al. (2016)  [79]  Investigate the differences in perception and categorization of emotional facial expressions of virtual characters between children and adolescents with autism, attention-deficit hyperactivity disorder, and neurotypical ones Enticott et al. (2014)  [61]  Examine facial emotion recognition of matched static and dynamic images among adolescents with autism and adults and neurotypical individuals Santhoshkumar et al. (2019)  [21]  Predict basic emotions from children with autism using body movements Sivasangari et al. (2019)  [22]  Propose a new methods for the automatic recognition of emotions Tang et al. (2017)  [51]  Compare the manual tagging of emotions by teachers/parents with the automatic one produced by an automatic system during naturalistic tasks Ley et al. (2019)  [62]  Evaluate existing tools for emotion recognition based on facial features as well as vocal features in voice interactions Chung et al. (2019)  [52]  Develop an augmented reality system for the presentation of the emotions detected via a facial expression recognition model  Smitha et al. (2015)    [53]  Determine a feasible method for realizing a portable emotion detector for children with autism Daniels et al. (2018)  [54]  Build a therapeutic tool for children with autism using wearable technologies to recognize emotions as well as estimate how these interpretations differ between children with autism and neurotypical children  Smitha et al. (2013)    [55]  Build a hardware efficient portable emotion recognizer on an FPGA to aid children with autism during the recognition of emotions Tang et al. (2016)  [56]  Develop an IoT natural play environment to help neurotypical children to understand children with autism emotions Liliana et al. (2020)  [23]  Develop an artificial intelligent model based on psychological knowledge to recognize emotions by analyzing facial expressions Ghorbandaei et al. (2018)  [72]  Build a robotic platform for reciprocal interaction in which a vision system recognizes the facial expressions of the user through a fuzzy clustering method Elamir et al. (2018)  [24]  Design an automatic emotion recognition system based on nonlinear analysis of various physiological signals  Fernandes et al. (2011)    [36]  Apply a game-based approach to teach children with autism to recognize facial emotions using realtime automatic facial expression analysis and virtual character synthesis Anishchenko et al. (2017)  [37]  Develop a tablet application for learning and detecting facial expressions Su et al. (2018)  [78]  Examine the differences of emotion recognition and eye gaze pattern between children with autism and neurotypical ones using facial expressions Arellano et al. (2015)  [75]  Assess how abstract emotional facial expressions influence the categorization of the emotions by children and adolescents with high functioning autism AndleebSiddiqui et al. (2020)  [25]  Recognize emotions via speech analysis using deep learning  Globerson et al. (2012)    [77]  Explore the association between psychoacoustic abilities and vocal emotion recognition in a group of individuals with autism and a matched group of neurotypical individuals Sunitha et al. (2014)  [73]  Collect a new dataset for emotion recognition from speech Bagirathan et al. (2020)  [47]  Compare psycho-physiological signals from neurotypical children and children with autism Piparsaniyan et al. (2014)  [26]  Propose a new method for facial expression recognition  Marchi et al. (2012)    [27]  Classify a number of emotions in different scenarios  Marchi et al. (2015)    [74]  Analyze various existing emotion recognition datasets Syeda et al. (2017)  [57]  Perform visual face scanning pattern and emotion perception analysis between neurotypical children and children with autism  Guha et al. (2018)    [76]  Assess the reduced complexity in facial expression dynamics of subjects with high functional autism relative to their neurotypical peers  Costescu et al. (2020)    [58]  Test the effectiveness of a facial expression recognition instrument in both neurotypical individuals and adolescents with autism  Tracy et al. (2011)    [80]  Show impaired recognition of all basic emotion expressions and more socially complex ones when forced to complete the recognition process in a very brief time frame Chung et al. (2020)  [52]  Design an e-learning model for students with autism Zhang et al. (2016)  [60]  Propose a new emotion recognition system based on facial expression images  Dantas et al. (2022)    [38]  Build a game to support the ability for children with autism to recognize and express basic emotions Saranya et al. (2022)  [28]  Develop an deep learning-based emotion recognition method for improving the rate of detection in children with autism  Sukumaran et al. (2021)    [63]  Identify the presence of ASD and to analyze the emotions of children with autism through their voices Wang et al. (2021)  [30]  Analyze an emotion care system based on big data analysis for autism disorder patient training, where emotion is detected in terms of facial expression Banire et al. (2021)  [29]  Develop a face-based attention recognition model using geometric feature transformation and time-domain spatial features Piana et al. (2021)  [39]  Build a system for the automatic emotion recognition designed for helping children with autism to learn to recognize and express emotions by means of their full-body movement  Ruan et al. (2022)    [64]  Design and build automatic computer-based learning tools for children with ASD to improve their performance in Maths  Milling et al. (2022)    [65]  Contribute with a voice activity detection (VAD) system specifically adapted to children with autism vocalisations  Chitre et al. (2022)    [66]  Model a Real-time Speech Emotion Recognition (SER) that takes audio signals as inputs and detects the emotions based on those signals  Wang et al. (2022)    [67]  Examine the effects of video-based intervention on emotion recognition in four children with ASD with imitation in speech  Wan et al. (2022)    [40]  Propose a novel framework for human-computer human-robot interaction and introduce a preliminary intervention study for improving the emotion recognition of Chinese children with autism Silva et al. (2021)  [41]  Develop a system capable of automatically detecting emotions through facial expressions and interfacing them with a robotic platform to allow social interaction with children with ASD Praveena et al. (2021)  [68]  Recognize and predict face emotion in ASD  Rojas et al. (2021)    [42]  Help people with a degree of difficulty in interpreting emotions so that they can have a normal social interaction through a mobile app in real-time  Karanchery et al. (2021)    [43]  Provide a solution to be deployed in learning environments for individuals with ASD to aid the primary caregivers in understanding their emotional states  Valles et al. (2021)    [69]  Develop a speech emotion recognition system to help children with autism to better identify the emotions of their communication partner DIzicheh et al. (2021)  [44]  Present a serious game called EmoAnim that utilizes animations to screen players' emotion recognition capabilities   [34]  Not sufficiently described Not sufficiently described  Murugaiyan et al. (2023)    [71]  Not sufficiently described Not sufficiently described   [32]  Not sufficiently described Not sufficiently described Not sufficiently described Li et al. (2021)  [33]  Not sufficiently described Not sufficiently described Not sufficiently described Ghanouni et al. (2021)  [45]  Vision sensor Kinect Not sufficiently described Zhang et al. (2023)  [70]  Vision Camera Not sufficiently described Talaat (2023)  [34]  Not sufficiently described Not sufficiently described Not sufficiently described  Murugaiyan et al. (2023)    [71]  Audio Microphone Not sufficiently described",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Prisma ﬂowchart.",
      "page": 3
    },
    {
      "caption": "Figure 1: 3.2. Research goal",
      "page": 3
    },
    {
      "caption": "Figure 2: Autism types investigated in each reviewed study. (For interpretation of the colours in the ﬁgure(s), the reader is referred to the web version of this article.)",
      "page": 4
    },
    {
      "caption": "Figure 3: Emotion recognition modality and sensed body regions for each reviewed study.",
      "page": 4
    },
    {
      "caption": "Figure 2: ). For example, 70%",
      "page": 4
    },
    {
      "caption": "Figure 3: ). The most common",
      "page": 4
    },
    {
      "caption": "Figure 4: Distribution of the reviewed studies according to the sample size.",
      "page": 5
    },
    {
      "caption": "Figure 4: One day was the minimum study duration [19] and 140 days the",
      "page": 5
    },
    {
      "caption": "Figure 5: and Fig. 6 show the emotions used in all the an-",
      "page": 5
    },
    {
      "caption": "Figure 5: Emotions used for the training-validation of the recognition models in the reviewed studies.",
      "page": 6
    },
    {
      "caption": "Figure 6: Emotions used for the testing of the recognition models in the reviewed studies.",
      "page": 6
    },
    {
      "caption": "Figure 7: Devices, sensors, and speciﬁc models used in the reviewed studies.",
      "page": 7
    },
    {
      "caption": "Figure 7: 3.8. Models and performance",
      "page": 7
    },
    {
      "caption": "Figure 8: Machine learning techniques, performance and metrics, and validation methods used in each of the reviewed studies.",
      "page": 8
    },
    {
      "caption": "Figure 8: This ﬁgure illustrates the variety of machine learning methods",
      "page": 8
    },
    {
      "caption": "Figure 9: Summary of the principal takeaways of the reviewed manuscripts.",
      "page": 10
    },
    {
      "caption": "Figure 9: The diagram shows the strengths and weaknesses of the re-",
      "page": 10
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Autism spectrum disorder",
      "authors": [
        "C Lord",
        "T Brugha",
        "T Charman",
        "J Cusack",
        "G Dumas",
        "T Frazier",
        "E Jones",
        "R Jones",
        "A Pickles",
        "M State"
      ],
      "year": "2020",
      "venue": "Nat. Rev. Dis. Primers"
    },
    {
      "citation_id": "2",
      "title": "Recognition of emotions in autism: a formal metaanalysis",
      "authors": [
        "M Uljarevic",
        "A Hamilton"
      ],
      "year": "2013",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition training in autism spectrum disorder: a systematic review of challenges related to generalizability",
      "authors": [
        "S Berggren",
        "S Fletcher-Watson",
        "N Milenkovic",
        "P Marschik",
        "S Bölte",
        "U Jonsson"
      ],
      "year": "2018",
      "venue": "Dev. Neurorehabil"
    },
    {
      "citation_id": "4",
      "title": "An exploration of the impact of contextual information on the emotion recognition ability of autistic adults",
      "authors": [
        "D Metcalfe",
        "K Mckenzie",
        "K Mccarty",
        "T Pollet",
        "G Murray"
      ],
      "year": "2022",
      "venue": "Int. J. Psychol"
    },
    {
      "citation_id": "5",
      "title": "Affective computing and autism",
      "authors": [
        "R Kaliouby",
        "R Picard",
        "S Baron-Cohen"
      ],
      "year": "2006",
      "venue": "Ann. N.Y. Acad. Sci"
    },
    {
      "citation_id": "6",
      "title": "The 'reading the mind in films' task [child version]: complex emotion and mental state recognition in children with and without autism spectrum conditions",
      "authors": [
        "O Golan",
        "S Baron-Cohen",
        "Y Golan"
      ],
      "year": "2008",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "7",
      "title": "Human emotion recognition: review of sensors and methods",
      "authors": [
        "A Dzedzickis",
        "A Kaklauskas",
        "V Bucinskas"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Autism screening using a video game based on emotions, in: 2018 2nd National and 1st International Digital Games Research Conference: Trends, Technologies, and Applications",
      "authors": [
        "A Irani",
        "H Moradi",
        "L Vahid"
      ],
      "year": "2018",
      "venue": "Autism screening using a video game based on emotions, in: 2018 2nd National and 1st International Digital Games Research Conference: Trends, Technologies, and Applications"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition and detection methods: a comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "J. Artif. Intell. Syst"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from physiological signal analysis: a review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electron. Notes Theor. Comput. Sci"
    },
    {
      "citation_id": "11",
      "title": "Emotion in a century: a review of emotion recognition",
      "authors": [
        "T Thanapattheerakul",
        "K Mao",
        "J Amoranto",
        "J Chan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 10th International Conference on Advances in Information Technology, IAIT 2018, Association for Computing Machinery"
    },
    {
      "citation_id": "12",
      "title": "The contribution of machine learning and eye-tracking technology in autism spectrum disorder research: a systematic review",
      "authors": [
        "K.-F Kollias",
        "C Syriopoulou-Delli",
        "P Sarigiannidis",
        "G Fragulis"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "13",
      "title": "Robot-based intervention for children with autism spectrum disorder: a systematic literature review",
      "authors": [
        "K Bartl-Pokorny",
        "M Pykała",
        "P Uluer",
        "D Barkana",
        "A Baird",
        "H Kose",
        "T Zorcec",
        "B Robins",
        "B Schuller",
        "A Landowska"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "A review of wearable solutions for physiological and emotional monitoring for use by people with autism spectrum disorder and their caregivers",
      "authors": [
        "M Taj-Eldin",
        "C Ryan",
        "B O'flynn",
        "P Galvin"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: a review",
      "authors": [
        "M Khodatars",
        "A Shoeibi",
        "D Sadeghi",
        "N Ghaasemi",
        "M Jafari",
        "P Moridian",
        "A Khadem",
        "R Alizadehsani",
        "A Zare",
        "Y Kong"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "16",
      "title": "Prisma 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews",
      "authors": [
        "M Page",
        "D Moher",
        "P Bossuyt",
        "I Boutron",
        "T Hoffmann",
        "C Mulrow",
        "L Shamseer",
        "J Tetzlaff",
        "E Akl",
        "S Brennan"
      ],
      "year": "2021",
      "venue": "BMJ"
    },
    {
      "citation_id": "17",
      "title": "Behavior-based emotion recognition using kinect and hidden Markov models",
      "authors": [
        "A Postawka"
      ],
      "year": "2019",
      "venue": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics"
    },
    {
      "citation_id": "18",
      "title": "Classifying individuals with ASD through facial emotion recognition and eye-tracking",
      "authors": [
        "M Jiang",
        "S Francis",
        "D Srishyla",
        "C Conelea",
        "Q Zhao",
        "S Jacob"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "19",
      "title": "Deep learning of EEG signals for emotion recognition",
      "authors": [
        "Y Gao",
        "H Lee",
        "R Mehmood"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Multimedia and Expo Workshops"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition in a social robot for robot-assisted therapy to autistic treatment using deep learning",
      "authors": [
        "L Joseph",
        "S Pramod",
        "L Nair"
      ],
      "year": "2017",
      "venue": "Proceedings of 2017 IEEE International Conference on Technological Advancements in Power and Energy: Exploring Energy Solutions for an Intelligent Power Grid"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition system for autism children using non-verbal communication",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Int. J. Innov. Technol. Explor. Eng"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition system for autism disordered people",
      "authors": [
        "A Sivasangari",
        "P Ajitha",
        "I Rajkumar",
        "S Poonguzhali"
      ],
      "year": "2019",
      "venue": "J. Ambient Intell. Humaniz. Comput"
    },
    {
      "citation_id": "23",
      "title": "High-level fuzzy linguistic features of facial components in human emotion recognition",
      "authors": [
        "D Liliana",
        "T Basaruddin",
        "M Widyanto",
        "I Oriza"
      ],
      "year": "2020",
      "venue": "J. Inf. Commun. Technol"
    },
    {
      "citation_id": "24",
      "title": "Intelligent emotion recognition system using recurrence quantification analysis (RQA)",
      "authors": [
        "M Elamir",
        "W Alatabany",
        "M Aldosoky"
      ],
      "year": "2018",
      "venue": "National Radio Science Conference, NRSC, Proceedings"
    },
    {
      "citation_id": "25",
      "title": "Performance evaluation of deep autoencoder network for speech emotion recognition",
      "authors": [
        "M Andleebsiddiqui",
        "W Hussain",
        "S Ali",
        "D Ur Rehman"
      ],
      "year": "2020",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "26",
      "title": "Robust facial expression recognition using Gabor feature and Bayesian discriminating classifier",
      "authors": [
        "Y Piparsaniyan",
        "V Sharma",
        "K Mahapatra"
      ],
      "year": "2014",
      "venue": "International Conference on Communication and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Speech, emotion, age, language, task, and typicality: trying to disentangle performance and feature relevance",
      "authors": [
        "E Marchi",
        "A Batliner",
        "B Schuller",
        "S Fridenzon",
        "S Tal",
        "O Golan"
      ],
      "year": "2012",
      "venue": "Proceedings -2012 ASE/IEEE International Conference on Privacy, Security, Risk and Trust and 2012 ASE/IEEE International Conference on Social Computing"
    },
    {
      "citation_id": "28",
      "title": "Facial action coding and hybrid deep learning architectures for autism detection",
      "authors": [
        "A Saranya",
        "R Anandan"
      ],
      "year": "2022",
      "venue": "Intell. Autom. Soft Comput"
    },
    {
      "citation_id": "29",
      "title": "Face-based attention recognition model for children with autism spectrum disorder",
      "authors": [
        "B Banire",
        "D Al Thani",
        "M Qaraqe",
        "B Mansoor"
      ],
      "year": "2021",
      "venue": "J. Healthc. Inform. Res"
    },
    {
      "citation_id": "30",
      "title": "Deep learning (DL)-enabled system for emotional big data",
      "authors": [
        "H Wang",
        "D Tobon",
        "M Hossain",
        "A Saddik"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "Ensemble of machine learning models for an improved facial emotion recognition",
      "authors": [
        "S Pulido-Castro",
        "N Palacios-Quecan",
        "M Ballen-Cardenas",
        "S Cancino-Suarez",
        "A Rizo-Arevalo",
        "J Lopez"
      ],
      "year": "2021",
      "venue": "Ensemble of machine learning models for an improved facial emotion recognition"
    },
    {
      "citation_id": "32",
      "title": "Image pre-processing significance on regions of impact in a trained network for facial emotion recognition",
      "authors": [
        "H Arabian",
        "V Wagner-Hartl",
        "J Chase",
        "K Möller"
      ],
      "year": "2021",
      "venue": "IFAC-PapersOnLine"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "O Banos",
        "Z Comas-González",
        "J Medina"
      ],
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "A two-stage multi-modal affect analysis framework for children with autism spectrum disorder",
      "authors": [
        "J Li",
        "A Bhat",
        "R Barmaki"
      ],
      "year": "2021",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "35",
      "title": "Real-time facial emotion recognition system among children with autism based on deep learning and iot",
      "authors": [
        "F Talaat"
      ],
      "year": "2023",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "36",
      "title": "Design of emotional educational system mobile games for autistic children",
      "authors": [
        "N Heni",
        "H Hamam"
      ],
      "year": "2016",
      "venue": "2nd International Conference on Advanced Technologies for Signal and Image Processing"
    },
    {
      "citation_id": "37",
      "title": "LIFEisGAME: a facial character animation system to help recognize facial expressions",
      "authors": [
        "T Fernandes",
        "S Alves",
        "J Miranda",
        "C Queirós",
        "V Orvalho"
      ],
      "year": "2011",
      "venue": "Communications in Computer and Information Science"
    },
    {
      "citation_id": "38",
      "title": "Mesnyankina, Mobile tutoring system in facial expression perception and production for children with autism spectrum disorder",
      "authors": [
        "S Anishchenko",
        "A Sarelaynen",
        "K Kalinin",
        "A Popova",
        "N Malygina-Lastovka"
      ],
      "year": "2017",
      "venue": "VISIGRAPP 2017 -Proceedings of the 12th International Joint Conference on Computer Vision"
    },
    {
      "citation_id": "39",
      "title": "Face emotions: improving emotional skills in individuals with autism",
      "authors": [
        "A Dantas",
        "M Nascimento"
      ],
      "year": "2022",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "40",
      "title": "Effects of computerized emotional training on children with high functioning autism",
      "authors": [
        "S Piana",
        "C Malagoli",
        "M Usai",
        "A Camurri"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "41",
      "title": "FECTS: a facial emotion cognition and training system for Chinese children with autism spectrum disorder",
      "authors": [
        "G Wan",
        "F Deng",
        "Z Jiang",
        "S Song",
        "D Hu",
        "L Chen",
        "H Wang",
        "M Li",
        "G Chen",
        "T Yan",
        "J Su",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "42",
      "title": "Fostering emotion recognition in children with autism spectrum disorder",
      "authors": [
        "V Silva",
        "F Soares",
        "J Esteves",
        "C Santos",
        "A Pereira"
      ],
      "year": "2021",
      "venue": "Multimodal Technol. Interact"
    },
    {
      "citation_id": "43",
      "title": "Intelligent system for the recognition of facial emotions: a tool for people with autism spectrum disorder",
      "authors": [
        "F Rojas",
        "J Silva",
        "F Betancourt"
      ],
      "year": "2021",
      "venue": "ARPN J. Eng. Appl. Sci"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition using one-shot learning for human-computer interactions",
      "authors": [
        "S Karanchery",
        "S Palaniswamy"
      ],
      "year": "2021",
      "venue": "ICCISc 2021 -2021 International Conference on Communication, Control and Information Sciences, Proceedings"
    },
    {
      "citation_id": "45",
      "title": "EmoAnim: a serious game for screening children with autism using emotions in animations",
      "authors": [
        "E Dizicheh",
        "H Moradi",
        "M Nezam Abadi",
        "F Shahrokh",
        "R Samani",
        "L Kashani-Vahid"
      ],
      "year": "2021",
      "venue": "Proceedings of the 3rd International Serious Games Symposium"
    },
    {
      "citation_id": "46",
      "title": "An interactive serious game to target perspective taking skills among children with asd: a usability testing",
      "authors": [
        "P Ghanouni",
        "T Jarus",
        "J Zwicker",
        "J Lucyshyn"
      ],
      "year": "2021",
      "venue": "Behav. Inf. Technol"
    },
    {
      "citation_id": "47",
      "title": "EEG analysis of facial affect recognition process of individuals with ASD performance prediction leveraging social context",
      "authors": [
        "J Fan",
        "E Bekele",
        "Z Warren",
        "N Sarkar"
      ],
      "year": "2017",
      "venue": "2017 7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "48",
      "title": "Recognition of positive and negative valence states in children with autism spectrum disorder (ASD) using discrete wavelet transform (DWT) analysis of electrocardiogram signals (ECG)",
      "authors": [
        "A Bagirathan",
        "J Selvaraj",
        "A Gurusamy",
        "H Das"
      ],
      "year": "2020",
      "venue": "J. Ambient Intell. Humaniz. Comput"
    },
    {
      "citation_id": "49",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "S Piana",
        "A Staglia Ńo",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Trans. Interact. Intell. Syst"
    },
    {
      "citation_id": "50",
      "title": "Automatic emotion recognition in robot-children interaction for ASD treatment",
      "authors": [
        "M Leo",
        "M Coco",
        "P Carcagnì",
        "C Distante",
        "M Bernava",
        "G Pioggia",
        "G Palestra"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "Development and evaluation of emotional robots for children with autism spectrum disorders",
      "authors": [
        "M Jeon",
        "R Zhang",
        "W Lehman",
        "S Fakhrhosseini",
        "J Barnes",
        "C Park"
      ],
      "year": "2015",
      "venue": "Communications in Computer and Information Science"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition via face tracking with RealSense™ 3D camera for children with autism",
      "authors": [
        "T Tang",
        "G Chen",
        "P Winoto"
      ],
      "year": "2017",
      "venue": "IDC 2017 -Proceedings of the 2017 ACM Conference on Interaction Design and Children"
    },
    {
      "citation_id": "53",
      "title": "Exploring the design space of an augmented display for conveying facial expressions for people with autism",
      "authors": [
        "S Chung",
        "U Oh"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct"
    },
    {
      "citation_id": "54",
      "title": "Facial emotion recognition system for autistic children: a feasible study based on FPGA implementation",
      "authors": [
        "K Smitha",
        "A Vinod"
      ],
      "year": "2015",
      "venue": "Med. Biol. Eng. Comput"
    },
    {
      "citation_id": "55",
      "title": "Feasibility testing of a wearable behavioral aid for social learning in children with autism",
      "authors": [
        "J Daniels",
        "N Haber",
        "C Voss",
        "J Schwartz",
        "S Tamura",
        "A Fazel",
        "A Kline",
        "P Washington",
        "J Phillips",
        "T Winograd",
        "C Feinstein",
        "D Wall"
      ],
      "year": "2018",
      "venue": "Appl. Clin. Inform"
    },
    {
      "citation_id": "56",
      "title": "Hardware efficient FPGA implementation of emotion recognizer for autistic children",
      "authors": [
        "K Smitha",
        "A Vinod"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Electronics, Computing and Communication Technologies"
    },
    {
      "citation_id": "57",
      "title": "Helping neuro-typical individuals to \"read\" the emotion of children with autism spectrum disorder: an internetof-things approach",
      "authors": [
        "T Tang"
      ],
      "year": "2016",
      "venue": "Proceedings of IDC 2016 -The 15th International Conference on Interaction Design and Children"
    },
    {
      "citation_id": "58",
      "title": "Visual face scanning and emotion perception analysis between autistic and typically developing children",
      "authors": [
        "U Syeda",
        "Z Zafar",
        "Z Islam",
        "S Tazwar",
        "M Rasna",
        "K Kise",
        "M Ahad"
      ],
      "year": "2017",
      "venue": "UbiComp/ISWC 2017 -Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "59",
      "title": "Educational tool for testing emotion recognition abilities in adolescents",
      "authors": [
        "C Costescu",
        "A Rosan",
        "A Hathazi",
        "N Brigitta",
        "A Kovari",
        "J Katona",
        "S Thill",
        "I Heldal"
      ],
      "year": "2020",
      "venue": "Acta Polytech. Hung"
    },
    {
      "citation_id": "60",
      "title": "Supporting e-learning with emotion regulation for students with autism spectrum disorder",
      "authors": [
        "H.-C Chu",
        "-J Tsai",
        "M.-J Liao",
        "Y.-M Chen",
        "J.-Y Chen"
      ],
      "year": "2020",
      "venue": "Educ. Technol. Soc"
    },
    {
      "citation_id": "61",
      "title": "Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and stratified cross validation",
      "authors": [
        "Y.-D Zhang",
        "Z.-J Yang",
        "H.-M Lu",
        "X.-X Zhou",
        "P Phillips",
        "Q.-M Liu",
        "S.-H Wang"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "62",
      "title": "Emotion recognition of static and dynamic faces in autism spectrum disorder",
      "authors": [
        "P Enticott",
        "H Kennedy",
        "P Johnston",
        "N Rinehart",
        "B Tonge",
        "J Taffe",
        "P Fitzgerald"
      ],
      "year": "2014",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "63",
      "title": "Evaluating methods for emotion recognition based on facial and vocal features",
      "authors": [
        "M Ley",
        "M Egger",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "64",
      "title": "Towards voice based prediction and analysis of emotions in ASD children",
      "authors": [
        "P Sukumaran",
        "K Govardhanan"
      ],
      "year": "2021",
      "venue": "J. Intell. Fuzzy Syst"
    },
    {
      "citation_id": "65",
      "title": "Real-time feedback based on emotion recognition for improving children's metacognitive monitoring skill",
      "authors": [
        "X Ruan",
        "C Palansuriya",
        "A Constantin"
      ],
      "year": "2022",
      "venue": "Proceedings of Interaction Design and Children"
    },
    {
      "citation_id": "66",
      "title": "Evaluating the impact of voice activity detection on speech emotion recognition for autistic children",
      "authors": [
        "M Milling",
        "A Baird",
        "K Bartl-Pokorny",
        "S Liu",
        "A Alcorn",
        "J Shen",
        "T Tavassoli",
        "E Ainger",
        "E Pellicano",
        "M Pantic",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Front. Comput. Sci"
    },
    {
      "citation_id": "67",
      "title": "Speech emotion recognition to assist autistic children",
      "authors": [
        "N Chitre",
        "N Bhorade",
        "P Topale",
        "J Ramteke",
        "C Gajbhiye"
      ],
      "year": "2022",
      "venue": "Proceedings -International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2022"
    },
    {
      "citation_id": "68",
      "title": "Effects of a video-based intervention on emotion recognition for children with autism who have limited speech in China",
      "authors": [
        "Z Wang",
        "L Cheong",
        "J Tian",
        "H Wang",
        "Y Yuan",
        "Q Zhang"
      ],
      "year": "2023",
      "venue": "J. Spec. Educ. Technol"
    },
    {
      "citation_id": "69",
      "title": "Multi label classification for emotion analysis of autism spectrum disorder children using deep neural networks",
      "authors": [
        "T Praveena",
        "N Lakshmi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 3rd International Conference on Inventive Research in Computing Applications, ICIRCA 2021"
    },
    {
      "citation_id": "70",
      "title": "An audio processing approach using ensemble learning for speech-emotion recognition for children with ASD",
      "authors": [
        "D Valles",
        "R Matin"
      ],
      "year": "2021",
      "venue": "2021 IEEE World AI IoT Congress"
    },
    {
      "citation_id": "71",
      "title": "Discriminative few shot learning of facial dynamics in interview videos for autism trait classification",
      "authors": [
        "N Zhang",
        "M Ruan",
        "S Wang",
        "L Paul",
        "X Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "72",
      "title": "Aspect-based sentiment analysis of customer speech data using deep convolutional neural network and bilstm",
      "authors": [
        "S Murugaiyan",
        "S Uyyala"
      ],
      "year": "2023",
      "venue": "Cogn. Comput"
    },
    {
      "citation_id": "73",
      "title": "Human-robot facial expression reciprocal interaction platform: case studies on children with autism",
      "authors": [
        "A Ghorbandaei Pour",
        "A Taheri",
        "M Alemi",
        "A Meghdari"
      ],
      "year": "2018",
      "venue": "Int. J. Soc. Robot"
    },
    {
      "citation_id": "74",
      "title": "Recognising and classify emotion from the speech of autism spectrum disorder children for tamil language using support vector machine",
      "authors": [
        "C Sunitha Ram",
        "R Ponnusamy"
      ],
      "year": "2014",
      "venue": "Int. J. Appl. Eng. Res"
    },
    {
      "citation_id": "75",
      "title": "Typicality and emotion in the voice of children with autism spectrum condition: evidence across three languages",
      "authors": [
        "E Marchi",
        "B Schuller",
        "S Baron-Cohen",
        "O Golan",
        "S Bölte",
        "P Arora",
        "R Häb-Umbach"
      ],
      "year": "2015",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "76",
      "title": "On the trail of facial processing in autism spectrum disorders",
      "authors": [
        "D Arellano",
        "U Schaller",
        "R Rauh",
        "V Helzle",
        "M Spicker",
        "O Deussen"
      ],
      "year": "2015",
      "venue": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics"
    },
    {
      "citation_id": "77",
      "title": "A computational study of expressive facial dynamics in children with autism",
      "authors": [
        "T Guha",
        "Z Yang",
        "R Grossman",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "78",
      "title": "Psychoacoustic abilities as predictors of vocal emotion recognition in autism",
      "authors": [
        "E Globerson",
        "N Amir",
        "M Lavidor",
        "L Kishon-Rabin",
        "O Golan"
      ],
      "year": "2012",
      "venue": "Proceedings of the 6th International Conference on Speech Prosody, SP 2012"
    },
    {
      "citation_id": "79",
      "title": "Multimodal emotion perception in children with autism spectrum disorder by eye tracking study",
      "authors": [
        "Q Su",
        "F Chen",
        "H Li",
        "N Yan",
        "L Wang"
      ],
      "year": "2018",
      "venue": "IECBES 2018 -Proceedings"
    },
    {
      "citation_id": "80",
      "title": "Emotion recognition in autism spectrum disorder: does stylization help?",
      "authors": [
        "M Spicker",
        "D Arellano",
        "U Schaller",
        "R Rauh",
        "V Helzle",
        "O Deussen"
      ],
      "year": "2016",
      "venue": "Proceedings of the ACM Symposium on Applied Perception"
    },
    {
      "citation_id": "81",
      "title": "Is emotion recognition impaired in individuals with autism spectrum disorders?",
      "authors": [
        "J Tracy",
        "R Robins",
        "R Schriber",
        "M Solomon"
      ],
      "year": "2011",
      "venue": "J. Autism Dev. Disord"
    },
    {
      "citation_id": "82",
      "title": "",
      "authors": [
        "O Banos",
        "Z Comas-González",
        "J Medina"
      ],
      "venue": ""
    },
    {
      "citation_id": "83",
      "title": "PlayCube: designing a tangible playware module for human-robot interaction",
      "authors": [
        "V Silva",
        "F Soares",
        "J Esteves",
        "A Pereira"
      ],
      "year": "2019",
      "venue": "Advances in Intelligent Systems and Computing"
    },
    {
      "citation_id": "84",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "Am. Psychol"
    },
    {
      "citation_id": "85",
      "title": "World medical association declaration of helsinki: ethical principles for medical research involving human subjects",
      "authors": [
        "W Association"
      ],
      "year": "2013",
      "venue": "JAMA"
    },
    {
      "citation_id": "86",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: a tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "87",
      "title": "A study on the perceptions of autistic adolescents towards mainstream emotion recognition technologies",
      "authors": [
        "W Nijeweme-D'hollosy",
        "T Notenboom",
        "O Banos"
      ],
      "year": "2018",
      "venue": "Proceedings"
    },
    {
      "citation_id": "88",
      "title": "Encyclopedia of Bioinformatics and Computational Biology",
      "year": "2019",
      "venue": "Encyclopedia of Bioinformatics and Computational Biology"
    },
    {
      "citation_id": "89",
      "title": "Beyond accuracy, F-score and ROC: a family of discriminant measures for performance evaluation",
      "authors": [
        "M Sokolova",
        "N Japkowicz",
        "S Szpakowicz"
      ],
      "year": "2006",
      "venue": "Australasian Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "90",
      "title": "The ECHOS platform to enhance communication for nonverbal children with autism: a case study",
      "authors": [
        "K Johnson",
        "J Narain",
        "C Ferguson",
        "R Picard",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "91",
      "title": "Towards explainable and privacy-preserving artificial intelligence for personalisation in autism spectrum disorder",
      "authors": [
        "M Mahmud",
        "M Kaiser",
        "M Rahman",
        "T Wadhera",
        "D Brown",
        "N Shopland",
        "A Burton",
        "T Hughes-Roberts",
        "S Mamun",
        "C Ieracitano"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "92",
      "title": "Guidelines for conducting research studies with the autism community",
      "authors": [
        "E Gowen",
        "R Taylor",
        "T Bleazard",
        "A Greenstein",
        "P Baimbridge",
        "D Poole"
      ],
      "year": "2019",
      "venue": "Guidelines for conducting research studies with the autism community"
    },
    {
      "citation_id": "93",
      "title": "Mendeley: creating communities of scholarly inquiry through research collaboration",
      "authors": [
        "H Zaugg",
        "E Richard",
        "I West",
        "D Tateishi",
        "D Randall"
      ],
      "year": "2011",
      "venue": "TechTrends"
    }
  ]
}