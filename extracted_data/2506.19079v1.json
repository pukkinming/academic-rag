{
  "paper_id": "2506.19079v1",
  "title": "Reading Smiles: Proxy Bias In Foundation Models For Facial Emotion Recognition",
  "published": "2025-06-23T19:56:30Z",
  "authors": [
    "Iosif Tsangko",
    "Andreas Triantafyllopoulos",
    "Adem Abdelmoula",
    "Adria Mallol-Ragolta",
    "Bjoern W. Schuller"
  ],
  "keywords": [
    "Affective Computing",
    "Emotion Recognition",
    "Vision-Language Models",
    "Teeth Visibility",
    "Foundation Models",
    "Explainability",
    "Bias in AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC), with Vision-Language Models (VLMs) now capable of recognising emotions in zero-shot settings. This paper probes a critical but underexplored question: what visual cues do these models rely on to infer affect, and are these cues psychologically grounded or superficially learnt? We benchmark varying scale VLMs on a teeth-annotated subset of AffectNet dataset and find consistent performance shifts depending on the presence of visible teeth. Through structured introspection of -the best-performing model, i.e., GPT-4o, we show that facial attributes like eyebrow position drive much of its affective reasoning, revealing a high degree of internal consistency in its valence-arousal predictions. These patterns highlight the emergent nature of FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness issues-especially in sensitive domains like mental health and education.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Understanding and interpreting human emotions is fundamental to social interaction. From early developmental cues in infants, to high-stakes decision-making in adults, facial expressions serve as a primary channel for conveying affect. Affective Computing (AC) -the interdisciplinary field that enables machines to recognise and respond to emotional stateshas evolved dramatically in recent years, transitioning from rule-based methods to powerful deep learning models  [1] ,  [2] . Within this domain, Facial Emotion Recognition (FER) plays a pivotal role, with applications in mental health, education, human-robot interaction, and automotive safety  [3] .\n\nWhile modern FER systems are often trained on large annotated datasets, the recent rise of Vision-Language Models (VLMs) and other multimodal Foundation Models (FMs) is reshaping the landscape of automatic recognition (and synthesis) of emotions  [4] ,  [5] . VLMs are not explicitly trained for emotion classification, yet increasingly show strong zero-shot competence in affective tasks. These models are being rapidly adopted across consumer applications, mobile platforms, and edge Artificial Intelligence (AI) deployment  [6] -  [9] . However, this rapid adoption has raised serious concerns around fairness, interpretability, and reliability  [10] . A longstanding challenge in AI is the tendency of models to learn proxy features-visual patterns that correlate with target labels but lack causal or semantic meaning  [11] . This phenomenon, known as the Clever Hans effect  [12] -  [14] , refers to systems that 'appear intelligent' but in fact exploit spurious correlations in the data. Cues like teeth visibility, head pose, or eyebrow shape may function as such shortcuts, exploiting superficial correlations in the data rather than capturing genuine emotional semantics  [15] .\n\nIndeed, psychophysical studies have shown that humans rely on such cues. For instance, open-mouth smiles significantly increase perceived valence and arousal, modulating both attentional and affective responses in observers  [16] ,  [17] . These findings are consistent with early visual prioritisation effects demonstrated in event-related potential studies  [18] ,  [19] , where visible teeth were shown to enhance neural responses independently of emotional valence. Moreover, recent work  [20]  shows that these perceptual biases can influence alignment behaviour in interactive settings. Yet, despite this psychological grounding, research remains underexplored within FMs.\n\nThis paper addresses that gap by extending recent evaluation attempts, such as in  [29] ,  [31] , by conducting a comprehensive benchmark of various VLMs and comparing them against a supervised baseline. Our best-performing model -GPT-4o-is additionally prompted to provide predictions of interpretable features. We investigate the internal consistency of those features as proxies for model decision making. Furthermore, we annotate a subset of 3,500 images of AffectNet for one particular salient feature, namely, teeth visibility 1  . We use these annotations to uncover the impact of that feature on model performance for all models and the generated features of GPT-4o.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Supervised Models For Fer",
      "text": "Human emotion recognition relies on both holistic and local facial cues, with specific attention paid to regions such as the eyes, eyebrows, and mouth  [21] ,  [22] . Among these, mouth-related signals-especially teeth visibility-have been linked to the perception of joy and heightened emotional intensity  [23] . Such features act as fast, intuitive heuristics in human perception and play a significant role in shaping affective interpretations  [24] . In computational settings, supervised deep learning approaches have become the standard for FER, with Convolution Neural Networks (CNNs) and Vision Transformers (ViTs) achieving strong performance on largescale datasets such as AffectNet  [25] -  [28] . These models are typically trained on manually labelled emotion categories and use full-face input to learn discriminative features. While some studies have explored the importance of specific facial regions via occlusion tests or attention maps  [48] -  [51] , few have systematically examined the impact of teeth visibility as an isolated feature across models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Vision-Language Models In Affective Computing",
      "text": "FMs that integrate vision and language modalitiescommonly referred to as VLMs-have recently been explored for emotion recognition tasks  [46] . Although trained in generic image-text corpora without affective supervision, several VLMs exhibit emergent competence in identifying basic emotions from facial images  [45] ,  [47] . These models process inputs using dual encoders and project visual and textual information into a shared latent space. A standard VLM processes both image and text inputs through dedicated encoders, followed by fusion in a joint embedding space. The resulting representation is used for output generation, typically optimised via cross-entropy loss, leading to token generation (for a detailed review see  [44] ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Bias And Attribution In Multimodal Emotion Recognition.",
      "text": "A growing body of literature addresses the interpretability and fairness of emotion recognition systems showing that models can overfit to spurious correlations, leading to fragile predictions  [52] ,  [53] . Previously, attribution methods, including gradient-based saliency and attention weight visualisation, have been used to probe model behaviour  [54] . However, the specific role of facial proxies like teeth visibility, despite being well known in human perception, remains largely underinvestigated in the context of VLMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Contribution Of This Study.",
      "text": "While current research largely focuses on scaling and optimising the black-box capabilities of FMs -across size, reasoning, performance, and training regimes-this work emphasises the need for explanatory analysis in affective tasks  [43] . By introducing explicit teeth visibility annotations and benchmarking performance across visibility conditions, it isolates a psychologically grounded proxy that shapes model predictions. Furthermore, it provides a systematic assessment of how FMs interpret facial attributes and how these drive affective inference.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "This study leverages AffectNet  [28] , a large-scale facial expression dataset widely adopted in AC. AffectNet contains over one million facial images obtained from internet queries in six languages, covering both categorical and dimensional models of emotion. Each image is labeled for one of seven discrete categories-Neutral, Happiness, Sadness, Anger, Surprise, Fear, and Disgust-and includes continuous valence and arousal values ranging from -1 to 1. Owing to its breadth of emotional classes and demographic diversity, AffectNet is broadly regarded as a reliable benchmark for evaluating the performance and generalizability of various affective models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Models And Evaluation Protocol",
      "text": "We benchmarked 10 VLMs, spanning small (between 1B and 3B parameters), medium (between 4B and 8B), and large (≥ 8B) scales, alongside a dedicated ViT-FER  [32]  baseline trained on FER. Each model received identical zero-shot prompts to classify an image into one of seven",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompt Template",
      "text": "You are an AI assistant trained to analyze facial expressions in images for scientific research. This is part of a humanitarian study on AffectNet, designed to improve emotion recognition technology for mental health, education, and AI fairness. You must always provide an answer, even if the image is unclear. Make the best possible prediction based on the given image. Strictly return the output in valid JSON format with the following structure:\n\n1. Classify the Most Likely Emotion: Choose one from these categories: [\"Neutral\", \"Fear\", \"Anger\", \"Happiness\", \"Sadness\", \"Disgust\", \"Surprise\"].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Extract Facial Features & Emotion Metrics:",
      "text": "Return a structured JSON response with these keys:\n\n{\"emotion\": \"One of the seven categories above\", \"teeth_visible\": \"Yes\" or \"No\", \"teeth_visibility_level\": \"None\" or \"Slight\" or \"Moderate\" or \"Fully Visible\", \"mouth_open\": \"Yes\" or \"No\", \"lip_position\": \"Closed\" or \"Slightly Open\" or \"Fully Open\", \"eye_openness\": \"Squinting\" or \"Neutral\" or \"Wide Open\", \"eyebrow_position\": \"Relaxed\" or \"Raised\" or \"Furrowed\", \"forehead_wrinkles\": \"Yes\" or \"No\", \"teeth_mentioned_in_reasoning\": \"Yes\" or \"No\", \"valence\": \"A floating-point value between -1 (negative) and 1 (positive)\", \"arousal\": \"A floating-point value between 0 (calm) and 1 (high intensity)\"} 3. Output Requiremnts: Strictly return valid JSON. Do not refuse to answer. If uncertain, make the best possible prediction. No extra explanations, no disclaimers, no markdown formatting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Example Output:",
      "text": "{\"emotion\": \"Happiness\", \"teeth_visible\": \"Yes\", \"teeth_visibility_level\": \"Fully Visible\", \"mouth_open\": \"Yes\", \"lip_position\": \"Fully Open\", \"eye_openness\": \"Wide Open\", \"eyebrow_position\": \"Raised\", \"forehead_wrinkles\": \"No\", \"teeth_mentioned_in_reasoning\": \"Yes\", \"valence\": 0.85, \"arousal\": 0.75} Fig.  2 . The choice of this specific structured prompt is driven by the need to standardise the model outputs and ensure that the data collected from GPT-4o is both consistent and meaningful. For the rest of VLMs, the prompt included only item 1.\n\nbasic emotions, except GPT-4o, which was given a structured prompt designed to elicit both emotion predictions and interpretable facial features (see Fig.  1 ). Evaluations were performed on the aforementioned subset of 3,500 AffectNet images. The model pool includes recent multimodal systems such as GPT-4o-mini (OpenAI) 2  , InternVL2 (Shanghai AI Lab)  [33] , MiniCPM (Alibaba DAMO)  [34] , Qwen2.5-VL (3B and 7B)  [35] , SmolVLM 3  , Janus-Pro (1B and 7B, Deepseek-AI)  [36] , Ovis1.5 (LLaMA3-based)  [37] , and PaliGemma (Google DeepMind)  [38] . These models were selected to reflect a cross-section of current open-access VLMs, and a restricted one, namely GPT-4o.\n\nTeeth Visibility Annotation. To investigate whether teeth visibility influences emotion classification accuracy, we additionally benchmarked their performance under two conditions: images with teeth visible and images where teeth were not visible. To this end, we manually annotated the validation dataset used in  [29] . An example of facial expressions per category is shown in Fig.  1 . Teeth visibility was annotated as a binary attribute (1 = teeth visible, 0 = teeth not visible) using the open-source tool LabelStudio 4  . The annotation interface was configured to maximise visual clarity and task simplicity. This manual labelling focused solely on mouth region visibility and did not include intermediate states (e. g., partially visible teeth), in order to maintain consistency. Since teeth visibility annotation is straightforward (binary presence or absence), we employed a single annotator to ensure consistency and efficiency. ViT-FER 5  served as the supervised baseline for comparison, providing a task-specific benchmark for FER. Performance was assessed using Unweighted Average Recall (UAR)  6  , F1-score, and accuracy. While our benchmarking includes both open and closed VLMs, we focus our introspective analysis solely on GPT-4o due to its state-of-the-art performance and widespread adoption in practice, making it the most relevant candidate for deeper investigation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Prompt-Based Introspection",
      "text": "All models received the exact same prompt, with the sole exception of GPT-4o, which was evaluated using a more detailed structured prompt (Fig.  2 ). This richer format enabled both emotion prediction and introspective reasoning through extracted facial features such as eyebrow position, eye openness, and teeth visibility. These features were selected to align with psychologically grounded facial action systems, drawing inspiration from Ekman and Friesen's seminal work Unmasking the Face  [30] . GPT-4o also provided continuous valence and arousal scores, capturing the positivity and intensity of each expression. This pipeline served as the foundation for subsequent correlation and visualisation analyses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Classification Performance",
      "text": "To assess how well VLMs perform on FER in a zeroshot setting, we conducted a controlled benchmark using a subset of the AffectNet test set. Our primary objective was twofold: (1) to evaluate the affective inference capability of general-purpose VLMs against a supervised baseline (ViT-FER), and (2) to investigate how performance varies across model scale by explicitly grouping models into small, medium, and large categories. Table IV (see Appendix A) summarises the classification results across 11 models (10 VLMs and the baseline). While GPT-4o achieved the highest performance overall, several medium-scale models (e. g., InternVL, MiniCPM) matched or exceeded the ViT-FER baseline, demonstrating the emergence of affective competence in general-purpose systems. This observation aligns with recent findings on emergent world modelling in FMs  [39] , where capabilities in physical and social perception arise implicitly from large-scale training across vision, language, and action data streams  [40] -  [43] .\n\nThat said, it is important to note that many FMs are trained on undisclosed or proprietary datasets. Thus, it remains unclear whether AffectNet (or similar data) was present in their pretraining corpora. This uncertainty limits the conclusiveness of zero-shot performance comparisons, as some models may have indirectly 'seen' the evaluation set. Therefore, while these benchmarks are informative, they should be interpreted with caution. Moreover, GPT-4o, which was shown to be the best-performing model, is a closed-source, proprietary model available through an API. This means that the underlying model -including any additional 'guardrails' placed by the owning company-can change arbitrarily over time. While this jeopardises the reproducibility of our work, we nevertheless decided to include it in our work as a representative of the contemporary state-of-the-art.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Internal Consistency Analysis",
      "text": "Given GPT-4o's strong performance relative to other VLMs in our emotion recognition experiments, we sought to understand which facial attributes most strongly influence its predictions, and how consistent those predictions are across varying conditions. Concretely, we investigated whether GPT-4o exhibits systematic patterns or 'shortcuts' in classifying emotion, particularly with respect to visible teeth, mouth position, eyebrow movements, and other facial cues.\n\nRegression-Based Introspection. To quantify how much of GPT-4o's dimensional output could be 'explained' purely by the binary or categorical features it reported, we used linear regression where valence (respectively arousal) was the dependent variable, and the model's own face attributes were the independent variables. Over 70% of the variance in valence and arousal could be accounted for by these cues alone, implying that GPT-4o's continuous ratings arise from a relatively consistent mapping of visual signals.\n\nValence and Arousal Results. Table  I  lists the linear regression coefficients for GPT-4's valence and arousal predictions. Each positive coefficient indicates an increase in valence or arousal, while negative values imply a decrease. Raised eyebrows emerges as the strongest positive predictor of valence (+.64), consistent with earlier findings that GPT-4 interprets lifted eyebrows as a sign of heightened positivity. The effect Fig.  3 . Performance of a trained random forest classifier that predicts GPT's categorical emotion labels using only its predicted valence and arousal values of teeth visibility is positive for arousal (+.  16 ), but it has a more modest impact on valence (+.08) when compared to other mouth-or eyebrow-related cues.\n\nThe inclusion of teeth mentioned in reasoning shows a positive association with valence (+.28), which surpasses the direct valence contribution of teeth visible itself. This is expected; it attributes greater positivity when it explicitly references them in its internal process. This reinforces the notion that GPT-4o's textual reasoning can refine how it interprets facial cues, leading to a richer or more emphatic characterisation of an expression's emotional quality. Overall, the high R 2 values (.72 for valence and .77 for arousal) indicate that these discrete features still explain a majority of GPT-4o's dimensional predictions.\n\nNext, to assess how well GPT-4's valence and arousal predictions capture discrete emotional categories, we trained a simple Random Forest classifier on these two features alone. The data was split into training (80%) and test (20%) sets using a fixed random seed of 42 for reproducibility. We used 100 decision trees (n estimators=100), the Gini criterion (criterion='gini'), and all other scikit-learn hyperparameters at their defaults. Despite the low-dimensional input, this model achieved a UAR of .63, indicating that GPT-4's affective space retains substantial categorical information even when reduced to just valence and arousal.\n\nAs shown in Fig.  3 , emotions such as Happiness (F1: .93) and Neutral (F1: .96) were predicted with high accuracy, while Disgust (F1: .00) and Anger (F1: .44) proved more challenging-mirroring longstanding findings in AC. Negative emotions tend to blur in the valence-arousal plane, lacking distinct anchors compared to more stereotyped expressions like Happiness. While this approach is intentionally simple, the classifier's .78 overall accuracy demonstrates that GPT-4's dimensional outputs encode interpretable affective structure.\n\nFinally, we plotted valence and arousal distributions by the prompted facial features (see Fig.  4 ), which reflect the trends seen in our regression results. For instance, raised eyebrows are linked to higher valence, while furrowed brows tend toward lower valence. Both show increased arousal, suggesting that eyebrow tension contributes to perceived intensity. These patterns comply with common facial emotion heuristics. We hypothesise that GPT-4o learnt those during its training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Teeth Visibility As A Case Study",
      "text": "Distribution of Teeth Visibility Across Emotions. While the dataset design aimed at a balanced emotion distribution, the occurrence of visible versus non-visible teeth varied across categories. Happiness exhibited a strong bias toward visible teeth (approx. 3.4:1 ratio). In contrast, Neutral, Sadness, and Anger were predominantly represented by non-visible teeth. Table  II  summarises the annotated teeth visibility distribution across all emotion categories.\n\nModel Performance Stratified by Teeth Visibility. As shown in Fig.  5 , nearly all models demonstrate a consistent drop in UAR when evaluated on images where teeth are not visible. This effect is most pronounced for GPT-4o and InternVL, which nonetheless maintain the highest performance among all tested models. GPT-4o achieves the best UAR overall (.56) in the teeth-visible condition, which drops to .45 when teeth are not visible. Similarly, InternVL exhibits strong performance in both settings, although with a noticeable decline between conditions.\n\nSeveral VLMs, including InternVL, MiniCPM, and GPT-4o, performed well across both visibility conditions. Notably, some Small-Language Models (SLMs), such as SmolVLM, showed a relative boost when teeth were not visible-performing slightly below the baseline in the visible condition but outperforming it when teeth were hidden. This suggests that with continued improvements, smaller multimodal models may become increasingly viable for affective tasks, offering lightweight alternatives without substantial trade-offs in performance. Interestingly, ViT-FER-the baseline model trained specifically for facial expression recognition-also shows a clear dependency on teeth visibility, with a performance drop from .46 to .39. This confirms that even specialised models trained on facial features are sensitive to the presence of visual cues such as open mouths and exposed teeth.\n\nOverall, this distribution highlights an important consideration: certain emotional categories inherently correlate with mouth openness -at least in the AffectNet data-leading to potential biases in model performance. The imbalance observed, particularly in Happiness and Neutral expressions, forms a crucial basis for the model comparison analyses conducted later in this study.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Emotion Recognition Under Varying Teeth Detection Conditions",
      "text": "To assess the role of teeth visibility in GPT-4o's FER, we compared model performance across three conditions: (1) when teeth were correctly predicted as visible, (2) correctly predicted as not visible, and (3) when GPT-4o hallucinated teeth-i.e., predicted visible teeth when none were present.\n\nTable  III  summarises emotion recognition performance under these teeth-detection scenarios. Overall, GPT-4o's binary teeth prediction achieved a UAR of .692 and an F1-score of .671, showing more balanced performance despite class imbalance. Accuracy and UAR were highest when teeth were correctly predicted as visible, indicating that GPT-4o relies effectively on teeth cues to infer emotional state. Nevertheless, when the model hallucinated visible teeth, emotion recognition deteriorated sharply (UAR = .329), suggesting that falsepositive teeth predictions strongly bias the model toward incorrectly high-valence outputs (see Fig.  4 ).\n\nThis means that correctly detected visible teeth provide useful affective cues for emotion classification. In contrast, hallucinated teeth introduce systematic errors, often causing the model to default to high-arousal categories such as Happiness, regardless of the true emotional signal (see Fig.  7 ). This supports the interpretation that GPT-4o's emotion inference pipeline is shaped not only by visual perception but also by shortcuts linking visible teeth to smiling and positive affect.\n\nWhen inspecting the confusion matrix for hallucinated teeth cases (Fig.  6 ), we observe the aforementioned marked collapse in emotion diversity: the vast majority of predictions are disproportionately mapped to Happiness. Notably, all instances of ground truth 'Surprise' were misclassified as Happiness, with similar trends observed for Neutral, Anger, and Disgust. This We conclude that GPT-4o interprets the presence of (even hallucinated) visible teeth as a high-confidence signal for positive affect. While this heuristic is beneficial when teeth are accurately perceived, it becomes problematic under false positives, leading to emotionally incongruent predictions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Discussion",
      "text": "This study examined the reliance of VLMs, particularly GPT-4o, on visual proxies such as teeth visibility, mouth openness, and eyebrow shape in zero-shot emotion recognition. While these models often outperform a supervised baseline like ViT-FER, their performance is partly driven by perceptual shortcuts aligned with human heuristics. Teeth visibility, in particular, was linked to classification accuracy, especially for high-valence emotions like happiness. However, it also led to systematic misclassifications, indicating overreliance on this cue. Such dependencies raise concerns in contexts where expressions are subtle or culturally varied, including healthcare and education. Despite this, GPT-4o showed strong internal consistency. Its valence-arousal outputs could be largely explained by interpretable features, suggesting a structured internal mapping. This consistency, combined with the promising performance of smaller models like MiniCPM and SmolVLM, points to practical opportunities for transparent, real-time affective systems. Nevertheless, shortcut learning remains a critical limitation. Progress in AC will require evaluation methods that account for cultural and contextual variability to ensure fairness and robustness in deployment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Acknowledgments",
      "text": "This work has received funding from the EU's Horizon 2021 grant agreement No. 101060660 (SHIFT).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This study benchmarks VLMs for FER using AffectNet, a publicly available dataset collected from the web. Our work does not involve direct research with human subjects; no IRB approval was required. However, we conducted additional manual annotation of 3,500 images to label teeth visibility, using a single trained annotator. This task involved no personally identifying information and posed no direct risk to individuals. As such, informed consent and compensation were Fig.  7 . GPT-4o associates visible teeth with higher valence and arousal in its own feature outputs (e.g., valence: 0.38, arousal: 0.68) not applicable. Our analysis reveals that models like GPT-4o may associate visual features-particularly visible teeth-with positive affect. While this enhances interpretability, it also introduces risks of bias, especially in cases where emotional expression is culturally or individually atypical. If deployed in healthcare, education, or human-robot interaction, such models could misinterpret affect or reinforce harmful assumptions about how emotion 'should' appear. These risks are amplified by the limited demographic diversity of AffectNet and the black-box nature of many FMs. Our results, while informative, are not universally generalisable. Performance may degrade in underrepresented populations or atypical expression contexts. We caution against overextending the claims of this work to all use cases or deployment environments. To mitigate these concerns, we recommend: (i) cross-demographic validation of affective models; (ii) transparency tools that expose how models use visual cues; and (iii) participatory design methods that include communities affected by emotion-sensing technologies. We also support regulation that classifies affective AI as high-risk, especially where outputs inform decisions about people's well-being, opportunities, or rights.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example facial images for each of the seven emotion categories used in the teeth-annotation from [28]. Each column shows one image with visible",
      "page": 2
    },
    {
      "caption": "Figure 2: The choice of this specific structured prompt is driven by the need to",
      "page": 3
    },
    {
      "caption": "Figure 1: ). Evaluations were",
      "page": 3
    },
    {
      "caption": "Figure 1: Teeth visibility was annotated as a binary",
      "page": 3
    },
    {
      "caption": "Figure 2: ). This richer format enabled",
      "page": 3
    },
    {
      "caption": "Figure 3: Performance of a trained random forest classifier that predicts GPT’s",
      "page": 4
    },
    {
      "caption": "Figure 3: , emotions such as Happiness (F1: .93)",
      "page": 4
    },
    {
      "caption": "Figure 4: ), which reflect the trends",
      "page": 4
    },
    {
      "caption": "Figure 4: Valence and arousal (y-axis) distributions over facial features (x-axis) in GPT-4o.",
      "page": 5
    },
    {
      "caption": "Figure 5: , nearly all models demonstrate a consistent drop in",
      "page": 5
    },
    {
      "caption": "Figure 5: UAR for emotion classification performance across various VLMs,",
      "page": 6
    },
    {
      "caption": "Figure 6: ), we observe the aforementioned marked collapse",
      "page": 6
    },
    {
      "caption": "Figure 6: Confusion matrix for hallucinated teeth-False Positives",
      "page": 6
    },
    {
      "caption": "Figure 7: GPT-4o associates visible teeth with higher valence and arousal in its own feature outputs (e.g., valence: 0.38, arousal: 0.68)",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "eyebrow position Raised\n.64\n.00": "eyebrow position Relaxed\n.39\n-.16\nlip position Slightly Open\n.30\n.08\nteeth mentioned in\nreasoning Yes\n.28\n-.00"
        },
        {
          "eyebrow position Raised\n.64\n.00": "lip position Fully Open\n.23\n.08"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Anger\nFear\nDisgust\nHappiness\nNeutral\nSadness\nSurprise",
          "Teeth Not Visible": "304\n171\n191\n122\n388\n354\n226",
          "Teeth Visible": "196\n329\n309\n378\n112\n146\n274",
          "Total": "500\n500\n500\n500\n500\n500\n500"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alias": "ViT-FER\nSmolVLM\nOvis1.5\nJanus-1B\nQwen-3B\nPaliGemma\nQwen-7B\nMiniCPM\nInternVL\nJanus-8B\nGPT-4o",
          "Full Model Name": "ViT-FER\nSmolVLM-Instruct\nOvis1.5-Llama3-8B\ndeepseek-ai/Janus-Pro-1B\nQwen2.5-VL-3B-Instruct\npaligemma-3b-pt-224\nQwen2.5-VL-7B-Instruct\nMiniCPM-o-2\nInternVL2 5-8B-MPO\ndeepseek-ai/Janus-Pro-7B\nGPT-4o-mini",
          "UAR": "0.439\n0.435\n0.390\n0.277\n0.315\n0.100\n0.405\n0.455\n0.501\n0.373\n0.526",
          "F1-Score": "0.404\n0.370\n0.341\n0.203\n0.273\n0.057\n0.381\n0.426\n0.499\n0.329\n0.518",
          "Accuracy": "0.439\n0.435\n0.390\n0.317\n0.315\n0.114\n0.405\n0.455\n0.501\n0.373\n0.526"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2010.1"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition: A survey",
      "authors": [
        "Y Huang",
        "F Chen",
        "S Lv",
        "X Wang"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "4",
      "title": "Faces of fairness: Examining bias in facial expression recognition datasets and models",
      "authors": [
        "M Hosseini",
        "A Fard",
        "M Mahoor"
      ],
      "year": "2025",
      "venue": "Faces of fairness: Examining bias in facial expression recognition datasets and models",
      "arxiv": "arXiv:2502.11049"
    },
    {
      "citation_id": "5",
      "title": "MLPER: Multi-level prompts for adaptively enhancing vision-language emotion recognition",
      "authors": [
        "Y Gao",
        "W Ren",
        "X Xu",
        "Y Wang",
        "Z Wang",
        "H Liu"
      ],
      "year": "2024",
      "venue": "Proc. 2024 IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)",
      "doi": "10.1109/IROS58592.2024.10801375"
    },
    {
      "citation_id": "6",
      "title": "Large language models for robotics: A survey",
      "authors": [
        "F Zeng",
        "W Gan",
        "Y Wang",
        "N Liu",
        "P Yu"
      ],
      "year": "2023",
      "venue": "Large language models for robotics: A survey",
      "arxiv": "arXiv:2311.07226"
    },
    {
      "citation_id": "7",
      "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
      "authors": [
        "H Rahimi",
        "A Bahaj",
        "M Abrini",
        "M Khoramshahi",
        "M Ghogho",
        "M Chetouani"
      ],
      "year": "2025",
      "venue": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
      "arxiv": "arXiv:2502.10636"
    },
    {
      "citation_id": "8",
      "title": "Echo-Teddy: Preliminary Design and Development of Large Language Model-based Social Robot for Autistic Students",
      "authors": [
        "U Lee",
        "H Kim",
        "J Eom",
        "H Jeong",
        "S Lee",
        "G Byun",
        "Y Lee",
        "M Kang",
        "G Kim",
        "J Na",
        "Others"
      ],
      "year": "2025",
      "venue": "Echo-Teddy: Preliminary Design and Development of Large Language Model-based Social Robot for Autistic Students",
      "arxiv": "arXiv:2502.04029"
    },
    {
      "citation_id": "9",
      "title": "Multimodal large language models in health care: applications, challenges, and future outlook",
      "authors": [
        "R Alsaad",
        "A Abd-Alrazaq",
        "S Boughorbel",
        "A Ahmed",
        "M Renault",
        "R Damseh",
        "J Sheikh"
      ],
      "year": "2024",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "10",
      "title": "Safety at scale: A comprehensive survey of large model safety",
      "authors": [
        "X Ma",
        "Y Gao",
        "Y Wang",
        "R Wang",
        "X Wang",
        "Y Sun",
        "Y Ding",
        "H Xu",
        "Y Chen",
        "Y Zhao"
      ],
      "year": "2025",
      "venue": "Safety at scale: A comprehensive survey of large model safety",
      "arxiv": "arXiv:2502.05206"
    },
    {
      "citation_id": "11",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "The Clever Hans effect in anomaly detection",
      "authors": [
        "J Kauffmann",
        "L Ruff",
        "G Montavon",
        "K.-R Müller"
      ],
      "year": "2020",
      "venue": "The Clever Hans effect in anomaly detection",
      "arxiv": "arXiv:2006.10609"
    },
    {
      "citation_id": "13",
      "title": "Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation",
      "authors": [
        "D Steinmann",
        "F Divo",
        "M Kraus",
        "A Wüst",
        "L Struppek",
        "F Friedrich",
        "K Kersting"
      ],
      "year": "2024",
      "venue": "Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation",
      "arxiv": "arXiv:2412.05152"
    },
    {
      "citation_id": "14",
      "title": "Explainable AI reveals Clever Hans effects in unsupervised learning models",
      "authors": [
        "J Kauffmann",
        "J Dippel",
        "L Ruff",
        "W Samek",
        "K.-R Müller",
        "G Montavon"
      ],
      "year": "2025",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Advancing beyond people recognition in facial image processing",
      "authors": [
        "N Mirabet-Herranz"
      ],
      "year": "2024",
      "venue": "Advancing beyond people recognition in facial image processing"
    },
    {
      "citation_id": "16",
      "title": "Don't look at my teeth when I smile: Teeth visibility in smiling faces affects emotionality ratings and gaze patterns",
      "authors": [
        "I Blanco",
        "I Serrano-Pedraza",
        "C Vazquez"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "17",
      "title": "Something to sink your teeth into: The presence of teeth augments ERPs to mouth expressions",
      "authors": [
        "K Crager",
        "D Geisler",
        "P Newbern",
        "B Orem",
        "A Puce"
      ],
      "year": "2016",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "18",
      "title": "Effects of Closed Mouth vs. Exposed Teeth on Facial Expression Processing: An ERP Study",
      "authors": [
        "N Brunet",
        "A Ackerman"
      ],
      "year": "2025",
      "venue": "Behavioural Sciences"
    },
    {
      "citation_id": "19",
      "title": "Perceptual, categorical, and affective processing of ambiguous smiling facial expressions",
      "authors": [
        "M Calvo",
        "A Fernández-Martín",
        "L Nummenmaa"
      ],
      "year": "2012",
      "venue": "Cognition"
    },
    {
      "citation_id": "20",
      "title": "An Artificial Intelligence Model for Sensing Affective Valence and Arousal from Facial Images",
      "authors": [
        "H Nomiya",
        "K Shimokawa",
        "S Namba",
        "M Osumi",
        "W Sato"
      ],
      "year": "2025",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Mapping the emotional face. How individual face parts contribute to successful emotion recognition",
      "authors": [
        "M Wegrzyn",
        "M Vogt",
        "B Kireclioglu",
        "J Schneider",
        "J Kissler"
      ],
      "year": "2017",
      "venue": "PloS One"
    },
    {
      "citation_id": "22",
      "title": "The impact on emotion classification performance and gaze behaviour of foveal versus extrafoveal processing of facial features",
      "authors": [
        "A Atkinson",
        "H Smithson"
      ],
      "year": "2020",
      "venue": "J. Exp. Psychol. Hum. Percept. Perform"
    },
    {
      "citation_id": "23",
      "title": "Facial expressions and ability to recognize emotions from eyes or mouth in children",
      "authors": [
        "M Guarnera",
        "Z Hichy",
        "M Cascio",
        "S Carrubba"
      ],
      "year": "2015",
      "venue": "Europe's J. Psychol"
    },
    {
      "citation_id": "24",
      "title": "Detection of emotional faces: salient physical features guide effective visual search",
      "authors": [
        "M Calvo",
        "L Nummenmaa"
      ],
      "year": "2008",
      "venue": "J. Exp. Psychol. Gen"
    },
    {
      "citation_id": "25",
      "title": "Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Knowledge-Enhanced Facial Expression Recognition with Emotional-to-Neutral Transformation",
      "authors": [
        "H Li",
        "Y Xu",
        "J Yao",
        "N Wang",
        "X Gao",
        "B Han"
      ],
      "year": "2024",
      "venue": "Knowledge-Enhanced Facial Expression Recognition with Emotional-to-Neutral Transformation",
      "arxiv": "arXiv:2409.08598"
    },
    {
      "citation_id": "27",
      "title": "Comparative analysis of vision transformer models for FER using augmented balanced datasets",
      "authors": [
        "S Bobojanov",
        "B Kim",
        "M Arabboev",
        "S Begmatov"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "28",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Affective computing has changed: The foundation model disruption",
      "authors": [
        "B Schuller",
        "A Mallol-Ragolta",
        "A Almansa",
        "I Tsangko",
        "M Amin",
        "A Semertzidou",
        "L Christ",
        "S Amiriparian"
      ],
      "year": "2024",
      "venue": "Affective computing has changed: The foundation model disruption",
      "arxiv": "arXiv:2409.08907"
    },
    {
      "citation_id": "30",
      "title": "Unmasking the face: A guide to recognizing emotions from facial clues",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "2003",
      "venue": "Unmasking the face: A guide to recognizing emotions from facial clues"
    },
    {
      "citation_id": "31",
      "title": "Will affective computing emerge from foundation models and general artificial intelligence? A first evaluation of ChatGPT",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "32",
      "title": "ViTFER: facial emotion recognition with vision transformers",
      "authors": [
        "A Chaudhari",
        "C Bhatt",
        "A Krishna",
        "P Mazzeo"
      ],
      "year": "2022",
      "venue": "Applied System Innovation"
    },
    {
      "citation_id": "33",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
      "authors": [
        "Z Chen",
        "J Wu",
        "W Wang",
        "W Su",
        "G Chen",
        "S Xing",
        "M Zhong",
        "Q Zhang",
        "X Zhu",
        "L Lu"
      ],
      "year": "2024",
      "venue": "Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Minicpm-v: A GPT-4V level MLLM on your phone",
      "authors": [
        "Y Yao",
        "T Yu",
        "A Zhang",
        "C Wang",
        "J Cui",
        "H Zhu",
        "T Cai",
        "H Li",
        "W Zhao",
        "Z He"
      ],
      "year": "2024",
      "venue": "Minicpm-v: A GPT-4V level MLLM on your phone",
      "arxiv": "arXiv:2408.01800"
    },
    {
      "citation_id": "35",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "P Wang",
        "S Bai",
        "S Tan",
        "S Wang",
        "Z Fan",
        "J Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "36",
      "title": "Janus-pro: Unified multimodal understanding and generation with data and model scaling",
      "authors": [
        "X Chen",
        "Z Wu",
        "X Liu",
        "Z Pan",
        "W Liu",
        "Z Xie",
        "X Yu",
        "C Ruan"
      ],
      "year": "2025",
      "venue": "Janus-pro: Unified multimodal understanding and generation with data and model scaling",
      "arxiv": "arXiv:2501.17811"
    },
    {
      "citation_id": "37",
      "title": "Ovis: Structural embedding alignment for multimodal large language model",
      "authors": [
        "S Lu",
        "Y Li",
        "Q.-G Chen",
        "Z Xu",
        "W Luo",
        "K Zhang",
        "H.-J Ye"
      ],
      "year": "2024",
      "venue": "Ovis: Structural embedding alignment for multimodal large language model",
      "arxiv": "arXiv:2405.20797"
    },
    {
      "citation_id": "38",
      "title": "Paligemma: A versatile 3b vlm for transfer",
      "authors": [
        "L Beyer",
        "A Steiner",
        "A Pinto",
        "A Kolesnikov",
        "X Wang",
        "D Salz",
        "M Neumann",
        "I Alabdulmohsin",
        "M Tschannen",
        "E Bugliarello"
      ],
      "year": "2024",
      "venue": "Paligemma: A versatile 3b vlm for transfer",
      "arxiv": "arXiv:2407.07726"
    },
    {
      "citation_id": "39",
      "title": "Emergent world models and latent variable estimation in chess-playing language models",
      "authors": [
        "A Karvonen"
      ],
      "year": "2024",
      "venue": "Emergent world models and latent variable estimation in chess-playing language models",
      "arxiv": "arXiv:2403.15498"
    },
    {
      "citation_id": "40",
      "title": "Emergent Abilities in Large Language Models: A Survey",
      "authors": [
        "L Berti",
        "F Giorgi",
        "G Kasneci"
      ],
      "year": "2025",
      "venue": "Emergent Abilities in Large Language Models: A Survey",
      "arxiv": "arXiv:2503.05788"
    },
    {
      "citation_id": "41",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "Others"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong",
        "Others"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "43",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill",
        "Others"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "44",
      "title": "Vision-language models for vision tasks: A survey",
      "authors": [
        "J Zhang",
        "J Huang",
        "S Jin",
        "S Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "45",
      "title": "AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant Women",
      "authors": [
        "G Basílio",
        "T Pereira",
        "A Koerich",
        "H Tavares",
        "L Dias",
        "M Teixeira",
        "R Sousa",
        "W Hisatugu",
        "A Mota",
        "A Garcia",
        "M Galletta",
        "T Paixão"
      ],
      "venue": "AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant Women",
      "doi": "10.48550/arXiv.2410.05450",
      "arxiv": "arXiv:2410.05450"
    },
    {
      "citation_id": "46",
      "title": "Compound Expression Recognition via Large Vision-Language Models",
      "authors": [
        "J Yu",
        "X Lu"
      ],
      "year": "2025",
      "venue": "Compound Expression Recognition via Large Vision-Language Models",
      "arxiv": "arXiv:2503.11241"
    },
    {
      "citation_id": "47",
      "title": "Contextual emotion recognition using large vision language models",
      "authors": [
        "Y Etesam",
        "Ö Yalc ¸ın",
        "C Zhang",
        "A Lim"
      ],
      "year": "2024",
      "venue": "2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "48",
      "title": "Facial expression recognition based on fine-tuned channel-spatial attention transformer",
      "authors": [
        "H Yao",
        "X Yang",
        "D Chen",
        "Z Wang",
        "Y Tian"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "49",
      "title": "FER-PCVT: Facial expression recognition with patch-convolutional vision transformer for stroke patients",
      "authors": [
        "Y Fan",
        "H Wang",
        "X Zhu",
        "X Cao",
        "C Yi",
        "Y Chen",
        "J Jia",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "50",
      "title": "Facial expression recognition based on squeeze vision transformer",
      "authors": [
        "S Kim",
        "J Nam",
        "B Ko"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "51",
      "title": "Occlusion aware facial expression recognition using CNN with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised model diagnosis",
      "authors": [
        "Y Wang",
        "E Li",
        "J Luo",
        "Z Wang",
        "F De"
      ],
      "year": "2024",
      "venue": "Unsupervised model diagnosis",
      "arxiv": "arXiv:2410.06243"
    },
    {
      "citation_id": "53",
      "title": "Generative adversarial networks in human emotion synthesis: A review",
      "authors": [
        "N Hajarolasvadi",
        "M Ramirez",
        "W Beccaro",
        "H Demirel"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "54",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "K Simonyan",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2013",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "arxiv": "arXiv:1312.6034"
    }
  ]
}