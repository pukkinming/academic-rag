{
  "paper_id": "2001.09177v2",
  "title": "Recognizing Developers' Emotions While Programming",
  "published": "2020-01-24T19:52:44Z",
  "authors": [
    "Daniela Girardi",
    "Nicole Novielli",
    "Davide Fucci",
    "Filippo Lanubile"
  ],
  "keywords": [
    "Emotion awareness",
    "emotion detection",
    "biometric sensors",
    "empirical software engineering",
    "human factors in software engineering"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Developers experience a wide range of emotions during programming tasks, which may have an impact on job performance. In this paper, we present an empirical study aimed at (i) investigating the link between emotion and progress, (ii) understanding the triggers for developers' emotions and the strategies to deal with negative ones, (iii) identifying the minimal set of non-invasive biometric sensors for emotion recognition during programming tasks. Results confirm previous findings about the relation between emotions and perceived productivity. Furthermore, we show that developers' emotions can be reliably recognized using only a wristband capturing the electrodermal activity and heart-related metrics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Software development is an intellectual activity requiring creativity and problem-solving skills, which are influenced by affective states  [18] . Previous work shows that positive emotions are beneficial for developers' well-being and productivity while negative ones lead to poor job performance and are detrimental to the software development process  [17, 27, 42] . Hence, emotion awareness-i.e, the awareness of own and others' emotions-is regarded as a key to success for software projects  [7] .\n\nEarly recognition of negative emotions, such as stress  [38] , frustration  [10] , and anger  [13]  can enable just-in-time corrective actions for developers and team managers, preventing burnout and undesired turnover  [38] . Recent research findings demonstrate that negative emotions can be caused by uneven task distribution, wrong estimation of size and time required to complete an assignment, difficulties solving a complex cognitive task, and obstacles when familiarizing with a new technology or programming language  [10] . Information about developers' emotional state can be leveraged to improve collaborative software development strategies  [23] . For instance, enriching retrospective meetings with feedback about developers' emotions can be used to reflect as a team on opportunities for improvement. Thus, we envision the emergence and adoption of tools for enhancing emotion awareness during software development.\n\nIn this study, we focus on the identification of the emotions experienced by developers engaged in a programming task. Specifically, we operationalize emotions along the valence and arousal dimensions of the Circumplex Model of affect  [49] . First, we build upon recent research investigating the relation between developers' emotions and perceived productivity  [19, 42] . We formulate our first research question as follows:\n\nRQ1 -What is the range of developers' emotions during a programming task and to what extent they correlate with their perceived progress? To address RQ1, we perform a study with 23 participants engaged in a programming task. We ask participants to periodically self-report their emotional state and self-assessed progress. We analyze the range of emotions reported and their correlation with perceived progress by fitting a linear-mixed effect model as done in the previous studies  [19, 42] .\n\nAs a second goal, we aim at discovering the causes of positive and negative emotions experienced during software development. Furthermore, we aim at identifying the coping strategies that might help programmers dealing with negative emotions. As such, we formulate our second research question:\n\nRQ2 -What are the triggers for developers' emotions and the strategies they implement to deal with negative ones? To address RQ2, we interview the participants at the end of the study and perform manual coding  [40]  of their answers to open-ended questions.\n\nPrevious work demonstrated the feasibility of sensor-based emotion detection using non-invasive biometric devices  [15, 42] . However, the experimental setting usually employed in a laboratory setting is too complex for being applied in practice. We aim at identifying the minimal set of sensors to wear in the work environment for reliable emotion recognition. Accordingly, we formulate our third research question:\n\nRQ3 -What is the minimal set of non-invasive biometric sensors to recognize developers' emotions? We use supervised machine learning to train a classifier for developers' emotions based on biometric features, with different sensor configurations.\n\nThe contributions of this work are:\n\n‚Ä¢ A list of emotional triggers related to software development, including strategies to deal with negative emotions. ‚Ä¢ A set of supervised classifiers of developers' emotions. These include, to the best of our knowledge, the first attempt at classifying arousal during a programming tasks.\n\n‚Ä¢ A lab package 1 to verify the results, replicate, and build upon this study. The reminder of the paper is structured as follows. Section 2 describes the theoretical model we use to operationalize emotions, reviews the existing literature on sensor-based emotion detection, and summarizes the empirical studies we build-upon in this work. In Section 3, we describe the design of this study. In Section 4, we report the analysis results and answer the research questions. In Section 5, we compare our findings with previous work and discuss their implications and limitations. Section 6 concludes the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "In this section, we introduce the most important theories on modeling emotions and summarize the state of the art on recognizing emotions using biometric sensors. Moreover, we report details about two studies relating emotions to perceived progress in software engineering.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Modeling",
      "text": "Psychologists worked on decoding emotions for decades, developing theories based on cognitive psychology and natural language communication. Two theories have emerged. The first poses that a limited set of basic emotions exists. However, there is no consensus about their number or nature  [9, 35] . The second theory considers emotions as a continuous function of one or more dimensions  [49] . Dimensional models are not influenced by cultural or linguistic factors  [22, 50] , which makes them more robust compared to discrete models. Consistently with prior research on emotion awareness in software engineering  [19, 24, 38, 42] , we use a continuous representation of developers' emotions. Specifically, we refer to the Circumplex Model of Affect (see Figure  1 ), which represents emotions according to two dimensions-valence (pleasant vs. unpleasant) and arousal (activation vs. deactivation). According to this model, each emotion can be considered a \"label for a fuzzy set, defined as a class without sharp boundaries\"  [49] . Pleasant emotional states, such as happiness, are associated with positive valence, while unpleasant ones, such as sadness, are associated with negative valence. Arousal describes the level of activation of the emotional state ranging from inactive or low, as in calmness or depression, to active or high, as in excitement or tension.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sensor-Based Emotion Classification",
      "text": "The link between emotions and physiological feedback-measured using biometric sensors-is investigated in the field of affective computing  [28, 29, 52] . Among the several physiological measures that correlate with emotions, previous research investigated the electrical activity of the brain (EEG)  [30, 37, 46, 52] , the electrical activity of the skin (EDA)  [4, 25] , the electrical activity of contracting muscles (EMG)  [15, 29, 44] , and the blood volume pulse (BVP) from which heart rate (HR) and its variability (HRV) can be derived  [5, 51] .\n\nSpecifically, changes in the EEG spectrum provide an indication of overall levels of arousal or alertness  [30]  as well as pleasantness of the emotion stimulus  [46] . For example, Soleymani et al.  [52]  1 https://figshare.com/articles/conference_contribution/Recognizing_Developers_ Emotions_while_Programming/9206474 found that high-frequencies sensed from electrodes positioned on the frontal, parietal, and occipital lobes have high correlation with valence. Similarly, Li and Lu  [37]  demonstrate that it is possible to discriminate between happiness and sadness based on the analysis of EEG signal.\n\nConcerning EDA, studies in psychology demonstrate how this signal considerably varies with changes in emotional intensity and specifically with the arousal dimension  [33] . Changes in EDA are a result of increased activity of the sweat glands, which takes place in presence of emotional arousal and cognitive workload. Hence, EDA has been employed to detect excitement, stress, interest, attention as well as anxiety and frustration  [4, 25] .\n\nBVP, HR, and HRV metrics-captured by a plethysmographhave been successfully employed for emotion recognition  [5, 51] .\n\nFacial EMG is particularly useful in predicting emotions  [29, 44] . However, its usage leads to poor results when the sensors are placed on body parts other than the face, such as the arms  [15] . Accordingly, we exclude EMG from this study.\n\nIn this study, we include measures from EEG, EDA, BVP, and HR as they can be collected using low-cost noninvasive sensors  [15, 42]  that can be comfortably used by developers during programming tasks (see Section 3.3). This choice is in line with current research investigating the use of lightweight biometric sensor for studying human aspects in software development. Fucci et al.  [12]  use EEG, EDA, and heart-related measurements for the automatic identification of code comprehension tasks. Fritz et al.  [11]  rely on a combination of EEG, BVP, and eye tracker to assess difficulty in code comprehension and prevent developers from introducing bugs. In a follow-up study, they employ the same set of sensors to distinguish between positive and negative emotions during programming tasks  [42] . Similarly, EDA, HR, HRV, and breath-related metrics have been used in a field study to identify code quality concerns during software development  [43] . Z√ºeger et al.  [58]  combine heart-related metrics with a wristband activity tracker to predict developers' interruptibility.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Former Studies",
      "text": "Our study builds upon the design and results of two former studies linking developers' emotions with their perceived progress  [19, 42] . They follow similar experimental protocols involving a longitudinal study with repeated measures of novice and professional developers' emotional states. In particular, Graziotin et al.  [19]  recruited eight subjects of which four professional developers and four undergraduate students with major in computer science, whereas M√ºller and Fritz  [42]  observed 17 subjects of which six professionals with an average experience of seven years and 11 PhD students in computer science.\n\nIn both studies, the participants were observed during a programming session and interrupted every five minutes to answer a selfreport survey. Emotions were self-reported along the valence and arousal dimensions. M√ºller and Fritz measured the self-perceived participants' progress while completing two programming tasks (30 minutes each). Conversely, Graziotin et al. asked participants to report on perceived productivity while working for 90 minutes in a natural setting-i.e., on their own projects. Graziotin et al. also measured dominance-i.e., the extent to which a subject feels in control or controlled  [2] .\n\nThe studies of Graziotin et al.  [19]  and M√ºller and Fritz  [42]  provide empirical evidence that valence correlates with self-perceived productivity and progress, respectively. Among the main causes for negative emotions, leading to the perception of being stuck, M√ºller and Fritz report cognitive difficulties, impossibility to fulfill information needs, and code not working. Conversely, being able to understand the code and identify a solution strategy are among the top reasons for positive affect.\n\nFinally, M√ºller and Fritz trained a supervised emotion classifier able to distinguish between positive and negative emotions with an accuracy of 71%. In their setting, they use multiple sensors including EEG, EDA, HR, and eye tracking metrics. However, they neglect the arousal classification.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Study Design",
      "text": "In this section, we report a brief characterization of the participants in our study, the study task, the tools and devices used to measure the relevant constructs, and the study protocol.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Participants",
      "text": "We recruited 27 CS students (23 males, four females) from the Department of Computer Science of our University of which 21 undergraduates, five graduates, and one post-graduate. Following a convenience sampling strategy, we recruited volunteers as participants only if they could provide evidence they cleared exams where Java programming (e.g., the programming language for the this study task) was used for capstone projects.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Development Task",
      "text": "We use one of the two tasks, including the materials, designed by M√ºller and Fritz in their study  [42] . The task consisted in writing a Java program using the StackExchange API 2  to retrieve all answers posted by a specific user on StackOverflow and sum up the scores the user earned for these answers. The participants were provided with a skeleton code which they had to modify to complete the task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Measurement Tools And Devices",
      "text": "Biometric sensors. We measure the subjects' physiological signals using lightweight biometric sensors analogous to those employed by M√ºeller and Fritz  [42] -i.e., comfortable to wear in the work environment  [16] . Specifically, we use the NeuroSky BrainLink headset to record the EEG waves and the Empatica E4 wristband for EDA, BVP, and heart-related metrics (see Fig.  2 ). The BrainLink EEG uses one electrode placed on the surface of the scalp and a reference placed on the earlobe 3  (see Figure  2 ). EEG waves are extracted by pre-processing the raw signal collected by the device with a sample frequency of 512Hz (see Section 4.3). Besides raw EEG signal, BrainLink extracts metrics related to mental focus (i.e., attention) and calmness (i.e, meditation).  4 The Empatica E4 wristband measures EDA with a sample frequency of 4Hz. It features a plethysmograph for collecting BVP sampled at a frequency of 64Hz. BVP is used to derive the HR and HRV. Following the guidelines provided by the Empatica, support  5  we decided to exclude HRV as it is not reliable in dynamic conditions (i.e., while typing).\n\nSelf-report of Emotions and Progress. The measurement of emotions and perceived progress is performed through experience sampling  [19, 42] . In line with the approach implemented by Graziotin et al.  [19] , we collect self-reported valence and arousal ratings during interruptions of the development task using Self-Assessment Manikin (SAM)  [2] . Consistently with previous assessment of valence and arousal in affective computing research  [29] , we use a nine-point scale. Figure  3  shows the SAM mannequins for valence and arousal (top) as well as a 5-point Likert item (bottom) to assess the perceived progress. During the interruptions, we prompt the participant to elaborate on the triggers for the emotional state and take notes of their answers.\n\nDebriefing questionnaire. To elicit triggers and strategies for handling emotions, the first author interviews each participant at the end of the development task, asking the following questions:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Protocol",
      "text": "We organize the study according to the following phases, as shown in Figure  4 .\n\nPre-experimental briefing. The participant gets acquainted with the settings-e.g., sitting in a comfortable position, adjusting the monitor height. The experimenter summarizes the upcoming steps and explains the programming task. The participant signs the consent form to allow anonymous treatment of the collected data.\n\nSensor calibration and emotion elicitation. The participant wears the biometric sensors (see Figure  2 ) and the experimenter checks that the devices record the signals correctly.\n\nBefore the participant starts working on the actual programming assignment, the experimenter asks her to take part in an emotion-elicitation task. The purpose of this step is two-fold. On one hand, it allows the subject to get acquainted with the SAM mannequins; on the other hand, it allows the experimenter to collect her biometric and SAM feedback, both in a neutral condition and in presence of controlled stimuli. This step follows the design described in a previous emotion-elicitation study  [15] . The participant watches eight videos, selected from the DEAP dataset  [29] , associated with valence and arousal scores on a scale from 1 to 9. Each video is mapped to the four quadrants of the emotional space in the Circumplex Model of Affect based on a discretization of the scores. Specifically, the eight videos used in the emotion elicitation are equally distributed among the four quadrants-i.e., ùëùùëúùë†ùëñùë°ùëñùë£ùëí valence and ‚Ñéùëñùëî‚Ñé arousal, ùëùùëúùë†ùëñùë°ùëñùë£ùëí valence and ùëôùëúùë§ arousal, ùëõùëíùëîùëéùë°ùëñùë£ùëí valence and ùëôùëúùë§ arousal, ùëõùëíùëîùëéùë°ùëñùë£ùëí valence and ‚Ñéùëñùëî‚Ñé arousal. The emotion elicitation step lasts 10 minutes, with the eight videos presented in four sessions. Each session consists of a 30-second baseline video showing a quiet image with relaxing music in the background, followed by a 2-minute display of the selected videos (one minute per video). At the beginning of each trial, a 3-second screen displays the current trial number to make the participant aware of her progress. After each video, the participant is instructed to report her emotions using the SAM mannequins. Therefore, each subject provided sixteen pairs of ratings-i.e., one valence and one arousal assessment per video.\n\nPrior to the development task, the participant watches a 2-minute relaxing video of a nature scenery to induce relaxation and a neutral emotional state  [48] . We use biometrics recorded when showing such video as physiological baselines for the participants.\n\nSoftware development task and self-report. The core of the study is a 30-minute development session during which we apply experience sampling  [34] . The experimenter (i.e., the first author) observed the behavior of participants during the entire session and interrupted them every five minutes, asking to report their emotions, perceived progress, and to provide information about the reasons for their emotions. We choose a time frame of five minutes as it represents the average time for which developers stay focused on a single task  [41, 42] . We collect the subjects' biometrics during the entire duration of the development task. After the task, participants watch again the 2-minutes relaxing video to ward-off possible induced emotions, for example, from not succeeding in solving the task. In total, each participant provides six ratings for valence and six for arousal (i.e., one valence-arousal rating for each interruption). In addition, participants provide six progress ratings (one per interruption). Finally, they provide six answers to the openended question about the triggers for reported emotions, which we use in the data quality assurance step.\n\nFinal interview. We run the post-experimental debriefing with each participant for approximately 10 minutes. The experimenter interviews participants to investigate i) the triggers for positive and negative emotions during the task, and ii) the strategies subjects implement to deal with negative emotions. The participants could also ask questions and give feedback about the experiment. Finally, participants are rewarded with a voucher for a meal.\n\nData Quality Assurance. Once the experiment was completed, but before analyzing the data, we manually performed a sanity check of the collected data. In particular, we investigated potential malfunctioning of the sensors which can introduce noise and discontinued recording of raw signals. In addition, we check the consistency of the self-reported valence, arousal, and progress scores with respect to the comments provided in the open questions. We looked for signs of negligence, inconsistencies, and misinterpretation of the guidelines for using the SAM-based report. For example, one subject scored his arousal as eight for all the interruptions, however, this did not match the content of his comments. As a results of this step, we discarded four participants (all males). Our final pool of participants include 23 subjects overall, of which four females.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis And Results",
      "text": "In this Section, we answer our research questions by analyzing the data collected in the study using a mix of quantitative and qualitative methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experienced Range Of Emotions And Their Correlation With Progress (Rq1)",
      "text": "We analyze the self-reported scores for emotions and progress collected through experience sampling. As described in Section 3.4, each participant provided self-assessment of emotions at each interruption, thus reporting six pairs of SAM-ratings for valence and arousal. As such, our dataset includes 138 (6ùë•23 subjects) ratings for each emotion dimension. Analogously, we collected 138 scores (6ùë•23) for progress.\n\nTo investigate the range of developers' emotions during the programming task, we following the approach proposed by M√ºeller and Fritz  [42] . We analyze and compare the emotion scores the developers reported during the programming task and emotion elicitation. Figure  5  reports the SAM scores for valence and arousal. We use the scores provided during the video-driven emotion elicitation as a reference for the way participants report valence and arousal in presence of the controlled stimulus. For example, the range of SAM scores reported for positive videos indicate how the participants rate their valence when experiencing a positive emotion. We observe that the entire range of emotions is covered by the scores reported while watching videos as well as programming. For valence, we observe a clear distinction between the ranges used for positive and negative videos (Figure  5a ). In addition, negative emotions tend to prevail during the programming task-i.e., the median score corresponds to the one reported for the negative videos. Conversely, the median value for the arousal distribution of the programming task (Figure  5b ) corresponds to the one reported for high arousal videos.   1 : Frequency of progress scores on a 1-5 scale (n=138).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Stuck",
      "text": "The participants report the whole range of progress (i.e., 1-5), from completely stuck (score equal to 1) to in flow (score equal to 5), with median = 3 and inter-quartile range = 2 (see Table  1 ). Most developers had troubles solving the task-only four succeeded in developing a complete solution. Accordingly, most of the time they reported being stuck (63% of answers). They reported being in a neutral state 26% of the cases, corresponding to 36 answers, and in flow in 11% of the cases (15 answers).\n\nWe investigate the link between reported emotions and perceived progress by fitting a linear mixed model, which is robust in case of repeated measurements and longitudinal data  [21] . To create the model, we used the lme4 R package  6  . Consistently with the approach adopted in the former studies  [19, 42] , we consider progress as the dependent variable and valence, arousal, and their interaction with time as fixed effects. Given our study design, we cannot exclude that the perceived progress can be impacted by time. Therefore, time and its interaction with the emotional dimensions are also included in the model. To account for individual differences in the SAM reports, we standardize the valence and arousal using Z-scores  [19, 42] . In Table  2 , we report the parameter estimation for the mixedeffect model and the percentage deviance explained by each effect. Our model significantly differs from the null model-i.e., the model with no correlation between fixed effects and progress (ùúí 2 (5) = 63, ùëù < 0.001). We observe a significant effect of valence on progress at 95% confidence level. Valence also shows the highest explanatory power with 27.80% of deviance explained, compared to 30.15% observed for the whole model. Conversely, we did not observe any effect of arousal on self-reported progress. This result holds for the effect of time and its interaction with arousal. Our model shows a statistically significant correlation between progress and the interaction between time and valence. However, the effect of such interaction on the overall model is small (1.4% of deviance explained).\n\nSummary RQ1 -Developers experience a wide range of emotions during programming tasks. We observe a prevalence of negative valence and high arousal. Valence is positively correlated with perceived progress.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Triggers And Strategies For Emotions (Rq2)",
      "text": "To investigate triggers for emotions, as well as the strategies to deal with negative ones, we manually analyzed the 69 answers provided during the debriefing questionnaire in the final interview, -i.e., three for each participant. We performed qualitative data analysis using a sentence-bysentence approach in a semi-exploratory fashion. We applied selective coding  [40]  based on the constructs associated with the research question (i.e., positive and negative emotions, as well as strategies for coping with the latter). We identified 29 sentences discussing positive emotions triggers, 41 for negative emotions triggers, and 47 for coping strategies. Subsequently, two researchers coded each sentence following an open coding approach  [40] . During a meeting, the researchers reconciled their codes in a single one. We obtained 23 codes-eight reasons for positive emotions, eight for negative ones, and seven strategies for dealing with negative emotions. These codes were then grouped to form relationships and themes captured by applying axial coding  [40] . Three themes emerge: self refers to the developers' dimension, social refers to peers and collaborators, and solution refers to issues with artifacts, design, and implementation of the task.\n\nTable  3  shows the themes identified as the result of the coding process. The most frequent trigger for emotions refers to the solution dimension (25 occurrences overall, of which 14 for positive and 11 for negative). The participants felt particularly happy when able to incrementally implement the designed solution and when believing the solution itself is simple. Analogously, unexpected output, unexpected usage of libraries, and unavailable documentation trigger negative emotions. The most frequent causes for negative valence relate to the self dimension (15 occurrences), with the developers reporting the feeling of being stuck and the awareness of time pressure as the main causes for negative emotions. Finally, the social theme appears with a low frequency as the participants had to complete the programming task by themselves.\n\nAs for strategies developers implement to deal with negative emotions, take breaks is the most popular one, immediately followed by look for collaboration-e.g., asking help from peers. Some participants also report changing task and starting over as strategies to regain focus and shift towards positive emotions. Similarly, they indicated changing approach to the solution and decomposing the problem into simpler ones as strategies to gain confidence and react to negative emotions.\n\nSummary RQ2 -Developers' positive emotion are mainly triggered by the effectiveness of the implemented solution.\n\nUnexpected code behavior and missing documentation cause negative emotions, which are also due to time pressure and self-perceived low productivity. To deal with negative emotions, developers take breaks and look for peers' help.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A Minimal Set Of Sensors For Classifying Developers' Emotions (Rq3)",
      "text": "We address RQ3 using a machine learning approach for classifying the participants' emotions during the development task. Fig.  6  shows the machine learning pipeline we implemented to answer this research question.\n\nDataset The dataset consists of the self-reported emotions of participants during the development task. Each of the 23 participants performed a total of six SAM-based assessments of valence and arousal. As a result, we obtained two datasets of 138 observations, one for valence and one for arousal. Accordingly, we trained two separate classifiers by considering features extracted from the biometric signals associated to each observation, as captured by the sensors.\n\nWe define the positive and negative labels for valence, and high and low labels for arousal. For this purpose, we discretize the SAM scores following the approach applied by M√ºller and Fritz  [42] . First, we adjusted the valence and arousal scores based on the mean values reported while watching the emotion-triggering videos. Following such approach, we defined gold labels for valence and arousal by taking into account (and correcting for) fluctuations due to the participants' subjective interpretation of the SAM scale. Then, we assigned a positive valence label (respectively a high arousal label) to instances with scores above the mean and a negative valence label (respectively a low arousal label) to instances with scores below it. Finally, following the recommendation reported in previous work  [42] , we manually inspected 31 observations for valence and 30 for arousal for which we observed scores in the ùëöùëíùëéùëõ ¬± 0.5 interval. For such cases, two authors manually assigned valence and arousal labels. They obtained a substantial agreement  [56] , with ùúÖ = .67 and observed agreement = 84%. The few disagreement cases (less than five for each dimension) were resolved in a discussion, following a consolidated approach in affective computing research  [1] . At the end of this process, we obtained the distribution reported in Table  4 .\n\nPreprocessing and Features extraction Although the biometric signals were recorded during the entire experimental session for all the participants, we only consider the signals recorded in proximity of the stimuli of interest-i.e., the signals collected in the 10 seconds before the subjects were interrupted. This choice is in line with consolidated practices in related research on sensorbased classification of emotional  [15, 42]  and cognitive states  [12]  of software developers. To synchronize the measurement of the biometric signals with the emotion self-assessment, we (i) save the timestamp of the interruption (t_interruption), (ii) calculate the timestamp for relevant timeframe for each interruption-i.e., 10 seconds before the self-report (t_start), and (iii) select each signal samples recorded between t_start and t_interruption.\n\nFor each participant, we normalize the signals to her baseline using Z-score normalization  [42] . The baseline is calculated considering the last 30 seconds of the video used to elicit a neutral state before starting the task  [11] .\n\nTo maximize the signal information and reduce noise caused by movements, we applied multiple filtering techniques. Regarding EEG and BVP, we extract frequency bands using a band-pass filter algorithm at different intervals  [5] . The EEG signal can be decomposed into five waves based on the frequency, namely delta (< 4Hz), theta (4-7,5Hz), alpha (4-12,5Hz), beta (13-30Hz), and gamma (> 30Hz). We apply the filter to extract the distinct cerebral waves as each spectrum might provide different information. The EDA signal is constituted by a tonic component, indicating the level of electrical conductivity of the skin, and a phasic component, representing the phasic changes in electrical conductivity or skin conductance response (SCR)  [3] . We applied the cvxEDA algorithm  [20]  to extract the two components.\n\nAfter signals pre-processing, we extracted the features presented in Table  5 , which we use to train our classifiers. We select features based on previous studies using the same signals for machine learning  [12, 15, 42] .\n\nClassification Settings. In line with previous research on biometrics  [12, 29, 42] , we choose eight popular machine learning algorithms-i.e., Naive Bayes (nb), K-Nearest Neighbor (knn), C4.5like trees (J48), SVM with linear kernel (svm), Multi-layer Perceptron for neural network (mlp), and Random Forest (rf).\n\nWe evaluate our classifiers in two different settings. In the Holdout setting, we split the gold standard into training (90%) and test (10%) sets using the stratified sampling strategy implemented in the R caret package  [31] . We search for the optimal hyperparameters  [54, 55]  using leave-one-out cross validation-i.e., the recommended approach for small training sets  [45] , such as ours. The resulting model is then evaluated on the held-out test set to",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Signal Features",
      "text": "Sensor: Brainlink EEG -Frequency bin for alpha, beta, gamma, delta, theta waves -Ratio between frequency bin of each band & one another -For attention and meditation: min, max, difference between mean for baseline and task Sensor: Empatica E4 EDA -mean tonic -phasic AUC -phasic min, max, sum peaks amplitudes BVP -phasic min, max, sum peaks amplitudes -mean peak amplitude (diff. between baseline and task) HR -mean peak amplitude (diff. between baseline and task) -heart-rate variance (diff. between baseline and task) Table  5 : Machine learning features grouped by physiological signal and by sensor. assess its performance on unseen data. We repeat this process 10 times to further increase the validity of the results. The performance is then evaluated by computing the mean for precision, recall, Fmeasure, and accuracy over the different runs. This setting is directly comparable to the one implemented by M√ºller and Fritz  [42] , which includes data from the same subject in both training and test sets.\n\nWe report a second evaluation setting to assess the classifiers performance on data obtained from unseen developers-i.e., leave-onesubject-out (LOSO). This setting was inspired by previous findings reporting different classification performance due to differences in biometrics between individuals  [42] . In this setting, the evaluation on a test set is repeated for 23 times-i.e., the number of subjects in our dataset. At each iteration, we use all the observations from the n-1 participants (i.e.,  22)  for training the model, and we test the performance on the remaining one.\n\nClassification Performance. In Table  6 , for each sensor and their combination, we report the classifier with the highest accuracy, together with its precision, recall, and F-measure. Moreover, we report the result of a trivial classifier always predicting the majority class (i.e., negative for valence and high for arousal)  7 In the hold-out setting, we observe substantial improvements over the baseline classifier. The valence classifier distinguishes between negative and positive emotions with an accuracy of .72 using the full set of sensors. While performance might appear close to the baseline value in terms of accuracy (baseline accuracy = .68), looking at precision and recall, we observe that the classifiers behavior is substantially different. In fact, we observe an increase of .34 in precision (from .34 of the baseline to .68 of the classifier) and of .10 (from .50 to .60) in recall, resulting in a .19 increase of the F1measure (from .41 to .60). The results show that the model trained using the full set of features achieves comparable performance to the one trained using only features extracted from the Empatica E4 device-i.e., EDA, BVP, and HR-related features. We observe a small increase in precision with respect to the full device setting (from .68 in the full set to .70 with Empatica) and a small decrease in recall (from .60 to .59). These results suggest that valence can be reliably detected using only the Empatica E4 wristband. The features associated to the Brainlink EEG helmet negligibly impact the classifiers performance.\n\nFor arousal, our best classifier distinguishes between high and low emotion activation with an accuracy of .65. The model trained using the full set of features substantially outperforms the baseline in terms of precision (+.31, from .31 to .62), recall (+.11, from .50 to .61), and F1-measure (+.21, from .38 to .59). Similarly to what observed for valence, the performance obtained with the full set of sensors is comparable to the one obtained with the Empatica E4 wristband only, which also achieves a better precision.\n\nThe LOSO setting results are similar, for both valence and arousal, to the ones reported in the hold-out settings. However, we observe variability for the individual performance on each test set as suggested by the higher standard deviation compared to the hold-out setting. These results provide evidence that biometrics are good predictors for emotions, although we observed variability between individuals.\n\nSummary RQ3 -Developers' emotions during programming can be recognized using features extracted by the Empatica E4 wristband (i.e., EDA, BVP, and HR).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we compare our findings with related studies, highlight their implications for researchers and practitioners, and report the threats to their validity.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison With Related Studies",
      "text": "Emotions as a proxy for progress. The analysis of the scores reported by the participants in our study during the programming task shows a prevalence of negative valence and high arousal. This result contrasts with the findings of M√ºller and Fritz  [42]  who observed that the distribution of emotions reported when programming is comparable to the one reported when watching emotion-triggering pictures. The prevalence of emotion with negative valence and high arousal in our study can be explained by our participants being less experienced. In previous work, M√§ntyl√§ et al.  [38]  presents evidence that novice developers are more inclined to negative valence and high arousal. Furthermore, experience is negatively correlated with effort-i.e., more experienced developers need less effort to complete a task  [32, 39] . The lower level of experience of our participants can be seen in their actual and perceived progress. In fact, the majority reported being either stuck or completely stuck (63% of self-report questionnaires filled-in during the interruptions). Conversely, they reported either being in flow or neutral in only 11% and 26% of cases, respectively. These results are consistent with the fact that only 4 over 23 participants completed the task. In contrast, M√ºller and Fritz  [42]  reports a more balanced distribution of progress, with the majority of participants feeling in flow (39%) rather than stuck (37%) or neutral (24%).\n\nThe results of the linear mixed model in our study are comparable to those reported in M√ºller and Fritz  [42] . We confirm that valence is positively correlated with perceived progress and that it is the main variable explaining the model deviance. Moreover, we did not show a significant relationship between productivity and arousal. Both results confirm a previous study by Graziotin et al.  [19] . The positive relationship between valence and progress is consistent with the findings of M√§ntyl√§ et al.  [38]  who observed positive emotions when resolving issues in the tracking system (i.e., emotions as a proxy for progress). The same study shows low variability in arousal supporting the lack of correlation between this emotional dimension and progress observed in our study.\n\nCauses for negative emotions and coping strategies. We confirm previous evidence on the causes of negative emotions and how developers deal with them to regain focus and positive emotions. Being stuck and working under time pressure emerged as the most frequent causes for negative emotions. Fear of failure was already reported as a cause for frustration in software development  [10] . Similarly, the detrimental impact of limited time on self-confidence, well-being, and emotional states was already observed  [10, 32] .\n\nTechnical difficulties (e.g., unexpected usage of libraries or unexpected output of code) and unfulfilled information needs (e.g., unavailable documentation) also emerge as causes for negative feelings. This is consistent with previous investigations of emotions  [10, 42]  and confusion  [8]   findings suggest that early detection of confusion is crucial for preventing burnout and loss of productivity. Moreover, demonstrates how confusion arises due to lack of documentation  [8] , in presence of unexpected code behavior  [8] , and bugs  [38] .\n\nWe also show that facing new challenges is a trigger for positive emotions, in line with previous work showing that the development of new features causes more positive emotions than bug fixing  [38] . Similarly, having new ideas and being in flow while programming is shown to be associated with positive emotions  [42] .\n\nA Minimal Set of Biometrics for Emotion Classification. On top of confirming M√ºller and Fritz  [42]  findings regarding the usage of non-invasive sensors for valence recognition, we also addressed the classification of the arousal dimension. As a novel finding, we identified the minimum set of sensors-EDA, BVP, and HR measured using the Empatica E4 wristband-that can be used in an experimental protocol for detecting emotions during software development tasks.\n\nUsing machine learning, we are able to distinguish between positive and negative valence. Using only the Empatica E4 wristband, the performance are comparable to ones obtained using the full sensors settings (i.e., wristband + EEG helmet). Our accuracy (.72) is comparable to the one (.71) reported by M√ºeller and Fritz  [42] . However, their results are obtained using features from an EEG helmet in combination with HR, and pupil size captured by an eye-tracker.\n\nFor arousal classification, our best classifier achieves an accuracy of .65 using only features from the Empatica E4 wristband. The accuracy of our classifiers is comparable to the one (.58) reported by Koelstra et al.  [29]  for arousal classification using a 32-electrode EEG helmet. Moreover, they show an accuracy of .61 for valence by combining EDA, EMG on facial muscles, features derived from respiration, blood pressure, and eye blinking rate. We outperform their classifiers using a minimal set of features obtained using the Empatica E4 wristband.\n\nCompared to ours, other studies show better performance-e.g., accuracy for arousal of .97  [6, 14, 53]  and .91 for valence  [44] . However, these studies rely on high-definition EEG helmets  [6, 14, 53]  and facial electrodes for EMG  [44] . Such sensors are invasive and cannot be used outside of a laboratory setting-e.g., in the work environment.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implications",
      "text": "Implication for researchers The results of this study provides evidence that we can recognize developers' emotions, while programming, by means of a minimal set of biometric features using the sensors mounted on a single wearable device (the Empatica E4 wristband). This opens up the possibility of further studies aimed at improving the ecological validity of our findings.\n\nOur results show between-subject variability of biometrics, already observed in previous studies  [42] . The higher standard deviation for accuracy in the LOSO setting can be problematic when classifying the emotions of a new unseen developer. Further studies with a wider pool of participants are required to assess the robustness of our valence and arousal classifiers. Such studies can investigate to what extent we can build more robust classifiers by performing preliminary subject-based calibration-e.g., by tuning the models based on individuals' biometrics collected while exposing participants to emotional stimuli in a controlled setting. Future studies can identify the amount of biometric data required to fine-tune the models for a reliable classification of emotions of new subjects.\n\nThe correlation between valence and progress can be interpreted as a proxy for self-perceived productivity. However, further investigation is required to (i) provide an explanation for the results of our correlation study, and (ii) understand whether a causal relation exists between emotions and productivity (or vice-versa)-e.g., using emotion-triggering techniques in a controlled setting, in line with previous research  [27] .\n\nImplication for practitioners and tool builders. We show that emotions can be detected using non-invasive wearable device, such as a wristband. This finding paves the way for tools and practices to prevent developers' distress and burnout.\n\nEarly recognition of negative emotions, integrated with the development environment, can be leveraged to suggest corrective actions. Developers can regain focus and restore positive moods in accordance with the strategies we observed in this study to cope with negative emotions.\n\nOur results demonstrate a positive correlation between valence and progress, suggesting that emotions might act as a proxy for productivity. For example, positive emotions can indicate that a developer is in flow and should not be disturbed. Hence, sensorbased emotion classifiers can improve state-of-the-art approaches for the automatic assessment of interruptibility  [58] . Similarly, the identification of negative emotions can indicate a stuck developer requiring support fulfilling her information needs. Accordingly, an emotion-aware component integrated in the development environment can recommend relevant colleagues to consult on the code base  [26]  or trigger utilities for on-demand documentation generation  [47] .\n\nBiometrics can enhance retrospective meetings by including emotional information collected day-to-day rather than at the end of an iteration or sprint. The team can better identify what are the activities and events that relate to positive and negative emotions  [16] .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Threats To Validity",
      "text": "In this section, we report the threats to the validity in increasing order of priority for the in vitro nature of this study, following the recommendations of Wohlin et al.  [57] .\n\nExternal validity Threats to external validity relate to the generalizability of the results. We chose the same task used in a former study  [42] , which simulated a new problem in a real scenario. Regarding participants, we covered different levels of academic experience (by including Bachelors, Masters, and PhD students) but with less professional experience compared to  [42] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion Validity",
      "text": "The validity of our conclusions relies on the robustness of the generalized linear model and machine learning models. We mitigated such threat by (i) running several algorithms addressing the same classification task, (ii) applying hyperparameters tuning to optimally solve the task, and i(iii) reporting results from two different evaluation settings-i.e., Hold-out and LOSO.\n\nConstruct validity Our study suffers from threats to construct validity-i.e., the reliability of our measures in capturing emotions and progress. When assessing the impact of arousal on progress, we did not observe a significant correlation. Although we cannot exclude that such result is due to the unreliability of the self-reported rating, we performed data quality assurance and did not consider participants who misinterpreted the concept of arousal-e.g., who reported always the same score also during emotion elicitation.\n\nInternal validity Threats to internal validity concern confounding factors that can influence the results. We collected data in a laboratory setting. Factors existing in our settings, such as the presence of the experimenter and the absence of real consequences when failing or succeeding in the task, can influence the triggered emotions-e.g., negative emotions due to the feeling of being observed or judged. In addition, interrupting developers during the task can have interfered with their work and elicited negative emotions. We mitigate this threat by interrupting the developers when we notice a task switch (e.g., opening a new browser window) in proximity of the five-minutes interval.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "We investigated the range and triggers of emotions experienced by software developers during a programming task. We confirm the link between emotions and self-reported progress observed in previous studies.\n\nUsing EDA and heart-related metrics collected using a wristband, we trained a machine learning classifier that can accurately recognize valence. A second classifier, trained to recognized arousal, has shown less successful but encouraging results.\n\nOur results can stimulate future in-vivo research (i.e., in software development companies) and the collection of biometrics to explore emotions during the entire working day, when developers are involved in different activities, not just programming. Furthermore, observing developers at the workplace also opens opportunities to build more sophisticated classifiers, which are able to identify, for example, the bad days (i.e., when mostly negative emotions are identified) or negative working conditions of developers (i.e., when negative emotions are observed over a long period of time).",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), which rep-",
      "page": 2
    },
    {
      "caption": "Figure 1: The Circumplex Model of Emotions [49].",
      "page": 2
    },
    {
      "caption": "Figure 2: A participant wearing the Empatica E4 wristband",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the SAM mannequins for valence",
      "page": 3
    },
    {
      "caption": "Figure 3: The SAM mannequins for assessment of valence",
      "page": 4
    },
    {
      "caption": "Figure 4: Pre-experimental briefing. The participant gets acquainted",
      "page": 4
    },
    {
      "caption": "Figure 2: ) and the experimenter",
      "page": 4
    },
    {
      "caption": "Figure 4: The timeline of the study.",
      "page": 5
    },
    {
      "caption": "Figure 5: Box plots for valence (a) and arousal (b) during the",
      "page": 5
    },
    {
      "caption": "Figure 5: reports the SAM scores for valence and arousal. We use",
      "page": 5
    },
    {
      "caption": "Figure 5: a). In addition, negative emotions",
      "page": 5
    },
    {
      "caption": "Figure 5: b) corresponds to the one reported for high arousal",
      "page": 5
    },
    {
      "caption": "Figure 6: The machine learning pipeline implemented, with",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Parameter estimation for the fixed effects on Per-",
      "data": [
        {
          "Fixed Effects": "Valence\nArousal\nTime\nValence:Time\nArousal:Time",
          "Estimate": "0.17 (*)\n-0.05\n-0.02\n0.03 (*)\n0.02",
          "Upper\np-value\n(132 d.f.)": "0.00\n0.23\n0.97\n0.05\n0.24",
          "Lower\np-value\n(103 d.f.)": "0.00\n0.23\n0.97\n0.05\n0.25",
          "Dev.\nex-\nplained": "27.8%\n0.5%\n0.0%\n1.4%\n0.48%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sentiment Polarity Classification at EVALITA: Lessons Learned and Open Challenges",
      "authors": [
        "Nicole Valerio Basile",
        "Danilo Novielli",
        "Francesco Croce",
        "Malvina Barbieri",
        "Viviana Nissim",
        "Patti"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2884015"
    },
    {
      "citation_id": "2",
      "title": "Measuring emotion: The selfassessment manikin and the semantic differential",
      "authors": [
        "Margaret Bradley",
        "Peter Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy & Experimental Psychiatry",
      "doi": "10.1016/0005-7916(94)90063-9"
    },
    {
      "citation_id": "3",
      "title": "A Guide for Analysing Electrodermal Activity (EDA) & Skin Conductance Responses (SCRs) for Psychological Experiments",
      "authors": [
        "Jason Braithwaite",
        "Derrick Watson",
        "Robert Jones",
        "Mickey Rowe"
      ],
      "year": "2015",
      "venue": "A Guide for Analysing Electrodermal Activity (EDA) & Skin Conductance Responses (SCRs) for Psychological Experiments"
    },
    {
      "citation_id": "4",
      "title": "Affective agents: Sustaining motivation to learn through failure and state of \"stuck",
      "authors": [
        "Winslow Burleson",
        "Rosalind Picard"
      ],
      "year": "2004",
      "venue": "Social and Emotional Intelligence in Learning Environments Workshop. In conjunction with the 7th International Conference on Intelligent Tutoring Systems"
    },
    {
      "citation_id": "5",
      "title": "Multimodal biosignal sensor data handling for emotion recognition",
      "authors": [
        "Filipe Canento",
        "Ana Fred",
        "Hugo Silva",
        "Hugo Gamboa",
        "Andr√© Louren√ßo"
      ],
      "year": "2011",
      "venue": "SENSORS",
      "doi": "10.1109/ICSENS.2011.6127029"
    },
    {
      "citation_id": "6",
      "title": "Identifying valence and arousal levels via connectivity between EEG channels",
      "authors": [
        "Junwei Mo Chen",
        "Lei Han",
        "Jiahui Guo",
        "Ioannis Wang",
        "Patras"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2015.7344552"
    },
    {
      "citation_id": "7",
      "title": "Moods. Commun. ACM",
      "authors": [
        "J Peter",
        "Denning"
      ],
      "year": "2012",
      "venue": "Moods. Commun. ACM",
      "doi": "10.1145/2380656.2380668"
    },
    {
      "citation_id": "8",
      "title": "Confusion in Code Reviews: Reasons, Impacts, and Coping Strategies",
      "authors": [
        "Felipe Ebert",
        "Fernando Castor",
        "Nicole Novielli",
        "Alexander Serebrenik"
      ],
      "year": "2019",
      "venue": "26th IEEE International Conference on Software Analysis, Evolution and Reengineering",
      "doi": "10.1109/SANER.2019.8668024"
    },
    {
      "citation_id": "9",
      "title": "Basic Emotions. Handbook of Cognition and Emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic Emotions. Handbook of Cognition and Emotion"
    },
    {
      "citation_id": "10",
      "title": "Exploring Causes of Frustration for Software Developers",
      "authors": [
        "Denae Ford",
        "Chris Parnin"
      ],
      "year": "2015",
      "venue": "th IEEE/ACM International Workshop on Cooperative and Human Aspects of Software Engineering, CHASE 2015",
      "doi": "10.1109/CHASE.2015.19"
    },
    {
      "citation_id": "11",
      "title": "Using psycho-physiological measures to assess task difficulty in software development",
      "authors": [
        "Thomas Fritz",
        "Andrew Begel",
        "Sebastian M√ºller",
        "Serap Yigit-Elliott",
        "Manuela Z√ºger"
      ],
      "year": "2014",
      "venue": "36th International Conference on Software Engineering, ICSE '14",
      "doi": "10.1145/2568225.2568266"
    },
    {
      "citation_id": "12",
      "title": "A replication study on code comprehension and expertise using lightweight biometric sensors",
      "authors": [
        "Davide Fucci",
        "Daniela Girardi",
        "Nicole Novielli",
        "Luigi Quaranta",
        "Filippo Lanubile"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th International Conference on Program Comprehension, ICPC 2019"
    },
    {
      "citation_id": "13",
      "title": "Anger and Its Direction in Collaborative Software Development",
      "authors": [
        "Daviti Gachechiladze",
        "Filippo Lanubile",
        "Nicole Novielli",
        "Alexander Serebrenik"
      ],
      "year": "2017",
      "venue": "39th IEEE/ACM International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track, ICSE-NIER 2017",
      "doi": "10.1109/ICSE-NIER.2017.18"
    },
    {
      "citation_id": "14",
      "title": "Gaussian process dynamical models for multimodal affect recognition",
      "authors": [
        "F Hern√°n",
        "Mauricio Garc√≠a",
        "√Ålvaro √Ålvarez",
        "Orozco"
      ],
      "year": "2016",
      "venue": "38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/EMBC.2016.7590834"
    },
    {
      "citation_id": "15",
      "title": "Emotion detection using noninvasive low cost sensors",
      "authors": [
        "Daniela Girardi",
        "Filippo Lanubile",
        "Nicole Novielli"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2017.8273589"
    },
    {
      "citation_id": "16",
      "title": "Towards Recognizing the Emotions of Developers Using Biometrics: The Design of a Field Study",
      "authors": [
        "Daniela Girardi",
        "Filippo Lanubile",
        "Nicole Novielli",
        "Luigi Quaranta",
        "Alexander Serebrenik"
      ],
      "year": "2019",
      "venue": "Proceedings of the 4th International Workshop on Emotion Awareness in Software Engineering",
      "doi": "10.1109/SEmotion.2019.00010"
    },
    {
      "citation_id": "17",
      "title": "What happens when software developers are (un)happy",
      "authors": [
        "Fabian Daniel Graziotin",
        "Fagerholm"
      ],
      "year": "2018",
      "venue": "Journal of Systems and Software",
      "doi": "10.1016/j.jss.2018.02.041"
    },
    {
      "citation_id": "18",
      "title": "Happy software developers solve problems better: psychological measurements in empirical software engineering",
      "authors": [
        "Xiaofeng Daniel Graziotin",
        "Pekka Wang",
        "Abrahamsson"
      ],
      "year": "2014",
      "venue": "PeerJ",
      "doi": "10.7717/peerj.289"
    },
    {
      "citation_id": "19",
      "title": "Do feelings matter? On the correlation of affects and the self-assessed productivity in software engineering",
      "authors": [
        "Xiaofeng Daniel Graziotin",
        "Pekka Wang",
        "Abrahamsson"
      ],
      "year": "2015",
      "venue": "Journal of Software: Evolution and Process",
      "doi": "10.1002/smr.1673"
    },
    {
      "citation_id": "20",
      "title": "cvxEDA: A Convex Optimization Approach to Electrodermal Activity Processing",
      "authors": [
        "Alberto Greco",
        "Gaetano Valenza",
        "Antonio Lanata",
        "Enzo Pasquale Scilingo",
        "Luca Citi"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2015.2474131"
    },
    {
      "citation_id": "21",
      "title": "Move over ANOVA: progress in analyzing repeated-measures data and its reflection in papers published in the Archives of General Psychiatry",
      "authors": [
        "Ralitza Gueorguieva",
        "Jhon Krystal"
      ],
      "year": "2004",
      "venue": "Archives of general psychiatry",
      "doi": "10.1001/archpsyc.61.3.310"
    },
    {
      "citation_id": "22",
      "title": "Deep Feelings: A Massive Cross-Lingual Study on the Relation between Emotions and Virality",
      "authors": [
        "Marco Guerini",
        "Jacopo Staiano"
      ],
      "year": "2015",
      "venue": "Proceedings of the 24th International Conference on World Wide Web Companion, WWW 2015",
      "doi": "10.1145/2740908.2743058"
    },
    {
      "citation_id": "23",
      "title": "Towards Emotional Awareness in Software Development Teams",
      "authors": [
        "Emitza Guzman",
        "Bernd Bruegge"
      ],
      "year": "2013",
      "venue": "Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE'13",
      "doi": "10.1145/2491411.2494578"
    },
    {
      "citation_id": "24",
      "title": "DEVA: Sensing Emotions in the Valence Arousal Space in Software Engineering Text",
      "authors": [
        "Rakibul Md",
        "Minhaz Islam",
        "Zibran"
      ],
      "year": "2018",
      "venue": "Proceedings of the 33rd Annual ACM Symposium on Applied Computing, SAC 2018",
      "doi": "10.1145/3167132.3167296"
    },
    {
      "citation_id": "25",
      "title": "Automatic Prediction of Frustration",
      "authors": [
        "Ashish Kapoor",
        "Winslow Burleson",
        "Rosalind Picard"
      ],
      "year": "2007",
      "venue": "International Journal Human-Computer Studies",
      "doi": "10.1016/j.ijhcs.2007.02.003"
    },
    {
      "citation_id": "26",
      "title": "Whom Are You Going to Call?: Determinants of @-Mentions in GitHub Discussions",
      "authors": [
        "David Kavaler",
        "T Premkumar",
        "Vladimir Devanbu",
        "Filkov"
      ],
      "year": "2019",
      "venue": "Empirical Software Engineering",
      "doi": "10.1007/s10664-019-09728-3"
    },
    {
      "citation_id": "27",
      "title": "Do moods affect programmers' debug performance? Cognition",
      "authors": [
        "Iftikhar Ahmed Khan",
        "Willem-Paul Brinkman",
        "Robert Hierons"
      ],
      "year": "2011",
      "venue": "Technology & Work",
      "doi": "10.1007/s10111-010-0164-1"
    },
    {
      "citation_id": "28",
      "title": "Emotion Recognition Based on Physiological Changes in Music Listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth Andr√©"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2008.26"
    },
    {
      "citation_id": "29",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian M√ºhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Thierry Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transaction on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "30",
      "title": "Physiological Metrics of Mental Workload: A Review of Recent Progress",
      "authors": [
        "Arthur Kramer"
      ],
      "year": "1990",
      "venue": "Physiological Metrics of Mental Workload: A Review of Recent Progress",
      "doi": "10.21236/ada223701"
    },
    {
      "citation_id": "31",
      "title": "The caret Package",
      "authors": [
        "Max Kuhn"
      ],
      "year": "2009",
      "venue": "The caret Package"
    },
    {
      "citation_id": "32",
      "title": "Time Pressure in Software Engineering: A Systematic Literature Review",
      "authors": [
        "Miikka Kuutila",
        "Mika M√§ntyl√§",
        "Umar Farooq",
        "Ma√´lick Claes"
      ],
      "year": "2019",
      "venue": "Time Pressure in Software Engineering: A Systematic Literature Review",
      "arxiv": "arXiv:1901.05771"
    },
    {
      "citation_id": "33",
      "title": "The International Affective Picture System (IAPS) in the Study of Emotion and Attention",
      "authors": [
        "Peter Lang",
        "Margaret Bradley"
      ],
      "year": "2007",
      "venue": "Handbook of Emotion Elicitation and Attention"
    },
    {
      "citation_id": "34",
      "title": "The Experience Sampling Method",
      "authors": [
        "Reed Larson",
        "Mihaly Csikszentmihalyi"
      ],
      "year": "2014",
      "venue": "The Experience Sampling Method",
      "doi": "10.1007/978-94-017-9088-8_2"
    },
    {
      "citation_id": "35",
      "title": "Emotion and Adaptation",
      "authors": [
        "Richard Lazarus"
      ],
      "year": "1991",
      "venue": "Emotion and Adaptation"
    },
    {
      "citation_id": "36",
      "title": "Understanding the Influences of EEG Reference: A Large-Scale Brain Network Perspective",
      "authors": [
        "Xu Lei",
        "Keren Liao"
      ],
      "year": "2017",
      "venue": "Front Neurosci",
      "doi": "10.3389/fnins.2017.00205"
    },
    {
      "citation_id": "37",
      "title": "Emotion classification based on gamma-band EEG",
      "authors": [
        "Mu Li",
        "Bao-Liang Lu"
      ],
      "year": "2009",
      "venue": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/IEMBS.2009.5334139"
    },
    {
      "citation_id": "38",
      "title": "Mining valence, arousal, and dominance: possibilities for detecting burnout and productivity",
      "authors": [
        "Mika M√§ntyl√§",
        "Bram Adams",
        "Giuseppe Destefanis",
        "Daniel Graziotin",
        "Marco Ortu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 13th International Conference on Mining Software Repositories, MSR 2016",
      "doi": "10.1145/2901739.2901752"
    },
    {
      "citation_id": "39",
      "title": "Time Pressure: A Controlled Experiment of Test Case Development and Requirements Review",
      "authors": [
        "Mika M√§ntyl√§",
        "Kai Petersen",
        "O Timo",
        "Casper Lehtinen",
        "Lassenius"
      ],
      "year": "2014",
      "venue": "36th International Conference on Software Engineering, ICSE '14",
      "doi": "10.1145/2568225.2568245"
    },
    {
      "citation_id": "40",
      "title": "Grounded Theory and Organizational Research",
      "authors": [
        "Patricia Martin",
        "Barry Turner"
      ],
      "year": "1986",
      "venue": "The Journal of Applied Behavioral Science",
      "doi": "10.1177/002188638602200207"
    },
    {
      "citation_id": "41",
      "title": "Software Developers' Perceptions of Productivity",
      "authors": [
        "N Andr√©",
        "Gail Meyer",
        "Thomas Murphy",
        "Thomas Fritz",
        "Zimmermann"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, (FSE-22)",
      "doi": "10.1145/2635868.2635892"
    },
    {
      "citation_id": "42",
      "title": "Stuck and Frustrated or in Flow and Happy: Sensing Developers' Emotions and Progress",
      "authors": [
        "Sebastian M√ºller",
        "Thomas Fritz"
      ],
      "year": "2015",
      "venue": "37th IEEE/ACM International Conference on Software Engineering, ICSE 2015",
      "doi": "10.1109/ICSE.2015.334"
    },
    {
      "citation_id": "43",
      "title": "Using (bio)metrics to predict code quality online",
      "authors": [
        "Sebastian M√ºller",
        "Thomas Fritz"
      ],
      "year": "2016",
      "venue": "Proceedings of the 38th International Conference on Software Engineering, ICSE 2016",
      "doi": "10.1145/2884781.2884803"
    },
    {
      "citation_id": "44",
      "title": "A Hybrid Approach at Emotional State Detection: Merging Theoretical Models of Emotion with Data-Driven Statistical Classifiers",
      "authors": [
        "Pedro Nogueira",
        "Rui Rodrigues",
        "C Eug√©nio",
        "Lennart Oliveira",
        "Nacke"
      ],
      "year": "2013",
      "venue": "2013 IEEE/WIC/ACM International Conferences on Intelligent Agent Technology",
      "doi": "10.1109/WI-IAT.2013.117"
    },
    {
      "citation_id": "45",
      "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning",
      "authors": [
        "Sebastian Raschka"
      ],
      "year": "2018",
      "venue": "Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning",
      "arxiv": "arXiv:1811.12808"
    },
    {
      "citation_id": "46",
      "title": "Valence, Arousal and Dominance in the EEG During Game Play",
      "authors": [
        "Boris Reuderink",
        "Christian M√ºhl",
        "Mannes Poel"
      ],
      "year": "2013",
      "venue": "International Journal of Autonomous and Adaptive Communications Systems",
      "doi": "10.1504/IJAACS.2013.050691"
    },
    {
      "citation_id": "47",
      "title": "On-demand Developer Documentation",
      "authors": [
        "Martin Robillard",
        "Andrian Marcus",
        "Christoph Treude",
        "Gabriele Bavota",
        "Oscar Chaparro",
        "Neil Ernst",
        "Marco Gerosa",
        "Michael Godfrey",
        "Michele Lanza",
        "Mario V√°squez",
        "Gail Murphy",
        "Laura Moreno",
        "David Shepherd",
        "Edmund Wong"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Software Maintenance and Evolution",
      "doi": "10.1109/ICSME.2017.17"
    },
    {
      "citation_id": "48",
      "title": "Emotion Elicitation Using Films",
      "authors": [
        "Jonathan Rottemberg",
        "Rebecca Ray",
        "James Gross"
      ],
      "year": "2007",
      "venue": "Handbook of Emotion Elicitation and Assesment"
    },
    {
      "citation_id": "49",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "50",
      "title": "Culture and the categorization of emotions",
      "authors": [
        "James Russell"
      ],
      "year": "1991",
      "venue": "Psychological Bulletin",
      "doi": "10.1037/0033-2909.110.3.426"
    },
    {
      "citation_id": "51",
      "title": "Frustrating the user on purpose: a step toward building an affective computer",
      "authors": [
        "Jocelyn Scheirer",
        "Raul Fernandez",
        "Jonathan Klein",
        "Rosalind Picard"
      ],
      "year": "2002",
      "venue": "Frustrating the user on purpose: a step toward building an affective computer",
      "doi": "10.1016/S0953-5438(01)00059-5"
    },
    {
      "citation_id": "52",
      "title": "Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection",
      "authors": [
        "Mohammad Soleymani",
        "Sadjad Asghari-Esfeden",
        "Yun Fu",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transaction on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2436926"
    },
    {
      "citation_id": "53",
      "title": "Multimodal emotion recognition in response to videos (Extended abstract)",
      "authors": [
        "Mohammad Soleymani",
        "Maja Pantic",
        "Thierry Pun"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2015.7344615"
    },
    {
      "citation_id": "54",
      "title": "Automated parameter optimization of classification techniques for defect prediction models",
      "authors": [
        "Chakkrit Tantithamthavorn",
        "Shane Mcintosh",
        "Ahmed Hassan",
        "Kenichi Matsumoto"
      ],
      "year": "2016",
      "venue": "Proceedings of the 38th International Conference on Software Engineering, ICSE 2016",
      "doi": "10.1145/2884781.2884857"
    },
    {
      "citation_id": "55",
      "title": "The impact of automated parameter optimization on defect prediction models",
      "authors": [
        "Chakkrit Tantithamthavorn",
        "Shane Mcintosh",
        "Ahmed Hassan",
        "Kenichi Matsumoto"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Software Engineering",
      "doi": "10.1109/TSE.2018.2794977"
    },
    {
      "citation_id": "56",
      "title": "Understanding Interobserver Agreement: The Kappa Statistic",
      "authors": [
        "Anthony Viera",
        "Joanne Garrett"
      ],
      "year": "2005",
      "venue": "Family medicine"
    },
    {
      "citation_id": "57",
      "title": "Experimentation in software engineering",
      "authors": [
        "Claes Wohlin",
        "Per Runeson",
        "Martin H√∂st",
        "Magnus Ohlsson",
        "Bj√∂rn Regnell"
      ],
      "year": "2012",
      "venue": "Springer Science & Business Media",
      "doi": "10.1007/978-3-642-29044-2"
    },
    {
      "citation_id": "58",
      "title": "Sensing Interruptibility in the Office: A Field Study on the Use of Biometric and Computer Interaction Sensors",
      "authors": [
        "Manuela Z√ºger",
        "Sebastian M√ºller",
        "Andr√© Meyer",
        "Thomas Fritz"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3173574.3174165"
    }
  ]
}