{
  "paper_id": "2504.07521v2",
  "title": "Why We Feel: Breaking Boundaries In Emotional Reasoning With Multimodal Large Language Models",
  "published": "2025-04-10T07:33:49Z",
  "authors": [
    "Yuxiang Lin",
    "Jingdong Sun",
    "Zhi-Qi Cheng",
    "Jue Wang",
    "Haomin Liang",
    "Zebang Cheng",
    "Yifei Dong",
    "Jun-Yan He",
    "Xiaojiang Peng",
    "Xian-Sheng Hua"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EI's potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at https://github.com/Lum1104/EIBench, offering a foundation for advanced multimodal causal analysis and nextgeneration affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion analysis plays a pivotal role in diverse fields such as human-computer interaction (HCI)  [24, 42, 46, 65] , healthcare  [15, 50, 52] , and market research  [6, 7, 51] . While recent advances in emotion recognition (e.g., predict-ing whether someone feels \"happy\" or \"sad\") have offered valuable insights, they often overlook the deeper question of why a particular emotion arises. Because emotions can be subtle and highly subjective, merely labeling the emotional state fails to capture the nuanced triggers that might underlie or amplify the expressed affect.\n\nTo address the limitations of focusing on which emotion is present, we highlight the significance of emotion interpretation, where the objective is to explain why an individual experiences a specific emotional response. In practical applications (e.g., empathic virtual assistants, mental health counseling, user experience evaluations), identifying the emotion alone provides incomplete information if underlying triggers remain unknown. For instance, knowing a user is \"angry\" but not understanding whether the anger stems from waiting in a queue, receiving unfavorable feedback, or personal stressors hampers targeted interventions. Consequently, there is a need for systematic frameworks to help AI models identify and communicate reasons behind emotional states, thereby enabling more empathetic and context-aware intelligent services.\n\nIn response, we propose Emotion Interpretation (EI), shifting emphasis from recognizing an emotion label to reasoning about triggers behind it. Unlike classical emotion recognition, EI centers on why the emotional state arises and accommodates both explicit cues (e.g., visible objects, interpersonal interactions) and implicit or off-screen factors (e.g., historical context, hidden storylines). As shown in Figure  1 , EI spans scenarios from straightforward triggers (e.g., prolonged waiting leading to frustration) to complex ones with multiple emotional facets (e.g., overlapping sadness and resentment). Modern Vision-Language Models (VLLMs)  [3, 9, 32, [37] [38] [39] [40] 57]  hold promise for EI by integrating visual cues with rich world knowledge to produce explanatory text.\n\nDespite progress in multimodal learning, most existing datasets still focus on emotion classification rather than arXiv:2504.07521v2 [cs.AI] 17 Apr 2025 causal factors. Moreover, standard unimodal benchmarks seldom capture how vision, language, and context interact to explain emotional triggers. To address this gap, we create the EIBench dataset, comprising 1615 well-annotated basic EI samples plus 50 complex EI samples. Each sample challenges models to reason more deeply about multi-layered or co-occurring emotions. This dataset thus supports advanced evaluation protocols reflecting real-world complexity, in line with the push for more sophisticated multimodal benchmarking. Building on these objectives, our main contributions include:\n\n1. Task Definition: We formally define Emotion Interpretation (EI) as moving beyond simple emotion labeling toward revealing the causes behind an individual's emotional state. This shift enables more empathetic and context-aware AI systems. 2. Benchmark Dataset: We introduce EIBench, a largescale resource specifically aimed at EI, spanning four primary emotion categories (e.g., angry, sad, excited, happy) and complex scenarios where multiple emotions interlace. This dataset allows for evaluating diverse dimensions of emotional interpretation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Annotation Method (Cfsa):",
      "text": "We develop a Coarse-to-Fine Self-Ask (CFSA) procedure inspired by chain-ofthought reasoning  [4, 43, 47, 66, 69, 70] . By leveraging advanced Vision-Language Models in a semi-automated workflow, CFSA collects and refines multi-round insights about emotional triggers, yielding high-quality annotations that capture both explicit and implicit factors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comprehensive Evaluation: We Perform Systematic",
      "text": "experiments on both open-source and proprietary LLMs under four different testing settings (e.g., using im-age captions, chain-of-thought prompting, and personabased variations). Our results highlight significant performance gaps across these models. Notably, some proprietary models (e.g., Claude-3, ChatGPT-4) excel in simpler emotion interpretation tasks yet struggle to maintain the same level of accuracy in multi-perspective, complex scenarios-indicating the need for enhanced interpretative strategies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We review the most relevant lines of research that inform our work on Emotion Interpretation (EI). Unlike prior methods that primarily recognize an emotion label, our approach aims to interpret the latent triggers behind that emotion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Context-Aware Emotion Recognition",
      "text": "Facial Expression Recognition (FER) focuses on perceiving emotion from faces alone  [11, 35, 44, 53, 55, 56, 71] , whereas Context-Aware Emotion Recognition (CAER) leverages broader contextual cues  [5, 27, 34, 45, 49, 58, 62, 63, 67]  such as body language or background details. For instance, EMOTIC  [27]  integrates the body region and the global scene, while CAER-S  [28]  captures human social contexts from movie clips. Recently, Xenos et al.  [58]  exploited commonsense knowledge from Vision-Language Models (VLLMs) to boost CAER performance. However, these endeavors predominantly concentrate on determining which emotion is expressed, not on uncovering why the emotion arises. What might have caused the man holding the rainbow flag to become so excited and joyful?\n\nUser Question Preprocessing Question Parse Who is the person user talks about?\n\n(VLLM)\n\nThe man holding the rain bow flag. What is the user's demand? To know why the man is so excited and joyful. Question Reconstruction The man holding the rainbow flag is a proud and enthusiastic participant in a parade, possibly celebrating LGBTQ+ pride or a similar event. He is waving the flag high in the air, showing his support and joy. The user wants to know why the man is so excited and joyful.\n\nGeneral Self-Ask Thinking 1. How many people in this image, what are they doing?\n\n(Question: Text-only LLM (GPT-4))\n\n... They are standing on top of a box ... are likely participating in a parade or public event.\n\n(Answer: VLLM)\n\n2. What does \"the person user mentioned\" do in this image? Does he have interaction with others? ... is holding the rainbow flag and waving it high in the air...",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "What Is The Facial Expression Of The Person Mentioned?",
      "text": "The facial expression of the person holding the flag is one of excitement and joy.\n\n4. How about his body language? ... holding the flag is expressive and energetic, ... active participation in the event.\n\nScenario Specific Self-Ask Thinking 1. What specific ... the man holding the rainbow flag ... relates to his excitement and joy?\n\n(VLLM)\n\nThe man is celebrating LGBTQ+ pride ... as indicated by the rainbow flag. ... participating in such an event is a way for him to express his support for the LGBTQ+ community and celebrate their achievements and progress.\n\n2. What factors, such as personal experiences or beliefs, might contribute to the man's enthusiastic participation in the parade or event? ... he may have friends or family members who identify as LGBTQ+ ... 3. How does the man's enthusiasm and joy impact the overall atmosphere and energy of the crowd and the event?\n\nThe man's enthusiasm ... energy of the crowd and the event... 4. Are there any interactions or reactions from the crowd that might be influencing or being influenced by the man's excitement and joy? ... provide him with a sense of validation and encouragement for his participation in the event.\n\nEmotion Summarization 1. Celebrating LGBTQ+ pride or a similar event. 2. Supporting the LGBTQ+ community. (Emotional Triggers)\n\n3. Expressing his personal beliefs and values. 4. Feeling a sense of unity and belonging with the crowd. 5. Being part of a positive and uplifting event. 6. Standing on top of a box.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition With Llms",
      "text": "The advent of Large Language Models (LLMs) has introduced new possibilities for explainable emotion recognition  [12, 14, 17, 30, 41, 60] . Some approaches use chain-of-thought prompting to help LLMs identify hidden or implicit sentiments  [17] , whereas others employ retrieval-augmented pipelines for conversational emotion detection  [30] . In the multimodal domain, VLLMs  [37, 39, 40]  enable image-grounded reasoning  [13, 58, 60] , but these systems still center on labeling emotions rather than interpreting the underlying causes. By contrast, EI explores deeper triggers-even those not directly visible-and generates generative, flexible explanations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Humor Study",
      "text": "Humor is a specialized affective phenomenon that has received extensive attention  [1, 8, 10, 18-20, 22, 23, 61] . These works investigate features eliciting laughter, from cartoon contexts  [8]  to internet memes  [22]  and video laugh reasoning  [23] . Hessel et al.  [20]  tested LLMs on a subset of the New Yorker Cartoon Caption Contest to see whether they grasp humor's intricacies. While humor research constitutes a form of emotional interpretation-aiming to elucidate what makes content funny-our approach is broader, targeting the triggers of various emotional states rather than focusing exclusively on amusement.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Cause Extraction",
      "text": "Emotion Cause Extraction (ECE) seeks to find textual or multimodal clues explaining a known emotion  [29, 59] .\n\nEarly ECE work focused on identifying cause-effect pairs in textual corpora, often via multi-task learning to predict both emotion labels and their antecedents  [59] . Recently, Wang et al.  [54]  extended ECE to a multimodal setting in a SemEval challenge, where participants leveraged powerful LLM-based methods  [13, 30, 68]  to identify emotional triggers in speaker-centric conversations. Our Emotion Interpretation framework is related to ECE but goes further: it does not simply locate a cause within the input; rather, it allows for generative, flexible triggers (including implicit or off-screen context) and produces deeper explanations about why an individual feels a specific emotion.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Chain-Of-Thought Prompting",
      "text": "Chain-of-thought (CoT) prompting improves problemsolving by prompting LLMs to articulate intermediate reasoning steps  [4, 43, 47, 66, 69, 70] . Press et al.  [47]  introduced the Self-Ask strategy, having LLMs generate and answer sub-questions. Zhang et al.  [70]  extended this approach to multimodal contexts by decoupling rationale generation and reasoning. Our Coarse-to-Fine Self-Ask (CFSA) method similarly structures an LLM's introspection but is specialized for emotion interpretation, transitioning from general queries (e.g., number of people, basic context) to scenario-specific analysis of triggers. This hierarchical questioning strategy uncovers both explicit and implicit factors behind emotions, thus expanding CoT approaches into deeper affective reasoning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "Proposed Task. To explain why a given emotion emerges, we introduce Emotion Interpretation (EI). Let X be the space of images, each image x ∈ X consisting of a face component x face and a broader context x context . Let E be the set of possible emotions (e.g., happy, unhappy). We then define the query space:\n\nwhere each query q ∈ Q is an ordered pair (x, e). Rather than predicting e, EI aims to generate a set of emotional triggers T . Let S be the set of all possible triggers, encompassing both free-form textual explanations (e.g., full sentences) and concise labels (e.g., \"job loss\"). Formally, we introduce a generative function\n\nwhere P(S) denotes the power set of S. For a query q = (x, e) ∈ Q, the output\n\nrepresents the set of emotional triggers. Each trigger t i ∈ T may be either a descriptive sentence (e.g., \"He is sad because he lost his job.\") or a concise tag (e.g., \"job loss\" LGBT event. By parsing the user's query and identifying pertinent triggers, the system explains why the individual experiences a particular emotion, rather than merely detecting which emotion is displayed. For a broader comparison against existing emotion-related tasks, Table  2  details their respective objectives and input-output formalizations. Critically, EI focuses on causal triggers and reasons for emotional states, whereas most conventional approaches emphasize label prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Interpretation Benchmark",
      "text": "We now introduce EIBench, a curated benchmark for EI that builds on CAER-S  [28]  and EmoSet  [64] . To the best of our knowledge, EIBench is the first dataset dedicated to explaining why an emotion arises (rather than merely classifying which emotion is present), featuring 1615 basic EI samples and 50 complex EI samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vllm-Assisted Dataset Construction",
      "text": "Coarse-to-Fine Annotation. As outlined in Appendix Figure  3 , our Coarse-to-Fine Self-Ask (CFSA) pipeline decomposes an initially implicit query into multiple simpler Visual Question Answering (VQA) tasks. CFSA involves four phases: (1) Initial Question Preprocessing, (2) General Self-Ask Thinking, (3) Scenario Self-Ask Thinking, and (4) Emotion Summarization. After these automated steps, four volunteers thoroughly refine the annotations. Initial Question Preprocessing. A concise prompt steers a Large Language Model (LLM), GPT-4 (denoted ϕ), to enrich the user's initial query s init . Let s par = ϕ(s init ). Given an image x i ∈ X , we reconstruct a more elaborate prompt:\n\nwhere LLaVA-v1.6-34B (llava) is a state-of-the-art Vision-Language Model. While such VLLMs capture many visual details, they tend to overlook subtle emotional cues  [12] , necessitating the next \"self-ask\" phase. General Self-Asking. We prompt GPT-4 to generate openended questions across the dataset, storing them in S gen . From S gen , we identify four frequently repeated queries, S freq = {s freq 1 , s freq 2 , s freq 3 , s freq 4 }, focusing on: (i) number of people, (ii) activities/interactions, (iii) facial expressions, and (iv) body language. Each query s freq j prompts llava to produce an answer a gen j , aggregated into A gen . Scenario Self-Asking. We then supply the user query s query , reconstructed prompt s rec i , and the pairs {S freq , A gen } to llava for scenario-level questioning, yielding S sce i . Finally, an advanced LLM (e.g., LLaMA-3) integrates all collected answers to summarize emotional triggers. Table  1  illustrates a CFSA-assisted annotation example. Human In-the-Loop Annotation. The CFSA pipeline serves as a baseline. Four human annotators refine these automatic labels by: (1) removing hallucinations (Appendix C.1), (2) adding commonsense knowledge (Appendix C.2), and (3) pruning irrelevant triggers. To validate annotation quality, we randomly sample 50 images from each emotion category (200 total) for a final review by three volunteers, who rate their confidence in triggers on a 0-5 scale (scores ¡ 3 signal poor or incomplete triggers). As shown in Table  3 , all final ratings exceed 4.0, demonstrating EIBench's reliable annotations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Dataset Overview & Evaluation",
      "text": "Data Sources. EIBench is derived from CAER-S  [28] , which features seven emotion types (angry, disgust, fear, happy, neutral, sad, surprise), and EmoSet  [64] , comprising eight (anger, disgust, fear, sadness, amusement, awe, contentment, excitement). To balance diversity with manageable annotation costs, we focus on four target emotions: angry, sad, excited, and happy. Data Composition & Trigger Distribution. Table  4  lists fine-grained variants (e.g., annoyed, forlorn, thrilled) for these four primary emotions. EIBench also includes 50 complex samples, each annotated from multiple emotional perspectives. Emotional triggers fall into ten broad categories (e.g., atmosphere, social interactions, body movements), as illustrated in Figure  2 . Notably, atmosphere and other predominate for basic emotions, while social interactions and body movements dominate the complex subset. Comparison with Existing Datasets. Table  5  contrasts EIBench with other emotion-related corpora. Unlike conventional datasets that classify a single dominant emotion,   EIBench enables generative explanations of why an emotion emerges, including complex labeling. Appendix B.4 provides further visualization of nuanced subset samples. Evaluation Metrics. We measure performance via:\n\n(1) Emotional Trigger Recall, which checks whether predicted triggers overlap with ground-truth annotations (multiple valid triggers can exist for one sample); and (2) Long-Term Coherence, which assesses whether a model maintains thematic and emotional consistency in longer outputs. Specifically, we extract triggers from LLaMA-3 or Chat-GPT3.5 responses, then use a BERT-based approach  [16]  to measure sentence-to-sentence similarity.\n\n1 https://qwenlm.github.io/blog/qwen-vl/ 2 https://openai.com/index/gpt-4-research/ 3 https://openai.com/index/hello-gpt-4o/ 4 https : / / docs . anthropic . com / en / docs / modelsoverview",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we evaluate both prominent open-source and proprietary models on our proposed benchmark. We design four distinct modes to assess each model's capability in Emotion Interpretation (EI), and we conclude with an indepth analysis of these results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Modes of Evaluation. We introduce four modes to investigate how LLMs approach EI: • User Question (UQ): In this zero-shot scenario, the user's question is provided verbatim. This setting examines each model's direct ability to handle natural, potentially ambiguous queries. • User Question + Caption (UQ+C): The user question is enriched by a caption (see Section 4 for details on caption generation). This aims to clarify context and improve accuracy. We also include a text-only baseline with LLaMA-3 fed the same caption. • User Question + CoT (UQ+CoT): In this mode, a succinct chain-of-thought style prompt (e.g., \"Let's think step by step\") is appended to the user's query. This setup intentionally encourages the model to reason more systematically, revealing key intermediate thought processes. • CFSA Setting (CFSA): We carefully employ the Coarseto-Fine Self-Ask (CFSA) method, implemented by LLaVA-NEXT (34B), to divide the EI task into more manageable sub-queries. This scenario essentially demonstrates an upper-bound performance facilitated by a well-structured question-answer pipeline.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Performance",
      "text": "Basic EI Results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation On Persona Prompts",
      "text": "Inspired by PsychoBench  [21] , we examine whether assigning different personas to LLMs modulates EI performance. Table  7  compares four settings: (i) no persona, (ii) AI Assistant persona, (iii) Architecture expert, and (iv) Emotion expert. Models consistently achieve higher scores when framed as emotion experts, suggesting domain-specific personas help center chain-of-thought on emotional triggers. In contrast, an architecture persona often degrades EI performance below the no-persona baseline, implying mismatched prompts overshadow emotional reasoning. These results show that well-chosen personas, aligned with the target domain, can guide LLMs toward more accurate, context-driven EI interpretations.   6 ) generally improves performance over UQ, indicating that a structured, step-by-step approach helps surface hidden emotional triggers. These gains align with the CFSA pipeline's rationale that detailed introspection (i.e., CoT) better exposes causal factors behind human emotions. Indeed, the higher performance in CoT-like settings further supports the idea that complex tasks-like explaining why a person feels a certain way-benefit more from reasoned dialogues than from direct, single-shot responses. CFSA Upper Bound. By converting queries into multiple simpler VQA tasks, the CFSA configuration yields the strongest results among open-source VLLMs, capturing around 68% of emotional triggers in Table  6 . This still falls short of manual annotations, highlighting the complexity of EI. Nonetheless, it demonstrates that a carefully structured pipeline can significantly narrow the gap between raw zeroshot performance and a more expert-level approach.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Key Observations And Limitations",
      "text": "Human-Level Annotation Gap. While CFSA-based methods show promise, they still exhibit a noticeable gap from human-labeled data, indicating that subtle emotional cues remain difficult for LLMs to capture. This gap reinforces the need for refined instruction tuning and more sophisticated context modeling. Discrepancies Across Emotions. Both Table  6  and Table  8  reveal that performance varies widely by emotion category. Models generally handle Happy and Sadness more successfully, whereas Excitement and Complex Mixed Emotions pose greater challenges-possibly due to more nuanced or overlapping triggers.\n\nOpen vs. Closed-Source Trade-offs. Although certain open-source systems (e.g., LLaVA-1.5, LLaVA-NEXT) rival or surpass smaller closed-source models, they still typically trail behind top-tier closed-source ones (e.g., Claude-3, ChatGPT-4o). This discrepancy emphasizes how additional proprietary data and advanced training can drive incremental performance gains.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This work reframes emotion analysis by asking why an emotion arises rather than which emotion is present. We introduced EIBench for Emotion Interpretation (EI), highlighting causal triggers of affective states via both explicit cues (e.g., visible objects) and implicit factors (e.g., cultural norms). Our Coarse-to-Fine Self-Ask pipeline and evaluations on open-source and proprietary large language models demonstrate the potential of EI to enrich empathy and context-awareness in AI. Nevertheless, models still struggle with overlapping emotions and subtle cues beyond their training scope, our dataset, though broad, cannot capture all real-world scenarios, and existing interpretability metrics for causal reasoning need further refinement. Future work should explore deeper integration with audio and textual dialogues, extended causal modeling to handle subtle emotional overlaps, and more adaptive evaluation protocols in dynamic contexts with user-specific adaptability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A.2. Intended Audiences",
      "text": "EIBench aims to advance EI by capturing the subjective nature of emotional states. Addressing the dataset's challenges can lead to empathetic AI systems, enriching emotion-driven applications and enhancing human-computer interactions. Additionally, these insights may benefit tasks like humor understanding, harmful stance detection, and other domains that hinge on implicit emotion cues. Overall, EIBench paves the way for multifaceted, context-driven emotion interpretation, pushing the boundaries of next-generation EI research. Trained in three stages-broad pretraining, task-specific fine-tuning on high-quality datasets, and multimodal instruction tuning-MiniGPT-v2 excels at chatbot-style interactions and complex multimodal tasks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Baseline Models",
      "text": "Otter. Otter  [32]  leverages OpenFlamingo  [2]  to perform multi-modal in-context instruction tuning. Each data instance in its MIMIC-IT  [31]  training set comprises an instruction-image-answer triplet along with relevant incontext examples. By conditioning the language model on image-caption or instruction-response pairs, Otter attains strong instruction-following skills and effectively learns from contextual exemplars. LLaVA-1.5. LLaVA-1.5  [40]  builds on CLIP-ViT-L-336px  [48]   C. Human-in-the-Loop Data Cleaning",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C.1. Addressing Hallucinations In Vllms",
      "text": "Vision Large Language Models (VLLMs) can sometimes produce hallucinated triggers unrelated to the actual image content. Table  14  shows examples in which the model invents triggers (e.g., \"Doing mountain biking\") with no supporting evidence. Such hallucinations undermine dataset quality by misrepresenting the visual context. To mitigate these errors, we implement a human-in-the-loop cleaning process: annotators review the VLLM's outputs, remove triggers not clearly supported by the image, and note ambiguous regions for further inspection. By systematically weeding out these misinterpretations, we reduce biases introduced by VLLM-driven hallucinations.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C.2. Incorporating Commonsense Knowledge",
      "text": "Even when models avoid overt hallucinations, they may overlook commonsense cues essential to explaining an emotional state. Table  15  illustrates how human annotators augment triggers with contextual or cultural knowledge absent from raw VLLM outputs. For instance, the model may label an emotion as \"angry\" but omit a crucial real-life cause (e.g., \"waiting for lost luggage\"), prompting annotators to add relevant details. By explicitly integrating commonsense reasoning, the final dataset more closely aligns with realworld emotional triggers, thus enhancing the fidelity and utility of EIBench for emotion interpretation tasks.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D. Case Study Of The Vllms' Ei Abilities",
      "text": "In this section, we present a detailed examination of how various Vision-Language Models (VLLMs) handle Emotion Interpretation (EI), focusing on both hallucinations and commonsense knowledge integration. Tables  14  and 15  illustrate how a human-in-the-loop data cleaning process identifies and corrects inaccuracies or omissions in VLLM outputs.\n\nHallucinations in VLLMs. Table  14  shows instances where the VLLM-generated triggers deviate from the image content (e.g., \"Doing mountain biking\" when no bike is present), misrepresenting the scene and undermining dataset quality. By having human annotators remove or adjust these erroneous details, we mitigate biases that might otherwise skew emotion interpretation.\n\nCommonsense Knowledge Integration.   10 ), VLLMs often identify straightforward triggers (e.g., \"long wait,\" \"enjoying the view\"). Meanwhile, Complex samples (Table  12 ) feature overlapping triggers or multiple emotional states, frequently exposing model challenges in capturing less obvious cues. Detailed Model Responses. Tables 14-15 present user queries and ground-truth triggers, alongside raw VLLM outputs (e.g., Qwen-VL-Chat, LLaVA family, MiniGPT, Otter, and ChatGPT-4). Each response is evaluated by LLaMA-3 and ChatGPT for alignment with the annotated triggers. A common pattern emerges: Certain triggers (e.g., metal claws, intense gaze) are detected reliably, while subtler elements (e.g., wide-opening eyes, \"defending gesture,\" \"shrunk muscle\") are overlooked or inconsistently recognized. Some VLLMs also invent erroneous triggers (e.g., \"concern about a meal he's preparing\") incongruent with the annotated details. Insights and Implications. These case studies highlight the complexity of moving from mere emotion recognition to interpretation. Straightforward triggers are typically recognized, but nuanced emotions often hinge on contextual, cultural, or implicit cues. Human review and data cleaning (Sections C.1-C.2) remain vital for honing outputs, particularly in ambiguous or subtle contexts. EIBench thus provides a structured environment for testing not only Basic scenarios but also the Complex interactions that more closely mirror real-world emotional landscapes.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , EI spans scenarios from straightforward trig-",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustrative examples of Emotion Interpretation in five categories: (a) Angry, (b) Sad, (c) Happy, (d) Excited, and (e) Complex.",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of emotional triggers across distinct cate-",
      "page": 5
    },
    {
      "caption": "Figure 2: Notably, atmosphere and",
      "page": 5
    },
    {
      "caption": "Figure 3: Pipeline of the VLLM-assisted dataset construction.",
      "page": 15
    },
    {
      "caption": "Figure 4: Visualization of the numbers of emotional triggers across different categories (Basic Emotions).",
      "page": 17
    },
    {
      "caption": "Figure 5: Visualization of the numbers of emotional triggers in the Complex EI Subset.",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "What might have caused the woman in the image to appear irritated or angry?\n1.Service issue\n2.Unpleasant environment\n3.Dissatisfaction with food or service\n4.Annoyance with the conversation or situation\n(a) Angry": "What might have caused the woman in the image to appear content and happy?\n1.Positive news about her health\n2.Pleasant interaction with a medical professional\n3.Comforting conversation with a friend or family member\n4.Good news about her health\n5.Positive relationship with the medical staff\n(c) Happy",
          "What might have caused the young child in the image to be crying?\n1.Physical pain or discomfort in the arm\n2.Distress or upset due to the unknown cause of the pain\n3.Fear or anxiety related to the unknown cause of the pain\n4.Discomfort or pain caused by a physical injury or trauma\n5.The adult's concern and attempt to comfort the child\n(b) Sad": "What might have caused the baby's delight in this image?\n1.Halloween costume and bib with a pumpkin design\n2.Interaction with the person holding them up\n3.Festive atmosphere from the person holding him up\n4.First Halloween experience\n(d) Excite"
        },
        {
          "What might have caused the woman in the image to appear irritated or angry?\n1.Service issue\n2.Unpleasant environment\n3.Dissatisfaction with food or service\n4.Annoyance with the conversation or situation\n(a) Angry": "What may caused the little girl upset? What may cause the woman angry? What may caused the little girl happy?\n1. Crying 1. The girl is not obedient 1. Crying but the women comfort her\n2. Can not making handiwork 2. The girl can't do handiwork 2. Can not making handiwork\n3. The woman blamed her 3. Step-by-step instruction 3. Woman help her finishing the work\n4. The girl can't learn no matter how much taught\n(e) Complex",
          "What might have caused the young child in the image to be crying?\n1.Physical pain or discomfort in the arm\n2.Distress or upset due to the unknown cause of the pain\n3.Fear or anxiety related to the unknown cause of the pain\n4.Discomfort or pain caused by a physical injury or trauma\n5.The adult's concern and attempt to comfort the child\n(b) Sad": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: This table demonstrates how the CFSA Method interprets excitement and joy at an LGBT event, where pink text highlights",
      "data": [
        {
          "herainbowflagisaproudanden": "possiblycelebratingLGBTQ+p",
          "thusiasticp": "rideorasi"
        },
        {
          "herainbowflagisaproudanden": "gtheflaghighintheair,showing",
          "thusiasticp": "hissuppo"
        },
        {
          "herainbowflagisaproudanden": "tstoknowwhythemanissoexci",
          "thusiasticp": "tedandjoy"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Scores are reported under LLaMA-3 / ChatGPT criteria, with \"Overall\" denoting the aggregated result. Models Happy Angry Sadness Excitement Overall User Question Qwen-VL-Chat",
      "venue": "Scores are reported under LLaMA-3 / ChatGPT criteria, with \"Overall\" denoting the aggregated result. Models Happy Angry Sadness Excitement Overall User Question Qwen-VL-Chat"
    },
    {
      "citation_id": "2",
      "title": "Effect of persona prompts on model performance, evaluated by LLaMA-3 / ChatGPT criteria",
      "venue": "w/o Persona\" indicates no explicit persona, while \"AI Assistant, Architecture"
    },
    {
      "citation_id": "3",
      "title": "Colbert: Using bert sentence embedding for humor detection",
      "authors": [
        "Issa Annamoradnejad",
        "Gohar Zoghi"
      ],
      "venue": "Colbert: Using bert sentence embedding for humor detection",
      "arxiv": "arXiv:2004.12765"
    },
    {
      "citation_id": "4",
      "title": "Openflamingo: An opensource framework for training large autoregressive visionlanguage models",
      "authors": [
        "Anas Awadalla",
        "Irena Gao",
        "Josh Gardner",
        "Jack Hessel",
        "Yusuf Hanafy",
        "Wanrong Zhu",
        "Yonatan Kalyani Marathe",
        "Samir Bitton",
        "Shiori Gadre",
        "Sagawa"
      ],
      "year": "2023",
      "venue": "Openflamingo: An opensource framework for training large autoregressive visionlanguage models",
      "arxiv": "arXiv:2308.01390"
    },
    {
      "citation_id": "5",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "6",
      "title": "Graph of thoughts: Solving elaborate problems with large language models",
      "authors": [
        "Maciej Besta",
        "Nils Blach",
        "Ales Kubicek",
        "Robert Gerstenberger",
        "Michal Podstawski",
        "Lukas Gianinazzi",
        "Joanna Gajda",
        "Tomasz Lehmann",
        "Hubert Niewiadomski",
        "Piotr Nyczyk"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Affective computing and sentiment analysis. A practical guide to sentiment analysis",
      "authors": [
        "Erik Cambria",
        "Dipankar Das",
        "Sivaji Bandyopadhyay",
        "Antonio Feraco"
      ],
      "year": "2017",
      "venue": "Affective computing and sentiment analysis. A practical guide to sentiment analysis"
    },
    {
      "citation_id": "9",
      "title": "Affective computing in marketing: practical implications and research opportunities afforded by emotionally intelligent machines",
      "authors": [
        "Delphine Caruelle",
        "Poja Shams",
        "Anders Gustafsson",
        "Line Lervik-Olsen"
      ],
      "year": "2022",
      "venue": "Marketing Letters"
    },
    {
      "citation_id": "10",
      "title": "We are humor beings: Understanding and predicting visual humor",
      "authors": [
        "Arjun Chandrasekaran",
        "K Ashwin",
        "Stanislaw Vijayakumar",
        "Mohit Antol",
        "Dhruv Bansal",
        "C Batra",
        "Devi Lawrence Zitnick",
        "Parikh"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "12",
      "title": "Can pre-trained language models understand chinese humor?",
      "authors": [
        "Yuyan Chen",
        "Zhixu Li",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Bang Liu",
        "Yunwen Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining"
    },
    {
      "citation_id": "13",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "authors": [
        "Zebang Cheng",
        "Fuqiang Niu",
        "Yuxiang Lin",
        "Zhi-Qi Cheng",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2024",
      "venue": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "arxiv": "arXiv:2404.00511"
    },
    {
      "citation_id": "16",
      "title": "Sztu-cmu at mer2024: Improving emotion-llama with convattention for multimodal emotion recognition",
      "authors": [
        "Zebang Cheng",
        "Shuyuan Tu",
        "Dawei Huang",
        "Minghan Li",
        "Xiaojiang Peng",
        "Zhi-Qi Cheng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Sleep in children and adolescents with behavioral and emotional disorders",
      "authors": [
        "E Ronald",
        "Allison Dahl",
        "Harvey"
      ],
      "year": "2007",
      "venue": "Sleep medicine clinics"
    },
    {
      "citation_id": "18",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "19",
      "title": "Reasoning implicit sentiment with chain-ofthought prompting",
      "authors": [
        "Bobo Hao Fei",
        "Qian Li",
        "Lidong Liu",
        "Fei Bing",
        "Tat-Seng Li",
        "Chua"
      ],
      "year": "2023",
      "venue": "Reasoning implicit sentiment with chain-ofthought prompting",
      "arxiv": "arXiv:2305.11255"
    },
    {
      "citation_id": "20",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Ur-funny: A multimodal language dataset for understanding humor",
      "arxiv": "arXiv:1904.06618"
    },
    {
      "citation_id": "21",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "22",
      "title": "Do androids laugh at electric sheep? humor \"understanding\" benchmarks from the new yorker caption contest",
      "authors": [
        "Jack Hessel",
        "Ana Marasović",
        "Jena Hwang",
        "Lillian Lee",
        "Jeff Da",
        "Rowan Zellers",
        "Robert Mankoff",
        "Yejin Choi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "On the humanity of conversational ai: Evaluating the psychological portrayal of llms",
      "authors": [
        "Jen-Tse Huang",
        "Wenxuan Wang",
        "Eric Li",
        "Man Lam",
        "Shujie Ren",
        "Youliang Yuan",
        "Wenxiang Jiao",
        "Zhaopeng Tu",
        "Michael Lyu"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "24",
      "title": "Memecap: A dataset for captioning and interpreting memes",
      "authors": [
        "Eunjeong Hwang",
        "Vered Shwartz"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Smile: Multimodal dataset for understand-ing laughter in video with language models",
      "authors": [
        "Lee Hyun",
        "Kim Sung-Bin",
        "Seungju Han",
        "Youngjae Yu",
        "Tae-Hyun Oh"
      ],
      "year": "2023",
      "venue": "Smile: Multimodal dataset for understand-ing laughter in video with language models",
      "arxiv": "arXiv:2312.09818"
    },
    {
      "citation_id": "26",
      "title": "Impact of irritation and negative emotions on the performance of voice assistants: Netting dissatisfied customers' perspectives",
      "authors": [
        "Shilpi Jain",
        "Sriparna Basu",
        "Arghya Ray",
        "Ronnie Das"
      ],
      "year": "2023",
      "venue": "International Journal of Information Management"
    },
    {
      "citation_id": "27",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "arxiv": "arXiv:2310.06825"
    },
    {
      "citation_id": "28",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "29",
      "title": "Emotic: Emotions in context dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "30",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "31",
      "title": "A text-driven rule-based system for emotion cause detection",
      "authors": [
        "Sophia Yat",
        "Mei Lee",
        "Ying Chen",
        "Chu-Ren Huang"
      ],
      "year": "2010",
      "venue": "Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text"
    },
    {
      "citation_id": "32",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "33",
      "title": "Mimicit: Multi-modal in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Fanyi Pu",
        "Jingkang Yang",
        "Chunyuan Li",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Mimicit: Multi-modal in-context instruction tuning",
      "arxiv": "arXiv:2306.05425"
    },
    {
      "citation_id": "34",
      "title": "Otter: A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "35",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "36",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "Weixin Li",
        "Xuan Dong",
        "Yunhong Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Fer-former: Multi-modal transformer for facial expression recognition",
      "authors": [
        "Yande Li",
        "Mingjie Wang",
        "Minglun Gong",
        "Yonggang Lu",
        "Li Liu"
      ],
      "year": "2023",
      "venue": "Fer-former: Multi-modal transformer for facial expression recognition",
      "arxiv": "arXiv:2303.12997"
    },
    {
      "citation_id": "38",
      "title": "Ex-plainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Ex-plainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "39",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "40",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Improved baselines with visual instruction tuning",
      "arxiv": "arXiv:2310.03744"
    },
    {
      "citation_id": "41",
      "title": "Llava-next: Improved reasoning, ocr, and world knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Llava-next: Improved reasoning, ocr, and world knowledge"
    },
    {
      "citation_id": "42",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "Generated knowledge prompting for commonsense reasoning",
      "authors": [
        "Jiacheng Liu",
        "Alisa Liu",
        "Ximing Lu",
        "Sean Welleck",
        "Peter West",
        "Le Ronan",
        "Yejin Bras",
        "Hannaneh Choi",
        "Hajishirzi"
      ],
      "year": "2021",
      "venue": "Generated knowledge prompting for commonsense reasoning",
      "arxiv": "arXiv:2110.08387"
    },
    {
      "citation_id": "44",
      "title": "How should voice assistants deal with users",
      "authors": [
        "Yong Ma",
        "Heiko Drewes",
        "Andreas Butz"
      ],
      "year": "2022",
      "venue": "How should voice assistants deal with users",
      "arxiv": "arXiv:2204.02212"
    },
    {
      "citation_id": "45",
      "title": "Self-refine: Iterative refinement with self-feedback",
      "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Prakhar Gupta",
        "Skyler Hallinan",
        "Luyu Gao",
        "Sarah Wiegreffe",
        "Uri Alon",
        "Nouha Dziri",
        "Shrimai Prabhumoye",
        "Yiming Yang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "Jiawei Mao",
        "Rui Xu",
        "Xuesong Yin",
        "Yuanqi Chang",
        "Binling Nie",
        "Aibin Huang"
      ],
      "year": "2023",
      "venue": "Poster++: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "47",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "Trisha Mittal",
        "Pooja Guhan",
        "Uttaran Bhattacharya",
        "Rohan Chandra",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Experiential qualities of whispering with voice assistants",
      "authors": [
        "Emmi Parviainen",
        "Marie Louise",
        "Juul Søndergaard"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "49",
      "title": "Measuring and narrowing the compositionality gap in language models",
      "authors": [
        "Ofir Press",
        "Muru Zhang",
        "Sewon Min",
        "Ludwig Schmidt",
        "Noah Smith",
        "Mike Lewis"
      ],
      "year": "2022",
      "venue": "Measuring and narrowing the compositionality gap in language models",
      "arxiv": "arXiv:2210.03350"
    },
    {
      "citation_id": "50",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "51",
      "title": "Context-aware generation-based net for multi-label visual emotion recognition",
      "authors": [
        "Kun Shulan Ruan",
        "Yijun Zhang",
        "Hanqing Wang",
        "Weidong Tao",
        "Guangyi He",
        "Enhong Lv",
        "Chen"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "52",
      "title": "Emotional development: Action, communication, and understanding. Handbook of child psychology",
      "authors": [
        "Carolyn Saarni",
        "Joseph Campos",
        "Linda Camras",
        "David Witherington"
      ],
      "year": "2007",
      "venue": "Emotional development: Action, communication, and understanding. Handbook of child psychology"
    },
    {
      "citation_id": "53",
      "title": "Modern-day marketing concepts based on face recognition and neuro-marketing: a review and future research directions",
      "authors": [
        "Gautam Srivastava",
        "Surajit Bag"
      ],
      "year": "2024",
      "venue": "Benchmarking: An International Journal"
    },
    {
      "citation_id": "54",
      "title": "Emotions and emotional communication in infants. Parent-infant psychodynamics",
      "authors": [
        "Edward Z Tronick"
      ],
      "year": "2018",
      "venue": "Emotions and emotional communication in infants. Parent-infant psychodynamics"
    },
    {
      "citation_id": "55",
      "title": "Pyramid with super resolution for in-thewild facial expression recognition",
      "authors": [
        "Thanh-Hung Vo",
        "Guee-Sang Lee",
        "Hyung-Jeong Yang",
        "Soo-Hyung Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "56",
      "title": "Semeval-2024 task 3: Multimodal emotion cause analysis in conversations",
      "authors": [
        "Fanfan Wang",
        "Heqing Ma",
        "Jianfei Yu",
        "Rui Xia",
        "Erik Cambria"
      ],
      "year": "2024",
      "venue": "Semeval-2024 task 3: Multimodal emotion cause analysis in conversations",
      "arxiv": "arXiv:2405.13049"
    },
    {
      "citation_id": "57",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "58",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Debin Meng",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "59",
      "title": "Skeleton-in-context: Unified skeleton sequence modeling with in-context learning",
      "authors": [
        "Xinshun Wang",
        "Zhongbin Fang",
        "Xia Li",
        "Xiangtai Li",
        "Chen Chen",
        "Mengyuan Liu"
      ],
      "year": "2023",
      "venue": "Skeleton-in-context: Unified skeleton sequence modeling with in-context learning",
      "arxiv": "arXiv:2312.03703"
    },
    {
      "citation_id": "60",
      "title": "Vllms provide better context for emotion understanding through common sense reasoning",
      "authors": [
        "Alexandros Xenos",
        "Niki Foteinopoulou"
      ],
      "year": "2024",
      "venue": "Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos",
      "arxiv": "arXiv:2404.07078"
    },
    {
      "citation_id": "61",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "62",
      "title": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "Hongxia Xie",
        "Chu-Jun Peng",
        "Yu-Wen Tseng",
        "Hung-Jen Chen",
        "Chan-Feng Hsu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2024",
      "venue": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "arxiv": "arXiv:2404.16670"
    },
    {
      "citation_id": "63",
      "title": "Humor recognition and humor anchor extraction",
      "authors": [
        "Diyi Yang",
        "Alon Lavie",
        "Chris Dyer",
        "Eduard Hovy"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "64",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "65",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Yuzheng Wang",
        "Shunli Wang",
        "Mingcheng Li",
        "Siao Liu",
        "Xiao Zhao",
        "Shuai Huang",
        "Zhiyan Dong",
        "Peng Zhai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "Jingyuan Yang",
        "Qirui Huang",
        "Tingting Ding",
        "Dani Lischinski",
        "Danny Cohen-Or",
        "Hui Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Understanding affective experiences with conversational agents",
      "authors": [
        "Xi Yang",
        "Marco Aurisicchio",
        "Weston Baxter"
      ],
      "year": "2019",
      "venue": "proceedings of the 2019 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "68",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Tom Griffiths",
        "Yuan Cao",
        "Karthik Narasimhan"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "Contextaware affective graph reasoning for emotion recognition",
      "authors": [
        "Minghui Zhang",
        "Yumeng Liang",
        "Huadong Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "70",
      "title": "Samsung research chinabeijing at semeval-2024 task 3: A multi-stage framework for emotion-cause pair extraction in conversations",
      "authors": [
        "Shen Zhang",
        "Haojie Zhang",
        "Jing Zhang",
        "Xudong Zhang",
        "Yimeng Zhuang",
        "Jinting Wu"
      ],
      "year": "2024",
      "venue": "Samsung research chinabeijing at semeval-2024 task 3: A multi-stage framework for emotion-cause pair extraction in conversations",
      "arxiv": "arXiv:2404.16905"
    },
    {
      "citation_id": "71",
      "title": "Automatic chain of thought prompting in large language models",
      "authors": [
        "Zhuosheng Zhang",
        "Aston Zhang",
        "Mu Li",
        "Alex Smola"
      ],
      "year": "2022",
      "venue": "Automatic chain of thought prompting in large language models",
      "arxiv": "arXiv:2210.03493"
    },
    {
      "citation_id": "72",
      "title": "Multimodal chain-ofthought reasoning in language models",
      "authors": [
        "Zhuosheng Zhang",
        "Aston Zhang",
        "Mu Li",
        "Hai Zhao",
        "George Karypis",
        "Alex Smola"
      ],
      "year": "2023",
      "venue": "Multimodal chain-ofthought reasoning in language models",
      "arxiv": "arXiv:2302.00923"
    },
    {
      "citation_id": "73",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "Ce Zheng",
        "Matias Mendieta",
        "Chen Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    }
  ]
}