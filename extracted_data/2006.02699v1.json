{
  "paper_id": "2006.02699v1",
  "title": "Pulsegan: Learning To Generate Realistic Pulse Waveforms In Remote Photoplethysmography",
  "published": "2020-06-04T08:38:30Z",
  "authors": [
    "Rencheng Song",
    "Huan Chen",
    "Juan Cheng",
    "Chang Li",
    "Yu Liu",
    "Xun Chen"
  ],
  "keywords": [
    "Heart rate estimation",
    "remote photoplethysmography",
    "generative adversarial network",
    "pulse waveform",
    "heart rate variability"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Remote photoplethysmography (rPPG) is a noncontact technique for measuring cardiac signals from facial videos. High-quality rPPG pulse signals are urgently demanded in many fields, such as health monitoring and emotion recognition. However, most of the existing rPPG methods can only be used to get average heart rate (HR) values due to the limitation of inaccurate pulse signals. In this paper, a new framework based on generative adversarial network, called PulseGAN, is introduced to generate realistic rPPG pulse signals through denoising the chrominance signals. Considering that the cardiac signal is quasiperiodic and has apparent time-frequency characteristics, the error losses defined in time and spectrum domains are both employed with the adversarial loss to enforce the model generating accurate pulse waveforms as its reference. The proposed framework is tested on the public UBFC-RPPG database in both within-database and cross-database configurations. The results show that the PulseGAN framework can effectively improve the waveform quality, thereby enhancing the accuracy of HR, the heart rate variability (HRV) and the interbeat interval (IBI). The proposed method achieves the best performance compared to the denoising autoencoder (DAE) and CHROM, with the mean absolute error of AVNN (the average of all normalto-normal intervals) improving 20.85% and 41.19%, and the mean absolute error of SDNN (the standard deviation of all NN intervals) improving 20.28% and 37.53%, respectively, in the cross-database test. This framework can be easily extended to other existing deep learning-based rPPG methods, which is expected to expand the application scope of rPPG techniques.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "C ARDIAC signal is an important physiological signal to monitor the human body's health and emotional status. The common ways for obtaining cardiac signals include electrocardiogram (ECG) and photoplethysmography (PPG). Both of them rely on specific sensors to contact with skins of subjects, which may be uncomfortable or unsuitable for people with sensitive skins  [1] . In recent years, there is a trend to develop non-contact heart rate measurements through the microwave Doppler or computer vision techniques. The remote photoplethysmography (rPPG)  [2]  is a kind of computer vision based technique to record color changes of facial skins caused by corresponding heartbeats using consumer-level cameras. R. Song, H. Chen, J. Cheng, C. Li and Y. Liu are with the Department of Biomedical Engineering, Hefei University of Technology, Hefei 230009, China (e-mail: rcsong@hfut.edu.cn; 2018110057@mail.hfut.edu.cn; chengjuan@hfut.edu.cn; changli@hfut.edu.cn; yuliu@hfut.edu.cn ).\n\nX. Chen is with the Department of Electronic Engineering & Information Science, University of Science and Technology of China, Hefei 230026, China (e-mail: xunchen@ustc.edu.cn).\n\nAfter years of development, a variety of rPPG methods have been introduced according to different assumptions and mechanisms  [3] . For example, blind source separation (BSS)  [4]  based methods are proposed under some specific statistical assumption, while the model-based rPPG methods  [5] ,  [6]  are derived from a skin optical reflection model. However, the assumptions of conventional methods usually cannot be fully met in realistic situations, and the accuracy of pulse signal extraction is limited. This causes a difficulty to calculate reliable heart rate (HR) feature information, especially for features like heart rate variability (HRV) that require highquality waveforms. The conventional methods usually aim to calculate the average HR values by calculating the dominate frequency of pulse signals.\n\nHowever, there is a growing demand for more diverse cardiac features in rPPG applications, such as stress detection, emotional classification, and health monitoring, etc. For example, HRV is the variation of HR cycles. It is a valuable predictor of sudden cardiac death and arrhythmic events. The spectral component of HRV can also reflect the activities of the parasympathetic and sympathetic nervous systems. Currently, these diverse cardiac features can usually be obtained from high-quality pulse waveforms measured by contact ECG or PPG. Therefore, it is urgent to develop rPPG technology which can extract accurate pulse waveform for calculating more physiological characteristics.\n\nOn the other hand, inspired by the rapid development of deep learning (DL) techniques, DL-based rPPG algorithms  [7] -  [13]  have also been proposed in recent years. The rPPG approaches based on DL can be generally divided into two types, the end-to-end type and the feature-decoder type. The former ones directly establish the mapping from video frames to the target HR values or pulse signals, while the latter ones get the HR targets through decoding the latent information preprocessed from video frames. Since DL is data-driven and neural networks have strong fitting capabilities, the results of DL-based rPPG methods often outperform the conventional ones, which inspires us to extract rPPG pulse waveforms under a DL framework.\n\nThe extraction of rPPG pulse waveform can be considered as a generative problem from the perspective of generative models. Since firstly proposed by Ian in 2014, generative adversarial networks (GAN)  [14]  has become the mainstream generative method due to its state-of-the-art performance, especially in image processing and computer vision areas. The GAN is consisted of two neural networks, the generator G and the discriminator D. The two networks are trained in an adversarial way, where G generates a fake target signal to confuse the discriminator, and D makes judgments on the generated signals from the real ones, thereby prompting the results of G to be closer to the references. With the rapid development of GAN, it has also been applied to denoise one-dimensional signals, such as speech signals  [15] ,  [16] , and ECG signals  [17] . These studies enlighten us to acquire reliable rPPG waveforms using GAN models.\n\nIn this paper, we propose a new framework, named as PulseGAN, to extract rPPG pulse signal with a conditional GAN (cGAN)  [18] . The rough pulse signal derived from CHROM method  [5]  is taken as the input of generator G, and the PPG signal synchronously recorded by a pulse oximeter is used as a reference. The discriminator D judges the generated signal from the reference one, where the rough input of G is taken as a conditioning. Considering the apparent characteristics of pulse signal, besides the adversarial loss, we also combine the waveform error loss in the time domain and the spectrum error loss in the frequency domain to enforce a match between the generated waveform and its reference. Through the adversarial training between G and D, the generator learns to construct a rPPG pulse as close as its ground truth. The proposed method is tested on a public UBFC-RPPG database  [19]  in two scenarios, including both within-and cross-database cases. The test results reveal that the PulseGAN effectively improves the accuracy of the HR, the HRV and the interbeat interval (IBI) indexes.\n\nIn summary, the main contribution of this paper is that we introduce a PulseGAN framework to extract realistic rPPG pulse waveforms from rough input signals derived by some conventional method. The high-quality waveform makes it possible to further calculate reliable cardiac features like HRV, which can potentially expand the application scopes of rPPG techniques. The framework effectively combines the benefits of conventional methods and GAN. The generator is enforced to learn features of reference PPG signals through error losses defined in both time and spectrum domains in addition to the adversarial loss. The PulseGAN framework and related loss functions can also be easily integrated by some other DLbased rPPG methods to further improve their performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In 2008, Verkruysse et al.  [2]  first verified the validity of rPPG for HR estimation from facial videos. They demonstrated that the green channel signal extracted from skin pixels contained strong pulsating information. Since then, a variety of rPPG methods have been proposed. Among them, the typical ones include those methods based on blind source separation (BSS) or the skin optical reflection model. The BSS method assumes that the pulse signal is linearly mixed with other noise signals, and all those signals satisfy some statistical property. For example, Poh et al.  [4]  applied independent component analysis (ICA) to separate the pulse signals from the color RGB signals.  Wei et al. [20]  employed the secondorder blind source separation to extract the target signal from six RGB channels obtained in two facial regions of interest (ROIs). On the other hand, the methods based on the optical reflection model extract pulse signal explicitly through a combination of individual color channels are combined with specific ratios. This is considered to eliminate the common interference sources from the RGB channels. For example, De Haan et al.  [5]  proposed a chrominance method (CHROM) to calculate the pulse signal. The CHROM method eliminates the specular reflection component with a projection and then obtains the pulse through an \"alpha tuning\". In  [6] , Wang et al. used a different projection plane orthogonal to skin color (POS) for rPPG signal extraction. These conventional methods have achieved excellent results in calculating the average HR values of rPPG, during both laboratory and realistic scenarios. However, the quality of the waveforms remains poor due to noise interference and model limitations, which still has large room for improvement.\n\nIn the last few years, DL techniques have been increasingly used in rPPG extraction. Here we list some typical methods. In 2018, Chen et al.  [7]  introduced an end-to-end system to obtain HR and respiration rate. A convolutional neural network (CNN) combined with an attention mechanism was designed to establish the mapping between video frames and the desired physiological information. In the same year, Špetlłk et al.  [8]  put forward a two-step CNN composed by a feature extractor and an HR estimator to estimate the HR from a series of facial images. Niu et al.  [9]  proposed a spatiotemporal representation of HR information and designed a general-tospecial transfer learning strategy to estimate HR from the representation. Later, the authors also applied a channel and spatial-temporal attention mechanism to further improve the HR estimation from face videos  [10] . Song et al.  [21]  designed a feature-decoder framework to map a novel spatiotemporal map to the corresponding HR value through a CNN. They also took a transfer learning to reduce the demand of training data and accelerate the convergence of model. The goal of above DL-based rPPG methods is to determine accurate HR values. There are also some DL methods that can directly generate pulse waveforms. For example, Bian et al.  [12]  proposed a new regression model that used a twolayer long short-term memory (LSTM) to filter the noisy rPPG signals. Slapničar et al.  [13]  also employed a LSTM model to enhance the rough rPPG signals obtained by the POS algorithm. In  [11] , Yu et al. introduced an end-to-end way to extract pulse signal with deep spatial-temporal convolutional networks from the original face sequences. Particularly, the authors also calculate the HRV features to evaluate the quality of pulse waveforms. Although these articles have made significant progresses in extracting waveforms, there is still room for further improvement. This paper aims to introduce a new framework to generate pulse waveforms with cGAN. We will verify that the proposed PulseGAN framework improves the quality of waveforms much better than that of using the generator network with only a waveform loss. By this means, the proposed PulseGAN framework can also be integrated into some existing methods to further improve their performance of generating pulse waveforms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Face Detection & Roi Selection",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Signal",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, we introduce the details of the proposed PulseGAN framework for cardiac pulse extraction. The overall framework of PulseGAN is shown in Fig.  1 . First, 68-point facial landmarks  [22]  are detected and a region of interest (ROI) is defined according to those landmarks covering the left and right cheeks. Second, the pixels within the selected ROI are averaged to get the RGB channels, and the CHROM algorithm is used to obtain a rough pulse signal that will be taken as the input of PulseGAN. Finally, a high-quality pulse waveform is obtained through denoising the rough CHROM signal with the PulseGAN.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Acquisition Of Rough Rppg Pulses",
      "text": "A rough rPPG pulse signal is obtained with some conventional method before feeding into the PulseGAN. It can significantly simplify the training difficulty of PulseGAN if the rough rPPG pulse is close enough to its reference one. In this paper, the CHROM  [5]  proposed by De Haan et al. is taken to extract the rough rPPG pulse signal. Theoretically, other conventional methods can also be used. We choose the CHROM method because it is fast and stable against motion artifacts.\n\nThe principle of CHROM is based on the skin optical reflection model  [6] . The chrominance signals S 1 and S 2 are defined based on a projection of standardized RGB signals to remove the specular reflection terms. The rough pulse signal X is then calculated through an alpha tuning technique as",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. The Pulsegan Framework",
      "text": "The overall structure of the PulseGAN is as shown in Fig.  2 . The PulseGAN is composed of a generator G and a discriminator D. The generator G is taken to map the rough CHROM signal X to a target rPPG signal G(X) that is close to the reference PPG signal X c . The discriminator D is used to distinguish the ground truth X c from the signals G(X). To better pair the inputs and outputs, we refer to the conditional GAN  [18]  approach, where the input X is set as a condition in the discriminator. Therefore, the input of the discriminator is composed of two channels as (G(X), X) or (X c , X). The discriminator D outputs a lower score for the input (G(X), X), while it gives a higher score for the input (X c , X). The characteristics of the PPG signal are continuously learned through an adversarial learning between the generator and the discriminator, so that the output signal has a distribution as close as that of the reference PPG signal.\n\nThe network structures of PulseGAN are designed with reference to SEGAN  [15] . The generator, as shown in Fig.  3(a) , is similar as a denoising autoencoder with several skip connections. As seen, both the encoder and the decoder have six hidden layers, which are less than the ones in SEGAN. Besides, we also remove the latent vector z in SEGAN. These modifications can reduce the risk of overfitting in generating the rPPG waveforms. In detail, the encoder is composed of six one-dimensional convolution layers, while the decoder has six deconvolution layers. The parametric rectified linear units (PReLUs) and Tanh are taken as the nonlinear activation functions. The skip connections are taken to transfer finegrained features from the encoder to its counterpart in the decoder. This is important for the generator to construct highquality waveforms.\n\nThe discriminator is also a stack of several 1D convolutional layers together with a fully connected layer in the last layer as shown in Fig.  3(b ). The LeakyReLU is chosen as the nonlinear activation function and batch normalization is employed to accelerate the convergence. The input of D has two channels, where the CHROM signal X is used as a condition. The discriminator makes judgments on the generated waveform (G(X), X) and its reference one (X c , X), respectively. The output value of D represents the probability that the discriminator considers the input to be real data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Loss Function",
      "text": "The purpose of PulseGAN is to generate a waveform G(X) from its input X. G(X) is expected to be as close as its reference signal X c . This is achieved through training the PulseGAN with a lot of paired data. Since the pulse signal has a clear time-domain and frequency-domain characteristics, we define error losses in both domains to better guide the generator to learn the features of the reference signal. Therefore, we define the loss function of the generator and discriminator as follows:\n\nand\n\nThe first term of L G is an adversarial loss similar as the least square GAN (LSGAN)  [23] , the second and third ones are the waveform loss and the spectrum loss defined in time domain and frequency domain, respectively. The loss function of discriminator remains the same as the LSGAN. It enforces D to distinguish the generated and the reference signals. Here the G f (X) and X cf in the spectrum loss are calculated by a 1024-point fast Fourier transform (FFT) on G(X) and X c , respectively. And • 1 indicates the L 1 norm. The λ and β are the weights of the waveform loss and the spectrum loss, respectively. The generator is enforced to learn the time-frequency characteristics through minimizing the error losses. Therefore, the quality of generated waveforms can be effectively improved.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we will evaluate the proposed PulseGAN on public databases to illustrate its effectiveness. The PulseGAN will be compared with several conventional methods  [2] ,  [4] -  [6]  as well as the denoising autoencoder (DAE). The conventional methods in Table  I  have been implemented with an open source toolbox  [24] . The DAE here refers to use the generator G of PulseGAN with only a waveform error loss. The quality of generated waveforms, indexed as averaged HR, the HRV, and the IBI, will be compared under both withindatabase and cross-database cases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "To ensure the consistency of the reference waveform data from different databases, we choose to evaluate our approach with publicly available databases using the same PPG acquisition device. This can avoid or minimize potential data errors due to differences in the PPG reference waveforms. Under this condition, three databases are selected in our experiment including the UBFC-RPPG  [19] , the PURE  [25] , and the inhouse BSIPL-RPPG databases. They all acquire the reference PPG signals by the Contec CMS50E pulse oximeter. The HR distribution of each database is shown in Fig.  4 . As can be seen, the UBFC-RPPG database has a wide range of HR distribution compared to that of the other two. The PURE database has a HR distribution mainly concentrating at both ends, whereas the BSIPL-RPPG database has a HR distribution in the vicinity of around 80 bpm.\n\nThe proposed method is tested on the UBFC-RPPG database under two scenarios, within-database and cross-database. In order to balance the HR distribution in training and testing sets, the PURE and BSIPL-RPPG databases are combined as training set for the cross-database case. We use a 10-second sliding window to process all videos and PPG signals for both scenarios. However, the sliding step in the within-database case is taken as 0.5 seconds, while 1 second is used for the cross-database case. A smaller sliding step can help to increase the number of training samples for the within-database case. All reference PPG signals are resampled to be aligned with the video frame rate.\n\nWe train the proposed PulseGAN for 30 epochs using the Adam optimizer. The initial learning rate is set to 0.001, and it is adaptively adjusted through a dynamic learning rate scheduler, the 'ReduceLROnPlateau' with the factor to 0.1 and patience to 3. The weight parameters α and β in Eq. (  1 ) are both taken as 10 to balance the waveform and spectrum losses. The batch size is set to 4 in the within-database scenario, and set to 8 for the cross-database case.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Databases",
      "text": "The UBFC-RPPG  [19]  database includes 42 videos under a realistic situation. The subjects were asked to play a timesensitive mathematical game in order to keep the HR varied. The videos were recorded by a webcam (Logitech C920 HD Pro) with a spatial resolution of 640 × 480 pixels and a frame rate of 30 fps. Each video is about 2 minutes long, and the The PURE  [25]  database contains 60 videos from 10 subjects (8 male and 2 female). Each subject performed six different kinds of head motions, including steady, talking, slow translation, fast translation, small rotation, and medium rotation. Each video is about 1 minute long and recorded by an ECO274CVGE camera with a resolution of 640 × 480 pixels and a frame rate of 30 fps. The PPG pulse signals are also collected by the Contec CMS50E pulse oximeter while recording each video.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ubfc-Rppg",
      "text": "The BSIPL-RPPG is an in-house rPPG database including 37 healthy student subjects (24 male and 13 female with age ranging from 18 to 25 years old). The experimental setup is illustrated in Fig.  5 . The subjects were asked to sit in front of the camera (Logitech C920 pro HD) at a distance of 1.0 meter. A Contec CMS50E pulse oximeter was clamped on the subject's finger to acquire the PPG signal synchronously. Both the camera and the pulse oximeter were connected to a computer to transfer the acquired data in real time. The videos were recorded with a resolution of 640 × 480 pixels under a frame rate of 30 fps. Meanwhile, the PPG signal was collected by the pulse oximeter at a 60 Hz sampling rate. Each video and its counterpart PPG signal last about 4.5 minutes long. The subjects were requested to sit still for the first 2 minutes, and perform some apparent head movements for the last 2.5 minutes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Metrics",
      "text": "We define several metrics to evaluate the quality of the generated pulse waveform. First, the IBI sequences are calculated separately for the generated and reference pulse signals. A series of cardiac features can then be defined according to the calculated IBI. For example, the average HR can be calculated from IBI as  [26]  HR = 60/IBI.\n\n(\n\nwhere IBI is the average value of the IBI sequence for the current processing window. Similarly, we can also get HRV features  [27]  of AVNN and SDNN as follows,\n\nand\n\nwhere AVNN indicates the average of all normal-to-normal (NN) intervals, SDNN is the standard deviation of all NN intervals, RR i represents the i-th R-R interval, and T is the total number of R-R intervals.\n\nFinally, we define the following error metrics to compare the HR, HRV (AVNN and SDNN), and IBI calculated from the PulseGAN and the reference signals.\n\n1) HR: The metrics of HR values include the mean absolute error HR mae , the root mean square error HR rmse , the mean error rate percentage HR mer , and the Pearsons correlation coefficient r. The formulas of these metrics refer to  [21] . 2) HRV: The mean absolute error of AVNN (or SDNN) is calculated as below:\n\nwhere Y n indicates the AVNN (or SDNN) for the nth window calculated from PulseGAN, Y ′ n is the AVNN (or SDNN) from its reference PPG signal, and N is the total number of time windows. 3) IBI: We also define metrics to evaluate the quality of IBI directly. Since the length of the IBI vectors may be different, we refer to a similar way in  [28]  to solve this issue. Namely, each IBI vector is expanded to the same length as the PPG signal. We pad the ith RR interval of the IBI sequence with values all equal to RR i . After the padding operation, we define the absolute error IBI (n) ae for the nth window as below\n\nwhere E refers to the mathematical expectation, IBI\n\npredict is the padded IBI vector of rPPG pulse, and\n\nlabel is the padded IBI vector of the ground truth. Finally, a mean absolute error for IBI vectors from all samples is calculated by\n\nwhere N is the total number of time windows.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Experimental Results",
      "text": "The experimental results are introduced following a sequence of within-database and cross-database configurations.\n\nWithin-database: We first perform the within-database testing on the UBFC-RPPG database. According to the time window and the sliding step configuration, we totally get 4234 samples, where we take the 3192 samples from the first 30 subjects as the training set, and the remaining 1042 samples from the last 12 subjects as the testing set.\n\nThe estimations of average HR are summarized in Table  I . It can be seen that the PulseGAN achieves the best performance. The DAE and PulseGAN can both improve the HR accuracy compared to the conventional methods. However, the PulseGAN outperforms the DAE through the use of the adversarial and spectrum losses. To further compare all the results, the Bland-Altman plots are shown in Fig.  6 . We can observe that the DAE has much better consistency with the ground truth compared to CHROM in Fig.  6(a) . The PulseGAN further reduces some large errors of DAE as demonstrated in Fig.  6(b ). The best consistency and smallest standard deviation of the PulseGAN results indicate that the proposed method achieves the most accurate and stable results of estimating average HR.\n\nMoreover, we compare the performance of CHROM, DAE and PulseGAN methods on two HRV indexes, the AVNN and SDNN. The results in Table  II  show that the PulseGAN also makes a clear improvement of HRV features in the withindatabase case. Compared to DAE, the PulseGAN improves both the AVNN mae and SDNN mae , which implies that the waveforms generated by PulseGAN have better qualities than those from DAE. We also present the statistical histograms of AVNN and SDNN errors in Fig.  7  seen that the errors of AVNN (also SDNN) by PulseGAN are more concentrated around zero than those of DAE. Similarly, the mean absolute errors of IBI vectors (i.e., IBI mae ) are listed in Table  III . The error distribution of the IBI ae for all samples is shown in Fig.  8(a) . The PulseGAN slightly improves the IBI mae compared to DAE under the withindatabase configuration. In order to observe the improvement of the waveform quality more intuitively, we demonstrate a sample of the pulse signal and corresponding IBI sequence, as shown in Fig.  9 . It can be seen that the waveform and IBI sequences of the example pulse signal are both significantly improved by DAE and PulseGAN compared to CHROM. The IBI ae errors of the example in Fig.  9 (a) are 112.44, 42.50, and 24.67 ms for CHROM, DAE, and PulseGAN, respectively. Cross-database: In the case of cross-database, we take the PURE and BSIPL-RPPG databases as the training set. This combination can effectively balance the number of samples in different HR ranges to achieve a more consistent HR distribution with the testing set. According to the configuration of cross-database scenario, there are total 13484 training samples obtained from the PURE (3727 samples) and BSIPL-RPPG (9757 samples) databases. Similarly, we get 1470 samples   from the UBFC-RPPG database as the testing set.\n\nThe average HR measurements are summarized in Table  IV . The proposed PulseGAN still achieves the best results compare to the other ones. Similarly, the Bland-Altman plots are illustrated in Fig.  10  to show the consistency of the predicted HR values with the reference ones. We can see that the DAE and PulseGAN both outperform the CHROM method. The PulseGAN achieves a even better performance compared to   II  for the cross-database case. The PulseGAN achieves the best performance, with the AVNN mae improves 20.85% (41.19%), and the SDNN mae improves 20.28% (37.53%), compared to DAE (CHROM), in the crossdatabase test. In addition, the error histogram and its distribution fitting curve are illustrated in Fig.  7(c ) and (d), respectively. The results show that the PulseGAN not only improves the accuracy of heart rate but also improves the quality of HRV features in the cross-database scenario. The mean absolute errors of IBI vectors (IBI mae ) are listed in    database and cross-database cases. Although the PulseGAN has not been compared with other existing deep learning-based rPPG methods, we want to emphasize that the framework of PulseGAN, including the usage of both time-domain and spectrum-domain losses under a GAN architecture, can be easily integrated with these existing methods to further improve their results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion",
      "text": "The cardiac pulse signal is very important to evaluate the healthy and emotional status of human bodies. In this paper, we have proposed a PulseGAN to extract high-quality pulse waveforms through remote photoplethysmogrpahy. The PulseGAN is designed based on a framework of generative adversarial network with error losses defined in both time and spectrum domains. It takes the rough CHROM signal as the input, and outputs a rPPG pulse through the deep generative model. This architecture is also easy to integrate with existing deep learning based rPPG methods and further improve their performance. The experimental results on a public UBFC-RPPG database demonstrate that the PulseGAN consistently outperforms the DAE and other conventional methods for both within-database and cross-database cases. The generated highquality waveforms from PulseGAN have made it possible to calculate more cardiac features like AVNN and SDNN through rPPG. Although the HRV characteristics calculated in this paper are relatively preliminary, these attempts are meaningful to expand the scope of application of rPPG.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of the proposed PulseGAN method.",
      "page": 3
    },
    {
      "caption": "Figure 2: The conditional GAN structure used in PulseGAN.",
      "page": 3
    },
    {
      "caption": "Figure 3: The network structure of PulseGAN. (a) The generator",
      "page": 4
    },
    {
      "caption": "Figure 4: The HR distributions of reference PPG pulses in BSIPL-RPPG, PURE and UBFC-RPPG databases, respectively.",
      "page": 5
    },
    {
      "caption": "Figure 5: The setup of the BSIPL-RPPG database.",
      "page": 5
    },
    {
      "caption": "Figure 6: Bland-Altman plots between the predicted HR",
      "page": 6
    },
    {
      "caption": "Figure 9: (a) are 112.44, 42.50, and",
      "page": 6
    },
    {
      "caption": "Figure 7: The statistical histograms of errors of AVNN (left) and SDNN (right). The (a) and (b) are for a within-database case.",
      "page": 7
    },
    {
      "caption": "Figure 8: The statistical histograms of absolute errors (IBIae) of IBI sequences: (a) within-database, (b) cross-database.",
      "page": 8
    },
    {
      "caption": "Figure 9: A comparison example of IBI sequences in (a) and",
      "page": 8
    },
    {
      "caption": "Figure 11: (a) are 65.01, 27.44, and 23.11 ms for CHROM,",
      "page": 8
    },
    {
      "caption": "Figure 10: Bland-Altman plots between the predicted HR",
      "page": 8
    },
    {
      "caption": "Figure 11: A comparison example of IBI sequences in (a) and",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Verkruysse et al.\n[2]",
          "HRmae\n(bpm)": "7.50",
          "HRrmse\n(bpm)": "14.41",
          "HRmer": "7.82%",
          "r": "0.62"
        },
        {
          "Method": "Poh et al.\n[4]",
          "HRmae\n(bpm)": "5.17",
          "HRrmse\n(bpm)": "11.76",
          "HRmer": "5.30%",
          "r": "0.65"
        },
        {
          "Method": "Wang et al.\n[6]",
          "HRmae\n(bpm)": "4.05",
          "HRrmse\n(bpm)": "8.75",
          "HRmer": "4.21%",
          "r": "0.78"
        },
        {
          "Method": "Haan et al.\n[5]",
          "HRmae\n(bpm)": "2.37",
          "HRrmse\n(bpm)": "4.91",
          "HRmer": "2.46%",
          "r": "0.89"
        },
        {
          "Method": "DAE",
          "HRmae\n(bpm)": "1.48",
          "HRrmse\n(bpm)": "2.49",
          "HRmer": "1.55%",
          "r": "0.97"
        },
        {
          "Method": "PulseGAN",
          "HRmae\n(bpm)": "1.19",
          "HRrmse\n(bpm)": "2.10",
          "HRmer": "1.24%",
          "r": "0.98"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CHROM\nDAE": "mean"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DAE\nPulseGAN": "mean"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HRVmae(ms)": "within-database\ncross-database"
        },
        {
          "HRVmae(ms)": "AVNNmae\nSDNNmae\nAVNNmae\nSDNNmae"
        },
        {
          "HRVmae(ms)": "16.54\n40.90\n25.30\n38.96"
        },
        {
          "HRVmae(ms)": "9.52\n19.25\n18.80\n30.53"
        },
        {
          "HRVmae(ms)": "7.52\n18.36\n14.88\n24.34"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Verkruysse et al.\n[2]",
          "HRmae\n(bpm)": "8.29",
          "HRrmse\n(bpm)": "15.82",
          "HRmer": "7.81%",
          "r": "0.68"
        },
        {
          "Method": "Poh et al.\n[4]",
          "HRmae\n(bpm)": "4.39",
          "HRrmse\n(bpm)": "11.60",
          "HRmer": "4.30%",
          "r": "0.82"
        },
        {
          "Method": "Wang et al.\n[6]",
          "HRmae\n(bpm)": "3.52",
          "HRrmse\n(bpm)": "8.38",
          "HRmer": "3.36%",
          "r": "0.90"
        },
        {
          "Method": "Haan et al.\n[5]",
          "HRmae\n(bpm)": "3.10",
          "HRrmse\n(bpm)": "6.84",
          "HRmer": "3.83%",
          "r": "0.93"
        },
        {
          "Method": "DAE",
          "HRmae\n(bpm)": "2.70",
          "HRrmse\n(bpm)": "5.17",
          "HRmer": "2.85%",
          "r": "0.96"
        },
        {
          "Method": "PulseGAN",
          "HRmae\n(bpm)": "2.09",
          "HRrmse\n(bpm)": "4.42",
          "HRmer": "2.23%",
          "r": "0.97"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:39)(cid:36)(cid:40)\nPulseGAN": "mean"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Photoplethysmography revisited: From contact to noncontact, from point to imaging",
      "authors": [
        "Y Sun",
        "N Thakor"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "2",
      "title": "Remote plethysmographic imaging using ambient light",
      "authors": [
        "W Verkruysse",
        "L Svaasand",
        "J Nelson"
      ],
      "year": "2008",
      "venue": "Optics express"
    },
    {
      "citation_id": "3",
      "title": "Videobased heart rate measurement: Recent advances and future prospects",
      "authors": [
        "X Chen",
        "J Cheng",
        "R Song",
        "Y Liu",
        "R Ward",
        "Z Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "4",
      "title": "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation",
      "authors": [
        "M.-Z Poh",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2010",
      "venue": "Optics Express"
    },
    {
      "citation_id": "5",
      "title": "Robust pulse rate from chrominance-based rPPG",
      "authors": [
        "G De Haan",
        "V Jeanne"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "6",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "W Wang",
        "A Brinker",
        "S Stuijk",
        "G De Haan"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "7",
      "title": "Deepphys: Video-based physiological measurement using convolutional attention networks",
      "authors": [
        "W Chen",
        "D Mcduff"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "8",
      "title": "Visual heart rate estimation with convolutional neural network",
      "authors": [
        "R Špetlík",
        "V Franc",
        "J Čech",
        "J Matas"
      ],
      "year": "2018",
      "venue": "Proceedings of British Machine Vision Conference 2018. British Machine Vision Association (BMVA)"
    },
    {
      "citation_id": "9",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "10",
      "title": "Robust remote heart rate estimation from face utilizing spatial-temporal attention",
      "authors": [
        "X Niu",
        "X Zhao",
        "H Han",
        "A Das",
        "A Dantcheva",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Recovering remote photoplethysmograph signal from facial videos using spatio-temporal convolutional networks",
      "authors": [
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Recovering remote photoplethysmograph signal from facial videos using spatio-temporal convolutional networks",
      "arxiv": "arXiv:1905.02419"
    },
    {
      "citation_id": "12",
      "title": "An accurate lstm based video heart rate estimation method",
      "authors": [
        "M Bian",
        "B Peng",
        "W Wang",
        "J Dong"
      ],
      "year": "2019",
      "venue": "An accurate lstm based video heart rate estimation method"
    },
    {
      "citation_id": "13",
      "title": "Contact-free monitoring of physiological parameters in people with profound intellectual and multiple disabilities",
      "authors": [
        "G Slapničar",
        "E Dovgan",
        "P Čuk",
        "M Luštrek"
      ],
      "year": "2019",
      "venue": "The IEEE International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "14",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "15",
      "title": "SEGAN: Speech enhancement generative adversarial network",
      "authors": [
        "S Pascual",
        "A Bonafonte",
        "J Serra"
      ],
      "year": "2017",
      "venue": "SEGAN: Speech enhancement generative adversarial network",
      "arxiv": "arXiv:1703.09452"
    },
    {
      "citation_id": "16",
      "title": "Speech enhancement via generative adversarial lstm networks",
      "authors": [
        "Y Xiang",
        "C Bao"
      ],
      "year": "2018",
      "venue": "2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)"
    },
    {
      "citation_id": "17",
      "title": "Adversarial de-noising of electrocardiogram",
      "authors": [
        "J Wang",
        "R Li",
        "R Li",
        "K Li",
        "H Zeng",
        "G Xie",
        "L Liu"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised skin tissue segmentation for remote photoplethysmography",
      "authors": [
        "S Bobbia",
        "R Macwan",
        "Y Benezeth",
        "A Mansouri",
        "J Dubois"
      ],
      "year": "2017",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "20",
      "title": "Non-contact, synchronous dynamic measurement of respiratory rate and heart rate based on dual sensitive regions",
      "authors": [
        "B Wei",
        "X He",
        "C Zhang",
        "X Wu"
      ],
      "year": "2017",
      "venue": "Biomedical engineering online"
    },
    {
      "citation_id": "21",
      "title": "Heart rate estimation from facial videos using a spatiotemporal representation with convolutional neural networks",
      "authors": [
        "R Song",
        "S Zhang",
        "C Li",
        "Y Zhang",
        "J Cheng",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "22",
      "title": "Facial landmark detection by deep multi-task learning",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2014",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "23",
      "title": "Least squares generative adversarial networks",
      "authors": [
        "X Mao",
        "Q Li",
        "H Xie",
        "R Lau",
        "Z Wang",
        "S Smolley"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "24",
      "title": "iphys: An open non-contact imaging-based physiological measurement toolbox",
      "authors": [
        "D Mcduff",
        "E Blackford"
      ],
      "year": "2019",
      "venue": "iphys: An open non-contact imaging-based physiological measurement toolbox",
      "arxiv": "arXiv:1901.04366"
    },
    {
      "citation_id": "25",
      "title": "Non-contact video-based pulse rate measurement on a mobile service robot",
      "authors": [
        "R Stricker",
        "S Mller",
        "H Gross"
      ],
      "year": "2014",
      "venue": "The 23rd IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "26",
      "title": "Advancements in noncontact, multiparameter physiological measurements using a webcam",
      "authors": [
        "M Poh",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "27",
      "title": "Heart rate variability: standards of measurement, physiological interpretation and clinical use. task force of the european society of cardiology and the north american society of pacing and electrophysiology",
      "authors": [
        "M Malik"
      ],
      "year": "1996",
      "venue": "Annals of Noninvasive Electrocardiology"
    },
    {
      "citation_id": "28",
      "title": "Detecting pulse wave from unstable facial videos recorded from consumer-level cameras: a disturbanceadaptive orthogonal matching pursuit",
      "authors": [
        "X Liu",
        "X Yang",
        "J Jin",
        "A Wong"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Biomedical Engineering"
    }
  ]
}