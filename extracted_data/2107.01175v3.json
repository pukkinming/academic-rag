{
  "paper_id": "2107.01175v3",
  "title": "Continuous Emotion Recognition With Audio-Visual Leader-Follower Attentive Fusion",
  "published": "2021-07-02T16:28:55Z",
  "authors": [
    "Su Zhang",
    "Yi Ding",
    "Ziquan Wei",
    "Cuntai Guan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose an audio-visual spatial-temporal deep neural network with: (1) a visual block containing a pretrained 2D-CNN followed by a temporal convolutional network (TCN); (2) an aural block containing several parallel TCNs; and (3) a leader-follower attentive fusion block combining the audio-visual information. The TCN with large history coverage enables our model to exploit spatialtemporal information within a much larger window length (i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36 or 48). The fusion block emphasizes the visual modality while exploits the noisy aural modality using the inter-modality attention mechanism. To make full use of the data and alleviate over-fitting, the cross-validation is carried out on the training and validation set. The concordance correlation coefficient (CCC) centering is used to merge the results from each fold. On the test (validation) set of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence and 0.492 (0.649) for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for valence and arousal, respectively. The code is available at https://github.com/sucv/ABAW2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is the process of identifying human emotion. It plays a crucial role for many human-computer interaction systems. To describe the human state of feeling, psychologists have developed the categorical and the dimensional  [1]  models. The categorical model is based on several basic emotions. It has been extensively exploited in affective computing largely due to its simplicity and universality. The dimensional model maps the emotion into a continuous space, where the valence and arousal are taken as the axes. It can describe more complex and subtle emotions. This paper focuses on developing a continuous emotion recognition method based on the dimensional model.\n\nContinuous emotion recognition seeks to automatically predict subject's emotional state in a temporally continuous manner. Given the subject's visual, aural, and physiological data which are temporally sequential and synchronous, the system aims to map all the information onto the dimensional space and produces the valence-arousal prediction. The latter is then evaluated against the expert annotated emotional trace using metrics such as the concordance correlation coefficient (CCC). A number of databases, including SEMAINE  [2] , RECOLA  [3] , MAHNOC-HCI  [4] , SEWA  [5] , and MuSe  [6]  have been built for this task. Depending on the subject's context, i.e., controlled or in-thewild environments, and induced or spontaneous behaviors, the task can be quite challenging due to varied noise levels, illumination, and camera calibration, etc. Recently, Kollias et al.  [7] [8] [9] [10] [11] [12] [13]  build the Aff-Wild2 database, which is by far the largest available in-the-wild database for continuous emotion recognition. The Affective Behavior Analysis inthe-wild (ABAW) competition  [14]  is later hosted using the Aff-Wild2 database.\n\nThis paper investigates one question, i.e., how to appropriately combine features of different modalities to achieve an \"1 + 1 > 2\" performance. Facial expression is one of the most powerful, natural, and universal signals for human beings to convey or regulate emotional states and intentions  [15, 16] . And, voice also serves as a key cue for both the emotion production and emotion perception  [17] . Though we human are good at recognizing emotion from multi-modal information, a straightforward feature concatenation may deteriorate an AI emotion recognition system. In a scene where one subject is watching a video or attending a talk show, more than one voice sources can exist, e.g., from the subject him/herself, the video, the anchor, and the audience. It is not trivial to design an appropriate fusion scheme so that the subject's visual information and the complementary aural information are captured.\n\nWe propose an audio-visual spatial-temporal deep neural network with an attentive feature fusion scheme for continuous valence-arousal emotion recognition. The network consists of three branches, fed by facial images, mfcc, and VG-Gish  [18]  features. A Resnet50 followed by a temporal convolutional network (TCN)  [19]  is used to extract the spatialtemporal visual feature of the facial images. Two TCNs are employed to extract the spatial-temporal aural feature from the mfcc and VGGish features. The three branches work in parallel and their outputs are sent to the attentive fusion block. To emphasize the dominance of the visual features, which we believe to have the strongest correlation with the label, a leader-follower strategy is designed. The visual feature plays as the leader and owns a skip connection down to the block's output. The mfcc and VGGish features play as the followers, together with the visual feature they are weighted by an attention score. Finally, the leading visual feature and the weighted attention feature are concatenated and a fully-connected layer is used for the regression. To alleviate over-fitting and exploit the available data, a 6-fold cross-validation is carried out on the combined training and validation set of the Aff-Wild2 database. For each emotion dimension, the final inference is determined by the CCCcentering across the 6 trained models  [20] .\n\nThe remainder of the paper is arranged as follows. Section 2 discusses the related works. Section 3 details the model architecture including the visual, aural, and attentive fusion blocks. Section 4 elaborates the implementation de-tails including the data pre-processing, training settings, and post-processing. Section 5 provides the continuous emotion recognition results on the Aff-Wild2 database. Section 6 concludes the work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Video-Based Emotion Recognition Methods",
      "text": "Along with the thriving deep CNN-based methods comes two fundamental frameworks of neural networks for video-based emotion recognition. The first type possesses a cascade spatial-temporal architecture. The convolutional neural networks (CNN) are used to extract spatial information, from which the temporal information is obtained by using temporal models such as Time-delay, recurrent neural networks (RNN), or long short-term memory networks (LSTM). The second type combines the two separated steps into one and extracts the spatial-temporal feature using 3D-CNN. Our model belongs to the first type.\n\nTwo issues hinder the performance of the 3D-CNN based emotion recognition methods. First, they have considerably more parameters than 2D-CNN due to extra kernel dimension. Hence, it is more difficult to train. Second, employing 3D-CNN means to preclude the benefits of large-scale 2D facial image databases (such as MS-CELEB-1M  [21]  and VGGFace2  [22] ). 3D-based emotion recognition databases  [23] [24] [25]  are much fewer. They are mostly based on posed behavior with limited subjects, diversity, and labels.\n\nIn this paper, we employ the cascade CNN-TCN architecture. Systematical comparison  [19]  demonstrated that TCNs convincingly outperform recurrent architectures across a broad range of sequence modeling tasks. With the dilated and casual convolutional kernel and stacked residual blocks, the TCN is capable of looking very far into the past to make a prediction  [19] . Compared to many other methods which utilize smaller window length, e.g., with a sequence length of 70 for AffWildNet  [26] , or 32 for the ABAW2020 VA-track champion  [27] , ours of length 300 has achieved promising improvement on the Aff-Wild2 database.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Architecture",
      "text": "The model architecture is illustrated in Fig.  1 . In this section, we detail the proposed audio-visual spatialtemporal model and the leader-follower attentive fusion scheme.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Block",
      "text": "The visual block consists of a Resnet50 and a TCN. The resnet50 is pre-trained on the MS-CELEB-1M dataset  [21]  as a facial recognition task, it is then fine-tuned on the FER+  [28]  dataset. The Resnet50 is used to extract the independent per-frame features of the video frame sequence, producing the 512-D spatial encodings. The latter is then stacked and fed to a TCN, generating the 128-D spatialtemporal visual features. The TCN utilizes 128 × 4 as the channel × layer setting with a kernel size of 5 and dropout of 0.1. Finally, a fully connected layer is employed to map the extracted features onto a 1-D sequence. Following the labeling scheme of the Aff-Wild2 database where the label frequency equals the video frame rate, each frame of the input video sequence is exactly corresponding to one label point.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Aural Block",
      "text": "The aural block consists of two parallel branches. The 39-D mfcc and 128-D VGGish  [18]  features are the inputs, respectively. The mfcc feature is extracted using the OpenSmile Toolkit  [29] , and the VGGish feature is obtained from the pre-trained VGGish model  [22] . They are fed to two independent TCNs and yield two 32-D spatial-temporal aural features. The TCN utilizes 32 × 4 as the channel × layer setting with the same kernel size and dropout as in the visual block.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Leader-Follower Attention Block",
      "text": "The motivation is two-fold. First, we believe that the representability of the multi-modal information is superior to the unimodal counterpart. In addition to the expressive visual information, the voice usually makes us resonate with the emotional context. Second, a direct feature concatenation may deteriorate the performance due to the noisy aural information. Voice separation is still a topic of research. When multiple voice sources exist, it is difficult to obtain the voice components relevant to a specific subject.\n\nThe block first maps the feature vectors to query, key, and value vectors by the following procedure. For the ith branch, its encoder consists of three independent linear layers, they adjust the dimension of feature vector producing a query Q i , a key K i , and a value V i . They are then regrouped and concatenated to form the crossmodal counterparts. For example, the cross-modal query\n\nAfter which, the attention feature is calculated as:\n\nwhere d K = 32 is the dimension of the key K. After which, the Attention is normalized and concatenated to the leader feature (i.e., the spatial-temporal visual feature in our case) producing the 224-D leader-follower attention feature. Finally, a fully connected layer is used to yield the inference. Note that the unimodal version of our model has only the visual block. The inference is obtained based on the 128-D spatial-temporal visual feature.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "Our work is based on Aff-Wild2 database. It consists of 548 videos collected from YouTube. All the video are captured in-the-wild. 545 out of 548 videos contain annotations in terms of valence-arousal. The annotations are provided by four experts using a joystick  [30] . The resulted valence and arousal values range continuously in [-1, 1]. The final label values are the average of the four raters. The database is split into the training, validation and test sets. The partitioning is done in a subject independent manner, so that every subject's data will present in only one subset. The partitioning produces 346, 68, and 131 videos for the training, validation, and test sets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing",
      "text": "The visual preprocessing is carried out as follows. The cropped-aligned image data provided by the ABAW2021 challenge are used. All the images are resized to 48×48×3. Given a trial, the length N is determined by the number of the rows which does not include -5. A zero matrix B of size N × 48 × 48 × 3 is initialized and then iterated over the rows. For the i-th row of B, it is assigned as the i-th jpg image if it exists, otherwise doing nothing. After which, the Table  1 . The validation result in CCC using 6-fold cross validation from the unimodal and multi-moda models. The 6-fold cross-validation is used for data expansion and over-fitting prevention, in which the fold 0 is exactly the original data partitioning provided by ABAW2021. The aural preprocessing firstly converts all the videos to mono with a 16K sampling rate in wav format. The synchronous mfcc and VGGish features are then extracted, respectively. For the mfcc feature, it is extracted using the OpenSmile Toolbox. The settings are the same as provided by AVEC2019 challenge  [31]  1 . Since the window and hop lengths of the short-term Fourier transform are fixed to 25ms and 10ms, respectively, the mfcc feature has a fixed frequency of 100 Hz over all trials. Given the varied labeling frequency and the fixed mfcc frequency, the synchronicity is achieved by pairing the i-th label point with the closest-in-time-stamp mfcc feature point. For example, given a label sequence in 30 Hz, the {0, 1, 2, 3}-rd label points sampled at {0, 0.033, 0.067, 0.100} seconds are paired with the {0, 3, 7, 10}-th feature points sampled at {0, 0.03, 0.07, 0.10} seconds. For all the label rows that do not contain -5, the paired mfcc feature points are selected in sequential to form the feature matrix.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotional Dimension",
      "text": "For the VGGish feature, it is extracted using the pretrained VGGish model 2 . First, the log-mel spectrogram is extracted and synchronized with the label points using the same operation above. The log-mel spectrogram matrix is then fed into the pre-trained model to extract the synchronized VGGish features. To ensure that the aural features and the labels have the same length, the feature matrices are repeatedly padded using the last feature points. The aural features are finally saved in npy format.\n\nFor the valence-arousal labels, all the rows containing -5 are excluded. The labels are then saved in npy format.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Expansion",
      "text": "The AffWild2 database contains 351 and 71 trials in the training and validation sets, respectively. To clarify, a la-1 https://github.com/AudioVisualEmotionChallenge/ AVEC2019/blob/master/Baseline_features_ extraction/Low-Level-Descriptors/extract_audio_ LLDs.py 2 https://github.com/tensorflow/models/tree/ master/research/audioset/vggish bel txt file and its corresponding data are taken as a trial. Note that some videos include two subjects, resulting in two separated cropped-aligned image folders and label txt files, with different suffixes. They are each taken as two trials.\n\nTo make full use of the available data and alleviate overfitting, the cross-validation is employed. By evenly splitting the training set into 5 folds, we have 6 folds in total with a roughly equal trial amount, i.e., 70×4+71+71 trials. Note that the 0-th fold is exactly the original data partitioning. And there is no subject overlap across different folds. The CCC-centering is employed to merge the inference result on the test set.\n\nMoreover, during training and validation, the resampling window has a 33% overlap, resulting in 33% more data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "Since we employ the 6-fold cross-validation on 2 emotional dimensions using 2 models, we have 24 training instances to run, each takes about 10Gb VRAM and 1 to 2 days. Multiple GPU cards including Nvidia Titan V and Tesla V100 from various servers are used. To maintain the reproducibility, the same Singularity container is shared over the 24 instances. The code is implemented using Pytorch.\n\nThe batch size is 2. For each batch, the resampling window length and hop length are 300 and 200, respectively. I.e., the dataloader loads consecutive 300 feature points to form a minibatch, with a stride of 200. For any trials having feature points smaller than the window length, zero padding is employed. For visual data, the random flip, random crop with a size of 40 are employed for training and only the center crop is employed for validation. The data are then normalized to have 0.5 mean and standard deviation. For aural data, the data is normalized to have 0 mean and unit standard deviation.\n\nThe CCC loss is used as the loss function. The Adam optimizer with a weight decay of 0.0001 is employed. The learning rate (LR) and minimal learning rate (MLR) are set to 1e -5 and 1e -6, respectively. The ReduceLROnPlateau scheduler with a patience of 5 and factor of 0.1 is employed based on the validation CCC. The maximal epoch number and early stopping counter are set to 100 and 20, respectively. Two groups of layers for the Resnet50 backbone are manually selected for further fine-tuning, which corresponds to the whole layer4 and the last three blocks of layer3.\n\nThe training strategy is as follows. The Resnet50 backbone is initially fixed except the output layer.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Post-Processing",
      "text": "The post-processing consists of CCC-centering and clipping. Given the predictions from 6-fold cross-validation, the CCC-centering aims to yield the weighted prediction based on the inter-class correlation coefficient (ICC)  [31] . This technique has been widely used in many emotion recognition challenges  [31] [32] [33] [34]  to obtain the gold-standard labels from multiple raters, by which the bias and inconsistency among individual raters are compensated. The clipping ensures that the inference is truncated within the interval [-1, 1], i,e., any values larger or smaller than 1 or -1 are set to 1 or -1. respectively.\n\nIn this work, two strategies, i.e., clipping-then-CCCcentering and CCC-centering-then-clipping are utilized. They are called early-clipping and late-clipping.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Result",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Validation Result",
      "text": "The validation results of the 6-fold cross-validation on valence and arousal are reported in Table  1 . Two types of models are trained. The unimodal model is fed by video frame only, and the multimodal model is fed by video frame, mfcc, and VGGish features.\n\nFor fold 0, namely the original data partitioning, both of our unimodal and multimodal models have significantly outperformed the baseline. For other five folds, interestingly, the multimodal information has positive and negative effects on the arousal and valence dimensions, respectively, as shown in Figure  2 . We therefore hypothesize that the annotation protocol weighs more on aural perspective for the arousal dimension.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Test Result",
      "text": "The comparison of our model against the baseline and state-of-the-art methods on the test set are shown in Table  2 . First and foremost, the multimodal model achieves great improvement against the unimodal counterpart, which is up to 70.41% and 58.42% gain (by comparing UM against MM in table 2) over the valence and arousal, respectively. The employment of cross-validation provides incremental improvement on the multimodal result.\n\nWe can also see that the three unimodal scenarios and multimodal scenarios have a sharp performance gap, whereas on the validation set the gap is incremental. We hypothesize that the unimodal models, fed only by visual information, suffer from over-fitting and insufficient robustness on the test set. The issue is alleviated by the fusion with aural information. Further investigation will be carried out in our future work using other audio-visual databases where labels of the test set are available.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed an audio-visual spatial-temporal deep neural network with an attentive feature fusion scheme for continuous valence-arousal emotion recognition. The model consists of a visual block, an aural block, and a leaderfollower attentive fusion block. The latter achieves the cross-modality fusion by emphasizing the leading visual modality while exploiting the noisy aural modality. Experiments are conducted on the Aff-Wild2 database and promising results are achieved. The achieved CCC on test (validation) set is 0.463 (0.469) for valence and 0.492 (0.649) for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for valence and arousal, respectively.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our audio-visual spatial-temporal model. The model consists of three components, i.e., the visual, aural, and",
      "page": 2
    },
    {
      "caption": "Figure 2: The 6-fold validation result in CCC obtained by our uni-",
      "page": 5
    },
    {
      "caption": "Figure 2: We therefore hypothesize that the an-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Two types of",
      "data": [
        {
          "Method": "Baseline",
          "Valence": "0.200",
          "Arousal": "0.190",
          "Mean": "0.195"
        },
        {
          "Method": "ICT-VIPL-VA [35]",
          "Valence": "0.361",
          "Arousal": "0.408",
          "Mean": "0.385"
        },
        {
          "Method": "NISL2020 [27]",
          "Valence": "0.440",
          "Arousal": "0.454",
          "Mean": "0.447"
        },
        {
          "Method": "NISL2021 [36]",
          "Valence": "0.533",
          "Arousal": "0.454",
          "Mean": "0.494"
        },
        {
          "Method": "Netease Fuxi Virtual\nHuman [37]",
          "Valence": "0.486",
          "Arousal": "0.495",
          "Mean": "0.491"
        },
        {
          "Method": "Morphoboid [38]",
          "Valence": "0.505",
          "Arousal": "0.475",
          "Mean": "0.490"
        },
        {
          "Method": "STAR [39]",
          "Valence": "0.478",
          "Arousal": "0.498",
          "Mean": "0.488"
        },
        {
          "Method": "UM",
          "Valence": "0.267",
          "Arousal": "0.303",
          "Mean": "0.285"
        },
        {
          "Method": "UM-CV-EC",
          "Valence": "0.264",
          "Arousal": "0.276",
          "Mean": "0.270"
        },
        {
          "Method": "UM-CV-LC",
          "Valence": "0.265",
          "Arousal": "0.276",
          "Mean": "0.271"
        },
        {
          "Method": "MM",
          "Valence": "0.455",
          "Arousal": "0.480",
          "Mean": "0.468"
        },
        {
          "Method": "MM-CV-EC",
          "Valence": "0.462",
          "Arousal": "0.492",
          "Mean": "0.477"
        },
        {
          "Method": "MM-CV-LC",
          "Valence": "0.463",
          "Arousal": "0.492",
          "Mean": "0.478"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Static and dynamic 3d facial expression recognition: A comprehensive survey",
      "authors": [
        "G Sandbach",
        "S Zafeiriou",
        "M Pantic",
        "L Yin"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "2",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "4",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "6",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Schumann",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "arxiv": "arXiv:2101.06053"
    },
    {
      "citation_id": "7",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "8",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond"
    },
    {
      "citation_id": "9",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "10",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "11",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "Analysing affective behavior in the first abaw 2020 competition"
    },
    {
      "citation_id": "12",
      "title": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "13",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "14",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "I Kotsia",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition"
    },
    {
      "citation_id": "15",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "year": "2015",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "16",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y.-I Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "17",
      "title": "Component models of emotion can inform the quest for emotional competence. u: G. mathews, m. zeidner, rd roberts (ur.)",
      "authors": [
        "K Scherer"
      ],
      "year": "2007",
      "venue": "Component models of emotion can inform the quest for emotional competence. u: G. mathews, m. zeidner, rd roberts (ur.)"
    },
    {
      "citation_id": "18",
      "title": "Cnn architectures for large-scale audio classification,\" in 2017 ieee international conference on acoustics, speech and signal processing (icassp)",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "Cnn architectures for large-scale audio classification,\" in 2017 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "19",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "20",
      "title": "Intraclass correlations: uses in assessing rater reliability",
      "authors": [
        "P Shrout",
        "J Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "21",
      "title": "Msceleb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "22",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "23",
      "title": "A 3-d audio-visual corpus of affective communication",
      "authors": [
        "G Fanelli",
        "J Gall",
        "H Romsdorfer",
        "T Weise",
        "L Van Gool"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "A 3d facial expression database for facial behavior research",
      "authors": [
        "L Yin",
        "X Wei",
        "Y Sun",
        "J Wang",
        "M Rosato"
      ],
      "year": "2006",
      "venue": "7th international conference on automatic face and gesture recognition (FGR06)"
    },
    {
      "citation_id": "25",
      "title": "Bp4dspontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "26",
      "title": "Aff-wild database and affwildnet",
      "authors": [
        "M Liu",
        "D Kollias"
      ],
      "year": "2019",
      "venue": "Aff-wild database and affwildnet",
      "arxiv": "arXiv:1910.05318"
    },
    {
      "citation_id": "27",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "28",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "29",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "31",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "32",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international work"
    },
    {
      "citation_id": "33",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "34",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "35",
      "title": "M 3 t: Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Y.-H Zhang",
        "R Huang",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "M 3 t: Multi-modal continuous valence-arousal estimation in the wild",
      "arxiv": "arXiv:2002.02957"
    },
    {
      "citation_id": "36",
      "title": "Towards better uncertainty: Iterative training of efficient networks for multitask emotion recognition",
      "authors": [
        "D Deng",
        "L Wu",
        "B Shi"
      ],
      "year": "2021",
      "venue": "Towards better uncertainty: Iterative training of efficient networks for multitask emotion recognition",
      "arxiv": "arXiv:2108.04228"
    },
    {
      "citation_id": "37",
      "title": "Prior aided streaming network for multitask affective recognitionat the 2nd abaw2 competition",
      "authors": [
        "W Zhang",
        "Z Guo",
        "K Chen",
        "L Li",
        "Z Zhang",
        "Y Ding"
      ],
      "year": "2021",
      "venue": "Prior aided streaming network for multitask affective recognitionat the 2nd abaw2 competition",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "38",
      "title": "Multitask multi-database emotion recognition",
      "authors": [
        "M Vu",
        "M Beurton-Aimar"
      ],
      "year": "2021",
      "venue": "Multitask multi-database emotion recognition",
      "arxiv": "arXiv:2107.04127"
    },
    {
      "citation_id": "39",
      "title": "A multi-task mean teacher for semi-supervised facial affective behavior analysis",
      "authors": [
        "L Wang",
        "S Wang"
      ],
      "year": "2021",
      "venue": "A multi-task mean teacher for semi-supervised facial affective behavior analysis",
      "arxiv": "arXiv:2107.04225"
    }
  ]
}