{
  "paper_id": "2110.04476v1",
  "title": "Label Quality In Affectnet: Results Of Crowd-Based Re-Annotation",
  "published": "2021-10-09T06:48:08Z",
  "authors": [
    "Doo Yon Kim",
    "Christian Wallraven"
  ],
  "keywords": [
    "Facial expression recognition",
    "crowd annotation",
    "AffectNet",
    "affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "AffectNet is one of the most popular resources for facial expression recognition (FER) on relatively unconstrained in-the-wild images. Given that images were annotated by only one annotator with limited consistency checks on the data, however, label quality and consistency may be limited. Here, we take a similar approach to a study that re-labeled another, smaller dataset (FER2013) with crowd-based annotations, and report results from a re-labeling and re-annotation of a subset of difficult AffectNet faces with 13 people on both expression label, and valence and arousal ratings. Our results show that human labels overall have medium to good consistency, whereas human ratings especially for valence are in excellent agreement. Importantly, however, crowd-based labels are significantly shifting towards neutral and happy categories and crowd-based affective ratings form a consistent pattern different from the original ratings. ResNets fully trained on the original AffectNet dataset do not predict human voting patterns, but when weakly-trained do so much better, particularly for valence. Our results have important ramifications for label quality in affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Despite recent advances in facial expression recognition (FER) from images, FER on so-called in-the-wild images taken in less constrained contexts still has much lower performance when compared with FER on controlled datasets. Recent performance on one of the most popular in-the-wild datasets -AffectNet  [15]  -is \"only\" around 61%  [11] . This performance falls far short of performance levels on controlled datasets, such as the CK+ dataset  [1]  with 99.69%  [2]  and other in-the-wild datasets, such as the (smaller) FER+ dataset  [4]  with 89.75%  [5] . For a more in-depth review of the current state-of-the-art in deep-learning-based FER, see  [3, 9, 12, 16, 13] .\n\nInterestingly, for FER+, an earlier version that was annotated by the original dataset creators  [10]  called FER2013, had a much lower recognition baseline of 70.22%. Whereas the FER2013 dataset was annotated by only one individual, a follow-up study  [4]  re-labeled the data based on crowd annotation and showed that the resulting maximum vote label also provided a better computational recognition performance.\n\nSince label quality directly determines performance outcomes, providing good and consistent labels has been a recent focus in the machine learning field. Performance increases can not only come from more advanced architectures, but much more \"trivially\", from annotating the data with clean, correct labels. Issues of label quality have, for example, been highlighted in the recent work by  [19] -the authors showed that there is an average of 3.4% errors in several of the surveyed datasets, and that by omitting 6% of correctly labeled images, a smaller ResNet18 can perform better than a larger ResNet50.\n\nGiven the relatively lower levels of performance on AffectNet, here we wanted to revisit this dataset with a similar approach to that taken to create FER+. As an example of the potential issues with AffectNet, see Figure  1 , which shows four examples of images from AffectNet that seem to have problematic labels. Given the large size of AffectNet, we here first report results of a pilot test that uses a subset of (difficult-to-recognize) images of the different expression categories of AffectNet. We re-labeled these both in terms of expression label and also in terms of affective rating of valence and arousal by 13 naive annotators. We compare the consistency of annotators and also how well the images fit to the predictions made by a deep-learning model naively trained on the original AffectNet before and after crowd-relabeling.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experiment",
      "text": "Originally, AffectNet is composed of 11 different categories that also include labels such as \"No face\", for example. Here, we solely focused on the seven emotional expressions plus the neutral category, resulting in: Neutral, Happy, Sad, Disgust, Surprise, Contempt, Fear, and Anger as labels. From each expression, 100 potentially-confusing images were selected by pre-screening the original images, resulting in a total of 800 images for re-annotation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Resnet50",
      "text": "As our deep neural network (DNN) backbone, we used a standard, ImageNet pre-trained ResNet-50, which is one of the most widely used standard architectures in image classification tasks  [20] . The architecture is based on 50 blocks of convolutional filters of size 3 X 3 with residual connections that allow the optimizer to skip a whole convolutional layer, thereby increasing the effectiveness of the total network.\n\nImplementation details for training the ResNet50 architecture on AffectNet were as follows: for augmentation, we added intensity normalization of the color data into 0 to 1, as well as clockwise and anti-clockwise rotations of up to 10 degrees, affine shearing up to 0.1, as well as horizontal flips. Class weights (given the imbalance in label categories) were used for a weighted cross-entropy loss that was optimized with Adam at an initial learning rate of 0.0001.\n\nThis model was trained with 256x256px color images of the AffectNet training set of 287,561 images for 50 epochs and a batch size of 64, after which it reached a validation accuracy of 53.15% (ResNet50 50 epoch) -its best validation accuracy throughout the run, however, was 59.25% (ResNet50 Best), which is only less than two percent worse than the state-of-the-art in AffectNet on eight classes with a much more involved architecture  [11] . To look at the early performance of this model, we also saved its snapshot after the first epoch (ResNet50 1 epoch).\n\nFinally, we set up another training scheme that would perhaps be more akin to human learning, in which one epoch used only 512 randomly-chosen images, but we trained for much longer (1000 epochs). Interestingly, this very weaklytrained model achieved also a reasonable accuracy of around 53.5%. All models are compared in Table  1 .\n\nFor valence and arousal, we followed the same model structure and created two regression models for each rating based on mean squared error (MSE) loss. These models were trained for 10 epochs (again, relatively weakly-trained), and resulted in comparable validation losses of 0.0177 (valence) and 0.0167 (arousal).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Selecting 800 Images For Re-Annotation",
      "text": "In total, there are 420,299 images in AffectNet. For our pilot exploration of label quality, we first pre-screened 100 images from each class to have a more manageable dataset size. To pick these images, we designed a HTML-based GUI that allowed us to quickly loop through all images for a class and select those exemplars that the trained annotator deemed \"confusing\".\n\nFig.  2 : Screenshot of the annotation GUI for the main experiment showing the image, the category selector and the SAM manikins for affective ratings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Experiment",
      "text": "For the experiment, we designed a software application based on HTML shown in Figure  2 . For each image, participants were asked to make three annotations: in the first row, they were to click on one of the eight categories that best described the expression content of the image; the second and third row showed self-assessment manikins (SAM,  [21] ) for rating of valence and arousal at 9 different rating levels.\n\nThe experiment was conducted in a quiet room and supervised in real-time by the experimenter. Participants were first instructed about the experimental procedure and were familiarized with expression labels and valence and arousal ratings. The experimenter also demoed the experiment with one dummy picture to instruct participants about how to pick the expression, to rate valence and arousal, and to move on to the next picture. Unlike AffectNet, we did not constrain valence and arousal values to pre-defined ranges (see  [15] ), so as not to affect participants' intuitive evaluations of the affective content.\n\nWe recruited a total of N=13 people. (7 female, 6 male, mean age (STD) = 33.23 (12.39) years) from the population of Korea University. Participants were naive as to the purpose of the experiment, and reported no neurological or psychological issues that would interfere with emotion processing.\n\n3 Re-annotation results",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Expression Categories",
      "text": "The total number of votes added up to 10,400-as a first, omnibus result, we observed that of these votes, 8,658 (83.25%) did not agree with the original AffectNet labels, indicating that these labels may have issues.\n\nFigure  3a  shows the overall results for participants' votes 1  in more detail (as box plots). The original AffectNet data was uniformly distributed across the eight classes (Figure  3a  orange line)-this is clearly not the case for the median votes, however: On average, participants chose a much higher proportion of neutral and happy labels and much lower proportions of contempt, fear, sad, and disgust. We also observed variability across expression categories in the voting patterns, especially for the neutral, and to lesser degrees for the happy and sad categories, indicating that there was less agreement among participants for these category labels.\n\nThe overall consistency of participants on voting was evaluated by taking each participant's individual voting pattern per expression and correlating this with all other participants' voting patterns. The average value of the upper triangular part of the resulting correlation matrix is a measure of relative consistency and was determined to be r expr = .550, indicating medium to good consistency levels.\n\nFigure  3a  also shows the original number of images in each category in Affect-Net (blue line, normalized). We can see that this distribution-as an extremely weak measure of a \"real-life\" distribution of these facial expression categoriesfits somewhat better to the median voting pattern (the correlation value for the human voting data to this data is r = 0.71, see also Figure  8b below ). This may be an indication that participants implicitly fitted their overall voting strategy to some sort of \"hidden distribution\"; for example, one would expect that most of the time, faces display no strong facial emotion and hence will be neutral. The next most common expression may be happy followed by the other categories with contempt and disgust being perhaps the least commonly-encountered expressions.\n\nFigure  3b  displays a type of confusion matrix, which shows how the original AffectNet labels and the maximum vote labels are connected. As can be seen, the matrix does not exhibit any diagonal structure owing to the large number of differing votes we received. Many contempt images, for example, got relabelled as happy or neutral. At the same time, and in line with the results in Figure  3a , many images originally annotated with an emotional label received votes for the neutral expression.  Overall, our results so far on the 800, pre-screened images suggest that the original expression labels in AffectNet for these difficult images are far from ideal.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Valence And Arousal",
      "text": "To better compare our data with those from AffectNet, we mapped the 1-9 scale into an -1.0 to 1.0 range. Figure  4a ) shows the full data of our experiment with our arousal and valence ratings colored by the maximum vote labels. First of all, we see a clear \"rotated U\"-shape that is reminiscent of the data from affective ratings obtained for the International Affective Picture System (IAPS,  [22] )-in other words, as valence increases positively and negatively arousal increases into the positive direction in both cases. This pattern is a well-established result for affective ratings of pictures and videos  [22, 17] .\n\nIn this context, it is important to mention that the AffectNet annotations for valence and arousal were collected in a different way compared to ours: their annotation scheme used a software application in which the annotator clicked into a pre-defined region within a two-dimensional coordinate system (see  [15] ). In our case, the two annotations were obtained in a more \"traditional\" fashion using the SAM method and without restricting participants to certain value ranges, depending on the label, which may explain some of the differences in rating pattern.\n\nOne of the reasons for the different annotation scheme for the original Af-fectNet ratings may have been to ensure a better consistency for valence and arousal evaluations. To look more closely into this matter, Figure  4  also shows coloring based on the expression categories obtained from the maximum votes, which clearly fit this valence/arousal space well: the green neutral expression stays near the center, whereas red anger and grey happiness clearly move to the upper positive and bottom negative quadrants of the space. If we change labels to the original AffectNet labels, however, the space becomes much more scattered and less consistent as shown in Figure  4b . Hence, the overall pattern of ratings seems consistent with the voted labels in our experiment. To reinforce this point, in Figure  5  we show the overall distribution of valence and arousal votes when labeled with the chosen, maximum vote. Here, the distributions also have peaks at expected values (neutral being at 0, happy peaking at positive valence values, and anger peaking at positive arousal values).\n\nWe next analyzed the standard deviation of valence and arousal ratings within each category selected by participants-these results are shown in Figure  6a, b . As can be seen, overall standard deviations were in the range from 0.2 to 1.0 and were lowest for neutral and contempt for both valence and arousal. Valence seemed to have similar variability for each expression compared to arousal ratings. Overall, absolute agreement on the rating values was relatively high at these low levels of variability.\n\nWe again analyzed the overall correlations among participants in the same manner as done for the expressions. These matrices are shown in Figure  7 . The average correlations measuring consistency in relative voting pattern among human participants was r val = .865 for valence and r aro = .468 for arousal. Relative agreement was very high for valence, but only at medium levels for arousal due to some participants choosing a different rating profile (see Figure  7b , Participant 10, for example). Our results show that participants can agree much better on a) b)\n\nFig.  5 : Human votes on a)valence and b)arousal for voted expressions. In Figure  5 , it shows how people annotated valence and arousal related to their votes to the expressions. For example, we can clearly see when people voted for 'neutral' they also voted mostly on middle point of both valence and arousal. valence ratings of the expressions than on the label-arousal is at similar levels of relative consistency as the labels.\n\n4 Humans vs. DNNs  Figure  8a  shows the different voting patterns across categories (line-plots were chosen to highlight the overall pattern similarity). At first glance, none of the ResNets' predictions matches the human pattern perfectly-however, when looking at Figure  8b , which plots the correlation values among all voting patterns, we can see that the most similar voting pattern to human performance is obtained for the ResNet trained very weakly with only 8 images for 1000 epochs. This is at similar levels to the correlation for human votes to the AffectNet label distribution (blue line). Overall, we find that ResNets trained on the AffectNet original label distribution cannot capture the human voting pattern well, except for a model that was only weakly-trained (yet still achieved a relatively high performance on the AffectNet validation set).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Valence And Arousal",
      "text": "We found that agreement with human valence ratings was very high for the simple ResNet we trained for only 10 epochs as shown in the scatter plot in Figure  9a . The coloring in this figure was done according to the human maximum voting label and shows the expected distribution of expression categories along  the valence range (from angry in red to happy in grey). Conversely, if we color the same data according to the original AffectNet labels, the plot looks much less well-structured (Figure  9b ).  Results for arousal also show good agreement of the ResNet model with human data (Figure  10a ). Again, the human maximum vote coloring produces the desired relationship of arousal to expression category label (cf. neutral at the bottom part to anger and surprised expressions towards the top). Compared to the valence data, however, arousal seems to have more spread-this may be due in part to the fact that arousal values did not span the whole scale, which could have resulted in worse generalization performance for the network. Again, Figure  10b , which uses the original AffectNet labels does not produce consistent relationships between ratings and expression categories. The last rows in Figure  7  show the correlations of the trained ResNet models with the human ratings. The overall, average correlation of the DNN with valence is r val,DN N = .866 and for arousal r aro,DN N = .504. These results confirm the previous analysis and show that, indeed, the ResNet with minimal training of 10 epochs is capable of capturing the valence ratings on our newly-annotated images well. The relative agreement with the arousal ratings is at medium levels-similar to human-human consistency values obtained above.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "Given the recent discussion in the field about label quality in datasets  [19]  and the \"success\" that crowd-based re-annotation had for the FER+ dataset, here we presented results of re-annotation experiments on the popular AffectNet dataset.\n\nThirteen participants re-annotated our pre-screened dataset of 800, difficultto-parse images with expression category and affective ratings. Overall, we found that expression category votes agreed only very weakly with the original Affect-Net labels: only 13% of votes kept the same label. Indeed, participants instead seemed to follow an implicit bias to real-life frequencies in their voting with large numbers of votes falling into the neutral category, for example. Adding the neutral category is relatively rare in human annotation experiments as participants usually are forced to choose one out of seven emotional categories from among expressive faces -this may bias the results, however, as shown, for example, in  [8, 7]  as participants cannot select a \"non-signal\". It will be interesting, nonetheless, to repeat the annotation with different amounts of categories or with a ranking strategy (similar to the TopN-accuracy results reported for large-scale classification tasks). Overall, our results so far clearly confirm the difficulty to assign unique labels to facial expressions out-of-context (that is, without temporal or auditory contextual information)-an issue that was also visible in the large variability in some of the expression categories. Importantly, this issue was also highlighted in a recent review paper in affective psychology  [18] , which cast doubt on context-free, unambiguous labeling of facial expressions.\n\nThe issue of labels is circumvented to some degree by the affective ratings we obtained in the annotation. Here, participants had less variability overall, but also showed clear, systematic, and consistent deviations from the original AffectNet ratings. In line with a large number of other results  [22, 17] , we obtained a valence-arousal space that showcased dependencies between the two dimensions. This may in part be due to the difference in methods, where the AffectNet annotations placed implicit and explicit restrictions on participants' rating pattern-a topic that will be interesting to follow up in future studies. The observed rating patterns, however, did match well with the average expression categories and were consistent among participants with little variability.\n\nFinally, we compared the human crowd annotations with predictions of DNNs. In terms of expression categories, we found no good match, except for a weaklytrained, somewhat \"unconventional\" DNN that showed good correlations. Given that the human data did not match well with the AffectNet labels, the failure of the AffectNet-trained DNNs to capture human voting patterns is perhaps not surprising. Only a much more extensive re-annotation of AffectNet will be able to shed final light on potential performance gains (and better fitting quality), as here we were only able to test a first set of 800 images. This work is currently underway.\n\nIn terms of valence and arousal, however, we found much better agreement between the ResNets and human performance, especially for valence but also to some degree for arousal. Since the number of networks we tested for these ratings, was limited, however, we can only cautiously suggest that this could be due to the fact that affective ratings for our (difficult) images suffered less from ambiguity compared to the labels (see human results). Future work will need to test the degree to which the weakly-trained networks used in our work are better at capturing the label-and/or rating space of human annotations.\n\nOverall, our results clearly highlight issues with expression labeling and point to the need for the field to use also other, continuous annotation schemes (like valence and arousal, but more evaluative dimensions are possible  [14] ) and/or focus on analysis of affective data in more contextually rich environments.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which shows four",
      "page": 2
    },
    {
      "caption": "Figure 1: Four images of AﬀectNet (originally labeled as anger, surprise, happy,",
      "page": 2
    },
    {
      "caption": "Figure 2: Screenshot of the annotation GUI for the main experiment showing the",
      "page": 4
    },
    {
      "caption": "Figure 2: For each image, participants were asked to make three annotations:",
      "page": 4
    },
    {
      "caption": "Figure 3: a shows the overall results for participants’ votes1 in more detail (as",
      "page": 5
    },
    {
      "caption": "Figure 3: a orange line)—this is clearly not the case for the median votes,",
      "page": 5
    },
    {
      "caption": "Figure 3: a also shows the original number of images in each category in Aﬀect-",
      "page": 5
    },
    {
      "caption": "Figure 8: b below). This may",
      "page": 5
    },
    {
      "caption": "Figure 3: b displays a type of confusion matrix, which shows how the original",
      "page": 5
    },
    {
      "caption": "Figure 3: Voting results and confusion matrix",
      "page": 6
    },
    {
      "caption": "Figure 4: a) shows the full data of our experiment with",
      "page": 6
    },
    {
      "caption": "Figure 4: also shows",
      "page": 7
    },
    {
      "caption": "Figure 4: b. Hence, the overall pattern",
      "page": 7
    },
    {
      "caption": "Figure 4: a) Arousal-Valence space colored with our max vote labels. b) Same space",
      "page": 7
    },
    {
      "caption": "Figure 5: we show the overall distribution of valence",
      "page": 7
    },
    {
      "caption": "Figure 6: a,b. As can be seen, overall standard deviations were in the range from 0.2 to",
      "page": 7
    },
    {
      "caption": "Figure 7: b, Participant",
      "page": 7
    },
    {
      "caption": "Figure 5: Human votes on a)valence and b)arousal for voted expressions. In Figure",
      "page": 8
    },
    {
      "caption": "Figure 6: Standard deviations of a) valence and b) arousal ratings across partici-",
      "page": 8
    },
    {
      "caption": "Figure 7: Correlation matrix between participants for a) valence and b) arousal",
      "page": 9
    },
    {
      "caption": "Figure 4: 1 compares voting patterns of the diﬀerent DNNs we trained to the",
      "page": 9
    },
    {
      "caption": "Figure 8: a shows the diﬀerent voting patterns across",
      "page": 9
    },
    {
      "caption": "Figure 8: b, which plots the correlation values",
      "page": 9
    },
    {
      "caption": "Figure 9: a. The coloring in this ﬁgure was done according to the human maximum",
      "page": 9
    },
    {
      "caption": "Figure 8: Comparison of human and machine voting patterns.",
      "page": 10
    },
    {
      "caption": "Figure 9: Valence Comparison between ResNet50 and humans.",
      "page": 10
    },
    {
      "caption": "Figure 10: a). Again, the human maximum vote coloring produces",
      "page": 10
    },
    {
      "caption": "Figure 10: Arousal Comparison between ResNet50 and humans.",
      "page": 11
    },
    {
      "caption": "Figure 10: b, which uses the original AﬀectNet labels does not produce consistent",
      "page": 11
    },
    {
      "caption": "Figure 7: show the correlations of the trained ResNet models",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Engineering, Korea University, Seoul, Korea2": "wallraven@korea.ac.kr"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "Abstract. AﬀectNet is one of the most popular resources for facial ex-"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "pression recognition (FER) on relatively unconstrained in-the-wild im-"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "ages. Given that\nimages were annotated by only one annotator with"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "limited consistency checks on the data, however,\nlabel quality and con-"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "sistency may be limited. Here, we take a similar approach to a study"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "that\nre-labeled another,\nsmaller dataset\n(FER2013) with crowd-based"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "annotations, and report results from a re-labeling and re-annotation of"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "a subset of diﬃcult AﬀectNet\nfaces with 13 people on both expression"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "label, and valence and arousal ratings. Our results show that human la-"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "bels overall have medium to good consistency, whereas human ratings"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "especially for valence are in excellent agreement.\nImportantly, however,"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "crowd-based labels are signiﬁcantly shifting towards neutral and happy"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "categories and crowd-based aﬀective ratings\nform a consistent pattern"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "diﬀerent\nfrom the original\nratings. ResNets\nfully trained on the origi-"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "nal AﬀectNet dataset do not predict human voting patterns, but when"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "weakly-trained do so much better, particularly for valence. Our results"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "have important ramiﬁcations for label quality in aﬀective computing."
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "Keywords: Facial expression recognition · crowd annotation · AﬀectNet"
        },
        {
          "Engineering, Korea University, Seoul, Korea2": "· aﬀective computing."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "Despite recent advances in facial expression recognition (FER) from images, FER",
          "Introduction": ""
        },
        {
          "1": "on so-called in-the-wild images taken in less constrained contexts still has much",
          "Introduction": ""
        },
        {
          "1": "lower performance when compared with FER on controlled datasets. Recent",
          "Introduction": ""
        },
        {
          "1": "performance on one of the most popular in-the-wild datasets - AﬀectNet [15] - is",
          "Introduction": ""
        },
        {
          "1": "”only” around 61% [11]. This performance falls far short of performance levels",
          "Introduction": ""
        },
        {
          "1": "on controlled datasets, such as the CK+ dataset [1] with 99.69% [2] and other",
          "Introduction": ""
        },
        {
          "1": "in-the-wild datasets,",
          "Introduction": "such as"
        },
        {
          "1": "For a more in-depth review of the current state-of-the-art in deep-learning-based",
          "Introduction": ""
        },
        {
          "1": "FER, see [3,9,12,16,13].",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nKim and Wallraven": "Interestingly, for FER+, an earlier version that was annotated by the original"
        },
        {
          "2\nKim and Wallraven": "dataset creators [10] called FER2013, had a much lower recognition baseline of"
        },
        {
          "2\nKim and Wallraven": "70.22%. Whereas the FER2013 dataset was annotated by only one individual, a"
        },
        {
          "2\nKim and Wallraven": "follow-up study [4] re-labeled the data based on crowd annotation and showed"
        },
        {
          "2\nKim and Wallraven": "that\nthe\nresulting maximum vote\nlabel also provided a better\ncomputational"
        },
        {
          "2\nKim and Wallraven": "recognition performance."
        },
        {
          "2\nKim and Wallraven": "Since label quality directly determines performance outcomes, providing good"
        },
        {
          "2\nKim and Wallraven": "and consistent labels has been a recent focus in the machine learning ﬁeld. Per-"
        },
        {
          "2\nKim and Wallraven": "formance increases can not only come from more advanced architectures, but"
        },
        {
          "2\nKim and Wallraven": "much more ”trivially”,\nfrom annotating the data with clean, correct labels.\nIs-"
        },
        {
          "2\nKim and Wallraven": "sues of\nlabel quality have,\nfor example, been highlighted in the recent work by"
        },
        {
          "2\nKim and Wallraven": "[19]—the authors\nshowed that\nthere is an average of 3.4% errors\nin several of"
        },
        {
          "2\nKim and Wallraven": "the surveyed datasets, and that by omitting 6% of correctly labeled images, a"
        },
        {
          "2\nKim and Wallraven": "smaller ResNet18 can perform better than a larger ResNet50."
        },
        {
          "2\nKim and Wallraven": "Given the relatively lower levels of performance on AﬀectNet, here we wanted"
        },
        {
          "2\nKim and Wallraven": "to revisit this dataset with a similar approach to that taken to create FER+. As"
        },
        {
          "2\nKim and Wallraven": "an example of the potential issues with AﬀectNet, see Figure 1, which shows four"
        },
        {
          "2\nKim and Wallraven": "examples of\nimages from AﬀectNet that seem to have problematic labels. Given"
        },
        {
          "2\nKim and Wallraven": "the large size of AﬀectNet, we here ﬁrst report results of a pilot test that uses a"
        },
        {
          "2\nKim and Wallraven": "subset of (diﬃcult-to-recognize) images of the diﬀerent expression categories of"
        },
        {
          "2\nKim and Wallraven": "AﬀectNet. We re-labeled these both in terms of expression label and also in terms"
        },
        {
          "2\nKim and Wallraven": "of aﬀective rating of valence and arousal by 13 naive annotators. We compare"
        },
        {
          "2\nKim and Wallraven": "the consistency of annotators and also how well the images ﬁt to the predictions"
        },
        {
          "2\nKim and Wallraven": "made by a deep-learning model naively trained on the original AﬀectNet before"
        },
        {
          "2\nKim and Wallraven": "and after crowd-relabeling."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: For valence and arousal, we followed the same model structure and created",
      "data": [
        {
          "Re-annotating AﬀectNet\n3": "2.1\nResNet50"
        },
        {
          "Re-annotating AﬀectNet\n3": "As our deep neural network (DNN) backbone, we used a standard,\nImageNet"
        },
        {
          "Re-annotating AﬀectNet\n3": "pre-trained ResNet-50, which is one of the most widely used standard architec-"
        },
        {
          "Re-annotating AﬀectNet\n3": "tures in image classiﬁcation tasks [20]. The architecture is based on 50 blocks of"
        },
        {
          "Re-annotating AﬀectNet\n3": "convolutional ﬁlters of size 3 X 3 with residual connections that allow the opti-"
        },
        {
          "Re-annotating AﬀectNet\n3": "mizer to skip a whole convolutional\nlayer, thereby increasing the eﬀectiveness of"
        },
        {
          "Re-annotating AﬀectNet\n3": "the total network."
        },
        {
          "Re-annotating AﬀectNet\n3": "Implementation details for training the ResNet50 architecture on AﬀectNet"
        },
        {
          "Re-annotating AﬀectNet\n3": "were as follows: for augmentation, we added intensity normalization of the color"
        },
        {
          "Re-annotating AﬀectNet\n3": "data into 0 to 1, as well as clockwise and anti-clockwise rotations of up to 10"
        },
        {
          "Re-annotating AﬀectNet\n3": "degrees, aﬃne shearing up to 0.1, as well as horizontal ﬂips. Class weights (given"
        },
        {
          "Re-annotating AﬀectNet\n3": "the imbalance in label categories) were used for a weighted cross-entropy loss"
        },
        {
          "Re-annotating AﬀectNet\n3": "that was optimized with Adam at an initial\nlearning rate of 0.0001."
        },
        {
          "Re-annotating AﬀectNet\n3": "This model was trained with 256x256px color images of the AﬀectNet training"
        },
        {
          "Re-annotating AﬀectNet\n3": "set of 287,561 images for 50 epochs and a batch size of 64, after which it reached a"
        },
        {
          "Re-annotating AﬀectNet\n3": "validation accuracy of 53.15% (ResNet50 50 epoch) - its best validation accuracy"
        },
        {
          "Re-annotating AﬀectNet\n3": "throughout\nthe run, however, was 59.25% (ResNet50 Best), which is only less"
        },
        {
          "Re-annotating AﬀectNet\n3": "than two percent worse than the state-of-the-art\nin AﬀectNet on eight classes"
        },
        {
          "Re-annotating AﬀectNet\n3": "with a much more involved architecture[11]. To look at the early performance of"
        },
        {
          "Re-annotating AﬀectNet\n3": "this model, we also saved its snapshot after the ﬁrst epoch (ResNet50 1 epoch)."
        },
        {
          "Re-annotating AﬀectNet\n3": "Finally, we set up another training scheme that would perhaps be more akin"
        },
        {
          "Re-annotating AﬀectNet\n3": "to human learning,\nin which one epoch used only 512 randomly-chosen images,"
        },
        {
          "Re-annotating AﬀectNet\n3": "but we trained for much longer\n(1000 epochs).\nInterestingly,\nthis very weakly-"
        },
        {
          "Re-annotating AﬀectNet\n3": "trained model achieved also a reasonable accuracy of around 53.5%. All models"
        },
        {
          "Re-annotating AﬀectNet\n3": "are compared in Table 1."
        },
        {
          "Re-annotating AﬀectNet\n3": "For valence and arousal, we followed the same model structure and created"
        },
        {
          "Re-annotating AﬀectNet\n3": "two regression models for each rating based on mean squared error (MSE) loss."
        },
        {
          "Re-annotating AﬀectNet\n3": "These models were trained for 10 epochs (again, relatively weakly-trained), and"
        },
        {
          "Re-annotating AﬀectNet\n3": "resulted in comparable validation losses of 0.0177 (valence) and 0.0167 (arousal)."
        },
        {
          "Re-annotating AﬀectNet\n3": "Table 1: Parameters and validation performance for our ResNet50 architectures."
        },
        {
          "Re-annotating AﬀectNet\n3": "Model\nEpoch\nPre-trained\nSteps Per Epoch\nVal Accuracy/Loss"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 50Epoch\n50\nImageNet\n287,651 / batch size\n0.5315"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 Best\n50\nImageNet\n287,651 / batch size\n0.5925"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 1Epoch\n1\nImageNet\n287,651 / batch size\n0.4972"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 8Step\n1000\nImageNet\n8\n0.5353"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 Valence\n10\nImageNet\n287,651 / batch size\n0.0177"
        },
        {
          "Re-annotating AﬀectNet\n3": "ResNet50 Arousal\n10\nImageNet\n287,651 / batch size\n0.0167"
        },
        {
          "Re-annotating AﬀectNet\n3": "2.2\nSelecting 800 images for re-annotation"
        },
        {
          "Re-annotating AﬀectNet\n3": "In total,\nthere are 420,299 images\nin AﬀectNet. For our pilot\nexploration of"
        },
        {
          "Re-annotating AﬀectNet\n3": "label quality, we ﬁrst pre-screened 100 images\nfrom each class\nto have a more"
        },
        {
          "Re-annotating AﬀectNet\n3": "manageable dataset size. To pick these images, we designed a HTML-based GUI"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nKim and Wallraven": "that allowed us to quickly loop through all\nimages for a class and select those"
        },
        {
          "4\nKim and Wallraven": "exemplars that the trained annotator deemed ”confusing”."
        },
        {
          "4\nKim and Wallraven": "Fig. 2: Screenshot of the annotation GUI\nfor the main experiment showing the"
        },
        {
          "4\nKim and Wallraven": "image, the category selector and the SAM manikins for aﬀective ratings."
        },
        {
          "4\nKim and Wallraven": "2.3\nAnnotation experiment"
        },
        {
          "4\nKim and Wallraven": "For the experiment, we designed a software application based on HTML shown"
        },
        {
          "4\nKim and Wallraven": "in Figure 2. For each image, participants were asked to make three annotations:"
        },
        {
          "4\nKim and Wallraven": "in the ﬁrst row, they were to click on one of the eight categories that best de-"
        },
        {
          "4\nKim and Wallraven": "scribed the expression content of\nthe image;\nthe second and third row showed"
        },
        {
          "4\nKim and Wallraven": "self-assessment manikins (SAM,\n[21]) for rating of valence and arousal at 9 dif-"
        },
        {
          "4\nKim and Wallraven": "ferent rating levels."
        },
        {
          "4\nKim and Wallraven": "The experiment was conducted in a quiet room and supervised in real-time"
        },
        {
          "4\nKim and Wallraven": "by the experimenter. Participants were ﬁrst instructed about the experimental"
        },
        {
          "4\nKim and Wallraven": "procedure and were familiarized with expression labels and valence and arousal"
        },
        {
          "4\nKim and Wallraven": "ratings. The experimenter also demoed the experiment with one dummy picture"
        },
        {
          "4\nKim and Wallraven": "to instruct participants about how to pick the expression,\nto rate valence and"
        },
        {
          "4\nKim and Wallraven": "arousal, and to move on to the next picture. Unlike AﬀectNet, we did not con-"
        },
        {
          "4\nKim and Wallraven": "strain valence and arousal values\nto pre-deﬁned ranges\n(see [15]),\nso as not\nto"
        },
        {
          "4\nKim and Wallraven": "aﬀect participants’\nintuitive evaluations of the aﬀective content."
        },
        {
          "4\nKim and Wallraven": "We recruited a total of N=13 people.\n(7 female, 6 male, mean age (STD)"
        },
        {
          "4\nKim and Wallraven": "= 33.23 (12.39) years)\nfrom the population of Korea University. Participants"
        },
        {
          "4\nKim and Wallraven": "were naive as to the purpose of the experiment, and reported no neurological or"
        },
        {
          "4\nKim and Wallraven": "psychological\nissues that would interfere with emotion processing."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.1\nExpression categories": "The total number of votes added up to 10,400—as a ﬁrst, omnibus\nresult, we"
        },
        {
          "3.1\nExpression categories": "observed that of\nthese votes, 8,658 (83.25%) did not\nagree with the original"
        },
        {
          "3.1\nExpression categories": "AﬀectNet labels,\nindicating that these labels may have issues."
        },
        {
          "3.1\nExpression categories": "Figure 3a shows the overall results for participants’ votes1 in more detail (as"
        },
        {
          "3.1\nExpression categories": "box plots). The original AﬀectNet data was uniformly distributed across the eight"
        },
        {
          "3.1\nExpression categories": "classes (Figure 3a orange line)—this is clearly not the case for the median votes,"
        },
        {
          "3.1\nExpression categories": "however: On average, participants chose a much higher proportion of neutral and"
        },
        {
          "3.1\nExpression categories": "happy labels and much lower proportions of contempt,\nfear,\nsad, and disgust."
        },
        {
          "3.1\nExpression categories": "We also observed variability across expression categories in the voting patterns,"
        },
        {
          "3.1\nExpression categories": "especially for the neutral, and to lesser degrees for the happy and sad categories,"
        },
        {
          "3.1\nExpression categories": "indicating that there was less agreement among participants for these category"
        },
        {
          "3.1\nExpression categories": "labels."
        },
        {
          "3.1\nExpression categories": "The overall consistency of participants on voting was evaluated by taking each"
        },
        {
          "3.1\nExpression categories": "participant’s individual voting pattern per expression and correlating this with"
        },
        {
          "3.1\nExpression categories": "all other participants’ voting patterns. The average value of the upper triangular"
        },
        {
          "3.1\nExpression categories": "part of the resulting correlation matrix is a measure of relative consistency and"
        },
        {
          "3.1\nExpression categories": "was determined to be rexpr = .550, indicating medium to good consistency levels."
        },
        {
          "3.1\nExpression categories": "Figure 3a also shows the original number of images in each category in Aﬀect-"
        },
        {
          "3.1\nExpression categories": "Net (blue line, normalized). We can see that this distribution—as an extremely"
        },
        {
          "3.1\nExpression categories": "weak measure of a ”real-life” distribution of these facial expression categories—"
        },
        {
          "3.1\nExpression categories": "ﬁts somewhat better to the median voting pattern (the correlation value for the"
        },
        {
          "3.1\nExpression categories": "human voting data to this data is r = 0.71, see also Figure 8b below). This may"
        },
        {
          "3.1\nExpression categories": "be an indication that participants implicitly ﬁtted their overall voting strategy"
        },
        {
          "3.1\nExpression categories": "to some sort of ”hidden distribution”;\nfor example, one would expect that most"
        },
        {
          "3.1\nExpression categories": "of\nthe time,\nfaces display no strong facial emotion and hence will be neutral."
        },
        {
          "3.1\nExpression categories": "The next most common expression may be happy followed by the other cate-"
        },
        {
          "3.1\nExpression categories": "gories with contempt and disgust being perhaps the least commonly-encountered"
        },
        {
          "3.1\nExpression categories": "expressions."
        },
        {
          "3.1\nExpression categories": "Figure 3b displays a type of confusion matrix, which shows how the original"
        },
        {
          "3.1\nExpression categories": "AﬀectNet labels and the maximum vote labels are connected. As can be seen,"
        },
        {
          "3.1\nExpression categories": "the matrix does not exhibit any diagonal structure owing to the large number of"
        },
        {
          "3.1\nExpression categories": "diﬀering votes we received. Many contempt images,\nfor example, got relabelled"
        },
        {
          "3.1\nExpression categories": "as happy or neutral. At the same time, and in line with the results in Figure 3a,"
        },
        {
          "3.1\nExpression categories": "many images originally annotated with an emotional\nlabel received votes for the"
        },
        {
          "3.1\nExpression categories": "neutral expression."
        },
        {
          "3.1\nExpression categories": "1 In determining the ﬁnal vote to be counted, we also experimented with diﬀerent kinds"
        },
        {
          "3.1\nExpression categories": "of maximum (modal) voting, such as maximum vote across all votes, maximum votes"
        },
        {
          "3.1\nExpression categories": "for each image with diﬀerent ways of breaking ties (counting the ﬁrst tie, the second,"
        },
        {
          "3.1\nExpression categories": "or the third). We found that the average voting pattern was not aﬀected by these"
        },
        {
          "3.1\nExpression categories": "methods, and hence used this pattern in the remainder of this paper."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Box-whisker plot showing the results": "of the human re-annotation in compari-",
          "(b)\nConfusion-type\nmatrix\nshowing": "which label participants chose depend-"
        },
        {
          "(a) Box-whisker plot showing the results": "son to the constant AﬀectNet label dis-",
          "(b)\nConfusion-type\nmatrix\nshowing": "ing on the original AﬀectNet label."
        },
        {
          "(a) Box-whisker plot showing the results": "tribution (orange\nline)\nand the distri-",
          "(b)\nConfusion-type\nmatrix\nshowing": ""
        },
        {
          "(a) Box-whisker plot showing the results": "bution of all\nimages\nin AﬀectNet\n(blue",
          "(b)\nConfusion-type\nmatrix\nshowing": ""
        },
        {
          "(a) Box-whisker plot showing the results": "line).",
          "(b)\nConfusion-type\nmatrix\nshowing": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Re-annotating AﬀectNet\n7": "the reasons\nfor\nthe diﬀerent annotation scheme for\nthe original Af-"
        },
        {
          "Re-annotating AﬀectNet\n7": "ratings may have been to ensure a better consistency for valence and"
        },
        {
          "Re-annotating AﬀectNet\n7": ""
        },
        {
          "Re-annotating AﬀectNet\n7": ""
        },
        {
          "Re-annotating AﬀectNet\n7": "this valence/arousal\nspace well:\nthe green neutral expression"
        },
        {
          "Re-annotating AﬀectNet\n7": "the\ncenter, whereas\nred anger and grey happiness\nclearly move\nto"
        },
        {
          "Re-annotating AﬀectNet\n7": "the space.\nIf we change"
        },
        {
          "Re-annotating AﬀectNet\n7": ""
        },
        {
          "Re-annotating AﬀectNet\n7": ""
        },
        {
          "Re-annotating AﬀectNet\n7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a)\nb)": "Fig. 5: Human votes on a)valence and b)arousal\nfor voted expressions. In Figure"
        },
        {
          "a)\nb)": "5,\nit shows how people annotated valence and arousal related to their votes to"
        },
        {
          "a)\nb)": "the expressions. For example, we can clearly see when people voted for ’neutral’"
        },
        {
          "a)\nb)": "they also voted mostly on middle point of both valence and arousal."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a)\nb)": "Fig. 7: Correlation matrix between participants\nfor a) valence and b) arousal"
        },
        {
          "a)\nb)": "ratings. The matrix also includes an extra row for the ResNet model (see analysis"
        },
        {
          "a)\nb)": "below)."
        },
        {
          "a)\nb)": "valence ratings of the expressions than on the label—arousal\nis at similar levels"
        },
        {
          "a)\nb)": "of relative consistency as the labels."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10\nKim and Wallraven": "(a) Average voting for the four diﬀerent\n(b) Correlation matrix showing similar-"
        },
        {
          "10\nKim and Wallraven": "ResNet models compared to the average\nities\nin voting pattern for all curves\nin"
        },
        {
          "10\nKim and Wallraven": "human vote (green) and the number of\nFigure 8a"
        },
        {
          "10\nKim and Wallraven": "images in AﬀectNet categories (blue)."
        },
        {
          "10\nKim and Wallraven": "Fig. 8: Comparison of human and machine voting patterns."
        },
        {
          "10\nKim and Wallraven": "the valence range (from angry in red to happy in grey). Conversely,\nif we color"
        },
        {
          "10\nKim and Wallraven": "the same data according to the original AﬀectNet\nlabels,\nthe plot\nlooks much"
        },
        {
          "10\nKim and Wallraven": "less well-structured (Figure 9b)."
        },
        {
          "10\nKim and Wallraven": "(a) Human max vote label.\n(b) AﬀectNet label"
        },
        {
          "10\nKim and Wallraven": "Fig. 9: Valence Comparison between ResNet50 and humans."
        },
        {
          "10\nKim and Wallraven": "Results\nfor arousal also show good agreement of\nthe ResNet model with"
        },
        {
          "10\nKim and Wallraven": "human data (Figure 10a). Again, the human maximum vote coloring produces"
        },
        {
          "10\nKim and Wallraven": "the desired relationship of arousal\nto expression category label\n(cf. neutral at"
        },
        {
          "10\nKim and Wallraven": "the bottom part to anger and surprised expressions towards the top). Compared"
        },
        {
          "10\nKim and Wallraven": "to the valence data, however, arousal seems to have more spread—this may be"
        },
        {
          "10\nKim and Wallraven": "due in part to the fact that arousal values did not span the whole scale, which"
        },
        {
          "10\nKim and Wallraven": "could have resulted in worse generalization performance for the network. Again,"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Human max vote label.": "",
          "(b) AﬀectNet label": "Fig. 10: Arousal Comparison between ResNet50 and humans."
        },
        {
          "(a) Human max vote label.": "Figure 10b, which uses the original AﬀectNet labels does not produce consistent",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "relationships between ratings and expression categories.",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "",
          "(b) AﬀectNet label": "The last rows in Figure 7 show the correlations of the trained ResNet models"
        },
        {
          "(a) Human max vote label.": "with the human ratings. The overall, average correlation of the DNN with valence",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "is rval,DN N = .866 and for arousal raro,DN N = .504. These results conﬁrm the",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "previous analysis and show that,",
          "(b) AﬀectNet label": "indeed, the ResNet with minimal training of 10"
        },
        {
          "(a) Human max vote label.": "epochs is capable of capturing the valence ratings on our newly-annotated images",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "well. The relative agreement with the arousal ratings is at medium levels—similar",
          "(b) AﬀectNet label": ""
        },
        {
          "(a) Human max vote label.": "to human-human consistency values obtained above.",
          "(b) AﬀectNet label": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nKim and Wallraven": "also highlighted in a recent review paper in aﬀective psychology [18], which cast"
        },
        {
          "12\nKim and Wallraven": "doubt on context-free, unambiguous labeling of\nfacial expressions."
        },
        {
          "12\nKim and Wallraven": "The issue of\nlabels\nis circumvented to some degree by the aﬀective ratings"
        },
        {
          "12\nKim and Wallraven": "we obtained in the annotation. Here, participants had less variability overall,"
        },
        {
          "12\nKim and Wallraven": "but also showed clear,\nsystematic, and consistent deviations\nfrom the original"
        },
        {
          "12\nKim and Wallraven": "AﬀectNet\nratings.\nIn line with a large number of other\nresults\n[22,17], we ob-"
        },
        {
          "12\nKim and Wallraven": "tained a valence-arousal\nspace\nthat\nshowcased dependencies between the\ntwo"
        },
        {
          "12\nKim and Wallraven": "dimensions. This may in part be due to the diﬀerence in methods, where the"
        },
        {
          "12\nKim and Wallraven": "AﬀectNet annotations placed implicit and explicit restrictions on participants’"
        },
        {
          "12\nKim and Wallraven": "rating pattern—a topic that will be interesting to follow up in future studies. The"
        },
        {
          "12\nKim and Wallraven": "observed rating patterns, however, did match well with the average expression"
        },
        {
          "12\nKim and Wallraven": "categories and were consistent among participants with little variability."
        },
        {
          "12\nKim and Wallraven": "Finally, we compared the human crowd annotations with predictions of DNNs."
        },
        {
          "12\nKim and Wallraven": "In terms of expression categories, we found no good match, except for a weakly-"
        },
        {
          "12\nKim and Wallraven": "trained, somewhat ”unconventional” DNN that showed good correlations. Given"
        },
        {
          "12\nKim and Wallraven": "that the human data did not match well with the AﬀectNet labels, the failure"
        },
        {
          "12\nKim and Wallraven": "of the AﬀectNet-trained DNNs to capture human voting patterns is perhaps not"
        },
        {
          "12\nKim and Wallraven": "surprising. Only a much more extensive re-annotation of AﬀectNet will be able"
        },
        {
          "12\nKim and Wallraven": "to shed ﬁnal\nlight on potential performance gains (and better ﬁtting quality), as"
        },
        {
          "12\nKim and Wallraven": "here we were only able to test a ﬁrst set of 800 images. This work is currently"
        },
        {
          "12\nKim and Wallraven": "underway."
        },
        {
          "12\nKim and Wallraven": "In terms of valence and arousal, however, we found much better agreement"
        },
        {
          "12\nKim and Wallraven": "between the ResNets and human performance, especially for valence but also"
        },
        {
          "12\nKim and Wallraven": "to some degree for arousal. Since the number of networks we tested for\nthese"
        },
        {
          "12\nKim and Wallraven": "ratings, was limited, however, we can only cautiously suggest that this could be"
        },
        {
          "12\nKim and Wallraven": "due to the fact that aﬀective ratings for our (diﬃcult) images suﬀered less from"
        },
        {
          "12\nKim and Wallraven": "ambiguity compared to the labels (see human results). Future work will need to"
        },
        {
          "12\nKim and Wallraven": "test the degree to which the weakly-trained networks used in our work are better"
        },
        {
          "12\nKim and Wallraven": "at capturing the label- and/or rating space of human annotations."
        },
        {
          "12\nKim and Wallraven": "Overall, our results clearly highlight issues with expression labeling and point"
        },
        {
          "12\nKim and Wallraven": "to the need for the ﬁeld to use also other, continuous annotation schemes (like"
        },
        {
          "12\nKim and Wallraven": "valence and arousal, but more evaluative dimensions are possible [14]) and/or"
        },
        {
          "12\nKim and Wallraven": "focus on analysis of aﬀective data in more contextually rich environments."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Re-annotating AﬀectNet\n13": "emotion-speciﬁed expression. 2010 IEEE Computer Society Conference on Com-"
        },
        {
          "Re-annotating AﬀectNet\n13": "puter Vision and Pattern Recognition - Workshops, CVPRW 2010, July, 94–101."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/CVPRW.2010.5543262"
        },
        {
          "Re-annotating AﬀectNet\n13": "2. Meng, D.,\nPeng, X., Wang, K., & Qiao, Y.\n(2019).\nFrame Attention Net-"
        },
        {
          "Re-annotating AﬀectNet\n13": "works\nfor Facial Expression Recognition in Videos. Proceedings\n-\nInternational"
        },
        {
          "Re-annotating AﬀectNet\n13": "Conference\non Image Processing,\nICIP,\n2019-September(September),\n3866–3870."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/ICIP.2019.8803603"
        },
        {
          "Re-annotating AﬀectNet\n13": "3. Savchenko, A. V.\n(2021). Facial\nexpression and attributes\nrecognition based on"
        },
        {
          "Re-annotating AﬀectNet\n13": "multi-task learning of lightweight neural networks. http://arxiv.org/abs/2103.17107"
        },
        {
          "Re-annotating AﬀectNet\n13": "4. Barsoum, E., Zhang, C., Ferrer, C. C., & Zhang, Z. (2016). Training deep networks"
        },
        {
          "Re-annotating AﬀectNet\n13": "for facial expression recognition with crowd-sourced label distribution. ICMI 2016 -"
        },
        {
          "Re-annotating AﬀectNet\n13": "Proceedings of the 18th ACM International Conference on Multimodal Interaction,"
        },
        {
          "Re-annotating AﬀectNet\n13": "279–283. https://doi.org/10.1145/2993148.2993165"
        },
        {
          "Re-annotating AﬀectNet\n13": "5. Vo, T. H., Lee, G. S., Yang, H. J., & Kim, S. H. (2020). Pyramid with Super Resolu-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tion for In-the-Wild Facial Expression Recognition. IEEE Access, 8, 131988–132001."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/ACCESS.2020.3010018"
        },
        {
          "Re-annotating AﬀectNet\n13": "6. Guan, M. Y., Gulshan, V., Dai, A. M., & Hinton, G. E.\n(2018). Who said what:"
        },
        {
          "Re-annotating AﬀectNet\n13": "Modeling individual\nlabelers improves classiﬁcation. 32nd AAAI Conference on Ar-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tiﬁcial Intelligence, AAAI 2018, 3109–3118."
        },
        {
          "Re-annotating AﬀectNet\n13": "7. Nusseck, M., Cunningham, D. W., Wallraven, C., & B¨ulthoﬀ, H. H. (2008). The con-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tribution of diﬀerent facial regions to the recognition of conversational expressions."
        },
        {
          "Re-annotating AﬀectNet\n13": "Journal of Vision, 8(8), 1–23. https://doi.org/10.1167/8.8.1"
        },
        {
          "Re-annotating AﬀectNet\n13": "8. Russell, J. (1992). Is There Universal Recognition of Emotion From Facial Expres-"
        },
        {
          "Re-annotating AﬀectNet\n13": "sion? A Review of the Cross-Cultural Studies. Psychological Bulletin 1994, Vol. 115,"
        },
        {
          "Re-annotating AﬀectNet\n13": "No. 1, 102-141."
        },
        {
          "Re-annotating AﬀectNet\n13": "9. Zhou, H., Meng, D., Zhang, Y., Peng, X., Du, J., Wang, K., & Qiao, Y.\n(2019)."
        },
        {
          "Re-annotating AﬀectNet\n13": "Exploring emotion features and fusion strategies for audio-video emotion recogni-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tion. ICMI 2019 - Proceedings of the 2019 International Conference on Multimodal"
        },
        {
          "Re-annotating AﬀectNet\n13": "Interaction, 562–566. https://doi.org/10.1145/3340555.3355713"
        },
        {
          "Re-annotating AﬀectNet\n13": "10. Goodfellow, I. J., Erhan, D., Luc Carrier, P., Courville, A., Mirza, M., Hamner, B.,"
        },
        {
          "Re-annotating AﬀectNet\n13": "Cukierski, W., Tang, Y., Thaler, D., Lee, D. H., Zhou, Y., Ramaiah, C., Feng, F., Li,"
        },
        {
          "Re-annotating AﬀectNet\n13": "R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J.,\n. . . Bengio,"
        },
        {
          "Re-annotating AﬀectNet\n13": "Y. (2015). Challenges in representation learning: A report on three machine learning"
        },
        {
          "Re-annotating AﬀectNet\n13": "contests. Neural Networks, 64, 59–63. https://doi.org/10.1016/j.neunet.2014.09.005"
        },
        {
          "Re-annotating AﬀectNet\n13": "11. Li,\nS.,\n&\nDeng,\nW.\n(2020).\nDeep\nFacial\nExpression\nRecogni-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tion:\nA\nSurvey.\nIEEE\nTransactions\non\nAﬀective\nComputing,\n1–25."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/TAFFC.2020.2981446"
        },
        {
          "Re-annotating AﬀectNet\n13": "12. Wang, K., Peng, X., Yang,\nJ., Meng, D., & Qiao, Y.\n(2020). Region At-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tention Networks\nfor\nPose\nand Occlusion\nRobust\nFacial\nExpression\nRecog-"
        },
        {
          "Re-annotating AﬀectNet\n13": "nition.\nIEEE\nTransactions\non\nImage\nProcessing,\n29(February),\n4057–4069."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/TIP.2019.2956143"
        },
        {
          "Re-annotating AﬀectNet\n13": "13. Mostafa, A., El-Sayed, H., & Belal, M. (2021). Facial Expressions Recognition Via"
        },
        {
          "Re-annotating AﬀectNet\n13": "CNNCraft-net for Static RGB Images. International Journal of Intelligent Engineer-"
        },
        {
          "Re-annotating AﬀectNet\n13": "ing and Systems, 14(4), 410–421. https://doi.org/10.22266/ijies2021.0831.36"
        },
        {
          "Re-annotating AﬀectNet\n13": "14. Derya, D., Kang, J., Kwon, D. Y., & Wallraven, C. (2019). Facial Expression Pro-"
        },
        {
          "Re-annotating AﬀectNet\n13": "cessing Is Not Aﬀected by Parkinson’s Disease, but by Age-Related Factors. Fron-"
        },
        {
          "Re-annotating AﬀectNet\n13": "tiers in Psychology, 10(November), 1–14. https://doi.org/10.3389/fpsyg.2019.02458"
        },
        {
          "Re-annotating AﬀectNet\n13": "15. Mollahosseini,\nA.,\nHasani,\nB.,\n&\nMahoor,\nM.\nH.\n(2019).\nAﬀectNet:"
        },
        {
          "Re-annotating AﬀectNet\n13": "A\nDatabase\nfor\nFacial\nExpression,\nValence,\nand\nArousal\nComputing\nin"
        },
        {
          "Re-annotating AﬀectNet\n13": "the\nWild.\nIEEE\nTransactions\non\nAﬀective\nComputing,\n10(1),\n18–31."
        },
        {
          "Re-annotating AﬀectNet\n13": "https://doi.org/10.1109/TAFFC.2017.2740923"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "16. Albanie, S., Nagrani, A., Vedaldi, A., & Zisserman, A. (2018). Emotion recognition",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": "ACM Multimedia Conference, 292–301. https://doi.org/10.1145/3240508.3240578"
        },
        {
          "14": "17. Castillo,",
          "Kim and Wallraven": "S., Wallraven,\nC.\nand\nCunningham,"
        },
        {
          "14": "",
          "Kim and Wallraven": "space\nfor\nfacial\ncommunication. Comp. Anim. Virtual Worlds,"
        },
        {
          "14": "",
          "Kim and Wallraven": "https://doi.org/10.1002/cav.1593"
        },
        {
          "14": "18. Barrett, L. F., Adolphs, R., Marsella, S., Martinez, A. M., & Pollak, S. D. (2019).",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": "Emotional Expressions Reconsidered: Challenges"
        },
        {
          "14": "",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": "https://doi.org/10.1177/1529100619832930"
        },
        {
          "14": "19. Northcutt, C. G., Athalye, A., & Mueller, J. (2021). Pervasive Label Errors in Test",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": "Sets Destabilize Machine Learning Benchmarks. http://arxiv.org/abs/2103.14749"
        },
        {
          "14": "20. He,",
          "Kim and Wallraven": "K.,\nZhang,\nX.,\nRen,\nS.,\n&\nSun,\nJ."
        },
        {
          "14": "",
          "Kim and Wallraven": "for\nimage\nrecognition.\nProceedings\nof\nthe"
        },
        {
          "14": "",
          "Kim and Wallraven": "ence\non Computer Vision\nand Pattern Recognition,"
        },
        {
          "14": "",
          "Kim and Wallraven": "https://doi.org/10.1109/CVPR.2016.90"
        },
        {
          "14": "21. Bradley MM, Lang PJ. Measuring emotion: the Self-Assessment Manikin and the",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": ""
        },
        {
          "14": "",
          "Kim and Wallraven": "10.1016/0005-7916(94)90063-9. PMID: 7962581."
        },
        {
          "14": "22. Lang,",
          "Kim and Wallraven": "P.\nJ.\n(1995).\nThe\nEmotion\nProbe:"
        },
        {
          "14": "",
          "Kim and Wallraven": "tention. American\nPsychologist,\n50(5),\n372–385."
        },
        {
          "14": "",
          "Kim and Wallraven": "066X.50.5.372"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops, CVPRW 2010",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "2",
      "title": "Frame Attention Networks for Facial Expression Recognition in Videos",
      "authors": [
        "D Meng",
        "X Peng",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "Proceedings -International Conference on Image Processing",
      "doi": "10.1109/ICIP.2019.8803603"
    },
    {
      "citation_id": "3",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks"
    },
    {
      "citation_id": "4",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "ICMI 2016 -Proceedings of the 18th ACM International Conference on Multimodal Interaction",
      "doi": "10.1145/2993148.2993165"
    },
    {
      "citation_id": "5",
      "title": "Pyramid with Super Resolution for In-the-Wild Facial Expression Recognition",
      "authors": [
        "T Vo",
        "G Lee",
        "H Yang",
        "S Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3010018"
    },
    {
      "citation_id": "6",
      "title": "Who said what: Modeling individual labelers improves classification. 32nd AAAI Conference on Artificial Intelligence",
      "authors": [
        "M Guan",
        "V Gulshan",
        "A Dai",
        "G Hinton"
      ],
      "year": "2018",
      "venue": "Who said what: Modeling individual labelers improves classification. 32nd AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "The contribution of different facial regions to the recognition of conversational expressions",
      "authors": [
        "M Nusseck",
        "D Cunningham",
        "C Wallraven",
        "H Bülthoff"
      ],
      "year": "2008",
      "venue": "Journal of Vision",
      "doi": "10.1167/8.8.1"
    },
    {
      "citation_id": "8",
      "title": "Is There Universal Recognition of Emotion From Facial Expression? A Review of the Cross-Cultural Studies",
      "authors": [
        "J Russell"
      ],
      "year": "1992",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "9",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "H Zhou",
        "D Meng",
        "Y Zhang",
        "X Peng",
        "J Du",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "ICMI 2019 -Proceedings of the 2019 International Conference on Multimodal Interaction",
      "doi": "10.1145/3340555.3355713"
    },
    {
      "citation_id": "10",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Luc Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        ". Bengio"
      ],
      "year": "2015",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "11",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "12",
      "title": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2019.2956143"
    },
    {
      "citation_id": "13",
      "title": "Facial Expressions Recognition Via CNNCraft-net for Static RGB Images",
      "authors": [
        "A Mostafa",
        "H El-Sayed",
        "M Belal"
      ],
      "year": "2021",
      "venue": "International Journal of Intelligent Engineering and Systems",
      "doi": "10.22266/ijies2021.0831.36"
    },
    {
      "citation_id": "14",
      "title": "Facial Expression Processing Is Not Affected by Parkinson's Disease, but by Age-Related Factors",
      "authors": [
        "D Derya",
        "J Kang",
        "D Kwon",
        "C Wallraven"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2019.02458"
    },
    {
      "citation_id": "15",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "MM 2018 -Proceedings of the 2018 ACM Multimedia Conference",
      "doi": "10.1145/3240508.3240578"
    },
    {
      "citation_id": "17",
      "title": "The semantic space for facial communication",
      "authors": [
        "S Castillo",
        "C Wallraven",
        "D Cunningham"
      ],
      "year": "2014",
      "venue": "Comp. Anim. Virtual Worlds",
      "doi": "10.1002/cav.1593"
    },
    {
      "citation_id": "18",
      "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest",
      "doi": "10.1177/1529100619832930"
    },
    {
      "citation_id": "19",
      "title": "Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks",
      "authors": [
        "C Northcutt",
        "A Athalye",
        "J Mueller"
      ],
      "year": "2021",
      "venue": "Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "21",
      "title": "Measuring emotion: the Self-Assessment Manikin and the Semantic Differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "J Behav Ther Exp Psychiatry",
      "doi": "10.1016/0005-7916(94)90063-9"
    },
    {
      "citation_id": "22",
      "title": "The Emotion Probe: Studies of Motivation and Attention",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "American Psychologist",
      "doi": "10.1037/0003-066X.50.5.372"
    }
  ]
}