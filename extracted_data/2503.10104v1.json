{
  "paper_id": "2503.10104v1",
  "title": "Mamba-Va: A Mamba-Based Approach For Continuous Emotion Recognition In Valence-Arousal Space",
  "published": "2025-03-13T07:02:07Z",
  "authors": [
    "Yuheng Liang",
    "Zheyu Wang",
    "Feng Liu",
    "Mingzhou Liu",
    "Yu Yao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Continuous Emotion Recognition (CER) plays a crucial role in intelligent human -computer interaction, mental health monitoring, and autonomous driving. Emotion modeling based on the Valence-Arousal (VA) space enables a more nuanced representation of emotional states. However, existing methods still face challenges in handling long-term dependencies and capturing complex temporal dynamics. To address these issues, this paper proposes a novel emotion recognition model, Mamba-VA, which leverages the Mamba architecture to efficiently model sequential emotional variations in video frames. First, the model employs a Masked Autoencoder (MAE) to extract deep visual features from video frames, enhancing the robustness of temporal information. Then, a Temporal Convolutional Network (TCN) is utilized for temporal modeling to capture local temporal dependencies. Subsequently, Mamba is applied for long-sequence modeling, enabling the learning of global emotional trends. Finally, a fully connected (FC) layer performs regression analysis to predict continuous valence and arousal values. Experimental results on the Valence-Arousal (VA) Estimation task of the 8th competition on Affective Behavior Analysis in-the-wild (ABAW) demonstrate that the proposed model achieves valence and arousal scores of 0.5362 (0.5036) and 0.4310 (0.4119) on the validation (test) set, respectively, outperforming the baseline. The source code is available on GitHub: https://github.com/FreedomPuppy77/Charon.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing, an essential research direction in computer vision and human-computer interaction (HCI), aims to equip computers with the ability to understand and recognize human emotions  [24] . Among various emotion recognition tasks, video-based emotion recognition has gained significant attention in recent years due to its nonintrusive nature and the richness of visual information  [25] .\n\nTraditional emotion recognition methods predominantly rely on discrete emotion classification (e.g., happiness, anger, sadness). However, given the continuous and complex nature of human emotions, such approaches have limitations in practical applications. In contrast, continuous emotion recognition (CER) based on the valencearousal (VA) space offers a more nuanced representation of emotional states employing two continuous variables  [27] . Within this framework, valence (V) describes the positivity or negativity of an emotion, while arousal (A) quantifies its intensity (e.g., calm versus excited). This methodology has demonstrated significant value in various application domains, including intelligent human-computer interaction, mental health monitoring, autonomous driving, and intelligent education  [28] .\n\nDespite the remarkable progress achieved through deep learning techniques in affective computing, accurately modeling the temporal evolution of emotional states remains a significant challenge due to the strong temporal dependencies and complex dynamic variations inherent in emotional signals.\n\nAffective Behavior Analysis in-the-wild (ABAW) Workshop  [6-12, 14-17, 19-21] will spotlight cutting-edge advancements in analyzing, generating, modeling, and understanding human affect and behavior across multiple modal-ities, including facial expressions, body movements, gestures and speech. A special emphasis is placed on the integration of state-of-the-art systems designed for in-thewild analysis, enabling research and applications in unconstrained environments.\n\nThe 8th ABAW  [13, 22]  Valence-Arousal (VA) Estimation Challenge.This challenge focuses on predicting two continuous affect dimensions -valence (ranging from -1 to 1, representing positivity to negativity) and arousal (ranging from -1 to 1,representing activity levels from passive to active). This challenge utilizes an augmented version of the audiovisual (A/V) and in-the-wild Aff-Wild2  [7, 10, 18]  database, which includes 594 videos with approximately 3 million frames from 584 subjects.\n\nDeep learning-based valence-arousal (VA) estimation faces several key challenges.Long-term temporal dependency: Traditional recurrent neural networks (RNNs) and long short-term memory (LSTM) networks suffer from the vanishing gradient problem when modeling long-range emotional dependencies, making it difficult to effectively capture extended emotional dynamics  [5] .Balancing local and global temporal modeling: While convolutional neural networks (CNNs) and temporal convolutional networks (TCNs) effectively capture short-term dynamic features, they lack the capability to model global temporal information, thereby limiting the robustness of emotion recognition  [1] .Trade-off between computational efficiency and accuracy: Transformer-based models and their variants have demonstrated strong performance in VA estimation tasks  [29] ; however, their high computational complexity poses challenges for efficient application in long-sequence tasks.\n\nTo address these challenges, researchers have proposed various methods to enhance continuous emotion recognition performance. LSTM networks have been widely adopted for temporal emotion modeling, yet their recurrent structure results in high computational costs and difficulties in capturing long-range dependencies  [2] . On the other hand, Transformer models and their variants, such as TimeSformer, leverage self-attention mechanisms to enhance long-term temporal modeling but suffer from high computational overhead  [25] . TCN, as a convolution-based temporal modeling approach, offers superior computational efficiency compared to RNNs and LSTMs; however, its ability to model long-range dependencies remains insufficient  [23] .\n\nRecently, Mamba, a novel state space model (SSM), has demonstrated superior performance in long-sequence modeling tasks  [4] . Compared to Transformers, Mamba exhibits higher computational efficiency, and compared to RNNs, it provides more stable gradient propagation, making it wellsuited for long-sequence modeling. Therefore, we hypothesize that integrating the Mamba architecture can effec-tively enhance the temporal modeling capability of VAbased emotion recognition.To address the aforementioned challenges, this paper proposes Mamba-VA, a novel continuous emotion recognition method based on the Mamba architecture. The key contributions of this work are as follows:\n\n(1) Extract of high-dimensional visual features: A masked autoencoder (MAE) is employed to extract highlevel visual representations from video frames, enhancing the model's ability to encode video data.\n\n(2) Efficient and stable temporal modeling: A TCN is utilized for short-term temporal modeling, while Mamba is employed for long-term dependency modeling, ensuring both computational efficiency and stability.\n\n(3) Extensive evaluation on Aff-Wild2: Comprehensive experiments on the Aff-Wild2 dataset validate the strong generalization capability of Mamba-VA in continuous emotion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In previous ABAW VA estimation challenges, several research teams have proposed innovative approaches to enhance emotion recognition performance.Netease Fuxi AI Lab  [31]  developed a Transformer-based feature fusion module to comprehensively integrate emotional information from audio signals, visual images, and text, thereby providing high-quality facial features for downstream tasks. To achieve high-quality facial feature representations, they employed a masked autoencoder (MAE) as the visual feature extraction model and fine-tuned it using facial datasets. Considering the complexity of video capture scenarios, they further refined the dataset based on scene characteristics and trained classifiers tailored to each scene.\n\nRJCMA  [32]  computed attention weights based on the cross-correlation between multimodal joint feature representations and individual modality feature representations, aiming to capture both intra-modal and inter-modal relationships simultaneously. In the recurrent mechanism, individual modality representations were reintroduced into the fusion model as input to obtain more fine-grained feature representations.\n\nCtyunAI  [3]  pre-trained a masked autoencoder (MAE) on facial datasets and fine-tuned it on the Aff-Wild2 dataset annotated with expression (Expr) labels. Additionally, they integrated TCN and Transformer encoders into the framework to enhance emotion recognition performance. SUN-CE  [26]  explored the effectiveness of fine-tuned convolutional neural networks (CNNs) and the public dimensional emotion model (PDEM) for video and audio modalities. They also compared different temporal modeling and fusion strategies by embedding these modalityspecific deep neural networks (DNNs) within multi-level training frameworks. USTC-IAT-United  [30]  proposed the LA-SE module to better capture local image details while enhancing channel selection and suppression capabilities. They also employed a multimodal data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and Transformerbased spatiotemporal feature modeling.\n\nInspired by prior research, we propose a novel emotion recognition framework that leverages MAE for highdimensional visual feature extraction from video frames. Furthermore, we design a model comprising a four-layer TCN and Mamba architecture to enhance the effectiveness of emotion recognition in continuous valence-arousal estimation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section, we will provide a detailed introduction to the proposed method for the Valence-Arousal Estimation challenge in the 8th ABAW competition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Feature Extraction",
      "text": "Inspired by prior works, we employ a Masked Autoencoder (MAE) as the visual feature extractor for video frames to leverage its generalizable visual representations learned through large-scale unsupervised pre-training. Unlike conventional approaches, we initialize the model with ViT-Large pre-trained weights to enhance feature representation capability and implement a parameter freezing strategy to improve generalization performance under limited labeled data.\n\nSpecifically, we freeze the parameters of the Patch Embedding layer and the first 16 Transformer Blocks while only fine-tuning the higher-level Transformer Blocks. Since the MAE has acquired rich low-level visual primitives (e.g., edges, textures, and local structures) in its early layers through large-scale unsupervised pre-training, this partial freezing strategy significantly reduces the number of trainable parameters compared to full-network fine-tuning. Such design enables more stable convergence and lower overfitting risks in data-scarce scenarios. For emotion recognition tasks requiring adaptation to facial expressions, body poses, and temporal variations, updating higher Transformer Blocks allows the model to learn task-discriminative high-level features. The CLS token embeddings, serving as global visual representations, are subsequently fed into temporal modeling modules for capturing dynamic information.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Convolutional Network",
      "text": "Videos are first split into segments with a window size w and stride s.Given the segment window w and stride s, a video with n frames would be split into ⌊n/s⌋ + 1 segments, where the i-th segment contains frames {F (i-1) * s+1 , . . . , F (i-1) * s+w }.\n\nThe video is segmented into a series of overlapping clips, each containing a fixed number of consecutive frames. This processing approach aims to decompose the original video into smaller temporal units for efficient computational processing and analytical tasks. Crucially, the overlapping intervals between adjacent clips (typically defined by frame offsets) ensure continuous temporal coverage, thereby preserving the integrity of the visual data throughout the entire sequence.\n\nWe denote visual features as f i corresponding to the i-th segment extracted by fine-tuned ViT-Large encoder.\n\nVisual feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:\n\nThe TCN with four hierarchical layers is employed for temporal modeling of visual features. In this architecture, the convolutional neural network processes input visual feature vectors through a sequence of convolutional layers characterized by varying kernel sizes and dilation rates. This multi-scale convolution operation generates output feature vectors with distinct dimensionality from the input, achieving feature space compression while effectively aggregating temporal contextual information across different receptive fields.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mamba Encoder",
      "text": "In the task of affective computing, effectively modeling temporal information is crucial for accurately predicting Valence-Arousal (VA) values. To achieve this, we incorporate the Mamba module for sequential modeling, building upon the temporal features extracted by the TCN. This process can be formally expressed as:\n\nMamba is an efficient sequence modeling architecture based on the State Space Model (SSM). Compared to traditional Transformer architectures, Mamba offers lower computational complexity while demonstrating superior performance in long-sequence modeling.\n\nAfter being processed by the TCN, the global interaction features g i are fed into the Mamba Encoder,where they first undergo dimensional reorganization to align the channel and sequence dimensions,ensuring compatibility with standard sequential modeling formats.Subsequently,within the four-layer cascaded MambaBlock,the features at each time step activate a selective state-space mechanism-dynamically generating a state transition matrix based on the current input.This process leverages a hardware-optimized parallel scan algorithm to efficiently model long-range dependencies,while local convolutional kernels capture transient patterns in neighboring time points.\n\nFurthermore,residual connections and layer normalization ensure stable gradient propagation.Finally,a linear projection maps the latent states into a bimodal affective space, yielding a continuous emotion trajectory.By employing an input-dependent parameter selection mechanism,Mamba adaptively focuses on emotion-relevant segments,while its linear computational complexity allows it to maintain global awareness over long sequences while enabling fine-grained analysis of subtle affective fluctuations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "This Challenge's dataset comprises 594 videos, an expansion of the Aff-Wild2 database, annotated in terms of valence and arousal. Notably, sixteen videos feature two subjects, both of whom are annotated. In total,annotations are provided for 2,993,081 frames from 584 subjects; these annotations have been conducted by four experts using the methodology outlined in  [8] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "All models were trained on two Nvidia GeForce GTX 4090 GPUs with each having 24GB of memory.In the hyperparameter settings, we set b the number of multi-head attention to 4, the number of Mamba layers to 4, the kernel size to 15, the output feature dimension to 256, the state dimension to 8, the convolutional channel dimension to 4, and the expansion factor to 1. Additionally, we attempted to utilize a pretrained model to enhance feature extraction capabilities, but this feature was not enabled in this experiment.\n\nFor training, we employed AdamW as the optimizer and adopted the Concordance Correlation Coefficient (CCC) loss as the objective function to more accurately measure the consistency between predicted and actual emotion values. The training process was conducted over 50 epochs, with an initial learning rate of 0.0003, and a 5-epoch warmup strategy was applied to improve training stability. We also set the weight decay to 0.001 to prevent overfitting. Furthermore, we incorporated a Dropout mechanism during training, setting its rate to 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "The performance metric for VA challenge is the average CCC for valence and arousal, defined as:\n\nThe CCC measures the agreement between two time series (e.g., annotations and predictions) while penalizing deviations in both correlation and mean squared differences. CCC values range from -1 to 1, where +1 indicates perfect agreement, 0 indicates no correlation, and -1 indicates complete discordance. A higher CCC value signifies better alignment between predictions and ground-truth annotations, and therefore high values are desired. The CCC formula is defined as follows:\n\nwhere ρ xy is the Pearson correlation coefficient, s x and s y represent the variances of the valence / arousal annotations and the predicted values, respectively, and s xy is the corresponding covariance value.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  1  presents the experimental results of our proposed method on the validation set for the Valence-Arousal (VA) prediction task, where the CCC is used as the evaluation metric for both valence and arousal predictions. Specifically, Fold 0 corresponds to the results on the official validation set, while Folds 1-5 represent the results obtained through five-fold cross-validation. As shown in the results, our proposed method consistently outperforms the baseline approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes Mamba-VA, a novel continuous emotion recognition model based on Mamba, designed to effectively model temporal emotional variations in video sequences. Our approach first employs a Masked Autoencoder (MAE) to extract high-dimensional visual features from video frames, enhancing the model's representation capability for video data. Then, a Temporal Convolutional Network (TCN) is utilized for sequential modeling to capture local temporal dependencies, followed by Mamba for long-sequence modeling, allowing the model to learn global emotional trends effectively. Experimental results demonstrate that Mamba-VA outperforms baseline methods in the 8th ABAW Valence-Arousal Estimation Challenge, verifying its superiority in long-term emotion modeling tasks.\n\nCompared to traditional methods, Mamba-VA integrates the advantages of CNN, TCN, and Mamba, achieving an efficient balance between computational efficiency and the ability to capture long-range dependencies in emotional states. The results on the Aff-Wild2 dataset confirm the strong generalization capability of our model, enabling stable emotion prediction in complex real-world scenarios. Additionally, our research further validates the potential of Mamba in long-sequence modeling tasks, providing an efficient and robust solution for continuous emotion recognition.\n\nIn the future, we plan to further optimize Mamba-VA by exploring multimodal fusion (e.g., audio, text, and physiological signals) to enhance emotion recognition performance. Additionally, we aim to refine training strategies to further improve the robustness and generalization ability of the model, making it more applicable to intelligent human-computer interaction, mental health monitoring, autonomous driving, and other real-world applications.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Computer and Artiﬁcial Intelligence": "wangzheyu@njust.edu.cn"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "Abstract"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "Continuous Emotion Recognition (CER) plays a crucial"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "role\nin\nintelligent\nhuman-computer\ninteraction, mental"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "health monitoring, and autonomous driving. Emotion mod-"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "eling based on the Valence-Arousal\n(VA) space enables a"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "more nuanced representation of emotional states. However,"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "existing methods still face challenges in handling long-term"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "dependencies and capturing complex temporal dynamics."
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "To\naddress\nthese\nissues,\nthis\npaper\nproposes\na\nnovel"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "emotion recognition model, Mamba-VA, which leverages"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "the Mamba\narchitecture\nto\nefﬁciently model\nsequential"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "emotional\nvariations\nin video frames.\nFirst,\nthe model"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "employs a Masked Autoencoder\n(MAE)\nto\nextract\ndeep"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "visual features from video frames, enhancing the robustness"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "of\ntemporal\ninformation. Then, a Temporal Convolutional"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "Network (TCN) is utilized for temporal modeling to capture"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "local\ntemporal\ndependencies.\nSubsequently, Mamba\nis"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "applied for long-sequence modeling, enabling the learning"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "of global emotional trends. Finally, a fully connected (FC)"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "layer performs\nregression analysis\nto predict\ncontinuous"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "valence and arousal values.\nExperimental results on the"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "Valence-Arousal\n(VA) Estimation task of\nthe 8th compe-"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "tition on Affective Behavior Analysis\nin-the-wild (ABAW)"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "demonstrate that the proposed model achieves valence and"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "arousal\nscores\nof\n0.5362 (0.5036) and 0.4310 (0.4119)"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "on\nthe\nvalidation (test)\nset,\nrespectively,\noutperforming"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "the baseline.\nThe\nsource\ncode\nis available on GitHub:"
        },
        {
          "School of Computer and Artiﬁcial Intelligence": "https://github.com/FreedomPuppy77/Charon."
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        },
        {
          "School of Computer and Artiﬁcial Intelligence": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ities,\nincluding facial expressions, body movements, ges-": "tures and speech.\nA special\nemphasis\nis placed on the",
          "tively enhance the\ntemporal modeling capability of VA-": "based emotion recognition.To address the aforementioned"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "integration of state-of-the-art systems designed for in-the-",
          "tively enhance the\ntemporal modeling capability of VA-": "challenges,\nthis paper proposes Mamba-VA, a novel con-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "wild analysis, enabling research and applications in uncon-",
          "tively enhance the\ntemporal modeling capability of VA-": "tinuous emotion recognition method based on the Mamba"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "strained environments.",
          "tively enhance the\ntemporal modeling capability of VA-": "architecture. The key contributions of this work are as fol-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "The 8th ABAW [13, 22] Valence-Arousal (VA) Estima-",
          "tively enhance the\ntemporal modeling capability of VA-": "lows:"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "tion Challenge.This challenge focuses on predicting two",
          "tively enhance the\ntemporal modeling capability of VA-": "(1) Extract\nof\nhigh-dimensional\nvisual\nfeatures:\nA"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "continuous affect dimensions —valence (ranging from -1 to",
          "tively enhance the\ntemporal modeling capability of VA-": "masked autoencoder\n(MAE)\nis employed to extract high-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "1,\nrepresenting positivity to negativity) and arousal (rang-",
          "tively enhance the\ntemporal modeling capability of VA-": "level visual\nrepresentations from video frames, enhancing"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "ing from -1 to 1,representing activity levels from passive",
          "tively enhance the\ntemporal modeling capability of VA-": "the model’s ability to encode video data."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "to active). This challenge utilizes an augmented version of",
          "tively enhance the\ntemporal modeling capability of VA-": "(2) Efﬁcient and stable temporal modeling: A TCN is"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "the audiovisual (A/V) and in-the-wild Aff-Wild2 [7, 10, 18]",
          "tively enhance the\ntemporal modeling capability of VA-": "utilized for\nshort-term temporal modeling, while Mamba"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "database, which includes 594 videos with approximately 3",
          "tively enhance the\ntemporal modeling capability of VA-": "is employed for long-term dependency modeling, ensuring"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "million frames from 584 subjects.",
          "tively enhance the\ntemporal modeling capability of VA-": "both computational efﬁciency and stability."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "(3) Extensive evaluation on Aff-Wild2: Comprehensive"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "Deep\nlearning-based valence-arousal\n(VA)\nestimation",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "experiments on the Aff-Wild2 dataset validate the strong"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "faces\nseveral key challenges.Long-term temporal depen-",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "generalization capability of Mamba-VA in continuous emo-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "dency: Traditional\nrecurrent neural networks (RNNs) and",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "tion"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "long\nshort-term memory\n(LSTM)\nnetworks\nsuffer\nfrom",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "the vanishing gradient problem when modeling long-range",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "2. Related Work"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "emotional dependencies, making it difﬁcult\nto effectively",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "capture extended emotional dynamics [5].Balancing local",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "In previous ABAW VA estimation challenges, several\nre-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "and global\ntemporal modeling: While convolutional neu-",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "search teams have proposed innovative approaches to en-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "ral networks (CNNs) and temporal convolutional networks",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "hance emotion recognition performance.Netease Fuxi AI"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "(TCNs)\neffectively capture\nshort-term dynamic\nfeatures,",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "Lab\n[31]\ndeveloped a Transformer-based feature\nfusion"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "they lack the\ncapability to model global\ntemporal\ninfor-",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "module to comprehensively integrate emotional\ninforma-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "mation,\nthereby limiting the robustness of emotion recog-",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "tion from audio signals, visual\nimages, and text,\nthereby"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "nition [1].Trade-off between computational efﬁciency and",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "providing high-quality facial features for downstream tasks."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "accuracy:\nTransformer-based models\nand\ntheir\nvariants",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "To achieve high-quality facial feature representations,\nthey"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "have demonstrated strong performance in VA estimation",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "employed a masked autoencoder (MAE) as the visual fea-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "their high computational complexity\ntasks [29]; however,",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "ture extraction model and ﬁne-tuned it using facial datasets."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "poses challenges for efﬁcient application in long-sequence",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "Considering the complexity of video capture scenarios, they"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "tasks.",
          "tively enhance the\ntemporal modeling capability of VA-": ""
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "",
          "tively enhance the\ntemporal modeling capability of VA-": "further reﬁned the dataset based on scene characteristics and"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "To address these challenges, researchers have proposed",
          "tively enhance the\ntemporal modeling capability of VA-": "trained classiﬁers tailored to each scene."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "various methods\nto\nenhance\ncontinuous\nemotion recog-",
          "tively enhance the\ntemporal modeling capability of VA-": "RJCMA [32] computed attention weights based on the"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "nition performance.\nLSTM networks have been widely",
          "tively enhance the\ntemporal modeling capability of VA-": "cross-correlation between multimodal\njoint\nfeature repre-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "adopted for\ntemporal emotion modeling, yet\ntheir\nrecur-",
          "tively enhance the\ntemporal modeling capability of VA-": "sentations and individual modality feature representations,"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "rent structure results in high computational costs and dif-",
          "tively enhance the\ntemporal modeling capability of VA-": "aiming to capture both intra-modal and inter-modal\nrela-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "ﬁculties in capturing long-range dependencies [2]. On the",
          "tively enhance the\ntemporal modeling capability of VA-": "tionships simultaneously. In the recurrent mechanism, indi-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "other hand, Transformer models and their variants,\nsuch",
          "tively enhance the\ntemporal modeling capability of VA-": "vidual modality representations were reintroduced into the"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "as TimeSformer, leverage self-attention mechanisms to en-",
          "tively enhance the\ntemporal modeling capability of VA-": "fusion model as input\nto obtain more ﬁne-grained feature"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "hance long-term temporal modeling but\nsuffer\nfrom high",
          "tively enhance the\ntemporal modeling capability of VA-": "representations."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "computational overhead [25]. TCN, as a convolution-based",
          "tively enhance the\ntemporal modeling capability of VA-": "CtyunAI\n[3] pre-trained a masked autoencoder (MAE)"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "temporal modeling approach, offers superior computational",
          "tively enhance the\ntemporal modeling capability of VA-": "on facial datasets and ﬁne-tuned it on the Aff-Wild2 dataset"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "efﬁciency compared to RNNs\nand LSTMs;\nhowever,\nits",
          "tively enhance the\ntemporal modeling capability of VA-": "annotated with expression (Expr) labels. Additionally, they"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "ability to model\nlong-range dependencies remains insufﬁ-",
          "tively enhance the\ntemporal modeling capability of VA-": "integrated TCN and Transformer encoders into the frame-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "cient [23].",
          "tively enhance the\ntemporal modeling capability of VA-": "work to enhance emotion recognition performance."
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "Recently, Mamba, a novel state space model (SSM), has",
          "tively enhance the\ntemporal modeling capability of VA-": "SUN-CE [26] explored the effectiveness of ﬁne-tuned"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "demonstrated superior performance in long-sequence mod-",
          "tively enhance the\ntemporal modeling capability of VA-": "convolutional neural networks (CNNs) and the public di-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "eling tasks [4]. Compared to Transformers, Mamba exhibits",
          "tively enhance the\ntemporal modeling capability of VA-": "mensional\nemotion model\n(PDEM)\nfor video and audio"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "higher computational efﬁciency, and compared to RNNs,\nit",
          "tively enhance the\ntemporal modeling capability of VA-": "modalities.\nThey also compared different\ntemporal mod-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "provides more stable gradient propagation, making it well-",
          "tively enhance the\ntemporal modeling capability of VA-": "eling and fusion strategies by embedding these modality-"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "suited for long-sequence modeling. Therefore, we hypoth-",
          "tively enhance the\ntemporal modeling capability of VA-": "speciﬁc deep neural networks\n(DNNs) within multi-level"
        },
        {
          "ities,\nincluding facial expressions, body movements, ges-": "esize\nthat\nintegrating the Mamba\narchitecture can effec-",
          "tively enhance the\ntemporal modeling capability of VA-": "training frameworks."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "better capture local image details while enhancing channel",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "{F(i−1)∗s+1, . . . , F(i−1)∗s+w}."
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "selection and suppression capabilities. They also employed",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "The video is segmented into a series of overlapping clips,"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "a multimodal data fusion approach,\nintegrating pre-trained",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "each containing a ﬁxed number of consecutive frames. This"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "audio and video backbones for feature extraction, followed",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "processing approach aims to decompose the original video"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "by TCN-based spatiotemporal encoding and Transformer-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "into smaller temporal units for efﬁcient computational pro-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "based spatiotemporal feature modeling.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "cessing and analytical tasks. Crucially,\nthe overlapping in-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Inspired by prior\nresearch, we propose a novel emo-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "tervals between adjacent clips (typically deﬁned by frame"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "tion recognition framework that\nleverages MAE for high-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "offsets) ensure continuous temporal coverage, thereby pre-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "dimensional visual\nfeature extraction from video frames.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "serving the integrity of the visual data throughout the entire"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Furthermore, we design a model comprising a four-layer",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "sequence."
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "TCN and Mamba architecture to enhance the effectiveness",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "We denote visual features as fi corresponding to the i-th"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "of emotion recognition in continuous valence-arousal esti-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "segment extracted by ﬁne-tuned ViT-Large encoder."
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "mation.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "Visual feature is fed into a dedicated Temporal Convolu-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "tional Network (TCN) for temporal encoding, which can be"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "formulated as follows:"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "3. Method",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "In this section, we will provide a detailed introduction to the",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "(1)\ngi = TCN(fi)"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "proposed method for the Valence-Arousal Estimation chal-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "lenge in the 8th ABAW competition.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "The TCN with four hierarchical\nlayers is employed for"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "temporal modeling of visual\nfeatures.\nIn this\narchitec-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "3.1. Visual Feature Extraction",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "ture,\nthe convolutional neural network processes input vi-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "sual\nfeature vectors\nthrough a sequence of convolutional"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Inspired by prior works, we\nemploy a Masked Autoen-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "layers characterized by varying kernel\nsizes and dilation"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "coder (MAE) as the visual feature extractor for video frames",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "rates. This multi-scale convolution operation generates out-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "to leverage its generalizable visual representations learned",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "put feature vectors with distinct dimensionality from the in-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "through large-scale unsupervised pre-training. Unlike con-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "put, achieving feature space compression while effectively"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "ventional approaches, we\ninitialize\nthe model with ViT-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "aggregating temporal contextual information across differ-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Large pre-trained weights to enhance feature representation",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "ent receptive ﬁelds."
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "capability and implement a parameter freezing strategy to",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "improve generalization performance under limited labeled",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "3.3. Mamba Encoder"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "data.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "In the\ntask of\naffective computing,\neffectively modeling"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Speciﬁcally, we freeze the parameters of the Patch Em-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "temporal\ninformation is\ncrucial\nfor accurately predicting"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "bedding layer and the ﬁrst 16 Transformer Blocks while",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "Valence-Arousal (VA) values. To achieve this, we incorpo-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "only ﬁne-tuning the higher-level Transformer Blocks. Since",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "rate the Mamba module for sequential modeling, building"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "the MAE has acquired rich low-level visual primitives (e.g.,",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "upon the temporal features extracted by the TCN. This pro-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "edges,\ntextures,\nand local\nstructures)\nin its\nearly layers",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "cess can be formally expressed as:"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "through large-scale unsupervised pre-training,\nthis partial",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "freezing strategy signiﬁcantly reduces the number of train-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "(2)\nmi = Mamba(gi)"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "able parameters compared to full-network ﬁne-tuning. Such",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "design enables more stable convergence and lower overﬁt-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "Mamba is an efﬁcient sequence modeling architecture"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "ting risks in data-scarce scenarios.\nFor emotion recogni-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "based on the State Space Model (SSM). Compared to tradi-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "tion tasks requiring adaptation to facial expressions, body",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "tional Transformer architectures, Mamba offers lower com-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "poses,\nand\ntemporal\nvariations,\nupdating\nhigher Trans-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "putational complexity while demonstrating superior perfor-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "former Blocks allows the model to learn task-discriminative",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "mance in long-sequence modeling."
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "high-level\nfeatures.\nThe CLS token embeddings, serving",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "After being processed by the TCN,\nthe global\ninterac-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "as global visual\nrepresentations, are subsequently fed into",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "tion features gi are fed into the Mamba Encoder,where they"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "temporal modeling modules for capturing dynamic infor-",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "ﬁrst undergo dimensional reorganization to align the chan-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "mation.",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "nel and sequence dimensions,ensuring compatibility with"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "standard sequential modeling formats.Subsequently,within"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "3.2. Temporal Convolutional Network",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": ""
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "the\nfour-layer\ncascaded MambaBlock,the\nfeatures\nat"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "Videos are ﬁrst\nsplit\ninto segments with a window size",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "each\ntime\nstep\nactivate\na\nselective\nstate-space mech-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "w and stride s.Given the segment window w and stride",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "anism—dynamically\ngenerating\na\nstate\ntransition ma-"
        },
        {
          "USTC-IAT-United [30] proposed the LA-SE module to": "split\ns,\na video with n frames would be\ninto ⌊n/s⌋ +",
          "segments,\nwhere\nthe\nsegment\ncontains\nframes\n1\ni-th": "trix based on the\ncurrent\ninput.This process\nleverages a"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: presents the experimentalresults of our proposed",
      "data": [
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "model\nlong-range dependencies,while local convolutional"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "kernels\ncapture\ntransient\npatterns\nin\nneighboring\ntime"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "points."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "Furthermore,residual connections and layer normaliza-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "tion ensure stable gradient propagation.Finally,a linear pro-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "jection maps\nthe\nlatent\nstates\ninto\na\nbimodal\naffective"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "space,\nyielding a\ncontinuous\nemotion trajectory.By em-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ploying\nan\ninput-dependent parameter\nselection mecha-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "nism,Mamba adaptively focuses on emotion-relevant seg-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ments,while its linear computational complexity allows it"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "to maintain global awareness over long sequences while en-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "abling ﬁne-grained analysis of subtle affective ﬂuctuations."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "4. Experiments and Results"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "4.1. Datasets"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "This Challenge’s dataset comprises 594 videos, an expan-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "sion of\nthe Aff-Wild2 database, annotated in terms of va-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "lence and arousal. Notably, sixteen videos feature two sub-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "jects, both of whom are annotated.\nIn total,annotations are"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "provided for 2,993,081 frames from 584 subjects; these an-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "notations have been conducted by four experts using the"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "methodology outlined in [8]. Valence and arousal values"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "are continuous and range in [-1, 1]."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "The dataset is divided into training, validation, and test-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ing sets, ensuring subject independence, meaning each sub-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ject appears in only one set. The splits are: 356 videos in"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "the training set; 76 videos in the Validation set; 162 videos"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "in the testing set."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "4.2. Implementation Details"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "All models were trained on two Nvidia GeForce GTX 4090"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "GPUs with each having 24GB of memory.In the hyperpa-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "rameter settings, we set b the number of multi-head atten-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "tion to 4, the number of Mamba layers to 4,\nthe kernel size"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "to 15, the output feature dimension to 256, the state dimen-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "sion to 8, the convolutional channel dimension to 4, and the"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "expansion factor to 1. Additionally, we attempted to utilize"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "a pretrained model\nto enhance feature extraction capabili-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ties, but this feature was not enabled in this experiment."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "For training, we employed AdamW as the optimizer and"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "adopted the Concordance Correlation Coefﬁcient\n(CCC)"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "loss as the objective function to more accurately measure"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "the consistency between predicted and actual emotion val-"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "ues. The training process was conducted over 50 epochs,"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": ""
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "with\nan\ninitial\nlearning\nrate\nof\n0.0003,\nand\na\n5-epoch"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "warmup strategy was applied to improve training stability."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "We also set the weight decay to 0.001 to prevent overﬁtting."
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "Furthermore, we incorporated a Dropout mechanism during"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "training, setting its rate to 0.3 to further enhance the model’s"
        },
        {
          "hardware-optimized parallel\nscan algorithm to efﬁciently": "generalization ability."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Foundation of China (Grant No.62177029)": ""
        },
        {
          "Foundation of China (Grant No.62177029)": ""
        },
        {
          "Foundation of China (Grant No.62177029)": "References"
        },
        {
          "Foundation of China (Grant No.62177029)": ""
        },
        {
          "Foundation of China (Grant No.62177029)": "[1] S. Bai, J. Kolter, and V. Koltun. An empirical evaluation of"
        },
        {
          "Foundation of China (Grant No.62177029)": "generic convolutional and recurrent networks for sequence"
        },
        {
          "Foundation of China (Grant No.62177029)": "modeling, 2018. arXiv preprint arXiv:1803.01271. 2"
        },
        {
          "Foundation of China (Grant No.62177029)": "[2]\nJ. Chung, C. Gulcehre, K. Cho, et al. Gated feedback re-"
        },
        {
          "Foundation of China (Grant No.62177029)": "current neural networks, 2015.\nIn Proceedings of ICML, pp."
        },
        {
          "Foundation of China (Grant No.62177029)": "2067–2075. 2"
        },
        {
          "Foundation of China (Grant No.62177029)": "[3] Denis\nDresvyanskiy,\nMaxim Markitantov,\nJiawei\nYu,"
        },
        {
          "Foundation of China (Grant No.62177029)": "Peitong Li, Heysem Kaya,\nand Alexey Karpov.\nSun"
        },
        {
          "Foundation of China (Grant No.62177029)": "team’s\ncontribution to abaw 2024 competition:\nAudiovi-"
        },
        {
          "Foundation of China (Grant No.62177029)": "sual valence-arousal estimation and expression recognition,"
        },
        {
          "Foundation of China (Grant No.62177029)": "2024. 2"
        },
        {
          "Foundation of China (Grant No.62177029)": "[4] A. Gu\net\nal.\nMamba:\nLinear-time\nsequence model-"
        },
        {
          "Foundation of China (Grant No.62177029)": "ing with\nselective\nstate\nspaces,\n2023.\narXiv\npreprint"
        },
        {
          "Foundation of China (Grant No.62177029)": "arXiv:2302.06665. 2"
        },
        {
          "Foundation of China (Grant No.62177029)": "[5] S. Hochreiter and J. Schmidhuber. Long short-term memory,"
        },
        {
          "Foundation of China (Grant No.62177029)": "1997. Neural Computation, vol. 9, no. 8, pp. 1735–1780. 2"
        },
        {
          "Foundation of China (Grant No.62177029)": "[6] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-"
        },
        {
          "Foundation of China (Grant No.62177029)": "pression recognition, action unit detection multi-task learn-"
        },
        {
          "Foundation of China (Grant No.62177029)": "ing challenges.\nIn Proceedings of the IEEE/CVF Conference"
        },
        {
          "Foundation of China (Grant No.62177029)": "on Computer Vision and Pattern Recognition, pages 2328–"
        },
        {
          "Foundation of China (Grant No.62177029)": "2336, 2022. 1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "from video frames, enhancing the model’s representation": "capability for video data. Then, a Temporal Convolutional",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "multi-task learning challenges.\nIn European Conference on"
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Computer Vision, pages 157–172. Springer, 2023. 2"
        },
        {
          "from video frames, enhancing the model’s representation": "Network (TCN) is utilized for sequential modeling to cap-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[8] Dimitrios Kollias. Multi-label compound expression recog-"
        },
        {
          "from video frames, enhancing the model’s representation": "ture local\ntemporal dependencies, followed by Mamba for",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "of\nnition:\nC-expr\ndatabase & network.\nIn Proceedings"
        },
        {
          "from video frames, enhancing the model’s representation": "long-sequence modeling, allowing the model to learn global",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "the IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "from video frames, enhancing the model’s representation": "emotional\ntrends effectively. Experimental results demon-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Recognition, pages 5589–5598, 2023."
        },
        {
          "from video frames, enhancing the model’s representation": "strate that Mamba-VA outperforms baseline methods in the",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[9] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "from video frames, enhancing the model’s representation": "8th ABAW Valence-Arousal Estimation Challenge, verify-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "from video frames, enhancing the model’s representation": "ing its superiority in long-term emotion modeling tasks.",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arcface. arXiv preprint arXiv:1910.04855, 2019."
        },
        {
          "from video frames, enhancing the model’s representation": "Compared to traditional methods, Mamba-VA integrates",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[10] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "from video frames, enhancing the model’s representation": "the advantages of CNN, TCN, and Mamba, achieving an",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "from video frames, enhancing the model’s representation": "efﬁcient balance between computational efﬁciency and the",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "from video frames, enhancing the model’s representation": "ability to capture\nlong-range dependencies\nin emotional",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "2"
        },
        {
          "from video frames, enhancing the model’s representation": "states.\nThe results on the Aff-Wild2 dataset conﬁrm the",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[11] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "from video frames, enhancing the model’s representation": "strong generalization capability of our model, enabling sta-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "from video frames, enhancing the model’s representation": "ble\nemotion prediction in complex real-world scenarios.",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "from video frames, enhancing the model’s representation": "Additionally, our research further validates the potential of",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[12] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing"
        },
        {
          "from video frames, enhancing the model’s representation": "Mamba in long-sequence modeling tasks, providing an ef-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "affective behavior\nin the ﬁrst abaw 2020 competition.\nIn"
        },
        {
          "from video frames, enhancing the model’s representation": "ﬁcient and robust solution for continuous emotion recogni-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "2020\n15th\nIEEE International Conference\non Automatic"
        },
        {
          "from video frames, enhancing the model’s representation": "tion.",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Face and Gesture Recognition (FG 2020)(FG), pages 794–"
        },
        {
          "from video frames, enhancing the model’s representation": "In the future, we plan to further optimize Mamba-VA by",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "800,\n. 1"
        },
        {
          "from video frames, enhancing the model’s representation": "exploring multimodal\nfusion (e.g., audio,\ntext, and phys-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[13] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen,\nIrene"
        },
        {
          "from video frames, enhancing the model’s representation": "iological signals)\nto enhance emotion recognition perfor-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Kotsia, UK Cogitat, Eric Granger, Marco Pedersoli, Simon"
        },
        {
          "from video frames, enhancing the model’s representation": "mance. Additionally, we aim to reﬁne training strategies",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Bacon, Alice Baird, Chunchang Shao, et al. Advancements"
        },
        {
          "from video frames, enhancing the model’s representation": "to further\nimprove the robustness and generalization abil-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "in affective and behavior analysis: The 8th abaw workshop"
        },
        {
          "from video frames, enhancing the model’s representation": "ity of\nthe model, making it more applicable to intelligent",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "and competition.\n. 2"
        },
        {
          "from video frames, enhancing the model’s representation": "human-computer interaction, mental health monitoring, au-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[14] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "from video frames, enhancing the model’s representation": "tonomous driving, and other real-world applications.",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "from video frames, enhancing the model’s representation": "Acknowledgements",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arXiv:1910.11111, 2019. 1"
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[15] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "from video frames, enhancing the model’s representation": "This work was supported by the National Natural Science",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "from video frames, enhancing the model’s representation": "Foundation of China (Grant No.62177029)",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "from video frames, enhancing the model’s representation": "References",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": ""
        },
        {
          "from video frames, enhancing the model’s representation": "",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "from video frames, enhancing the model’s representation": "[1] S. Bai, J. Kolter, and V. Koltun. An empirical evaluation of",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "pages 1–23, 2019."
        },
        {
          "from video frames, enhancing the model’s representation": "generic convolutional and recurrent networks for sequence",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[16] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "from video frames, enhancing the model’s representation": "modeling, 2018. arXiv preprint arXiv:1803.01271. 2",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "from video frames, enhancing the model’s representation": "[2]\nJ. Chung, C. Gulcehre, K. Cho, et al. Gated feedback re-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "from video frames, enhancing the model’s representation": "current neural networks, 2015.\nIn Proceedings of ICML, pp.",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arXiv:2105.03790, 2021."
        },
        {
          "from video frames, enhancing the model’s representation": "2067–2075. 2",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[17] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "from video frames, enhancing the model’s representation": "[3] Denis\nDresvyanskiy,\nMaxim Markitantov,\nJiawei\nYu,",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal es-"
        },
        {
          "from video frames, enhancing the model’s representation": "Peitong Li, Heysem Kaya,\nand Alexey Karpov.\nSun",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "timation, expression recognition, action unit detection emo-"
        },
        {
          "from video frames, enhancing the model’s representation": "team’s\ncontribution to abaw 2024 competition:\nAudiovi-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "tional reaction intensity estimation challenges.\nIn Proceed-"
        },
        {
          "from video frames, enhancing the model’s representation": "sual valence-arousal estimation and expression recognition,",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "ings of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "from video frames, enhancing the model’s representation": "2024. 2",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Pattern Recognition, pages 5888–5897, 2023. 1"
        },
        {
          "from video frames, enhancing the model’s representation": "[4] A. Gu\net\nal.\nMamba:\nLinear-time\nsequence model-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[18] D. Kollias, P. Tzirakis, A. Baird,\net al.\nAbaw: Valence-"
        },
        {
          "from video frames, enhancing the model’s representation": "ing with\nselective\nstate\nspaces,\n2023.\narXiv\npreprint",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "arousal estimation,\nexpression recognition,\naction unit de-"
        },
        {
          "from video frames, enhancing the model’s representation": "arXiv:2302.06665. 2",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "tection emotional\nreaction intensity estimation challenges,"
        },
        {
          "from video frames, enhancing the model’s representation": "[5] S. Hochreiter and J. Schmidhuber. Long short-term memory,",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "2023. In Proceedings of the IEEE/CVF Conference on Com-"
        },
        {
          "from video frames, enhancing the model’s representation": "1997. Neural Computation, vol. 9, no. 8, pp. 1735–1780. 2",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "puter Vision and Pattern Recognition, pages 5888–5897. 2"
        },
        {
          "from video frames, enhancing the model’s representation": "[6] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "[19] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "from video frames, enhancing the model’s representation": "pression recognition, action unit detection multi-task learn-",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "Zafeiriou. Distribution matching for multi-task learning of"
        },
        {
          "from video frames, enhancing the model’s representation": "ing challenges.\nIn Proceedings of the IEEE/CVF Conference",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "classiﬁcation tasks:\na large-scale study on faces & beyond."
        },
        {
          "from video frames, enhancing the model’s representation": "on Computer Vision and Pattern Recognition, pages 2328–",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "the AAAI Conference on Artiﬁcial Intelli-\nIn Proceedings of"
        },
        {
          "from video frames, enhancing the model’s representation": "2336, 2022. 1",
          "[7] Dimitrios Kollias.\nAbaw:\nlearning from synthetic data &": "gence, pages 2813–2821, 2024. 1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "fanos Zafeiriou,\nIrene Kotsia, Alice Baird, Chris Gagne,"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Chunchang Shao, and Guanyu Hu. The 6th affective behav-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "ior analysis in-the-wild (abaw) competition.\nIn Proceedings"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "of\nthe IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "tern Recognition, pages 4587–4598, 2024."
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[21] Dimitrios Kollias, Stefanos Zafeiriou, Irene Kotsia, Abhinav"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Dhall, Shreya Ghosh, Chunchang Shao,\nand Guanyu Hu."
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "7th abaw competition: Multi-task learning and compound"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "expression recognition.\narXiv preprint arXiv:2407.03835,"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "2024. 1"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[22] Dimitrios Kollias, Panagiotis Tzirakis, Alan S. Cowen, Ste-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "fanos Zafeiriou,\nIrene Kotsia, Eric Granger, Marco Peder-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "soli, Simon L. Bacon, Alice Baird, Chris Gagne, Chun-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "chang Shao, Guanyu Hu, Souﬁane Belharbi, and Muham-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "mad Haseeb Aslam. Advancements in Affective and Behav-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "ior Analysis: The 8th ABAW Workshop and Competition."
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "2025. 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[23] A. Oord et al. Wavenet: A generative model for raw audio,"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "2016.\nIn Proceedings of SSW, pp. 125–128. 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[24] R. W. Picard. Affective computing, 1997. MIT Press. 1"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[25] S. Poria, E. Cambria, R. Bajpai, and A. Hussain. A review of"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "affective computing: From unimodal analysis to multimodal"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "fusion, 2017.\nInformation Fusion, vol. 37, pp. 98–125. 1, 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[26] R. G. Praveen and J. Alam. Recursive joint cross-modal at-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "tention for multimodal fusion in dimensional emotion recog-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "nition, 2024. 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[27]\nJ. Russell.\nA circumplex model of affect, 1980.\nJournal"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "of Personality and Social Psychology, vol. 39, no. 6, pp."
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "1161–1178. 1"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[28] M. Valstar et al. Avec 2016: Depression, mood, and emotion"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "recognition workshop and challenge, 2016.\nIn Proceedings"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "of ACM MM, pp. 3–10. 1"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[29] A. Vaswani et al. Attention is all you need, 2017.\nIn Pro-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "ceedings of NeurIPS, pp. 5998–6008. 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[30]\nJun Yu, Gongpeng Zhao, Yongqi Wan, Zhihong Wei, Yang"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Zheng, Zerui Zhang, Zhongpeng Cai, Guochen Xie, Jichao"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Zhu,\nand Wangyuan Zhu.\nMultimodal\nfusion method"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "with spatiotemporal sequences and relationship learning for"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "valence-arousal estimation, 2024. 3"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[31] Wei Zhang, Feng Qiu, Chen Liu, Lincheng Li, Heming Du,"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Tiancheng Guo, and Xin Yu. Affective behaviour analysis"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "via integrating multi-modal knowledge, 2024. 2"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "[32] Weiwei Zhou, Jiada Lu, Chenkun Ling, Weifeng Wang, and"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "Shaowei Liu. Boosting continuous emotion recognition with"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "self-pretraining using masked autoencoders,\ntemporal con-"
        },
        {
          "[20] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-": "volutional networks, and transformers, 2024. 2"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271.2"
    },
    {
      "citation_id": "2",
      "title": "Gated feedback recurrent neural networks",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho"
      ],
      "year": "2015",
      "venue": "Proceedings of ICML"
    },
    {
      "citation_id": "3",
      "title": "Sun team's contribution to abaw 2024 competition: Audiovisual valence-arousal estimation and expression recognition",
      "authors": [
        "Denis Dresvyanskiy",
        "Maxim Markitantov",
        "Jiawei Yu",
        "Peitong Li",
        "Heysem Kaya",
        "Alexey Karpov"
      ],
      "year": "2024",
      "venue": "Sun team's contribution to abaw 2024 competition: Audiovisual valence-arousal estimation and expression recognition"
    },
    {
      "citation_id": "4",
      "title": "Linear-time sequence modeling with selective state spaces",
      "authors": [
        "A Gu"
      ],
      "year": "2023",
      "venue": "Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2302.06665.2"
    },
    {
      "citation_id": "5",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "10",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "11",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "13",
      "title": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Irene Kotsia",
        "Eric Cogitat",
        "Marco Granger",
        "Simon Pedersoli",
        "Alice Bacon",
        "Chunchang Baird",
        "Shao"
      ],
      "venue": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition"
    },
    {
      "citation_id": "14",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "15",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "17",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Abaw: Valencearousal estimation, expression recognition, action unit detection emotional reaction intensity estimation challenges, 2023",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "22",
      "title": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Eric Granger",
        "Marco Pedersoli",
        "Simon Bacon",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2025",
      "venue": "Soufiane Belharbi, and Muhammad Haseeb Aslam. Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition"
    },
    {
      "citation_id": "23",
      "title": "Wavenet: A generative model for raw audio",
      "authors": [
        "A Oord"
      ],
      "year": "2016",
      "venue": "Proceedings of SSW"
    },
    {
      "citation_id": "24",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective computing"
    },
    {
      "citation_id": "25",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "26",
      "title": "Recursive joint cross-modal attention for multimodal fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "J Alam"
      ],
      "year": "2024",
      "venue": "Recursive joint cross-modal attention for multimodal fusion in dimensional emotion recognition"
    },
    {
      "citation_id": "27",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "28",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar"
      ],
      "year": "2016",
      "venue": "Proceedings of ACM MM"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Proceedings of NeurIPS"
    },
    {
      "citation_id": "30",
      "title": "Multimodal fusion method with spatiotemporal sequences and relationship learning for valence-arousal estimation",
      "authors": [
        "Jun Yu",
        "Gongpeng Zhao",
        "Yongqi Wan",
        "Zhihong Wei",
        "Yang Zheng",
        "Zerui Zhang",
        "Zhongpeng Cai",
        "Guochen Xie",
        "Jichao Zhu",
        "Wangyuan Zhu"
      ],
      "year": "2024",
      "venue": "Multimodal fusion method with spatiotemporal sequences and relationship learning for valence-arousal estimation"
    },
    {
      "citation_id": "31",
      "title": "Affective behaviour analysis via integrating multi-modal knowledge",
      "authors": [
        "Wei Zhang",
        "Feng Qiu",
        "Chen Liu",
        "Lincheng Li",
        "Heming Du",
        "Tiancheng Guo",
        "Xin Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via integrating multi-modal knowledge"
    },
    {
      "citation_id": "32",
      "title": "Boosting continuous emotion recognition with self-pretraining using masked autoencoders, temporal convolutional networks, and transformers",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Chenkun Ling",
        "Weifeng Wang",
        "Shaowei Liu"
      ],
      "year": "2024",
      "venue": "Boosting continuous emotion recognition with self-pretraining using masked autoencoders, temporal convolutional networks, and transformers"
    }
  ]
}