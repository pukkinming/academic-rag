{
  "paper_id": "2305.19184v1",
  "title": "Leveraging Semantic Information For Efficient Self-Supervised Emotion Recognition With Audio-Textual Distilled Models",
  "published": "2023-05-30T16:29:33Z",
  "authors": [
    "Danilo de Oliveira",
    "Navin Raj Prabhu",
    "Timo Gerkmann"
  ],
  "keywords": [
    "speech emotion recognition",
    "self-supervised learning",
    "knowledge distillation",
    "paralinguistics",
    "semantics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In large part due to their implicit semantic modeling, selfsupervised learning (SSL) methods have significantly increased the performance of valence recognition in speech emotion recognition (SER) systems. Yet, their large size may often hinder practical implementations. In this work, we take HuBERT as an example of an SSL model and analyze the relevance of each of its layers for SER. We show that shallow layers are more important for arousal recognition while deeper layers are more important for valence. This observation motivates the importance of additional textual information for accurate valence recognition, as the distilled framework lacks the depth of its large-scale SSL teacher. Thus, we propose an audio-textual distilled SSL framework that, while having only ∼20% of the trainable parameters of a large SSL model, achieves on par performance across the three emotion dimensions (arousal, valence, dominance) on the MSP-Podcast v1.10 dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech signals carry rich information on an individual's emotional states, expressed through both paralinguistic and semantic cues. Backed by the circumplex model  [1] , research on speech emotion recognition (SER) is typically performed by studying the emotional expressions across multiple dimensions, namely the activation-deactivation dimension (arousal), the pleasuredispleasure dimension (valence), and the speaker confidencediffidence dimension (dominance). Prior works in literature reveal that certain speech cues carry more information on a particular emotional dimension. For instance, the arousal dimension is well explained by paralinguistic cues  [2] , and semantic cues are more informative of the valence dimension  [3] . This calls for techniques that learn both paralinguistic and semantic cues simultaneously.\n\nThe self-supervised learning (SSL) paradigm is a good candidate to fulfill these requirements. Methods from this domain leverage unlabeled data in a pre-training stage as a way of learning robust representations of an input's underlying structure. The pre-training of SSL models is usually succeeded by a fine-tuning stage, in which the model is fed labeled data and trained in a supervised manner for a target downstream task. Following the success of BERT  [4]  in the natural language processing (NLP) domain, models like wav2vec 2.0  [5]  and HuBERT  [6]  have provided great performance boosts in speech tasks, ranging from automatic speech recognition (ASR) to speaker identification (SI)  [7]  and SER  [8, 9] .\n\nWhile SER systems typically perform well in terms of arousal and dominance, modeling valence from speech signals is a challenging task  [10, 2] , resulting in a performance gap between arousal/dominance and valence estimation. Prior works have successfully managed to reduce this gap by fine-tuning wav2vec 2.0 and HuBERT for emotion recognition  [9] . The improved valence prediction results come from the fact that these transformer-based SSL models can implicitly capture semantic content present in speech, along with the paralinguistics  [11] . However, such models still underperform on valence compared with models that explicitly include semantic information through BERT-encoded features  [12] . These findings suggest that the semantic information is not completely modeled by speech-only SSL techniques.\n\nDespite the reduced performance gap between arousal and valence, an important drawback of SSL models is their size: the smallest versions of BERT, wav2vec 2.0, and HuBERT contain 110, 95, and 90 million parameters, respectively, making their use costly or even prohibitive in some cases. Knowledge distillation  [13]  methods have found success in addressing this issue, in particular in the NLP domain  [14, 15] . The goal of these methods is to compress the knowledge acquired by a large network (the teacher) by transferring it to a smaller one (the student). Applied to HuBERT, this framework results in DistilHuBERT  [16] , with only 25% of the parameters of its teacher.\n\nAn analysis of DistilHuBERT suggests that the distilled network is good at encoding paralinguistic information, given the high importance of its representations in the SI task  [16] ; this indicates potentially good performance in arousal estimation. Moreover, the inner representations of wav2vec 2.0 have an acoustic-linguistic hierarchy  [17] , where the information embedded in each layer output mutates with network depth, from an acoustic to a semantic nature. These findings further motivate us to investigate how well the distilled model fares in predicting each of the emotional dimensions, especially valence.\n\nIn this paper, we make the following contributions: We show that the shallow layers of SSL models are more important for arousal recognition while the deeper layers are more important for valence recognition. We argue that the students in distilled models like DistilHuBERT lack the required depth to properly model valence. Therefore, we show that adding textual information is particularly helpful for distilled SSL networks like DistilHuBERT, considerably more important than for nondistilled large-scale SSL networks. To the best of our knowledge, we are the first in the literature to use distilled SSL models in arousal, valence and dominance modeling and to analyze the implication of the distillation process (i.e., layer selection and compression) towards bridging the gap between valence and arousal estimation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methodology",
      "text": "The task of emotion recognition is formulated here as follows: given a single-channel audio input containing a spoken utterance XA ∈ R 1×S , where S is the number of samples, we want to estimate three emotional expression scalar values: arousal (Ya), valence (Yv) and dominance (Y d ). Our SER model fA should map the input utterance XA to an estimate of the three emotion dimensions, simultaneously:\n\nwhere Y = concat( Ya, Yv, Y d ).\n\nIn the multi-modal case, we also have text as an additional input. The tokenized text is denoted by XT ∈ N 1×N , where N is the number of tokens in the utterance. The audio-textual model mapping is therefore Y = fA+T(XA, XT ).\n\n(\n\nFigure  1  illustrates the models. They contain essentially audio and text encoders (SSLA and SSLT , respectively), pooling operations and a feed-forward regression head (FFN). The pooling block consists of average-and max-pooling across the sequence dimension. The FFN layer contains two fully-connected layers and a hyperbolic tangent activation function. Its final layer has three output features to estimate Y.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio-Only Ssl Framework",
      "text": "The audio-only system is depicted in Figure  1a . Aiming at extracting features from the time-domain input XA, we employ pre-trained models as SSLA, namely wav2vec 2.0, HuBERT and DistilHuBERT. They share the same structure: a convolutional encoder followed by transformer blocks. The convolutional encoder creates frames of latent representations that act as tokens for the creation of contextualized features CA ∈ R d A ×K by the transformer encoder, where dA is the hidden dimension size and K is the number of frames, dependent on the configuration of the convolutional blocks. The transformer encoder is similar to its text counterpart, presented in the next section.\n\nWav2vec 2.0 uses product quantization to generate a finite set of representations and compares them against the context representations using a contrastive loss. HuBERT, on the other hand, is trained in two separate steps: the first is an offline step that creates discrete pseudo-labels by clustering audio-based features, and the second consists in masked prediction of cluster assignments. Finally, DistilHuBERT is a model obtained by distilling HuBERT through a framework of three prediction heads that aim at estimating HuBERT's 4th, 8th and 12th layers. Though the models make use of largely different pre-training techniques, during fine-tuning/inference their architectures differ essentially in the number of layers and hidden size.\n\nIn order to use the extracted representations for our emotion recognition task, the pooling block compresses them to an utterance-level representation PA ∈ R d A ×1 . This is then fed to the FFN head, which outputs the estimates for arousal, valence and dominance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio-Textual Ssl Framework",
      "text": "For experiments including text inputs, the token IDs from the tokenized text XT pass through an embedding layer, and are then fed to a BERT-like encoder. This class of language models is pretrained by randomly masking tokens and attempting to predict them using the unmasked ones as context. The architecture is essentially a sequence of bidirectional transformer layers. The resulting vector CT ∈ R d T ×N , with dT being the size of the text hidden dimension, is an embedding enriched by contextual information.\n\nIn our experiments, we use TinyBERT  [15]  as SSLT , with the intent of minimizing overhead. It is a distilled version of BERT, trained by applying distillation in two steps: pre-training and fine-tuning, aiming at reproducing BERT's capabilities of generalization and downstream task performance, respectively. The authors report 96.8% of the performance of BERT base on an NLP task benchmark, even though the model is 7.5x smaller.\n\nInspired by  [18, 19, 9] , we use a simple concatenation method for the fusion of modalities, shown in Figure  1b : the pooled features from each encoder are concatenated in the feature dimension, resulting in PA+T ∈ R (d A +d T )×1 . We then pass it through a feed-forward block similar to the audio-only case, only with the number of input features adjusted to incorporate the additional text representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "For training, validation and testing, we use the MSP-Podcast dataset  [20] , version 1.10. It contains approximately 166 hours of audio extracted from podcast data, labeled at utterance level for arousal, valence and dominance on a scale of 1 to 7. V1.10 features human-labeled transcripts, which serve as our oracle text information. The dataset is split into four partitions: train, development, test1 and test2. Differently from test1, the segments in test2 originate from podcasts not present in any other partitions. We therefore consider it our unseen scenarios set, while test1 is our seen scenarios test data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Baseline And Proposed Models",
      "text": "In our experiments, we propose a framework based exclusively on distilled models and compare it against base and large SSLA baselines. As the base SSLA model, we use HuBERT base, which has 12 transformer layers and hidden size dA = 768  [6] . As the large SSLA model, we use the pruned w2v2-Lrobust from  [9] , which we denote as w2v2-L-robust(p). It uses the first 12 (out of 24) layers of the original model, with a hidden size dA = 1024. As the proposed distilled SSLA, we We make use of the pre-trained SSL models available on Huggingface 1 . When fine-tuning, we follow the usual procedure of freezing the weights of the convolutional encoders present in the speech models. Emotional expression target labels are normalized to [0, 1]. We use Adam  [21]  as the optimizer, with a learning rate of 10 -5 and early stopping to prevent overfitting. We employ dropout of 0.1 throughout the model. The training data are batched with batch size 16, using buckets sorted by length in order to reduce the amount of padding. The networks are fine-tuned on a single NVIDIA A6000 GPU.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Loss Function",
      "text": "To train the proposed architecture, we use the concordance correlation coefficient (CCC) loss  [22] , a widely used loss function in SER research  [23, 2, 3] . The CCC measures the similarity between two variables and varies between -1 and +1, where +1 denotes perfect similarity and 0 denotes perfect orthogonality between the variables. For Pearson's correlation coefficient ρ, the CCC between y and its estimate y is formulated as\n\nwhere µ, and σ are the mean and standard deviation, respectively, of the corresponding variables. From (3), it can be noted that the CCC takes both the linear correlation and the bias between y and y into consideration while quantifying their similarity, hence it is preferred over the Pearson correlation as a loss function for SER. Our training minimizes 1 -LCCC(y, y) averaged across the three emotional dimensions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Explicit Inclusion Of Semantic Information",
      "text": "We evaluate our audio-only and audio-textual versions of Distil-HuBERT along with the HuBERT base and w2v2-L-robust(p) baselines. The results are shown in Table  1 . Firstly, from the audio-only models, it can be observed that DistilHuBERT manages to match the large SSL baseline w2v2-L-robust(p), for both seen and unseen scenario test data even though it has only ∼15% of the total number of parameters. However, the lack of modeling depth and capacity seems to heavily impact valence estimation.\n\nThe HuBERT base version lags behind in arousal and dominance, but has valence scores closer to those of w2v2-L-robust(p). Secondly, the audio-textual models considerably outperform their audio-only counterparts in terms of valence estimation, even in the case of w2v2-L-robust(p), whose dominance modeling performance decreased, while arousal remained at roughly the same level. The audio-textual versions of the base and distilled HuBERT models either improved or remained at the same level in all cases. Notably, the audio-textual distilled model performs on par with the larger model not only in arousal and dominance but also valence, thus confirming the effectiveness of the explicit inclusion of text, particularly for distilled models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Layer Importance For The Emotional Dimensions",
      "text": "As presented in Table  1 , the audio-only DistilHuBERT, despite its performance in arousal estimation, performs poorly in terms of valence. To explain this behavior, and also to better understand what kind of information the distilled model learns from the teacher, we proceed by analyzing the relevance of the transformer layers of the pre-trained HuBERT base encoder for arousal, valence and dominance modeling, this time individually. In a similar fashion to [8, 16], we perform a weighted sum of the outputs of the encoder's layers. The weights are normalized to sum to one, and the encoder weights are kept frozen while training the regression head for estimating the target emotional expression. The resulting layer-weighting parameter values found during training are plotted in Figure  2 . Index 0 corresponds to the convolutional encoder's outputs, and 1-12 are each of the transformer layers.\n\nFigure  2  reveals that for arousal estimation the first layers up to layer 4 are more important than deeper layers, which hints at the specialization of the first layers on paralinguistic features. This coincides with the findings of  [17]  for the similar wav2vec 2.0 model. It is interesting to see that the layer importance follows an opposite trend for valence: Here, lower importance is observed in the first few transformer blocks while deeper layers are more important to model valence. A peak is observed at layers 9 and 10. It raises a concern for the use of DistilHuBERT, since it lacks depth to model the deeper layers. Additionally, its two layers are initialized from the first two of HuBERT. This can explain the distilled model's performance gap between arousal and valence estimations. Furthermore, it also motivates the explicit inclusion of text in SSL models, especially for the distilled models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fine-Tuning Vs. Freezing",
      "text": "Motivated by related works in audio-textual emotion recognition that either fine-tune the SSL encoders' weights  [19]  or keep them frozen while only training a regression head  [18, 9, 12] , we run experiments to compare how much each approach helps to improve valence predictions. Figure  3  displays the relative improvement of the valence CCC score over the audio-only case for each of the considered models: large-pruned, base and distilled. \"FT\" indicates a model with fine-tuned SSL encoders, while \"FT⇒FRZ\" represents the fusion process used in  [9] : a pre-trained speech model is fine-tuned for emotion recognition, then its encoder weights are frozen and used alongside an also frozen text encoder to train a regression head.\n\nWe observe in Figure  3  that textual information is particularly helpful for smaller models. This indicates that larger capacity and/or more training data help the model learn some level of semantic information even without explicit textual input. Nevertheless, in all cases the performance is increased including textual information, confirming the fact that there's still relevant information in the textual input that is not captured by pre-training with audio alone. Furthermore, we observe that fine-tuning brings even larger improvements in all cases; in particular, for the case of the distilled model, valence estimation performance was double that of the audio-only setting in the unseen scenario test set. We therefore hypothesize that fine-tuning the text model helps it focus on modeling the specific semantic information missing from the speech representation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Quality Of Transcripts",
      "text": "When implementing systems that make additional use of text, the audio needs to be transcribed, most practically via an automatic speech recognition (ASR) system. Although practical, ASR models may introduce transcription errors. In order to examine the robustness of the text-informed framework, we run our proposed distilled-only, fine-tuned framework using text transcribed by two ASR systems with different model sizes, and compare it with the model trained on human transcriptions.\n\nThe results are presented in Table  2 . We can see that the performance is maintained across ASR models, irrespective of their number of parameters. This suggests that the framework is robust with respect to text, as long as the main semantic information is preserved. This claim is further backed by the fact that performance of ASR-generated transcripts is as good as that of human transcripts. It should be noted that the models are run on clean speech; noisy and reverberant conditions are expected to worsen the quality of transcripts and should be addressed by future studies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "% Relative Improvement",
      "text": "Valence for Unseen Scenarios FT⇒FRZ  [9]  FT Figure  3 : Relative valence CCC improvement of audio+text models over the audio-only case. \"FT⇒FRZ\" corresponds to the method used in  [9] . \"FT\" refers to the fine-tuning used in this paper.\n\nTable  2 : CCC performance of the fine-tuned DistilHuBERT + TinyBERT system with different transcription methods. \"A\", \"V\" and \"D\" denote arousal, valence and dominance, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Transcription Method #Params (Only Asr)",
      "text": "Seen Scenarios WER A V D Human --0.614 0.519 0.509 Whisper base  [24]  74M 22.2% 0.620 0.521 0.511 Whisper tiny  [24]  39M 24.1% 0.618 0.524 0.510",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this study we proposed an audio-textual emotion recognition framework based on distilled models. We highlighted the particular importance of multi-modal audio and text inputs for robust arousal, valence and dominance estimation when using our distilled model. Despite having only ∼20% of the trainable parameters of the largest baseline, the proposed framework's performance is on par with base and large models not only on seen scenarios, but importantly also on unseen scenario data. We investigated the relevance of HuBERT's inner representations to each of the three emotion dimensions and found the initial layers to be more important for arousal modeling, while the deeper layers focus on information instrumental to valence estimation. This analysis further validates the need for text as extra input to distilled networks for improved valence modeling, as these shallow models cannot extract semantic information from speech as easily as their teacher counterparts. Lastly, we confirmed the robustness of our audio-textual network by training it on machine-transcribed audio-text pairs, without loss of performance. Distillation is a promising way to make SSL models more practical, but it is necessary to ensure that both paralinguistic and semantic information is available in order to have robust arousal and valence estimation.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the models. They contain essentially",
      "page": 2
    },
    {
      "caption": "Figure 1: a. Aiming at",
      "page": 2
    },
    {
      "caption": "Figure 1: System architecture, mapping a spoken utterance XA",
      "page": 2
    },
    {
      "caption": "Figure 2: Normalized importance given to each layer of HuBERT",
      "page": 3
    },
    {
      "caption": "Figure 2: Index 0 corresponds to",
      "page": 3
    },
    {
      "caption": "Figure 2: reveals that for arousal estimation the first layers",
      "page": 3
    },
    {
      "caption": "Figure 3: displays the relative",
      "page": 4
    },
    {
      "caption": "Figure 3: that textual information is partic-",
      "page": 4
    },
    {
      "caption": "Figure 3: Relative valence CCC improvement of audio+text",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "{",
          "timo.gerkmann\n@uni-hamburg.de": "}"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "Abstract",
          "timo.gerkmann\n@uni-hamburg.de": "is a challenging task [10, 2],\nresulting in a performance gap"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "between arousal/dominance and valence estimation. Prior works"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "In large part due\nto their\nimplicit\nsemantic modeling,\nself-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "have successfully managed to reduce this gap by fine-tuning"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "supervised learning (SSL) methods have significantly increased",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "wav2vec 2.0 and HuBERT for emotion recognition [9]. The im-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "the performance of valence recognition in speech emotion recog-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "proved valence prediction results come from the fact that these"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "nition (SER) systems. Yet,\ntheir\nlarge size may often hinder",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "transformer-based SSL models can implicitly capture semantic"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "practical implementations. In this work, we take HuBERT as an",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "content present\nin speech, along with the paralinguistics [11]."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "example of an SSL model and analyze the relevance of each of its",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "However, such models still underperform on valence compared"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "layers for SER. We show that shallow layers are more important",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "with models that explicitly include semantic information through"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "for arousal recognition while deeper layers are more important",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "BERT-encoded features [12]. These findings suggest\nthat\nthe"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "for valence. This observation motivates the importance of addi-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "semantic information is not completely modeled by speech-only"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "tional textual information for accurate valence recognition, as",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "SSL techniques."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "the distilled framework lacks the depth of its large-scale SSL",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "teacher. Thus, we propose an audio-textual distilled SSL frame-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "Despite the reduced performance gap between arousal and"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "work that, while having only\n20% of the trainable parameters",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "∼",
          "timo.gerkmann\n@uni-hamburg.de": "valence, an important drawback of SSL models is their size:\nthe"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "of a large SSL model, achieves on par performance across the",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "smallest versions of BERT, wav2vec 2.0, and HuBERT contain"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "three emotion dimensions (arousal, valence, dominance) on the",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "110, 95, and 90 million parameters, respectively, making their"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "MSP-Podcast v1.10 dataset.",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "use costly or even prohibitive in some cases. Knowledge distilla-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "Index Terms: speech emotion recognition, self-supervised learn-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "tion [13] methods have found success in addressing this issue, in"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "ing, knowledge distillation, paralinguistics, semantics",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "particular in the NLP domain [14, 15]. The goal of these meth-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "ods is to compress the knowledge acquired by a large network"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "1.\nIntroduction",
          "timo.gerkmann\n@uni-hamburg.de": "(the teacher) by transferring it\nto a smaller one (the student)."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "Applied to HuBERT, this framework results in DistilHuBERT"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "Speech signals carry rich information on an individual’s emo-",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "",
          "timo.gerkmann\n@uni-hamburg.de": "[16], with only 25% of the parameters of its teacher."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "tional states, expressed through both paralinguistic and semantic",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "cues. Backed by the circumplex model [1], research on speech",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "emotion recognition (SER) is typically performed by studying",
          "timo.gerkmann\n@uni-hamburg.de": "An analysis of DistilHuBERT suggests that the distilled net-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "the emotional expressions across multiple dimensions, namely",
          "timo.gerkmann\n@uni-hamburg.de": "work is good at encoding paralinguistic information, given the"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "the activation-deactivation dimension (arousal),\nthe pleasure-",
          "timo.gerkmann\n@uni-hamburg.de": "high importance of its representations in the SI task [16];\nthis"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "displeasure dimension (valence), and the speaker confidence-",
          "timo.gerkmann\n@uni-hamburg.de": "indicates potentially good performance in arousal estimation."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "diffidence dimension (dominance).\nPrior works in literature",
          "timo.gerkmann\n@uni-hamburg.de": "Moreover,\nthe inner\nrepresentations of wav2vec 2.0 have an"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "reveal that certain speech cues carry more information on a par-",
          "timo.gerkmann\n@uni-hamburg.de": "acoustic-linguistic hierarchy [17], where the information embed-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "ticular emotional dimension. For instance, the arousal dimension",
          "timo.gerkmann\n@uni-hamburg.de": "ded in each layer output mutates with network depth, from an"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "is well explained by paralinguistic cues [2], and semantic cues",
          "timo.gerkmann\n@uni-hamburg.de": "acoustic to a semantic nature. These findings further motivate"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "are more informative of the valence dimension [3]. This calls",
          "timo.gerkmann\n@uni-hamburg.de": "us to investigate how well the distilled model fares in predicting"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "for techniques that learn both paralinguistic and semantic cues",
          "timo.gerkmann\n@uni-hamburg.de": "each of the emotional dimensions, especially valence."
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "simultaneously.",
          "timo.gerkmann\n@uni-hamburg.de": ""
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "The self-supervised learning (SSL) paradigm is a good can-",
          "timo.gerkmann\n@uni-hamburg.de": "In this paper, we make the following contributions: We"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "didate to fulfill these requirements. Methods from this domain",
          "timo.gerkmann\n@uni-hamburg.de": "show that\nthe shallow layers of SSL models are more impor-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "leverage unlabeled data in a pre-training stage as a way of learn-",
          "timo.gerkmann\n@uni-hamburg.de": "tant for arousal recognition while the deeper layers are more"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "ing robust representations of an input’s underlying structure. The",
          "timo.gerkmann\n@uni-hamburg.de": "important for valence recognition. We argue that the students in"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "pre-training of SSL models is usually succeeded by a fine-tuning",
          "timo.gerkmann\n@uni-hamburg.de": "distilled models like DistilHuBERT lack the required depth to"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "stage,\nin which the model\nis fed labeled data and trained in a",
          "timo.gerkmann\n@uni-hamburg.de": "properly model valence. Therefore, we show that adding textual"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "supervised manner for a target downstream task. Following the",
          "timo.gerkmann\n@uni-hamburg.de": "information is particularly helpful for distilled SSL networks"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "success of BERT [4] in the natural language processing (NLP)",
          "timo.gerkmann\n@uni-hamburg.de": "like DistilHuBERT, considerably more important than for non-"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "domain, models like wav2vec 2.0 [5] and HuBERT [6] have",
          "timo.gerkmann\n@uni-hamburg.de": "distilled large-scale SSL networks. To the best of our knowledge,"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "provided great performance boosts in speech tasks, ranging from",
          "timo.gerkmann\n@uni-hamburg.de": "we are the first in the literature to use distilled SSL models in"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "automatic speech recognition (ASR) to speaker identification",
          "timo.gerkmann\n@uni-hamburg.de": "arousal, valence and dominance modeling and to analyze the"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "(SI) [7] and SER [8, 9].",
          "timo.gerkmann\n@uni-hamburg.de": "implication of the distillation process (i.e., layer selection and"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "While SER systems\ntypically perform well\nin terms of",
          "timo.gerkmann\n@uni-hamburg.de": "compression)\ntowards bridging the gap between valence and"
        },
        {
          "danilo.oliveira,\nnavin.raj.prabhu,": "arousal and dominance, modeling valence from speech signals",
          "timo.gerkmann\n@uni-hamburg.de": "arousal estimation."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": "Modality"
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": "Audio"
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": "Audio + Text"
        },
        {
          "development sets, while the unseen scenarios set contains segments from podcasts not present in any other partition.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "learning rate of 10−5 and early stopping to prevent overfitting.",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "We employ dropout of 0.1 throughout the model. The training",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "Layer index"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "data are batched with batch size 16, using buckets sorted by",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "length in order to reduce the amount of padding. The networks",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "are fine-tuned on a single NVIDIA A6000 GPU.",
          "7": "Figure 2: Normalized importance given to each layer of HuBERT"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "when training in a frozen weights setting. Index 0 corresponds to"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "the output of the convolutional encoder. The subsequent indexes"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "3.3. Loss function",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "reference each of the transformer encoder’s 12 layers."
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "To train the proposed architecture, we use the concordance cor-",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "relation coefficient (CCC) loss [22], a widely used loss function",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "in SER research [23, 2, 3]. The CCC measures the similarity",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "Secondly, the audio-textual models considerably outperform"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "between two variables and varies between\n1 and +1, where",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "−",
          "7": "their audio-only counterparts in terms of valence estimation, even"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "+1 denotes perfect similarity and 0 denotes perfect orthogonality",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "in the case of w2v2-L-robust(p), whose dominance modeling"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "between the variables. For Pearson’s correlation coefficient ρ,",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "performance decreased, while arousal remained at roughly the"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "the CCC between y and its estimate\ny is formulated as",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "same level. The audio-textual versions of the base and distilled"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "2ρσyσ(cid:98)y",
          "7": "HuBERT models either improved or remained at the same level"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "y) =\n(3)",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "(cid:98)\nLCCC(y,",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "σ2\ny + σ2\ny + (µy\nµ(cid:98)y)2 ,",
          "7": "in all cases. Notably, the audio-textual distilled model performs"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "(cid:98)\n−",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "on par with the larger model not only in arousal and dominance"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "where µ, and σ are the mean and standard deviation, respectively,",
          "7": "but also valence, thus confirming the effectiveness of the explicit"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "of the corresponding variables. From (3), it can be noted that the",
          "7": "inclusion of text, particularly for distilled models."
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "CCC takes both the linear correlation and the bias between y and",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "y into consideration while quantifying their similarity, hence it",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "4.2. Layer importance for the emotional dimensions"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "is preferred over the Pearson correlation as a loss function for",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "SER. Our training minimizes 1\ny) averaged across",
          "7": "As presented in Table 1, the audio-only DistilHuBERT, despite"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "(cid:98)\n− LCCC(y,",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "the three emotional dimensions.",
          "7": "its performance in arousal estimation, performs poorly in terms"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "of valence.\nTo explain this behavior, and also to better un-"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "(cid:98)",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "derstand what kind of\ninformation the distilled model\nlearns"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "4. Results and Discussion",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "from the teacher, we proceed by analyzing the relevance of the"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "4.1. Explicit inclusion of semantic information",
          "7": "transformer layers of the pre-trained HuBERT base encoder for"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "arousal, valence and dominance modeling, this time individually."
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "We evaluate our audio-only and audio-textual versions of Distil-",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "In a similar fashion to [8, 16], we perform a weighted sum of the"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "HuBERT along with the HuBERT base and w2v2-L-robust(p)",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "outputs of the encoder’s layers. The weights are normalized to"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "baselines. The results are shown in Table 1. Firstly, from the",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "sum to one, and the encoder weights are kept frozen while train-"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "audio-only models, it can be observed that DistilHuBERT man-",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "ing the regression head for estimating the target emotional ex-"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "ages to match the large SSL baseline w2v2-L-robust(p), for both",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "pression. The resulting layer-weighting parameter values found"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "seen and unseen scenario test data even though it has only\n15%",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "∼",
          "7": "during training are plotted in Figure 2. Index 0 corresponds to"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "of the total number of parameters. However, the lack of modeling",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "the convolutional encoder’s outputs, and 1-12 are each of the"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "depth and capacity seems to heavily impact valence estimation.",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "transformer layers."
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "The HuBERT base version lags behind in arousal and dominance,",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "Figure 2 reveals that for arousal estimation the first layers"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "but has valence scores closer to those of w2v2-L-robust(p).",
          "7": ""
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "",
          "7": "up to layer 4 are more important than deeper layers, which hints"
        },
        {
          "normalized to [0, 1]. We use Adam [21] as the optimizer, with a": "1https://huggingface.co",
          "7": "at the specialization of the first layers on paralinguistic features."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This coincides with the findings of [17] for the similar wav2vec": "2.0 model. It is interesting to see that the layer importance fol-",
          "Valence for Seen Scenarios": "100"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "80"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "lows an opposite trend for valence: Here, lower importance is",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "60"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "observed in the first few transformer blocks while deeper layers",
          "Valence for Seen Scenarios": "%Relative\nImprovement"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "40"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "are more important\nto model valence. A peak is observed at",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "20"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "layers 9 and 10. It raises a concern for the use of DistilHuBERT,",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "0"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "since it lacks depth to model the deeper layers. Additionally, its",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "w2v2-L-robust(p) HuBERT base\nDistilHuBERT"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "two layers are initialized from the first two of HuBERT. This can",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "explain the distilled model’s performance gap between arousal",
          "Valence for Seen Scenarios": "Valence for Unseen Scenarios"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "and valence estimations. Furthermore, it also motivates the ex-",
          "Valence for Seen Scenarios": "100"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "80"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "plicit inclusion of text in SSL models, especially for the distilled",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "60"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "models.",
          "Valence for Seen Scenarios": "%Relative\nImprovement"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "40"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "20"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "4.3. Fine-tuning vs. freezing",
          "Valence for Seen Scenarios": "0"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "w2v2-L-robust(p) HuBERT base\nDistilHuBERT"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "Motivated by related works in audio-textual emotion recognition",
          "Valence for Seen Scenarios": "FT\nFT\nFRZ [9]"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "⇒"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "that either fine-tune the SSL encoders’ weights [19] or keep",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "them frozen while only training a regression head [18, 9, 12],",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "Figure 3: Relative valence CCC improvement of audio+text"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "we run experiments to compare how much each approach helps",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "models over the audio-only case. “FT\nFRZ” corresponds to"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "to improve valence predictions. Figure 3 displays the relative",
          "Valence for Seen Scenarios": "⇒"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "the method used in [9]. “FT” refers to the fine-tuning used in"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "improvement of\nthe valence CCC score over\nthe audio-only",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "this paper."
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "case for each of the considered models:\nlarge-pruned, base and",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "distilled. “FT” indicates a model with fine-tuned SSL encoders,",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "the fine-tuned DistilHuBERT +\nTable 2: CCC performance of"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "while “FT\nFRZ” represents the fusion process used in [9]: a",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "TinyBERT system with different transcription methods. “A”, “V”"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "⇒",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "pre-trained speech model is fine-tuned for emotion recognition,",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "and “D” denote arousal, valence and dominance, respectively."
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "then its encoder weights are frozen and used alongside an also",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "frozen text encoder to train a regression head.",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "Transcription\n#Params\nSeen Scenarios"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "We observe in Figure 3 that\ntextual\ninformation is partic-",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "method\n(only ASR)\nWER\nA\nV\nD"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "ularly helpful\nfor smaller models.\nThis indicates that\nlarger",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "Human\n0.614\n0.519\n0.509"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "capacity and/or more training data help the model\nlearn some",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "−\n−"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "Whisper base [24]\n74M\n22.2%\n0.620\n0.521\n0.511"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "level of semantic information even without explicit\ntextual\nin-",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "Whisper tiny [24]\n39M\n24.1%\n0.618\n0.524\n0.510"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "put. Nevertheless, in all cases the performance is increased by",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "including textual\ninformation, confirming the fact\nthat\nthere’s",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "still relevant information in the textual input that is not captured",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "by pre-training with audio alone. Furthermore, we observe that",
          "Valence for Seen Scenarios": "5. Conclusions"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "fine-tuning brings even larger improvements in all cases; in par-",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "In this study we proposed an audio-textual emotion recogni-"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "ticular, for the case of the distilled model, valence estimation",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "tion framework based on distilled models. We highlighted the"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "performance was double that of the audio-only setting in the un-",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "particular importance of multi-modal audio and text inputs for"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "seen scenario test set. We therefore hypothesize that fine-tuning",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "robust arousal, valence and dominance estimation when using"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "the text model helps it focus on modeling the specific semantic",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "our distilled model. Despite having only\n20% of the trainable"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "information missing from the speech representation.",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "∼"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "parameters of the largest baseline,\nthe proposed framework’s"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "performance is on par with base and large models not only on"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "4.4. Quality of transcripts",
          "Valence for Seen Scenarios": "seen scenarios, but importantly also on unseen scenario data. We"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "investigated the relevance of HuBERT’s inner representations"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "When implementing systems that make additional use of text, the",
          "Valence for Seen Scenarios": "to each of the three emotion dimensions and found the initial"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "audio needs to be transcribed, most practically via an automatic",
          "Valence for Seen Scenarios": "layers to be more important\nfor arousal modeling, while the"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "speech recognition (ASR) system. Although practical, ASR",
          "Valence for Seen Scenarios": "deeper layers focus on information instrumental to valence esti-"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "models may introduce transcription errors. In order to examine",
          "Valence for Seen Scenarios": "mation. This analysis further validates the need for text as extra"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "the robustness of the text-informed framework, we run our pro-",
          "Valence for Seen Scenarios": "input to distilled networks for improved valence modeling, as"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "posed distilled-only, fine-tuned framework using text transcribed",
          "Valence for Seen Scenarios": "these shallow models cannot extract semantic information from"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "by two ASR systems with different model sizes, and compare it",
          "Valence for Seen Scenarios": "speech as easily as their teacher counterparts. Lastly, we con-"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "with the model trained on human transcriptions.",
          "Valence for Seen Scenarios": "firmed the robustness of our audio-textual network by training"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "it on machine-transcribed audio-text pairs, without loss of per-"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "The results are presented in Table 2. We can see that\nthe",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "formance. Distillation is a promising way to make SSL models"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "performance is maintained across ASR models, irrespective of",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "more practical, but it is necessary to ensure that both paralinguis-"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "their number of parameters. This suggests that the framework",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "tic and semantic information is available in order to have robust"
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "is robust with respect\nto text, as long as the main semantic",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "",
          "Valence for Seen Scenarios": "arousal and valence estimation."
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "information is preserved. This claim is further backed by the fact",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "that performance of ASR-generated transcripts is as good as that",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "of human transcripts. It should be noted that the models are run",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "on clean speech; noisy and reverberant conditions are expected",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "to worsen the quality of transcripts and should be addressed by",
          "Valence for Seen Scenarios": ""
        },
        {
          "This coincides with the findings of [17] for the similar wav2vec": "future studies.",
          "Valence for Seen Scenarios": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "ing Through Cross-Modal Conditional Teacher-Student Training"
        },
        {
          "6. References": "J. A. Russell, “A Circumplex Model of Affect,” Journal of Person-\n[1]",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "For Speech Emotion Recognition,” in IEEE Int. Conf. on Acoustics,"
        },
        {
          "6. References": "ality and Social Psychology, vol. 39, no. 6, p. 1161, 1980.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Speech and Signal Process. (ICASSP), May 2022, pp. 6442–6446."
        },
        {
          "6. References": "[2] N. Raj Prabhu, G. Carbajal, N. Lehmann-Willenbrock, and T. Gerk-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[13] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in"
        },
        {
          "6. References": "mann, “End-to-end Label Uncertainty Modeling for Speech-based",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "a neural network,” in NIPS Deep Learning and Representation"
        },
        {
          "6. References": "Arousal Recognition Using Bayesian Neural Networks,” in Inter-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Learning Workshop, 2015."
        },
        {
          "6. References": "speech, Sep. 2022.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[14] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a Dis-"
        },
        {
          "6. References": "[3]\nP. Tzirakis, A. Nguyen, S. Zafeiriou, and B. W. Schuller, “Speech",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "tilled Version of BERT: Smaller, Faster, Cheaper and Lighter,” in"
        },
        {
          "6. References": "Emotion Recognition Using Semantic Information,” in IEEE Int.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "5th Workshop on Energy Efficient Machine Learning and Cognitive"
        },
        {
          "6. References": "Conf. on Acoustics, Speech and Signal Process. (ICASSP), Jun.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Computing (EMC2)- NeurIPS 2019, Feb. 2020."
        },
        {
          "6. References": "2021.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[15] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and"
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Q. Liu, “TinyBERT: Distilling BERT for Natural Language Un-"
        },
        {
          "6. References": "[4]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "derstanding,” in Proceedings of the 2020 Conference on Empirical"
        },
        {
          "6. References": "training of Deep Bidirectional Transformers for Language Under-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Methods in Natural Language Processing (EMNLP), Nov. 2020,"
        },
        {
          "6. References": "standing,” in Proceedings of",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "American Chapter of the Association for Computational Linguis-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "pp. 4163–4174."
        },
        {
          "6. References": "tics: Human Language Technologies (NAACL-HLT), vol. 1, Jun.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[16] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “Distilhubert: Speech"
        },
        {
          "6. References": "2019, pp. 4171–4186.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Representation Learning by Layer-Wise Distillation of Hidden-"
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Unit Bert,” in IEEE Int. Conf. on Acoustics, Speech and Signal"
        },
        {
          "6. References": "[5] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Process. (ICASSP), May 2022, pp. 7087–7091."
        },
        {
          "6. References": "Framework for Self-Supervised Learning of Speech Representa-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "tions,” in Advances in Neural Inf. Proc. Systems (NeurIPS), vol. 33,",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[17] A. Pasad, J.-C. Chou, and K. Livescu, “Layer-Wise Analysis of"
        },
        {
          "6. References": "2020, pp. 12 449–12 460.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "a Self-Supervised Speech Representation Model,” in IEEE Auto-"
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "matic Speech Recognition and Understanding Workshop (ASRU),"
        },
        {
          "6. References": "[6] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Dec. 2021, pp. 914–921."
        },
        {
          "6. References": "and A. Mohamed, “HuBERT: Self-Supervised Speech Represen-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[18]\nL. Pepino, P. Riera, L. Ferrer, and A. Gravano, “Fusion Approaches"
        },
        {
          "6. References": "tation Learning by Masked Prediction of Hidden Units,” IEEE",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "for Emotion Recognition from Speech Using Acoustic and Text-"
        },
        {
          "6. References": "Trans. on Audio, Speech, and Lang. Process. (TASLP), vol. 29, pp.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Based Features,” in IEEE Int. Conf. on Acoustics, Speech and"
        },
        {
          "6. References": "3451–3460, 2021.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Signal Process. (ICASSP), May 2020, pp. 6484–6488."
        },
        {
          "6. References": "[7] N. Vaessen and D. A. Van Leeuwen, “Fine-Tuning Wav2Vec2 for",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[19]\nS. Siriwardhana, A. Reis, R. Weerasekera, and S. Nanayakkara,"
        },
        {
          "6. References": "Speaker Recognition,” in IEEE Int. Conf. on Acoustics, Speech and",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "“Jointly Fine-Tuning “BERT-Like” Self Supervised Models to Im-"
        },
        {
          "6. References": "Signal Process. (ICASSP), May 2022, pp. 7967–7971.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "prove Multimodal Speech Emotion Recognition,” in Interspeech,"
        },
        {
          "6. References": "[8]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Oct. 2020, pp. 3755–3759."
        },
        {
          "6. References": "Speech Using wav2vec 2.0 Embeddings,” in Interspeech, Aug.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[20] R. Lotfian and C. Busso, “Building Naturalistic Emotionally Bal-"
        },
        {
          "6. References": "2021, pp. 3400–3404.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "anced Speech Corpus by Retrieving Emotional Speech from Ex-"
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "isting Podcast Recordings,” IEEE Trans. on Affective Computing,"
        },
        {
          "6. References": "[9]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt,",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "vol. 10, no. 4, pp. 471–483, Oct. 2019."
        },
        {
          "6. References": "F. Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the Trans-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "former Era in Speech Emotion Recognition: Closing the Valence",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[21] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Opti-"
        },
        {
          "6. References": "Gap,” Mar. 2022, arXiv:2203.07378 [cs, eess].",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "mization,” in Int. Conf. on Learning Representations (ICLR), 2015."
        },
        {
          "6. References": "[10] K. Sridhar and C. Busso, “Unsupervised personalization of an",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[22]\nL. I.-K. Lin, “A Concordance Correlation Coefficient to Evaluate"
        },
        {
          "6. References": "emotion recognition system: The unique properties of the external-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Reproducibility,” Biometrics, vol. 45, no. 1, p. 255, Mar. 1989."
        },
        {
          "6. References": "ization of valence in speech,” IEEE Trans. on Affective Computing,",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[23] B. W. Schuller, “Speech Emotion Recognition: Two Decades in a"
        },
        {
          "6. References": "vol. 13, no. 4, pp. 1959–1972, 2022.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Nutshell, Benchmarks, and Ongoing Trends,” Communications of"
        },
        {
          "6. References": "",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "the ACM, vol. 61, no. 5, pp. 90–99, Apr. 2018."
        },
        {
          "6. References": "[11] A. Triantafyllopoulos, J. Wagner, H. Wierstorf, M. Schmitt, U. Re-",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": ""
        },
        {
          "6. References": "ichel, F. Eyben, F. Burkhardt, and B. W. Schuller, “Probing Speech",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "6. References": "Emotion Recognition Transformers for Linguistic Knowledge,” in",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "I. Sutskever, “Robust Speech Recognition via Large-Scale Weak"
        },
        {
          "6. References": "Interspeech, Sep. 2022, pp. 146–150.",
          "[12]\nS. Srinivasan, Z. Huang, and K. Kirchhoff, “Representation Learn-": "Supervision,” Dec. 2022, arXiv:2212.04356 [cs, eess]."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "3",
      "title": "End-to-end Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks",
      "authors": [
        "N Prabhu",
        "G Carbajal",
        "N Lehmann-Willenbrock",
        "T Gerkmann"
      ],
      "year": "2022",
      "venue": "End-to-end Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks"
    },
    {
      "citation_id": "4",
      "title": "Speech Emotion Recognition Using Semantic Information",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Inf. Proc. Systems (NeurIPS)"
    },
    {
      "citation_id": "7",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Audio, Speech, and Lang. Process. (TASLP)"
    },
    {
      "citation_id": "8",
      "title": "Fine-Tuning Wav2Vec2 for Speaker Recognition",
      "authors": [
        "N Vaessen",
        "D Van Leeuwen"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings"
    },
    {
      "citation_id": "10",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised personalization of an emotion recognition system: The unique properties of the externalization of valence in speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Probing Speech Emotion Recognition Transformers for Linguistic Knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Probing Speech Emotion Recognition Transformers for Linguistic Knowledge"
    },
    {
      "citation_id": "13",
      "title": "Representation Learning Through Cross-Modal Conditional Teacher-Student Training For Speech Emotion Recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "NIPS Deep Learning and Representation Learning Workshop"
    },
    {
      "citation_id": "15",
      "title": "DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "5th Workshop on Energy Efficient Machine Learning and Cognitive Computing"
    },
    {
      "citation_id": "16",
      "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
      "authors": [
        "X Jiao",
        "Y Yin",
        "L Shang",
        "X Jiang",
        "X Chen",
        "L Li",
        "F Wang",
        "Q Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "17",
      "title": "Distilhubert: Speech Representation Learning by Layer-Wise Distillation of Hidden-Unit Bert",
      "authors": [
        "H.-J Chang",
        "S.-W Yang",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Layer-Wise Analysis of a Self-Supervised Speech Representation Model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2021",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "19",
      "title": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly Fine-Tuning \"BERT-Like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "21",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Int. Conf. on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "A Concordance Correlation Coefficient to Evaluate Reproducibility",
      "authors": [
        "-K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "24",
      "title": "Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "25",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "arxiv": "arXiv:2212.04356"
    }
  ]
}