{
  "paper_id": "2209.13542v2",
  "title": "A Multimodal Stress Detection Dataset With Facial Expressions And Physiological Signals",
  "published": "2022-08-29T22:19:18Z",
  "authors": [
    "Majid Hosseini",
    "Fahad Sohrab",
    "Raju Gottumukkala",
    "Ravi Teja Bhupatiraju",
    "Satya Katragadda",
    "Jenni Raitoharju",
    "Alexandros Iosifidis",
    "Moncef Gabbouj"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing has garnered the attention and interest of researchers in recent years, as there is a need for AI systems to better understand and react to human emotions. However, analyzing human emotions, such as mood or stress, is quite complex. While various stress studies use facial expressions and wearables, most existing datasets rely on processing data from a single modality. This paper presents EmpathicSchool, a novel dataset that captures facial expressions and the associated physiological signals, such as heart rate, electrodermal activity, and skin temperature, under different stress levels. The data was collected from 30 participants during different sessions for about ninety minutes each (for a total of 40 hours). The data includes seven different signal types, including both computer vision and physiological features that can be used to detect stress. In addition, various experiments were conducted to validate the signal quality.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Summary",
      "text": "Affective computing is an interdisciplinary field aimed at developing systems that can identify, recognize, and interpret human emotions  1  . Affective computing leverages physiological signals to detect the individual's emotions or state of stress. Stress is one of the major problems in modern society that leads to significant health expenditure  2  . Early ambulatory stress detection has become an exciting challenge  3, 4  because early stress detection can mitigate the conversion of stress to a chronic disease  2  and prevent stress-related health issues  5  . Human emotions manifest through physiological, behavioral, and cognitive changes, which can be utilized to adapt digital environments. For example, the current state of a smartphone user can be continuously computed based on their interactions with various digital services  6  , allowing personalized services to be provided in response to the user's mental state  7  . Additionally, stress factors, such as high workload, lack of autonomy, and long working hours, can negatively impact people's health  8  . Many studies point out that prolonged exposure to stress leads to chronic conditions, such as obesity  9  or hypertension  10  , which may exacerbate conditions, such as type-II diabetes  11  . Therefore, monitoring and understanding stress in workplaces is essential, especially in professions with increased exposure to stress, often leading to burnout and increased turnover  12  .\n\nStress is primarily a physiological response to a stimulus, typically triggered by an external factor in an environment  13, 14  . Epinephrine, commonly known as adrenaline, is a significant stress hormone  15  . In a daily routine, a moderate amount of stress is considered beneficial  16  . It causes gentle excitement and improves performance when carrying out regular daily tasks. A reasonable amount of stress that causes excitement is termed eustress or positive stress  17, 18  . On the other hand, undesirable stress is called distress or negative stress  18, 19  . Repeated stress events can lead to emotional exhaustion  20  , and the resulting long-term physiological stress can be harmful; such stress is termed chronic stress  21  . The effects of chronic stress may include anxiety, depression, sleep disturbances, and weight gain  22  . It may even compromise immune responses, increasing vulnerability to several diseases  23  . Studies related to stress have gained interest in recent years due to its widespread effects on health, family, workplace productivity, society, and the economy  24  .\n\nThe analysis of stress involves using wearable sensors to deduce physiological signals. Table  1  shows the standard signals provided by wearables. In contrast, the study and analysis of human emotions use a camera to infer facial expressions that represent emotions. In the literature, emotions and stress signals are typically considered two disjoint topics and are usually studied independently. Schmidt et al.  25  identify the gap in analyzing basic emotions and stress together; however, there is still a need for a dataset that addresses the issue of correlating stress and facial expressions under different stressful situations. This work presents a multimodal stress-emotion dataset containing stress data from wearable devices and emotion data extracted from facial expressions through a video feed. We analyze human expressions and physiological signals under different stress levels. The dataset presented is the first effort toward identifying stress from non-wearable devices in general and facial expressions in particular. The current study is inspired by our previous work on stress detection of nurses using wearable devices  26  and our initial effort to use facial expressions to study satisfaction  27  . This study collected stress and facial expression data from 30 participants under different stress levels. There is a host of surveys in the stress research literature. Alberdi et al.  28  listed physiological signals used for stress detection. Fukazawa et al.  29  introduced standard stress detection tools and devices. Carneiro  30  described techniques to access and monitor stress in offices and other workplaces. Alberdi et al.  31  provided a comprehensive survey on stress recognition for office environments. They described the signals and their related features, as well as successful neural network models that detect stress. Using wearables and smartphones, 2 investigated stress in daily life. They briefly described stress and its impact on society and investigated the stress tests and related signals. However, they did not discuss instruments and devices detecting stress, signal properties, machine learning features, or training parameters. In another paper, Fukazawa et al.  32  provided a comprehensive survey of stress detection signals and techniques from a machine learning perspective. They only used location, activity, phone usage, context, sleep, and speech features to detect stress. They covered several papers that used a specific signal for stress detection and investigated how commonly a method or signal is used by showing its using percentage across the years. Panicker et al.  33  presented a comprehensive survey on stress detection. They provided a comprehensive description of emotions and their organismic subsystems. They also provided definitions of stress, enumerated stress detection signals and devices, presented three different stages of stress, and evaluated the correlation and differences between different types of stress and emotions. Zhang et al.  34  explored the integration of multiple physiological signals, including EEG, EMG, GSR, and ECG, for emotion detection across various daily activities. By constructing a deep neural network architecture, they achieved 95 percent accuracy in binary classification of arousal by combining task-specific representations for each signal type. The proposed LSTM-based model demonstrated high accuracy, emphasizing the complementary nature of physiological and visual stress indicators. Kurniawan  35  investigated several classification methods, such as SVM and Gaussian Mixture Models (GMM), to detect stress levels using GSR and speech features. They gained between 70 to 80 percent accuracy using GSR and 92 percent accuracy by combining both the speech and GSR using four classification techniques, namely K-means, decision tree, GMM, and SVM classification models. Moreover, some authors use thermal images to detect stress or deception detection  36  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Signal",
      "text": "The AffectiveRoad dataset  37  contains physiological signals to study the stress level of 10 drivers while driving in various environments. The study was conducted with drivers taking an 86-minute driving test in Tunisia. Schmidt et al.  25  introduced the WESAD (Wearable Stress and Affect Detection) dataset and studied the stress of 15 students while watching a movie and taking a TSST (Trier Social Stress Test) test 38 using E4 signals. MDPSD (multimodal dataset for psychological stress detection)  39  is a multimodal stress detection dataset of EDA and PPG signals collected from university students while performing different tests (e.g., TSST  40  , IQ test, and color-word tests  41  ). Mundnich et al.  42  provided TILES-2018, a multi-sensor dataset that provides a battery of surveys to cover personality traits, behavioral states, job performance, and well-being over time. The SWELL  43  dataset provides data corresponding to different stress levels of participants while performing some office work (e.g., answering email) at three different stress levels (neutral, stressor, and stressor with interruption) using Kinect, ECG, and emotional expressions extracted from videos. However, this dataset does not provide the videos of the participants and uses outdated facial features to detect emotion and stress. In addition, the SWELL dataset provides the stress level of the entire task as one label. Zaman et al.  44  studied the impact of email interruptions and office activities on productivity and stress, using multimodal data from 63 workers who were exposed to batch or continual email interruptions, with or without additional stress. The data collected, including physiological stress indicators, writing quality measures, keystroke dynamics, and participant profiles, is expected to provide insights into personalized email management strategies and a better understanding of office activity dynamics.\n\nAlthough WESAD  25  and SWELL  43  have been invaluable for stress detection research, they have some limitations. WESAD provides synchronized ECG, EDA, and motion data, but no facial video or expression labels. This makes WESAD single-modal from the vision standpoint and unable to capture behavioral cues. SWELL adds facial-expression features, but only reports them as coarse, single-label summaries per task (neutral vs. stressed), does not provide the raw video data, and relies on feature extractors, preventing reanalysis or application of new deep learning methods. Furthermore, both datasets offer workload labels at session-level granularity, which limits analysis of rapid stress fluctuations. As a result, there is a need for a multimodal, high-resolution, open dataset that couples raw facial video and physiological streams with minute-level stress annotations.\n\nOur dataset, EmpathicSchool, provides physiological stress signals streams collected from Empatica E4 and facial features from videos of engineering and computer science students performing different tasks. Our primary motivation for creating this dataset was to generate a multimodal emotion-stress detection dataset and analyze students' emotions and stress levels while performing routine tasks such as preparing for a presentation or taking an exam. In addition, we analyze the emotions and stress levels of the students after task fulfillment. To summarize, EmpathicSchool addresses the gaps observed in the existing datasets in four principal ways:\n\n• True multimodality: we release both raw facial-video 1080p at 30 fps frame rate and synchronized physiological data (EDA, BVP, ACC, HR, and TEMP), enabling joint vision-biometric signal modeling.\n\n• Fine-grained labels: self-assessment stress and NASA-TLX workload scores are provided at 2-minute intervals, allowing detection of transient stress changes, unlike the task-level single labels in SWELL.\n\n• High temporal resolution: all modalities are timestamp-aligned, and video frames and biometric samples can be precisely upsampled or downsampled to any common rate up to 30 Hz, supporting both frame-based and windowed analysis.\n\n• Open, extensible data: we publish raw videos, landmark files, raw E4 streams, and all code for feature extraction and preprocessing so new facial-expression or biometric signal feature extraction algorithms can be applied directly.\n\nBy combining these improvements, EmpathicSchool substantially broadens the scope for research in near-real-time, multimodal stress detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "This section briefly introduces the sensors and protocols used in the data collection process. We also discuss the methods used to extract features from the video data and provide a summary of the sensors used for biomedical data acquisition. The data was collected in two locations: Tampere University, Finland, and the University of Louisiana at Lafayette, USA. We received approval from the institutional review boards of the University of Louisiana at Lafayette (SP-21-144-IRI) and the Tampere region's ethics committee (50/2021) for the study's protocol for the corresponding locations. Before data collection, each participant received an IRB-approved presentation detailing the nature and duration of the experimental tasks, the exact signals to be captured by the Empatica E4 wristband (including electrodermal activity, blood-volume pulse, skin temperature, and tri-axial acceleration) and a laptop camera, and the intention to publish de-identified physiological time-series with either their videos or facial-landmark data in an open repository, with a suggested consent revocation window of two months after data collection. For the data collected at Tampere University, participants may request the withdrawal of their data at any time prior to the publication of the paper. After publication, withdrawal is no longer possible, as all personally identifiable information is permanently removed and the data is fully anonymized. A written informed consent was then obtained in two stages. The main consent form authorized data collection and explicitly granted permission for public dissemination of the de-identified dataset, while preserving the participant's right to withdraw at any point before release. Immediately after the experiment, a separate debriefing form required participants to choose whether they would (a) share the de-identified facial-feature data only or (b) share both the video recordings and facial-feature data; this second document was also signed by all the participants in this dataset. All personal identifiers are stored separately under lock and key (consent forms) and on an encrypted server (video data) for six months after the data collection and before destruction, ensuring compliance with the confidentiality safeguards mandated by both ethics committees. No waiver of consent was requested was requested from the ethical review board.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Participants",
      "text": "The participants were recruited via flyers and recruitment emails in the engineering and computer science departments of the University of Louisiana at Lafayette and via email at Tampere University. We recruited 31 students, and after the data collection, 1 subject asked to be removed from the dataset. As a result, we had data from 30 participants aged 21 to 35 (Mean=25.3, Standard-Deviation=4.3). We asked participants (S7-S30) in Louisiana not to drink alcohol or coffee 24 hours before data collection. No such restrictions were applied to participants S1-S6 in Tampere. The exclusion criteria were pregnancy, heavy smoking, mental disorders, and chronic or cardiovascular diseases. Data collection in the United States was conducted in two phases to achieve the suggested minimum of 30 subjects for studies involving physiological and behavioral data  45  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Video Data",
      "text": "We asked the participants to perform all the tasks in a sitting position, and the camera captured only their faces and shoulders. The videos of the participants were collected using a 1080p external webcam mounted on top of a laptop screen with 30 frames per second. As approved by the institutional review board at the University of Louisiana at Lafayette, the video data of the participants who consented to release their video files are provided as part of the dataset. We post-processed the recorded videos to extract facial expressions as discussed in the following subsections. Figure  1  illustrates the experimental setup and the camera's position.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "The facial expression recognition system consists of two modules. In the first module, we used the Haar cascade  46  frontal face detection algorithm implemented in the OpenCV 47 library to detect the face in the video frame. Haar cascade uses Haar-like features to encode the local appearance of faces  48  . The detected faces were processed and passed through a pre-trained model to recognize facial expressions in the second module. We used MiniXception  49  , trained over the Facial Expression Recognition 2013 (FER-2013) dataset  50  for facial expression recognition. FER-2013 dataset contains 28,709 training images, 3,589 validation images, and 3,589 test images from the seven basic facial expression categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). Our experiments deduced the facial expressions from all the video recordings. Figure  2  demonstrates real-time emotion display through the utilization of the Haar cascade algorithm for face detection in conjunction with the MiniXception model for facial expression recognition. These data are provided as part of the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dlib Features",
      "text": "Facial landmarks are used in various computer vision tasks, such as drowsiness detection  51  , fatigue detection  52  , facial expression recognition  53  , and micro-expression classification  54  . Therefore, we provide the facial landmarks of all the video frames collected as part of the dataset. We utilize Dlib  55  , which returns the position of 68 facial landmarks. An example of landmark detection on a facial image based on Dlib is shown in Figure  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Action Units",
      "text": "Facial Action Coding System (FACS) is a comprehensive framework for understanding facial expressions, particularly the discrete movements and gestures referred to as Action Units (AUs). Recent studies have leveraged the granularity of the FACS to analyze stress levels through facial expressions  [56] [57] [58] [59]  . We do not provide pre-computed AUs in the dataset. However, we provide the code to generate Facial Action Unit features from facial landmarks features in our GitHub repository  60  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Physiological Signals/Biometric Data",
      "text": "We used the E4 wristband (Empatica Inc., Milano, Italy) for physiological data acquisition. E4 is a medical-grade wearable device that offers real-time acquisition of EDA, heart rate, skin temperature, and accelerometer data from the subject's dominant hand. EDA is measured via E4's silver (Ag) electrode (valid range [0.01-100] µS), while heart rate is measured via E4's photoplethysmographic (PPG) sensor. Using Bluetooth, E4 transmits data to the subject's smartphone. All data collected from the E4 wristband and the sampling frequencies are presented in Table  2 . The physiological signals that we measured via the E4 wristband and are provided in our dataset are as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Signal",
      "text": "• Heart rate: The heart rate of a healthy individual, irrespective of gender, ranges from 60 to 100 beats per minute at rest  61  . However, the heart rate varies significantly with activity or emotional state  62  . A high heart rate is generally associated with stressful situations  63, 64  , but a high heart rate should not by itself be interpreted as high stress  65  . Heart rate is not primary data and is generated from the BVP signal by the Empatica E4 device.\n\n• Skin temperature: Skin temperature varies for various activities due to skin blood flow  66  . Skin temperature ranges from 33.5 to 36.93 Celsius degrees  67  . However, this can vary quite widely based on the type and length of activity and ambient temperature  68  .\n\n• Electrodermal activity: EDA measures the amount of sweat gland activity by calculating the skin's electrical conductance using silver-chloride electrodes. The EDA signal is measured in units of micro-siemens (µS). Stadler et al,  69  mention that EDA peaks are event-related and can be a good estimator of the body's response to the stimulus  70  . Collecting EDA signal using wristbands is not very accurate, and some artifacts can affect the data collection results. Additionally, in some subjects, the EDA signal cannot be captured, or it can be inaccurate. However, the purpose of this study is to use the data in multimodal stress detection, which remains robust in the absence of a modality or noisy data. We believe that the combination of video and physiological signals can help researchers detect stress using off-the-shelf wristbands and video, despite imperfections in any single signal stream.\n\n• Accelerometer data: Accelerometer sensors can be used for multiple tasks (e.g., human action recognition and step counting)  71  . By measuring orientation and acceleration force, the accelerometer sensor can determine the device's orientation (horizontal or vertical) and the type of movement  72  . The accelerometers vary in type (digital and analog), sensitivity, and number of axes. Our dataset provides three-axis accelerometer data that measures the orientation of the sensor in three dimensions, which enables capturing the activity more precisely.\n\n• Blood Volume Pulse: Blood volume pulse is measured using a PPG that measures light absorption to find pulse volume peaks  73  . Heart rate is calculated by analyzing the time intervals between consecutive peaks. Thus, the BVP signal provides valuable information about heart activity.\n\n• Inter Beat Interval: The inter-beat interval is the time difference between two consecutive peaks in seconds, which can provide information about heart activity, such as heart rate. IBI is not primary data and is generated from the BVP signal by the Empatica E4 device.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Collection Protocol",
      "text": "Our stress detection study was conducted in a laboratory setting. We aimed to detect students' stress during their typical activities in a laboratory setting, while sitting behind a computer wearing a wristband. We collected data from 30 participants under different stress levels (normal and high stress). The normal stress level was assumed when the participants were at rest or performing a task that did not require significant mental effort. For the stressful tasks, we asked participants to accomplish the given assignment in a limited time  43  . Each of these tasks was supposed to be performed for 10 minutes. However, 30 seconds of data was trimmed from the beginning and end of each task to synchronize the video and physiological signals. This gives us 9 minutes of data for each participant and each task. The actions for nine different data collection sessions (tasks) and the duration of each session are provided in Table  3 . All subjects used the complete allotted time for all the tasks, except eight out of the 30 participants who completed delivering the presentation task (T3) earlier than the allotted time. In task T1, the students in the United States were asked to find and read an arbitrary article from Ars Technica or Wired Internet magazines, while at the Finland site, the students were asked to read an arbitrary magazine from the Internet according to their choice to reduce their stress level to achieve a baseline level of stress  74  . In task T2, the participants were asked to prepare for a presentation in a limited time, followed by the presentation task T3 in the allocated time. We considered both tasks (preparing and delivering presentations) as stressful tasks. In the next task (T4), participants were asked to take a rest. In task T5, the Stroop Color-Word (Finland) or IQ test (United States) was taken by each participant to observe their stress level while taking tests in a limited time period. In task T6, we asked the participants to listen to calm music, and we considered them not to be stressed during this activity  75  . The rest of the data collection included watching an amusing video (T7) followed by one or two different rest activities, namely controlled breathing exercises (T8) and a recovery period (T9). Only six (S1-S6) out of 30 participants were asked to have a recovery task (T9) after a controlled breathing exercise. The video and wristband data have different start and end times, so we trimmed 60 seconds of each task for data length consistency after time synchronization (30 seconds from the beginning and the end). In our study, there is a potential for the order effect. We could not mitigate it by testing different task orders, as we lacked a subject pool of sufficient size to have statistical significance. However, we sought to minimize the order effect by introducing rest periods between the tasks as illustrated in Figure  4 .\n\nOur protocol includes three stressful tasks that induce cognitive load and social-evaluative threat, identified as two dimensions of acute stress in the stress-induction literature  38, 76  . The participants prepared for a presentation under strict time pressure (T2), a manipulation that elicits moderate cognitive load and mirrors the preparation phase of the Trier Social Stress Test (TSST)  38  . They then delivered this talk to a camera while answering questions (T3), recreating the speech-and-arithmetic blocks of the TSST and, thereby, adding a social-evaluative component  38, 77  . Pure cognitive interference was induced in a timed Stroop/IQ task (T5), which elevates heart-rate and electrodermal activity  41, 78  . To bracket these stressors with low-arousal or down-regulation conditions, we included quiet reading (T1) as a resting baseline commonly used in stress laboratories  74  , listening to calm music (T6) to induce relaxation  75  , and paced breathing (T8) to recruit parasympathetic activity and vagal tone  79  . Collectively, this sequence reproduces the most widely used laboratory stress paradigms, such as public speaking, time-pressured mental arithmetic, and Stroop interference, and embeds them between three validated recovery tasks, enabling inter-subject comparisons across the full arousal continuum  38, 41, 80  . Moreover, these tasks were related to the students' daily routines, which we believe to have the potential to facilitate research on the stress of students while performing daily tasks.\n\nAn Empatica E4 was worn on the wrist of the dominant arm, and a camera collected facial videos during the studies. We continuously collected the students' physiological data and facial features during the sessions. We detected stress events while monitoring the physiological signal streams of the students. Further, We asked participants S7-S30 to validate their stress level during each stressful task by completing a self-assessment stress level check for each of the 2-minute intervals, along with a NASA questionnaire (NASA-TLX can be further used as a proxy for stress along with the stress assessment forms). The participants were also asked to complete a questionnaire if they experienced stress during the normal stress tasks. Figure  5  presents the framework for the data collection. A detailed explanation of all collected data is given in the Data records Section.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Figure 5. Data Collection Apparatus",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Stress Detection Surveys",
      "text": "Stress levels can be determined via subjects' self-assessment and National Aeronautics and Space Administration Task Load Index (NASA-TLX) questionnaires  81, 82  . In this study, we provided NASA-TLX forms to participants S7-S30 and asked them to complete the questionnaires after each stressful task. Each task was divided into 5 two-minute sub-intervals, except for the first and last sub-intervals that span over 90 seconds (1.5 minutes), adding up to 9 minutes for each task. Therefore, participants (S7-S30) were asked to determine their overall stress level at each sub-interval during stressful tasks. In addition, we also asked participants (S7-S30) to rate their overall workload during different stressful tasks and provided this information and labels in the dataset. Table  4  shows the NASA questionnaires that were asked to evaluate participants' stress levels from 0 to 20 for each sub-interval. To ensure the reliability of these scales, this study calculated Cronbach's α 83 for the questionnaire scores (0.922), which shows good internal consistency.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "No",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Records",
      "text": "The dataset is anonymized and available on Zenodo  84  . The dataset contains 30 folders (S1 to S30), one for each participant. Each folder consists of sub-folders (T1 to T9) according to the tasks performed by participants; the participants S7 to S30 did not have the recovery task T9 in their data collection. Table  5  shows the data availability for the participants in each folder. Each folder follows these naming conventions: The first letter of the filename shows the subject ID, followed by two letters showing the task number (e.g., T1). For example, LX corresponds to the validated stress levels of the task TX.\n\nTo assess stress levels during the stressful tasks, we administered self-assessment stress level questionnaires at 2-minute intervals. However, due to concerns about survey bias arising from the predictability of the questions  85  when frequently completing both the NASA-TLX and self-assessment forms, we decided to have participants complete the NASA-TLX forms only once, at the end of the task, to measure the overall workload of the task. The labels (overall and intervals) are provided in the stress_level.csv files. Task T3 (delivering a presentation) lasted 5-9 minutes because some participants finished the task early. In these cases, we split the task duration into 2-minute segments for labeling, resulting in fewer labeled intervals for those participants who finished early.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Data Files Description",
      "text": "We provide the files in the task sub-folders:\n\n• Xception.xlsx: The Excel file contains the facial expressions deduced at frame level for each task using MiniXception  49  , trained over the FER-2013 dataset  50  . The details are given in Section \"Facial expression recognition\". The sampling rate is 30 frames per second.\n\n• HR.csv: The file contains a single column with the average heart rate. The first row is the start time of the session in UTC time-stamp. The second row is the sampling rate expressed in Hz.\n\n• EDA.csv: The file contains a single column with the average electrodermal activity. The first row is the start time of the session in UTC time-stamp. The second row is the sampling rate expressed in Hz.\n\n• TEMP.csv: The file contains a single column with the average skin temperature. The first row is the start time of the session in UTC time-stamp. The second row is the sampling rate expressed in Hz.\n\n• Acc.csv The file contains three columns corresponding to the x-axis, y-axis, and z-axis accelerometer data, respectively.\n\nThe first row has the start times of the session in UTC time-stamp. The second row gives the sampling rates expressed in Hz.\n\n• V.mp4: The video file contains raw video files of participants who consented to release their videos. This file is not available for all participants.\n\n• Stress_level.csv: Thefile holds the validated stress level of the participants extracted from their answers. Each label is for a two-minute interval (represented as 0, 1, 2, and unknown, where 0 = no-stress; 1=low-stress; 2=high-stress, unknown for unknown stress level).\n\nID/Task T1 T2 L2 T3 L3 T4 T5 L5 T6 T7 L7 T8 T9 Video labels Site\n\nTable  5 . The availability of the tasks/data for each of the participants (✗ mark corresponds to the case where the data is not available for the participant, ✓shows the availability of the data).\n\n• Landmarks.csv: The file contains the normalized position of the participant's 68 facial landmarks (Dlib features described in Section \"Dlib features\") over time. The sampling rate of the facial landmarks is 30 frames per second, which is not provided in the landmarks csv file.\n\n• BVP.csv: The file contains a single column with the blood volume pulse signal. The first row is the start time of the session in UTC time-stamp. The second row is the sampling rate expressed in Hz.\n\n• IBI.csv: The file contains two columns; the first column shows the time-stamp, and the second column contains the corresponding inter-beat interval values.\n\nThe videos and physiological signals have exact start times and can be synced by interpolation, downsampling, or upsampling data processing methods.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Technical Validation",
      "text": "For the analysis presented in this paper, only subjects with complete and labeled datasets were considered. As a result, subjects S1 to S6, S20, and S30 were excluded. This exclusion was necessary to ensure the reliability and validity of the results, as missing labels or incomplete datasets could introduce bias or inaccuracies in the analysis. We used multi-class macro-averaging that calculates the metric independently for each stress level (low-, medium-, and high-stress) and then takes the average, treating all classes equally.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Facial Action Units",
      "text": "In this paper, we investigated the correlations between stress level and frequency of the different AUs in a time window of 20 seconds among 24 subjects with self-reported stress level labels. Table  6  shows the incidence of stress manifestation through facial AUs across the subjects. We generated AUs from facial landmarks  86  and compared the scores during different stress levels. It is noteworthy that not all subjects exhibited identical AUs when exposed to stress  87  . Despite this variability, we identified and reported correlations between specific AUs and stress in certain individuals, with detailed results presented in Table  6 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Action",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Machine Learning And Stress Detection Model",
      "text": "The EDA, HR, Temp, ACC signals, and video were collected at different sampling rates due to the variation of sensors. The frequency of the signals ranges from 1 to 30 Hz for heart rate and video data, respectively. We used the physiological signals, facial expressions, and AUs generated from videos as features for our stress detection model. We decided to use a frequency of 4 Hz after evaluating the accuracy of different models to minimize the information loss while monitoring the computational cost of the models. The code associated with the data preprocessing, upsampling, and downsampling is available on the GitHub repository  60  . The machine learning classification models process the physiological signals and facial expression features. The stress detection models are trained and tested based on the participants' survey answers. The output of the stress detection models is the stress level of the participant at the mentioned time step (0: No stress, 1: medium stress, 2: high stress).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Physiological Signals Vs. Facial Expressions",
      "text": "We analyzed the physiological signals obtained from the Empatica E4 watch and the cla facial expression probabilities from the video data for different tasks. Since the frame rate for the video is higher than the frequency of the physiological signals, we interpolated the physiological data to make it equal in length to the length of the deduced emotions from the video frames. To smooth the emotion data (Figure  6 ), we use a quadratic polynomial used in the Savitzky-Golay method  88  . The smoothing filter was applied to the data at the task level by keeping the span of the smoothing filter at 30 percent. After applying the smoothing filter, we scaled the data between 0 and 1 by applying xi = x i -x min x max -x min , where xi is the scaled value. x min is the minimum, and x max is the maximum smoothed value at the corresponding task/session for the participant. Figure  6  shows the happiness curves and physiological signals of subjects S1-S6 performing different tasks (T1-T9). The happiness curve is observed to fluctuate more in T5 and T7 for subjects S1-S6 as compared to other tasks. For T5, the fluctuation is attributed to the cognitive load and conflicting stimuli inherent in the task at hand, which evoke momentary frustration or challenge followed by happiness upon correct responses, resulting in emotional variability  89  . The fluctuations observed during T7 likely reflect the dynamic, amusing content of the videos themselves, which may induce peaks of amusement incorporated with neutral or less engaging moments  90  . For a given task, the temperature is observed to stay steady within the task; however, some differences have been observed in the temperature values between the tasks. The EDA curve is observed to stay steady during T1 and T2, while a slight variation is observed in T3. We also observed a peak heart rate value for subjects S2, S3, and S4 for T3. Although these curves illustrate how each task uniquely modulates facial expression and physiological signals, they do not establish causal relationships or explain why those modulations occur. Detailed modeling or controlled follow-up studies will be required to find the patterns of these observed fluctuations. Similar plots can be generated for comparing other emotions (Angry, Disgust, Fear, Sad, Surprise) to the physiological signals provided in the dataset.\n\nFigure  7  shows the overall stress level of the students S7-S30 performing different tasks based on participants' feedback. For example, the overall stress level in delivering a presentation was higher than in the other tasks. The stress level of the task in the presentation had a peak exactly in the middle of the presentation, compared to reading and IQ tests, where the overall stress level of the subjects increased while reaching the end of the task. In addition, the stress level during preparation for a presentation showed similar results to those of taking a test for the participants. A stress detection algorithm was designed to detect stress from facial expressions and physiological signals. In order to extract the features for the stress detection model, we first performed facial expression recognition in the video as described in the Section \"Facial expression recognition\". Consistent with prior work on wearable stress classification  [91] [92] [93]  , we extracted features in 20-second windows with a 5-second step (75% overlap). Twenty seconds is long enough to capture several phasic EDA responses  94  and to stabilize HRV  95  statistics while remaining short enough to detect rapid stress onsets  70, 96  . In total, we used 28 features extracted from both video and physiological signals.\n\n• Video signal features:\n\nThe mean of the facial expressions extracted from the video was concatenated with the features extracted from biometric signals. For each of the 3 physiological signals, we calculated the mean, minimum, maximum, and standard deviation along the 20-second sliding window.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "• Physiological Signals Features:",
      "text": "In addition, we extracted the kurtosis, skewness, number of peaks, amplitude, and duration of the peak for the EDA signal. For HR, we extracted the root mean square (quadratic mean), number of peaks, amplitude, and duration of the peaks. These statistical features for physiological signals were extracted based on an earlier work by  97  .\n\n• Labels:\n\nThe labels for each window were calculated based on the average stress values during the time window: \"no stress\" when S ≤ 0.65, \"medium stress\" when 0.65 < S ≤ 1.3, and \"high stress\" when S > 1.3. The source code for the stress detection algorithm comprises of feature extraction, stress detection, and change-point detection, and is provided in the GitHub repository  60  .\n\nThree models (Random Forest, Decision Tree, and XGBoost) were trained from the features extracted in the second step to predict stress during the 20-second window. To evaluate the effectiveness of the model and avoid overfitting, we followed a leave-one-subject-out strategy on the test dataset, where the model is trained on data from several participants and evaluated against the remainder. This process is repeated for all the participants. This dataset is imbalanced, and stress levels are distributed as: 1-low stress: 76%, 2-medium stress: 16%, and 3-high stress: 8%. We employed the SMOTE oversampling approach to balance the data  98  . The precision, recall, and F-score of these three approaches are presented in Table  7 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Model",
      "text": "Features The emotions model used the mean of the facial expressions extracted from the video to detect stress. The biometrics model used the 21 biometric features to train the machine learning model. A large number of features may lead to an overfitted model that is not generalizable to unseen data (curse of dimensionality)  99  . To reduce the number of features, we used Pearson correlation analysis  100  to select the top 10 features correlated with stress. The top 10 features we observed were 'lip_puckerer_mean', 'lower_lip_depressor_mean', 'temp_mean', 'temp_max', 'temp_min', 'eda_mean', 'eda_max', 'eda_min', 'eda_peak_amplitude', and 'hr_min' using information gain. We trained each machine learning model with these 10 features to predict the value of stress. The code for training the machine learning models and evaluating the results is available in GitHub  60  .\n\nWe evaluated the performance of Random Forest, Decision Tree, and XGBoost, using balanced data and a Leave-One-Subject-Out (LOSO) cross-validation. The models were trained using facial expression, physiological signals, and a multimodal combination of the top ten features. The Random Forest classifier, with 100 trees (n_estimators=100), a depth of 7(max_depth=7), and a minimum sample leaf size of 5, achieved the best performance with the multimodal feature set, with a recall of 79.72%, precision of 73.46%, and an F1-score of 69.32% compared to emotions alone with a recall of 48.64% and an F1-score of 48.02%. The Decision Tree classifier, using a maximum depth of 5 and a minimum sample leaf size of 3, with the multimodal features, achieved a recall of 69.89%, precision of 63.86%, and an F1-score of 60.19%, while its performance with emotion signals alone remained limited, with a recall of 45.56% and an F1-score of 45.51%. The XGBoost model, with a maximum depth of 6 and a gamma value of 0, outperformed other models with the biometric feature set, achieving a recall of 79.30, a precision of 74.28, and an F1-score of 70. 16.\n\nEmotions, such as anger (angry), surprise, and sadness (sad), exhibit high correlation with stress  56, 101, 102  . Several authors have previously discussed this relationship between heart rate, emotions, and stress  63, 65, 103, 104  . Earlier studies on stress detection evaluated the relationship between stress and skin temperature  105, 106  . The skin temperature drops during the onset of stress  107  and increases above the normal temperature after the onset  107, 108  . Therefore, skin temperature and stress are not directly correlated. However, a sudden decrease in skin temperature is a good indicator of early stress, but during later time periods, the skin temperature is higher. The machine learning models can use these complex relationships to identify high-stress levels, as represented in Table  7 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Linear Models With Mixed Effects",
      "text": "Linear Models with Mixed effects (LMMs) are an extension of linear models that incorporate fixed and random effects. These models are especially useful for hierarchical or nested data.\n\n• Mixed effects in LMMs are analogous to the coefficients in standard linear regression models, representing the average effect of explanatory variables on the response variable.\n\n• Random effects account for variations at different levels of the hierarchy, such as differences between subjects or groups. This dual structure allows LMMs to model the overall trend and the individual deviations from this trend. To handle potential outliers, we applied user-level standardization. Table  8  presents the LMM results for physiological features and facial AUs when participants were treated as random effects. For example, the feature hr_std has a significant negative correlation (p<0.05) to high stress, b = -0.111, z = -3.163, p = 0.002. Similarly, inner_brow_raiser_mean has a negative coefficient of -0.058 (p = 0.001), illustrating that higher inner brow-raiser activity is associated with low-stress levels. We also observe positive relationships for lid_tightener_mean (coefficient = 0.141, p = 0.010) and lip_corner_depressor_mean (coefficient = 0.085, p = 0.001) with higher stress levels. Multimodal features in the form of facial AUs and physiological signals together showed statistically significant correlations that are predictive for stress modeling.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Feature",
      "text": "For model fitting, we used the Limited-memory Broyden-Fletcher-Goldfarb-Shanno optimization algorithm, method='lbfgs', which is quite efficient for large datasets with complex random effect structures. We also have robust maximum likelihood",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the experimental setup and the",
      "page": 4
    },
    {
      "caption": "Figure 1: Participants’ sitting and camera positions. *This image is captured from one of the authors who consented to have",
      "page": 4
    },
    {
      "caption": "Figure 2: demonstrates real-time emotion display through the utilization of the Haar cascade algorithm for face detection in conjunction",
      "page": 4
    },
    {
      "caption": "Figure 3: Facial Action Units",
      "page": 4
    },
    {
      "caption": "Figure 2: The deployed system captures and displays emotions from live video frames, using the Haar cascade algorithm for",
      "page": 5
    },
    {
      "caption": "Figure 3: 68 facial landmarks. *This image is captured from one of the authors who consented to have his image in the paper;",
      "page": 5
    },
    {
      "caption": "Figure 4: Schematic overview of the EmpathicSchool data collection protocol, showing the sequence and duration of each",
      "page": 7
    },
    {
      "caption": "Figure 4: Our protocol includes three stressful tasks that induce cognitive load and social-evaluative threat, identified as two",
      "page": 7
    },
    {
      "caption": "Figure 5: presents the framework for the data collection. A detailed explanation of all collected data is given in the Data records Section.",
      "page": 7
    },
    {
      "caption": "Figure 5: Data collection apparatus",
      "page": 7
    },
    {
      "caption": "Figure 6: ), we use a quadratic polynomial used in the Savitzky-Golay method88. The smoothing filter",
      "page": 10
    },
    {
      "caption": "Figure 6: Analysis of subjects S1-S6 physiological signals and facial expressions for tasks T1-T9.",
      "page": 10
    },
    {
      "caption": "Figure 6: shows the happiness curves and physiological signals of subjects S1-S6 performing different tasks (T1-T9). The",
      "page": 11
    },
    {
      "caption": "Figure 7: shows the overall stress level of the students S7-S30 performing different tasks based on participants’ feedback.",
      "page": 11
    },
    {
      "caption": "Figure 7: Overall stress level of the students performing different stressful tasks",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Signal": "Heart Activity",
          "Device": "Electrocardiography",
          "Abbr": "ECG\nEKG"
        },
        {
          "Signal": "Skin Response",
          "Device": "Electrodermal Activity",
          "Abbr": "EDA"
        },
        {
          "Signal": "Skin Response",
          "Device": "Galvanic Skin Response",
          "Abbr": "GSR"
        },
        {
          "Signal": "Muscle Activity",
          "Device": "Electromyography",
          "Abbr": "EMG"
        },
        {
          "Signal": "Respiratory Response",
          "Device": "Electromagnetic Generation",
          "Abbr": "RR"
        },
        {
          "Signal": "Respiratory Response",
          "Device": "Respiratory Inductive Plethysmograph",
          "Abbr": "RIP"
        },
        {
          "Signal": "Blood Volume Pulse",
          "Device": "Cardiovascular Dynamics",
          "Abbr": "BVP\nHR\nHRV"
        },
        {
          "Signal": "Skin Temperature",
          "Device": "Thermistor",
          "Abbr": "TEMP"
        },
        {
          "Signal": "Brain Activity",
          "Device": "Electroencephalogram",
          "Abbr": "EEG"
        },
        {
          "Signal": "Eye Activity",
          "Device": "Corneo-retinal Standing",
          "Abbr": "EOG"
        },
        {
          "Signal": "Physical Activity",
          "Device": "3-axis Accelerometer",
          "Abbr": "ACC"
        },
        {
          "Signal": "Physical Activity",
          "Device": "3-axis Magnetometer",
          "Abbr": "MGM"
        },
        {
          "Signal": "Heart Activity",
          "Device": "Sphygmomanometer",
          "Abbr": "BP"
        },
        {
          "Signal": "Eye Characteristics",
          "Device": "Pupil Diameter Detector Module",
          "Abbr": "PD"
        },
        {
          "Signal": "Body Temperature",
          "Device": "Thermal Imaging",
          "Abbr": "TI"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy": "",
          "Angry:  0.01%\nDisgust:  0.00%\nFear:  0.00%": "Happy:  99.84%"
        },
        {
          "Happy": "",
          "Angry:  0.01%\nDisgust:  0.00%\nFear:  0.00%": "Sad:  0.01%\nSurprised:  0.01%\nNeutral:  0.13%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Signal": "Electrodermal Activity\nHeart Rate\nSkin Temperature\nAccelerometer\nInter-Beat Interval\nBlood Volume Pulse",
          "Abbreviation": "EDA\nHR\nST\nACC\nIBI\nBVP",
          "Frequency": "4.0 Hz\n1.0 Hz\n1.0 Hz\n32 Hz\n—-\n64 Hz"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "T1",
          "Action performed": "Reading a magazine",
          "Duration (min)": "9",
          "Expected Stress Level": "Normal"
        },
        {
          "Task": "T2",
          "Action performed": "Preparing a presentation",
          "Duration (min)": "9",
          "Expected Stress Level": "Stressed"
        },
        {
          "Task": "T3",
          "Action performed": "Delivering the presentation",
          "Duration (min)": "5-9",
          "Expected Stress Level": "Stressed"
        },
        {
          "Task": "T4",
          "Action performed": "Rest and recovery",
          "Duration (min)": "9",
          "Expected Stress Level": "Normal"
        },
        {
          "Task": "T5",
          "Action performed": "IQ test / Stroop Color-Word Test",
          "Duration (min)": "9",
          "Expected Stress Level": "Stressed"
        },
        {
          "Task": "T6",
          "Action performed": "Listening to calm music",
          "Duration (min)": "9",
          "Expected Stress Level": "Normal"
        },
        {
          "Task": "T7",
          "Action performed": "Watching amusing video(s)",
          "Duration (min)": "9",
          "Expected Stress Level": "Amused"
        },
        {
          "Task": "T8",
          "Action performed": "Controlled breathing exercise",
          "Duration (min)": "9",
          "Expected Stress Level": "Normal"
        },
        {
          "Task": "T9",
          "Action performed": "Recovery",
          "Duration (min)": "9",
          "Expected Stress Level": "Normal"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No": "1",
          "Category": "Mental Demand",
          "Question": "How mentally demanding was the task?"
        },
        {
          "No": "2",
          "Category": "Physical Demand",
          "Question": "How physically demanding was the task?"
        },
        {
          "No": "3",
          "Category": "Temporal Demand",
          "Question": "How hurried or rushed was the pace of the task?"
        },
        {
          "No": "4",
          "Category": "Performance",
          "Question": "How successful were you in accomplishing what you were asked to do?"
        },
        {
          "No": "5",
          "Category": "Effort",
          "Question": "How hard did you have to work to accomplish your level of performance?"
        },
        {
          "No": "6",
          "Category": "Frustration",
          "Question": "How insecure, discouraged, irritated, stressed, and annoyed were you?"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID/Task": "S1",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S2",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S3",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S4",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S5",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S6",
          "T1": "✓",
          "T2": "✓",
          "L2": "✗",
          "T3": "✓",
          "L3": "✗",
          "T4": "✓",
          "T5": "✓",
          "L5": "✗",
          "T6": "✓",
          "T7": "✓",
          "L7": "✗",
          "T8": "✓",
          "T9": "✓",
          "Video": "✗",
          "labels": "✗",
          "Site": "Finland"
        },
        {
          "ID/Task": "S7",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S8",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S9",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S10",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S11",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S12",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S13",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S14",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S15",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S16",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S17",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S18",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S19",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S20",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✗",
          "T5": "✗",
          "L5": "✗",
          "T6": "✗",
          "T7": "✗",
          "L7": "✗",
          "T8": "✗",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S21",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S22",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S23",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S24",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S25",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S26",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S27",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S28",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S29",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✓",
          "L5": "✓",
          "T6": "✓",
          "T7": "✓",
          "L7": "✓",
          "T8": "✓",
          "T9": "✗",
          "Video": "✓",
          "labels": "✓",
          "Site": "U.S."
        },
        {
          "ID/Task": "S30",
          "T1": "✓",
          "T2": "✓",
          "L2": "✓",
          "T3": "✓",
          "L3": "✓",
          "T4": "✓",
          "T5": "✗",
          "L5": "✗",
          "T6": "✗",
          "T7": "✗",
          "L7": "✗",
          "T8": "✗",
          "T9": "✗",
          "Video": "✗",
          "labels": "✓",
          "Site": "U.S."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Action Unit": "Lip Tightener",
          "AU number": "12",
          "Number of subjects": "17"
        },
        {
          "Action Unit": "Lip Stretcher",
          "AU number": "11",
          "Number of subjects": "16"
        },
        {
          "Action Unit": "Mouth Stretch",
          "AU number": "15",
          "Number of subjects": "15"
        },
        {
          "Action Unit": "Lip Depressor",
          "AU number": "8",
          "Number of subjects": "14"
        },
        {
          "Action Unit": "Lid Droop",
          "AU number": "25",
          "Number of subjects": "13"
        },
        {
          "Action Unit": "Nostril Compressor",
          "AU number": "24",
          "Number of subjects": "10"
        },
        {
          "Action Unit": "Cheek Raiser",
          "AU number": "4",
          "Number of subjects": "9"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Random Forest"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Emotions\n7\n48.64\n50.08\n48.02"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Biometric\n21\n65.62\n64.07\n61.18"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Multimodal (Top 10)\n79.72\n73.46\n69.32\n10"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Decision Tree"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Emotions\n7\n45.56\n46.13\n45.51"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Biometric\n21\n58.25\n58.49\n56.37"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Top 10\n69.89\n63.86\n60.19\n10"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "XGBoost"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Emotions\n7\n70.52\n69.69\n66.44"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Biometric\n79.30\n70.16\n21\n74.28"
        },
        {
          "Model\nFeatures\nRecall\nPrecision\nF1_score": "Multimodal (Top 10)\n81.64\n74.82\n10\n69.28"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "hr_std",
          "Coef.": "-0.111",
          "Std.Err.": "0.035",
          "Z": "-3.163",
          "P>|Z|": "0.002",
          "[0.025": "-0.180",
          "0.975]": "-0.042"
        },
        {
          "Feature": "hr_peak_amplitude",
          "Coef.": "0.038",
          "Std.Err.": "0.017",
          "Z": "2.299",
          "P>|Z|": "0.021",
          "[0.025": "0.006",
          "0.975]": "0.071"
        },
        {
          "Feature": "hr_peak_duration",
          "Coef.": "-0.020",
          "Std.Err.": "0.007",
          "Z": "-2.855",
          "P>|Z|": "0.004",
          "[0.025": "-0.034",
          "0.975]": "-0.006"
        },
        {
          "Feature": "eda_peak_duration",
          "Coef.": "0.007",
          "Std.Err.": "0.003",
          "Z": "2.249",
          "P>|Z|": "0.025",
          "[0.025": "0.001",
          "0.975]": "0.013"
        },
        {
          "Feature": "inner_brow_raiser_mean",
          "Coef.": "-0.058",
          "Std.Err.": "0.018",
          "Z": "-3.216",
          "P>|Z|": "0.001",
          "[0.025": "-0.093",
          "0.975]": "-0.023"
        },
        {
          "Feature": "inner_brow_raiser_max",
          "Coef.": "-0.046",
          "Std.Err.": "0.017",
          "Z": "-2.725",
          "P>|Z|": "0.006",
          "[0.025": "-0.079",
          "0.975]": "-0.013"
        },
        {
          "Feature": "inner_brow_raiser_kurtosis",
          "Coef.": "0.007",
          "Std.Err.": "0.003",
          "Z": "2.053",
          "P>|Z|": "0.040",
          "[0.025": "0.000",
          "0.975]": "0.013"
        },
        {
          "Feature": "inner_brow_raiser_skewness",
          "Coef.": "0.007",
          "Std.Err.": "0.004",
          "Z": "1.968",
          "P>|Z|": "0.049",
          "[0.025": "0.000",
          "0.975]": "0.015"
        },
        {
          "Feature": "brow_lowerer_mean",
          "Coef.": "-0.060",
          "Std.Err.": "0.029",
          "Z": "-2.095",
          "P>|Z|": "0.036",
          "[0.025": "-0.117",
          "0.975]": "-0.004"
        },
        {
          "Feature": "upper_lid_raiser_min",
          "Coef.": "0.076",
          "Std.Err.": "0.038",
          "Z": "2.006",
          "P>|Z|": "0.045",
          "[0.025": "0.002",
          "0.975]": "0.151"
        },
        {
          "Feature": "upper_lid_raiser_max",
          "Coef.": "-0.091",
          "Std.Err.": "0.036",
          "Z": "-2.555",
          "P>|Z|": "0.011",
          "[0.025": "-0.160",
          "0.975]": "-0.021"
        },
        {
          "Feature": "upper_lid_raiser_std",
          "Coef.": "0.051",
          "Std.Err.": "0.022",
          "Z": "2.272",
          "P>|Z|": "0.023",
          "[0.025": "0.007",
          "0.975]": "0.095"
        },
        {
          "Feature": "upper_lid_raiser_skewness",
          "Coef.": "0.020",
          "Std.Err.": "0.010",
          "Z": "2.073",
          "P>|Z|": "0.038",
          "[0.025": "0.001",
          "0.975]": "0.039"
        },
        {
          "Feature": "cheek_raiser_feature_min",
          "Coef.": "0.061",
          "Std.Err.": "0.026",
          "Z": "2.362",
          "P>|Z|": "0.018",
          "[0.025": "0.010",
          "0.975]": "0.111"
        },
        {
          "Feature": "lid_tightener_mean",
          "Coef.": "0.141",
          "Std.Err.": "0.055",
          "Z": "2.583",
          "P>|Z|": "0.010",
          "[0.025": "0.034",
          "0.975]": "0.249"
        },
        {
          "Feature": "lid_tightener_std",
          "Coef.": "-0.066",
          "Std.Err.": "0.021",
          "Z": "-3.058",
          "P>|Z|": "0.002",
          "[0.025": "-0.108",
          "0.975]": "-0.024"
        },
        {
          "Feature": "nose_wrinkler_std",
          "Coef.": "-0.033",
          "Std.Err.": "0.012",
          "Z": "-2.719",
          "P>|Z|": "0.007",
          "[0.025": "-0.057",
          "0.975]": "-0.009"
        },
        {
          "Feature": "nose_wrinkler_kurtosis",
          "Coef.": "0.008",
          "Std.Err.": "0.004",
          "Z": "1.982",
          "P>|Z|": "0.047",
          "[0.025": "0.000",
          "0.975]": "0.017"
        },
        {
          "Feature": "upper_lip_raiser_min",
          "Coef.": "-0.031",
          "Std.Err.": "0.013",
          "Z": "-2.376",
          "P>|Z|": "0.018",
          "[0.025": "-0.056",
          "0.975]": "-0.005"
        },
        {
          "Feature": "upper_lip_raiser_kurtosis",
          "Coef.": "-0.012",
          "Std.Err.": "0.004",
          "Z": "-3.291",
          "P>|Z|": "0.001",
          "[0.025": "-0.019",
          "0.975]": "-0.005"
        },
        {
          "Feature": "nasolabial_furrow_deepener_mean",
          "Coef.": "0.049",
          "Std.Err.": "0.025",
          "Z": "1.983",
          "P>|Z|": "0.047",
          "[0.025": "0.001",
          "0.975]": "0.097"
        },
        {
          "Feature": "dimpler_std",
          "Coef.": "0.029",
          "Std.Err.": "0.014",
          "Z": "2.040",
          "P>|Z|": "0.041",
          "[0.025": "0.001",
          "0.975]": "0.058"
        },
        {
          "Feature": "lip_corner_depressor_mean",
          "Coef.": "0.085",
          "Std.Err.": "0.025",
          "Z": "3.339",
          "P>|Z|": "0.001",
          "[0.025": "0.035",
          "0.975]": "0.135"
        },
        {
          "Feature": "lip_corner_depressor_max",
          "Coef.": "0.056",
          "Std.Err.": "0.022",
          "Z": "2.525",
          "P>|Z|": "0.012",
          "[0.025": "0.013",
          "0.975]": "0.100"
        },
        {
          "Feature": "lip_corner_depressor_kurtosis",
          "Coef.": "-0.011",
          "Std.Err.": "0.003",
          "Z": "-3.336",
          "P>|Z|": "0.001",
          "[0.025": "-0.018",
          "0.975]": "-0.005"
        },
        {
          "Feature": "lower_lip_depressor_min",
          "Coef.": "0.241",
          "Std.Err.": "0.076",
          "Z": "3.190",
          "P>|Z|": "0.001",
          "[0.025": "0.093",
          "0.975]": "0.389"
        },
        {
          "Feature": "lower_lip_depressor_skewness",
          "Coef.": "-0.007",
          "Std.Err.": "0.003",
          "Z": "-2.011",
          "P>|Z|": "0.044",
          "[0.025": "-0.014",
          "0.975]": "-0.000"
        },
        {
          "Feature": "chin_raiser_max",
          "Coef.": "-0.088",
          "Std.Err.": "0.027",
          "Z": "-3.255",
          "P>|Z|": "0.001",
          "[0.025": "-0.140",
          "0.975]": "-0.035"
        },
        {
          "Feature": "chin_raiser_std",
          "Coef.": "-0.048",
          "Std.Err.": "0.014",
          "Z": "-3.280",
          "P>|Z|": "0.001",
          "[0.025": "-0.076",
          "0.975]": "-0.019"
        },
        {
          "Feature": "lip_puckerer_kurtosis",
          "Coef.": "0.009",
          "Std.Err.": "0.004",
          "Z": "2.093",
          "P>|Z|": "0.036",
          "[0.025": "0.001",
          "0.975]": "0.017"
        },
        {
          "Feature": "lip_funneler_max",
          "Coef.": "0.054",
          "Std.Err.": "0.023",
          "Z": "2.384",
          "P>|Z|": "0.017",
          "[0.025": "0.010",
          "0.975]": "0.099"
        },
        {
          "Feature": "lip_funneler_skewness",
          "Coef.": "0.013",
          "Std.Err.": "0.005",
          "Z": "2.612",
          "P>|Z|": "0.009",
          "[0.025": "0.003",
          "0.975]": "0.023"
        },
        {
          "Feature": "jaw_drop_min",
          "Coef.": "-0.066",
          "Std.Err.": "0.030",
          "Z": "-2.229",
          "P>|Z|": "0.026",
          "[0.025": "-0.124",
          "0.975]": "-0.008"
        },
        {
          "Feature": "jaw_thrust_skewness",
          "Coef.": "-0.040",
          "Std.Err.": "0.017",
          "Z": "-2.387",
          "P>|Z|": "0.017",
          "[0.025": "-0.073",
          "0.975]": "-0.007"
        },
        {
          "Feature": "puff_std",
          "Coef.": "-0.030",
          "Std.Err.": "0.013",
          "Z": "-2.382",
          "P>|Z|": "0.017",
          "[0.025": "-0.055",
          "0.975]": "-0.005"
        },
        {
          "Feature": "tongue_bulge_std",
          "Coef.": "-0.032",
          "Std.Err.": "0.013",
          "Z": "-2.548",
          "P>|Z|": "0.011",
          "[0.025": "-0.057",
          "0.975]": "-0.007"
        },
        {
          "Feature": "nostil_dilator_std",
          "Coef.": "0.027",
          "Std.Err.": "0.012",
          "Z": "2.199",
          "P>|Z|": "0.028",
          "[0.025": "0.003",
          "0.975]": "0.052"
        },
        {
          "Feature": "lid_droop_mean",
          "Coef.": "0.037",
          "Std.Err.": "0.013",
          "Z": "2.821",
          "P>|Z|": "0.005",
          "[0.025": "0.011",
          "0.975]": "0.063"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "2",
      "title": "Stress detection in daily life scenarios using smart phones and wearable sensors: A survey",
      "authors": [
        "Y Can",
        "B Arnrich",
        "C Ersoy"
      ],
      "year": "2019",
      "venue": "J. biomedical informatics"
    },
    {
      "citation_id": "3",
      "title": "Continuous inference of psychological stress from sensory measurements collected in the natural environment",
      "authors": [
        "K Plarre"
      ],
      "year": "2011",
      "venue": "Proceedings of the 10th ACM/IEEE international conference on information processing in sensor networks"
    },
    {
      "citation_id": "4",
      "title": "Large-scale wearable data reveal digital phenotypes for daily-life stress detection",
      "authors": [
        "E Smets"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "5",
      "title": "Towards mental stress detection using wearable physiological sensors",
      "authors": [
        "J Wijsman",
        "B Grundlehner",
        "H Liu",
        "H Hermens",
        "J Penders"
      ],
      "year": "2011",
      "venue": "2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "6",
      "title": "Contextual usage patterns in smartphone communication services",
      "authors": [
        "J Karikoski",
        "T Soikkeli"
      ],
      "year": "2013",
      "venue": "Pers. ubiquitous computing"
    },
    {
      "citation_id": "7",
      "title": "A survey on mobile affective computing",
      "authors": [
        "E Politou",
        "E Alepis",
        "C Patsakis"
      ],
      "year": "2017",
      "venue": "Comput. Sci. Rev"
    },
    {
      "citation_id": "8",
      "title": "Fear of job loss: Racial/ethnic differences in privileged occupations",
      "authors": [
        "G Wilson",
        "K Mossakowski"
      ],
      "year": "2009",
      "venue": "Du Bois Rev. Soc. Sci. Res. on Race"
    },
    {
      "citation_id": "9",
      "title": "A presence-based context-aware chronic stress recognition system",
      "authors": [
        "K Peternel",
        "M Pogačnik",
        "R Tavčar",
        "A Kos"
      ],
      "year": "2012",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Stress in the workplace: A general overview of the causes, the effects, and the solutions",
      "authors": [
        "M Bickford"
      ],
      "year": "2005",
      "venue": "Can. Mental Heal. Assoc. Nfld. Labrador Div"
    },
    {
      "citation_id": "11",
      "title": "Inflammation, stress, and diabetes",
      "authors": [
        "K Wellen",
        "G Hotamisligil"
      ],
      "year": "2005",
      "venue": "The J. clinical investigation"
    },
    {
      "citation_id": "12",
      "title": "Workload and burnout in nurses",
      "authors": [
        "E Greenglass",
        "R Burke",
        "L Fiksenbaum"
      ],
      "year": "2001",
      "venue": "J. community & applied social psychology"
    },
    {
      "citation_id": "13",
      "title": "Physiology, stress reaction",
      "authors": [
        "B Chu",
        "K Marwaha",
        "T Sanvictores",
        "A Awosika",
        "D Ayers"
      ],
      "year": "2024",
      "venue": "StatPearls"
    },
    {
      "citation_id": "14",
      "title": "Review of psychophysiology: Human behavior and physiological response",
      "authors": [
        "S Filipovic"
      ],
      "year": "2001",
      "venue": "PsycNet"
    },
    {
      "citation_id": "15",
      "title": "",
      "authors": [
        "B Hoffman",
        "Adrenaline"
      ],
      "year": "2013",
      "venue": ""
    },
    {
      "citation_id": "16",
      "title": "Stress appraisal in the workplace and its associations with productivity and mood: Insights from a multimodal machine learning analysis",
      "authors": [
        "M Awada",
        "B Becerik Gerber",
        "G Lucas",
        "S Roll"
      ],
      "year": "2024",
      "venue": "Plos one"
    },
    {
      "citation_id": "17",
      "title": "Eustress or distress: An empirical study of perceived stress in everyday college life",
      "authors": [
        "C.-T Li",
        "J Cao",
        "T Li"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "18",
      "title": "Does leisure time moderate or mediate the effect of daily stress on positve affect? an examination using eight-day diary data",
      "authors": [
        "X Qian",
        "C Yarnal",
        "D Almeida"
      ],
      "year": "2014",
      "venue": "J. leisure research"
    },
    {
      "citation_id": "19",
      "title": "Sense of purpose moderates the associations between daily stressors and daily well-being",
      "authors": [
        "P Hill",
        "N Sin",
        "N Turiano",
        "A Burrow",
        "D Almeida"
      ],
      "year": "2018",
      "venue": "Annals Behav"
    },
    {
      "citation_id": "20",
      "title": "The impact of emotional exhaustion on psychological factors in workers with secondary traumatic experiences: A multi-group path analysis",
      "authors": [
        "M Jin"
      ],
      "year": "2020",
      "venue": "Psychiatry Investig"
    },
    {
      "citation_id": "21",
      "title": "Protective and damaging effects of stress mediators: central role of the brain",
      "authors": [
        "B Mcewen"
      ],
      "year": "2006",
      "venue": "Dialogues clinical neuroscience"
    },
    {
      "citation_id": "22",
      "title": "Stress and health: psychological, behavioral, and biological determinants",
      "authors": [
        "N Schneiderman",
        "G Ironson",
        "S Siegel"
      ],
      "year": "2005",
      "venue": "Annu. Rev. Clin. Psychol"
    },
    {
      "citation_id": "23",
      "title": "A multidimensional theory of burnout",
      "authors": [
        "C Maslach"
      ],
      "year": "1998",
      "venue": "Theor. organizational stress"
    },
    {
      "citation_id": "24",
      "title": "The usa perspective: Current issues and trends in the management of work stress",
      "authors": [
        "L Murphy",
        "S Sauter"
      ],
      "year": "2003",
      "venue": "Aust. Psychol"
    },
    {
      "citation_id": "25",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "26",
      "title": "A multi-modal sensor dataset for continuous stress detection of nurses in a hospital",
      "authors": [
        "S Hosseini"
      ],
      "year": "2021",
      "venue": "A multi-modal sensor dataset for continuous stress detection of nurses in a hospital",
      "arxiv": "arXiv:2108.07689"
    },
    {
      "citation_id": "27",
      "title": "Facial expression based satisfaction index for empathic buildings",
      "authors": [
        "F Sohrab",
        "J Raitoharju",
        "M Gabbouj"
      ],
      "year": "2020",
      "venue": "Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "28",
      "title": "Stress detection using physiological sensors",
      "authors": [
        "R Sioni",
        "L Chittaro"
      ],
      "year": "2015",
      "venue": "Computer"
    },
    {
      "citation_id": "29",
      "title": "Stress detection and management: A survey of wearable smart health devices",
      "authors": [
        "H Thapliyal",
        "V Khalus",
        "C Labrado"
      ],
      "year": "2017",
      "venue": "IEEE Consumer Electron. Mag"
    },
    {
      "citation_id": "30",
      "title": "New methods for stress assessment and monitoring at the workplace",
      "authors": [
        "D Carneiro",
        "P Novais",
        "J Augusto",
        "N Payne"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affect. Comput"
    },
    {
      "citation_id": "31",
      "title": "Towards an automatic early stress recognition system for office environments based on multimodal measurements: A review",
      "authors": [
        "A Alberdi",
        "A Aztiria",
        "A Basarab"
      ],
      "year": "2016",
      "venue": "J. biomedical informatics"
    },
    {
      "citation_id": "32",
      "title": "Predicting anxiety state using smartphone-based passive sensing",
      "authors": [
        "Y Fukazawa"
      ],
      "year": "2019",
      "venue": "J. biomedical informatics"
    },
    {
      "citation_id": "33",
      "title": "A survey of machine learning techniques in physiology based mental stress detection systems",
      "authors": [
        "S Panicker",
        "P Gayathri"
      ],
      "year": "2019",
      "venue": "Biocybern. Biomed. Eng"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "35",
      "title": "Stress detection from speech and galvanic skin response signals",
      "authors": [
        "H Kurniawan",
        "A Maslov",
        "M Pechenizkiy"
      ],
      "year": "2013",
      "venue": "Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems"
    },
    {
      "citation_id": "36",
      "title": "System and method using thermal image analysis for polygraph testing",
      "authors": [
        "I Pavlidis"
      ],
      "year": "2005",
      "venue": "US Patent"
    },
    {
      "citation_id": "37",
      "title": "Affectiveroad system and database to assess driver's attention",
      "authors": [
        "N Haouij",
        "J.-M Poggi",
        "S Sevestre-Ghalila",
        "R Ghozi",
        "M Jaïdane"
      ],
      "year": "2018",
      "venue": "Proceedings of the 33rd Annual ACM Symposium on Applied Computing"
    },
    {
      "citation_id": "38",
      "title": "The 'trier social stress test'-a tool for investigating psychobiological stress responses in a laboratory setting",
      "authors": [
        "C Kirschbaum",
        "K Pirke",
        "D Hellhammer"
      ],
      "year": "1993",
      "venue": "Neuropsychobiology"
    },
    {
      "citation_id": "39",
      "title": "Introducing mdpsd, a multimodal dataset for psychological stress detection",
      "authors": [
        "W Chen",
        "S Zheng",
        "X Sun"
      ],
      "year": "2020",
      "venue": "Big Data: 8th CCF Conference"
    },
    {
      "citation_id": "40",
      "title": "The trier social stress test protocol for inducing psychological stress",
      "authors": [
        "M Birkett"
      ],
      "year": "2011",
      "venue": "JoVE (Journal Vis. Exp. e"
    },
    {
      "citation_id": "41",
      "title": "The stroop color and word test",
      "authors": [
        "F Scarpina",
        "S Tagini"
      ],
      "year": "2017",
      "venue": "Front. Psychol",
      "doi": "10.3389/fpsyg.2017.00557"
    },
    {
      "citation_id": "42",
      "title": "Tiles-2018, a longitudinal physiologic and behavioral data set of hospital workers",
      "authors": [
        "K Mundnich"
      ],
      "year": "2020",
      "venue": "Sci. Data"
    },
    {
      "citation_id": "43",
      "title": "The swell knowledge work dataset for stress and user modeling research",
      "authors": [
        "S Koldijk",
        "M Sappelli",
        "S Verberne",
        "M Neerincx",
        "W Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th international conference on multimodal interaction"
    },
    {
      "citation_id": "44",
      "title": "Stress and productivity patterns of interrupted, synergistic, and antagonistic office activities",
      "authors": [
        "S Zaman"
      ],
      "year": "2019",
      "venue": "Sci. data"
    },
    {
      "citation_id": "45",
      "title": "Understanding power and rules of thumb for determining sample sizes",
      "authors": [
        "C Vanvoorhis",
        "B Morgan"
      ],
      "year": "2007",
      "venue": "Tutor Quant Methods Psychol"
    },
    {
      "citation_id": "46",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR"
    },
    {
      "citation_id": "47",
      "title": "The opencv library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal: Softw. Tools for Prof. Program"
    },
    {
      "citation_id": "48",
      "title": "Evaluation of haar cascade classifiers designed for face detection",
      "authors": [
        "R Padilla",
        "C Costa Filho",
        "M Costa"
      ],
      "year": "2012",
      "venue": "World Acad. Sci. Eng. Technol"
    },
    {
      "citation_id": "49",
      "title": "Real-time convolutional neural networks for emotion and gender classification",
      "authors": [
        "O Arriaga",
        "M Valdenegro-Toro",
        "P Plöger"
      ],
      "year": "2017",
      "venue": "Real-time convolutional neural networks for emotion and gender classification",
      "arxiv": "arXiv:1710.07557"
    },
    {
      "citation_id": "50",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "51",
      "title": "Design of real-time drowsiness detection system using dlib",
      "authors": [
        "S Mohanty",
        "S Hegde",
        "S Prasad",
        "J Manikandan"
      ],
      "year": "2019",
      "venue": "IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE)"
    },
    {
      "citation_id": "52",
      "title": "Fatigue detection using facial landmarks",
      "authors": [
        "N Irtija",
        "M Sami",
        "M Ahad"
      ],
      "year": "2018",
      "venue": "International Symposium on Affective Science and Engineering ISASE2018"
    },
    {
      "citation_id": "53",
      "title": "Progressive spatio-temporal bilinear network with monte carlo dropout for landmark-based facial expression recognition with uncertainty estimation",
      "authors": [
        "N Heidari",
        "A Iosifidis"
      ],
      "year": "2021",
      "venue": "IEEE International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Micro-expression classification based on landmark relations with graph attention convolutional network",
      "authors": [
        "A Kumar",
        "B Bhanu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The J. Mach. Learn. Res"
    },
    {
      "citation_id": "56",
      "title": "Automatic stress detection evaluating models of facial action units",
      "authors": [
        "G Giannakakis",
        "M Koujan",
        "A Roussos",
        "K Marias"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE international conference on automatic face and gesture recognition (FG 2020)"
    },
    {
      "citation_id": "57",
      "title": "Predicting depression, anxiety, and stress levels from videos using the facial action coding system",
      "authors": [
        "A Authors"
      ],
      "year": "2019",
      "venue": "Sensors",
      "doi": "10.3390/s19173693"
    },
    {
      "citation_id": "58",
      "title": "Automatic stress detection evaluating models of facial action units",
      "authors": [
        "A Authors"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "59",
      "title": "Automatic stress analysis from facial videos based on deep facial action units recognition",
      "authors": [
        "A Authors"
      ],
      "year": "2021",
      "venue": "Pattern Analysis Appl",
      "doi": "10.1007/s10044-021-01012-9"
    },
    {
      "citation_id": "60",
      "title": "",
      "authors": [
        "Cphslab",
        "Empathicschool"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "61",
      "title": "Resting heart rate in cardiovascular disease",
      "authors": [
        "K Fox"
      ],
      "year": "2007",
      "venue": "J. Am. Coll. Cardiol"
    },
    {
      "citation_id": "62",
      "title": "Differences of heart rate variability between happiness and sadness emotion states: a pilot study",
      "authors": [
        "H Shi"
      ],
      "year": "2017",
      "venue": "J. Med. Biol. Eng"
    },
    {
      "citation_id": "63",
      "title": "Influence of mental stress on heart rate and heart rate variability",
      "authors": [
        "J Taelman",
        "S Vandeput",
        "A Spaepen",
        "S Van Huffel"
      ],
      "year": "2009",
      "venue": "4th European Conference of the International Federation for Medical and Biological Engineering",
      "doi": "10.1007/978-3-540-89208-3_324"
    },
    {
      "citation_id": "64",
      "title": "Stress and heart rate variability: A meta-analysis and review of the literature",
      "authors": [
        "H.-G Kim",
        "E.-J Cheon",
        "D.-S Bai",
        "Y.-H Lee",
        "B.-H Koo"
      ],
      "year": "2018",
      "venue": "Psychiatry Investig",
      "doi": "10.30773/pi.2017.08.17"
    },
    {
      "citation_id": "65",
      "title": "Effects of stress on heart rate complexity-a comparison between short-term and chronic stress",
      "authors": [
        "C Schubert"
      ],
      "year": "2009",
      "venue": "Biol. psychology"
    },
    {
      "citation_id": "66",
      "title": "A simplified approach to describe the mean skin temperature variations during prolonged running exercise",
      "authors": [
        "G Tanda"
      ],
      "year": "2021",
      "venue": "J. Therm. Biol"
    },
    {
      "citation_id": "67",
      "title": "The temperature of the skin surface",
      "authors": [
        "W Bierman"
      ],
      "year": "1936",
      "venue": "J. Am. Med. Assoc"
    },
    {
      "citation_id": "68",
      "title": "Skin temperature changes during physical activities and passive heating",
      "authors": [
        "J Choi",
        "S Yeom"
      ],
      "year": "2013",
      "venue": "J. Phys. Ther. Sci",
      "doi": "10.1589/jpts.25.1029"
    },
    {
      "citation_id": "69",
      "title": "Electrodermal activity measurement within a qualitative methodology: Exploring emotion in leisure experiences",
      "authors": [
        "R Stadler",
        "A Jepson",
        "E Wood"
      ],
      "year": "2018",
      "venue": "Int. J. Contemp. Hosp. Manag"
    },
    {
      "citation_id": "70",
      "title": "Electrodermal Activity",
      "authors": [
        "W Boucsein"
      ],
      "year": "2012",
      "venue": "Electrodermal Activity"
    },
    {
      "citation_id": "71",
      "title": "A study on human activity recognition using accelerometer data from smartphones",
      "authors": [
        "A Bayat",
        "M Pomplun",
        "D Tran"
      ],
      "year": "2014",
      "venue": "Procedia Comput. Sci"
    },
    {
      "citation_id": "72",
      "title": "Validity and reliability of accelerations and orientations measured using wearable sensors during functional activities",
      "authors": [
        "T Cudejko",
        "K Button",
        "M Al-Amri"
      ],
      "year": "2022",
      "venue": "Sci. reports"
    },
    {
      "citation_id": "73",
      "title": "Photoplethysmography for blood volumes and oxygenation changes during intermittent vascular occlusions",
      "authors": [
        "T Abay",
        "P Kyriacou"
      ],
      "year": "2018",
      "venue": "J. clinical monitoring computing"
    },
    {
      "citation_id": "74",
      "title": "Relaxing effects of reading and listening to music after a brief stressor",
      "authors": [
        "D Knight",
        "N Rickard"
      ],
      "year": "2001",
      "venue": "Aust. J. Psychol"
    },
    {
      "citation_id": "75",
      "title": "Playing music reduces salivary cortisol levels and improves mood after acute stress",
      "authors": [
        "A Ferrer"
      ],
      "year": "2014",
      "venue": "Med. Sci. Monit"
    },
    {
      "citation_id": "76",
      "title": "The trier social stress test: principles and practice",
      "authors": [
        "A Allen"
      ],
      "year": "2017",
      "venue": "Neurobiol. stress"
    },
    {
      "citation_id": "77",
      "title": "The trier social stress test: Principles and practice",
      "authors": [
        "A Allen"
      ],
      "year": "2017",
      "venue": "Neurobiol. Stress"
    },
    {
      "citation_id": "78",
      "title": "The stress of stroop performance: Physiological and emotional responses to color-word interference, task pacing, and pacing speed",
      "authors": [
        "P Renaud",
        "J.-P Blondin"
      ],
      "year": "1997",
      "venue": "Int. J. Psychophysiol",
      "doi": "10.1016/S0167-8760(97)00049-4"
    },
    {
      "citation_id": "79",
      "title": "Slow-paced breathing for stress management",
      "authors": [
        "S Laborde"
      ],
      "year": "2021",
      "venue": "Psychophysiology",
      "doi": "10.1111/psyp.13770"
    },
    {
      "citation_id": "80",
      "title": "The paced auditory serial addition test (pasat): A review and update",
      "authors": [
        "T Tombaugh"
      ],
      "year": "2018",
      "venue": "Clinical Tests of Working Memory"
    },
    {
      "citation_id": "81",
      "title": "Nasa-task load index (nasa-tlx); 20 years later",
      "authors": [
        "S Hart"
      ],
      "year": "2006",
      "venue": "Proceedings of the human factors and ergonomics society annual meeting"
    },
    {
      "citation_id": "82",
      "title": "Development of nasa-tlx (task load index): Results of empirical and theoretical research",
      "authors": [
        "S Hart",
        "L Staveland"
      ],
      "year": "1988",
      "venue": "Advances in psychology"
    },
    {
      "citation_id": "83",
      "title": "Making sense of cronbach's alpha. Int",
      "authors": [
        "M Tavakol",
        "R Dennick"
      ],
      "year": "2011",
      "venue": "journal medical education"
    },
    {
      "citation_id": "84",
      "title": "Empathicschool: A multimodal dataset for real-time facial expressions and physiological data analysis under different stress conditions",
      "authors": [
        "M Hosseini"
      ],
      "year": "2023",
      "venue": "Empathicschool: A multimodal dataset for real-time facial expressions and physiological data analysis under different stress conditions",
      "doi": "10.5281/zenodo.15556502"
    },
    {
      "citation_id": "85",
      "title": "Extreme nonresponse and response bias: A \"worst case",
      "authors": [
        "O Hellevik"
      ],
      "year": "2016",
      "venue": "analysis. Qual. & Quant"
    },
    {
      "citation_id": "86",
      "title": "Man's face and mimic language. Studen litteratur",
      "authors": [
        "C.-H Hjortsjo"
      ],
      "year": "1969",
      "venue": "Man's face and mimic language. Studen litteratur"
    },
    {
      "citation_id": "87",
      "title": "Robust continuous prediction of human emotions using multiscale dynamic cues",
      "authors": [
        "J Nicolle",
        "V Rapp",
        "K Bailly",
        "L Prevost",
        "M Chetouani"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "88",
      "title": "What is a savitzky-golay filter?[lecture notes]",
      "authors": [
        "R Schafer"
      ],
      "year": "2011",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "89",
      "title": "Emotional processing in anterior cingulate and medial prefrontal cortex",
      "authors": [
        "A Etkin",
        "T Egner",
        "R Kalisch"
      ],
      "year": "2011",
      "venue": "Trends cognitive sciences"
    },
    {
      "citation_id": "90",
      "title": "Video affective content analysis: A survey of state-of-the-art methods",
      "authors": [
        "S Wang",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affect. Comput"
    },
    {
      "citation_id": "91",
      "title": "Continuous stress detection using wearable sensors in real life: Algorithm selection and imu validation",
      "authors": [
        "M Gjoreski",
        "H Gjoreski",
        "M Luštrek"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp '17"
    },
    {
      "citation_id": "92",
      "title": "Using wearable sensors to detect stress in the wild",
      "authors": [
        "J Hernández",
        "R Picard"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 IEEE International Conference on Affective Computing and Intelligent Interaction, ACII '11"
    },
    {
      "citation_id": "93",
      "title": "Stress classification from wearable sensor data: Impact of feature selection and class imbalance",
      "authors": [
        "J Kim",
        "E Garcia",
        "R Picard"
      ],
      "year": "2018",
      "venue": "IEEE J. Biomed. Heal. Informatics",
      "doi": "10.1109/JBHI.2017.2654359"
    },
    {
      "citation_id": "94",
      "title": "A continuous measure of phasic electrodermal activity",
      "authors": [
        "M Benedek",
        "C Kaernbach"
      ],
      "year": "2010",
      "venue": "J. neuroscience methods"
    },
    {
      "citation_id": "95",
      "title": "Heart rate variability: new perspectives on physiological mechanisms, assessment of selfregulatory capacity, and health risk",
      "authors": [
        "R Mccraty",
        "F Shaffer"
      ],
      "year": "2015",
      "venue": "Glob. advances health medicine"
    },
    {
      "citation_id": "96",
      "title": "An overview of heart rate variability metrics and norms",
      "authors": [
        "F Shaffer",
        "J Ginsberg"
      ],
      "year": "2017",
      "venue": "An overview of heart rate variability metrics and norms",
      "doi": "10.3389/fpubh.2017.00258"
    },
    {
      "citation_id": "97",
      "title": "Random forest-based approach for physiological functional variable selection for driver's stress level classification",
      "authors": [
        "N El Haouij",
        "J.-M Poggi",
        "R Ghozi",
        "S Sevestre-Ghalila",
        "M Jaïdane"
      ],
      "year": "2019",
      "venue": "Stat. Methods & Appl"
    },
    {
      "citation_id": "98",
      "title": "Synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer",
        "Smote"
      ],
      "year": "2002",
      "venue": "J. Artif. Intell. Res"
    },
    {
      "citation_id": "99",
      "title": "The effect of different dimensionality reduction techniques on machine learning overfitting problem",
      "authors": [
        "M Salam",
        "A Azar",
        "M Elgendy",
        "K Fouad"
      ],
      "year": "2021",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "100",
      "title": "Pearson correlation coefficient",
      "authors": [
        "J Benesty",
        "J Chen",
        "Y Huang",
        "I Cohen"
      ],
      "year": "2009",
      "venue": "Noise reduction in speech processing"
    },
    {
      "citation_id": "101",
      "title": "Heightened stress in employed individuals is linked to altered variability and inertia in emotions",
      "authors": [
        "D Wang",
        "S Schneider",
        "J Schwartz",
        "A Stone"
      ],
      "year": "2020",
      "venue": "Front. psychology"
    },
    {
      "citation_id": "102",
      "title": "Remote measurement of cognitive stress via heart rate variability",
      "authors": [
        "D Mcduff",
        "S Gontarek",
        "R Picard"
      ],
      "year": "2014",
      "venue": "36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1109/EMBC.2014.6944243"
    },
    {
      "citation_id": "103",
      "title": "Effects of work stress on ambulatory blood pressure, heart rate, and heart rate variability",
      "authors": [
        "T Vrijkotte",
        "L Van Doornen",
        "E De Geus"
      ],
      "year": "2000",
      "venue": "Hypertension"
    },
    {
      "citation_id": "104",
      "title": "Validation of polar heart rate monitor for assessing heart rate during physical and mental stress",
      "authors": [
        "J Goodie",
        "K Larkin",
        "S Schauss"
      ],
      "year": "2000",
      "venue": "J. Psychophysiol"
    },
    {
      "citation_id": "105",
      "title": "Pain and stress detection using wearable sensors and devices-a review",
      "authors": [
        "J Chen",
        "M Abbod",
        "J.-S Shieh"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "106",
      "title": "Stress detection using deep neural networks",
      "authors": [
        "R Li",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "BMC Med. Informatics Decis. Mak"
    },
    {
      "citation_id": "107",
      "title": "Stress detection in computer users based on digital signal processing of noninvasive physiological variables",
      "authors": [
        "J Zhai",
        "A Barreto"
      ],
      "year": "2006",
      "venue": "2006 international conference of the IEEE engineering in medicine and biology society"
    },
    {
      "citation_id": "108",
      "title": "Feasibility study on driver's stress detection from differential skin temperature measurement",
      "authors": [
        "T Yamakoshi"
      ],
      "year": "2008",
      "venue": "2008 30th annual international conference of the ieee engineering in medicine and biology society"
    },
    {
      "citation_id": "109",
      "title": "Effects of recall bias and nonresponse bias on self-report estimates of angling participation. North Am",
      "authors": [
        "M Tarrant",
        "M Manfredo",
        "P Bayley",
        "R Hess"
      ],
      "year": "1993",
      "venue": "J. Fish. Manag"
    },
    {
      "citation_id": "110",
      "title": "Newcomer socialization and stress: Formal peer relationships as a source of support",
      "authors": [
        "T Allen",
        "S Mcmanus",
        "J Russell"
      ],
      "year": "1999",
      "venue": "J. Vocat. Behav"
    },
    {
      "citation_id": "111",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}