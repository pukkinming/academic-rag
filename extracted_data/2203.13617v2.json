{
  "paper_id": "2203.13617v2",
  "title": "Emotionnas: Two-Stream Neural Architecture Search For Speech Emotion Recognition",
  "published": "2022-03-25T12:35:44Z",
  "authors": [
    "Haiyang Sun",
    "Zheng Lian",
    "Bin Liu",
    "Ying Li",
    "Licai Sun",
    "Cong Cai",
    "Jianhua Tao",
    "Meng Wang",
    "Yuan Cheng"
  ],
  "keywords": [
    "speech emotion recognition",
    "neural architecture search",
    "two-stream model",
    "information supplement module"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is an important research topic in human-computer interaction. Existing works mainly rely on human expertise to design models. Despite their success, different datasets often require distinct structures and hyperparameters. Searching for an optimal model for each dataset is time-consuming and labor-intensive. To address this problem, we propose a two-stream neural architecture search (NAS) based framework, called \"EmotionNAS\". Specifically, we take two-stream features (i.e., handcrafted and deep features) as the inputs, followed by NAS to search for the optimal structure for each stream. Furthermore, we incorporate complementary information in different streams through an efficient information supplement module. Experimental results demonstrate that our method outperforms existing manually-designed and NASbased models, setting the new state-of-the-art record.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has received increasing attention due to its contribution to human-computer interactions  [1, 2, 3] . SER aims to understand how humans express their emotions and then classify each utterance into its emotional state  [4, 5] . Existing works are mainly manually-designed models  [6, 7, 8, 9] . Despite their success, these works rely on historical experience to design model structures, which is often time-consuming and labor-intensive  [10, 11] . Therefore, how to design networks more intelligently has been brought into focus. To this end, we explore neural architecture search (NAS) for SER  [12, 13] . By setting the search space, the search strategy, and the evaluation metric, we can optimize the model architecture automatically with little human intervention.\n\nPreviously, search methods based on reinforcement learning  [14]  or evolutionary algorithms  [15]  avoided the exhaustive search and provided deep learning-inspired design principles. However, these methods need the retraining process every time a new substructure is sampled, which consumes numerous computational resources. To speed up the search process, researchers propose differentiable architecture search (DARTS)  [10, 16, 17, 18] , which relaxes the search space to be continuous by applying a softmax operation. Additionally, some efforts  [19, 20]  build a model library by pre-training multiple structures. These methods successfully alleviate the time-consuming process in previous works. In this paper, we rely on more efficient search methods to design SER models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "*Equal Contribution ‚Ä† Corresponding Author",
      "text": "In addition to model design, another challenge in SER is how to represent audios  [21] . Among existing handcrafted features, spectrograms are widely utilized in SER  [22, 23, ?, 24] . However, the spectrogram loses some phase information during calculation. Recently, the deep feature, wav2vec  [25] , has demonstrated its effectiveness in speech representation learning  [26] . It is a self-supervised framework that can learn powerful acoustic representations with the help of large amounts of unlabeled data. However, wav2vec may lose some emotionrelated information due to different training objectives. To obtain a more comprehensive speech representation, we integrate handcrafted and deep features via a two-stream framework.\n\nMeanwhile, how to fuse multiple features also affects the classification performance. Unsuitable fusion approaches cannot effectively utilize the complementary information in different features, which may lead to performance degradation compared to the best-performing feature (denoted as the dominant feature in this paper). Therefore, we further design an effective fusion method, called \"information supply module (ISM)\", to ensure that the performance of the dominant feature is not degraded and can be further improved.\n\nIn summary, to address the low efficiency of the manuallydesigned approach and inefficient fusion of different features, we propose a novel two-stream framework called \"Emotion-NAS\". Figure  1  shows the overall structure of our proposed method. Experimental results show that EmotionNAS outperforms existing manually-designed and NAS-based models. The contributions of this paper can be summarized as follows:\n\n‚Ä¢ We propose a novel framework called \"EmotionNAS\". It incorporates complementary information in handcrafted and deep features, followed by NAS to efficiently search through numerous possible networks to find the optimal structure. ‚Ä¢ We further design ISM to effectively fuse complementary information in different features. ‚Ä¢ Experimental results on IEMOCAP show that our method successfully outperforms existing manually-designed and NAS-based models, setting the new state-of-the-art record.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "EmotionNAS is a two-stream framework that combines handcrafted and deep features for SER. To search for the optimal structure, we explore NAS in model design. Subsequently, we employ ISM to fuse two-stream features effectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Architecture Search For Spectrogram",
      "text": "To better utilize the time-frequency two-dimensional information in the spectrogram, our method automatically optimizes the CNN-based structure through an efficient architecture search al-  The overall structure of EmotionNAS. It is a twostream architecture that takes spectrogram and wav2vec as the inputs, followed by NAS to design the model automatically. We further fuse the outputs of two branches through ISM to achieve better performance. gorithm DARTS  [16] . It divides the whole network into normal and reduction cells. All operations adjacent to the reduction cells have a stride of 2, while others have a stride of 1.\n\nFigure  2  shows the overall search process in a cell. Suppose each cell consists of an ordered sequence of N nodes. Let x (i) denotes the latent feature of the i-th node. Assume O is a set of candidate operations, and |O| represents the number of operations. To make the search space continuous, DARTS incorporates all possible operations through weights. We take the operations from x (i) to x (j) as an example:\n\nwhere Œ∏\n\nrepresent the raw and normalized weights of the operation o ‚àà O from x (i) to x (j) , respectively. The task of architecture search reduces to learning a set of continuous weights {Œ± (i,j) o }. At the end of the search, we only keep the operation with the highest weight.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Architecture Search For Wav2Vec",
      "text": "Since wav2vec contains temporal information, we search for the optimal RNN-based structure for this branch  [20] . The search space includes all regular operations in RNN, such as linear mapping, blending, activation function, and element-wise operation. The optimized cell structure is shared across different timesteps. For timestep t, the initial nodes consist of the input vector xt and two hidden states h 1 t-1 and h 2 t-1 . We aim to optimize the cell architecture and generate new hidden states for the next timestep, i.e., h 1 t and h 2 t (see Section 4.6 for details). At the end of the search, we feed wav2vec into the optimized recurrent neural architecture to generate frame-level representations. Since different frames play different parts in SER, we further exploit an attention mechanism to prioritize important frames and fuse them  [27] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Information Supplement Module",
      "text": "After finding the optimal structure for each stream, we propose an ISM to fuse complementary information from two streams. To ensure that the performance of the dominant feature is not degraded after fusion, we abandon to compress this feature. Instead, we select complementary information from other inputs and fuse them with the dominant feature.\n\nSpecifically, suppose the outputs of two branches are denoted as X1 ‚àà R d 1 and X2 ‚àà R d 2 , respectively. Here, d1 and d2 are the feature dimensions of X1 and X2. Suppose X2 is the dominant feature. The calculation formula of our ISM can be summarized as follows:\n\nwhere Proj(‚Ä¢) and ‚äô represent a linear transformation function and an element-wise multiplication, respectively. Here,\n\nIn the end, we leverage the fused feature F ‚àà R d 2 for emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Database And Setup",
      "text": "We first describe the benchmark dataset in our experiments.\n\nFollowing that, we illustrate the feature extraction process, implementation details, and various current advanced baselines.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Database",
      "text": "IEMOCAP  [28]  is a benchmark dataset for SER. It consists of five sessions, each with two speakers. For a fair comparison, we adopt five-fold cross-validation with the leave-one-session-out strategy  [9] . Eight speakers from four sessions are used as the training set. One speaker in the remaining session is used as the validation set and the other as the test set. We evaluate four emotions (i.e., neutral, angry, happy and sad) on improvised data, in line with previous works  [9] . Due to the imbalanced class distribution, we use unweighted accuracy (UA) as the primary metric and also report weighted accuracy (WA).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "In EmotionNAS, we take spectrogram and wav2vec as inputs. The extraction processes are described as follows: Spectrogram: We utilize the librosa toolkit  [29]  to extract spectrograms from audio. We unify the duration of audio into 8 seconds by zero padding and truncation. Then, a spectrogram is extracted with 25ms Hamming windows and 14ms overlap. Finally, we downsample the spectrogram to (140 √ó 140) using an average pooling operation.\n\nWav2vec: We use the pretrained wav2vec-large  [25]  as the acoustic feature extractor. We unify the wav2vec feature to the maximum length by zero padding, followed by average pooling to downsample it to (727 √ó 512).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation Details",
      "text": "Our method consists of three key modules: 1) NAS for spectrogram; 2) NAS for wav2vec; 3) ISM for fusion. Since wav2vec can achieve better performance in SER, we treat this feature as the dominant feature. For the search process, we set hyperparameters based on validation performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Nas For Spectrogram:",
      "text": "In each cell, we set the number of nodes N = 4 and the number of operations |O| = 8. The search space contains regular operations in CNNs, such as 3 √ó 3 max pooling, 3 √ó 3 average pooling, skip connection, 3 √ó 3 separable convolution, 5√ó5 separable convolution, 3√ó3 dilated convolution, 5 √ó 5 dilated convolution, and no connection. At the same time, we set the initial number of channels C = 6 and the number of layers L = 3.\n\nNAS for wav2vec: The search space contains regular operations in RNNs, such as linear function, blending, element-wise product, element-wise sum, tanh function, sigmoid function, and Leaky ReLU function. Previous work  [20]  provided various RNN-based cell structures searched on the Penn Tree Bank dataset  [30] . We choose the structure with the lowest validation loss in the emotion dataset. Meanwhile, we set the number of layers L = 1 and the hidden feature dimension H = 256.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baselines",
      "text": "To verify the effectiveness of our method, we compare the performance of EmotionNAS with various baselines: CNN-GRU  [6]  combines CNN and GRU to recognize emotional states. CTC-RNN  [7]  exploits connectionist temporal classification  [31]  to annotate labels for audio segments automatically. Seq-Cap  [8]  uses the capsule network and GRUs to capture spatialtemporal information. PCNSE-SADRN-CTC  [9]  models longrange dependencies by combining parallel convolutional layers, squeeze-and-excitation networks, and self-attention dilated residual networks. UniformNAS  [32]  employs NAS to search the optimal structure, and uses a uniform path dropout strategy to encourage all structures to be searched equally.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "In this section, we first conduct comparative experiments with currently advanced systems to verify the effectiveness of our method. Then, we systemically investigate the importance of each module in EmotionNAS, including the NAS-based approach, two-stream framework, and ISM fusion strategy. Finally, we visualize the confusion matrices and search results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison With Existing Works",
      "text": "To verify the effectiveness of our method, we treat current advanced approaches as our baselines. These methods include existing manually-designed and NAS-based models with consistent data division. We report baseline results according to their original papers. Experimental results in Table  1  show that EmotionNAS outperforms most existing methods and achieves a performance improvement of 2.8% over manually-designed models  [9] . Compared with these approaches, our NAS-based framework can optimize structures and find more effective models than manually-designed approaches, automatically. Meanwhile, we observe that EmotionNAS outperforms UniformNAS  [32] , a NAS-based approach, by 12.2% on UA and 1.2% on WA. But limited by handcrafted features, this baseline cannot leverage the useful information from deep features. Our framework, by contrast, not only enables efficient network design but also effectively fuses handcrafted and deep features, bringing better performance on SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Advantage Of Nas",
      "text": "In this section, we compare the performance of several NASbased and manually-designed models. Experimental results are shown in Table  2‚àº3 . For the spectrogram branch, NAS-based methods can achieve better performance with fewer parameters (see Table  2 ). For the wav2vec branch, our method also outperforms RNN and LSTM in emotion recognition (see Table  3 ). These results reveal the advantage of NAS in model design.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Necessity Of Two Branches",
      "text": "In this section, we further reveal the necessity of two branches. As shown in Table  4 , spectrogram and wav2vec achieve 57.3% and 66.2% on UA, 63.2% and 70.3% on WA, respectively. After multi-branch fusion, we further improve UA to 69.1% and  WA to 72.1%. These results confirm that handcrafted and deep features contain complementary information in emotion recognition. Through the fusion process, we can achieve better classification performance on SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Effectiveness Of Ism",
      "text": "To verify the effectiveness of our method, we compare ISM with other fusion strategies. Followed with the symbols in Section 2.3, we denote the outputs of two branches as X1 and X2. Here, X1 is the dominant feature and H1 is the linear transformation of X1. Concat concatenates X1 and X2. Sum, Max and Min denote the corresponding operations on H1 and X2.\n\nExperimental results are listed in Table  5 . Among the four comparison strategies, only Min and Concat surpass the performance of the dominant branch. It indicates that most strategies cannot effectively fuse complementary information in different features. But through ISM, we can significantly improve the performance of UA and WA. These results demonstrate the effectiveness of our fusion strategy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visualization Of Confusion Matrices",
      "text": "Figure  3  shows the confusion matrices for Min and Emotion-NAS. We observe that EmotionNAS achieves better performance in most emotion categories, especially happy and angry. Our findings also highlight the limitations of traditional fusion strategies. Compared with Min, our fusion method ISM can effectively fuse multiple features and achieve better performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visualization Of Search Results",
      "text": "In this section, we further visualize the search results for each branch. Figure  4 (a)‚àº4(b) show the search results for the spectrogram branch. There are two types of cells: the normal cell and the reduction cell. For cell k, the initial nodes consist of the outputs in previous cells, i.e., c k-2 and c k-1 . Figure  4(c)  shows the search result for the wav2vec branch. For timestep t, the initial nodes consist of the input vector xt and two hidden states h 1 t-1 and h 2 t-1 . We aim to generate new hidden states for the next timestep, i.e., h 1 t and h 2 t .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel framework for SER, called \"EmotionNAS\". It takes spectrogram and wav2vec as the inputs, followed by NAS to optimize the network structure automatically. We further design ISM to integrate complementary information in different branches. Experimental results demonstrate that our method outperforms existing manually-designed and NAS-based methods. Meanwhile, we also systemically prove the importance of each component in EmotionNAS, including the NAS-based method, the two-stream framework and the ISM fusion strategy.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the overall structure of our proposed",
      "page": 1
    },
    {
      "caption": "Figure 1: The overall structure of EmotionNAS. It is a two-",
      "page": 2
    },
    {
      "caption": "Figure 2: DARTS architecture search in a cell: (a) the initial",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the overall search process in a cell. Sup-",
      "page": 2
    },
    {
      "caption": "Figure 3: Visualization of confusion matrices. (a) Results of",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the confusion matrices for Min and Emotion-",
      "page": 4
    },
    {
      "caption": "Figure 4: (a)‚àº4(b) show the search results for the spec-",
      "page": 4
    },
    {
      "caption": "Figure 4: Visualization of search results: (a) and (b) are search",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "emotions and then classify each utterance into its emotional"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "state [4, 5]. Existing works are mainly manually-designed mod-"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "els [6, 7, 8, 9]. Despite their success,\nthese works rely on his-"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "torical experience to design model\nstructures, which is often"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "time-consuming and labor-intensive [10, 11]. Therefore, how to"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "design networks more intelligently has been brought into focus."
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": ""
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "To this end, we explore neural architecture search (NAS)\nfor"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "SER [12, 13]. By setting the search space,\nthe search strategy,"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "and the evaluation metric, we can optimize the model architec-"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "ture automatically with little human intervention."
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "Previously, search methods based on reinforcement\nlearn-"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "ing [14] or evolutionary algorithms\n[15] avoided the exhaus-"
        },
        {
          "[1, 2, 3].\nSER aims to understand how humans express their": "tive search and provided deep learning-inspired design princi-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "3Department of Automation, Tsinghua University, 4Ant Group"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "lianzheng2016}@ia.ac.cn"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "In addition to model design, another challenge in SER is"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "how to represent audios [21]. Among existing handcrafted fea-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "tures, spectrograms are widely utilized in SER [22, 23, ?, 24]."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "However,\nthe spectrogram loses some phase information dur-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "ing calculation. Recently,\nthe deep feature, wav2vec [25], has"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "demonstrated its effectiveness in speech representation learning"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "[26].\nIt\nis a self-supervised framework that can learn power-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "ful acoustic representations with the help of\nlarge amounts of"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "unlabeled data.\nHowever, wav2vec may lose some emotion-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "related information due to different\ntraining objectives. To ob-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "tain a more comprehensive speech representation, we integrate"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "handcrafted and deep features via a two-stream framework."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "Meanwhile, how to fuse multiple features also affects the"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "classification performance. Unsuitable fusion approaches can-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "not effectively utilize the complementary information in differ-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "ent features, which may lead to performance degradation com-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "pared to the best-performing feature (denoted as the dominant"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "feature in this paper).\nTherefore, we further design an effec-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "tive fusion method, called ‚Äúinformation supply module (ISM)‚Äù,"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "to ensure that\nthe performance of\nthe dominant\nfeature is not"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "degraded and can be further improved."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "In summary, to address the low efficiency of the manually-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "designed approach and inefficient\nfusion of different\nfeatures,"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "we propose a novel\ntwo-stream framework called ‚ÄúEmotion-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "NAS‚Äù. Figure 1 shows\nthe overall\nstructure of our proposed"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "method. Experimental results show that EmotionNAS outper-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "forms existing manually-designed and NAS-based models. The"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "contributions of this paper can be summarized as follows:"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "‚Ä¢ We propose a novel framework called ‚ÄúEmotionNAS‚Äù. It in-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "corporates\ncomplementary information in handcrafted and"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "deep features, followed by NAS to efficiently search through"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "numerous possible networks to find the optimal structure."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "‚Ä¢ We further design ISM to effectively fuse complementary in-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "formation in different features."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "‚Ä¢ Experimental\nresults on IEMOCAP show that our method"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "successfully\noutperforms\nexisting manually-designed\nand"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "NAS-based models, setting the new state-of-the-art record."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "2. Methodology"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "EmotionNAS is a two-stream framework that combines hand-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "crafted and deep features for SER. To search for\nthe optimal"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "structure, we explore NAS in model design. Subsequently, we"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "employ ISM to fuse two-stream features effectively."
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "2.1. Architecture Search for Spectrogram"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "To better utilize the time-frequency two-dimensional\ninforma-"
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": ""
        },
        {
          "2State Key Laboratory of Multimodal Artificial Intelligence Systems": "tion in the spectrogram, our method automatically optimizes the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "At\nthe end of\nthe search, we feed wav2vec into the opti-"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "mized recurrent neural architecture to generate frame-level rep-"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "resentations. Since different frames play different parts in SER,"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "we further exploit an attention mechanism to prioritize impor-"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "tant frames and fuse them [27]."
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "2.3.\nInformation Supplement Module"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "After finding the optimal structure for each stream, we propose"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "an ISM to fuse complementary information from two streams."
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "To ensure that\nthe performance of the dominant feature is not"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "degraded after fusion, we abandon to compress this feature. In-"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "stead, we select complementary information from other inputs"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "and fuse them with the dominant feature."
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "Specifically,\nsuppose the outputs of\ntwo branches are de-"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "noted as X1 ‚àà Rd1 and X2 ‚àà Rd2 , respectively. Here, d1 and"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "d2 are the feature dimensions of X1 and X2. Suppose X2 is the"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "dominant feature. The calculation formula of our ISM can be"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": "summarized as follows:"
        },
        {
          "the next timestep, i.e., h1\n(see Section 4.6 for details).\nt and h2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Performance of different approaches. Bold font repre-": "sents the best performance.",
          "Table 3: Performance comparison between NAS-based method": ""
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "Method",
          "Table 3: Performance comparison between NAS-based method": ""
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "",
          "Table 3: Performance comparison between NAS-based method": "Params"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "CNN-GRU [6]",
          "Table 3: Performance comparison between NAS-based method": ""
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "CTC-RNN [7]",
          "Table 3: Performance comparison between NAS-based method": "1.05M"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "SeqCap [8]",
          "Table 3: Performance comparison between NAS-based method": "0.53M"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "PCNSE-SADRN-CTC [9]",
          "Table 3: Performance comparison between NAS-based method": "4.21M"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "UniformNAS [32]",
          "Table 3: Performance comparison between NAS-based method": "2.10M"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "EmotionNAS (Ours)",
          "Table 3: Performance comparison between NAS-based method": "2.37M"
        },
        {
          "Table 1: Performance of different approaches. Bold font repre-": "",
          "Table 3: Performance comparison between NAS-based method": "0.73M"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "and ResNet. ‚àó represents our fair comparison by adjusting the"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "model structure with the similar number of parameters."
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "Spectrogram"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "Branch"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "ResNet18"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "ResNet*"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "NAS-C4L3"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "NAS-C6L3"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": "NAS-C8L4"
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        },
        {
          "Table 2: Performance comparison between NAS-based methods": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Importance of ISM.": ""
        },
        {
          "Table 5: Importance of ISM.": ""
        },
        {
          "Table 5: Importance of ISM.": "UA(%)"
        },
        {
          "Table 5: Importance of ISM.": "66.2"
        },
        {
          "Table 5: Importance of ISM.": ""
        },
        {
          "Table 5: Importance of ISM.": "67.1"
        },
        {
          "Table 5: Importance of ISM.": ""
        },
        {
          "Table 5: Importance of ISM.": "63.8"
        },
        {
          "Table 5: Importance of ISM.": ""
        },
        {
          "Table 5: Importance of ISM.": "65.0"
        },
        {
          "Table 5: Importance of ISM.": "68.1"
        },
        {
          "Table 5: Importance of ISM.": "69.1"
        },
        {
          "Table 5: Importance of ISM.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b)": ""
        },
        {
          "(b)": "linear"
        },
        {
          "(b)": "$\nlinear"
        },
        {
          "(b)": "‚Ñé!\"#"
        },
        {
          "(b)": "tanh\nlinear\nsigmoid\n10"
        },
        {
          "(b)": "$\n1\nlinear"
        },
        {
          "(b)": "‚Ñé!\n2"
        },
        {
          "(b)": "linear\n#"
        },
        {
          "(b)": "‚Ñé!\"#"
        },
        {
          "(b)": "linear"
        },
        {
          "(b)": "5\nlinear"
        },
        {
          "(b)": "leaky relu\nlinear"
        },
        {
          "(b)": "#\nlinear"
        },
        {
          "(b)": "3\n‚Ñé!"
        },
        {
          "(b)": "0\nlinear"
        },
        {
          "(b)": "linear"
        },
        {
          "(b)": "ùë•!\nblending\nblending"
        },
        {
          "(b)": "4\nblending"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "(c)"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "Figure 4: Visualization of search results: (a) and (b) are search"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "results of the normal cell and the reduction cell for the spectro-"
        },
        {
          "(b)": "gram branch; (c) is the search result for the wav2vec branch."
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "states h1\nt‚àí1 and h2\nt‚àí1. We aim to generate new hidden states for"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "the next timestep, i.e., h1\nt and h2\nt ."
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "5. Conclusions"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "In this paper, we propose a novel\nframework for SER, called"
        },
        {
          "(b)": "‚ÄúEmotionNAS‚Äù.\nIt\ntakes spectrogram and wav2vec as the in-"
        },
        {
          "(b)": "puts, followed by NAS to optimize the network structure auto-"
        },
        {
          "(b)": "matically. We further design ISM to integrate complementary"
        },
        {
          "(b)": "information in different branches. Experimental results demon-"
        },
        {
          "(b)": "strate that our method outperforms existing manually-designed"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "and NAS-based methods. Meanwhile, we also systemically"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "prove the importance of each component\nin EmotionNAS,\nin-"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "cluding the NAS-based method, the two-stream framework and"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "the ISM fusion strategy."
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": "6. Acknowledgments"
        },
        {
          "(b)": ""
        },
        {
          "(b)": "This work\nis\nsupported\nby\nthe National Natural\nScience"
        },
        {
          "(b)": "Foundation\nof China\n(NSFC)\n(No.61831022, No.62276259,"
        },
        {
          "(b)": "No.62201572,\nNo.U21B2010),\nBeijing\nMunicipal\nSci-"
        },
        {
          "(b)": "ence&Technology Commission, Administrative Commission"
        },
        {
          "(b)": "of Zhongguancun Science Park No.Z211100004821013, Open"
        },
        {
          "(b)": "Research\nProjects\nof Zhejiang Lab\n(NO.\n2021KH0AB06),"
        },
        {
          "(b)": "CCF-Baidu Open Fund (No.OF2022025)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "tiable architecture search: Bridging the depth gap between search"
        },
        {
          "7. References": "[1] M. Neumann and N. T. Vu, ‚ÄúImproving speech emotion recog-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "and evaluation,‚Äù in 2019 IEEE/CVF International Conference on"
        },
        {
          "7. References": "nition with unsupervised representation learning on unlabeled",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Computer Vision, ICCV, 2019, pp. 1294‚Äì1303."
        },
        {
          "7. References": "speech,‚Äù in IEEE International Conference on Acoustics, Speech",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "and Signal Processing, ICASSP, 2019, pp. 7390‚Äì7394.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[19] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy,\nand"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "F. Hutter, ‚ÄúNas-bench-101: Towards reproducible neural architec-"
        },
        {
          "7. References": "[2] H. Jian, L. Ya, T. Jianhua, L. Zheng, N. Mingyue, and Y. Jiangyan,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "ture search,‚Äù in International Conference on Machine Learning."
        },
        {
          "7. References": "‚ÄúSpeech emotion recognition using semi-supervised learning with",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "PMLR, 2019, pp. 7105‚Äì7114."
        },
        {
          "7. References": "ladder networks,‚Äù in Proceedings of the First Asian Conference on",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "Affective Computing and Intelligent Interaction, 2018, pp. 1‚Äì5.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[20] N. Klyuchnikov,\nI. Trofimov, E. Artemova, M. Salnikov, M. Fe-"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "dorov,\nand E. Burnaev,\n‚ÄúNas-bench-nlp:\nNeural\narchitecture"
        },
        {
          "7. References": "[3]\nZ. Lian, Y. Li,\nJ. Tao,\nand J. Huang,\n‚ÄúSpeech emotion recog-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "search benchmark for natural\nlanguage processing,‚Äù CoRR, vol."
        },
        {
          "7. References": "nition via contrastive loss under siamese networks,‚Äù CoRR, vol.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "abs/2006.07116, 2020."
        },
        {
          "7. References": "abs/1910.11174, 2019.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[21] C. Vinola and K. Vimaladevi, ‚ÄúA survey on human emotion recog-"
        },
        {
          "7. References": "[4] M. M. H. E. Ayadi, M. S. Kamel,\nand F. Karray,\n‚ÄúSurvey on",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "nition approaches, databases and applications,‚Äù ELCVIA: elec-"
        },
        {
          "7. References": "speech emotion recognition: Features, classification schemes, and",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "tronic letters on computer vision and image analysis, pp. 24‚Äì44,"
        },
        {
          "7. References": "databases,‚Äù Pattern Recognit., vol. 44, no. 3, pp. 572‚Äì587, 2011.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "2015."
        },
        {
          "7. References": "[5] R. A. Khalil, E.\nJones, M.\nI. Babar, T.\nJan, M. H. Zafar,\nand",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "T. Alhussain, ‚ÄúSpeech emotion recognition using deep learning",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[22]\nS. Khorram, Z. Aldeneh, D. Dimitriadis, M. G. McInnis,\nand"
        },
        {
          "7. References": "techniques: A review,‚Äù IEEE Access, vol. 7, pp. 117 327‚Äì117 345,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "E. M. Provost, ‚ÄúCapturing long-term temporal dependencies with"
        },
        {
          "7. References": "2019.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "convolutional networks for continuous emotion recognition,‚Äù in"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Interspeech 2017, 18th Annual Conference of\nthe International"
        },
        {
          "7. References": "[6] X. Ma, Z. Wu,\nJ.\nJia, M. Xu, H. Meng,\nand L. Cai,\n‚ÄúEmo-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Speech Communication Association, 2017, pp. 1253‚Äì1257."
        },
        {
          "7. References": "tion recognition from variable-length speech segments using deep",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "learning on spectrograms,‚Äù in Interspeech 2018, 19th Annual Con-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[23]\nP. Li, Y. Song, I. McLoughlin, W. Guo, and L. Dai, ‚ÄúAn attention"
        },
        {
          "7. References": "ference of\nthe International Speech Communication Association,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "pooling based representation learning method for\nspeech emo-"
        },
        {
          "7. References": "2018, pp. 3683‚Äì3687.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "tion recognition,‚Äù in Interspeech 2018, 19th Annual Conference of"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "the International Speech Communication Association, 2018, pp."
        },
        {
          "7. References": "[7] W. Han, H. Ruan, X. Chen, Z. Wang, H. Li, and B. W. Schuller,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "3087‚Äì3091."
        },
        {
          "7. References": "‚ÄúTowards\ntemporal modelling\nof\ncategorical\nspeech\nemotion",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "recognition,‚Äù in Interspeech 2018, 19th Annual Conference of the",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[24] Y. Song, J. Liu, L. Wang, R. Yu, and J. Dang, ‚ÄúMulti-stage graph"
        },
        {
          "7. References": "International Speech Communication Association, 2018, pp. 932‚Äì",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "representation learning for dialogue-level speech emotion recog-"
        },
        {
          "7. References": "936.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "nition,‚Äù in IEEE International Conference on Acoustics, Speech"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "and Signal Processing (ICASSP).\nIEEE, 2022, pp. 6432‚Äì6436."
        },
        {
          "7. References": "[8] X. Wu, S. Liu, Y. Cao, X. Li,\nJ. Yu, D. Dai, X. Ma, S. Hu,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "Z. Wu, X. Liu et al., ‚ÄúSpeech emotion recognition using capsule",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[25]\nS. Schneider, A. Baevski, R. Collobert, and M. Auli, ‚Äúwav2vec:"
        },
        {
          "7. References": "networks,‚Äù in ICASSP 2019-2019 IEEE International Conference",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Unsupervised pre-training for speech recognition,‚Äù in Interspeech"
        },
        {
          "7. References": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "2019, 20th Annual Conference of\nthe International Speech Com-"
        },
        {
          "7. References": "2019, pp. 6695‚Äì6699.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "munication Association, 2019, pp. 3465‚Äì3469."
        },
        {
          "7. References": "[9]\nZ. Zhao, Q. Li, Z. Zhang, N. Cummins, H. Wang,\nJ. Tao, and",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[26]\nS.\nSiriwardhana,\nT.\nKaluarachchi,\nM.\nBillinghurst,\nand"
        },
        {
          "7. References": "B. W. Schuller, ‚ÄúCombining a parallel 2d cnn with a self-attention",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "S.\nNanayakkara,\n‚ÄúMultimodal\nemotion\nrecognition\nwith"
        },
        {
          "7. References": "dilated residual network for ctc-based discrete speech emotion",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "IEEE Ac-\ntransformer-based\nself\nsupervised\nfeature\nfusion,‚Äù"
        },
        {
          "7. References": "recognition,‚Äù Neural Networks, vol. 141, pp. 52‚Äì60, 2021.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "cess, vol. 8, pp. 176 274‚Äì176 285, 2020."
        },
        {
          "7. References": "[10] Y. Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[27]\nZ. Lian, B. Liu, and J. Tao, ‚ÄúCtnet: Conversational\ntransformer"
        },
        {
          "7. References": "‚ÄúPc-darts: Partial channel connections for memory-efficient archi-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "IEEE/ACM Transactions on\nnetwork for emotion recognition,‚Äù"
        },
        {
          "7. References": "tecture search,‚Äù in International Conference on Learning Repre-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Audio, Speech, and Language Processing, vol. 29, pp. 985‚Äì1000,"
        },
        {
          "7. References": "sentations (ICLR), 2019, pp. 1‚Äì13.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "2021."
        },
        {
          "7. References": "[11] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, ‚ÄúEfficient neu-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[28] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "7. References": "ral architecture search via parameters sharing,‚Äù in International",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, ‚ÄúIemocap:"
        },
        {
          "7. References": "conference on machine learning.\nPMLR, 2018, pp. 4095‚Äì4104.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Interactive emotional dyadic motion capture database,‚Äù Language"
        },
        {
          "7. References": "[12]\nT. Elsken,\nJ. H. Metzen,\nand F. Hutter,\n‚ÄúNeural\narchitecture",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Resources and Evaluation, vol. 42, pp. 335‚Äì359, 2008."
        },
        {
          "7. References": "search: A survey,‚Äù The Journal of Machine Learning Research,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[29] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Batten-"
        },
        {
          "7. References": "vol. 20, no. 1, pp. 1997‚Äì2017, 2019.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "berg, and O. Nieto, ‚Äúlibrosa: Audio and music signal analysis in"
        },
        {
          "7. References": "[13] X. Chen, L. Xie, J. Wu, and Q. Tian, ‚ÄúProgressive differentiable",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "python,‚Äù in Proceedings of the 14th python in science conference,"
        },
        {
          "7. References": "architecture search: Bridging the depth gap between search and",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "vol. 8.\nCiteseer, 2015, pp. 18‚Äì25."
        },
        {
          "7. References": "the IEEE/CVF International Con-\nevaluation,‚Äù in Proceedings of",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[30] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, ‚ÄúBuilding a"
        },
        {
          "7. References": "ference on Computer Vision, 2019, pp. 1294‚Äì1303.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "large annotated corpus of english: The penn treebank,‚Äù Computa-"
        },
        {
          "7. References": "[14] B. Zoph and Q. Le, ‚ÄúNeural architecture search with reinforce-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "tional Linguistics, vol. 19, no. 2, pp. 313‚Äì330, 1993."
        },
        {
          "7. References": "ment\nlearning,‚Äù in International Conference on Learning Repre-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "sentations (ICLR), 2017, pp. 1‚Äì16.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[31] A. Graves, S. Fern¬¥andez, F. Gomez, and J. Schmidhuber, ‚ÄúCon-"
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "nectionist\ntemporal\nclassification:\nlabelling\nunsegmented\nse-"
        },
        {
          "7. References": "[15]\nE. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "quence data with recurrent neural networks,‚Äù in Proceedings of"
        },
        {
          "7. References": "Q. V. Le, and A. Kurakin, ‚ÄúLarge-scale evolution of image classi-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "the 23rd international conference on Machine learning, 2006, pp."
        },
        {
          "7. References": "fiers,‚Äù in International Conference on Machine Learning.\nPMLR,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "369‚Äì376."
        },
        {
          "7. References": "2017, pp. 2902‚Äì2911.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "[32] X. Wu, S. Hu, Z. Wu, X. Liu, and H. Meng, ‚ÄúNeural architecture"
        },
        {
          "7. References": "[16] H. Liu, K. Simonyan, and Y. Yang, ‚ÄúDarts: Differentiable archi-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "search for\nspeech emotion recognition,‚Äù\nin ICASSP 2022-2022"
        },
        {
          "7. References": "tecture search,‚Äù in International Conference on Learning Repre-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "7. References": "sentations (ICLR), 2019, pp. 1‚Äì13.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": "Processing (ICASSP).\nIEEE, 2022, pp. 6902‚Äì6906."
        },
        {
          "7. References": "[17] M. Li, Q. Jiang, H. Lin, and H. An, ‚ÄúGdarts: A gpu-based run-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "time system for dataflow task programming on dependency ap-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "plications,‚Äù in IEEE Intl Conf on Parallel & Distributed Process-",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "ing with Applications, Big Data & Cloud Computing, Sustainable",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "Computing & Communications, Social Computing & Networking,",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        },
        {
          "7. References": "2019, pp. 547‚Äì552.",
          "[18] X. Chen, L. Xie,\nJ. Wu,\nand Q. Tian,\n‚ÄúProgressive differen-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using semi-supervised learning with ladder networks",
      "authors": [
        "H Jian",
        "L Ya",
        "T Jianhua",
        "L Zheng",
        "N Mingyue",
        "Y Jiangyan"
      ],
      "year": "2018",
      "venue": "Proceedings of the First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition via contrastive loss under siamese networks",
      "authors": [
        "Z Lian",
        "Y Li",
        "J Tao",
        "J Huang"
      ],
      "year": "1910",
      "venue": "CoRR"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, 19th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "Towards temporal modelling of categorical speech emotion recognition",
      "authors": [
        "W Han",
        "H Ruan",
        "X Chen",
        "Z Wang",
        "H Li",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, 19th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "11",
      "title": "Pc-darts: Partial channel connections for memory-efficient architecture search",
      "authors": [
        "Y Xu",
        "L Xie",
        "X Zhang",
        "X Chen",
        "G.-J Qi",
        "Q Tian",
        "H Xiong"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "12",
      "title": "Efficient neural architecture search via parameters sharing",
      "authors": [
        "H Pham",
        "M Guan",
        "B Zoph",
        "Q Le",
        "J Dean"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "13",
      "title": "Neural architecture search: A survey",
      "authors": [
        "T Elsken",
        "J Metzen",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "14",
      "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "authors": [
        "X Chen",
        "L Xie",
        "J Wu",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Neural architecture search with reinforcement learning",
      "authors": [
        "B Zoph",
        "Q Le"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Large-scale evolution of image classifiers",
      "authors": [
        "E Real",
        "S Moore",
        "A Selle",
        "S Saxena",
        "Y Suematsu",
        "J Tan",
        "Q Le",
        "A Kurakin"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Darts: Differentiable architecture search",
      "authors": [
        "H Liu",
        "K Simonyan",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "18",
      "title": "Gdarts: A gpu-based runtime system for dataflow task programming on dependency applications",
      "authors": [
        "M Li",
        "Q Jiang",
        "H Lin",
        "H An"
      ],
      "year": "2019",
      "venue": "IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing"
    },
    {
      "citation_id": "19",
      "title": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation",
      "authors": [
        "X Chen",
        "L Xie",
        "J Wu",
        "Q Tian"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV"
    },
    {
      "citation_id": "20",
      "title": "Nas-bench-101: Towards reproducible neural architecture search",
      "authors": [
        "C Ying",
        "A Klein",
        "E Christiansen",
        "E Real",
        "K Murphy",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Nas-bench-nlp: Neural architecture search benchmark for natural language processing",
      "authors": [
        "N Klyuchnikov",
        "I Trofimov",
        "E Artemova",
        "M Salnikov",
        "M Fedorov",
        "E Burnaev"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "A survey on human emotion recognition approaches, databases and applications",
      "authors": [
        "C Vinola",
        "K Vimaladevi"
      ],
      "year": "2015",
      "venue": "A survey on human emotion recognition approaches, databases and applications"
    },
    {
      "citation_id": "23",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Interspeech 2017, 18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "24",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, 19th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "25",
      "title": "Multi-stage graph representation learning for dialogue-level speech emotion recognition",
      "authors": [
        "Y Song",
        "J Liu",
        "L Wang",
        "R Yu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "27",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "30",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "31",
      "title": "Building a large annotated corpus of english: The penn treebank",
      "authors": [
        "M Marcus",
        "B Santorini",
        "M Marcinkiewicz"
      ],
      "year": "1993",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fern√°ndez",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "33",
      "title": "Neural architecture search for speech emotion recognition",
      "authors": [
        "X Wu",
        "S Hu",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}