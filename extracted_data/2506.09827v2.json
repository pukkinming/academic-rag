{
  "paper_id": "2506.09827v2",
  "title": "Emonet-Voice: A Fine-Grained, Expert-Verified Benchmark For Speech Emotion Detection",
  "published": "2025-06-11T15:06:59Z",
  "authors": [
    "Christoph Schuhmann",
    "Robert Kaczmarczyk",
    "Gollam Rabby",
    "Felix Friedrich",
    "Maurice Kraus",
    "Kourosh Nadi",
    "Huu Nguyen",
    "Kristian Kersting",
    "Sören Auer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EMONET-VOICE, a new resource for speech emotion detection, which includes EMONET-VOICE BIG, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EMONET-VOICE BENCH, a novel benchmark dataset with human expert annotations. EMONET-VOICE is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce EMPATHICINSIGHT-VOICE models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. 2 * Contributed equally and jointly supervised this project. 2 links to our data, models and code Preprint. Under review.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Synthetic speech technology has reached unprecedented fidelity, with state-of-the-art text-to-speech (TTS) and audio generation models, e.g., GPT-4 OmniAudio  [25] , achieving prosody, timbre, and expressiveness comparable to humans. These advancements significantly enhance human-computer interaction (HCI), enabling virtual assistants to convey appropriate emotional qualities across diverse contexts  [14] . However, this advancement remains asymmetric: while machines can to some extent effectively synthesize convincing affective speech, they still struggle to recognize the nuanced, contextdependent emotional information humans naturally convey  [15, 34] , a critical capability for truly conversational AI.\n\nDespite steady progress in speech emotion recognition (SER) through deep architectures and selfsupervised representations, evaluation remains constrained by datasets predominantly built around a limited set of \"basic\" emotions  [8, 38] . Established benchmarks such as IEMOCAP  [3] , RAVDESS  [19] , and CREMA-D  [4]  have been invaluable for the field but exhibit three fundamental limitations:\n\n(i) Insufficient Granularity. Coarse taxonomies fail to capture subtle or compound emotional states (e.g., bittersweet, embarrassment, envy) that are essential for naturalistic interaction  [5] . (ii) Limited Representativeness. Current datasets predominantly consist of studio-quality acted speech, lacking linguistic diversity and omitting sensitive emotional states due to privacy constraints  [20, 33] . (iii) Restricted Scalability. Licensing restrictions, privacy concerns, and annotation costs severely limit dataset size, impeding the data-intensive training regimes required by modern deep learning approaches  [37, 28] , specifically for open-source and -science.\n\nThese limitations are further compounded by evolving perspectives in affective science. Constructionist theories, particularly Barrett's Theory of Constructed Emotion  [1] , conceptualize emotions as context-dependent constructions rather than universal biological packages. This perspective aligns with dimensional frameworks such as Russell's valence-arousal circumplex  [30]  and supports multilabel approaches that treat affect as overlapping estimates rather than discrete categories  [22, 23] .\n\nConsequently, SER research must evolve along two parallel trajectories: developing richer datasets with evaluation protocols that respect emotional complexity, and creating modeling strategies that go beyond simplistic classification paradigms.\n\nTo address these challenges, we introduce two complementary datasets. First, EMONET-VOICE BIG, a foundational dataset for pretraining models on SER. It is a comprehensive synthetic voice corpus exceeding 4,500 hours in four languages (English, German, Spanish, French), featuring 11 distinct voices with different gender identities and a fine-grained taxonomy of 40 emotion categories. As such, it provides an open, privacy-compliant foundation for emotional TTS research and multilingual speech analysis at scale. Second, from this corpus we curate EMONET-VOICE BENCH, comprising 12,600 audio clips annotated by psychology experts using a strict consensus protocol that evaluates both the presence and intensity of each target emotion across our 40-category emotion taxonomy. This approach yields a high-quality, multilingual benchmark for fine-grained SER while circumventing the privacy barriers that inhibit the collection of authentic sensitive vocal expressions.\n\nBuilding on these datasets, we develop EMPATHICINSIGHT-VOICE (Small and Large), novel SER models that achieve state-of-the-art performance in fine-grained emotion recognition while demonstrating strong alignment with human expert judgments. Through comprehensive evaluation across the concurrent SER model landscape, we reveal critical insights into current SER capabilities, including systematic patterns in which emotions prove more challenging to recognize (e.g., lowarousal states like concentration versus high-arousal emotions like anger).\n\nIn summary, our contributions are four-fold: (1) We build EMONET-VOICE BIG, a pretraining, openaccess, 4,500-hour multilingual synthetic speech corpus featuring 11 distinct synthetic voices across 4 languages and 40 emotion categories.  (2)  We introduce EMONET-VOICE BENCH, a meticulously curated and expert-verified benchmark dataset of 12,600 high-quality audio samples for fine-grained SER, featuring 40 emotion categories with 3 intensity levels.\n\n(3) We build EMPATHICINSIGHT-VOICE (Small and Large), novel SER models designed for nuanced emotion estimation. (4) We conduct comprehensively evaluations on our novel benchmark, providing critical insights into current SER capabilities and limitations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Current SER research operates on a constrained empirical foundation. The field still relies on a small set of acted corpora recorded in controlled studio conditions-IEMOCAP (12 h, 9 emotions)  [3] , RAVDESS (1 h, 8 emotions, speech & song)  [19] , SAVEE (0.8 h, 7 emotions, four male speakers)  [12] , the German EMODB  [2] , and the multi-ethnic CREMA-D  [4] . While these corpora provide clean labels and high acoustic quality, they share four persistent weaknesses. First, they use restrictive taxonomies-typically Ekman's six basic emotions  [8] -omitting compound or socially nuanced states such as embarrassment, envy, or contemplation  [27, 5] . Second, their acted prosody exaggerates emotional cues and reduces generalization to spontaneous speech  [20] . Third, privacy and ethics hinder collection of intimate or stigmatizing emotions (e.g. shame, desire, grief)  [33] .\n\nFourth, scale and linguistic diversity remain limited: most corpora contain < 100 speakers, just a few hours of audio, and are largely English-centric. Recent efforts to expand this foundation include early multilingual sets such as EMOREACT and the parallel English-Mandarin ESD, which broaden language coverage but still cap labels at six basic categories  [24, 39] . Aggregation benchmarks go further-SERAB pools nine legacy corpora in six languages  [31] ; EMOBOX widens the scope to 32 datasets in 14 languages with turnkey evaluation splits  [21] ; SER EVALS organises 18 minoritylanguage corpora into in-and out-of-domain test beds for robustness analysis  [26] ; and BERST collects ≈ 4 h of shouted and distanced English speech from 98 actors at 19 smartphone positions  [36] . Yet these resources still inherit the core constraints of their sources: acted or scripted speech, narrow taxonomies (≤ 8 emotions), modest duration per language, and a lack of expert-validated intensity labels or sensitive affective states.\n\nThese existing datasets, summarized and contrasted with our contributions in Table  1 , highlight a clear gap. While valuable, they are often restricted by licensing, limited in scale (both in total hours and number of utterances), offer a narrow range of emotion categories (typically 9 or fewer), rely on human actors which limits the privacy-preserving access to sensitive emotions, and many lack multilingual support. EMONET-VOICE BIG and EMONET-VOICE BENCH directly address these shortcomings by providing a large-scale, openly licensed, synthetic, multilingual corpus with a significantly expanded emotion taxonomy.\n\nTaxonomic limitations exacerbate data-scarcity and theoretical gaps. Modern affective science models emotions as context-dependent and graded rather than discrete  [1, 18] . Dimensional (valence-arousal-dominance) and multi-label schemes  [30, 37]  better capture blended affect, yet almost all benchmarks still assign a single discrete label per clip. When intensity annotations exist, they typically rely on crowdsourcing and show low agreement  [13, 35] . Consequently, the community lacks benchmarks that reflect contemporary understanding of emotion as multidimensional and graded, particularly for sensitive affective states that cannot be ethically collected from human participants.\n\nExpert-validated intensity annotations across multidimensional affective spaces are missing from existing benchmarks, and we fill this critical gap by contributing EMONET-VOICE BENCH with 12,600 carefully chosen clips whose emotional presence and intensity we had annotated by psychology experts, yielding a high-agreement subset. We overcome the taxonomic, scale, and ethical limitations of existing corpora by combining broad multilingual coverage, a 40-category taxonomy grounded in contemporary affective science  [6, 1] , and privacy-preserving synthetic speech generation, offering the first benchmark that provides expert-validated ratings across a multidimensional affective space. This section describes how we built the EMONET-VOICE resources, beginning with our emotion taxonomy, followed by the creation of the large-scale dataset EMONET-VOICE BIG, and concluding with the expert-validated EMONET-VOICE BENCH subset used for final evaluation. Lastly, we introduce EMPATHICINSIGHT-VOICE models setting a new standard in SER.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Emotion Taxonomy",
      "text": "For EMONET-VOICE, we adopt the comprehensive 40-category emotion taxonomy originally developed for EMONET-FACE  [32] . The taxonomy includes a diverse set of categories spanning positive emotions (e.g., Elation, Contentment, Affection, Awe), negative emotions (e.g., Distress, Sadness, Bitterness, Contempt), cognitive states (e.g., Concentration, Confusion, Doubt), physical states (e.g., Pain, Fatigue), and socially mediated emotions (e.g., Embarrassment, Shame, Pride, Teasing). This fine-grained structure enables the evaluation of models beyond binary or basic categorical classification. The full set of 40 emotion categories and their descriptive terms can be found in App.A.1. A comprehensive description of the methodology used to construct the taxonomy, including literature-based extraction and expert-guided refinement, is provided in App.A.4.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emonet-Voice Big: Building A Large-Scale Synthetic Ser Dataset",
      "text": "The foundational dataset, EMONET-VOICE BIG, consists of emotionally expressive speech samples synthesized using the GPT-4 OmniAudio model  3  . An overview of EMONET-VOICE BIG's scale and language distribution is provided in Table  2 . Our prompting strategy cast the model as an actor auditioning for a film, tasked with performing texts designed to evoke one of 40 emotion categories (from the taxonomy in Section 3.1). Key prompt elements included directives for strong emotional expression from the outset and naturalistic human speech patterns (e.g., varied rhythm, volume, tone, and appropriate vocal bursts). This aimed to ensure perceptible emotional content and avoid monotonous delivery. Audio was generated as 3-to 30-second, 24kHz WAV files, utilizing 11 synthetic voices (6 female/5 male) across English, German, French, and Spanish to build a diverse Figure  1 : Annotator agreement for human ratings on perceived emotions in audio samples. Stacked horizontal bars display the proportion of audio-emotion instances for each emotion, categorized by agreement type. These categories include full agreement on emotion presence (e.g., '3:0 (+)', '2:0 (+)'), partial agreement where presence is favored (e.g., '2:1 (+ favored)'), disagreement (e.g., '1:1'), partial agreement where absence is favored (e.g., '1:2 (-favored)'), and full agreement on emotion absence (e.g., '2:0 (-)', '3:0 (-)'). Instances with other rating configurations are grouped under 'Other'. The numbers to the right of each bar indicate the total number of instances (n) for that emotion, along with the percentage of these instances rated by two (%2r) or three (%3r) annotators; 'Other' denotes instances with four annotators. The annotation process ensured all audio-emotion pairs were initially rated by two annotators. If both these annotators marked an emotion as present (rating > 0), the instance was subsequently rated by a third annotator. Additionally, a random subset of instances received a fourth annotation.\n\nmultilingual corpus. The full prompting template and detailed methodology, including the importance of specific instructions and language-specific adaptations for vocal burst generation, are presented in the Supplement.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emonet-Voice Bench: A Human Expert Benchmark For Ser",
      "text": "From EMONET-VOICE BENCH, we created a subset of 12,600 unique audio files annotated for emotion by human experts on a three-point annotation scale, summarized in Table  3 . We depict the annotation platform for our human experts in Appendix Figures  2  and 3 . The dataset features 11 distinct synthetic voices (6 female and 5 male) across four languages: English (48.9%), German (15.0%), Spanish (17.4%), and French (18.8%). The average clip duration is 10.36 seconds, resulting in a total playtime of 36.26 hours.\n\nTable  4  summarizes our annotation procedure. Ensuring the quality and reliability of the emotion annotations was a central priority in constructing the EMONET-VOICE BENCH. We recruited a team of six human experts with at least a Bachelor's degree in Psychology to serve as benchmark annotators, thereby guaranteeing familiarity with emotional theory and terminology. In total, 33,605 singleemotion labels across 12,600 unique audio samples were contributed -some samples ultimately received more than three annotations. Each audio clip was first labeled independently by two experts who were presented with the audio alongside one specific target emotion category from our taxonomy in addition to a three-point scale: 0 indicating the emotion was not perceived, 1 indicating it was mildly present at low intensity, and 2 indicating it was intensely present and clearly perceptible.\n\nIf both human experts agreed that the emotion was present (either \"weakly present\" or \"strongly present\"), the clip was sent to a third expert for confirmation. Additionally, we randomly selected a subset of clips to receive a third or even a fourth annotation regardless of whether the first two annotators agreed. To reduce potential gender biases in emotional perception, each group assigned per snippet was balanced in gender composition. Importantly, annotators performed their assessments independently and were blinded to the ratings of others.\n\nFigure  1  illustrates inter-annotator agreement patterns across emotion categories, showing the distribution of full agreement, partial agreement, and disagreement for each emotion-audio pair. The numbers alongside each bar indicate total instances and rating distributions across multiple annotators.\n\nThe analysis reveals clear consensus patterns: emotions like concentration and bitterness achieve strong expert agreement, while others such as numbness and awe show notable disagreement even among psychology professionals. The overall inter-rater reliability measured by Cronbach's α is 0.14 (95% CI [0.12, 0.15]), with per-emotion values detailed in Appendix Table  9 . While this low α might initially suggest poor reliability, it actually reflects the inherent complexity of fine-grained emotion perception rather than annotation deficiencies. Unlike simpler emotion taxonomies, our 40-category framework captures subtle distinctions that legitimately evoke different interpretations among experts. These patterns demonstrate that while human agreement is robust for many emotions, certain categories naturally elicit diverse interpretations-underscoring the nuanced nature of affective expression in speech. Rather than indicating weak annotation quality, this variability highlights EMONET-VOICE's sensitivity to the inherent complexity of emotional perception. Our annotations thus capture both the challenges and opportunities in modeling authentic emotional diversity at scale.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Empathicinsight-Voice: Training State-Of-The-Art Ser Models",
      "text": "Another contribution of this work, based on the datasets we built, is to establish a novel state-of-the-art speech emotion recognition model.\n\nFirst linear probing experiments as well as previous works  [17, 7]  show that the off-the-shelf Whisper encoders  [29]  are not capable of reflecting on emotions-an essential capability for emotion-aware audio generation and captioning. Specifically, at a fine-grained level, existing TTS models fail to recognize emotions effectively, as we will discuss later. To address this limitation, we continually pre-trained Whisper encoders as the backbone of our EMPATHICINSIGHT-VOICE. Specifically, we leverage EMONET-VOICE BIG as a pretraining dataset and train emotion-experts in two stages. We base our experiments on Whisper-Small to optimize for the performance-efficiency tradeoff.\n\nIn the first stage, the Whisper encoder is trained on a combination of EMONET-VOICE BIG and another 4,500 hours of public emotion-related content  4  to develop general emotional acoustic representations. This data was annotated using an iterative process with Gemini Flash 2.0 to obtain emotion scores (0-4 scale) for all audio snippets. In the second stage, we freeze the Whisper encoder and train MLP expert heads-one per emotion dimension-on top of the fixed encoder embeddings. This way, each MLP receives the full voice audio sequence from the Whisper encoder as sequence flattened token embeddings and then regresses a single emotion intensity score. We propose two model sizes to accommodate different performance requirements, namenly EMPATHICINSIGHT-VOICE SMALL with 74M paremeter MLP heads and EMPATHICINSIGHT-VOICE LARGE with 148M paremeter MLP heads. We optimize them using mean absolute error (MAE) on the Gemini Flash 2.0-generated emotion scores.\n\nThrough this two-stage fine-tuning and dedicated MLP ensemble, EMPATHICINSIGHT-VOICE effectively captures and predicts fine-grained emotional content from speech with high human alignment, as we demonstrate in the following. Further details are outlined in Appendix A.2.\n\n4 Experiments: Do they hear what we hear?\n\nIn this section, we evaluate current SER models on our novel benchmark. Before that, we start by introducing our experimental setup. We benchmarked general-purpose multimodal models (e.g., Gemini, GPT-4o) via zero-shot prompting, as well as specialized speech models (e.g., Hume Voice). Hume Voice was subject to constraints on input length (≤5s) and taxonomy coverage. Initial experiments with Whisper failed, due to a general lack of emotion understanding, which led to our development of EMPATHICINSIGHT-VOICE, which pair continually pre-trained Whisper encoders with MLP regressors on our EMONET-VOICE dataset.\n\nWe report four key metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to quantify the average magnitude and larger deviations of prediction error on this 0-10 scale. Additionally, Pearson Correlation (Pearson r) and Spearman Rank Correlation (Spearman r) are used to assess the linear and monotonic agreement, respectively, between model-predicted intensities and human judgments. These metrics collectively provide a comprehensive view of how well models capture both the absolute values and the relative ordering of perceived emotional intensities.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluating Speech Emotion Recognition Models",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "Our analysis reveals a fundamental relationship between human annotation consensus and model performance in audio-based emotion recognition, with implications that extend beyond the specific task to the broader understanding of machine learning on subjective human judgments.\n\nWe demonstrated that EMPATHICINSIGHT-VOICE models advance the state-of-the-art significantly, with best error and ordering values. Yet, MAE values around 3.0 on a 0-10 scale indicate substantial room for improvement even in our best model. The consistent pattern of high-arousal emotions being more detectable than low-arousal states across all architectures suggests this represents a fundamental challenge in audio-based emotion recognition rather than a limitation of specific models.\n\nASR models don't (yet) understand emotions. ASR models like Whisper currently lack the ability to accurately understand and represent nuanced emotions  [17, 7] . However, by continually pretraining these models, we can enable them to perceive and interpret emotions in a way that supports more human-like predictions, as we demonstrated. Our EMONET-VOICE BIG dataset represents a crucial first step toward equipping foundation models with this emotional understanding.\n\nAnnotation Ambiguity Predicts Model Performance. The most striking finding from our comparative analysis is the systematic correlation between inter-annotator agreement and model performance across the emotional spectrum. Emotions exhibiting strong human consensus, such as Teasing (Spearman's ρ = 0.617), Embarrassment (ρ = 0.583), and Anger (ρ = 0.536), demonstrate both high agreement rates (predominantly green regions in Figure  1 ) and superior model alignment (top, dark blue in Table  6 ). Conversely, cognitively complex emotions like Concentration (ρ = 0.118), Contemplation (ρ = 0.151), and Contentment (ρ = 0.123) exhibit substantial human disagreement and correspondingly poor model performance.\n\nThis pattern suggests that model failures may not represent algorithmic inadequacies but rather reflect genuine perceptual ambiguities inherent in the emotional recognition task itself. We propose that inter-annotator agreement might establish a practical upper bound for model performance, as it is not be expected from computational systems to exceed human consensus on subjective human judgments.\n\nArousal-Dependent Recognition Bias. Our results demonstrate a clear arousal-based performance hierarchy, with high-energy emotions consistently outperforming their low-arousal counterparts. This bias appears across all model architectures, from transformer-based systems (GPT-4o variants) to specialized audio models (Hume Voice), suggesting a fundamental limitation in current acoustic feature extraction paradigms.\n\nHigh-arousal emotions like Anger, Embarrassment, and Impatience and Irritability likely produce more distinctive acoustic signatures-increased pitch variance, amplitude fluctuations, and prosodic changes-that are readily captured by existing audio processing pipelines. In contrast, low-arousal states such as Contemplation and Concentration may manifest through subtle changes in speech patterns that fall below current model sensitivity thresholds.\n\nThis finding has significant implications for real-world applications: current audio emotion recognition systems may be inherently biased toward detecting emotional extremes while systematically underperforming on the nuanced, everyday emotional states that characterize much of human interaction. Furthermore, the arousal-dependent performance bias indicates that current audio processing architectures may be learning acoustic stereotypes of emotions rather than developing genuine emotional understanding. Models excel at detecting prototypical emotional expressions while failing on subtle variations, suggesting they may be capturing surface-level patterns rather than underlying emotional concepts.\n\nThe Cognitive Emotion Recognition Gap. A particularly noteworthy pattern emerges for cognitively-oriented emotions-states that require contextual understanding beyond immediate acoustic features. Emotions such as Contemplation, Interest, and Concentration represent mental pro-cesses rather than affective responses, and their recognition may fundamentally require understanding why someone is in a particular state, not merely how they sound while experiencing it.\n\nThis limitation points to a broader challenge in current emotion recognition paradigms: the reliance on acoustic features alone may be insufficient for detecting emotions that are primarily cognitive rather than affective. Future architectures might need to incorporate contextual information, dialogue history, or multimodal inputs to bridge this gap, going toward multimodal AI assistants.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Limitations And Future Directions",
      "text": "While our analysis provides valuable insights, several limitations should be acknowledged and addressed in future work.\n\nThe fidelity of synthetic data from GPT-4o Audio generations underlying our datasets, while stateof-the-art, may still exhibit subtle differences from genuine human vocalizations, meaning model performance on this benchmark might not directly generalize to spontaneous real-world speech.\n\nThe benchmark primarily evaluates the recognition of emotional portrayals in synthetic speech driven by acting scenarios, which differs from the often more nuanced or mixed cues in authentic, spontaneous expressions. Furthermore, the dataset inherently reflects the capabilities and potential biases of the specific audio generation model used. While EMONET-VOICE incorporates 11 voices and 4 languages, this speaker and linguistic diversity does not yet encompass the full spectrum of human identities, accents, dialects, or age ranges. Finally, emotion perception is inherently subjective; while expert consensus minimizes variability, the labels represent a reliable approximation of perceived emotion in synthetic stimuli rather than an objective internal state.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ethical Considerations",
      "text": "This work responds to growing concerns about the unintended effects of emotionally uncalibrated AI systems. As AI models become more capable of producing emotionally charged content, it is essential to understand how people interpret and respond to these synthetic expressions. Our datasets offer a basis for exploring potential risks, including miscommunication and emotional manipulation. We recognize the ethical challenges, especially regarding misuse for manipulative ends-concerns that underscore our commitment to transparency and safety. In response, we advocate for the development of safeguards to mitigate such misuse  [10] .\n\nThe development of EMONET-VOICE was guided by a strong ethical commitment, primarily addressed through the exclusive use of synthetic voice generation. This approach deliberately avoids the privacy risks associated with collecting real human emotional expressions, particularly those tied to sensitive or deeply personal experiences-such as pain, shame, or sexual desire-that would be difficult, if not impossible, to collect ethically and at scale from human participants. All voice samples in EMONET-VOICE are artificially generated using TTS models, with manual filtering and prompt diversification to reflect a broad range of gender, demographic, and accent representations while minimizing problematic content, motivated by Friedrich et al.  [9] . Although the likelihood is extremely low, we acknowledge the remote possibility that some samples may resemble real individuals  [11] ; however, no personally identifiable data was used at any stage.\n\nWe release EMONET-VOICE as a research artifact with the recommendation to use it for academic purposes and encourage thorough examination of potential downstream biases and ethical implications.\n\nWe invite users to engage with our tools, transparently report any unexpected behaviors, and contribute feedback to help advance responsible data curation and safer AI development.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We introduced EMONET-VOICE, novel datasets for fine-grained speech emotion estimation, designed to address critical limitations in existing SER resources. We create a large-scale, pretraining datasets EMONET-VOICE BIG, which is a synthetic multilingual voice dataset. Derived from it, we establish EMONET-VOICE BENCH which has psychology expert annotation, utilizing a 40-category emotion taxonomy with 3-level ratings. Their synthetic nature, combined with diverse voice and language coverage (11 voices, 4 languages, with balanced representations), prevents privacy concerns inherent in collecting authentic sensitive emotional data and broadens diversity. Furthermore, we create EMPATHICINSIGHT-VOICE models (Small and Large), which establish state-of-the-art in speech emotion recognition. Existing foundation models like Gemini, GPT4o and Hume Voice perform significantly worse.\n\nOur results indicate gaps in current emotion recognition and hint to several future research paths. Future research should investigate whether the agreement-performance relationship holds across different modalities (text, video, physiological signals) and develop targeted architectures to handle low-agreement emotional categories more effectively. The development of context-aware models that can leverage situational information may be particularly promising for addressing the cognitive emotion recognition gap.\n\nExpanding EMONET-VOICE with more samples, languages, and speaker profiles using nextgeneration voice synthesis represents a key priority, along with exploring multiple generative models to mitigate single-model bias. Investigating cross-modal consistency by generating corresponding facial expressions or scenarios for the same emotional prompts offers a path toward richer multimodal benchmarks. Further analysis could also explore model performance variations across different languages or speaker voices within the current dataset to better understand the scope and limitations of current approaches.\n\n• Pleasure/Ecstasy: 'ecstasy', 'pleasure', 'bliss', 'rapture', 'Beatitude' • Contentment: 'contentment', 'relaxation', 'peacefulness', 'calmness', 'satisfaction', 'Ease', 'Serenity', 'fulfillment', 'gladness', 'lightness', 'serenity', 'tranquility' • Thankfulness/Gratitude: 'thankfulness', 'gratitude', 'appreciation', 'gratefulness' • Affection: 'sympathy', 'compassion', 'warmth', 'trust', 'caring', 'Clemency', 'forgiveness', 'Devotion', 'Tenderness', 'Reverence' • Infatuation: 'infatuation', 'having a crush', 'romantic desire', 'fondness', 'butterflies in the stomach', 'adoration' • Hope/Enthusiasm/Optimism: 'hope', 'enthusiasm', 'optimism', 'Anticipation', 'Courage', 'Encouragement', 'Zeal', 'fervor', 'inspiration', 'Determination' • Triumph: 'triumph', 'superiority' • Pride: 'pride', 'dignity', 'self-confidently', 'honor', 'self-consciousness' • Interest: 'interest', 'fascination', 'curiosity', 'intrigue' • Awe: 'awe', 'awestruck', 'wonder' • Astonishment/Surprise: 'astonishment', 'surprise', 'amazement', 'shock', 'startlement' • Concentration: 'concentration', 'deep focus', 'engrossment', 'absorption', 'attention' • Contemplation: 'contemplation', 'thoughtfulness', 'pondering', 'reflection', 'meditation', 'Brooding', 'Pensiveness' • Relief: 'relief', 'respite', 'alleviation', 'solace', 'comfort', 'liberation' • Longing: 'yearning', 'longing', 'pining', 'wistfulness', 'nostalgia', 'Craving', 'desire', 'Envy', 'homesickness', 'saudade' • Teasing: 'teasing', 'bantering', 'mocking playfully', 'ribbing', 'provoking lightly' • Impatience and Irritability: 'impatience', 'irritability', 'irritation', 'restlessness', 'shorttemperedness', 'exasperation' • Sexual Lust: 'sexual lust', 'carnal desire', 'lust', 'feeling horny', 'feeling turned on' • Doubt: 'doubt', 'distrust', 'suspicion', 'skepticism', 'uncertainty', 'Pessimism' • Fear: 'fear', 'terror', 'dread', 'apprehension', 'alarm', 'horror', 'panic', 'nervousness' • Distress: 'worry', 'anxiety', 'unease', 'anguish', 'trepidation', 'Concern', 'Upset', 'pessimism', 'foreboding' • Confusion: 'confusion', 'bewilderment', 'flabbergasted', 'disorientation', 'Perplexity' • Embarrassment: 'embarrassment', 'shyness', 'mortification', 'discomfiture', 'awkwardness', 'Self-Consciousness' • Shame: 'shame', 'guilt', 'remorse', 'humiliation', 'contrition' • Disappointment: 'disappointment', 'regret', 'dismay', 'letdown', 'chagrin' • Sadness: 'sadness', 'sorrow', 'grief', 'melancholy', 'Dejection', 'Despair', 'Self-Pity', 'Sullenness', 'heartache', 'mournfulness', 'misery' • Bitterness: 'resentment', 'acrimony', 'bitterness', 'cynicism', 'rancor' • Contempt: 'contempt', 'disapproval', 'scorn', 'disdain', 'loathing', 'Detestation' • Disgust: 'disgust', 'revulsion', 'repulsion', 'abhorrence', 'loathing' • Anger: 'anger', 'rage', 'fury', 'hate', 'irascibility', 'enragement', 'Vexation', 'Wrath', 'Peevishness', 'Annoyance' • Malevolence/Malice: 'spite', 'sadism', 'malevolence', 'malice', 'desire to harm', 'schadenfreude' • Sourness: 'sourness', 'tartness', 'acidity', 'acerbity', 'sharpness' (Note: Primarily gustatory, vocal correlates might be subtle reactions) This section provides an in-depth description of the training procedures for the models discussed in Section 4: i.e. the Whisper backbone and the EMPATHICINSIGHT-VOICE ensembles.\n\nData Curation and Fine-tuning for Emotion Captioning. Our goal was to adapt pre-trained Whisper models  [29]  for the task of generating nuanced emotional captions from speech. The data generation and fine-tuning pipeline involved several key steps:\n\n1. Initial Large-Scale Data Sources: The primary data source was the EMONET-VOICE BIG synthetic voice-acting dataset. This was augmented with approximately 4,500 hours of audio extracted from publicly available online videos (vlogs, diaries, documentaries). We applied voice activity detection (VAD) to isolate speech segments ranging from 3 to 12 seconds.\n\n2. Dimensional Emotion Scoring with Gemini Flash 2.0: All audio snippets-both from EMONET-VOICE BIG and the VAD-extracted clips-were annotated using Gemini Flash 2.0. A complex, multi-shot prompt (detailed in the supplementary materials) guided the model to produce intensity scores on a 0-4 scale (0 = absent, 4 = extremely present) for each of our 40 emotion dimensions simultaneously. This provided a structured, dimensional representation of perceived emotional content.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Iterative Caption Generation For Whisper Training:",
      "text": "• Our initial attempt was to fine-tune Whisper to directly regress these 40-dimensional scores (i.e., to output numerical values), but this approach consistently collapsed into predicting nonsensical sequences of numbers. Similarly, training a specialized output head to perform ordinal regression utilizing a Wasserstein distance loss did not yield more sophisticated or coherent captions. • We then converted the dimensional scores into procedurally generated string captions using predefined templates (e.g., \"The speaker sounds strongly amused and slightly joyful.\"). Training on these templated captions improved over direct regression, but the resulting Whisper outputs still tended toward repetitive or syntactically unnatural phrasing. • The most effective strategy was to take those procedurally generated captions and run them back through Gemini Flash 2.0 for paraphrasing. This second pass introduced significant linguistic diversity and more natural sentence structures, while preserving the original 40-dimensional semantics. The paraphrasing prompt specifically encouraged varied wording and sentence complexity.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "4.",
      "text": "Training Data Preparation: All EMONET-VOICE BIG audio segments longer than 30 seconds were truncated to their first 30 seconds, to meet Whisper's input constraints. Very long segments were further subdivided at silent regions into shorter clips, resulting in a final training pool of over 2 million audio-caption pairs when combined with the processed VAD data.\n\n5. Whisper Fine-tuning: Various sizes of OpenAI's Whisper models were then fine-tuned on this dataset of audio paired with the paraphrased emotional captions. The objective was to teach Whisper to generate fluid, context-sensitive descriptions of emotional content given raw speech input. Iteratively refining the captions via paraphrasing proved crucial for yielding outputs that were both semantically accurate and linguistically natural. We also experimented with incorporating synthetic \"emotion bursts\" during fine-tuning, but this led to degraded embedding quality and was therefore not used in the final models.\n\nEMPATHICINSIGHT-VOICE: MLP Ensembles for Dimensional Emotion Prediction. The EMPATHICINSIGHT-VOICE models were designed to provide direct predictions for each of the 40 emotion dimensions-complementing the captioning approach with explicit scalar estimates.\n\n1. Feature Extraction: We used the encoder from our best-performing Whisper variant as a fixed feature extractor. For any input audio, we ran it through the Whisper encoder and collected the full sequence of token embeddings (sequence length = 1,500; embedding dimension = 768), yielding 1,152,000 features when flattened. Preliminary experiments showed that preserving the entire unpooled sequence outperformed all tested pooling strategies (mean, max, min, concatenation) for downstream MLP regression.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Mlp \"Expert\" Heads:",
      "text": "We trained an ensemble of 40 independent MLP models. Each MLP served as an \"expert\" head dedicated to regressing the intensity score for exactly one of the 40 emotion dimensions using the corresponding flattened Whisper embeddings as input.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Training Targets:",
      "text": "The regression targets were the direct 0-4 intensity scores produced by Gemini Flash 2.0 (via the multi-shot prompt described in the supplementary files).\n\nDuring the encoder fine-tuning stage, we experimented with injecting synthetic \"emotion bursts\"-artificially boosting certain dimension signals in the audio-to encourage a more robust embedding space. However, this augmentation degraded the underlying Whisper embeddings and ultimately hurt downstream MLP performance. Consequently, no synthetic bursts were used for final training.\n\n4. MLP Architecture: Both the Small and Large EMPATHICINSIGHT-VOICE variants share the same overall architectural pattern for regressing from the high-dimensional flattened embeddings:\n\n• Input Projection: A first linear layer reduces the 1,152,000-dimensional input to a much smaller embedding space. • Hidden Layers: Three fully connected layers with ReLU activations, each followed by dropout for regularization to mitigate overfitting. • Output Layer: A final linear projection that outputs a single continuous value in [0, 4], corresponding to the predicted intensity for that emotion.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Model Sizes:",
      "text": "• EMPATHICINSIGHT-VOICE SMALL: The initial projection reduces All trained EMPATHICINSIGHT-VOICE models (Small and Large) and the associated inference code are available via our project page.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "A.3 Hume Voice Mapping A.4 Detailed Taxonomy Construction Methodology",
      "text": "The 40-category emotion taxonomy utilized in both the EMONET-VOICE foundation and benchmark datasets was originally developed for the EmoNet-Face Benchmark  [32] .\n\nThe primary objective was to create a taxonomy that supports a more fine-grained and nuanced understanding of affective states in AI, moving beyond the limitations of traditional basic emotion models. This development was rooted in contemporary psychological research and significantly informed by the principles of the Theory of Constructed Emotion (TCE)  [1] .\n\nThe taxonomy was designed to encompass a wide array of affective experiences, including not only common positive and negative emotions but also intricate social emotions (e.g., Embarrassment, Shame, Pride), cognitive states (e.g., Concentration, Doubt, Confusion), and bodily states (e.g., Pain, Fatigue, Intoxication). Less typical but experientially relevant categories like Sourness and Helplessness were also incorporated. The full list of 40 categories and their descriptive word clusters can be found in App. A.1 (cross-referencing the list you already have, which is similar to App. Tab. 4 from the EmoNet-Face paper).\n\nThe construction process involved several key stages:   9 : Cronbach's α inter-rater reliability (0 = emotion absent; 1 = weakly present; 2 = strongly present) for each emotion category (n = 300 items per label), with 95% confidence intervals obtained via non-parametric bootstrap (1 000 resamples, seed = 42). \"Overall\" reports α and CI computed across all 40 emotion categories + 2 extra categories (12 000 + 600 = 12 600 total annotations). Note that the analysis contains two extra categories (authenticity and arousal) that is not present in the narrow emotion category definition A.1.",
      "page_start": 16,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Annotator agreement for human ratings on perceived emotions in audio samples. Stacked",
      "page": 5
    },
    {
      "caption": "Figure 1: illustrates inter-annotator agreement patterns across emotion categories, showing the dis-",
      "page": 6
    },
    {
      "caption": "Figure 1: ) and superior model alignment (top,",
      "page": 9
    },
    {
      "caption": "Figure 2: Instructions given to the human annotator for the expert annotation of EMONET-VOICE",
      "page": 18
    },
    {
      "caption": "Figure 3: UI of our expert annotation tool for EMONET-VOICE BENCH.",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion\nMini Audio\nAudio\nVoice\n2.0 Flash\n2.5 Pro\nVOICE SMALL (ours)\nVOICE LARGE (ours)": "Teasing\nEmbarrassment\nAnger\nImpatience and Irritability\nMalevolence/Malice\nShame\nSadness\nHelplessness\nAstonishment/Surprise\nPleasure/Ecstasy\nDisgust\nContempt\nFear\nAmusement\nRelief\nPain\nJealousy/ Envy\nElation\nPride\nConfusion\nDisappointment\nDoubt\nTriumph\nInfatuation\nBitterness\nFatigue/Exhaustion\nThankfulness/Gratitude\nIntoxication/ Altered States\nof Consciousness\nDistress\nSexual Lust\nAffection\nLonging\nAwe\nHope/Enthusiasm/Optimism\nSourness\nInterest\nContemplation\nContentment\nEmotional Numbness\nConcentration",
          "avg.": ""
        },
        {
          "emotion\nMini Audio\nAudio\nVoice\n2.0 Flash\n2.5 Pro\nVOICE SMALL (ours)\nVOICE LARGE (ours)": "",
          "avg.": "0.617\n0.585\n0.536\n0.500\n0.477\n0.474\n0.461\n0.457\n0.456\n0.447\n0.432\n0.427"
        },
        {
          "emotion\nMini Audio\nAudio\nVoice\n2.0 Flash\n2.5 Pro\nVOICE SMALL (ours)\nVOICE LARGE (ours)": "",
          "avg.": "0.411\n0.408\n0.407\n0.404\n0.402\n0.401\n0.396\n0.390\n0.386\n0.385\n0.371\n0.367\n0.352\n0.351\n0.347\n0.336\n0.327\n0.326\n0.325\n0.317\n0.275\n0.263\n0.258\n0.221\n0.219\n0.151\n0.123\n0.118"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The theory of constructed emotion",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2017",
      "venue": "Social Cognitive and Affective Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "Crowd-sourced emotional multimodal actors dataset"
    },
    {
      "citation_id": "5",
      "title": "Mapping 24 emotions conveyed by brief human vocalization",
      "authors": [
        "Alan Cowen",
        "Hillary Anger Elfenbein",
        "Petri Laukka",
        "Dacher Keltner"
      ],
      "year": "2019",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "6",
      "title": "The sound of 24 emotions: A new audio dataset of emotional vocalizations and a cross-cultural validation",
      "authors": [
        "Petri Alan S Cowen",
        "Dacher Laukka",
        "Klaus Keltner",
        "Scherer"
      ],
      "year": "2020",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "7",
      "title": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2024",
      "venue": "Leveraging content and acoustic representations for efficient speech emotion recognition",
      "arxiv": "arXiv:2409.05566"
    },
    {
      "citation_id": "8",
      "title": "An Argument for Basic Emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "An Argument for Basic Emotions"
    },
    {
      "citation_id": "9",
      "title": "Multilingual text-to-image generation magnifies gender stereotypes",
      "authors": [
        "Felix Friedrich",
        "Katharina Hämmerl",
        "Patrick Schramowski",
        "Manuel Brack",
        "Jindřich Libovický",
        "Alexander Fraser",
        "Kristian Kersting"
      ],
      "venue": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL, main)"
    },
    {
      "citation_id": "10",
      "title": "Llavaguard: An open vlm-based framework for safeguarding vision datasets and models",
      "authors": [
        "Lukas Helff",
        "Felix Friedrich",
        "Manuel Brack",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "venue": "Proceedings of the 41st International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "11",
      "title": "Does clip know my face",
      "authors": [
        "Dominik Hintersdorf",
        "Lukas Struppek",
        "Manuel Brack",
        "Felix Friedrich",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "12",
      "title": "Surrey audio-visual expressed emotion (savee) database. University of Surrey",
      "authors": [
        "Philip Jackson",
        "Syed Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database. University of Surrey"
    },
    {
      "citation_id": "13",
      "title": "Wrime: A new dataset for emotional intensity estimation with subjective and objective annotations",
      "authors": [
        "Tomoyuki Kajiwara",
        "Chenhui Chu",
        "Noriko Takemura",
        "Yuta Nakashima",
        "Hajime Nagahara"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "14",
      "title": "Why human-ai relationships need socioaffective alignment",
      "authors": [
        "Rose Hannah",
        "Iason Kirk",
        "Chris Gabriel",
        "Bertie Summerfield",
        "Scott Vidgen",
        "Hale"
      ],
      "year": "2025",
      "venue": "Nature, Humanities and Social Sciences Communications"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in human-computer interaction: Why affective understanding matters",
      "authors": [
        "Clara Lee",
        "Rafael Gomez"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "16",
      "title": "Handbook of Emotions",
      "authors": [
        "M Lewis",
        "J Haviland-Jones",
        "L Barrett"
      ],
      "year": "2016",
      "venue": "Handbook of Emotions"
    },
    {
      "citation_id": "17",
      "title": "Asr and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Zeyu Zhao",
        "Ondrej Klejch",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "Asr and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition",
      "arxiv": "arXiv:2305.16065"
    },
    {
      "citation_id": "18",
      "title": "Emotions emerge from more basic psychological ingredients: A modern psychological constructionist model",
      "authors": [
        "Kristen Lindquist"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "19",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "Jaime Lorenzo-Trueba",
        "Junichi Yamagishi",
        "Tomoki Toda",
        "Daisuke Saito",
        "Fernando Villavicencio",
        "Tomi Kinnunen",
        "Zhenhua Ling"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "EmoBox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "authors": [
        "Ziyang Ma",
        "Mingjie Chen",
        "Hezhao Zhang",
        "Zhisheng Zheng",
        "Wenxi Chen",
        "Xiquan Li",
        "Jiaxin Ye",
        "Xie Chen",
        "Thomas Hain"
      ],
      "year": "2024",
      "venue": "EmoBox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark",
      "arxiv": "arXiv:2406.07162"
    },
    {
      "citation_id": "22",
      "title": "Computationally modeling human emotion",
      "authors": [
        "Stacy Marsella",
        "Jonathan Gratch",
        "Paolo Petta"
      ],
      "year": "2010",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "23",
      "title": "A framework for automatic human emotion classification using emotion profiles",
      "authors": [
        "Emily Mower",
        "J Maja",
        "Shrikanth Matarić",
        "Narayanan"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Emoreact: Detecting emotions in children's speech and gestures",
      "authors": [
        "Niloofar Nojavan",
        "Mohammad Soleymani"
      ],
      "year": "2021",
      "venue": "Emoreact: Detecting emotions in children's speech and gestures"
    },
    {
      "citation_id": "25",
      "title": "Gpt-4 omniaudio: Unified audio generation for speech and sound",
      "year": "2024",
      "venue": "Accessed via HyperLab reseller"
    },
    {
      "citation_id": "26",
      "title": "SER Evals: In-domain and out-ofdomain benchmarking for speech emotion recognition",
      "authors": [
        "Mohamed Osman",
        "Daniel Kaplan",
        "Tamer Nadeem"
      ],
      "year": "2024",
      "venue": "SER Evals: In-domain and out-ofdomain benchmarking for speech emotion recognition",
      "arxiv": "arXiv:2408.07851"
    },
    {
      "citation_id": "27",
      "title": "The nature of emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "28",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Rada Mihalcea"
      ],
      "year": "2020",
      "venue": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "arxiv": "arXiv:2005.00357"
    },
    {
      "citation_id": "29",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "31",
      "title": "SERAB: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "Neil Scheidwasser-Clow",
        "Mikolaj Kegler",
        "Pierre Beckmann",
        "Milos Cernak"
      ],
      "year": "2021",
      "venue": "SERAB: A multi-lingual benchmark for speech emotion recognition",
      "arxiv": "arXiv:2110.03414"
    },
    {
      "citation_id": "32",
      "title": "EmoNet-Face: An expert-annotated benchmark for synthetic emotion recognition",
      "authors": [
        "Christoph Schuhmann",
        "Robert Kaczmarczyk",
        "Gollam Rabby",
        "Felix Friedrich",
        "Maurice Kraus",
        "Krishna Kalyan",
        "Kourosh Nadi",
        "Huu Nguyen",
        "Kristian Kersting",
        "Sören Auer"
      ],
      "year": "2025",
      "venue": "EmoNet-Face: An expert-annotated benchmark for synthetic emotion recognition"
    },
    {
      "citation_id": "33",
      "title": "The INTER-SPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Alessandro Vinciarelli",
        "Klaus Scherer",
        "Fabien Ringeval",
        "Mohamed Chetouani",
        "Felix Weninger",
        "Florian Eyben",
        "Erik Marchi",
        "Marcello Mortillaro",
        "Hugues Salamin",
        "Anna Polychroniou",
        "Fabio Valente",
        "Samuel Kim"
      ],
      "year": "2013",
      "venue": "ISCA"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "35",
      "title": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Christ",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Eva-Maria Messner",
        "Erik Cambria",
        "Guoying Zhao",
        "Bjoern Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd International Multimodal Sentiment Analysis Challenge and Workshop"
    },
    {
      "citation_id": "36",
      "title": "BERSting at the screams: A benchmark for distanced, emotional and shouted speech recognition",
      "authors": [
        "Paige Tuttösí",
        "Mantaj Dhillon",
        "Luna Sang",
        "Shane Eastwood",
        "Poorvi Bhatia",
        "Minh Quang",
        "Avni Dinh",
        "Yewon Kapoor",
        "Angelica Jin",
        "Lim"
      ],
      "year": "2025",
      "venue": "BERSting at the screams: A benchmark for distanced, emotional and shouted speech recognition",
      "arxiv": "arXiv:2505.00059"
    },
    {
      "citation_id": "37",
      "title": "Multimodal multi-label emotion detection with modality and label dependence",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "38",
      "title": "Speech emotion recognition: A review of datasets, methods, and challenges",
      "authors": [
        "Lin Zhao",
        "Anand Kumar"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Emotional speech dataset (esd): A parallel corpus in English and Mandarin",
      "authors": [
        "Kun Zhou",
        "Jia Liu",
        "Yuexian Chen"
      ],
      "year": "2021",
      "venue": "Emotional speech dataset (esd): A parallel corpus in English and Mandarin"
    }
  ]
}