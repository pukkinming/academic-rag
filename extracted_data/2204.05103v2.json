{
  "paper_id": "2204.05103v2",
  "title": "Transformer-Based Self-Supervised Learning For Emotion Recognition",
  "published": "2022-04-08T07:14:55Z",
  "authors": [
    "Juan Vazquez-Rodriguez",
    "Grégoire Lefebvre",
    "Julien Cumin",
    "James L. Crowley"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In order to exploit representations of time-series signals, such as physiological signals, it is essential that these representations capture relevant information from the whole signal. In this work, we propose to use a Transformer-based model to process electrocardiograms (ECG) for emotion recognition. Attention mechanisms of the Transformer can be used to build contextualized representations for a signal, giving more importance to relevant parts. These representations may then be processed with a fully-connected network to predict emotions. To overcome the relatively small size of datasets with emotional labels, we employ self-supervised learning. We gathered several ECG datasets with no labels of emotion to pre-train our model, which we then fine-tuned for emotion recognition on the AMIGOS dataset. We show that our approach reaches state-of-the-art performances for emotion recognition using ECG signals on AMIGOS. More generally, our experiments show that transformers and pre-training are promising strategies for emotion recognition with physiological signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "When processing time-series signals with deep learning approaches, it is useful to be able to aggregate information from the whole signal, including long-range information, in a way that the most relevant parts are given more importance. One way of doing this is by employing an attention mechanism  [1]  that uses attention weights to limit processing to relevant contextual information, independent of distance.\n\nArguably, the Transformer  [2]  is one of the most successful attention-based approaches. Developed for Natural Language Processing (NLP), the Transformer uses attention mechanisms to interpret sequences of words, and is suitable for use in other tasks requiring interpretation of sequences, such as time series forecasting,  [3] , analysis of medical physiological signals  [4, 5] , and recognition of human activity from motion  [6] .\n\nPhysiological signal analysis can be seen as a form of time-series analysis and are thus amenable to processing with Transformers. Moreover, these signals can be used to predict emotions  [7] , and sensors for these types of signals can be incorporated into wearable devices, as a non-invasive means for monitoring the emotional reaction of users. Several works in this direction have emerged using signals like electrocardiograms (ECG)  [8, 9] , electroencephalograms (EEG)  [10, 11] , electrodermal activity (EDA)  [12] , and other types of physiological signals  [13, 14] .\n\nEstablished approaches for deep learning with Convolutions and Recurrent networks require large datasets of labeled training data. However, providing ground truth emotion labels for physiological data is a difficult and expensive process, limiting the availability of data for training  [15, 16, 17] . Pre-training models with self-supervised learning can help to overcome this lack of labeled training data. With such an approach, during pre-training the model learns general data representations using large volumes of unlabeled data. The model is then fine tuned for a specific task using labeled data. This approach has been successfully used in other domains including NLP  [18, 19]  and Computer Vision  [20, 21] . It has also been successfully used in affective computing, in tasks like emotion recognition from physiological signals  [9, 22]  and from speech  [23] , personality recognition  [24] , and facial expression recognition  [25, 26, 27] .\n\nIn this paper, we address the problem of predicting emotions from ECG signals. We are interested in obtaining contextualized representations from these signals using a Transformer-based architecture, and then using these representations to predict low/high levels of arousal and valence. We believe that the contextualized representations obtained with the Transformer should capture relevant information from the whole signal, which the performance of the downstream task of emotion recognition should benefit from. Our main contributions are: 1. We show that it is feasible to use a Transformer-based architecture for emotion prediction from ECG signals. 2. We show that using a self-supervised technique to pre-train the model is useful for ECG signals, achieving superior performance in emotion recognition than a fully-supervised approach. 3. We show that our pre-trained Transformer-based model reaches state-of-the-art performances on a dataset of the literature.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Traditional techniques for emotion recognition from physiological signals include Gaussian naive Bayes, Support Vector Machines, k-Nearest Neighbours, and Random Forests.  [16, 17, 28, 29, 30, 31] . These approaches typically use manually-selected time and frequency features derived from intuition and domain knowledge. Shukla et al.  [12]  show that commonly used features for arousal and valence prediction are not necessarily the most discriminant. This illustrates the difficulty of selecting good hand-crafted features.\n\nTo overcome this, researchers have increasingly used deep learning techniques to extract features from physiological signals for emotion recognition. A common approach, described by Santamaria et al.  [8] , is to use a 1D Convolutional Neural Network (CNN) to extract the features (also called representations), followed by a fully-connected network (FCN) used as classifier to predict emotions. As an alternative, Harper and Southern  [32]  use a Long Short-Term Memory (LSTM) network concurrently with a 1D-CNN. Siddharth et al.  [33] , first convert signals into an image using spectrograms  [34] , and then use a 2D-CNN for feature extraction, followed by an extreme learning machine  [35]  for classification.\n\nOne drawback of these CNN-based approaches is that they do not take context into account: after training, kernel weights of the CNN are static, no matter the input. For this reason, attention-based architectures such as the Transformer  [2] , capable of incorporating contextual information, have started to be used for emotion prediction. Transformers have been successfully used to recognize emotions with multimodal inputs composed of text, visual, audio and physiological signals  [36, 37, 38, 39, 40] . In addition, Transformers have been used to process time-series in general  [3, 41] , and also to process uni-modal physiological signals in particular, with the aim of recognizing emotions. Arjun et al.  [42]  employ a variation of the Transformer, the Vision Transformer  [43]  to process EEG signals for emotion recognition, converting the EEG signals into images using continuous wavelet transform. Behinaein et al.  [44]  propose to detect stress from ECG signals, by using a 1D-CNN followed by a Transformer and a FCN as classifier.\n\nMost of the approaches for measuring emotions, including those using multimodal physiological data, have relied on supervised learning, and thus are limited by the availability of labeled training data. Using self-supervised pre-training can improve performances of a model  [45] , as it allows to learn more general representations, thus avoiding overfitting in the downstream task. This is especially important for tasks with limited labeled data. Sarkar and Etemad  [9]  pretrain a 1D-CNN using a self-supervised task to learn representations from ECG signals. Their self-supervised task consists in first transforming the signal, with operations such as scaling or adding noise, and then using the network to predict which transformation has been applied. Ross et al.  [22]  learn representations from ECG signals using autoencoders based on 1D-CNN. In both approaches, once the representations have been learned, they are used to predict emotions.\n\nIn contrast with the two previously mentioned approaches, we propose to take into account contextual information during pre-training by using a Transformer-based model. Such an approach has been used for pre-training Transformers from visual, speech and textual modalities  [23, 46, 47, 48, 49] .  Haresamudram et al. use  this approach to pre-train a Transformer for human activity recognition using accelerometer and gyroscope data  [6] . Zerveas et al.  [50]  develop a framework for multivariate time-series representation learning, by pre-training a Transformer-based architecture. However, none of these works deal with uni-modal physiological signals. In this work, we have extended this approach for use with ECG signals. Specifically, we investigate the effectiveness of pre-training a Transformer for ECG emotion recognition, which to the best of our knowledge has not been done before.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Our Approach",
      "text": "Our framework for using deep learning for emotion recognition is based on the following two steps: first, we need to obtain contextualized representations from time-series signals using a deep model; then, we use those representations to perform the targeted downstream task. In this paper, the considered physiological time-series are raw ECG signals, and the downstream task is binary emotion recognition: predicting high/low levels of arousal, and high/low levels of valence.\n\nFor the first step (see Figure  1 .a), we developed a signal encoder based on deep neural networks and attention, to obtain contextualized representations from ECG signals. The main component of the signal encoder is a Transformer  [2] . This signal encoder is pre-trained with a self-supervised task, using unlabeled ECG data. For the second step (see Figure  1 .b), we fine-tune the whole model (the signal encoder and the fully-connected classifier) for our downstream task of binary emotion recognition, using labeled ECG data.\n\nIn the following subsections, we describe in detail the different components of our approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learning Contextualized Representations",
      "text": "At the heart of our signal encoder is a Transformer encoder  [2] , which we use to learn contextualized representations of ECG signals. In Transformers, contextual information is obtained through an attention mechanism, with the attention function considered as a mapping of a query vector along with a group of key-value vector pairs to an output. In the case of the Transformer encoder, each position in the output pays attention to all positions in the input. Several attention modules (also called heads) are used, creating various representation subspaces and improving the ability of the model to be attentive to different positions. The Transformer encoder is constructed by stacking several layers containing a multi-head attention module followed by a fully-connected network applied to each position, with residual connections. Since our implementation of the Transformer is almost identical to the one described in  [2] , we refer the readers to this paper for further details.\n\nIn Figure  2 , we present our signal encoder, which we describe in the remainder of this subsection.\n\nInput Encoder: to process an ECG signal with the Transformer, we first encode it into s feature vectors of dimension d model that represent each one of the s values of the ECG signal. We use 1D Convolutional Neural Networks (1D-CNN) to perform this encoding, like in  [6, 36, 51] . Thus, for a raw input signal X = {x 1 , ..., x s } where x i is a single value, after encoding X with the input encoder we obtain features F = {f 1 , ..., f s } where f i ∈ R dmodel .\n\nCLS token: given that our downstream task is a classification task, we need to obtain a single representation of the whole processed signal at the output of our signal encoder. Similar to what is done in BERT  [19] , we append a special classification token (CLS) at the start of the feature sequence F , resulting in the sequence F ′ = {CLS, f 1 , ..., f s }. We use a trainable vector of dimension d model as CLS token. At the output of the Transformer, we obtain an embedding of the CLS token (e CLS ), along with the rest of the representations of the signal (see Figure  2  and Equation  2 ). Through the  attention mechanisms of the Transformer, e CLS is capable of aggregating information from the entire input signal and its contextualized representations. For this reason, at classification time, e CLS can be used as input for the classifier network.\n\nPositional Encoding: positional information of each input is required so that the Transformer can take into account the actual ordering of time-steps in the input sequence. As in  [2] , we use fixed sinusoidal positional embeddings. We sum the positional embeddings with the features F ′ :\n\nwhere pe i ∈ R dmodel is the positional embedding for time-step i. We then apply layer normalization  [52]  to Z. Please refer to  [2]  for details on how to obtain the positional embeddings.\n\nTransformer Encoder: we obtain contextualized representations E using a Transformer encoder with h heads and l layers on the sequence Z:\n\nWe then use the representations E for emotion recognition, as is described in Section 3.3",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Pre-Training Task",
      "text": "To pre-train our signal encoder, we employ a self-supervised approach inspired in BERT  [19] . We mask random segments of a certain length by replacing them with zeros, and then we train our model to predict the masked values, as shown in Figure  1a . Labeled data is not needed for this step.\n\nSimilar to  [51] , a proportion p of points is randomly selected from the input signal as starting points for masked segments, and then for each starting point the subsequent M points are masked. The masked segments may overlap.\n\nTo predict masked points, we use a fully-connected network (FCN) on top of the signal encoder, as shown in Figure  1a .\n\nWe only predict values of masked inputs, as opposed to reconstructing the whole signal. We use the mean square error between predicted and real values as the reconstruction loss L r during pre-training:\n\nwhere N m is the number of masked values, xj is the prediction corresponding to the j th masked value, and x p(j) is the original input value selected to be the j th masked value, whose position is p(j) in the input signal.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fine-Tuning",
      "text": "We fine-tune our model to perform binary emotion prediction, as shown in Figure  1b . This step is supervised, using labeled data. To make the prediction, a FCN is added on top of the signal encoder, using e CLS as input. We initialize the signal encoder with the weights obtained after pre-training, while the FCN is randomly initialized. We then fine-tune all the parameters of the model, including the pre-trained weights. For this task, we minimize the binary cross-entropy loss L f t :\n\n) where y is an indicator variable with value 1 if the class of the ground truth is positive and 0 if it is negative, out is the output of the classifier, σ is the sigmoid function, and w p is the ratio of negative to positive training samples, used to compensate unbalances that may be present in the dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we describe the experimental choices taken to evaluate our approach for a downstream task of binary emotion recognition (high/low levels of arousal and valence), on ECG signals. We present the datasets used, the pre-processes employed, and the parametrization of our two steps of pre-training and fine-tuning.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "For pre-training, we only require datasets that contain ECG signals, regardless of why they were actually collected or which labeling they have, if any. The datasets that we use in our experiments are: ASCERTAIN  [16] , DREAMER  [53] , PsPM-FR  [54] , PsPM-HRM5  [55] , PsPM-RRM1-2  [56] , and PsPM-VIS  [57] . We also employ the AMIGOS dataset  [17] , taking care of not using the same data for pre-training and evaluating our model, as this dataset is also used for the downstream task. To gather as much data as possible, we use all the ECG channels available in the datasets. For ASCERTAIN, we discard some signals according to the quality evaluation provided in the dataset: if a signal has a quality level of 3 or worse in the provided scale, it is discarded. In total, there are around 230 hours of ECG data for pre-training.\n\nTo fine-tune our model to predict emotions, we use the AMIGOS dataset  [17] . In this dataset, 40 subjects watched videos specially selected to evoke an emotion. After watching each video, a self-assessment of their emotional state is conducted. In this assessment, subjects rated their levels of arousal and valence on a scale of 1 to 9. Of the 40 subjects, 37 watched a total of 20 videos, while the other 3 subjects watched only 16 videos. During each trial, ECG data were recorded on both left and right arms. We use data only from the left arm to fine-tune our model. AMIGOS includes a pre-processed version of the data, that was down-sampled to 128Hz and filtered with a low-pass filter with 60Hz cut-off frequency. We use these pre-processed data for our experiments, including the pre-training phase. The ECG data that we use for fine-tuning amounts to around 65 hours of recordings.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Signal Pre-Processing",
      "text": "We first filter signals with an 8 th order Butterworth band-pass filter, having a low-cut-off frequency of 0.8Hz and a high-cut-off frequency of 50Hz. We then down-sample the signals to 128 Hz, except for AMIGOS which already has that sampling rate. Signals are normalized so they have zero-mean and unit-variance, for each subject independently. Signals are finally divided into 10-second segments (we also report results for segments of 20 seconds and 40 seconds).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Training",
      "text": "As stated previously, we use ASCERTAIN, DREAMER, PsPM-FR, PsPM-RRM1-2, PsPM-VIS, and AMIGOS for pre-training. Since we also use AMIGOS for fine-tuning, we need to avoid using the same segments both for pretraining and for evaluating the model. To do this, we pre-train two models, one using half of the data from AMIGOS, and the second using the other half. When testing our model with certain segments from AMIGOS, we fine-tune the model that was pre-trained with the half of AMIGOS that do not contain those segments. More details are given in Section 4.4. In total, both of our models are pre-trained with 83401 10-second segments.\n\nWe select a proportion of p = 0.0325 points from each input segment to be the starting point of a masked span of length M = 20, resulting in around 47% of the input values masked.\n\nThe input encoder is built with 3 layers of 1D-CNN with ReLU activation function. We use layer normalization  [52]  on the first layer, and at the output of the encoder. Kernel sizes are  (65, 33, 17) , the numbers of channels are (64, 128, 256) and the stride for all layers is 1. This results in a receptive field of 113 input values or 0.88s. We selected this receptive field size because it is comparable with the typical interval between peaks on an ECG signal, which is between 0.6s and 1s, including when experiencing emotions  [58] .\n\nThe Transformer in our signal encoder has a model dimension d model = 256, 2 layers and 2 attention heads, with its FCN size of d model • 4 = 1024. The FCN used to predict the masked values consists of a single linear layer of size d model /2 = 128 followed by a ReLU activation function. An additional linear layer is used to project the output vector to a single value, which corresponds to the predicted value of a masked point.\n\nWe pre-train the two models for 500 epochs, warming up the learning rate over the first 30 epochs up to a value of 0.001 and using linear decay after that. We employ Adam optimization, with β 1 = 0.9, β 2 = 0.999, and L 2 weight decay of 0.005. We use dropout of 0.1 at the end of the input encoder, after the positional encoding, and inside the Transformer.\n\nWe tuned the number of layers and heads in the Transformer, the learning rate, and the warm-up duration using the Ray Tune framework  [59]  with BOHB optimization  [60] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning",
      "text": "We fine-tune our model (both the signal encoder and FCN classifier) for emotion recognition with the AMIGOS dataset, using each of the 10-second segments as a sample. As labels, we use the emotional self-assessments given in the dataset. Since these assessments provide values of arousal and valence on a scale 1 to 9, we use the average arousal and the average valence as threshold value to determine a low or a high level.\n\nWe use 10-fold cross-validation to evaluate our approach. Recall that we pre-train two signal encoders. After dividing AMIGOS into 10 folds, we use folds 1 to 5 to pre-train one signal encoder (SE 1 ), and folds 6 to 10 to pre-train the second one (SE 2 ) (and all data from the other datasets, for both). Then, when we fine-tune the models to be tested with folds 1 to 5, we use the weights from SE 2 to initialize the signal encoder parameters. In a similar fashion, we use SE 1 as initialization point of the signal encoder when we fine-tune the models to be tested with folds 6 to 10. This method allows us to pre-train, fine-tune and test our model in a more efficient way than pre-training 10 different models, one for each fold, while retaining complete separations between training and testing data.\n\nThe FCN classifier used to predict emotions has two hidden layers of sizes [1024, 512] with ReLU activation functions, and an output layer that projects the output to a single value. We fine-tune one model to predict arousal and another to predict valence. For each task, we fine-tune our model for 100 epochs using Adam optimization, with β 1 = 0.9, β 2 = 0.999 and L 2 weight decay of 0.00001. We start with a learning rate of 0.0001, and decrease it every 45 epochs by a factor of 0.65. We keep using a dropout of 0.1 at the end of the input encoder, after the positional encoding, and inside the Transformer. We use dropout of 0.3 in the FCN classifier.\n\nWe used Ray Tune with BOHB, as we did on pre-training, to tune the learning rate, the learning rate schedule, the shape and dropout of the FCN classifier, and the L 2 weight decay.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "In our results, we use as metrics the mean accuracy and mean F1-score between positive and negative classes. We report the mean and confidence intervals of the metrics across our 10 folds of cross-validation. The confidence intervals are calculated using a t-distribution with 9 degrees of freedom, for a two-sided 95% confidence.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparing Aggregation Methods And Segment Lengths",
      "text": "We report in Table  1  the performances of our approach for different strategical choices. Firstly, we compare different aggregation approaches to combine the contextualized representations at the output of the signal encoder, given to the FCN classifier. Secondly, we compare performances for different segment lengths used to divide the input signals.   [32]  Yes Not segmented --0.81 0.80 Convolutional autoencoder  [22]  No 10 seconds 0.85 0.89 --",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Our Protocol",
      "text": "Pre-trained CNN  [9]  No 10 seconds 0.85±5.4e -3 0.84±5.3e -3 0.77±5.5e -3 0.77±5.1e -3 Pre-trained Transformer (ours) No 10 seconds 0.88±5.4e -3 0.87±5.4e -3 0.83±7.8e -3 0.83±7.4e -3\n\nAggregation Method: we compared 4 strategies for aggregating representations, to be given as input to the FCN: maxpooling, average-pooling, using only the last representation e s , and using only the embedding of the CLS token e CLS (we call this strategy CLS). Max-pooling 1 and average-pooling 1 are the result of max-pooling and average-pooling across all representations, to obtain a single representation of size d model = 256. Max-pooling 2 was optimized on the validation set: representations are reduced to a size of 64, divided into two groups, then max-pooling was applied on each group and the results concatenated to obtain a single representation of size 128. Average-pooling 2 was optimized on the validation set: representations are divided into 4 groups, average-pooling is applied on each group and the results concatenated to obtain a single representation of size 1024.\n\nWe see in Table  1  that the best results were obtained with average-pooling strategies and with CLS, with accuracies up to 0.88 for arousal, for example. In the following experiments, we will thus use CLS as our aggregation method. Indeed, although results are practically identical for CLS and average-pooling 2 (e.g. 0.88±5.4e -3 compared to 0.88±4.4e -3 accuracies for arousal), CLS has the advantage of being a commonly-used strategy for Transformers, which does not require any kind of tuning on validation data, contrary to average-pooling 2.\n\nSegment length: we compare 3 different segment lengths for dividing ECG signals into input instances: 10, 20, and 40 second segments. We can see in Table  1  that shorter segments lead to better results on average, both for arousal and valence. For example for arousal, 10-second segments lead to an accuracy of 0.88±5.4e -3 , compared to 0.87±5.6e -3 for 20-second segments, and 0.86±1.2e -2 for 40-second segments.\n\nTwo explanations emerge for this observation: firstly, since emotions are relatively volatile states, longer segmentation might cover fluctuating emotional states, thus making it harder to characterize emotion; secondly, longer segments should require more complex models (i.e. bigger Transformer and FCN), which are harder to train with the relatively restricted amount of labeled data in AMIGOS. Moreover, shorter segments are faster to process, allowing a high number of training epochs and smaller learning rates. In the following experiments, we will thus use 10-second segments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effectiveness Of Pre-Training",
      "text": "To demonstrate the effectiveness of our pre-training approach, we tested our architecture by fine-tuning our model on AMIGOS with all parameters randomly initialized, instead of using a pre-trained signal encoder (thus skipping step (a) of our process in Figure  1 ). As reported in Table  2 , the pre-trained model is on average significantly better than the model with no pre-training, for both accuracy and F1-score. For example, for arousal, the pre-trained model reaches an average accuracy of 0.88 ± 5.4e -3 , compared to 0.85 ± 5.6e -3 for the model with no pre-training. These results illustrate the benefits of pre-training Transformers for our task. Moreover, during our experiments, we observed that the model with no pre-training had a tendency to overfit quickly, which was not the case for the pre-trained model.\n\nPre-training the model on many different datasets should increase its robustness to overfitting when fine-tuning on a specific dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparisons With Other Approaches",
      "text": "We report in Table  3   Nevertheless, we report them to showcase the variety of state-of-the-art approaches published for this task, and give a relative idea of achieved performances on AMIGOS.\n\nTo compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly the same experiment protocol. For this, we fully retrained and tested the pre-trained CNN approach proposed by Sarkar and Etemad  [9] , with the experiment protocol we presented. To this end, we use the implementation provided by the authors 1  . To ensure fair comparisons, the exact same data was used to pre-train, fine-tune, and test both our approach and also Sarkar and Etemad's approach, for each fold of cross-validation.\n\nWe see in Table  3  that our approach achieves better performance on average than Sarkar and Etemad's approach with the same experiment protocol, for both arousal and valence. For example, our approach achieves an F1-score of 0.83±7.4e -3 for valence, compared to 0.77±5.1e -3 for the pre-trained CNN. These results are statistically significant with p < 0.01 following a t-test.\n\nThis final set of results shows that our approach, and more generally self-supervised Transformer-based approaches, can be successfully applied to obtain contextualized representations from ECG signals for emotion recognition tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions And Perspectives",
      "text": "In this paper, we investigate the use of transformers for recognizing arousal and valence from ECG signals. This approach used self-supervised learning for pre-training from unlabeled data, followed by fine-tuning with labeled data.\n\nOur experiments indicate that the model builds robust features for predicting arousal and valence on the AMIGOS dataset, and provides very promising results in comparison to recent state-of-the-art methods. This work showcases that self-supervision and attention-based models such as Transformers can be successfully used for research in affective computing.\n\nMultiple perspectives emerge from our work. New pre-training tasks can be investigated: other methods such as contrastive loss or triplet loss might be more efficient with regards to the specificities of ECG signals, compared to masked points prediction which we used in this work. Extending our work to other input modalities (EEC, GSR, and even non-physiological inputs such as ambient sensors) and, in general, to process multimodal situations could prove useful for improving performances of emotion recognition. Finally, larger scale experiments, with new datasets captured in varied situations, will allow for a better understanding of the behaviour of our approach.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our approach with self supervised learning based on a Transformer (a) and ﬁne-tuning strategy for learning",
      "page": 3
    },
    {
      "caption": "Figure 1: a), we developed a signal encoder based on deep neural networks and attention, to",
      "page": 3
    },
    {
      "caption": "Figure 1: b), we ﬁne-tune the whole model (the signal encoder and the fully-connected classiﬁer) for our downstream",
      "page": 3
    },
    {
      "caption": "Figure 2: , we present our signal encoder, which we describe in the remainder of this subsection.",
      "page": 3
    },
    {
      "caption": "Figure 2: and Equation 2). Through the",
      "page": 3
    },
    {
      "caption": "Figure 2: Our Transformer-based signal encoder that produces contextualized representations. The aggregated repre-",
      "page": 4
    },
    {
      "caption": "Figure 1: a. Labeled data is not needed for this step.",
      "page": 4
    },
    {
      "caption": "Figure 1: b. This step is supervised, using",
      "page": 4
    },
    {
      "caption": "Figure 1: ). As reported in Table 2, the pre-trained model is on average signiﬁcantly better than the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "In order to exploit representations of time-series signals, such as physiological signals,\nit\nis essen-"
        },
        {
          "ABSTRACT": "tial that these representations capture relevant information from the whole signal.\nIn this work, we"
        },
        {
          "ABSTRACT": "propose to use a Transformer-based model to process electrocardiograms (ECG) for emotion recog-"
        },
        {
          "ABSTRACT": "nition. Attention mechanisms of the Transformer can be used to build contextualized representations"
        },
        {
          "ABSTRACT": "for a signal, giving more importance to relevant parts. These representations may then be processed"
        },
        {
          "ABSTRACT": "with a fully-connected network to predict emotions."
        },
        {
          "ABSTRACT": "To overcome the relatively small size of datasets with emotional labels, we employ self-supervised"
        },
        {
          "ABSTRACT": "learning. We gathered several ECG datasets with no labels of emotion to pre-train our model, which"
        },
        {
          "ABSTRACT": "we then ﬁne-tuned for emotion recognition on the AMIGOS dataset. We show that our approach"
        },
        {
          "ABSTRACT": "reaches\nstate-of-the-art performances\nfor emotion recognition using ECG signals on AMIGOS."
        },
        {
          "ABSTRACT": "More generally, our experiments show that\ntransformers and pre-training are promising strategies"
        },
        {
          "ABSTRACT": "for emotion recognition with physiological signals."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nIntroduction": "When processing time-series signals with deep learning approaches,\nit\nis useful\nto be able to aggregate information"
        },
        {
          "1\nIntroduction": "from the whole signal,\nincluding long-range information,\nin a way that\nthe most\nrelevant parts are given more im-"
        },
        {
          "1\nIntroduction": "portance. One way of doing this is by employing an attention mechanism [1]\nthat uses attention weights to limit"
        },
        {
          "1\nIntroduction": "processing to relevant contextual information, independent of distance."
        },
        {
          "1\nIntroduction": "Arguably, the Transformer [2] is one of the most successful attention-based approaches. Developed for Natural Lan-"
        },
        {
          "1\nIntroduction": "guage Processing (NLP), the Transformer uses attention mechanisms to interpret sequences of words, and is suitable"
        },
        {
          "1\nIntroduction": "for use in other tasks requiring interpretation of sequences, such as time series forecasting, [3], analysis of medical"
        },
        {
          "1\nIntroduction": "physiological signals [4, 5], and recognition of human activity from motion [6]."
        },
        {
          "1\nIntroduction": "Physiological signal analysis can be seen as a form of time-series analysis and are thus amenable to processing with"
        },
        {
          "1\nIntroduction": "Transformers. Moreover, these signals can be used to predict emotions [7], and sensors for these types of signals can"
        },
        {
          "1\nIntroduction": "be incorporated into wearable devices, as a non-invasive means for monitoring the emotional reaction of users. Several"
        },
        {
          "1\nIntroduction": "works in this direction have emerged using signals like electrocardiograms (ECG) [8, 9], electroencephalograms (EEG)"
        },
        {
          "1\nIntroduction": "[10, 11], electrodermal activity (EDA) [12], and other types of physiological signals [13, 14]."
        },
        {
          "1\nIntroduction": "Established approaches for deep learning with Convolutions and Recurrent networks require large datasets of labeled"
        },
        {
          "1\nIntroduction": "training data. However, providing ground truth emotion labels for physiological data is a difﬁcult and expensive"
        },
        {
          "1\nIntroduction": "process,\nlimiting the availability of data for training [15, 16, 17]. Pre-training models with self-supervised learning"
        },
        {
          "1\nIntroduction": "can help to overcome this lack of labeled training data. With such an approach, during pre-training the model learns"
        },
        {
          "1\nIntroduction": "general data representations using large volumes of unlabeled data. The model\nis then ﬁne tuned for a speciﬁc task"
        },
        {
          "1\nIntroduction": "using labeled data. This approach has been successfully used in other domains including NLP [18, 19] and Computer"
        },
        {
          "1\nIntroduction": "Vision [20, 21].\nIt has also been successfully used in affective computing,\nin tasks like emotion recognition from"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "[25, 26, 27]."
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "In this paper, we address the problem of predicting emotions from ECG signals. We are interested in obtaining"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "contextualized representations from these signals using a Transformer-based architecture, and then using these rep-"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "resentations to predict\nlow/high levels of arousal and valence. We believe that\nthe contextualized representations"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "obtained with the Transformer should capture relevant information from the whole signal, which the performance of"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "the downstream task of emotion recognition should beneﬁt from. Our main contributions are: 1. We show that\nit\nis"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "feasible to use a Transformer-based architecture for emotion prediction from ECG signals. 2. We show that using a"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "self-supervised technique to pre-train the model\nis useful for ECG signals, achieving superior performance in emo-"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "tion recognition than a fully-supervised approach. 3. We show that our pre-trained Transformer-based model reaches"
        },
        {
          "physiological signals [9, 22] and from speech [23], personality recognition [24], and facial expression recognition": "state-of-the-art performances on a dataset of the literature."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Masked Values": "Prediction",
          "Emotion": "Prediction"
        },
        {
          "Masked Values": "",
          "Emotion": "FULLY-CONNECTED"
        },
        {
          "Masked Values": "",
          "Emotion": "Emotion classiﬁer"
        },
        {
          "Masked Values": "Contextualized",
          "Emotion": ""
        },
        {
          "Masked Values": "Representations",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": "TRANSFORMER"
        },
        {
          "Masked Values": "",
          "Emotion": "ENCODER"
        },
        {
          "Masked Values": "",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": "Positional"
        },
        {
          "Masked Values": "",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": "Encoding"
        },
        {
          "Masked Values": "SIGNAL",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": ""
        },
        {
          "Masked Values": "ENCODER",
          "Emotion": "+"
        },
        {
          "Masked Values": "",
          "Emotion": ""
        },
        {
          "Masked Values": "",
          "Emotion": "INPUT ENCODER"
        },
        {
          "Masked Values": "",
          "Emotion": "(1D-CNN)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Encoding": "+\n+\n+\n+\n+"
        },
        {
          "Encoding": "Features F’"
        },
        {
          "Encoding": "...\nInput Encoder\nCLS\nCNN\nCNN\nCNN"
        },
        {
          "Encoding": "Figure 2: Our Transformer-based signal encoder that produces contextualized representations. The aggregated repre-"
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": "attention mechanisms of the Transformer, eCLS is capable of aggregating information from the entire input signal and"
        },
        {
          "Encoding": "its contextualized representations. For this reason, at classiﬁcation time, eCLS can be used as input for the classiﬁer"
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": "actual ordering of time-steps in the input sequence. As in [2], we use ﬁxed sinusoidal positional embeddings. We sum"
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": "(1)\nZ = {CLS + pe0, f1 + pe1, ..., fs + pes},"
        },
        {
          "Encoding": "is the positional embedding for time-step i. We then apply layer normalization [52] to Z. Please"
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": ""
        },
        {
          "Encoding": "(2)\nE = {eCLS, e1, ..., es} = Transformerh,l(Z)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": ""
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "Max-Pooling 1"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "Max-Pooling 2"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "Average-Pooling 1"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "Average-Pooling 2"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "Last Representation"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "40 seconds"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": ""
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "20 seconds"
        },
        {
          "Table 1: Comparison of different strategies of our approach on AMIGOS dataset": "CLS with 10s segment"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nExperimental Setup": "In this section, we describe the experimental choices taken to evaluate our approach for a downstream task of binary"
        },
        {
          "4\nExperimental Setup": "emotion recognition (high/low levels of arousal and valence), on ECG signals. We present\nthe datasets used,\nthe"
        },
        {
          "4\nExperimental Setup": "pre-processes employed, and the parametrization of our two steps of pre-training and ﬁne-tuning."
        },
        {
          "4\nExperimental Setup": "4.1\nDatasets"
        },
        {
          "4\nExperimental Setup": "For pre-training, we only require datasets that contain ECG signals, regardless of why they were actually collected or"
        },
        {
          "4\nExperimental Setup": "which labeling they have, if any. The datasets that we use in our experiments are: ASCERTAIN [16], DREAMER [53],"
        },
        {
          "4\nExperimental Setup": "PsPM-FR [54], PsPM-HRM5 [55], PsPM-RRM1-2 [56], and PsPM-VIS [57]. We also employ the AMIGOS dataset"
        },
        {
          "4\nExperimental Setup": "[17],\ntaking care of not using the same data for pre-training and evaluating our model, as this dataset is also used for"
        },
        {
          "4\nExperimental Setup": "the downstream task. To gather as much data as possible, we use all\nthe ECG channels available in the datasets. For"
        },
        {
          "4\nExperimental Setup": "ASCERTAIN, we discard some signals according to the quality evaluation provided in the dataset:\nif a signal has a"
        },
        {
          "4\nExperimental Setup": "quality level of 3 or worse in the provided scale,\nit is discarded.\nIn total,\nthere are around 230 hours of ECG data for"
        },
        {
          "4\nExperimental Setup": "pre-training."
        },
        {
          "4\nExperimental Setup": "To ﬁne-tune our model\nto predict emotions, we use the AMIGOS dataset [17].\nIn this dataset, 40 subjects watched"
        },
        {
          "4\nExperimental Setup": "videos specially selected to evoke an emotion. After watching each video, a self-assessment of their emotional state is"
        },
        {
          "4\nExperimental Setup": "conducted. In this assessment, subjects rated their levels of arousal and valence on a scale of 1 to 9. Of the 40 subjects,"
        },
        {
          "4\nExperimental Setup": "37 watched a total of 20 videos, while the other 3 subjects watched only 16 videos. During each trial, ECG data were"
        },
        {
          "4\nExperimental Setup": "recorded on both left and right arms. We use data only from the left arm to ﬁne-tune our model. AMIGOS includes"
        },
        {
          "4\nExperimental Setup": "a pre-processed version of the data,\nthat was down-sampled to 128Hz and ﬁltered with a low-pass ﬁlter with 60Hz"
        },
        {
          "4\nExperimental Setup": "cut-off frequency. We use these pre-processed data for our experiments,\nincluding the pre-training phase. The ECG"
        },
        {
          "4\nExperimental Setup": "data that we use for ﬁne-tuning amounts to around 65 hours of recordings."
        },
        {
          "4\nExperimental Setup": "4.2\nSignal Pre-processing"
        },
        {
          "4\nExperimental Setup": "We ﬁrst ﬁlter signals with an 8th order Butterworth band-pass ﬁlter, having a low-cut-off frequency of 0.8Hz and a"
        },
        {
          "4\nExperimental Setup": "high-cut-off frequency of 50Hz. We then down-sample the signals to 128 Hz, except for AMIGOS which already has"
        },
        {
          "4\nExperimental Setup": "that sampling rate. Signals are normalized so they have zero-mean and unit-variance, for each subject independently."
        },
        {
          "4\nExperimental Setup": "Signals are ﬁnally divided into 10-second segments (we also report results for segments of 20 seconds and 40 seconds)."
        },
        {
          "4\nExperimental Setup": "4.3\nPre-training"
        },
        {
          "4\nExperimental Setup": "As stated previously, we use ASCERTAIN, DREAMER, PsPM-FR, PsPM-RRM1-2, PsPM-VIS, and AMIGOS for"
        },
        {
          "4\nExperimental Setup": "pre-training. Since we also use AMIGOS for ﬁne-tuning, we need to avoid using the same segments both for pre-"
        },
        {
          "4\nExperimental Setup": "training and for evaluating the model. To do this, we pre-train two models, one using half of the data from AMIGOS,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "model\nthat was pre-trained with the half of AMIGOS that do not contain those segments. More details are given in"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "Section 4.4. In total, both of our models are pre-trained with 83401 10-second segments."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "to be the starting point of a masked span of\nWe select a proportion of p = 0.0325 points from each input segment"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "length M = 20, resulting in around 47% of the input values masked."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "The input encoder is built with 3 layers of 1D-CNN with ReLU activation function. We use layer normalization [52]"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "on the ﬁrst\nlayer, and at\nthe output of the encoder. Kernel sizes are (65, 33, 17),\nthe numbers of channels are (64,"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "128, 256) and the stride for all\nlayers is 1. This results in a receptive ﬁeld of 113 input values or 0.88s. We selected"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "this receptive ﬁeld size because it\nis comparable with the typical interval between peaks on an ECG signal, which is"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "between 0.6s and 1s, including when experiencing emotions [58]."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "The Transformer in our signal encoder has a model dimension dmodel = 256, 2 layers and 2 attention heads, with its"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "· 4 = 1024. The FCN used to predict the masked values consists of a single linear layer of size\nFCN size of dmodel"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "dmodel/2 = 128 followed by a ReLU activation function. An additional linear layer is used to project the output vector"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "to a single value, which corresponds to the predicted value of a masked point."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "We pre-train the two models for 500 epochs, warming up the learning rate over the ﬁrst 30 epochs up to a value of"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "0.001 and using linear decay after that. We employ Adam optimization, with β1 = 0.9, β2 = 0.999, and L2 weight"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "decay of 0.005. We use dropout of 0.1 at\nthe end of the input encoder, after the positional encoding, and inside the"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "Transformer."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "We tuned the number of layers and heads in the Transformer,\nthe learning rate, and the warm-up duration using the"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "Ray Tune framework [59] with BOHB optimization [60]."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "4.4\nFine-Tuning"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "We ﬁne-tune our model\n(both the signal encoder and FCN classiﬁer)\nfor emotion recognition with the AMIGOS"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "dataset, using each of the 10-second segments as a sample. As labels, we use the emotional self-assessments given"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "in the dataset. Since these assessments provide values of arousal and valence on a scale 1 to 9, we use the average"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "arousal and the average valence as threshold value to determine a low or a high level."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "We use 10-fold cross-validation to evaluate our approach. Recall that we pre-train two signal encoders. After dividing"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "AMIGOS into 10 folds, we use folds 1 to 5 to pre-train one signal encoder (SE1), and folds 6 to 10 to pre-train the"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "second one (SE2) (and all data from the other datasets, for both). Then, when we ﬁne-tune the models to be tested"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "In a similar fashion, we\nwith folds 1 to 5, we use the weights from SE2 to initialize the signal encoder parameters."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "use SE1 as initialization point of the signal encoder when we ﬁne-tune the models to be tested with folds 6 to 10."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "This method allows us to pre-train, ﬁne-tune and test our model in a more efﬁcient way than pre-training 10 different"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "models, one for each fold, while retaining complete separations between training and testing data."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "The FCN classiﬁer used to predict emotions has two hidden layers of sizes [1024, 512] with ReLU activation functions,"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "and an output layer that projects the output to a single value. We ﬁne-tune one model to predict arousal and another"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "to predict valence. For each task, we ﬁne-tune our model for 100 epochs using Adam optimization, with β1 = 0.9,"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "β2 = 0.999 and L2 weight decay of 0.00001. We start with a learning rate of 0.0001, and decrease it every 45 epochs"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "by a factor of 0.65. We keep using a dropout of 0.1 at the end of the input encoder, after the positional encoding, and"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "inside the Transformer. We use dropout of 0.3 in the FCN classiﬁer."
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "We used Ray Tune with BOHB, as we did on pre-training,\nto tune the learning rate,\nthe learning rate schedule,\nthe"
        },
        {
          "and the second using the other half. When testing our model with certain segments from AMIGOS, we ﬁne-tune the": "shape and dropout of the FCN classiﬁer, and the L2 weight decay."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": ""
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "Subj. Ind."
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": ""
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "Yes"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "No"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "Yes"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "Yes"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "No"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "No"
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": ""
        },
        {
          "Table 3: Comparison of different methods on AMIGOS dataset": "No"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: that our approach achieves better performance on average than Sarkar and Etemad’s approach",
      "data": [
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "the same experiment protocol. For this, we fully retrained and tested the pre-trained CNN approach proposed by Sarkar"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "and Etemad [9], with the experiment protocol we presented. To this end, we use the implementation provided by the"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "authors1. To ensure fair comparisons, the exact same data was used to pre-train, ﬁne-tune, and test both our approach"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "and also Sarkar and Etemad’s approach, for each fold of cross-validation."
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "We see in Table 3 that our approach achieves better performance on average than Sarkar and Etemad’s approach"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "with the same experiment protocol, for both arousal and valence. For example, our approach achieves an F1-score of"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "0.83±7.4e−3 for valence, compared to 0.77±5.1e−3 for the pre-trained CNN. These results are statistically signiﬁcant"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "with p < 0.01 following a t-test."
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "This ﬁnal set of results shows that our approach, and more generally self-supervised Transformer-based approaches,"
        },
        {
          "To compare our approach with another state-of-the-art approach as fairly as possible, it is required that both use exactly": "can be successfully applied to obtain contextualized representations from ECG signals for emotion recognition tasks."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: that our approach achieves better performance on average than Sarkar and Etemad’s approach",
      "data": [
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "References"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "align and translate.\nIn Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "and Illia Polosukhin. Attention is All you Need. Advances in Neural Information Processing Systems, 30, 2017."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[3] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting.\nIn Advances in"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[4] Genshen Yan, Shen Liang, Yanchun Zhang, and Fan Liu.\nFusing Transformer Model with Temporal Features"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "for ECG Heartbeat Classiﬁcation.\nIn 2019 IEEE International Conference on Bioinformatics and Biomedicine"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "(BIBM), pages 898–905, November 2019."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[5] David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, and Lars Petersson. At-"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "the IEEE\ntention Networks for Multi-Task Signal Analysis.\nIn 2020 42nd Annual International Conference of"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Engineering in Medicine Biology Society (EMBC), pages 184–187, July 2020."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[6] Harish Haresamudram, Apoorva Beedu, Varun Agrawal, Patrick L. Grady,\nIrfan Essa,\nJudy Hoffman,\nand"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Thomas Plötz. Masked reconstruction based self-supervision for human activity recognition.\nIn Proceedings"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "of\nthe 2020 International Symposium on Wearable Computers, pages 45–49, New York, NY, USA, September"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "2020. Association for Computing Machinery."
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[7] Lin Shu, Jinyan Xie, Mingyue Yang, Ziyi Li, Zhenqi Li, Dan Liao, Xiangmin Xu, and Xinyi Yang. A Review of"
        },
        {
          "Grenoble Alpes: (MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Emotion Recognition Using Physiological Signals. Sensors, 18(7):2074, July 2018."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "ing Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "IEEE Access, 7:57–67, 2019."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[9] P. Sarkar and A. Etemad.\nSelf-Supervised Learning for ECG-Based Emotion Recognition.\nIn ICASSP 2020 -"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3217–3221,"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "May 2020."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[10] T. Song, W. Zheng, P. Song, and Z. Cui.\nEEG Emotion Recognition Using Dynamical Graph Convolutional"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Neural Networks.\nIEEE Transactions on Affective Computing, 11(3):532–541, July 2020."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[11] H. Becker, J. Fleureau, P. Guillotel, F. Wendling, I. Merlet, and L. Albera. Emotion Recognition Based on High-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Resolution EEG Recordings and Reconstructed Brain Sources.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "11(2):244–257, April 2020."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[12]\nJ. Shukla, M. Barreda-Angeles, J. Oliver, G. C. Nandi, and D. Puig. Feature Extraction and Selection for Emotion"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Recognition from Electrodermal Activity.\nIEEE Transactions on Affective Computing, pages 1–1, 2019."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[13] Shuhao Chen, Ke Jiang, Haoji Hu, Haoze Kuang, Jianyi Yang, Jikui Luo, Xinhua Chen, and Yubo Li. Emotion"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Recognition Based on Skin Potential Signals with a Portable Wireless Device.\nSensors, 21(3):1018, January"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "2021."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[14] Mangesh Ramaji Kose, Mitul Kumar Ahirwal, and Anil Kumar.\nA new approach for emotions recognition"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "through EOG and EMG signals. Signal, Image and Video Processing, 15(8):1863–1871, November 2021."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[15] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi,"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Thierry Pun, Anton Nijholt, and Ioannis Patras. DEAP: A Database for Emotion Analysis ;Using Physiological"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Signals.\nIEEE Transactions on Affective Computing, 3(1):18–31, January 2012."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[16] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L. Vieriu, Stefan Winkler, and Nicu"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "IEEE Transactions on\nSebe. ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Affective Computing, 9(2):147–160, April 2018."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[17]\nJ. A. Miranda Correa, M. K. Abadi, N. Sebe, and I. Patras. AMIGOS: A Dataset\nfor Affect, Personality and"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Mood Research on Individuals and Groups.\nIEEE Transactions on Affective Computing, pages 1–1, 2018."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[18] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "moyer. Deep Contextualized Word Representations.\nIn Proceedings of the 2018 Conference of the North Ameri-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "can Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Papers), pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[19]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chap-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Learning of Visual Representations.\nIn Proceedings of the 37th International Conference on Machine Learning,"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "pages 1597–1607. PMLR, November 2020."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[21] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka,"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual Transformers: Token-based Image Representation and"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Processing for Computer Vision. arXiv:2006.03677 [cs, eess], November 2020."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[22] Kyle Ross, Paul Hungler, and Ali Etemad. Unsupervised multi-modal representation learning for affective com-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "puting with multi-corpus wearable data.\nJournal of Ambient Intelligence and Humanized Computing, October"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "2021."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[23] Manon Macary, Marie Tahon, Yannick Estève, and Anthony Rousseau.\nOn the Use of Self-Supervised Pre-"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition.\nIn 2021 IEEE Spoken"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Language Technology Workshop (SLT), pages 373–380, January 2021."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[24] Siyang Song, Shashank Jaiswal, Enrique Sanchez, Georgios Tzimiropoulos, Linlin Shen, and Michel Valstar."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "IEEE\nSelf-supervised Learning of Person-speciﬁc Facial Dynamics for Automatic Personality Recognition."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "Transactions on Affective Computing, pages 1–1, 2021."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[25] Olivia Wiles, A. Sophia Koepke, and Andrew Zisserman. Self-supervised learning of a facial attribute embedding"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "from video. arXiv:1808.06882 [cs], August 2018."
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "[26] Yong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Self-Supervised Representation Learning From Videos"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "for Facial Action Unit Detection.\nIn 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
        },
        {
          "[8] L. Santamaria-Granados, M. Munoz-Organero, G. Ramirez-González, E. Abdulhay, and N. Arunkumar. Us-": "(CVPR), pages 10916–10925, Long Beach, CA, USA, June 2019. IEEE."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Proceedings of\nthe 2021 International Conference on Multimodal\nInteraction, pages 253–257, Montréal QC"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Canada, October 2021. ACM."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[28] Martin Gjoreski, Blagoj Mitrevski, Mitja Luštrek, and Matjaž Gams. An Inter-domain Study for Arousal Recog-"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "nition from Physiological Signals.\nInformatica, 42(1), March 2018."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[29] De˘ger Ayata, Yusuf Yaslan, and Mustafa E. Kamasak.\nEmotion Recognition from Multimodal Physiological"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Signals for Emotion Aware Healthcare Systems.\nJournal of Medical and Biological Engineering, 40(2):149–"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "157, April 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[30] Y. Hsu, J. Wang, W. Chiang, and C. Hung. Automatic ECG-Based Emotion Recognition in Music Listening."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "IEEE Transactions on Affective Computing, 11(1):85–99, January 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[31] Lin Shu, Yang Yu, Wenzhuo Chen, Haoqiang Hua, Qin Li, Jianxiu Jin, and Xiangmin Xu. Wearable Emotion"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Recognition Using Heart Rate Data from a Smart Bracelet. Sensors, 20(3):718, January 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[32] R. Harper and J. Southern. A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Heartbeat.\nIEEE Transactions on Affective Computing, pages 1–1, 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[33] S. Siddharth, T. Jung, and T. J. Sejnowski.\nUtilizing Deep Learning Towards Multi-modal Bio-sensing and"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Vision-based Affective Computing. IEEE Transactions on Affective Computing, pages 1–1, 2019."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[34] Sean A. Fulop and Kelly Fitz. Algorithms for computing the time-corrected instantaneous frequency (reassigned)"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "spectrogram, with applications. The Journal of the Acoustical Society of America, 119(1):360–371, January 2006."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[35] Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. Extreme learning machine: Theory and applications."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Neurocomputing, 70(1-3):489–501, December 2006."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[36] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhut-"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "the 57th\ndinov. Multimodal Transformer for Unaligned Multimodal Language Sequences.\nIn Proceedings of"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Annual Meeting of the Association for Computational Linguistics, pages 6558–6569, Florence, Italy, July 2019."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Association for Computational Linguistics."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[37] Z. Wu, X. Zhang, T. Zhi-Xuan,\nJ. Zaki, and D. C. Ong.\nAttending to Emotional Narratives.\nIn 2019 8th"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "International Conference on Affective Computing and Intelligent Interaction (ACII), pages 648–654, September"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "2019."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[38]\nJian Huang, Jianhua Tao, Bin Liu, Zheng Lian, and Mingyue Niu. Multimodal Transformer Fusion for Contin-"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "uous Emotion Recognition.\nIn ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Signal Processing (ICASSP), pages 3507–3511, May 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[39] Cong Cai, Yu He, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao, Mingyu Xu, and Kexin Wang. Multimodal"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "the 2nd\nSentiment Analysis based on Recurrent Neural Network and Multimodal Attention.\nIn Proceedings of"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "on Multimodal Sentiment Analysis Challenge, pages 61–67, Virtual Event China, October 2021. ACM."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[40] Woan-Shiuan Chien, Huang-Cheng Chou, and Chi-Cun Lee. Self-assessed Emotion Classiﬁcation from Acoustic"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "the 2021 Interna-\nand Physiological Features within Small-group Conversation.\nIn Companion Publication of"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "tional Conference on Multimodal Interaction, pages 230–239, Montreal QC Canada, October 2021. ACM."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[41] Neo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep Transformer Models for Time Series Forecasting:"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "The Inﬂuenza Prevalence Case. arXiv:2001.08317 [cs, stat], January 2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[42] Arjun Arjun, Aniket Singh Rajpoot, and Mahesh Raveendranatha Panicker. Introducing Attention Mechanism for"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "EEG Signals: Emotion Recognition with Vision Transformers.\nIn 2021 43rd Annual International Conference"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "of the IEEE Engineering in Medicine Biology Society (EMBC), pages 5723–5726, November 2021."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[43] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\narXiv:2010.11929 [cs], October"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "2020."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[44] Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Paul Hungler, and Ali Etemad. A Transformer Architec-"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "ture for Stress Detection from ECG.\nIn 2021 International Symposium on Wearable Computers, pages 132–134,"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Virtual USA, September 2021. ACM."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[45] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why Does Unsupervised Pre-training"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "the Thirteenth International Conference on Artiﬁcial Intelligence and\nHelp Deep Learning?\nIn Proceedings of"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "Statistics, pages 201–208. JMLR Workshop and Conference Proceedings, March 2010."
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "[46] Aparna Khare, Srinivas Parthasarathy, and Shiva Sundaram. Multi-Modal Embeddings Using Multi-Task Learn-"
        },
        {
          "[27] Shuvendu Roy and Ali Etemad.\nSelf-supervised Contrastive Learning of Multi-view Facial Expressions.\nIn": "ing for Emotion Recognition.\nIn Interspeech 2020, pages 384–388. ISCA, October 2020."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Morency, and Ehsan Hoque.\nIntegrating Multimodal Information in Large Pretrained Transformers.\nIn Proceed-"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "ings of\nthe 58th Annual Meeting of\nthe Association for Computational Linguistics, pages 2359–2369, Online,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "July 2020. Association for Computational Linguistics."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[48] S. Siriwardhana, T. Kaluarachchi, M. Billinghurst, and S. Nanayakkara. Multimodal Emotion Recognition With"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Transformer-Based Self Supervised Feature Fusion.\nIEEE Access, 8:176274–176285, 2020."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[49] Aparna Khare, Srinivas Parthasarathy, and Shiva Sundaram. Self-Supervised Learning with Cross-Modal Trans-"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "formers for Emotion Recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 381–388,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "January 2021."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[50] George Zerveas, Srideepika\nJayaraman, Dhaval Patel, Anuradha Bhamidipaty,\nand Carsten Eickhoff.\nA"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Transformer-based Framework for Multivariate Time Series Representation Learning. In Proceedings of the 27th"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2114–2124, Virtual Event Singapore,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "August 2021. ACM."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[51] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: A Framework for Self-"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Supervised Learning of Speech Representations. Advances in Neural Information Processing Systems, 33:12449–"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "12460, 2020."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[52]\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv:1607.06450 [cs, stat],"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "July 2016."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[53] Stamos Katsigiannis and Naeem Ramzan. DREAMER: A Database for Emotion Recognition Through EEG"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "IEEE Journal of Biomedical and Health\nand ECG Signals From Wireless Low-cost Off-the-Shelf Devices."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Informatics, 22(1):98–107, January 2018."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[54] Athina Tzovara, Dominik R. Bach, Giuseppe Castegnetti, Samuel Gerster, Nicolas Hofer, Saurabh Khemka,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Christoph W. Korn, Philipp C. Paulus, Boris B. Quednow, and Matthias Staib. PsPM-FR: SCR, ECG and respi-"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "ration measurements in a delay fear conditioning task with visual CS and electrical US., August 2018."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[55] Philipp C. Paulus, Giuseppe Castegnetti, and Dominik R. Bach.\nPsPM-HRM5:\nSCR, ECG and respiration"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "measurements in response to positive/negative IAPS pictures, and neutral/aversive sounds, June 2020."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[56] Dominik R. Bach, Samuel Gerster, Athina Tzovara, and Giuseppe Castegnetti.\nPsPM-RRM1-2: SCR, ECG,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "respiration and eye tracker measurements in response to electric stimulation or visual targets, September 2019."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[57] Yanfang Xia, Filip Melinšˇcak, and Dominik R. Bach. PsPM-VIS: SCR, ECG, respiration and eyetracker mea-"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "surements in a delay fear conditioning task with visual CS and electrical US, July 2020."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[58] Yan Wu, Ruolei Gu, Qiwei Yang, and Yue-jia Luo. How Do Amusement, Anger and Fear Inﬂuence Heart Rate"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "and Heart Rate Variability? Frontiers in Neuroscience, 13:1131, 2019."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[59] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion Stoica.\nTune: A"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Research Platform for Distributed Model Selection and Training. arXiv:1807.05118 [cs, stat], July 2018."
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "[60] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and Efﬁcient Hyperparameter Optimization at"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "Scale.\nIn Proceedings of\nthe 35th International Conference on Machine Learning, pages 1437–1446. PMLR,"
        },
        {
          "[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe": "July 2018."
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "2",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting",
      "authors": [
        "Shiyang Li",
        "Xiaoyong Jin",
        "Xiyou Yao Xuan",
        "Wenhu Zhou",
        "Yu-Xiang Chen",
        "Xifeng Wang",
        "Yan"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Fusing Transformer Model with Temporal Features for ECG Heartbeat Classification",
      "authors": [
        "Genshen Yan",
        "Shen Liang",
        "Yanchun Zhang",
        "Fan Liu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "5",
      "title": "Attention Networks for Multi-Task Signal Analysis",
      "authors": [
        "David Ahmedt-Aristizabal",
        "Mohammad Ali Armin",
        "Simon Denman",
        "Clinton Fookes",
        "Lars Petersson"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC)"
    },
    {
      "citation_id": "6",
      "title": "Masked reconstruction based self-supervision for human activity recognition",
      "authors": [
        "Harish Haresamudram",
        "Apoorva Beedu",
        "Varun Agrawal",
        "Patrick Grady",
        "Irfan Essa",
        "Judy Hoffman",
        "Thomas Plötz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Symposium on Wearable Computers"
    },
    {
      "citation_id": "7",
      "title": "A Review of Emotion Recognition Using Physiological Signals",
      "authors": [
        "Lin Shu",
        "Jinyan Xie",
        "Mingyue Yang",
        "Ziyi Li",
        "Zhenqi Li",
        "Dan Liao",
        "Xiangmin Xu",
        "Xinyi Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Self-Supervised Learning for ECG-Based Emotion Recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition Based on High-Resolution EEG Recordings and Reconstructed Brain Sources",
      "authors": [
        "H Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Albera"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Feature Extraction and Selection for Emotion Recognition from Electrodermal Activity",
      "authors": [
        "J Shukla",
        "M Barreda-Angeles",
        "J Oliver",
        "G Nandi",
        "D Puig"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Emotion Recognition Based on Skin Potential Signals with a Portable Wireless Device",
      "authors": [
        "Shuhao Chen",
        "Ke Jiang",
        "Haoji Hu",
        "Haoze Kuang",
        "Jianyi Yang",
        "Jikui Luo",
        "Xinhua Chen",
        "Yubo Li"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "A new approach for emotions recognition through EOG and EMG signals. Signal",
      "authors": [
        "Mangesh Ramaji Kose",
        "Mitul Kumar Ahirwal",
        "Anil Kumar"
      ],
      "year": "2021",
      "venue": "Image and Video Processing"
    },
    {
      "citation_id": "15",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors",
      "authors": [
        "Ramanathan Subramanian",
        "Julia Wache",
        "Mojtaba Khomami Abadi",
        "Radu Vieriu",
        "Stefan Winkler",
        "Nicu Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Miranda Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Deep Contextualized Word Representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "19",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision",
      "authors": [
        "Bichen Wu",
        "Chenfeng Xu",
        "Xiaoliang Dai",
        "Alvin Wan",
        "Peizhao Zhang",
        "Zhicheng Yan",
        "Masayoshi Tomizuka",
        "Joseph Gonzalez",
        "Kurt Keutzer",
        "Peter Vajda"
      ],
      "year": "2020",
      "venue": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision",
      "arxiv": "arXiv:2006.03677"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "Kyle Ross",
        "Paul Hungler",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "23",
      "title": "On the Use of Self-Supervised Pre-Trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition",
      "authors": [
        "Manon Macary",
        "Marie Tahon",
        "Yannick Estève",
        "Anthony Rousseau"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "24",
      "title": "Self-supervised Learning of Person-specific Facial Dynamics for Automatic Personality Recognition",
      "authors": [
        "Siyang Song",
        "Shashank Jaiswal",
        "Enrique Sanchez",
        "Georgios Tzimiropoulos",
        "Linlin Shen",
        "Michel Valstar"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Self-supervised learning of a facial attribute embedding from video",
      "authors": [
        "Olivia Wiles",
        "A Sophia Koepke",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "Self-supervised learning of a facial attribute embedding from video",
      "arxiv": "arXiv:1808.06882"
    },
    {
      "citation_id": "26",
      "title": "Self-Supervised Representation Learning From Videos for Facial Action Unit Detection",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "Self-supervised Contrastive Learning of Multi-view Facial Expressions",
      "authors": [
        "Shuvendu Roy",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "An Inter-domain Study for Arousal Recognition from Physiological Signals",
      "authors": [
        "Martin Gjoreski",
        "Blagoj Mitrevski",
        "Mitja Luštrek",
        "Matjaž Gams"
      ],
      "year": "2018",
      "venue": "Informatica"
    },
    {
      "citation_id": "29",
      "title": "Emotion Recognition from Multimodal Physiological Signals for Emotion Aware Healthcare Systems",
      "authors": [
        "Deger Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "30",
      "title": "Automatic ECG-Based Emotion Recognition in Music Listening",
      "authors": [
        "Y Hsu",
        "J Wang",
        "W Chiang",
        "C Hung"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Wearable Emotion Recognition Using Heart Rate Data from a Smart Bracelet",
      "authors": [
        "Lin Shu",
        "Yang Yu",
        "Wenzhuo Chen",
        "Haoqiang Hua",
        "Qin Li",
        "Jianxiu Jin",
        "Xiangmin Xu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "32",
      "title": "A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat",
      "authors": [
        "R Harper",
        "J Southern"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Utilizing Deep Learning Towards Multi-modal Bio-sensing and Vision-based Affective Computing",
      "authors": [
        "S Siddharth",
        "T Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Algorithms for computing the time-corrected instantaneous frequency (reassigned) spectrogram, with applications",
      "authors": [
        "Sean Fulop",
        "Kelly Fitz"
      ],
      "year": "2006",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "35",
      "title": "Extreme learning machine: Theory and applications",
      "authors": [
        "Guang-Bin Huang",
        "Qin-Yu Zhu",
        "Chee-Kheong Siew"
      ],
      "year": "2006",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Attending to Emotional Narratives",
      "authors": [
        "Z Wu",
        "X Zhang",
        "T Zhi-Xuan",
        "J Zaki",
        "D Ong"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "38",
      "title": "Multimodal Transformer Fusion for Continuous Emotion Recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "39",
      "title": "Multimodal Sentiment Analysis based on Recurrent Neural Network and Multimodal Attention",
      "authors": [
        "Cong Cai",
        "Yu He",
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao",
        "Mingyu Xu",
        "Kexin Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "40",
      "title": "Self-assessed Emotion Classification from Acoustic and Physiological Features within Small-group Conversation",
      "authors": [
        "Woan-Shiuan Chien",
        "Huang-Cheng Chou",
        "Chi-Cun Lee"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "41",
      "title": "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case",
      "authors": [
        "Neo Wu",
        "Bradley Green",
        "Xue Ben",
        "Shawn O' Banion"
      ],
      "year": "2020",
      "venue": "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case",
      "arxiv": "arXiv:2001.08317"
    },
    {
      "citation_id": "42",
      "title": "Introducing Attention Mechanism for EEG Signals: Emotion Recognition with Vision Transformers",
      "authors": [
        "Arjun Arjun",
        "Aniket Singh Rajpoot",
        "Mahesh Panicker"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC)"
    },
    {
      "citation_id": "43",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2020",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "44",
      "title": "A Transformer Architecture for Stress Detection from ECG",
      "authors": [
        "Anubhav Behnam Behinaein",
        "Dirk Bhatti",
        "Paul Rodenburg",
        "Ali Hungler",
        "Etemad"
      ],
      "year": "2021",
      "venue": "2021 International Symposium on Wearable Computers"
    },
    {
      "citation_id": "45",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
      "authors": [
        "Dumitru Erhan",
        "Aaron Courville",
        "Yoshua Bengio",
        "Pascal Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "46",
      "title": "Multi-Modal Embeddings Using Multi-Task Learning for Emotion Recognition",
      "authors": [
        "Aparna Khare",
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2020",
      "venue": "Interspeech 2020"
    },
    {
      "citation_id": "47",
      "title": "Integrating Multimodal Information in Large Pretrained Transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amirali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "48",
      "title": "Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "49",
      "title": "Self-Supervised Learning with Cross-Modal Transformers for Emotion Recognition",
      "authors": [
        "Aparna Khare",
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "50",
      "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
      "authors": [
        "George Zerveas",
        "Srideepika Jayaraman",
        "Dhaval Patel",
        "Anuradha Bhamidipaty",
        "Carsten Eickhoff"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "51",
      "title": "Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "Layer Normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer Normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "53",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "54",
      "title": "PsPM-FR: SCR, ECG and respiration measurements in a delay fear conditioning task with visual CS and electrical US",
      "authors": [
        "Athina Tzovara",
        "R Dominik",
        "Giuseppe Bach",
        "Samuel Castegnetti",
        "Nicolas Gerster",
        "Saurabh Hofer",
        "Christoph Khemka",
        "Korn",
        "C Philipp",
        "Boris Paulus",
        "Matthias Quednow",
        "Staib"
      ],
      "year": "2018",
      "venue": "PsPM-FR: SCR, ECG and respiration measurements in a delay fear conditioning task with visual CS and electrical US"
    },
    {
      "citation_id": "55",
      "title": "PsPM-HRM5: SCR, ECG and respiration measurements in response to positive/negative IAPS pictures, and neutral/aversive sounds",
      "authors": [
        "C Philipp",
        "Giuseppe Paulus",
        "Dominik Castegnetti",
        "Bach"
      ],
      "year": "2020",
      "venue": "PsPM-HRM5: SCR, ECG and respiration measurements in response to positive/negative IAPS pictures, and neutral/aversive sounds"
    },
    {
      "citation_id": "56",
      "title": "PsPM-RRM1-2: SCR, ECG, respiration and eye tracker measurements in response to electric stimulation or visual targets",
      "authors": [
        "R Dominik",
        "Samuel Bach",
        "Athina Gerster",
        "Giuseppe Tzovara",
        "Castegnetti"
      ],
      "year": "2019",
      "venue": "PsPM-RRM1-2: SCR, ECG, respiration and eye tracker measurements in response to electric stimulation or visual targets"
    },
    {
      "citation_id": "57",
      "title": "PsPM-VIS: SCR, ECG, respiration and eyetracker measurements in a delay fear conditioning task with visual CS and electrical US",
      "authors": [
        "Yanfang Xia",
        "Filip Melinščak",
        "Dominik Bach"
      ],
      "year": "2020",
      "venue": "PsPM-VIS: SCR, ECG, respiration and eyetracker measurements in a delay fear conditioning task with visual CS and electrical US"
    },
    {
      "citation_id": "58",
      "title": "How Do Amusement, Anger and Fear Influence Heart Rate and Heart Rate Variability?",
      "authors": [
        "Yan Wu",
        "Ruolei Gu",
        "Qiwei Yang",
        "Yue-Jia Luo"
      ],
      "year": "2019",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "59",
      "title": "Tune: A Research Platform for Distributed Model Selection and Training",
      "authors": [
        "Richard Liaw",
        "Eric Liang",
        "Robert Nishihara",
        "Philipp Moritz",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "year": "2018",
      "venue": "Tune: A Research Platform for Distributed Model Selection and Training",
      "arxiv": "arXiv:1807.05118"
    },
    {
      "citation_id": "60",
      "title": "BOHB: Robust and Efficient Hyperparameter Optimization at Scale",
      "authors": [
        "Stefan Falkner",
        "Aaron Klein",
        "Frank Hutter"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning"
    }
  ]
}