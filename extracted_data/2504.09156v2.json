{
  "paper_id": "2504.09156v2",
  "title": "Lel: A Novel Lipschitz Continuity-Constrained Ensemble Learning Model For Eeg-Based Emotion Recognition",
  "published": "2025-04-12T09:41:23Z",
  "authors": [
    "Shengyu Gong",
    "Yueyang Li",
    "Zijian Kang",
    "Weiming Zeng",
    "Hongjie Yan",
    "Zhiguo Zhang",
    "Wai Ting Siok",
    "Nizhuan Wang"
  ],
  "keywords": [
    "Electroencephalography (EEG)",
    "Lipschitz Continuity",
    "Ensemble Learning",
    "EEG-based Emotion Recognition (EER)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The accurate and efficient recognition of emotional states in oneself and others is critical, as impairments in this ability can lead to significant psychosocial difficulties. While electroencephalography (EEG) offers a powerful tool for emotion detection, current EEG-based emotion recognition (EER) methods face key limitations: insufficient model stability, limited accuracy in processing highdimensional nonlinear EEG signals, and poor robustness against intra-subject variability and signal noise. To address these challenges, we introduce LEL (Lipschitz continuity-constrained Ensemble Learning), a novel framework that enhances EEG-based emotion recognition. By integrating Lipschitz continuity constraints, LEL ensures greater model stability and improves generalization, thereby reducing sensitivity to signal variability and noise while significantly boosting the model's overall accuracy and robustness. Its ensemble learning strategy optimizes overall performance by fusing decisions from multiple classifiers to reduce single-model bias and variance. Experimental results on three public benchmark datasets (EAV, FACED and SEED) demonstrated the LEL's state-of-the-art performance, achieving average recognition accuracies of 76.43%, 83.00% and 87.22%, respectively. The official implementation codes are released at https://github.com/NZWANG/LEL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition-the capacity to assess emotional states-is fundamental to social functioning, enabling relationship building, social understanding, and adaptive responses. While sentiment analysis offers a method for efficient psychological assessment  [1, 2, 3] , this capacity is often significantly impaired in individuals with neurodevelopmental and psychiatric conditions such as Autism Spectrum Disorder (ASD), Attention-Deficit/Hyperactivity Disorder (ADHD), depression and schizophrenia  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] . These impairments lead to substantial personal suffering and societal burdens  [14] , necessitating efficient and accurate recognition tools for both clinical and computational applications  [15] . EEG-based Emotion Recognition (EER) addresses this need by providing an objective assessment of emotional disorders, particularly in cases where communication barriers exist  [16] . EEG's direct measurement of neurophysiological activity captures internal states with high temporal resolution, enabling: 1) detection of rapid emotional dynamics critical for mental health research, 2) non-invasive continuous monitoring for longitudinal studies, and 3) practical deployment through single-channel approaches  [17, 18] . This capability enables early intervention and facilitates precision treatment strategies  [1] .\n\nEmotion complexity-shaped by neurobiological, interpersonal, cultural, and individual factors  [2] -presents fundamental challenges. The absence of universal categorization frameworks and significant expression variations in clinical populations  [3]  necessitate precise quantification of internal states for diagnostics and personalized therapies  [19, 20] . Recent EER advancements exemplify sophisticated solutions: FreqDGT  [21]  and the EEG emotion copilot  [22]  leverage EEG's spatiotemporal dynamics to advance objective emotion modeling  [23] .\n\nIn this study, we propose a Lipschitz continuity-constrained Ensemble Learning (LEL) framework for advancing EER with the following key contributions:\n\n• The integration of Lipschitz continuity constraints across different modules enhances model stability and generalization during training. This effectively addresses EEG signal variability while improving robustness and recognition accuracy. By restricting the Lipschitz constant, the model's sensitivity to input perturbations is controlled, thereby enhancing its robustness against noise and artifacts in EEG signals.\n\n• The LEL employs an ensemble learning strategy that harnesses the synergistic strengths of multiple classifiers, significantly enhancing both the precision and resilience of emotion recognition. By combining the predictions of multiple classifiers, we can reduce the risk of overfitting associated with individual classifiers and improve the model's ability to recognize complex emotional patterns.\n\n• The LEL demonstrates outstanding performance in emotion recognition across multiple benchmark datasets, confirming its effectiveness and generalizability.\n\nThe remainder of this paper is organized as follows. Section 2 reviews recent advances and key challenges in EER and Lipschitz continuity constraints. Section 3 delineates the methodology of the proposed LEL framework, detailing the integration of Lipschitz constraints and ensemble learning strategies. Section 4 presents the experimental setup and assesses the framework's performance across multiple benchmark datasets. Section 5 interprets the experimental results and discusses their implications for EER research and applications. Finally, Section 6 summarizes the principal contributions and outlines prospective research directions.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Recent Advances And Challenges In Eer",
      "text": "Recent advancements in EER have been characterized by the development of increasingly accurate  [24, 17]  and robust models  [25] . A prominent strategy contributing to this progress is multimodal data fusion, which integrates EEG signals with other physiological modalities such as eye movement and heart rate. This approach enables the extraction of complementary information from diverse sources, thereby facilitating a more holistic understanding of emotional states  [16] . For instance, Kang et al. introduced a hypergraph multimodal learning framework that effectively combines EEG with additional physiological signals to enhance emotion recognition accuracy  [14] .\n\nAnother significant development in EER is the construction of brain network features to capture complex signal patterns and dynamic neural interactions. This method provides deeper insights into the neural mechanisms underlying emotional processing  [26] . The application of two-dimensional convolutional neural networks has shown promise in improving emotion recognition performance  [18] . Spatial-temporal transformers further refine these systems by modeling both spatial and temporal dependencies in EEG data  [23] . A case-based channel selection technique using interpretable transformer networks optimizes efficiency and accuracy  [20] . A novel direction involves neural multimodal contrastive representation learning, applied to EEG-based visual decoding, offering a fresh perspective on emotion recognition methodologies  [19] .\n\nDespite these promising developments, EER continues to face several critical challenges. Firstly, the inherently low signal-to-noise ratio of EEG data complicates feature extraction, as weak brain signals are often obscured by background noise  [26] . Secondly, EEG recordings are highly susceptible to artifacts arising from eye blinks, muscle movements, and other physiological activities, which can distort the data and lead to erroneous interpretations  [27] . Thirdly, there exists substantial intra-individual variability in EEG signals, where the brain activity of the same individual may vary significantly across different contexts or time points, posing difficulties for model stability and reliability  [28] . Lastly, the generalization capability of current EER models remains limited. Models that perform well on specific datasets often exhibit a marked decline in performance when applied to different datasets or populations, thereby hindering the scalability and practical deployment of EER technologies  [29, 30, 31] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Lipschitz Continuity Constraints And Their Applications",
      "text": "In the field of machine learning, ensuring model stability and robustness is crucial, especially when dealing with noisy or perturbed data. Lipschitz continuous constraints have emerged as a powerful tool to achieve this. They provide a rigorous mathematical framework that enhances model stability by restricting gradient variations. By limiting the rate of change of the output with respect to the input, these constraints ensure the model's robustness to minor perturbations. This is particularly beneficial in real-world applications where data can be messy and unpredictable. For example, Jia et al. demonstrated improved node-level adversarial robustness in graph neural networks through Lipschitz regularization, significantly enhancing stability with noisy data  [32] . Their subsequent work applied similar constraints in a fairness regularization method to control output variations and ensure individual fairness  [33] .\n\nLipschitz continuous constraints also strengthen the generalization ability of models. By regularizing model behavior across diverse data distributions and individual variations, these constraints help in creating models that perform consistently across different scenarios. This makes Lipschitz continuous constraints particularly valuable for addressing EER challenges, where signal variability and noise sensitivity are persistent issues  [34]  [35]  [33] . While Lipschitz constraints address model stability, attention mechanisms significantly advance deep learning models' ability to handle complex data structures. The Transformer architecture achieves efficient parallel computation and long-range dependency modeling through self-attention mechanisms  [36] . However, the standard dot-product self-attention mechanism tends toward overconfidence with unbounded input domains, producing high-confidence predictions despite uncertainty  [37] . Moreover, deep Graph Attention Networks (GAT) are prone to gradient explosion when increased layers, limiting their performance in long-rang dependency tasks  [38] .\n\nTo address these issues, researchers have proposed several improvements. For example, Ye et al. introduced Lipschitz regularization to mitigate the overconfidence of Transformer models  [37] . Dasoulas et al. proposed a Lipschitz normalization method for self-attention layers, significantly improving the performance of deep Graph Neural Networks  [38] .\n\n3 Method",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition",
      "text": "Generally speaking, the EEG signal can be expressed as X = {x 1 , x 2 , . . . , x T }, where x T ∈ R C represents the signal vector at time point t, C is the number of EEG channels, and T is the length of the time series. Further, we can define the emotional category set Y = {y 1 , y 2 , . . . , y K }, K is the total number of emotional categories. Thus, we can define the EER problem as a task that aims to discern an individual's emotional state at a particular moment or over a temporal span based on EEG data. The corresponding mathematic expression can be formulated as the emotional category label ŷ prediction process based on the given X, following the maximization of the prediction probability as:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Lipschitz Continuity",
      "text": "Lipschitz continuity is a property that describes the rate of change of a function and is strictly defined in metric space.\n\nIncorporating Lipschitz continuity into models can effectively enhance their generalization and robustness, making it particularly valuable for EEG signal processing where noise and variability are commonly challenging  [39] [40][41]  [42] .\n\nLet (X, d X ) and (Y, d Y ) be two metric spaces, where d X and d Y are the distance functions on X and Y , respectively. A function f : X → Y is said to be Lipschitz continuous if there exists a non-negative constant L such that for any two points x 1 , x 2 ∈ X, the following inequality holds:\n\nThis inequality indicates the extent to which the function f \"amplifies\" or \"reduces\" the distance between any two points in X. For real-valued functions f : R → R, Lipschitz continuity can be expressed as:\n\nThis inequality implies that the absolute value of the slope of the line connecting any two points on the graph of the function will not exceed L. In other words, the \"steepness\" of the function is bounded by L.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Lipschitz Gradient-Constrained Band Extraction Component (Lgcbe)",
      "text": "By integrating band decomposition, attention mechanisms and Lipschitz constraints, the LGCBE significantly enhances signal quality and precisely extracts key features. The core strength of LGCBE lies in its ability to capture higher-order relationships within the signals while dynamically adjusting modality weights to enable flexible resource allocation based on signal characteristics and context, effectively handling complex interactions and subtle contextual changes. This dynamic adjustment mechanism not only increases the flexibility of signal processing but also enhances the robustness of the system, enabling it to maintain high performance even in the presence of noise interference or incomplete data. The implementation of LGCBE is shown in Figure  1b , while LGCBE-E, LGCBE-F, and LGCBE-B represent the three different output results of the module, corresponding to channel energy, channel feature and band feature, respectively. These outputs provide a rich information base for subsequent signal analysis and processing.\n\nThe input signal X has dimensions with [B, C, T ], where B is the batch size, C is the number of channels, and T is the sequence length. First, the input signal is transformed from the time domain to the frequency domain using the Fast Fourier Transform (FFT):\n\nThe resulting frequency-domain signal F has dimensions with [B, C, F ], where F is the dimension of the frequency domain. For each band b ∈ {δ, θ, α, β, γ}, the band mask is computed and band features are extracted as:\n\nwhere I is the indicator function and ⊙ denotes element-wise multiplication. The energy of each frequency band is calculated as:\n\nThe resulting band energy P b has dimensions [B, C]. The channel attention weights are computed as:\n\nwhere σ is the Sigmoid activation function and MLP channel is a network comprising two fully connected layers. The spectral attention scores are computed as:\n\nwhere MLP spectral is a network comprising two fully connected layers. To satisfy the Lipschitz continuity constraints, the spectral attention scores are normalized:\n\nwhere ϵ = 10 -6 is a numerical stability constant and L is the Lipschitz constant. Then, the enhanced signal is reconstructed by combining the band features through weighted summation:\n\nwhere W s,b is the expanded form of the spectral weights.\n\nThe signal is transformed back to the time domain using the Inverse Fast Fourier Transform (IFFT) by applying the IFFT to the combined frequency-domain data X combined along the last dimension for a time length of T, and then normalization is applied using LayerNorm.\n\nTo enhance model stability, spectral normalization is applied to the weights of the linear layers. This involves scaling the weights W by their spectral norm ∥W∥ 2 and then multiplying by a Lipschitz constant L:\n\nwhere ∥W∥ 2 is the spectral norm of the weight matrix.\n\nThrough the above steps, the LGCBE component effectively enhances signals and extracts key features while ensuring model stability and generalization through Lipschitz continuity constraints.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Lipschitz Gradient-Constrained Attention Component (Lgca)",
      "text": "Lipschitz continuity regularization enhances neural networks' robustness and generalization ability while stabilizing the training process  [43, 44] . Inspired by these studies, we designed an LGCA module to enhance the model's stability, robustness, and generalization ability by restricting its Lipschitz constant  [45] . The implementation of LGCA is shown in Figure  1c .\n\nGiven an input tensor x ∈ R B×T ×D , where B is the batch size, T is the sequence length, and D is the embedding dimension, the conventional attention mechanism is implemented through the following steps:\n\nThe input tensor x is mapped to the Query, Key, and Value spaces through linear transformations:\n\nwhere W Q , W K , W V are the weight matrices, and b Q , b K , b V are the bias vectors.\n\nThe dot product is computed between the query and key, and scaled by the factor √ d h to prevent the dot product results from becoming too large, which can be expressed as:\n\nwhere S denotes the scaled dot-product scores and d h is the dimension of each attention head.\n\nThe attention scores S are normalized to a probability distribution using the Softmax function. This normalization ensures that the scores sum up to 1, making them suitable for use as weights. The result of this normalization is referred to as the attention weights P.\n\nThen, the dropout operation is applied to the attention weights P to prevent overfitting. This involves randomly setting a subset of the weights to zero during training, which helps in making the model more robust and generalizable. The resulting attention weights after dropout are denoted as P ′ .\n\nNext, the value tensor V is combined with the attention weights P ′ through weighted summation, and the multi-head results are merged into the final output through the output projection layer. Let Y denote the final output:\n\nwhere W O and b O are the weight matrix and bias vector of the output projection layer, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Lipschitz Gradient-Constrained Normalization Component (Lgcn)",
      "text": "To enhance the stability and generalization capability of models when processing complex EEG signals, we propose the LGCN, which is detailed as follows:\n\nGiven an input tensor X ∈ R B×D , where B denotes the batch size and D denotes the feature dimension, the Lipschitz constant L is used to control the gradient changes after normalization, and ϵ is a small constant to prevent division by zero.\n\nWe first perform the initial normalization of the input tensor by subtracting the mean and dividing by the standard deviation:\n\nwhere µ and σ are the mean and standard deviation of X, respectively.\n\nTo introduce the Lipschitz continuity constraint, we calculate the gradient norm of the normalized tensor:\n\nThen, we compute the Lipschitz constraint factor to ensure that the gradient norm does not exceed the Lipschitz constant:\n\nHere, λ is a scalar used to adjust X norm so that its gradient norm does not exceed L.\n\nFinally, we apply the Lipschitz constraint by adjusting the normalized tensor:\n\nThis step ensures that the gradient changes of X lip are bounded by the Lipschitz constant L, which improves training stability and enhances the model's generalization capability when processing complex EEG signals. The implementation of LGCBE is shown in Figure  1d .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss Function",
      "text": "To effectively evaluate the model's prediction accuracy, cross -entropy is employed as the loss function for analysis. Cross -entropy loss is a crucial tool for quantifying the discrepancy between the model -predicted probabilities and the one -hot encoded labels in multi -class classification tasks. The model -predicted probabilities are represented as a vector ŷ = [ŷ 1 , ŷ2 , . . . , ŷn ], and the one -hot encoded true labels are represented as a vector y = [y 1 , y 2 , . . . , y n ] (with only one element being 1 and the rest 0). The cross -entropy loss function is defined as follows:\n\n4 Experiments",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "To comprehensively evaluate the proposed LEL, we utilized three representative EEG-based emotion recognition datasets.\n\nThe EAV dataset  [46]  provides a multimodal resource for affective computing research, combining synchronized EEG, audio, and video recordings from 42 participants engaged in naturalistic conversational interactions. This dataset captures five distinct emotional states-neutral, anger, happiness, sadness, and calmness-and comprises 8,400 annotated records, establishing itself as a pioneering benchmark in emotion recognition studies. Since the EAV dataset adopts two data collection methods, active and passive, in the experiments, except for experiment Section 4.6, which is used to test the effect of LEL on passive data, the active data of the EAV dataset is used in the other experiments.\n\nThe FACED dataset  [47]  advances EER research by leveraging 32-channel electrophysiological recordings from 123 participants exposed to 28 emotionally evocative video stimuli. This comprehensive resource spans nine discrete emotion categories and is further enriched with subjective self-assessment labels for emotional states, alongside multidimensional affective parameters such as arousal and valence.\n\nThe SEED dataset  [48]  comprises high-resolution 62-channel EEG recordings and synchronized eye movement data acquired from 15 participants exposed to emotionally evocative film clips. These stimuli were meticulously designed to elicit three distinct affective states: positive, negative, and neutral. The experimental protocol involved three sessions per subject, each containing 15 trials.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Preprocessing",
      "text": "We conducted the following preprocessing steps on the raw EEG data: The raw EEG data was resampled to 100Hz to reduce data volume while retaining key features. The resampled data was divided into segments of 500 samples each to extract local features and reduce complexity for subsequent analysis. Finally, the data was partitioned into training and testing sets using stratified sampling to ensure consistent sample distribution, thereby enhancing model training stability and generalization.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "For model training, the following strategies were adopted: The Adam optimizer was used for model training due to its strong convergence and adaptability, which accelerates the training process and improves model learning efficiency and performance. Additionally, ensemble learning was implemented by freezing classifiers within the model and training each individually before combining them using a learnable weight mechanism. This approach improves the model's recognition accuracy, stability, and adaptability.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Validation",
      "text": "To evaluate the validity of the research, the following steps were performed: LEL was evaluated on three datasets: EAV, FACED, and SEED. On the EAV dataset, LEL was compared with results from  [46]  and the EEG performance of AMERL  [15] . On the FACED and SEED datasets, LEL was compared with the original benchmark results. Accuracy (ACC) and F1-score (F1) were used as evaluation metrics. Classification results were visualized using confusion matrices, and model performance was analyzed using Receiver Operating Characteristic (ROC) curves. Additionally, t-SNE visualization was employed to analyze the effects of different training iterations and Lipschitz constants on the model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Validation Process",
      "text": "The validation process in this study includes the following aspects: First, the effects of Lipschitz continuity constraints on model performance were analyzed from three perspectives: training process, ROC curves, and t-SNE visualization. Second, ablation studies were conducted to evaluate the performance of each individual classifier in the LEL framework and compare it with the full model's performance to assess the effectiveness of the ensemble learning strategy. Third, LEL's robustness was validated on passive signals from the EAV dataset. Fourth, LEL's real-time emotion recognition performance was compared with conventional methods. Finally, the relationships between channels corresponding to different emotions were analyzed using chord diagrams to identify unique neural patterns associated with each emotion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "In this part, we conducted a comprehensive and systematic evaluation of LEL on the EAV dataset and compared it with the results reported in  [46]  and those of AMERL (An Multimodal Emotion Recognition Learning)  [15]  significant advantages in multiple evaluation metrics, especially in processing EEG data. This indicates that even without multimodal data, LEL's single EEG signal recognition performance can outperform multimodal approaches.\n\nThe comparative results are presented in Table  1 , and the confusion matrix illustrating LEL's classification performance on the EAV dataset is shown in Figure  2a . On the FACED dataset, we conducted a comprehensive evaluation of LEL and compared it with the original benchmark results from  [47] . The FACED dataset is renowned for its diversity and complexity, encompassing a wide range of emotional states and EEG signal characteristics, which makes evaluation on this dataset particularly challenging. As shown in comparative results are presented in Table  3 , and Figure  2c  intuitively shows the confusion matrix of LEL's classification performance.\n\nExperimental results demonstrate LEL's robust performance across benchmark datasets: on EAV, it achieved 76.43% accuracy and 76.22% F1-score; on FACED, 83.00% accuracy; on SEED, 87.22% accuracy and 87.06% F1-score. These metrics confirm the framework's efficacy in capturing complex emotional features and its reliability for diverse emotion recognition tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effects Of Lipschitz Continuity Constraints",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis Of Training Process",
      "text": "In this study, we introduce Lipschitz continuity constraints into key components of the LEL framework. Lipschitz continuity is a mathematical property that bounds the rate of change of a function, ensuring that small variations in input do not lead to disproportionately large changes in output. In the context of deep learning, enforcing Lipschitz constraints serves to regulate model complexity, mitigate overfitting, and enhance robustness against noise and perturbations-factors that are particularly critical in emotion recognition tasks, where data is inherently noisy, variable, and susceptible to outliers.\n\nBy embedding Lipschitz constraints within the LEL architecture, we effectively controlled the magnitude of weight updates during training. This led to reduced fluctuations and facilitated smoother convergence. As illustrated in Figures  3  and 4 , our experimental results demonstrate that these constraints significantly improve training stability, especially on low-accuracy datasets characterized by noisy signals and complex feature distributions. Such stability is essential not only for optimizing LEL during training but also for ensuring its reliability in real-world applications.  In this context, the Lipschitz constant plays a critical role in regulating model complexity, preventing overfitting, and controlling the degree of adaptation to the training data. For models trained with fewer epochs, selecting an appropriate Lipschitz constant is particularly important. A smaller Lipschitz constant constrains model complexity, enabling the acquisition of stable feature representations during the early training phase (Figure  5a ). This facilitates the learning of fundamental data patterns, preserving a baseline level of emotional feature discrimination. A moderate Lipschitz constant strikes a balance between overfitting prevention and sufficient learning capacity (Figure  5b ). It allows the model to more effectively capture emotional features with a moderate number of training epochs, resulting in steeper ROC curves and higher AUC values. Conversely, a larger Lipschitz constant permits the model to learn more intricate feature representations (Figure  5c ), which can further steepen the ROC curve and increase the AUC. However, this also heightens the model's sensitivity to noise, potentially increasing the risk of overfitting-especially when the number of training epochs is high.\n\nFrom the perspectives of epochs and Lipschitz constant, they jointly influence the training process and final performance of the model. When training starts and epochs are few, choosing a smaller Lipschitz constant helps the model quickly and stably learn basic data features, avoiding overfitting and laying a solid foundation for subsequent training. As training progresses and epochs increase, the Lipschitz constant can be appropriately increased to enable the model to learn more complex feature representations and further enhance classification performance. However, when adjusting the Lipschitz constant, it is crucial to monitor the model's generalization ability to prevent overfitting due to excessive model complexity. Reasonable epoch planning and Lipschitz constant selection allow the model to continuously optimize during training with appropriate learning speed and feature extraction ability. This ultimately leads to better classification results on test data. In emotion classification tasks, which are complex and prone to noise interference, this balance and optimization are especially important.\n\nFrom the figures, it can be observed that the ROC curves under different epochs and Lipschitz constants show the changing trend of model performance. When both epochs and Lipschitz constant are small, the curve is relatively flat, indicating limited learning ability and weak emotional feature distinction at this stage (Figure  5a ). As epochs increase, the curve gradually moves toward the upper left corner, and model classification performance improves significantly (Figure  5d ). At the same time, appropriately increasing the Lipschitz constant can make the curve steeper and further enhance classification performance (Figure  5e  and 5f ). However, when the Lipschitz constant is too large, the curve may fluctuate (Figure  5h  and 5i ), indicating the model is starting to overfit the training data, leading to a decline in generalization ability. Therefore, in practical applications, it is necessary to comprehensively consider the settings of epochs and Lipschitz constant. Through experiments and validation, the optimal combination should be found to achieve the best model performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization Using T-Sne",
      "text": "To analyze and compare how different training iterations and Lipschitz constants affect the model, we conducted experiments on the FACED dataset. This dataset contains nine 9 emotion classes, offering greater complexity and diversity that better demonstrates the experimental results. The dataset covers a wide range of emotions, including joy, gentleness, inspiration, entertainment, anger, disgust, fear, sadness, and neutrality. These are representative of human daily emotions and offer comprehensive data for emotion analysis studies. We changed the training epochs (30, 50, and 100) to see how training times impact model performance. We also used Lipschitz constants (0.1, 1.0, and 10.0) as control variables to study their influence on the model's generalization and feature learning. Lipschitz constants limit model complexity and affect how well the model fits the training data, helping prevent overfitting and improve robustness.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ablation Studies",
      "text": "The LEL proposed in this study employs an integrated learning strategy with four sets of feature extractors, enhancers, and classifiers to process EEG signals independently and enhance feature distinctiveness. A learnable weight mechanism combines these classifiers to boost overall performance. As shown in Table  4 , each individual classifier achieves a relatively high accuracy: Classifier 1 at 69.56%, Classifier 2 at 68.23%, Classifier 3 at 61.01%, and Classifier 4 at 62.10%. When these classifiers are integrated, the average accuracy of the full model rises significantly to 74.25%, with the best -case accuracy reaching 76.43%, and the F1 -score improving to 73.94% and 76.22% respectively. This highlights the integrated strategy's effectiveness in boosting stability, robustness, and reducing misclassification.\n\nFurther analysis reveals that the integrated learning strategy not only mitigates overfitting by aggregating predictions from multiple classifiers but also enhances the model's adaptability to noisy and complex emotional features. This enables the LEL to deliver more reliable outputs in complex emotional recognition tasks. The learnable weight mechanism for classifier integration allows the model to automatically adjust each classifier's weight in the final decision -making process, further enhancing overall performance and ensuring good generalization across different datasets and emotional categories.\n\nIn summary, the integrated learning strategy in the LEL significantly enhances the accuracy and reliability of emotion recognition. The excellent performance of each individual classifier, combined with the optimization from the integrated strategy, offers a novel approach for EEG-based emotion recognition with a promising future outlook. Future work will explore applying this method to more emotion recognition tasks to evaluate its potential and value in diverse scenarios.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Validation On The Eav Passive Dataset",
      "text": "The previous experimental results on the EAV positive dataset provide compelling evidence of the superiority and robustness of the LEL framework. These results demonstrate that LEL consistently delivers stable and reliable performance in emotion recognition tasks, even when applied to diverse signal types. The EAV dataset comprises both active and passive signals, with the latter being particularly challenging due to their high noise levels, low amplitudes, and weak correlations with emotional states.\n\nTo rigorously evaluate LEL's robustness, we applied the model specifically to passive signals. As shown in Figure  7 , LEL maintained high classification accuracy under these adverse conditions, highlighting its strong signal processing stability. Notably, the LEL variant with Lipschitz constraints (Lc -ECL Best) consistently achieved the highest accuracy across most emotion recognition tasks. Even in instances where other models outperformed LEL in specific cases, its performance remained comparably high, with minimal degradation.\n\nThese findings underscore LEL's effectiveness not only in handling passive signals but also in delivering reliable performance across a broad range of emotion recognition scenarios.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Real-Time Validation Test",
      "text": "Real-time validation paradigms demand stricter performance criteria than traditional approaches. While conventional methods use complete contextual information through random sampling to optimize accuracy (Figure  8 ), real-time systems must operate solely on available signals without future context (Figure  9 ). This fundamental limitation necessitates superior generalization capacity and enhanced model robustness.\n\nThe proposed LEL model demonstrates robust real-time emotion recognition, achieving consistently high classification accuracy on EAV, FACED and SEED datasets (Figures 10). These results confirm its stronggeneralization capacity and practical applicability in complex real-world environments. Specifically, LEL's temporal dependency modeling enables effective EEG feature extraction and noise suppression, ensuring stable performance under challenging conditions.    An analysis of the nine emotions in the FACED dataset shows that the channels with high -impact proportions for different emotions vary significantly. To better illustrate the relationships between these channels, chord diagrams are used, where the connection strength between channels is represented by the depth of color in the chords, offering a visual depiction of their interrelations.\n\nThese chord diagrams reveal the unique neural patterns associated with different emotions, highlighting their distinct activity modes in the brain (Figure  11 ). The findings are highly significant for understanding the brain mechanisms of emotion processing and for developing EEG-based emotion recognition systems. By identifying channel combinations",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Discussion",
      "text": "The",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a): Overall framework of the LEL (b): The architecture of LGCBE module (c): The architecture of LGCN",
      "page": 2
    },
    {
      "caption": "Figure 1: b, while LGCBE-E, LGCBE-F, and LGCBE-B",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion Matrices of LEL (Our Model) Across Three Datasets: Left for EAV (a), Center for FACED (b),",
      "page": 8
    },
    {
      "caption": "Figure 2: b visually illustrates LEL’s",
      "page": 8
    },
    {
      "caption": "Figure 2: c intuitively shows the confusion matrix of LEL’s classification",
      "page": 9
    },
    {
      "caption": "Figure 3: EAV Sub2 with different Lipschitz constant.",
      "page": 10
    },
    {
      "caption": "Figure 4: EAV Sub35 with different Lipschitz constant.",
      "page": 10
    },
    {
      "caption": "Figure 5: a). This facilitates the learning",
      "page": 10
    },
    {
      "caption": "Figure 5: b). It allows the",
      "page": 10
    },
    {
      "caption": "Figure 5: c), which can further steepen the ROC curve and increase the AUC. However, this also",
      "page": 10
    },
    {
      "caption": "Figure 5: a). As epochs increase,",
      "page": 10
    },
    {
      "caption": "Figure 5: d). At the same time, appropriately increasing the Lipschitz constant can make the curve steeper and further",
      "page": 10
    },
    {
      "caption": "Figure 5: e and 5f). However, when the Lipschitz constant is too large, the curve",
      "page": 10
    },
    {
      "caption": "Figure 5: The figure presents nine line charts visualizing the ROC curves for emotion classification under different",
      "page": 11
    },
    {
      "caption": "Figure 5: h and 5i), indicating the model is starting to overfit the training data, leading to a decline in",
      "page": 12
    },
    {
      "caption": "Figure 6: f shows the t-SNE visualization of raw EEG signals. The different emotion categories show some clustering",
      "page": 12
    },
    {
      "caption": "Figure 6: a, 6b and 6c show the t-SNE visualization after 30 epochs. The model’s output indicates improved emotion",
      "page": 12
    },
    {
      "caption": "Figure 6: d), clustering",
      "page": 12
    },
    {
      "caption": "Figure 6: e) , the model shows nearly ideal emotion category",
      "page": 12
    },
    {
      "caption": "Figure 6: a shows the t-SNE visualization with a Lipschitz constant of 0.1. The model’s output shows limited emotion",
      "page": 12
    },
    {
      "caption": "Figure 6: b), the model balances complexity and stability. Under moderate training",
      "page": 12
    },
    {
      "caption": "Figure 6: c), the model is permitted to learn more complex feature representations. With fewer training",
      "page": 12
    },
    {
      "caption": "Figure 6: The figure presents six scatter plots visualizing emotion classification results under different training",
      "page": 13
    },
    {
      "caption": "Figure 8: ), real-time",
      "page": 14
    },
    {
      "caption": "Figure 9: ). This fundamental limitation",
      "page": 14
    },
    {
      "caption": "Figure 7: Using passive data experiments to compare with other active data experiments on the EAV dataset.",
      "page": 15
    },
    {
      "caption": "Figure 8: Partitioning with contextual information.",
      "page": 15
    },
    {
      "caption": "Figure 9: Partitioning without contextual information.",
      "page": 15
    },
    {
      "caption": "Figure 10: Real-time validation test on EAV (a), FACED (b) and SEED (c)",
      "page": 15
    },
    {
      "caption": "Figure 11: ). The findings are highly significant for understanding the brain mechanisms of",
      "page": 15
    },
    {
      "caption": "Figure 11: This figure shows nine chord diagrams representing different emotions: Amusement, Anger, Disgust, Fear,",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance Comparison of Different Methods on the EAV Dataset",
      "page": 8
    },
    {
      "caption": "Table 1: , and the confusion matrix illustrating LEL’s classification performance",
      "page": 8
    },
    {
      "caption": "Table 2: , our LEL demonstrates superior performance on the FACED dataset. Specifically, when faced with",
      "page": 8
    },
    {
      "caption": "Table 2: Additionally, Figure 2b visually illustrates LEL’s",
      "page": 8
    },
    {
      "caption": "Table 2: Performance Comparison of Different Methods on the FACED Dataset",
      "page": 9
    },
    {
      "caption": "Table 3: Performance Comparison of Different Methods on the SEED Dataset",
      "page": 9
    },
    {
      "caption": "Table 3: , and Figure 2c intuitively shows the confusion matrix of LEL’s classification",
      "page": 9
    },
    {
      "caption": "Table 4: , each individual classifier achieves a",
      "page": 14
    },
    {
      "caption": "Table 4: The results of ablation studies on the EAV positive dataset.",
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human emotion recognition from eeg-based brain-computer interface using machine learning: a comprehensive review",
      "authors": [
        "Asmaa Essam H Houssein",
        "Abdelmgeid A Hammad",
        "Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "2",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "3",
      "title": "Autism and depression: clinical presentation, evaluation and treatment",
      "authors": [
        "Amaia Hervás"
      ],
      "year": "2023",
      "venue": "Autism and depression: clinical presentation, evaluation and treatment"
    },
    {
      "citation_id": "4",
      "title": "Information bottleneck-guided heterogeneous graph learning for interpretable neurodevelopmental disorder diagnosis",
      "authors": [
        "Yueyang Li",
        "Lei Chen",
        "Wenhao Dong",
        "Shengyu Gong",
        "Zijian Kang",
        "Boyang Wei",
        "Weiming Zeng",
        "Hongjie Yan",
        "Lingbin Bian",
        "Wai Ting Siok"
      ],
      "year": "2025",
      "venue": "Information bottleneck-guided heterogeneous graph learning for interpretable neurodevelopmental disorder diagnosis",
      "arxiv": "arXiv:2502.20769"
    },
    {
      "citation_id": "5",
      "title": "Mhnet: Multi-view high-order network for diagnosing neurodevelopmental disorders using resting-state fmri",
      "authors": [
        "Yueyang Li",
        "Weiming Zeng",
        "Wenhao Dong",
        "Luhui Cai",
        "Lei Wang",
        "Hongyu Chen",
        "Hongjie Yan",
        "Lingbin Bian",
        "Nizhuan Wang"
      ],
      "year": "2025",
      "venue": "Journal of imaging informatics in medicine"
    },
    {
      "citation_id": "6",
      "title": "Neural modulation alteration to positive and negative emotions in depressed patients: Insights from fmri using positive/negative emotion atlas",
      "authors": [
        "Yu Feng",
        "Weiming Zeng",
        "Yifan Xie",
        "Hongyu Chen",
        "Lei Wang",
        "Yingying Wang",
        "Hongjie Yan",
        "Kaile Zhang",
        "Ran Tao",
        "Wai Ting Siok"
      ],
      "year": "2024",
      "venue": "Tomography"
    },
    {
      "citation_id": "7",
      "title": "Emotion awareness and regulation in individuals with schizophrenia: Implications for social functioning",
      "authors": [
        "David Kimhy",
        "Julia Vakhrusheva",
        "Lauren Jobson-Ahmed",
        "Nicholas Tarrier",
        "Dolores Malaspina",
        "James Gross"
      ],
      "year": "2012",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "8",
      "title": "Stanet: A novel spatio-temporal aggregation network for depression classification with small and unbalanced fmri data",
      "authors": [
        "Wei Zhang",
        "Weiming Zeng",
        "Hongyu Chen",
        "Jie Liu",
        "Hongjie Yan",
        "Kaile Zhang",
        "Ran Tao",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2024",
      "venue": "Tomography"
    },
    {
      "citation_id": "9",
      "title": "Emotional dysregulation is a primary symptom in adult attention-deficit/hyperactivity disorder (adhd)",
      "authors": [
        "Oliver Hirsch",
        "Miralynn Chavanon",
        "Elke Riechmann",
        "Hanna Christiansen"
      ],
      "year": "2018",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "10",
      "title": "Emotion dysregulation is substantially elevated in autism compared to the general population: Impact on psychiatric services",
      "authors": [
        "Caitlin Conner",
        "Josh Golt",
        "Rebecca Shaffer",
        "Giulia Righi",
        "Matthew Siegel",
        "Carla Mazefsky"
      ],
      "year": "2021",
      "venue": "Autism Research"
    },
    {
      "citation_id": "11",
      "title": "Examining emotion regulation in depression: A review and future directions",
      "authors": [
        "Jutta Joormann",
        "Colin Stanton"
      ],
      "year": "2016",
      "venue": "Behaviour research and therapy"
    },
    {
      "citation_id": "12",
      "title": "Mm-gtunets: Unified multi-modal graph deep learning for brain disorders prediction",
      "authors": [
        "Luhui Cai",
        "Weiming Zeng",
        "Hongyu Chen",
        "Hua Zhang",
        "Yueyang Li",
        "Yu Feng",
        "Hongjie Yan",
        "Lingbin Bian",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "13",
      "title": "Starformer: A novel spatio-temporal aggregation reorganization transformer of fmri for brain disorder diagnosis",
      "authors": [
        "Wenhao Dong",
        "Yueyang Li",
        "Weiming Zeng",
        "Lei Chen",
        "Hongjie Yan",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2025",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Hypergraph multi-modal learning for eeg-based emotion recognition in conversation",
      "authors": [
        "Zijian Kang",
        "Yueyang Li",
        "Shengyu Gong",
        "Weiming Zeng",
        "Hongjie Yan",
        "Lingbin Bian",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2025",
      "venue": "Hypergraph multi-modal learning for eeg-based emotion recognition in conversation",
      "arxiv": "arXiv:2502.21154"
    },
    {
      "citation_id": "15",
      "title": "Eeg-based multimodal representation learning for emotion recognition",
      "authors": [
        "Hye-Bin Kang Yin",
        "Dan Shin",
        "Seong-Whan Li",
        "Lee"
      ],
      "year": "2025",
      "venue": "2025 13th International Conference on Brain-Computer Interface (BCI)"
    },
    {
      "citation_id": "16",
      "title": "A review on eeg-based multimodal learning for emotion recognition",
      "authors": [
        "Rajasekhar Pillalamarri",
        "Udhayakumar Shanmugam"
      ],
      "year": "2025",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "17",
      "title": "A tale of single-channel electroencephalography: Devices, datasets, signal processing, applications, and future directions",
      "authors": [
        "Yueyang Li",
        "Weiming Zeng",
        "Wenhao Dong",
        "Di Han",
        "Lei Chen",
        "Hongyu Chen",
        "Zijian Kang",
        "Shengyu Gong",
        "Hongjie Yan",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "18",
      "title": "Research on two-dimensional convolution-based eeg emotion recognition methods",
      "authors": [
        "Hongna Li",
        "Yue Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 International Conference on Biomedicine and Intelligent Technology"
    },
    {
      "citation_id": "19",
      "title": "Neural-mcrl: Neural multimodal contrastive representation learning for eeg-based visual decoding",
      "authors": [
        "Yueyang Li",
        "Zijian Kang",
        "Shengyu Gong",
        "Wenhao Dong",
        "Weiming Zeng",
        "Hongjie Yan",
        "Wai Ting Siok",
        "Nizhuan Wang"
      ],
      "year": "2024",
      "venue": "Neural-mcrl: Neural multimodal contrastive representation learning for eeg-based visual decoding",
      "arxiv": "arXiv:2412.17337"
    },
    {
      "citation_id": "20",
      "title": "A case-based channel selection method for eeg emotion recognition using interpretable transformer networks",
      "authors": [
        "Yang Du",
        "Zijing Guan",
        "Weichen Huang",
        "Xichun Zhang",
        "Qiyun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology, ICCVIT '23"
    },
    {
      "citation_id": "21",
      "title": "Freqdgt: Frequencyadaptive dynamic graph networks with transformer for cross-subject eeg emotion recognition",
      "authors": [
        "Yueyang Li",
        "Shengyu Gong",
        "Weiming Zeng",
        "Nizhuan Wang",
        "Wai Ting"
      ],
      "year": "2025",
      "venue": "Freqdgt: Frequencyadaptive dynamic graph networks with transformer for cross-subject eeg emotion recognition",
      "arxiv": "arXiv:2506.22807"
    },
    {
      "citation_id": "22",
      "title": "Eeg emotion copilot: Optimizing lightweight llms for emotional eeg interpretation with assisted medical record generation",
      "authors": [
        "Hongyu Chen",
        "Weiming Zeng",
        "Chengcheng Chen",
        "Luhui Cai",
        "Fei Wang",
        "Yuhu Shi",
        "Lei Wang",
        "Wei Zhang",
        "Yueyang Li",
        "Hongjie Yan"
      ],
      "year": "2025",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "23",
      "title": "Spatial-temporal transformers for eeg emotion recognition",
      "authors": [
        "Jiyao Liu",
        "Hao Wu",
        "Li Zhang",
        "Yanxi Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 6th International Conference on Advances in Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Deep learning-based eeg emotion recognition: a comprehensive review",
      "authors": [
        "Yuxiao Geng",
        "Shuo Shi",
        "Xiaoke Hao"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "25",
      "title": "Adaptive construction of critical brain functional networks for eeg-based emotion recognition. Signal, Image and Video Processing",
      "authors": [
        "Zhao Ying",
        "He Hong",
        "Bi Xiaoying",
        "Lu Yue"
      ],
      "year": "2025",
      "venue": "Adaptive construction of critical brain functional networks for eeg-based emotion recognition. Signal, Image and Video Processing"
    },
    {
      "citation_id": "26",
      "title": "Research progress of eeg-based emotion recognition: a survey",
      "authors": [
        "Yiming Wang",
        "Bin Zhang",
        "Lamei Di"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "27",
      "title": "Towards practical emotion recognition: An unsupervised source-free approach for eeg domain adaptation",
      "authors": [
        "Md Niaz",
        "Naimul Khan"
      ],
      "year": "2025",
      "venue": "Towards practical emotion recognition: An unsupervised source-free approach for eeg domain adaptation",
      "arxiv": "arXiv:2504.03707"
    },
    {
      "citation_id": "28",
      "title": "Electroencephalography signal processing: A comprehensive review and analysis of methods and techniques",
      "authors": [
        "Ahmad Chaddad",
        "Yihang Wu",
        "Reem Kateb",
        "Ahmed Bouridane"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Mini review: Challenges in eeg emotion recognition",
      "authors": [
        "Zhihui Zhang",
        "Josep Fort",
        "Lluis Giménez"
      ],
      "year": "2024",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "30",
      "title": "Current status, challenges, and possible solutions of eeg-based brain-computer interface: a comprehensive review",
      "authors": [
        "Mamunur Rashid",
        "Norizam Sulaiman",
        "Abdul Anwar",
        "Rabiu Majeed",
        "Ahmad Musa",
        "Fakhri Ab",
        "Bifta Nasir",
        "Bari Sama",
        "Sabira Khatun"
      ],
      "year": "2020",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "31",
      "title": "Research on gcn-lstm emotion recognition algorithm with attention mechanism based on eeg",
      "authors": [
        "Lina Chang",
        "Qi Li",
        "Xurong Yan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing, BIC '24"
    },
    {
      "citation_id": "32",
      "title": "Enhancing node-level adversarial defenses by lipschitz regularization of graph neural networks",
      "authors": [
        "Yaning Jia",
        "Dongmian Zou",
        "Hongfei Wang",
        "Hai Jin"
      ],
      "year": "2023",
      "venue": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "33",
      "title": "Aligning relational learning with lipschitz fairness",
      "authors": [
        "Yaning Jia",
        "Chunhui Zhang",
        "Soroush Vosoughi"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "Eeg-based emotional valence and emotion regulation classification: a data-centric and explainable approach",
      "authors": [
        "Linda Fiorini",
        "Francesco Bossi",
        "Francesco Gruttola"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "35",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Mitigating transformer overconfidence via lipschitz regularization",
      "authors": [
        "Wenqian Ye",
        "Yunsheng Ma",
        "Xu Cao",
        "Kun Tang"
      ],
      "year": "2023",
      "venue": "Uncertainty in Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Lipschitz normalization for self-attention layers with application to graph neural networks",
      "authors": [
        "George Dasoulas",
        "Kevin Scaman",
        "Aladin Virmaux"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Lipschitz continuous algorithms for covering problems",
      "authors": [
        "Soh Kumabe",
        "Yuichi Yoshida"
      ],
      "year": "2025",
      "venue": "Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)"
    },
    {
      "citation_id": "40",
      "title": "Data-driven lipschitz continuity: A costeffective approach to improve adversarial robustness",
      "authors": [
        "Erh-Chung Chen",
        "Pin-Yu Chen",
        "I-Hsin Chung",
        "Che-Rung Lee"
      ],
      "year": "2024",
      "venue": "Data-driven lipschitz continuity: A costeffective approach to improve adversarial robustness"
    },
    {
      "citation_id": "41",
      "title": "Some fundamental aspects about lipschitz continuity of neural network functions",
      "authors": [
        "Grigory Khromov",
        "Sidak Pal Singh"
      ],
      "year": "2024",
      "venue": "ICLR 2024"
    },
    {
      "citation_id": "42",
      "title": "Multiplayer information asymmetric bandits in metric spaces",
      "authors": [
        "William Chang",
        "Aditi Karthik"
      ],
      "year": "2025",
      "venue": "Multiplayer information asymmetric bandits in metric spaces",
      "arxiv": "arXiv:2503.08004"
    },
    {
      "citation_id": "43",
      "title": "",
      "authors": [
        "Martin Arjovsky",
        "Soumith Chintala",
        "Léon Bottou"
      ],
      "year": "2017",
      "venue": "",
      "arxiv": "arXiv:1701.07875"
    },
    {
      "citation_id": "44",
      "title": "Fast gaussian process based gradient matching for parameter identification in systems of nonlinear odes",
      "authors": [
        "Philippe Wenk",
        "Alkis Gotovos",
        "Stefan Bauer",
        "Nico Gorbach",
        "Andreas Krause",
        "Joachim Buhmann"
      ],
      "year": "2019",
      "venue": "The 22nd International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "45",
      "title": "Improving robustness of hyperbolic neural networks by lipschitz analysis",
      "authors": [
        "Yuekang Li",
        "Yidan Mao",
        "Yifei Yang",
        "Dongmian Zou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "46",
      "title": "Eav: Eeg-audio-video dataset for emotion recognition in conversational contexts",
      "authors": [
        "Min-Ho Lee",
        "Adai Shomanov",
        "Balgyn Begim",
        "Zhuldyz Kabidenova",
        "Aruna Nyssanbay",
        "Adnan Yazici",
        "Seong-Whan Lee"
      ],
      "year": "2024",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "47",
      "title": "A large finer-grained affective computing eeg dataset. Scientific Data",
      "authors": [
        "Jingjing Chen",
        "Xiaobin Wang",
        "Chen Huang",
        "Xin Hu",
        "Xinke Shen",
        "Dan Zhang"
      ],
      "venue": "A large finer-grained affective computing eeg dataset. Scientific Data"
    },
    {
      "citation_id": "48",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "49",
      "title": "Accnet: Adaptive cross-frequency coupling graph attention for eeg emotion recognition",
      "authors": [
        "Dongyuan Tian",
        "Yucheng Wang",
        "Peiliang Gong",
        "Zhewen Xu",
        "Zhenghua Chen",
        "Xiaohui Wei",
        "Min Wu"
      ],
      "year": "2025",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "50",
      "title": "Fully-connected spatial-temporal graph for multivariate time-series data",
      "authors": [
        "Yucheng Wang",
        "Yuecong Xu",
        "Jianfei Yang",
        "Min Wu",
        "Xiaoli Li",
        "Lihua Xie",
        "Zhenghua Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "51",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "52",
      "title": "Lggnet: Learning from localglobal-graph representations for brain-computer interface",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Chengxuan Tong",
        "Qiuhao Zeng",
        "Cuntai Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "53",
      "title": "An efficient graph learning system for emotion recognition inspired by the cognitive prior graph of eeg brain network",
      "authors": [
        "Cunbo Li",
        "Tian Tang",
        "Yue Pan",
        "Lei Yang",
        "Shuhan Zhang",
        "Zhaojin Chen",
        "Peiyang Li",
        "Dongrui Gao",
        "Huafu Chen",
        "Fali Li",
        "Dezhong Yao",
        "Zehong Cao",
        "Peng Xu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "54",
      "title": "A multi-view cnn with novel variance layer for motor imagery brain computer interface",
      "authors": [
        "Ravikiran Mane",
        "A Neethu Robinson",
        "Seong-Whan Vinod",
        "Cuntai Lee",
        "Guan"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    }
  ]
}