{
  "paper_id": "2302.13661v1",
  "title": "Using Auxiliary Tasks In Multimodal Fusion Of Wav2Vec 2.0 And Bert For Multimodal Emotion Recognition",
  "published": "2023-02-27T10:59:08Z",
  "authors": [
    "Dekai Sun",
    "Yancheng He",
    "Jiqing Han"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "BERT",
    "Wav2vec2.0",
    "Cross-attention",
    "Auxiliary task"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The lack of data and the difficulty of multimodal fusion have always been challenges for multimodal emotion recognition (MER). In this paper, we propose to use pretrained models as upstream network, wav2vec 2.0 for audio modality and BERT for text modality, and finetune them in downstream task of MER to cope with the lack of data. For the difficulty of multimodal fusion, we use a K-layer multi-head attention mechanism as a downstream fusion module. Starting from the MER task itself, we design two auxiliary tasks to alleviate the insufficient fusion between modalities and guide the network to capture and align emotion-related features. Compared to the previous state-of-the-art models, we achieve a better performance by 78.42% Weighted Accuracy (WA) and 79.71% Unweighted Accuracy (UA) on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition is a significant capability in human-machine interaction and has attracted widespread attention in industry and academia. As we all know that emotions are expressed in extremely complex and ambiguous ways, perhaps through linguistic content, speech intonation, facial expression and body actions. There have been many related studies on text emotion recognition  [1, 2] , and also on audio emotion recognition  [3, 4, 5] . However, by observing these results, the research on single modality has reached a certain bottleneck which leads to increasing attention devoted to the use of multimodal approach. Some studies propose that the information of different modalities is often complementary and verified, and the full use of the information of different modalities can help the model to better learn the key content  [6, 7] .\n\nIn recent years, pretrained self-supervised learning has performed prominently in several research fields such as natural language processing (NLP)  [8]  and automatic speech recognition (ASR)  [9] . For the multimodal emotion recognition (MER) task, there are also studies that have done a lot of exploration on the basis of pretrained models. For the first time, Siriwardhana et al.  [5]  jointly finetuned modalityspecific \"BERT-like\" pretrained Self Supervised Learning (SSL) architectures to represent both audio and text modalities for the task of MER. Similarly, Yang et al.  [10]  also proposed to finetune two pretrained self-supervised learning models (Text-RoBERTa and Speech-RoBERTa) for MER. Based on pretrained models, Zhao et al.  [11]  explored Multilevel fusion approaches, including coattention-based early fusion and late fusion with the models trained on both embeddings. Compared with the MCSAN  [12]  using traditional features (MFCC & GloVe) for modal fusion, the works mentioned above have greatly improved performance. From the perspective of making full use of contextual data, Wu et al.  [13]  took advantage of contextual information and proposed a two-branch neural network structure including time synchronous branch and time asynchronous branch. By modifying the structure of network, SMCN  [14]  realize multi-modal alignment which can capture the global connections without interfering with unimodal learning. However, these previous works focused more on sophisticated fusion structure design and the use of larger and stronger pretrained models, or the use of contextual information that breaks data constraints. They did not start from the MER task itself to explore the bottleneck of insufficient fusion, or capture the feature of emotion itself and the alignment of emotion in different modalities. We believe that the parameters of the network are already sufficient, and the complex fusion module design has not brought enough benefits. Thus, we hope to guide the model to fully exploit the potential of the fusion module by designing just the right auxiliary tasks.\n\nIn this work, we propose a modular end-to-end approach for the MER task. The general framework is shown in figure 1. First, we learn the semantic information of the respective modalities through the pretrained models, wav2vec 2.0  [9]  for audio modality and BERT  [8]  for text modality. Then, we map text and audio modal feature information into a unified semantic vector space through a k-layer cross-attention mechanism for more adequate modal fusion. Furthermore, we design two auxiliary tasks to help fully fuse the features of the two modalities and learn the alignment information of the emotion itself between different modalities. In the first one, we randomly recombine text and audio modalities and let the model to predict the combination of the two modali-Fig.  1 . Our framework ties through the vector obtained by fusion. This decoupling of multimodal data enables the model to see more complex input combinations, and the constraint of this auxiliary task forces the network to not ignore the role of any modality in the task of MER. In the second one, we randomly replace one of the modalities with other data of the same emotion category, and hope that the model can capture the feature related to emotion and the alignment information beyond the content itself.\n\nWe comprehensively evaluated the performance of the model proposed on the IEMOCAP dataset in terms of average weighted accuracy (WA) and unweighted accuracy (UA). In additional, we compared it with the SOTA methods and presented relevant ablation experiments that illustrate the effectiveness of each module.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "The framework of our proposed model is showned in Figure  1 , which consists of three modules, i.e., text encoder, audio encoder, and fusion module.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text Encoder",
      "text": "The emergence of BERT has brought NLP into a new era, and gradually refreshed the effect of multiple NLP domain tasks. And \"Pretrain + Finetune\" has gradually become a new paradigm. Pre-training models such as BERT can be used to transform text into word vectors with contextual semantic in-formation. In this paper, we choose bert-base-uncased  1  as the text modal encoder, which consists 12 layers of transformer encoder. It converts the text into 768-dimensional vectors, which are fed into the fusion module. During training, we also finetune its weights to make it more suitable for our multimodal emotion recognition task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio Encoder",
      "text": "We choose wav2vec2-base 2  as the audio modality encoder, which consists of feature encoder, contextualized representations with Transformers, and quantization module. The base model contains 12 transformer blocks, and it is pretrained in Librispeech corpus containing 960 hours of 16kHz speech. It is able to learn 768-dimensional latent representation directly from raw audio every 20ms (16Khz sampling rate). We also finetune its parameters during training similar to BERT.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion Module",
      "text": "The fusion module is based on the multi-head cross attention mechanism  [15] . In addition, two auxiliary tasks (Section 2.4) help the model to better handle the feature relationship between the two modalities. Figure  3  shows the specific details of the fusion module, and each layer of the fusion module consists of two branches, which have the same structure but different Q, K, and V. In addition, we use residual linking to reduce the loss of information of the original modalities. The calculation process of multi-head cross attention is as follows:\n\nwhere subscript a represents audio modality and subscript t represents text modality. d Ka and d Kt represent dimension of the embeddings. F t : (B, T t , C) is the text feature outputed by BERT, and F a : (B, T a , C) is the audio feature outputed by Wav2vec 2.0. Q a , K a , V a are given here (same of\n\nFinally, we average pooling F a and F t in the time dimension, and concatenate them in the feature dimension to obtain the fusion embedding (B, 2C), which is sent to the classifier to get the emotion category.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Auxiliary Tasks",
      "text": "In order to help the model fully fuse the features of the two modalities and learn the alignment information of the emotion itself between different modalities, we design two auxiliary modal interaction tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Auxiliary Task1",
      "text": "In MER tasks, audio and text have the same semantics. In the modal fusion of the downstream network, we analyze that the reason for insufficient fusion comes from the fact that the overall emotional orientation can be obtained just from the information of one modality. In some cases, this approach leads to the right results. But for complex cases, we want the network to be more \"humble\", making full use of the information of the two modalities. As shown in Figure  2 , we decouple the pairs of {Audio, Text} in a batch of data, and then randomly scramble and recombine them to get Aux batch1.\n\nDuring the training process, we not only let the model predict the emotion category of the original data pair, but also predict the combined category of this reorganized data pair {Audio, Text} (a total of emotion num × emotion num kinds), and its label (label new ) is defined as follows:\n\nThe main task MER requires the downstream network to receive the features from the two modalities and output the emotion category, while the auxiliary task 1 requires the downstream network to predict not only the emotion but also the combination of the two modalities according to the fusion embedding. It forces the downstream network to not ignore any modal information during the feature fusion process of the two modalities, that is, both modal information contributes to the final fusion embedding.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Auxiliary Task2",
      "text": "In order to guide the fusion network to learn the alignment information of emotion itself between different modalities, we break the strong semantic correlation between modalities. As shown in Figure  2 , for the pairs of {Audio, Text} in a batch of data, we randomly replace one of the modalities (Audio or Text) with other data of the same emotion category. In Aux batch2, different modalities have same emotional label but different semantics. We hope that the fusion network can focus on the features of emotion itself in different modalities and align them. At the same time, the model can better learn common features of the same emotion category.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The dataset used in the experiment is the Interactive Emotional Dyadic Motion Cap-ture (IEMOCAP)  [16] , which is a dialogues dataset and performs improvised and scripts by 10 actors. The 10 actors are divided into 5 sessions, and every session consists of 1 male and 1 female. There are a total of 7529 utterances in IEMOCAP (happy 595, excited 1,041, angry 1,103, sad 1,084, neutral 1,708, frustra-tion 1,849, fear 40, surprise 107, disgust 2). To be consistent and compare with previous studies  [17] , only utterances with ground truth labels belonging to \"angry\", \"happy\", \"excited\", \"sad\", and \"neutral\" were used. The \"excited\" class was merged with \"happy\" to better balance the size of each emotion class, which results in a total of 5,531 utterances (happy 1,636, angry 1,103, sad 1,084, neutral 1,708).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "In order to fully evaluate our proposed model and maintain the same test conditions as previous studies  [13] , a leave-onesession-out 5-fold cross-validation (CV) configuration was implemented to evaluate our model. We divide IEMOCAP into five folds according to sessions in our experiments. At each fold we keep one session for testing, and other sessions are used for training. Therefore, for each fold we can get one result, and we take the average of the results as the final result of our experiments. We implement our model within the PyTorch framework and select the AdamW  [18]  optimizer for model optimization with a learning rate of 1 × 10 -5 , where cross attention had 8 heads.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  1  shows the performance of our method on audioonly, text-only, and multimodal (audio and text) emotion recognition tasks. Compared with a single modality, we simply concatenate the features of the two modalities and feed them into a downstream network constructed with a fully connected layer (FC), which improves the performance by about 6%. Further, we use the single-layer (K=1) multi-head cross-attention downstream network in Figure  3  for modality fusion, which achieves WA : 77.19%, UA : 78.47%. In the current state, we also verify the gains of Auxiliary Task 1 and Auxiliary Task 2, of which Auxiliary Task 2 has the better performance. We also try to use both auxiliary tasks with performance WA : 78.34%, UA : 79.59%.\n\nTable  2  shows that when both auxiliary tasks are used simultaneously, the effect of multi-head cross-attention layer K on the performance of emotion recognition task. When K is 2, we get the best performance WA : 78.42%, UA : 79.71%. We found that with the introduction of auxiliary tasks, the overall training objective of the model became difficult to achieve. By appropriately increasing the number of layers in the downstream network, we could obtain better performance. However, due to the limited size of the IEMOCAP dataset, continuously increasing the number of network layers will make it difficult to fully train the network parameters, resulting in performance degradation. The performance of previous stateof-the-art multimodal models is mentioned in Table  3 , and our proposed method has better performance than previous works.   [10]  77.70 78.50 BERT + FBK  [13]  77.57 78.41 SMCN  [14]  75.60 77.60 BERT + FBK  [19]  70.56 71.46 MCSAN  [12]  61.20 56.00 Our proposed (best) 78.42 79.71",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose to use wav2vec 2.0 and BERT as upstream network and K-layer downstream network based on multi-head cross-attention mechanism for multimodal emotion recognition task. In addition, we design two auxiliary tasks for the model to help the audio and text be fully integrated, and capture and align the features of emotion itself in different modalities. Finally our method outperforms the previous work on the 5-fold CV result of IEMOCAP, achieved the state-of-the-art, WA : 78.42%, UA : 79.71%.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our framework",
      "page": 2
    },
    {
      "caption": "Figure 1: , which consists of three modules, i.e., text encoder, audio",
      "page": 2
    },
    {
      "caption": "Figure 3: shows the speciﬁc details",
      "page": 2
    },
    {
      "caption": "Figure 2: Auxiliary Batch",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed model structure",
      "page": 3
    },
    {
      "caption": "Figure 2: , for the pairs of {Audio, Text} in a batch",
      "page": 3
    },
    {
      "caption": "Figure 3: for modality",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Harbin Institute of Technology, Harbin, China": "ﬁrst\ntime, Siriwardhana et al.\n[5] jointly ﬁnetuned modality-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "speciﬁc\n“BERT-like”\npretrained Self Supervised Learning"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "(SSL) architectures to represent both audio and text modal-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "ities for\nthe task of MER. Similarly, Yang et al.\n[10] also"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "proposed to ﬁnetune two pretrained self-supervised learning"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "models\n(Text-RoBERTa\nand Speech-RoBERTa)\nfor MER."
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "Based on pretrained models, Zhao et al. [11] explored Multi-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "level\nfusion approaches,\nincluding coattention-based early"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "fusion and late fusion with the models trained on both em-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "beddings. Compared with the MCSAN [12] using traditional"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "features (MFCC & GloVe) for modal fusion, the works men-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "tioned\nabove\nhave\ngreatly\nimproved\nperformance.\nFrom"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "the perspective of making full use of contextual data, Wu"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "et\nal.\n[13]\ntook advantage of\ncontextual\ninformation and"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "proposed a\ntwo-branch neural network structure\nincluding"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "time\nsynchronous\nbranch\nand\ntime\nasynchronous\nbranch."
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "By modifying the structure of network, SMCN [14]\nrealize"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "multi-modal alignment which can capture the global connec-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "tions without\ninterfering with unimodal\nlearning. However,"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "these previous works focused more on sophisticated fusion"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "structure design and the use of larger and stronger pretrained"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "models, or the use of contextual information that breaks data"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "constraints.\nThey did not start\nfrom the MER task itself\nto"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "explore the bottleneck of\ninsufﬁcient\nfusion, or capture the"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "feature of\nemotion itself\nand the\nalignment of\nemotion in"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "different modalities. We believe that\nthe parameters of\nthe"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "network are already sufﬁcient, and the complex fusion mod-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "ule design has not brought enough beneﬁts. Thus, we hope"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "to guide the model to fully exploit the potential of the fusion"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "module by designing just the right auxiliary tasks."
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "In this work, we propose a modular end-to-end approach"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "for\nthe MER task.\nThe general\nframework is shown in ﬁg-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "ure 1. First, we learn the semantic information of the respec-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "tive modalities through the pretrained models, wav2vec 2.0"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "[9] for audio modality and BERT [8] for text modality. Then,"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "we map text and audio modal feature information into a uni-"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "ﬁed semantic vector space through a k-layer cross-attention"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "mechanism for more adequate modal\nfusion.\nFurthermore,"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "we design two auxiliary tasks to help fully fuse the features"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "of the two modalities and learn the alignment\ninformation of"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "the emotion itself between different modalities.\nIn the ﬁrst"
        },
        {
          "Harbin Institute of Technology, Harbin, China": ""
        },
        {
          "Harbin Institute of Technology, Harbin, China": "one, we randomly recombine text and audio modalities and"
        },
        {
          "Harbin Institute of Technology, Harbin, China": "let\nthe model\nto predict\nthe combination of the two modali-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "text modal encoder, which consists 12 layers of transformer"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "encoder.\nIt converts\nthe text\ninto 768-dimensional vectors,"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "which are fed into the fusion module. During training, we"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "also ﬁnetune its weights to make it more suitable for our mul-"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "timodal emotion recognition task."
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "2.2. Audio Encoder"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "We choose wav2vec2-base2 as the audio modality encoder,"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "which consists of feature encoder, contextualized representa-"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "tions with Transformers, and quantization module. The base"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "model contains 12 transformer blocks, and it\nis pretrained in"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "Librispeech corpus containing 960 hours of 16kHz speech. It"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "is able to learn 768-dimensional latent representation directly"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "from raw audio every 20ms (16Khz sampling rate). We also"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "ﬁnetune its parameters during training similar to BERT."
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "2.3. Fusion Module"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "The fusion module is based on the multi-head cross attention"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "mechanism [15]. In addition, two auxiliary tasks (Section 2.4)"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "help the model\nto better handle the feature relationship be-"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "tween the two modalities. Figure 3 shows the speciﬁc details"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "of\nthe fusion module, and each layer of\nthe fusion module"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "consists of two branches, which have the same structure but"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "different Q, K, and V. In addition, we use residual\nlinking to"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "reduce the loss of information of the original modalities. The"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "calculation process of multi-head cross attention is as follows:"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(1)\nFa = Fa + Attentionat(Qa, Kt, Vt)"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "QaK T"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(2)\n)Vt\nAttentionat(Qa, Kt, Vt) = sof tmax("
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(cid:112)dKt"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(3)\nFt = Ft + Attentionta(Qt, Ka, Va)"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "QtK T"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(4)\nAttentionta(Qt, Ka, Va) = sof tmax(\n)Va"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "(cid:112)dKa"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": ""
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "where subscript a represents audio modality and subscript t"
        },
        {
          "formation. In this paper, we choose bert-base-uncased1 as the": "represents text modality. dKa and dKt represent dimension of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. Auxiliary Batch": "2.4. Auxiliary Tasks"
        },
        {
          "Fig. 2. Auxiliary Batch": "In order to help the model fully fuse the features of the two"
        },
        {
          "Fig. 2. Auxiliary Batch": "modalities and learn the alignment information of the emotion"
        },
        {
          "Fig. 2. Auxiliary Batch": "itself between different modalities, we design two auxiliary"
        },
        {
          "Fig. 2. Auxiliary Batch": "modal interaction tasks."
        },
        {
          "Fig. 2. Auxiliary Batch": "2.4.1. Auxiliary Task1"
        },
        {
          "Fig. 2. Auxiliary Batch": "In MER tasks, audio and text have the same semantics.\nIn"
        },
        {
          "Fig. 2. Auxiliary Batch": "the modal fusion of the downstream network, we analyze that"
        },
        {
          "Fig. 2. Auxiliary Batch": "the reason for insufﬁcient fusion comes from the fact that the"
        },
        {
          "Fig. 2. Auxiliary Batch": "overall emotional orientation can be obtained just\nfrom the"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "information of one modality.\nIn some cases,\nthis approach"
        },
        {
          "Fig. 2. Auxiliary Batch": "leads to the right results. But for complex cases, we want the"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "network to be more “humble”, making full use of the infor-"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "mation of the two modalities. As shown in Figure 2, we de-"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "couple the pairs of {Audio, Text} in a batch of data, and then"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "randomly scramble and recombine them to get Aux batch1."
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "During the training process, we not only let the model predict"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "the emotion category of the original data pair, but also predict"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "the combined category of this reorganized data pair {Audio,"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "Text} (a total of emotion num × emotion num kinds), and"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "its label (labelnew) is deﬁned as follows:"
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": ""
        },
        {
          "Fig. 2. Auxiliary Batch": "(8)\nlabeloriginal = labela = labelt"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Weighted Accuracy (WA) and Unweighted Accu-",
      "data": [
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": "and multi modality.(FC - Fully Connected; CA - Multi-Head"
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": "Cross Attention (K=1); Aux1 - Auxiliary Task1; Aux2 - Aux-"
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "iliary Task2.)",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "Methods",
          "the 5-fold CV results using single modality": "WA(%)"
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "Text-only",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "BERT",
          "the 5-fold CV results using single modality": "70.53"
        },
        {
          "racy (UA) of": "Audio-only",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "Wav2vec2",
          "the 5-fold CV results using single modality": "69.92"
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "Audio and Text",
          "the 5-fold CV results using single modality": ""
        },
        {
          "racy (UA) of": "BERT+Wav2vec2+FC",
          "the 5-fold CV results using single modality": "76.24"
        },
        {
          "racy (UA) of": "BERT+Wav2vec2+CA",
          "the 5-fold CV results using single modality": "77.19"
        },
        {
          "racy (UA) of": "BERT+Wav2vec2+CA+Aux1",
          "the 5-fold CV results using single modality": "77.67"
        },
        {
          "racy (UA) of": "BERT+Wav2vec2+CA+Aux2",
          "the 5-fold CV results using single modality": "78.11"
        },
        {
          "racy (UA) of": "BERT+Wav2vec2+CA+Aux1&2",
          "the 5-fold CV results using single modality": "78.34"
        },
        {
          "racy (UA) of": "",
          "the 5-fold CV results using single modality": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Weighted Accuracy (WA) and Unweighted Accu-",
      "data": [
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "tent and compare with previous studies [17], only utterances"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "with ground truth labels belonging to “angry”, “happy”, “ex-"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "cited”, “sad”, and “neutral” were used. The “excited” class"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "was merged with “happy” to better balance the size of each"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "emotion class, which results\nin a total of 5,531 utterances"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "(happy 1,636, angry 1,103, sad 1,084, neutral 1,708)."
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "3.2.\nImplementation Details"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "In order\nto fully evaluate our proposed model and maintain"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "the same test conditions as previous studies [13], a leave-one-"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "session-out 5-fold cross-validation (CV)\nconﬁguration was"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "implemented to evaluate our model. We divide IEMOCAP"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "into ﬁve folds according to sessions in our experiments. At"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "each fold we keep one session for testing, and other sessions"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "are used for training. Therefore, for each fold we can get one"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "result, and we take the average of the results as the ﬁnal result"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "of our experiments."
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "We implement our model within the PyTorch framework"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "and select the AdamW [18] optimizer for model optimization"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "with a learning rate of 1 × 10−5, where cross attention had 8"
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": "heads."
        },
        {
          "tion 1,849,\nfear 40, surprise 107, disgust 2).\nTo be consis-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Weighted Accuracy (WA) and Unweighted Accu-",
      "data": [
        {
          "recognition tasks. Compared with a single modality, we sim-": "ply concatenate the features of"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "them into a downstream network constructed with a"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "connected layer\n(FC), which improves"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "about 6%. Further, we use the single-layer (K=1) multi-head"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "cross-attention downstream network in Figure 3 for modality"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "fusion, which achieves WA : 77.19%, UA : 78.47%."
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "current state, we also verify the gains of Auxiliary Task 1 and"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "Auxiliary Task 2, of which Auxiliary Task 2 has the better"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "performance. We also try to use both auxiliary tasks with"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "performance WA : 78.34%, UA : 79.59%."
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "Table 2 shows that when both auxiliary tasks are used si-"
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": ""
        },
        {
          "recognition tasks. Compared with a single modality, we sim-": "multaneously, the effect of multi-head cross-attention layer K"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Jun Ogata,\n“Exploiting Fine-tuning of Self-supervised"
        },
        {
          "6. REFERENCES": "[1] Acheampong Francisca Adoma, Nunoo-Mensah Henry,",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Learning Models\nfor\nImproving Bi-modal Sentiment"
        },
        {
          "6. REFERENCES": "and Wenyu Chen,\n“Comparative\nanalyses\nof\nbert,",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Inter-\nAnalysis and Emotion Recognition,”\nin Proc."
        },
        {
          "6. REFERENCES": "roberta,\ndistilbert,\nand\nxlnet\nfor\ntext-based\nemotion",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "speech 2022, 2022, pp. 1998–2002."
        },
        {
          "6. REFERENCES": "recognition,” in 2020 17th International Computer Con-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[11] Zihan Zhao, Yanfeng Wang, and Yu Wang, “Multi-level"
        },
        {
          "6. REFERENCES": "ference on Wavelet Active Media Technology and In-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Fusion of Wav2vec 2.0 and BERT for Multimodal Emo-"
        },
        {
          "6. REFERENCES": "formation Processing (ICCWAMTIP).\nIEEE, 2020, pp.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "tion Recognition,” in Proc. Interspeech 2022, 2022, pp."
        },
        {
          "6. REFERENCES": "117–121.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "4725–4729."
        },
        {
          "6. REFERENCES": "[2] Francisca Adoma Acheampong, Henry Nunoo-Mensah,",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[12] Licai Sun, Bin Liu, Jianhua Tao, and Zheng Lian, “Mul-"
        },
        {
          "6. REFERENCES": "and Wenyu Chen,\n“Transformer models for text-based",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "timodal\ncross-and\nself-attention\nnetwork\nfor\nspeech"
        },
        {
          "6. REFERENCES": "emotion detection: a review of bert-based approaches,”",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "emotion recognition,”\nin ICASSP 2021-2021 IEEE In-"
        },
        {
          "6. REFERENCES": "Artiﬁcial Intelligence Review, vol. 54, no. 8, pp. 5789–",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "ternational Conference on Acoustics, Speech and Signal"
        },
        {
          "6. REFERENCES": "5829, 2021.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Processing (ICASSP). IEEE, 2021, pp. 4275–4279."
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[13] Wen Wu, Chao Zhang, and Philip C Woodland,\n“Emo-"
        },
        {
          "6. REFERENCES": "[3] Peipei Shen, Zhou Changjun, and Xiong Chen,\n“Auto-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "tion recognition by fusing time synchronous and time"
        },
        {
          "6. REFERENCES": "matic speech emotion recognition using support vector",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "asynchronous representations,”\nin ICASSP 2021-2021"
        },
        {
          "6. REFERENCES": "machine,” in Proceedings of 2011 international confer-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "IEEE International Conference on Acoustics,\nSpeech"
        },
        {
          "6. REFERENCES": "ence on electronic & mechanical engineering and infor-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "and\nSignal\nProcessing\n(ICASSP).\nIEEE,\n2021,\npp."
        },
        {
          "6. REFERENCES": "mation technology. IEEE, 2011, vol. 2, pp. 621–625.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "6269–6273."
        },
        {
          "6. REFERENCES": "[4] KV Krishna Kishore and P Krishna Satish,\n“Emotion",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[14] Mixiao Hou, Zheng Zhang, and Guangming Lu, “Multi-"
        },
        {
          "6. REFERENCES": "recognition in speech using mfcc and wavelet features,”",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "modal emotion recognition with self-guided modality"
        },
        {
          "6. REFERENCES": "in 2013 3rd IEEE International Advance Computing",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "calibration,”\nin ICASSP 2022-2022 IEEE International"
        },
        {
          "6. REFERENCES": "Conference (IACC). IEEE, 2013, pp. 842–847.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "(ICASSP). IEEE, 2022, pp. 4688–4692."
        },
        {
          "6. REFERENCES": "[5] Shamane\nSiriwardhana,\nAndrew\nReis,\nRivindu",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "Weerasekera,\nand\nSuranga Nanayakkara,\n“Jointly",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[15] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "6. REFERENCES": "ﬁne-tuning” bert-like”\nself\nsupervised models\nto im-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,"
        },
        {
          "6. REFERENCES": "arXiv\nprove multimodal speech emotion recognition,”",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Ad-\nand Illia Polosukhin,\n“Attention is all you need,”"
        },
        {
          "6. REFERENCES": "preprint arXiv:2008.06682, 2020.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "vances\nin neural\ninformation processing systems, vol."
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "30, 2017."
        },
        {
          "6. REFERENCES": "[6] Mohammad\nSoleymani, Maja\nPantic,\nand\nThierry",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[16] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "6. REFERENCES": "Pun,\n“Multimodal emotion recognition in response to",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "6. REFERENCES": "videos,” IEEE transactions on affective computing, vol.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "6. REFERENCES": "3, no. 2, pp. 211–223, 2011.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "“Iemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "database,” Language resources and evaluation, vol. 42,"
        },
        {
          "6. REFERENCES": "[7] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra,",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "no. 4, pp. 335–359, 2008."
        },
        {
          "6. REFERENCES": "Aniket Bera, and Dinesh Manocha, “M3er: Multiplica-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "tive multimodal emotion recognition using facial,\ntex-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[17] Leonardo Pepino,\nPablo Riera,\nand Luciana Ferrer,"
        },
        {
          "6. REFERENCES": "the AAAI\ntual,\nand speech cues,”\nin Proceedings of",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "“Emotion Recognition from Speech Using wav2vec 2.0"
        },
        {
          "6. REFERENCES": "conference on artiﬁcial\nintelligence, 2020, vol. 34, pp.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Embeddings,”\nin Proc.\nInterspeech 2021, 2021, pp."
        },
        {
          "6. REFERENCES": "1359–1367.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "3400–3404."
        },
        {
          "6. REFERENCES": "[8]\nJacob Devlin, Ming-Wei Chang, Kenton\nLee,\nand",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[18]\nIlya\nLoshchilov\nand\nFrank\nHutter,\n“Decoupled"
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "arXiv\npreprint\nweight\ndecay\nregularization,”"
        },
        {
          "6. REFERENCES": "Kristina Toutanova, “Bert: Pre-training of deep bidirec-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "arXiv\ntional\ntransformers for language understanding,”",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "arXiv:1711.05101, 2017."
        },
        {
          "6. REFERENCES": "preprint arXiv:1810.04805, 2018.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "[19] Edmilson Morais, Ron Hoory, Weizhong Zhu, Itai Gat,"
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Matheus Damasceno, and Hagai Aronowitz,\n“Speech"
        },
        {
          "6. REFERENCES": "[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "emotion recognition using self-supervised features,”\nin"
        },
        {
          "6. REFERENCES": "and Michael Auli,\n“wav2vec 2.0: A framework for",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "ICASSP 2022-2022 IEEE International Conference on"
        },
        {
          "6. REFERENCES": "self-supervised learning of speech representations,” Ad-",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP)."
        },
        {
          "6. REFERENCES": "vances in Neural Information Processing Systems, vol.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        },
        {
          "6. REFERENCES": "",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": "IEEE, 2022, pp. 6922–6926."
        },
        {
          "6. REFERENCES": "33, pp. 12449–12460, 2020.",
          "[10] Wei Yang, Satoru Fukayama, Panikos Heracleous, and": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "Francisca Acheampong",
        "Nunoo-Mensah Adoma",
        "Wenyu Henry",
        "Chen"
      ],
      "year": "2020",
      "venue": "2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing"
    },
    {
      "citation_id": "3",
      "title": "Transformer models for text-based emotion detection: a review of bert-based approaches",
      "authors": [
        "Francisca Adoma",
        "Henry Nunoo-Mensah",
        "Wenyu Chen"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "4",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "Peipei Shen",
        "Zhou Changjun",
        "Xiong Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of 2011 international conference on electronic & mechanical engineering and information technology"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in speech using mfcc and wavelet features",
      "authors": [
        "Krishna Kv",
        "Krishna Kishore",
        "Satish"
      ],
      "year": "2013",
      "venue": "2013 3rd IEEE International Advance Computing Conference (IACC)"
    },
    {
      "citation_id": "6",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "7",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "Mohammad Soleymani",
        "Maja Pantic",
        "Thierry Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "10",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Wei Yang",
        "Satoru Fukayama",
        "Panikos Heracleous",
        "Jun Ogata"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
      "authors": [
        "Zihan Zhao",
        "Yanfeng Wang",
        "Yu Wang"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Zheng Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion recognition with self-guided modality calibration",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "18",
      "title": "Emotion Recognition from Speech Using",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "venue": "Emotion Recognition from Speech Using"
    },
    {
      "citation_id": "19",
      "title": "Proc. Interspeech",
      "authors": [
        "Embeddings"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}