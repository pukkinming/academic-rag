{
  "paper_id": "2506.10930v1",
  "title": "Developing A High-Performance Framework For Speech Emotion Recognition In Naturalistic Conditions Challenge For Emotional Attribute Prediction",
  "published": "2025-06-12T17:38:06Z",
  "authors": [
    "Thanathai Lertpetchpun",
    "Tiantian Feng",
    "Dani Byrd",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "speech emotion recognition",
    "speech foundation model",
    "multi-task learning",
    "representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) in naturalistic conditions presents a significant challenge for the speech processing community. Challenges include disagreement in labeling among annotators and imbalanced data distributions. This paper presents a reproducible framework that achieves superior (top 1) performance in the Emotion Recognition in Naturalistic Conditions Challenge (IS25-SER Challenge) -Task 2, evaluated on the MSP-Podcast dataset. Our system is designed to tackle the aforementioned challenges through multimodal learning, multitask learning, and imbalanced data handling. Specifically, our best system is trained by adding text embeddings, predicting gender, and including \"Other\" (O) and \"No Agreement\" (X) samples in the training set. Our system's results secured both first and second places in the IS25-SER Challenge, and the top performance was achieved by a simple two-system ensemble.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent advances in foundation models have opened new frontiers in speech processing, leading to more accurate and robust solutions in automatic speech recognition  [1, 2] , speaker recognition  [3] , and spoken language understanding  [4] . Despite the success of these foundation models, accurately predicting emotions expressed in speech in natural interaction contexts still encounters several challenges, including perception (annotator) subjectivity  [5, 6] , imbalanced distribution in emotion labels, and mismatch in speaker and environment settings  [7] . The Speech Emotion Recognition in Naturalistic Conditions Challenge (IS25-SER Challenge) offers a platform to investigate some of these limitations within a research community \"challenge\" setting.\n\nIn this paper, we focus on Task 2 IS25-SER Challenge 1  [8] . Task 2 involves predicting the level of three-dimensional motion attributes: arousal, valence, and dominance  [9] . Arousal reflects the intensity of the emotion, while valence indicates its polarity. Dominance represents the level of control a person feels in an emotional state. These emotion attributes have proved challenging to approach computationally; this was reflected in the previous Odyssey-Speech Emotion Challenge  [10] , in which none of the participating teams scored higher than 0.5327 in concordance correlation coefficient (CCC)  [11] . Consequently, in this study, we follow that baseline to extract speech embeddings, namely a pre-trained speech model (WavLM Large  [12] ) with a series of linear classification layers.\n\nAside from the architecture choices, we aim for a simple and reproducible system design. Although ensembling a set of trained models is a common practice that yields competitive performance in several domains  [13, 14, 15] , here we demonstrate that a combination of just two systems can achieve promising performance-top-tier in the challenge, outscoring the baselines by a considerable margin. Specifically, our best system reaches an average CCC of 0.6076, 4.8% higher than the baseline. While this improvement may seem small overall, it is 35.43% higher than the first runner-up. While our current system yields state-of-the-art performance, our exploration during the challenge reveals many design considerations that lead to compromises in performance. Therefore, we share both positive findings, and the limitations that remain, as insights for future system design.\n\nWhile previous work by  [16]  suggests that speaker information is beneficial in predicting emotions, most systems in the recent Odyssey-SER challenge do not integrate speaker-related information, such as gender or speaker identity. This motivates us to explore joint modeling of the emotional attributes and speaker information. Specifically, we experiment with speaker representation learning and show that not all speaker-related information is helpful for emotion attribute prediction. Our results suggest that learning gender information improves the model on the SER task, whereas learning speaker representations leads to decreased performance. Hence, we perform a detailed analysis to explain why speaker representation learning fails to improve SER performance in this challenge.\n\nFurthermore, we address the challenge of imbalanced emotion classes in the training dataset by undersampling, so that the model identifies emotion attributes more uniformly across each primary emotion. Our results show that this simple method is effective in improving SER performance. Building upon this, we further investigate the impact of including or excluding speech samples with mixed annotations, labeled as \"Other\" (O) and \"No Agreement\" (X). Most studies choose to discard these labels without an explicit rationale. Instead, our findings indicate that these labels contain valuable information that helps the model better learn emotion attributes.\n\nOur model is at https://github.com/tiantiaf0627/vox-profilerelease. We believe that our system can serve as a new baseline candidate for future SER challenges. The key contributions and findings are summarized below:\n\n• Incorporating text representations alongside speech embeddings significantly improves model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Foundation Models",
      "text": "Foundation models such as WavLM  [12]  and Whisper  [17]  have been shown to improve the performance in SER  [18, 19] . In the previous Odyssey-SER Challenge, the baseline system  [10] , which used the foundation model following downstream MLPs, outperformed every submission. Thus, following the baseline system, our system is built upon speech foundation models, including WavLM and Whisper. For WavLM, we computed the encoder output using a weighted average pooling from all hidden outputs from encoder layers. On the other hand, for Whisper. we used the hidden output from the last layer as the encoder output Specifically, we process the encoder outputs using 1D point-wise convolutions. Furthermore, previous studies find that text embeddings from a pre-trained text model  [19, 20]  and speaker embeddings from a speaker feature extractor  [16]  are beneficial to improve SER. We concatenated the speech embedding, text embedding, and speaker embedding to obtain the final representation. Our proposed framework is illustrated in Figure  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attribute Representation Learning",
      "text": "The final representations are fed into three distinct 2-layer MLPs to predict arousal, valence, and dominance in a regression task. Since the challenge uses the concordance correlation coefficient (CCC) as the evaluation metric  [11] , we apply CCC loss as our learning objective. Moreover, we perform multi-task learning to simultaneously learn emotion attributes alongside speaker-specific information such as gender and speaker identity. To achieve this, we pass the final representation through a separate 2-layer downstream model to predict the gender information, optimizing with cross-entropy loss. Subsequently, we feed the same representation into another downstream model to learn the speaker identity representation. When training the model for speaker prediction, we use additive angular margin loss  [21] , a widely used loss for speaker verification tasks, and exclude the speaker embedding from the final representation. Our final loss function is as Eq. 1.\n\nwhere Lemotion represents summation of the CCC loss for each attribute, L gender indicates the cross-entropy loss between for gender prediction, L speaker represents the AAM-Softmax loss for speaker prediction, and α and β denote weights of each loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "'Other' And 'No Agreement' Label Inclusion",
      "text": "One debated topic in SER is how to handle No Agreement (X) samples and Other (O) samples (we will refer to these two labels as \"O/X\"). No Agreement (X) samples represent speech samples that lack consensus in emotion annotation, while the Other (O) category indicates that the emotion category falls outside the eight primary emotions described in  [10] . Many previous studies simply excluded these two labels from training, development, and test  [22, 23] . However, the speech samples of these two labels account for 22.46% of the training set. Instead of discarding these samples, we include them in the training set and demonstrate their effectiveness in improving SER modeling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Engineering Design Choices To Improve Performance",
      "text": "Despite our efforts to tackle the challenges described in the previous section, many hurdles still need to be addressed. As announced by the organizer, the test set is balanced in primary emotion labels. Therefore, the proportion of each primary emotion in the test set differs substantially from that in the training set. Training the model on the training set can lead the model to overfit to the majority class, such as Neutral. Moreover, validating our model to choose the best model based on the development may not reflect the actual score on the test set due to the distribution mismatch between the development and test set.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Validation Selection",
      "text": "We develop another auxiliary metric to choose our best model. We aim to establish another CCC to reflect on the balanced dataset using existing imbalanced datasets. Specifically, we perform n trials to compute the average CCC in this manner. In each trial, we randomly select k samples from each primary emotion and compute CCC using those samples. Although the metrics may exhibit noise due to randomness in the sample selection, this approach ensures that the CCC is evaluated on the balanced set following the same primary emotion distribution as the test set.\n\nUndersampling We apply the undersampling to ensure that each emotion sample is fed into the model with equal probability. We find m = mini(|emotioni|) where | • | denotes the number of samples in that emotion. Then, we randomly select m samples from each class into the training set.\n\nTarget Normalization We apply the sigmoid function to the model output to map the emotion attribute prediction to the range [0, 1]. These values are subsequently mapped back to the range  [1, 7]  using the min-max normalization. Specifically, the output can be computed as:\n\nwhere xi represents the prediction of each attribute and yij denotes the value of each sample (j) from each attribute (i).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Msp-Podcast Dataset",
      "text": "The MSP-Poscast dataset v1.12  [24]  was used as the dataset for the IS25-SER Challenge. The dataset comprises podcast data from the Internet which are spontaneous speech and contain human emotions expressed in a naturalistic context. The dataset was annotated with different emotion categories and attributes through crowd-sourcing. Moreover, there are at least five annotators per sample to ensure the reliable labeling. However, as annotators sometimes cannot converge on a single emotion, such samples will be labeled as \"No Agreement\" (X) and are excluded from the test set. The dataset contains five subsets: the training set, the development set, and the three unique test sets. In the challenge, we were given the training and development set, while, the test-3 set, for which annotation labels were not made publicly available, was used for test evaluation. The annotators also labeled each sample with regard to three different attributes: arousal (negative to positive), valence (calm to active), and dominance (weak to strong), with ratings on a scale from 1 to 7. Our experiment used the entire training set, consisting of ten emotion classes: Neutral, Happy, Sad, Disgust, Angry, Contempt, Fear, Surprise, Other, and No Agreement. Detailed information about the training and development sets are reported in Table  1  Since the challenge rules allowed the training and validation set to be used arbitrarily, we chose to include some parts of the validation set in the training set to improve the system. Specifically, the samples with label \"Other\" (O) and \"No Agreement\" (X) were included in the training set and excluded from the development set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Details",
      "text": "Although we acknowledge that hyperparameter tuning for each different system can help improve the aggregate models significantly, we did not focus on this aspect for this challenge: -our goal in this work was to design a reproducible and simple stateof-the-art pipeline. Consequently, all the systems were trained with the same hyperparameters. We used a learning rate of 5e-5 and trained the model for a total of 50 epochs. The filter size in the downstream model is 256 across all experiments. The maximum input speech duration is 15 seconds. The best model was selected based on the performance on the validation set. For the multitask learning, we used α = 1 and β = 0.1.\n\nThe systems are trained using the undersampling method as the default method in most experiments. We use WavLM Large and Whisper Large-V3 for the speech foundation model, and RoBERTa-Large as the pre-trained text model. Following the challenge baseline, we fine-tune the pre-trained WavLM Large along with the downstream models. However, we froze the model for the Whisper Large-V3 and RoBERTa-Large. We will add the prefix \"MM (multimodal)\" to the model to indicate that the model uses text embedding as an auxiliary input. As we did not have access to the test set and we can only submit the score to the challenge once a week, it was challenging for us to do a systematic evaluation on the test set. Therefore, the performances except for Table  6  are reported on the validation set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Trained Text Model",
      "text": "First, we investigated whether textual information impacts the SER performance by adding text embedding to the final representation. We designed an experiment to compare WavLM Large and WavLM Large + RoBERTa-Large (MMWavLM). Table  2  shows that the MMWavLM performs better in every attribute prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attributes Learning",
      "text": "As gender prediction has proven effective in classifying emotions  [25] , we investigate whether it can also improve the regression task. Table  3  shows that multi-task learning with gender information increases the CCC scores in arousal, valence, and dominance predictions. However, adding the speaker embedding or predicting the speaker label worsens the model performance. We analyzed the speaker label to understand the reasons behind this. First, approximately 80% of the shared dataset were represented by just 26% of the total speakers, causing the model to be overfit to a limited subset of seen speakers. Second, the pre-trained speaker extractor does not generalize well on the dataset. We randomly selected 100 utterances from 10 speakers and extracted speaker embedding from them. As shown in Fig  2 , speaker embeddings failed to cluster together, making it difficult for the model to learn.   Although O/X samples are not in test sets, adding or removing them substantially impacts performance. Table  5  shows that removing data from the training set, whether it belongs to O or X, degrades performance. The results also show that adding O/X (V) samples to the training set improves the performance. Moreover, adding O/X (V) samples and removing non-O/X (T) samples give the best performance for the SER system. This indicates that, despite the annotators' lack of agreement in classifying emotions for these samples, they still provide valuable information to the SER system. However, due to submission limitations, we did not submit the system trained with the whole training set + O/X (V) -Non-O/X (T) to the leaderboard, even though it achieved the best performance on the validation set. The results from the IS25 SER Challenge leaderboard show that our two systems achieved first and second place in the Challenge. \"+ S Emb\" denotes that the model uses speaker embedding from the pre-trained speaker extractor in the final repre- As shown in Table  6 , MMWavLM performs significantly better than WavLM, indicating that textual embeddings also improve the performance in the test set. Moreover, adding speaker embedding causes the model to overfit on the speaker information, leading to degraded performance, as suggested by our experiments on the validation set.\n\nOur best system consists of only two models: WavLM with gender prediction, undersampling technique and O/X (V) inclusion, and MMWhisper with only undersampling. We ensembled the system by averaging the CCC prediction from each one. The results indicate that ensembling only a few systems can exhibit strong performance. Moreover, we test a 3-system ensemble: WavLM with gender prediction, undersampling technique and O/X (V) inclusion; WavLM with gender prediction and undersampling technique; and MMWhisper with gender prediction, undersampling technique and O/X (V) inclusion. However, this 3-system underperforms the 2-system on the test set. Lastly, it is worth noting that since the number of submissions is limited, it is challenging to demonstrate which aspects of our methods contribute the most to improving scores.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "On Further Improvement",
      "text": "While our current system achieves competitive results in the Challenge, we identify several promising directions for developing next-generation, state-of-the-art SER systems. As suggested by the previous baseline in the Odyssey-SER Challenge  [10] , training on a separate system for each attribute demonstrated strong performance. Moreover, instead of freezing the pre-trained Whisper Large-V3 and RoBERTa-Large, we could apply LoRa as proposed by  [22, 26]  to further improve the performance. We believe that integrating these two methods would not only improve the performance of our system but also maintain its simplicity.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a simple and reproducible state-of-the-art model for the IS25 SER Challenge. Our system consists of the ensemble between pre-trained MMWavLM Large fine-tuned with gender prediction, inclusion of O/X label from both the training and validation sets, and using undersampling, with MMWhisper Large-V3 fine-tuned via undersampling. The ensemble system can gain performance up to 0.6076 in average CCC and achieved 1st place in the Challenge leaderboard. Lastly, our analysis shows why using learning speaker representation does not improve the performance of the SER system in this dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed multimodal SER framework utilizes a",
      "page": 2
    },
    {
      "caption": "Figure 1: 2.2. Attribute Representation Learning",
      "page": 2
    },
    {
      "caption": "Figure 2: , speaker embeddings failed to cluster together, making it",
      "page": 3
    },
    {
      "caption": "Figure 2: The t-sne plot of speaker embeddings from the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lertpetc@usc.edu,": "Abstract",
          "tiantiaf@usc.edu": "Aside\nfrom the\narchitecture\nchoices, we\naim for\na\nsim-"
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "ple and reproducible system design.\nAlthough ensembling a"
        },
        {
          "lertpetc@usc.edu,": "Speech emotion recognition (SER)\nin naturalistic\nconditions",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "set of\ntrained models\nis a common practice that yields com-"
        },
        {
          "lertpetc@usc.edu,": "presents a significant challenge for the speech processing com-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "petitive performance in several domains [13, 14, 15], here we"
        },
        {
          "lertpetc@usc.edu,": "munity. Challenges include disagreement in labeling among an-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "demonstrate that a combination of just two systems can achieve"
        },
        {
          "lertpetc@usc.edu,": "notators and imbalanced data distributions. This paper presents",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "promising performance—top-tier\nin the challenge, outscoring"
        },
        {
          "lertpetc@usc.edu,": "a reproducible framework that achieves\nsuperior\n(top 1) per-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "the baselines by a considerable margin.\nSpecifically, our best"
        },
        {
          "lertpetc@usc.edu,": "formance in the Emotion Recognition in Naturalistic Condi-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "system reaches an average CCC of 0.6076, 4.8% higher\nthan"
        },
        {
          "lertpetc@usc.edu,": "tions Challenge (IS25-SER Challenge)\n- Task 2, evaluated on",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "the baseline. While this improvement may seem small overall,"
        },
        {
          "lertpetc@usc.edu,": "the MSP-Podcast dataset. Our system is designed to tackle the",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "it\nis 35.43% higher than the first runner-up. While our current"
        },
        {
          "lertpetc@usc.edu,": "aforementioned challenges through multimodal learning, multi-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "system yields state-of-the-art performance, our exploration dur-"
        },
        {
          "lertpetc@usc.edu,": "task learning, and imbalanced data handling. Specifically, our",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "ing the challenge reveals many design considerations that\nlead"
        },
        {
          "lertpetc@usc.edu,": "best system is trained by adding text embeddings, predicting",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "to compromises in performance. Therefore, we share both pos-"
        },
        {
          "lertpetc@usc.edu,": "gender, and including “Other” (O) and “No Agreement” (X)",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "itive findings, and the limitations that\nremain, as insights for"
        },
        {
          "lertpetc@usc.edu,": "samples in the training set. Our system’s results secured both",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "future system design."
        },
        {
          "lertpetc@usc.edu,": "first and second places in the IS25-SER Challenge, and the top",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "While previous work by [16] suggests that speaker\ninfor-"
        },
        {
          "lertpetc@usc.edu,": "performance was achieved by a simple two-system ensemble.",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "mation is beneficial in predicting emotions, most systems in the"
        },
        {
          "lertpetc@usc.edu,": "Index Terms:\nspeech emotion recognition, speech foundation",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "recent Odyssey-SER challenge do not integrate speaker-related"
        },
        {
          "lertpetc@usc.edu,": "model, multi-task learning, representation learning",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "information, such as gender or speaker identity. This motivates"
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "us\nto explore joint modeling of\nthe emotional attributes and"
        },
        {
          "lertpetc@usc.edu,": "1.\nIntroduction",
          "tiantiaf@usc.edu": "speaker information. Specifically, we experiment with speaker"
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "representation learning and show that not all\nspeaker-related"
        },
        {
          "lertpetc@usc.edu,": "Recent advances in foundation models have opened new fron-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "information is helpful\nfor emotion attribute prediction.\nOur"
        },
        {
          "lertpetc@usc.edu,": "tiers\nin speech processing,\nleading to more accurate and ro-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "results suggest\nthat\nlearning gender\ninformation improves the"
        },
        {
          "lertpetc@usc.edu,": "bust solutions in automatic speech recognition [1, 2], speaker",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "model on the SER task, whereas learning speaker\nrepresenta-"
        },
        {
          "lertpetc@usc.edu,": "recognition [3], and spoken language understanding [4]. De-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "tions leads to decreased performance. Hence, we perform a de-"
        },
        {
          "lertpetc@usc.edu,": "spite the success of\nthese foundation models, accurately pre-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "tailed analysis to explain why speaker\nrepresentation learning"
        },
        {
          "lertpetc@usc.edu,": "dicting emotions expressed in speech in natural interaction con-",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "fails to improve SER performance in this challenge."
        },
        {
          "lertpetc@usc.edu,": "texts still encounters several challenges,\nincluding perception",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "(annotator) subjectivity [5, 6],\nimbalanced distribution in emo-",
          "tiantiaf@usc.edu": "Furthermore, we address the challenge of imbalanced emo-"
        },
        {
          "lertpetc@usc.edu,": "tion labels, and mismatch in speaker and environment settings",
          "tiantiaf@usc.edu": "tion classes in the training dataset by undersampling,\nso that"
        },
        {
          "lertpetc@usc.edu,": "[7].\nThe Speech Emotion Recognition in Naturalistic Condi-",
          "tiantiaf@usc.edu": "the model\nidentifies emotion attributes more uniformly across"
        },
        {
          "lertpetc@usc.edu,": "tions Challenge (IS25-SER Challenge) offers a platform to in-",
          "tiantiaf@usc.edu": "each primary emotion. Our results show that this simple method"
        },
        {
          "lertpetc@usc.edu,": "vestigate some of these limitations within a research community",
          "tiantiaf@usc.edu": "is\neffective\nin improving SER performance.\nBuilding upon"
        },
        {
          "lertpetc@usc.edu,": "“challenge” setting.",
          "tiantiaf@usc.edu": "this, we further\ninvestigate the impact of\nincluding or exclud-"
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "ing speech samples with mixed annotations, labeled as “Other”"
        },
        {
          "lertpetc@usc.edu,": "In this paper, we focus on Task 2 IS25-SER Challenge 1 [8].",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "(O) and “No Agreement” (X). Most studies choose to discard"
        },
        {
          "lertpetc@usc.edu,": "Task 2 involves predicting the level of three-dimensional motion",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "these labels without an explicit rationale.\nInstead, our findings"
        },
        {
          "lertpetc@usc.edu,": "attributes: arousal, valence, and dominance [9]. Arousal reflects",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "indicate that these labels contain valuable information that helps"
        },
        {
          "lertpetc@usc.edu,": "the intensity of the emotion, while valence indicates its polarity.",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "the model better learn emotion attributes."
        },
        {
          "lertpetc@usc.edu,": "Dominance represents the level of control a person feels in an",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "emotional state. These emotion attributes have proved challeng-",
          "tiantiaf@usc.edu": "Our model is at https://github.com/tiantiaf0627/vox-profile-"
        },
        {
          "lertpetc@usc.edu,": "ing to approach computationally;\nthis was reflected in the pre-",
          "tiantiaf@usc.edu": "release. We believe that our system can serve as a new baseline"
        },
        {
          "lertpetc@usc.edu,": "vious Odyssey-Speech Emotion Challenge [10], in which none",
          "tiantiaf@usc.edu": "candidate for future SER challenges. The key contributions and"
        },
        {
          "lertpetc@usc.edu,": "of the participating teams scored higher than 0.5327 in concor-",
          "tiantiaf@usc.edu": "findings are summarized below:"
        },
        {
          "lertpetc@usc.edu,": "dance correlation coefficient (CCC) [11]. Consequently, in this",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "•\nIncorporating text\nrepresentations alongside speech embed-"
        },
        {
          "lertpetc@usc.edu,": "study, we follow that baseline to extract speech embeddings,",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "dings significantly improves model performance."
        },
        {
          "lertpetc@usc.edu,": "namely a pre-trained speech model (WavLM Large [12]) with a",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "• Leveraging auxiliary labels such as gender information dur-"
        },
        {
          "lertpetc@usc.edu,": "series of linear classification layers.",
          "tiantiaf@usc.edu": ""
        },
        {
          "lertpetc@usc.edu,": "",
          "tiantiaf@usc.edu": "ing training enhances the model’s ability to generalize."
        },
        {
          "lertpetc@usc.edu,": "*These authors contributed equally.",
          "tiantiaf@usc.edu": "• Addressing data imbalance through undersampling improves"
        },
        {
          "lertpetc@usc.edu,": "1https://lab-msp.com/MSP-Podcast Competition/IS2025/",
          "tiantiaf@usc.edu": "model performance, and accelerates training speed."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "exclude the speaker embedding from the final\nrepresentation.": "Our final loss function is as Eq. 1."
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "(1)\nL = Lemotion + αLgender + βLspeaker"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": ""
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "where Lemotion represents summation of the CCC loss for"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "each attribute, Lgender indicates the cross-entropy loss between"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "represents the AAM-Softmax\nfor gender prediction, Lspeaker"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": ""
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "loss for speaker prediction, and α and β denote weights of each"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "loss."
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": ""
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": ""
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "2.3.\n‘Other’ and ‘No Agreement’ Label Inclusion"
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": ""
        },
        {
          "exclude the speaker embedding from the final\nrepresentation.": "One debated topic in SER is how to handle No Agreement (X)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: Comparison of models trained with speaker embed-",
      "data": [
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "beddings and with text embeddings."
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "A\nV\nD\nAverage"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "WavLM\n0.6530\n0.7000\n0.5651\n0.6394"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "MMWavLM\n0.6694\n0.7334\n0.5735\n0.6588"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "Large along with the downstream models. However, we froze"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "the model for the Whisper Large-V3 and RoBERTa-Large. We"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "will add the prefix “MM (multimodal)” to the model to indicate"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "that\nthe model uses text embedding as an auxiliary input. As"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "we did not have access to the test set and we can only submit"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "the score to the challenge once a week,\nit was challenging for"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "us to do a systematic evaluation on the test set. Therefore,\nthe"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "performances except for Table 6 are reported on the validation"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "set."
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "4. Results and Discussion"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "4.1. Pre-trained Text Model"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "First, we investigated whether textual\ninformation impacts the"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "SER performance by adding text embedding to the final\nrep-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "resentation. We designed an experiment\nto compare WavLM"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "Large and WavLM Large + RoBERTa-Large (MMWavLM). Ta-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "ble 2 shows that\nthe MMWavLM performs better\nin every at-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "tribute prediction."
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "4.2. Attributes Learning"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "As gender prediction has proven effective in classifying emo-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "tions [25], we investigate whether\nit can also improve the re-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": "gression task. Table 3 shows that multi-task learning with gen-"
        },
        {
          "text em-\nTable 2: Comparison between models trained without": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Comparison of models trained with speaker embed-",
      "data": [
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": "gression task. Table 3 shows that multi-task learning with gen-"
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": ""
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": "der\ninformation increases the CCC scores in arousal, valence,"
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": "and dominance predictions. However, adding the speaker em-"
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": ""
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": "bedding or predicting the speaker label worsens the model per-"
        },
        {
          "the development set.": "Neutral",
          "tions [25], we investigate whether\nit can also improve the re-": "formance. We analyzed the speaker label to understand the rea-"
        },
        {
          "the development set.": "Happy",
          "tions [25], we investigate whether\nit can also improve the re-": "sons behind this. First, approximately 80% of the shared dataset"
        },
        {
          "the development set.": "Sad",
          "tions [25], we investigate whether\nit can also improve the re-": "were represented by just 26% of the total speakers, causing the"
        },
        {
          "the development set.": "Disgust",
          "tions [25], we investigate whether\nit can also improve the re-": "model to be overfit to a limited subset of seen speakers. Second,"
        },
        {
          "the development set.": "Angry",
          "tions [25], we investigate whether\nit can also improve the re-": "the pre-trained speaker extractor does not generalize well on the"
        },
        {
          "the development set.": "Contempt",
          "tions [25], we investigate whether\nit can also improve the re-": "dataset. We randomly selected 100 utterances from 10 speak-"
        },
        {
          "the development set.": "Fear",
          "tions [25], we investigate whether\nit can also improve the re-": "ers and extracted speaker embedding from them. As shown in"
        },
        {
          "the development set.": "Surprise",
          "tions [25], we investigate whether\nit can also improve the re-": "Fig 2, speaker embeddings failed to cluster together, making it"
        },
        {
          "the development set.": "Other",
          "tions [25], we investigate whether\nit can also improve the re-": "difficult for the model to learn."
        },
        {
          "the development set.": "No Agreement",
          "tions [25], we investigate whether\nit can also improve the re-": ""
        },
        {
          "the development set.": "",
          "tions [25], we investigate whether\nit can also improve the re-": "Table 3: Comparison of models trained with speaker embed-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 6: The results from the IS25-SER Challenge on Task 2.",
      "data": [
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "respectively."
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": ""
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "Baseline"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "WavLM"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "MMWavLM"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "+ Undersampling"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "+ S Emb"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "3-system (2nd)"
        },
        {
          "“A”, “V”, and “D” denote arousal, valence, and dominance,": "2-system (1st)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 6: The results from the IS25-SER Challenge on Task 2.",
      "data": [
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "training with the undersampling method."
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "As shown in Table 6, MMWavLM performs significantly"
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "better than WavLM, indicating that textual embeddings also im-"
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "prove the performance in the test set. Moreover, adding speaker"
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "embedding causes the model to overfit on the speaker informa-"
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": ""
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "tion, leading to degraded performance, as suggested by our ex-"
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": ""
        },
        {
          "sentation while “+ Undersampling” denotes that\nthe model\nis": "periments on the validation set."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 6: The results from the IS25-SER Challenge on Task 2.",
      "data": [
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "training or validation set"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "Training Set"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "- O/X(T)"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "- Non-O/X (T)"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "+ O/X(V)"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": "- Non-O/X (T)"
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        },
        {
          "data, and (T) and (V) indicate whether the samples are from the": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Frequency Translational Invariance in TDNNs and Frequency Po-"
        },
        {
          "6. Acknowledgment": "We gratefully acknowledge support from IARPA ARTS (award",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "sitional Information in 2D ResNets to Enhance Speaker Verifica-"
        },
        {
          "6. Acknowledgment": "number 140D0424C0067, JHU subcontract) from the Office of",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "tion,” in Interspeech 2021, 2021, pp. 2302–2306."
        },
        {
          "6. Acknowledgment": "the Director of National Intelligence. NSF (SCH 2204942).",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[16]\nT. Lertpetchpun and E. Chuangsuwanich, “Instance-based Tem-"
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "INTER-\nporal Normalization for Speaker Verification,” in Proc."
        },
        {
          "6. Acknowledgment": "7. References",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "SPEECH 2023, 2023, pp. 3172–3176."
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[17] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "6. Acknowledgment": "[1]\nJ. Shi, D. Berrebbi, W. Chen, E.-P. Hu, W.-P. Huang, H.-L. Chung,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak"
        },
        {
          "6. Acknowledgment": "X. Chang, S.-W. Li, A. Mohamed, H. yi Lee,\nand S. Watan-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "supervision,”\nin International conference on machine learning."
        },
        {
          "6. Acknowledgment": "abe,\n“ML-SUPERB: Multilingual\nSpeech Universal\nPERfor-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "PMLR, 2023, pp. 28 492–28 518."
        },
        {
          "6. Acknowledgment": "mance Benchmark,” in Interspeech 2023, 2023, pp. 884–888.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[18]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from"
        },
        {
          "6. Acknowledgment": "[2]\nJ. Shi, S.-H. Wang, W. Chen, M. Bartelds, V. Bannihatti Ku-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Speech Using wav2vec 2.0 Embeddings,” in Interspeech 2021,"
        },
        {
          "6. Acknowledgment": "mar,\nJ. Tian, X. Chang, D.\nJurafsky, K. Livescu, H. yi Lee,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "2021, pp. 3400–3404."
        },
        {
          "6. Acknowledgment": "and S. Watanabe, “ML-SUPERB 2.0: Benchmarking Multilin-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "gual Speech Models Across Modeling Constraints, Languages,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[19]\nT. Feng and S. Narayanan, “Foundation model assisted automatic"
        },
        {
          "6. Acknowledgment": "and Datasets,” in Interspeech 2024, 2024, pp. 1230–1234.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "speech emotion recognition: Transcribing, annotating, and aug-"
        },
        {
          "6. Acknowledgment": "[3]\nS. Peng, W. Guo, H. Wu, Z. Li, and J. Zhang, “Fine-tune Pre-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "menting,” in ICASSP 2024-2024 IEEE International Conference"
        },
        {
          "6. Acknowledgment": "Trained Models with Multi-Level Feature Fusion for Speaker Ver-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "6. Acknowledgment": "ification,” in Interspeech 2024, 2024, pp. 2110–2114.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "2024, pp. 12 116–12 120."
        },
        {
          "6. Acknowledgment": "[4] H. Futami, S. Arora, Y. Kashiwagi, E. Tsunoo, and S. Watan-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[20] Y. Li, P. Bell, and C. Lai, “Fusing asr outputs in joint training for"
        },
        {
          "6. Acknowledgment": "abe,\n“Finding Task-specific Subnetworks\nin Multi-task Spoken",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "speech emotion recognition,” in ICASSP 2022-2022 IEEE Inter-"
        },
        {
          "6. Acknowledgment": "Language Understanding Model,” in Interspeech 2024, 2024, pp.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "national Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. Acknowledgment": "802–806.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "(ICASSP).\nIEEE, 2022, pp. 7362–7366."
        },
        {
          "6. Acknowledgment": "[5] W.-S. Chien, S. G. Upadhyay, and C.-C. Lee, “Balancing Speaker-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[21]\nJ. Deng,\nJ. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive"
        },
        {
          "6. Acknowledgment": "Rater Fairness for Gender-Neutral Speech Emotion Recognition,”",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "angular margin loss for deep face recognition,” in CVPR, 2019,"
        },
        {
          "6. Acknowledgment": "in ICASSP 2024-2024 IEEE International Conference on Acous-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "pp. 4690–4699."
        },
        {
          "6. Acknowledgment": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2024, pp.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[22] H. H¨arm and T. Alum¨ae, “TalTech Systems for the Odyssey 2024"
        },
        {
          "6. Acknowledgment": "11 861–11 865.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Emotion Recognition Challenge,” in The Speaker and Language"
        },
        {
          "6. Acknowledgment": "[6] B. M. Booth and S. S. Narayanan, “People make mistakes: Ob-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Recognition Workshop (Odyssey 2024), 2024, pp. 255–259."
        },
        {
          "6. Acknowledgment": "taining accurate ground truth from continuous annotations of sub-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[23] M. Chen, H. Zhang, Y. Li, J. Luo, W. Wu, Z. Ma, P. Bell, C. Lai,"
        },
        {
          "6. Acknowledgment": "jective constructs,” Behavior Research Methods, vol. 56, no. 8,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "J. D. Reiss, L. Wang, P. C. Woodland, X. Chen, H. Phan, and"
        },
        {
          "6. Acknowledgment": "pp. 8784–8800, 2024.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "T. Hain,\n“1st Place Solution to Odyssey Emotion Recognition"
        },
        {
          "6. Acknowledgment": "[7] C.-C. Lee, T. Chaspari, E. M. Provost,\nand S. S. Narayanan,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Challenge Task1:\nTackling Class\nImbalance Problem,”\nin The"
        },
        {
          "6. Acknowledgment": "“An Engineering View on Emotions and Speech: From Analysis",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Speaker and Language Recognition Workshop (Odyssey 2024),"
        },
        {
          "6. Acknowledgment": "and Predictive Models to Responsible Human-Centered Applica-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "2024, pp. 260–265."
        },
        {
          "6. Acknowledgment": "tions,” Proceedings of the IEEE, pp. 1–17, 2023.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[24] R. Lotfian and C. Busso, “Building naturalistic emotionally bal-"
        },
        {
          "6. Acknowledgment": "¨\n[8] A. R. Naini, L. Goncalves, A. N. Salman, P. Mote,\nI. R.\nUlgen,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "anced speech corpus by retrieving emotional speech from existing"
        },
        {
          "6. Acknowledgment": "T. Thebaud, L. Velazquez, L. P. Garcia, N. Dehak, B. Sisman, and",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "podcast recordings,” IEEE Transactions on Affective Computing,"
        },
        {
          "6. Acknowledgment": "C. Busso, “The interspeech 2025 challenge on speech emotion",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "vol. 10, no. 4, pp. 471–483, 2017."
        },
        {
          "6. Acknowledgment": "recognition in naturalistic conditions,” in Interspeech 2025, vol.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[25] Y. Gao, C. Chu,\nand T. Kawahara,\n“Two-stage Finetuning of"
        },
        {
          "6. Acknowledgment": "To appear, Rotterdam, The Netherlands, August 2025.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Wav2vec 2.0 for Speech Emotion Recognition with ASR and"
        },
        {
          "6. Acknowledgment": "[9] M. Grimm, K. Kroschel,\nE. Mower,\nand\nS.\nS. Narayanan,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "Gender Pretraining,” in Interspeech 2023, 2023, pp. 3637–3641."
        },
        {
          "6. Acknowledgment": "“Primitives-based\nevaluation\nand\nestimations\nof\nemotions\nin",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "[26]\nT. Feng and S. Narayanan, “Peft-ser: On the use of parameter effi-"
        },
        {
          "6. Acknowledgment": "speech,” Speech Communication, vol. 49, no. 10-11, pp. 787–800,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "cient transfer learning approaches for speech emotion recognition"
        },
        {
          "6. Acknowledgment": "nov 2007.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "using pre-trained speech models,” in 2023 11th International Con-"
        },
        {
          "6. Acknowledgment": "[10]\nL. Goncalves, A. N. Salman, A. R. Naini, L. M. Velazquez,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "ference on Affective Computing and Intelligent Interaction (ACII)."
        },
        {
          "6. Acknowledgment": "T. Thebaud, L. P. Garcia, N. Dehak, B. Sisman, and C. Busso,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": "IEEE, 2023, pp. 1–8."
        },
        {
          "6. Acknowledgment": "“Odyssey 2024-Speech Emotion Recognition Challenge: Dataset,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "Baseline Framework,\nand Results,” Development,\nvol. 10,\nno.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "9,290, pp. 4–54, 2024.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "[11] B. T. Atmaja and M. Akagi, “Evaluation of error-and correlation-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "based loss\nfunctions\nfor multitask learning dimensional\nspeech",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "emotion recognition,” in Journal of Physics: Conference Series,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "vol. 1896, no. 1.\nIOP Publishing, 2021, p. 012004.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "[12]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "supervised pre-training for\nfull\nstack speech processing,” IEEE",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "Journal of Selected Topics in Signal Processing, vol. 16, no. 6,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "pp. 1505–1518, 2022.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "[13]\nT. Tran, T. D. Bui,\nand P. Simatis,\n“ParallelChain Lab’s anti-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "spoofing systems for ASVspoof 5,” in The Automatic Speaker Ver-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "ification Spoofing Countermeasures Workshop (ASVspoof 2024),",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "2024, pp. 9–15.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "[14] A. Tomilov, A. Svishchev, M. Volkova, A. Chirkovskiy, A. Kon-",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "dratev, and G. Lavrentyeva, “STC Antispoofing Systems for the",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "the Automatic\nASVspoof2021 Challenge,”\nin 2021 Edition of",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "Speaker Verification and Spoofing Countermeasures Challenge,",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        },
        {
          "6. Acknowledgment": "2021, pp. 61–67.",
          "[15]\nJ. Thienpondt, B. Desplanques, and K. Demuynck, “Integrating": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
      "authors": [
        "J Shi",
        "D Berrebbi",
        "W Chen",
        "E.-P Hu",
        "W.-P Huang",
        "H.-L Chung",
        "X Chang",
        "S.-W Li",
        "A Mohamed",
        "H Yi Lee",
        "S Watanabe"
      ],
      "venue": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark"
    },
    {
      "citation_id": "2",
      "title": "ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets",
      "authors": [
        "J Shi",
        "S.-H Wang",
        "W Chen",
        "M Bartelds",
        "V Bannihatti",
        "J Kumar",
        "X Tian",
        "D Chang",
        "K Jurafsky",
        "H Livescu",
        "S Yi Lee",
        "Watanabe"
      ],
      "venue": "ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets"
    },
    {
      "citation_id": "3",
      "title": "Fine-tune Pre-Trained Models with Multi-Level Feature Fusion for Speaker Verification",
      "authors": [
        "S Peng",
        "W Guo",
        "H Wu",
        "Z Li",
        "J Zhang"
      ],
      "venue": "Fine-tune Pre-Trained Models with Multi-Level Feature Fusion for Speaker Verification"
    },
    {
      "citation_id": "4",
      "title": "Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model",
      "authors": [
        "H Futami",
        "S Arora",
        "Y Kashiwagi",
        "E Tsunoo",
        "S Watanabe"
      ],
      "venue": "Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model"
    },
    {
      "citation_id": "5",
      "title": "Balancing Speaker-Rater Fairness for Gender-Neutral Speech Emotion Recognition",
      "authors": [
        "W.-S Chien",
        "S Upadhyay",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "People make mistakes: Obtaining accurate ground truth from continuous annotations of subjective constructs",
      "authors": [
        "B Booth",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "7",
      "title": "An Engineering View on Emotions and Speech: From Analysis and Predictive Models to Responsible Human-Centered Applications",
      "authors": [
        "C.-C Lee",
        "T Chaspari",
        "E Provost",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "8",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I Ülgen",
        "T Thebaud",
        "L Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "9",
      "title": "Primitives-based evaluation and estimations of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "10",
      "title": "Odyssey 2024-Speech Emotion Recognition Challenge: Dataset, Baseline Framework, and Results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Development"
    },
    {
      "citation_id": "11",
      "title": "Evaluation of error-and correlationbased loss functions for multitask learning dimensional speech emotion recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "12",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "ParallelChain Lab's antispoofing systems for ASVspoof 5",
      "authors": [
        "T Tran",
        "T Bui",
        "P Simatis"
      ],
      "year": "2024",
      "venue": "The Automatic Speaker Verification Spoofing Countermeasures Workshop"
    },
    {
      "citation_id": "14",
      "title": "STC Antispoofing Systems for the ASVspoof2021 Challenge",
      "authors": [
        "A Tomilov",
        "A Svishchev",
        "M Volkova",
        "A Chirkovskiy",
        "A Kondratev",
        "G Lavrentyeva"
      ],
      "year": "2021",
      "venue": "2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge"
    },
    {
      "citation_id": "15",
      "title": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification",
      "authors": [
        "J Thienpondt",
        "B Desplanques",
        "K Demuynck"
      ],
      "venue": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification"
    },
    {
      "citation_id": "16",
      "title": "Instance-based Temporal Normalization for Speaker Verification",
      "authors": [
        "T Lertpetchpun",
        "E Chuangsuwanich"
      ],
      "year": "2023",
      "venue": "Proc. INTER-SPEECH 2023"
    },
    {
      "citation_id": "17",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "18",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "venue": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings"
    },
    {
      "citation_id": "19",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "TalTech Systems for the Odyssey 2024 Emotion Recognition Challenge",
      "authors": [
        "H Härm",
        "T Alumäe"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "23",
      "title": "1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem",
      "authors": [
        "M Chen",
        "H Zhang",
        "Y Li",
        "J Luo",
        "W Wu",
        "Z Ma",
        "P Bell",
        "C Lai",
        "J Reiss",
        "L Wang",
        "P Woodland",
        "X Chen",
        "H Phan",
        "T Hain"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "24",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
      "authors": [
        "Y Gao",
        "C Chu",
        "T Kawahara"
      ],
      "year": "2023",
      "venue": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining"
    },
    {
      "citation_id": "26",
      "title": "Peft-ser: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pre-trained speech models",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    }
  ]
}