{
  "paper_id": "2506.16448v1",
  "title": "Consumer-Friendly Eeg-Based Emotion Recognition System: A Multi-Scale Convolutional Neural Network Approach",
  "published": "2025-06-19T16:33:31Z",
  "authors": [
    "Tri Duc Ly",
    "Gia H. Ngo"
  ],
  "keywords": [
    "Deep learning",
    "Electroencephalogram",
    "Emotion recognition.",
    "EEG-based emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG is a non-invasive, safe, and low-risk method to record electrophysiological signals inside the brain. Especially with recent technology developments like dry electrodes, consumer-grade EEG devices, and rapid advances in machine learning, EEG is commonly used as a resource for automatic emotion recognition. With the aim to develop a deep learning model that can perform EEG-based emotion recognition in a real-life context, we propose a novel approach to utilize multi-scale convolutional neural networks to accomplish such tasks. By implementing feature extraction kernels with many ratio coefficients as well as a new type of kernel that learns key information from four separate areas of the brain, our model consistently outperforms the state-of-the-art TSception model in predicting valence, arousal, and dominance scores across many performance evaluation metrics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a fundamental part of human life. Associated with feelings, emotions are important to one's decision-making, learning process, and many other cognitive processes  [1] . They are also a key tool for human interaction and communication, serving as a way for the communicator to express themself, providing information on their state of mind, feelings, motives, and intentions  [2] . Therefore, the interest in researching, learning, and further understanding human emotion and its impact has been growing, especially in the field of neuroscience  [3] . While much progress is still to be made with research regarding the matter due to the complexity behind human emotion  [4] , many advancements have been achieved thanks to the development of technology in the field, contributing to novel approaches that researchers can use to further study human emotion. With more knowledge of emotion, a key to perceiving feelings, expression, and cognitive information of a person, there are a number of ways this can be used to improve the quality of life, one of which, is therapy.\n\nIn recent years, a number of technological advancements have led to an increased interest in a resource that can be used for the task: electroencephalography (EEG), one of the most widely used brain imaging technologies. EEG presents a non-invasive way to measure brain electrical activities, which can then be passed through a brain-computer interface (BCI) to further process the information and identify human emotion. This surge in interest is due to the development of consumer-grade EEG devices with dry electrodes. Before this development, despite its potential, the applications of EEG-BCI systems outside of research labs are extremely sparse due to the limitations of research-grade EEG devices:  (1)  it is time-consuming to set up a research-grade EEG device, typically taking from 30 to 60 minutes, (2) user's mobility is heavily restricted due to the high number of wires, and (3) the extremely high cost of the devices  [5] . Even though data recorded by research-grade EEG devices may provide more information and allow EEG-BCI systems to yield better results in the task of emotion recognition, consumer-grade EEG devices open up countless more research and consumer applications with their affordability, portability and simplicity, while still providing reliable results  [6] .\n\nWith automatic emotion recognition using EEG signals, the use of machine learning (ML) algorithms, specifically deep learning (DL), is one of the most popular and reliable methods due to its known ability to learn non-linear patterns from complex information -an aspect that is found in EEG. Thus, we developed a DL model inspired by TSception  [7]  that predicts human emotion based on EEG with reliable accuracy. Our method provides a novel approach to which a DL architecture can be used for the task.\n\nThe remainder of this paper is organized as follows. In Section 2, we give background information on emotion, EEG, EEG-based emotion recognition, and deep learning. Section 3 provides the details of our materials and method by introducing the dataset and the performance evaluation metrics that we used, the experimental setup of the DL model, and the mechanism behind our model. In Section 4, we present the results of our model and analyze how these results can be interpreted. Future implications, suggestions, and a discussion regarding our study are given in Section 5. Finally, the conclusion and guidance acknowledgment are presented in Sections 6 and 7.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion",
      "text": "Emotion has long been a complicated subject of research. One of the earliest attempts to explore the mechanism behind human emotions comes from the James-Lange theory of emotion in 1884  [8] , in which psychologist William James and psychologist Carl Lange suggested that emotion is the result of a physiological response to external stimuli (e.g., your body trembling and your heart beating rapidly would cause you to feel fear). This theory was challenged in 1927 by Walter Cannon and Phillip Bard, who argued in the Cannon-Bard theory that emotion and physiological response occur simultaneously, not consequently  [9] .\n\nThe Freudian theory by Sigmund Freud, one of the greatest and most well-known theories in psychology, introduced the idea that much of human behavior, including human emotion, is greatly influenced by the unconscious mind. In the 1950s, the cognitive revolution led to the development of novel theories such as the Schachter-Singer Two-Factor Theory of Emotion, which suggests that emotion consists of two components: physical arousal and a cognitive interpretation  [10] . In the 1960s and the 1970s, Richard Lazarus pioneered the advancement of the cognitive appraisal theory, stating emotion is the response to an individual's evaluation of a situation. As the 21st century approaches, advances in brain imaging like functional magnetic resonance imaging led to many studies about the brain's role in human emotions, the emergence of the field of affective neuroscience, and the popularization of emotional intelligence. These advancements, with pioneers like Jaak Panksepp and Daniel Goleman, have led to rapid progress in extensive research on the role of emotion and the mechanism behind it.\n\nEmotion plays many roles in human life: (1) Emotions serve as social signals in human exchanges, conveying one's feelings, perceptions, and situational understanding to the interaction partner  [11] ; (2) Emotion greatly influences cognitive function, which includes decision making, perception, learning, and maintaining health  [1] ; (3) Emotion significantly affects the course of actions, execution, control, and the way one explains their actions  [12] . It also aids our daily lives in various ways. Most evidently, emotion is known to provide memory benefits, though it enhances memory more for negative experiences than positive ones  [13] . As a social signal, it serves as a tool for humans to connect with others, navigating social interactions so that we can resonate with and care for another person  [14] . The positive and negative nature of emotions can also affect human health, as well as influencing and experiencing work processes  [15] ,  [16] .\n\nThere is no unitary definition for emotion  [17] , and the way emotion is perceived has vastly varied throughout modern science. The way people feel certain emotions can be different from how others do, and the description of an emotion one feels can vastly vary from another  [18] . P. R. Kleinginna and A. M. Kleinginna  [19]  famously defined emotions as \"a complex set of interactions among subjective and objective factors, mediated by neural hormonal systems, which can (a) give rise to affective experiences such as feelings of arousal, pleasure/displeasure; (b) generate cognitive processes such as emotionally relevant perceptual effects, appraisals, labeling processes; (c) activate widespread physiological adjustments to the arousing conditions; and (d) lead to behavior that is often, but not always, expressive, goal-directed, and adaptive.\" Generally, emotion is described as the brain's consistent response toward an external stimulus. To classify emotion, most approaches are either categorical or dimensional. The categorical approach classifies emotions into a group of defined emotional states, or discrete emotions. Many researchers have presented various ways to define such groups:\n\n1. Kemper  [20]  stated that there are four primary emotions -fear, anger, depression, and satisfaction -and many secondary emotions that can be acquired via social agents like guilt, shame, pride, gratitude, love, nostalgia, and ennui. 2. Levenson  [21]  stated that basic emotions need to meet three criteria of distinctness, hard-wiredness, and functionality, and he found six emotions that suffice: enjoyment, anger, disgust, fear, surprise, and sadness. 3. Ekman and Cordaro  [22]  stated that most basic emotions share 13 common characteristics and summarized seven basic emotions: Anger, fear, surprise, sadness, disgust, contempt, and happiness. 4. Plutchik  [23]  proposed eight basic emotions described in a wheel model: joy, trust, fear, surprise, sadness, disgust, anger, and anticipation. 5. Izard  [24]  proposed 10 basic emotions: interest, joy, surprise, sadness, fear, shyness, guilt, anger, disgust, and contempt.\n\nOn the contrary, the dimensional approach better captures the complexity of emotion by using a scale for different affective states. While this approach is still incompatible with depicting the full complexity and high-dimensional nature of emotions  [25] , it allows for much easier implementation and interoperability, as well as improving accuracy for emotion classification systems. One of the most commonly used dimensional models in emotion classification systems is Russell's circumplex model of affect, shown in Fig.  1    [26] . It is a two-dimensional model that uses two scales -a valence scale and an arousal scale. The valence scale shows how positive or negative a person is feeling, and the arousal scale shows how high or low a person's attention level is. Russell's and Steiger's extended version of this model, often regarded as the valence-arousal-dominance (VAD) model,  [27]  is also just as commonly used, which adds an extra dimension called dominance to show how in-controlled or submissive a person is feeling, shown in Fig.  2 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg",
      "text": "EEG is the electrophysiological process of measuring and collecting information on electrical fields in the brain by placing electrodes on the scalp  [29] , providing a display of electrical activities in the brain in the form of waves with varying frequencies, amplitudes, and shapes. These fields are formed when billions of electrochemical signals spontaneously pass between brain neurons in an extended space, causing the sum of these fields to be powerful enough to be recorded from outside, making EEG a completely safe and non-invasive procedure. In other words, EEG signals are the result of impulses of brain neurons. EEG does not record the activities of every individual neuron but rather captures the signals created when neurons activate at the same time. One major limitation of EEG is that its signals, which come from cerebral activity, can be contaminated easily by other physiological signals from other electrical activities generated by the body during common occasions such as movements or eye blinks, causing EEG signals to be non-linear, non-stationary, and often overwhelmed by noise  [30],  [31] . Raw EEG data is known to be extremely complex and challenging to interpret, requiring advanced analysis, signal processing, and feature extraction to be correctly interpreted  [32] . EEG feature is a pattern associated with a particular sensory or cognitive process  [29] , which can capture human states of mind and emotional states. Thus, EEG, especially when observed in combination with EEG features, is proven to be able to provide useful information that can reflect characteristics in response to emotional states, leading to it being a very popular choice for the task of emotion recognition  [33] ,  [34] . EEG electrodes record the signals of a small area surrounding them. An EEG headset can have as few as four electrodes and as many as 256 electrodes. Electrodes are often distributed all across the scalp to cover as much area as possible. Strategically placing these electrodes to yield the most satisfactory recording is one of the most important aspects of an EEG test, study, and lab research. One standardized technique to place these electrodes across the scalp is to follow the International 10-20 system, depicted in Fig.  3    [35] ,  [36] . The International 10-20 system uses four anatomical landmarks on the scalp -the nasion (the bridge of the nose), the inion (the lowest point of the skull at the back of the head), and the left and right preauricular points (the point just in front of the ears where the upper jaw and the lower jaw meet) -as references to ensure consistent placement across EEG research:\n\n1. The nasion: The bridge of the nose. 2. The inion: The lowest point of the skull at the back of the head. 3. The left and right preauricular points: The two points located just in front of the ears, where the upper jaw and the lower jaw meet.\n\nThe numbers 10 and 20 refer to the distance between one electrode to another. Every electrode that is next to an anatomical landmark will be placed 10% of the entire distance from the front to the back or from the right to the left of the brain, and 20% of the distance for every other electrode. A unique label is assigned to every electrode, each consisting of one or two letters and one number, representing the location of the brain where the electrode is placed. The assigned number is even if the electrode is placed in the right hemisphere of the brain, and is odd if the electrode is placed in the left hemisphere. The assigned letter (or letters) is an abbreviation of different parts of the brain: Fp for pre-frontal, F for frontal, T for temporal, O for occipital, P for parietal, and C for central. Fig.  3 . The International 10-20 electrodes placement system (Source:  [37] ).\n\nAn extension of the International 10-20 system, the 10-10 system, is also a very commonly used electrode placement system in EEG research. Instead of 10% and 20% intervals, the 10-10 system only uses 10% intervals, which allows more possible positions for electrode placements, which can be more useful for purpose-specific signal recording in research.\n\nThanks to the rapid development of dry electrodes and consumer-grade EEG devices from companies such as Emotiv, OpenBCI, and NeuroSky, there has been a massive boost in EEG and EEG-based emotion recognition research. Compared to the most commonly used wet electrodes, dry electrodes yield performance on a similar level to wet electrodes (often considered the gold standard for EEG electrodes)  [38] ,  [39] ,  [40] ,  [41]  while still presenting a number of advantages:  1. Less preparation time: Dry electrodes take significantly less preparation time, as EEG headsets with wet electrodes require additional steps such as applying conductive gel  [39] ,  [42] . 2. Better user comfort: With the use of conductive gel in wet electrodes, participants often experience discomfort, including skin irritation and inconvenient scalp preparation, especially when used for an extended time duration  [38] ,  [40] ,  [43] . 3. Easy maintenance: Since dry electrodes do not require the use of conductive gel, they are much easier to clean and require much less maintenance effort. 4. Portability: EEG headsets with dry electrodes are more wearable and portable, as well as requiring very little scalp preparation  [44] . 5. Low cost: EEG systems with dry electrodes cost significantly less compared to systems with wet electrodes  [41] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "Emotion recognition refers to the task of identifying and interpreting human emotions with the help of various indications such as facial expressions, body language, or physiological signals like photoplethysmography, electromyography, and electrocardiogram (ECG). In our case, EEG-based emotion recognition uses EEG, the most commonly used option among physiological signals  [6] ,  [45] ,  [46] . In an EEG-based emotion recognition process, a study typically needs to recruit participants and select a stimulus to evoke a targeted emotion. During a recording session, the participant is often asked to wear an EEG device. The participant is then exposed to the stimulus, and the voltage fluctuations in the brain are then recorded. EEG presents an image of electrical activity in the brain. The data is then passed through a preprocessing stage, where it is denoised and artifacts are filtered out. The clean data is then analyzed and goes through the feature extraction stage, one of the most essential stages in EEG-based emotion recognition. Only then, an ML classifier is used for training and learning the EEG data, which allows it to predict the emotional state of a person, producing the final output.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deep Learning",
      "text": "For the task of emotion recognition, artificial intelligence (AI)-based classifiers using ML algorithms have been widely used, especially in recent years, due to the rapid development of AI in general and ML specifically. While the terms AI, ML, and DL are often incorrectly used in interchange with each other, they are completely separate terms that cover different ranges of algorithms and techniques. AI includes all algorithms and techniques that allow computers to mimic human behavior  [47] . A lot of AI systems behave based on hard-coded statements, which limits their adaptability and ability to complete tasks with a high level of complexity. ML overcomes this by learning and improving through experience. In more technical terms, ML systems train through many iterations, compare their predictions to the actual answers to calculate a loss value, and then improve and adapt by modifying their algorithm to minimize the loss. When the loss or the error rate becomes a constant after many iterations, indicating that it is minimized, and the model's performance stops improving, this state of an ML model is often referred to as convergence. Literature tackling emotion classification using ML often approaches it either as a classification or a regression problem. A classification task has outputs (also often regarded as labels in ML) that can be divided into a finite number of groups. An example of this would be predicting whether an email is spam or not, in which the labels can be divided into two groups: spam or non-spam. A regression task has continuous outputs that are specific values that cannot be divided into a finite number of groups. An example of this would be predicting house prices, in which the output can be any positive decimal value. There are four types of ML algorithms:\n\n1. Supervised learning: These algorithms train using datasets that have pairs of input and label, and then later predict an outcome from a completely new input. Most ML algorithms fall in this group. 2. Unsupervised learning: These algorithms train using datasets that only have outcomes and no inputs. The algorithm would then use the available data to complete a task such as association (e.g, detecting a pattern in a user's activities in a music app and then using that pattern to suggest a song that he or she would enjoy). 3. Semi-supervised learning: In most supervised learning tasks, it is rare to find a complete dataset. While not as commonly used, some regard to tasks where a fraction of the dataset is missing labels as semi-supervised learning. 4. Reinforcement learning: These algorithms help a system learn to make decisions to optimize its performance using the trial-and-error learning process, which is mostly used in game theory. One wellknown example of this is Google's AlphaGo, an AI that famously defeated Go world champion Lee Sedol in 2016.\n\nDL builds on an ML technique called artificial neural network (ANN), which mimics how a neural network in the human brain works, amplifying or reducing signals transmitted between neurons by increasing or decreasing a weight value that is assigned to each neuron. DL systems often show better learning capabilities and accuracies compared to ML systems in many applications with high-dimensional data like text, image, video, and audio data, which is why there has been a massive interest in DL applications.\n\nArtificial Neural Network In the human brain, there are approximately 86 billion neurons forming 100 trillion connections to each other. To process the unending flow of information from the body, these neurons create electrical impulses non-stop to move and transmit information in a neural network. This is the basis and the inspiration for the artificial neural network: To complete a complex task, a neuron will pass on information to another neuron  [48] , which allows it to thrive in tasks with large amounts of data that linear ML algorithms would not be able to handle. A key characteristic of ANN is its ability to learn and extract features from data, helping it make optimal decisions. An ANN has three types of layers:\n\n1. Input layer: This is the layer where the data enters the ANN. Each neuron represents a distinct feature in the data. Every ANN only has one input layer. 2. Hidden layer(s): This is where the 'learning' process happens in an ANN. The depth (the number of hidden layers) and the width (the number of neurons in each hidden layer) are hyperparameters that form the network architecture, which allows the network to extract and recognize complex patterns from the data. An ANN can have one or more hidden layers.\n\n3. Output layer: This is the final layer of the network. The output of this layer is the output of the whole network. Every ANN only has one output layer.\n\nInput Layer ∈ ℝ³ Hidden Layer ∈ ℝ⁴ Hidden Layer ∈ ℝ⁶ Output Layer ∈ ℝ² Fig.  5 . An example of an artificial neural network, produced using NN-SVG  [49] .\n\nIn an ANN, connections between neurons allow them to process information from each other with the use of weights. Each connection has its own weight, and the closer the value of the weight is to 0, the weaker the connection is. When an ANN learns via training iterations, the ANN adjusts these weights accordingly in order to minimize the error. However, for the network to process and learn complex patterns like those from real-life applications, this linear learning process would often not be good enough. To add a non-linear aspect to neurons' connections, an ANN uses activation functions, which process the output of a neuron to determine whether it should be activated or not. Activation functions are only applied in hidden layers, and the activation function used in each hidden layer varies, making it another hyperparameter. In a linear ANN model, the activation function would simply be f (x) = x. For more complex ANN models, other activation functions are used, like Rectified Linear Unit (ReLU), Sigmoid, and Tanh. Finally, a bias might be applied to adjust the output independently of the input data, allowing the model to better fit the data when training.\n\nDeep Learning ANN is the backbone of DL. The field of DL focuses on utilizing neural networks (NN) in a more complicated way to learn complex patterns in large datasets. DL has continuously outperformed traditional ML techniques and achieved state-of-the-art results in emotion recognition  [50] ,  [51] ,  [52]  thanks to its effectiveness in processing auditory and visual data. Unlike traditional ML techniques, DL removes the need for manual feature extraction and feature engineering as it can automatically learn from raw data after many iterations (often regarded in ML as epochs). DL models excel when there is a large amount of data, as their performance generally improves as more data is fed into them, making them suitable for applications with large datasets. However, a major limitation of DL models is that training them can be computationally expensive, requiring powerful GPUs, large amounts of memory, and long training time.\n\nConvolutional Neural Network Convolutional neural network (CNN) is a deep learning technique that specializes in processing structured grid-like data like images. The most fundamental component of CNN is the convolutional layer. A convolutional layer has parameters called kernels or sliding windows, which act as filters sliding over input data and performing convolution operations to detect complex patterns in the data. The output of a convolutional layer is called a feature map. To make CNN models more robust to minor changes and noise, like a slight shift or a slight rotation in an image, pooling (subsampling) layers are used to reduce the width and height of the feature maps, filtering out insignificant details and retaining the most useful information. This process is often regarded in ML as downsampling. After the data is processed through many convolutional and pooling layers, the output is passed to one or several fully connected layers similar to an ANN. The fully connected layers used the features extracted to produce the final output, which in the example shown in Fig.  6  would be class labels.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset",
      "text": "The DREAMER dataset is a commonly used public dataset in the field of affective computing, especially for emotion recognition based on EEG and ECG. It is a multi-modal dataset that contains both EEG and ECG signals recorded during affect elicitation using audio-visual stimuli  [54] . The dataset includes recordings from 25 participants (14 males and 11 females) aged between 22 and 33 with an average age of 26.6 years, though technical problems caused recording from two subjects (both females) to be unsuitable for use. In emotion recognition, audiovisual stimuli (video) guarantee more valence intensity in subjects compared to visual-only stimuli (images)  [55] . In each session, which lasted approximately one hour, each subject started by watching a neutral film clip in order to record the baseline signals and to return the emotional state of the subject to normal after watching an emotion-eliciting film clip. Every subject will watch 18 film clips with nine different targeted emotions (calmness, surprise, amusement, fear, excitement, disgust, happiness, anger, and sadness) with durations ranging from 65 to 393 seconds (the mean duration was 199 seconds). While the subjects were viewing the film clip, their EEG was being recorded at a sample rate of 128 Hz using the Emotiv EPOC wireless EEG headset, which has 16 gold-plated contact sensors placed on locations in accordance with the International 10-10 system. One mastoid sensor was placed at M1 as a ground reference point to measure the voltage for other sensors, and another mastoid sensor was placed at M2 as a feed-forward reference for reducing external electrical interference. The other 14 sensors were placed in the following locations: AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, and AF4, as shown in Fig 7 . After watching each film clip, the subject will be asked to evaluate their emotion (based on what they actually felt, not what they thought the targeted emotion of the film clip is) by reporting the felt valence, arousal, and dominance scores on a five-point scale for each of them. An example of how the EEG signal plot would look is shown in Fig.  8 , which is the signal plot for one second of recording taken randomly from the DREAMER dataset. The recorded EEG and ECG data, the participants' data, and their valence, arousal, and dominance evaluation were then stored in a data structure. A summary of the experiment is shown in Table  2 . Since we are developing an EEG-based emotion recognition system, only the EEG data of the participants is considered. A threshold of three was used to convert the arousal, valence, and dominance scores from 1-5 to zero and one, which means all scores with values greater than or equal to three would be converted to one, and all other values would be converted to 0. This is done to minimize the effect of the possible variation for each participant in the evaluation of elicited emotion.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experimental Setup",
      "text": "Metrics To compare our model to the TSCeption model, we evaluate the performance of each model using many different metrics: precision, recall, F1 score, accuracy, Matthew's correlation coefficient (MCC), Cohen's kappa, and area under the receiver operating characteristic curve (AUROC). All of those metrics can be calculated using the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) outcomes of the models, which represent when:\n\n1. TP: Both the prediction and the actual value are positive.\n\n2. TN: Both the prediction and the actual value are negative.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Fp:",
      "text": "The predicted value is positive, but the actual value is negative.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Fn:",
      "text": "The predicted value is negative, but the actual value is positive.\n\nDespite the most applied metrics for evaluating binary classification models being F1 score and accuracy, we evaluated the models using this wide variety of metrics, as each comes with its own strengths and limitations. Thus, for each different potential application of the model, one metric may provide a better evaluation compared to others. Precision, recall, F1 score, accuracy, and MCC can be calculated using these equations:\n\nAUROC = 1 2\n\nPrecision is defined as the number of correct positive predictions divided by the total number of predicted positives. In other words, it evaluates the accuracy of a model's positive prediction. Recall is defined as the number of correct positive predictions divided by the number of actual positive values. In our case, this is the number of ones in predicted values divided by the number of ones in labels. Combining the pair of precision and recall provides a useful evaluation of classification models, which the F1 score achieves by calculating the harmonic mean of precision and recall. Thus, the F1 score remains one of the most popular choices for evaluating classification models. However, it is worth noting that the F1 score also has a number of disadvantages that prevent it from fully reflecting the performance of an ML model:\n\n1. The F1 score can be easily affected when dealing with an imbalanced class, as it can be dominated by the majority class. 2. As can be seen in equation (  3 ), the F1 score does not take into account any true negative values, which can result in misleading conclusions, especially when evaluating models with a task that might be important to understand true negatives, like certain medical tests or anomaly detections.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Precision And Recall Have Equal Weights In The Calculation Of The F1",
      "text": "score, assuming equal importance between them, which results in domain knowledge not being taken into account. In many cases, this can lead to significant, misleading conclusions from the evaluation (e.g., precision might be more important in fraud detection, whereas recall might be more important in medical diagnostics). 4. The F1 score is sensitive to minor changes in small datasets, which can result in instability in the metric. 5. As the F1 score is the harmonic mean of precision and recall, it is less interpretable compared to other metrics like precision, recall, or accuracy.\n\nAccuracy, like the F1 score, is one of the most popular choices for evaluating an ML model. It is highly utilized, especially among models built for non-complex tasks, due to its simplicity in calculating and interpretability. Accuracy represents the number of correct predictions divided by the total number of predictions. This simplicity, on the contrary, also brings many limitations to the metric. Accuracy is only effective when the dataset has a balanced class, as in a dataset with class imbalance, a model can achieve a good accuracy score simply by having all predictions as the majority class, resulting in ineffective training. Moreover, since accuracy only considers true positives and negatives, the metric lacks information that would otherwise be reflected in other metrics like precision and recall, which might lead to incomprehensive conclusions.\n\nMCC, ranging from -1 to +1, represents the correlation coefficient between the observed and predicted binary classifications. Since MCC takes into account all four categories of the confusion matrix (TP, FP, TN, and FN), MCC is considered a balanced metric, particularly useful for imbalanced datasets  [56] . An MCC of +1 indicates perfect predictions, 0 indicates a prediction capability equal to completely random, and -1 indicates complete disagreement between predictions and actual outcomes.\n\nAUROC (sometimes simply referred to as AUC) measures the ability of a binary classifier to distinguish between positive and negative classes across different thresholds. It is calculated by finding the area under the ROC curve, which plots the true positive rate (TPR) against the false positive rate (FPR). AUROC ranges from 0 to 1, with 1 indicating a perfect classifier, 0.5 indicating a completely random classification, and any value <0.5 indicating a worse capability than random guessing.\n\nCohen's kappa measures the agreement between two raters (in our case, the DL model and the labels) while taking into account the possibility of agreement occurring by chance. This metric is widely used to assess inter-rater reliability and in classification tasks with more than two classes. The kappa ranges from -1 to 1, with +1 indicating perfect agreement beyond chance, 0 indicating that agreement is no better than chance, and any value <0 indicating systematic disagreement.\n\nSoftware, Programming Language, and Libraries All data pre-processing, analysis, and evaluation were conducted and implemented using Python 3.10.12 and the TorchEEG library on Google Colab. Developed by Zhang et al.  [56] , TorchEEG is the first and one of the most widely used DL toolboxes for EEGbased emotion recognition, which was developed to aid researchers with EEG and ML research. TorchEEG separates the DL workflow into five modules: datasets, transforms, model_selection, models, and trainers, each with its own plugand-play functionalities for EEG-based emotion recognition. These modules incorporate state-of-the-art algorithms in the field, one of which we utilized for result comparison was the TSception model, as well as provide unique adaptations of DL models like transformers and diffusion models. Since the experimental protocol of DREAMER was implemented using the MATLAB environment  [54] , the SciPy library is used for loading the DREAMER dataset into the Google Colab notebook. We used the Scikit-learn library to implement some ML algorithms, which include logistic regression, random forest, and support vector machine, to do the same task with the DREAMER dataset for comparison. Our multi-scale CNN was implemented using PyTorch Lightning, a popular wrapper for PyTorch developed by Meta AI, and was heavily inspired by TSception  [7] . Compared to other commonly used libraries like Google's TensorFlow or Py-Torch, PyTorch Lightning has three strengths: (1) it minimizes boilerplate code for training loops, (2) it has a great advantage in distributed GPU training for scalability, and (3) it offers built-in features for model checkpoints, logging, and experiment tracking. These strengths allow developers and researchers to focus on the experimental aspects of a DL model, which is why we chose to utilize PyTorch Lightning for the development of this multi-scale CNN model.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Methodology",
      "text": "This section presents the mechanism behind our approach. In our EEG-based emotion recognition workflow, we first preprocess the raw EEG data so to increase the model performance, then the key features of the data are extracted using a multi-scale CNN, which is then finally passed into a classifier to perform emotion classification.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Preprocess",
      "text": "In EEG-based emotion recognition, preprocessing raw EEG data is one of the most essential steps for several reasons. Raw EEG signals often contain a lot of noise from body movement, electrical interference, and external environmental factors, and can be overwhelmed very easily by other artifacts  [30] . EEG signals can also vary across individuals due to the many differences, like head size. Thus, it is crucial to preprocess the raw EEG data from the DREAMER dataset. This would not only prevent the EEG data from being too noisy and complex but also allow the DL model to improve its reliability and computational expense and generalize well across different subjects, making it more suitable for real-world EEG-based emotion recognition applications  [57] . A high-level overview of the data preprocessing step is shown in Fig 9 . First, we subtracted the baseline signals from the emotional signals, then normalized the data using Z-score normalization; next, we converted the data from a MAT file to PyTorch tensor to a two-dimensional representation, converting all labels to a binary form of zero or one, and finally split the data in two ways: five-fold cross-validation and train-validation-test. After loading the DREAMER dataset into the Google Colab notebook environment using the SciPy library, all of the data preprocessing was done using built-in functions from TorchEEG's transforms module. First, the BaselineRemoval method was used to subtract the baseline signals from the experimental signals (or emotional signals). The baseline signals represent the electrical activity of the brain in a neutral or resting state with no elicited emotional state. In the DREAMER dataset, these signals are achieved by showing a neutral video clip used in the work of Gabert-Quillen et al.  [58] . By subtracting the baseline signals from the emotional signals, the noisy signals from background activities and signals that are unrelated to the emotional stimulus are removed, highlighting the brain's neural activity in response to the emotion-eliciting film clips. Moreover, every individual has a distinct neutral brain activity due to various factors, such as mood, cognitive state, and anatomical differences  [59] ,  [60] . Subtracting the baseline signals will allow our EEG-based emotion recognition system to better generalize across different sessions, individual users, and stimuli in real-life applications.\n\nNext, the MeanStdNormalize method was utilized to perform Z-score normalization on the EEG data. This normalization technique scales every value in a dataset so that the mean of all values is zero and the standard deviation is one. The formula for Z-score normalization is presented in the Equation:\n\nwhere Z is the calculated value, x is the original value, µ is the mean of the data, and σ is the standard deviation of the data. EEG typically consists of multiple channels, one for each electrode, that can have different baseline levels and variances. Z-score normalization ensures that signals from different channels and individuals are comparable. This allows the EEG-based emotion recognition system to be more robust, as well as helping ML and DL models to perform better and converge faster since the input features now have similar scales.\n\nFurthermore, we also used the ToTensor and the To2d methods to convert the EEG data to a PyTorch tensor, then to a two-dimensional EEG signal representation, with the electrode index as the row index, and the temporal index as the column index (an additional dimension is also appended only because Py-Torch requires an extra dimension to perform convolution on a two-dimensional tensor). This is a mandatory step for our model, as we utilized CNN for emotion recognition, which is designed to capture spatial patterns and features, thus the need to treat the EEG data as a two-dimensional representation. Moreover, by arranging the electrode index as the row index, the spatial relationship between electrode placements is preserved. This is beneficial to our model as the spatial configuration of the electrodes  [61] . With the temporal index as the column index, the temporal information is also preserved, which can be crucial for reallife applications, as EEG-based emotion recognition systems need to recognize emotional states that are unfolding over time.\n\nAfter that, we utilized the Binary method to convert all labels (the valence, arousal, and dominance scores) from a scale of 1-5 to a value of zero or one. As mentioned in Section 3.2, this conversion minimizes the effect of the possible variation for each participant in the evaluation of elicited emotion, helping the model generalize in real-life applications.\n\nFinally, we split the data in two ways: five-fold cross-validation and trainvalidation-test split. For five-fold cross-validation, we split the partitioned dataset into five nearly equally-sized subsets (or folds) at the dimension of trials, with four subsets being the training data and one being the test set. For the trainvalidation-test split, we applied the standard 80-20 split ratio twice, which means the ratio between the train, validation, and test sets is 64:16:20. The model's performance on each split method is evaluated separately. This is done to ensure that the performance evaluation of the model is as objective and comprehensive as possible.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Multi-Scale Convolutional Neural Network Model",
      "text": "The architecture of our multi-scale CNN model is inspired by TSception, developed by Ding et al.  [7] . We utilized multi-scale 1D convolutional temporal kernels (T kernels) in order to extract time-frequency representations and features in EEG data. This is important due to how rapid brain activities can vary as a person is being exposed to the stimulus, which can be seen from an example shown in Fig 10 . \nHowever, while TSception used the ratio coefficients of [0.5, 0.25, 0.125] from their hypothesis that multi-scale T kernels can better learn dynamic frequency representations in EEG, we decided to use the ratio coefficients of [0.5, 0.25, 0.125, 0.0625, 0.03125] so that the model can learn even more diverse representations. Although the higher-level T kernels with smaller ratio coefficients reduce the lengths of the convolutional kernels, allowing the model to learn more diverse representations, one limitation of this approach is that it can lead to the model being more computationally expensive and taking more time to run. Kernels with high ratio coefficients will learn low-frequency and long-term representations, and vice versa. In PyTorch Lightning, while there is no built-in method of implementing a one-dimensional convolutional kernel, we implemented this by setting the stride of the kernel to one. Average pooling is applied after every convolutional operation to prevent noise in the EEG data from affecting the performance of the model. All outputs of every level of T kernels would then be concatenated along the feature dimension. In the asymmetric spatial layer, TSception implemented two types of spatial kernels: a global kernel and a hemisphere kernel. Similar to their implementation, the global kernel has a size of (c, 1), where c is the number of channels, and in our case, with the DREAMER dataset, 14 channels. This is done so that the length of the kernel equals the channel dimension of the EEG data, allowing the kernel to learn global spatial information through the whole scalp. The hemisphere kernel has the function of extracting the features and the relations between the left and right hemispheres of the brain. The size and step of this kernel are both (0.5 • c, 1), which allows the kernel to learn the patterns of the two hemispheres without overlapping, essentially extracting key features from them separately. However, we expanded on this approach by implementing an additional type of spatial kernel, which, instead of learning the patterns from two sides of the brain separately, the kernel extracts information from four separate areas. Essentially, we used the same implementation approach to TSception, where we set both the size and step of this kernel (0.25 • c, 1), allowing the model to learn key neural features of the brain in four parts. For both the hemisphere and our novel kernel to be well applied, the sequence of channels in the input EEG samples needs to be arranged in a specific way so that the step of the kernel would extract information from the intended areas of the brain. In order to achieve this, we arranged the channels in the input EEG data in an anti-clockwise order.\n\nBefore the output of the multi-scale CNN is finally passed onto the fully connected layer to classify the emotional states, the learned information of the three types of spatial kernels is then fused in a high-level fusion layer. Here, we implemented the proposed approach of TSception to use a 1D convolutional layer with a kernel size of (8, 1) (changed from the choice of (3, 1) from TSception due to our implementation of an additional type of spatial kernel) to fuse the learned information along the spatial dimension.\n\nClassifier After the key information from the EEG data is extracted using the multi-scale CNN, it is then passed into a classifier to perform emotion classification, producing the final output. This classifier essentially works like the hidden and output layers of an ANN. For each score type, the number of classes in the output would be two, making it a binary classifier.",
      "page_start": 17,
      "page_end": 20
    },
    {
      "section_name": "Results",
      "text": "In this section, we report and compare the results of our multi-scale CNN model against the state-of-the-art TSception model in terms of precision, recall, F1 score, accuracy, MCC, AUROC, and Cohen's kappa, as well as the results of both models in five-fold cross-validation. Across these metrics, our proposed method consistently outperforms TSception, demonstrating a significant improvement in EEG-based emotion recognition with the DREAMER dataset. The results of each model across different metrics are presented in Table  3      As shown in Table  3 -5, when evaluating valence scores, our multi-scale CNN model outperforms TSception in all metrics; when evaluating arousal scores, our model outperforms TSception in every metric except recall; and when evaluating dominance scores, our model outperforms TSception in precision, accuracy, MCC, and AUROC. These results indicate that our model not only surpasses TSception in traditional metrics like accuracy and F1 score but also presents significant improvements in more nuanced metrics like MCC and AUROC. Moreover, as shown in Table  6 -8, even when comparing test accuracies from five-fold cross-validation, our method achieves substantial improvements from the results of the TSception model. When the average test accuracy of the five folds is calculated, our model shows better performance in all three valence, arousal, and dominance scores. Interestingly, for predicting valence scores, both these evaluation methods show that our model outperforms TSception in all metrics and in all five folds, indicating that our model is objectively better than TSception at recognizing valence in EEG-based emotion recognition. For the arousal score, while TSception presents a better result in recall, our model still obtained a higher F1 score, indicating a better balance between precision and recall, as well as a greater proficiency in identifying positive instances (when a person is feeling positive/pleased). Finally, for the dominance score, our model still outperforms TSception in most metrics, including precision, accuracy, MCC, and AUROC. Across all three valence, arousal, and dominance scores, our model consistently outperforms TSception in accuracy, MCC, and AUROC. For accuracy, our model improved TSception's performance in valence score by 0.89%, arousal score by 1.02%, and dominance score by 0.51%, indicating that our model was making more correct predictions overall. For MCC, our model outperforms TSception's coefficients in valence score by 0.0221, arousal score by 0.0068, and dominance score by 0.0003, suggesting that our model is better at handling both positive and negative classes well, even in imbalanced datasets. Lastly, our model achieves similar improvements across all three scores compared to the TSception model in AUROC, which reflects the ability to discriminate between",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Discussion",
      "text": "This study presented a novel approach to utilizing DL for EEG-based emotion recognition with visual-auditory stimuli in a consumer-friendly setting, predicting the emotional state of a person in the form of valence, arousal, and dominance scores. Inspired by the TSception model, we used five different ratio coefficients to allow our model to have more diverse representations in the EEG data than previous methods. Moreover, we expanded on the proposed model architecture of TSception, further using a similar approach to the model's hemisphere kernel to implement a new type of kernel that captures key features from four separate areas of the brain, while still retaining the global and hemisphere kernel from TSception. This approach results in our model consistently outperforming TSception in all valence, arousal, and dominance scores across multiple performance evaluation metrics, including five-fold cross-validation (test accuracy across five folds), precision, recall, F1 score, accuracy, MCC, AUROC, and Cohen's kappa.\n\nOur study also opens up many potential applications in different real-life contexts of EEG-based emotion recognition, as our model was trained on a dataset recorded using a consumer-grade EEG device.\n\nMost EEG-based emotion recognition studies have been using other widely used datasets like DEAP, SEED, and MAHNOB-HCI  [6] ,  [32] ,  [62] . These datasets, while providing more EEG data with higher quality, use research-grade EEG devices. The use of these devices prevents these studies from developing a system that can be more widely used in real-life contexts, as research-grade EEG devices are extremely costly and take a significant amount of time to set up. The DREAMER dataset recorded EEG data using the Emotiv EPOC system, a consumer-grade EEG system that takes way less time to set up and has more affordability while recording EEG data with satisfactory quality and reproducibility. By training on the DREAMER dataset, our model can better predict emotional states using EEG data from EEG systems with low cost and high user convenience, providing an inexpensive option for hospitals, clinicians, or mental health services with a low budget to install such applications.\n\nWith affordability and user convenience in mind, our EEG-based emotion recognition system may be used in a wide range of real-life applications. One of the prime examples of such applications would be monitoring patients' emotional state during a therapy session. In 2023, one in five people from Gen Z or Millennials (people who were born between 1981 and 2012) are currently treated with psychotherapy as it remains the most popular treatment method for mental disorders  [63] ,  [64] . As the communication and interaction between the therapist and the clinical patient is the most important aspect of a therapy session  [65] , therapy experience for individuals with cognitive impairments, especially Alzheimer's disease, becomes challenging due to them facing difficulties in communication and expressing themselves  [66] . Hence, there is a rising need for ways to aid the communication between the therapist and individuals with Alzheimer's disease in a therapy session. One method that can contribute to this purpose is to develop an automatic emotion recognition system that allows the therapist to understand what state of mind the patient is feeling. By monitoring the valence of a patient, the therapist can better understand their patient's feelings, detect early warnings of mental problems like depression or anxiety, and then tailor treatments accordingly. During a therapy session that focuses on the use of a stimulus, not only the valence, but the arousal and dominance indication of a patient may provide the therapist with more information regarding the efficacy of the activity and whether a stimulus is working as intended. Music therapy is an evidence-based therapeutic practice that uses music to address the physical, cognitive, and social needs of an individual  [67] . In an active listening activity in a music therapy session, where the patient would listen to a recording of a song, watch a music video, or a live performance of other patients and/or of music therapists. This activity would provide a perfect setting for EEG-based emotion recognition to come into use, as the music therapist would be able to observe whether the activity is eliciting the planned emotion in the patient. Furthermore, the arousal score indicated by the emotion recognition system would be able to suggest the efficacy of the current music therapy activity. Attention is a human's most basic resource at a psychic level  [68] , and thus, many studies exploring the factors affecting the efficacy of music therapy for individuals with dementia place an emphasis on capturing the patient's attention [69],  [70] . The arousal score, which presents the attention level of a person, would be a suitable indicator for the music therapist to adjust their activity in accordance with the score. Another useful application for this system would be in a group music therapy session, where tracking the emotional state of multiple patients at once may become challenging for the music therapist. In particular, the dominance score may be a good indication for the music therapist to recognize if there is a patient who is feeling left out or overwhelmed, which would be suggested by a low dominance score.\n\nOur study is not without its limitations and challenges, and a discussion regarding these limitations would open up future works and ideas. One major challenge that we faced during this study was the lack of data for consumerfriend EEG-based emotion recognition. While the DREAMER dataset was able to provide us with a satisfactory number of samples, the dataset only recorded data from 23 subjects. In the context of a real-life application for an emotion recognition system, it is essential that a model can perform well across many different subjects and different stimuli. For a ML model to achieve that, we hypothesized that it is better for the model to treat the whole recording of a subject with a stimulus as one sample. In the context of the DREAMER dataset, this would mean that the EEG signals recorded while one subject was watching a film clip would count as one sample. However, in the observed EEG datasets during this study, doing this would leave the ML model with too few samples to train on (e.g. the DREAMER dataset would have 414 samples, as it has 23 subjects watching 18 film clips), making convergence extremely unlikely. This is the reason why most EEG-based emotion recognition systems would treat the recorded EEG signals during one time window (this varies in accordance with the recording frequency of the EEG device). Thus, we believe that the field of EEGbased emotion recognition systems with real-life applications would massively benefit from an EEG dataset recorded using a consumer-grade EEG device with emotion evaluations.\n\nWe also encourage future works exploring EEG-based emotion recognition systems to experiment with more unique DL architectures. While the TSception model achieved quality results by designing their architecture to capture the key information from the brain's activity in the two hemispheres separately, our model managed to achieve consistently better results by separating the EEG data into four areas of the brain. Similar to TSception, our approach was inspired by the brain's anatomy, specifically the four lobes of the cerebral cortex: the frontal lobe, the parietal lobe, the temporal lobe, and the occipital lobe, even though the electrode placements in the DREAMER dataset did not allow us to design an architecture that would capture this information perfectly. Hence, we highly encourage future works to try novel model architectures, even if they are illogical or if the logic behind them is incomplete. Additionally, future works may explore the performance of our model in a dataset that would better fit for capturing the brain information in the four lobes of the cerebral cortex.\n\nThough this study focuses specifically on the real-life application aspect of EEG-based emotion recognition, we were unable to obtain permission to put the model into real-life use in a suitable setting such as during an active listening activity in a music therapy session. Since modern EEG devices are non-invasive and completely safe with extremely low health risks  [71] , we suggest testing out such EEG-based emotion recognition systems in a real-life setting in order to better evaluate the shortcomings and practicality of such systems.\n\nAs related fields like technology, AI, ML, DL, affective neuroscience, and EEG-based emotion recognition advance in the future, the development and application of these systems should be further explored for their wide range of potential utilization. We call for the creation of more EEG-based emotion recognition datasets that would simulate practical settings for the use of these systems in real life. We hope that this study succeeds in encouraging researchers to experiment with more architectures for such systems and to shift the focus to the development of them in real-life settings, providing easy access to hospitals, clinicians, therapists, and consumers. In the quest for further understanding of the great complexity behind human emotions and using them to improve quality of life, the development of real-life EEG-based emotion recognition systems would ameliorate countless lives.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we developed a DL model to predict an individual's emotional state using EEG signals using a multi-scale CNN approach inspired by TSception. By using more ratio coefficients for T kernels and creating a novel type of kernel that would capture the key information of EEG data from four separate areas of the brain, we consistently outperformed the results of the TSception model in all valence, arousal, and dominance scores across a wide range of evaluation metrics, including precision, recall, F1 score, accuracy, MCC, AUROC, and Cohen's kappa. This result indicates the potential of novel architectures in such systems as well as the need for testing such systems in a real-life setting. We call for further research to experiment with various types of model architectures, as well as the development of an EEG-based emotion recognition system using consumer-grade EEG devices with a considerable number of subjects and stimuli, allowing future development of these systems to better generalize in real-life applications.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Acknowledgement",
      "text": "I would like to thank my advisor, Gia Ngo from the School of Electrical & Computer Engineering, Cornell University, for his advice and mentorship in the development of this research paper.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: [26]. It is a two-dimensional model",
      "page": 4
    },
    {
      "caption": "Figure 1: Russell’s circumplex model of affect.",
      "page": 5
    },
    {
      "caption": "Figure 2: Russell’s and Steiger’s VAD model [28].",
      "page": 5
    },
    {
      "caption": "Figure 3: [35], [36]. The",
      "page": 6
    },
    {
      "caption": "Figure 3: The International 10-20 electrodes placement system (Source: [37]).",
      "page": 6
    },
    {
      "caption": "Figure 4: The 10-10 electrode placement system.",
      "page": 7
    },
    {
      "caption": "Figure 5: An example of an artificial neural network, produced using NN-SVG [49].",
      "page": 10
    },
    {
      "caption": "Figure 6: would be class labels.",
      "page": 11
    },
    {
      "caption": "Figure 6: An example of CNN [53].",
      "page": 11
    },
    {
      "caption": "Figure 7: After watching each film",
      "page": 12
    },
    {
      "caption": "Figure 8: , which is the signal plot for one second of recording taken randomly from",
      "page": 12
    },
    {
      "caption": "Figure 7: All sensor locations used in the DREAMER dataset are colored in red.",
      "page": 12
    },
    {
      "caption": "Figure 8: The signal plot for one second of recording taken from the DREAMER dataset.",
      "page": 13
    },
    {
      "caption": "Figure 9: Data Processing Overview.",
      "page": 17
    },
    {
      "caption": "Figure 10: However, while TSception used the ratio coefficients of [0.5, 0.25, 0.125]",
      "page": 18
    },
    {
      "caption": "Figure 10: A topographic map of the raw EEG signal across 0.75s.",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 1: A comparison of the 10-20 system and the 10-10 system.",
      "data": [
        {
          "Feature": "Number of electrodes",
          "10-20 system": "19 electrodes",
          "10-10 system": "Typically 64 electrodes"
        },
        {
          "Feature": "Resolution",
          "10-20 system": "Lower spatial resolution",
          "10-10 system": "Higher spatial resolution"
        },
        {
          "Feature": "Electrode placement",
          "10-20 system": "10% and 20% intervals",
          "10-10 system": "10% intervals"
        },
        {
          "Feature": "Common usage",
          "10-20 system": "Clinical EEG, routine\nmonitoring",
          "10-10 system": "Research, studies"
        },
        {
          "Feature": "Convenience",
          "10-20 system": "Easier to set up and\ninterpret",
          "10-10 system": "More complex, require more\nsetup time"
        },
        {
          "Feature": "Applications",
          "10-20 system": "Routine diagnostics, sleep\nstudies",
          "10-10 system": "Detailed brain mapping,\nresearch studies"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: The experiment summary of the DREAMER dataset [54].",
      "data": [
        {
          "Audio-visual stimuli": "18\nNumber of videos"
        },
        {
          "Audio-visual stimuli": "Audio-video\nVideo content"
        },
        {
          "Audio-visual stimuli": "65–393s (M=199s)\nVideo duration"
        },
        {
          "Audio-visual stimuli": "Experiment information"
        },
        {
          "Audio-visual stimuli": "Number of participants"
        },
        {
          "Audio-visual stimuli": "Number of males"
        },
        {
          "Audio-visual stimuli": "Number of\nfemales"
        },
        {
          "Audio-visual stimuli": "Age of participants"
        },
        {
          "Audio-visual stimuli": "Rating scales"
        },
        {
          "Audio-visual stimuli": "Rating values"
        },
        {
          "Audio-visual stimuli": "Recorded signals"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion and the human brain",
      "authors": [
        "A Damasio"
      ],
      "year": "2001",
      "venue": "Annals of the New York Academy of Sciences"
    },
    {
      "citation_id": "2",
      "title": "The emerging view of emotion as social information",
      "authors": [
        "G Van Kleef"
      ],
      "year": "2010",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "3",
      "title": "A review of emotions, behavior and cognition",
      "authors": [
        "K Ali"
      ],
      "venue": "Journal of Biomedical and Sustainable Healthcare Applications"
    },
    {
      "citation_id": "4",
      "title": "The sociology of emotions: Four decades of progress",
      "authors": [
        "E Bericat"
      ],
      "year": "2015",
      "venue": "Current Sociology"
    },
    {
      "citation_id": "5",
      "title": "Measurement of neural signals from inexpensive, wireless and dry eeg systems",
      "authors": [
        "T Grummett",
        "R Leibbrandt",
        "T Lewis",
        "D Delosangeles",
        "D Powers",
        "J Willoughby",
        "K Pope",
        "S Fitzgibbon"
      ],
      "year": "2015",
      "venue": "Physiological Measurement"
    },
    {
      "citation_id": "6",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2021",
      "venue": "arXiv"
    },
    {
      "citation_id": "8",
      "title": "What is emotion? 1884",
      "authors": [
        "W James"
      ],
      "year": "1948",
      "venue": "Readings in the history of psychology"
    },
    {
      "citation_id": "9",
      "title": "The james-lange theory of emotions: A critical examination and an alternative theory",
      "authors": [
        "W Cannon"
      ],
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "10",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "S Schachter",
        "J Singer"
      ],
      "venue": "Psychological Review"
    },
    {
      "citation_id": "11",
      "title": "Cognition and emotion",
      "authors": [
        "F Pons",
        "M De Rosnay",
        "F Cuisinier"
      ],
      "year": "2010",
      "venue": "International Encyclopedia of Education"
    },
    {
      "citation_id": "12",
      "title": "Emotion and action",
      "authors": [
        "J Zhu",
        "P Thagard"
      ],
      "year": "2002",
      "venue": "Philosophical Psychology"
    },
    {
      "citation_id": "13",
      "title": "Remembering the details: Effects of emotion",
      "authors": [
        "E Kensinger"
      ],
      "year": "2009",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "14",
      "title": "Editorial: Advances in emotion regulation: From neuroscience to psychotherapy",
      "authors": [
        "A Grecucci",
        "J Frederickson",
        "R Job"
      ],
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition involving physiological and speech signals: A comprehensive review",
      "authors": [
        "M Ali",
        "A Mosa",
        "F Machot",
        "K Kyamakya"
      ],
      "venue": "Studies in Systems, Decision and Control"
    },
    {
      "citation_id": "16",
      "title": "The neglect and importance of emotion at work",
      "authors": [
        "R Briner"
      ],
      "year": "1999",
      "venue": "European Journal of Work and Organizational Psychology"
    },
    {
      "citation_id": "17",
      "title": "The many meanings/aspects of emotion: Definitions, functions, activation, and regulation",
      "authors": [
        "C Izard"
      ],
      "year": "2010",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "18",
      "title": "An appraisal account of individual differences in emotional experience",
      "authors": [
        "P Kuppens",
        "E Tong"
      ],
      "year": "2010",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "19",
      "title": "A categorized list of emotion definitions, with suggestions for a consensual definition",
      "authors": [
        "P Kleinginna",
        "A Kleinginna"
      ],
      "year": "1981",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "20",
      "title": "How many emotions are there? wedding the social and the autonomic components",
      "authors": [
        "T Kemper"
      ],
      "year": "1987",
      "venue": "American Journal of Sociology"
    },
    {
      "citation_id": "21",
      "title": "Basic emotion questions",
      "authors": [
        "R Levenson"
      ],
      "year": "2011",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "22",
      "title": "What is meant by calling emotions basic",
      "authors": [
        "P Ekman",
        "D Cordaro"
      ],
      "year": "2011",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "23",
      "title": "The nature of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "24",
      "title": "Basic emotions, natural kinds, emotion schemas, and a new paradigm",
      "authors": [
        "C Izard"
      ],
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "25",
      "title": "Clarifying the conceptualization, dimensionality, and structure of emotion: Response to barrett and colleagues",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2018",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "26",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "27",
      "title": "The structure in persons' implicit taxonomy of emotions",
      "authors": [
        "J Russell",
        "J Steiger"
      ],
      "year": "1982",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition using a reduced set of eeg channels based on holographic feature maps",
      "authors": [
        "A Topic",
        "M Russo",
        "M Stella",
        "M Saric"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Where does eeg come from and what does it mean?",
      "authors": [
        "M Cohen"
      ],
      "venue": "Trends in Neurosciences"
    },
    {
      "citation_id": "30",
      "title": "Eeg denoising through a wide and deep echo state network optimized by upso algorithm",
      "authors": [
        "W Sun",
        "Y Su",
        "X Wu",
        "X Wu",
        "Y Zhang"
      ],
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "31",
      "title": "Eeg signal analysis: A survey",
      "authors": [
        "D Subha",
        "P Joseph",
        "R Acharya",
        "C Lim"
      ],
      "year": "2008",
      "venue": "Journal of Medical Systems"
    },
    {
      "citation_id": "32",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Y Roy",
        "H Banville",
        "I Albuquerque",
        "A Gramfort",
        "T Falk",
        "J Faubert"
      ],
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "33",
      "title": "Feature extraction from eegs associated with emotions",
      "authors": [
        "T Musha",
        "Y Terasaki",
        "H Haque",
        "G Ivamitsky"
      ],
      "year": "1997",
      "venue": "Artificial Life and Robotics"
    },
    {
      "citation_id": "34",
      "title": "Feature extraction from eegs associated with emotions",
      "authors": [
        "T Musha",
        "Y Terasaki",
        "H Haque",
        "G Ivamitsky"
      ],
      "year": "1997",
      "venue": "Artificial Life and Robotics"
    },
    {
      "citation_id": "35",
      "title": "Using the international 10-20 eeg system for positioning of transcranial magnetic stimulation",
      "authors": [
        "U Herwig",
        "P Satrapi",
        "C Schönfeldt-Lecuona"
      ],
      "year": "2003",
      "venue": "Brain Topography"
    },
    {
      "citation_id": "36",
      "title": "The ten-twenty electrode system of the international federation. the international federation of clinical neurophysiology",
      "authors": [
        "G Klem"
      ],
      "year": "1999",
      "venue": "Electroencephalogr. Clin. Neurophysiol. Suppl"
    },
    {
      "citation_id": "37",
      "title": "Eeg based cognitive workload assessment for maximum efficiency",
      "authors": [
        "R Shriram",
        "M Sundhararajan",
        "N Daimiwal"
      ],
      "year": "2013",
      "venue": "Int. Organ. Sci. Res. IOSR"
    },
    {
      "citation_id": "38",
      "title": "Pdms-based low cost flexible dry electrode for long-term eeg measurement",
      "authors": [
        "L.-F Wang",
        "J.-Q Liu",
        "B Yang",
        "C.-S Yang"
      ],
      "year": "2012",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "39",
      "title": "Validation of soft multipin dry eeg electrodes",
      "authors": [
        "J Heijs",
        "R Havelaar",
        "P Fiedler",
        "R Van Wezel",
        "T Heida"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "Soft, comfortable polymer dry electrodes for high quality ecg and eeg recording",
      "authors": [
        "Y.-H Chen",
        "M Beeck",
        "L Vanderheyden",
        "E Carrette",
        "V Mihajlović",
        "K Vanstreels",
        "B Grundlehner",
        "S Gadeyne",
        "P Boon",
        "C Van Hoof"
      ],
      "year": "2014",
      "venue": "Sensors"
    },
    {
      "citation_id": "41",
      "title": "Dry eeg electrodes",
      "authors": [
        "M Lopez-Gordo",
        "D Sanchez-Morillo",
        "F Valle"
      ],
      "year": "2014",
      "venue": "Sensors"
    },
    {
      "citation_id": "42",
      "title": "Multi-center evaluation of gel-based and dry multipin eeg caps",
      "authors": [
        "C Ng",
        "P Fiedler",
        "L Kuhlmann",
        "D Liley",
        "B Vasconcelos",
        "C Fonseca",
        "G Tamburro",
        "S Comani",
        "-Y Lui",
        "C.-Y Tse",
        "I Warsito",
        "E Supriyanto",
        "J Haueisen"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "43",
      "title": "3d printed dry eeg electrodes",
      "authors": [
        "S Krachunov",
        "A Casson"
      ],
      "year": "2016",
      "venue": "Sensors"
    },
    {
      "citation_id": "44",
      "title": "A novel dry-contact electrode for measuring electroencephalography signals",
      "authors": [
        "J Liu",
        "X Liu",
        "E He",
        "F Gao",
        "Z Li",
        "G Xiao",
        "S Xu",
        "X Cai"
      ],
      "year": "2019",
      "venue": "Sensors and Actuators A: Physical"
    },
    {
      "citation_id": "45",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "46",
      "title": "Subject independent emotion recognition using eeg and physiological signals -a comparative study",
      "authors": [
        "M Ramaswamy",
        "S Palaniswamy"
      ],
      "venue": "Applied Computing and Informatics"
    },
    {
      "citation_id": "47",
      "title": "Machine learning and deep learning",
      "authors": [
        "C Janiesch",
        "P Zschech",
        "K Heinrich"
      ],
      "venue": "Electronic Markets"
    },
    {
      "citation_id": "48",
      "title": "The perceptron: A probabilistic model for information storage and organization in the brain",
      "authors": [
        "F Rosenblatt"
      ],
      "year": "1958",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "49",
      "title": "Nn-svg: Publication-ready neural network architecture schematics",
      "authors": [
        "A Lenail"
      ],
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "50",
      "title": "Endto-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Automated emotion recognition based on higher order statistics and deep learning algorithm",
      "authors": [
        "R Sharma",
        "R Pachori",
        "P Sircar"
      ],
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "52",
      "title": "Deep continual learning for emerging emotion recognition",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "A high-accuracy model average ensemble of convolutional neural networks for classification of cloud image patches on small datasets",
      "authors": [
        "V Phung",
        "E Rhee"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "54",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "55",
      "title": "Eeg-based detection of emotional valence towards a reproducible measurement of emotions",
      "authors": [
        "A Apicella",
        "P Arpaia",
        "G Mastrati",
        "N Moccaldi"
      ],
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "56",
      "title": "Torcheegemo: A deep learning toolbox towards eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "57",
      "title": "Testing the feasibility of eeg signals for emotion recognition",
      "authors": [
        "J Li"
      ],
      "year": "2021",
      "venue": "Advances in Computer, Signals and Systems"
    },
    {
      "citation_id": "58",
      "title": "Ratings for emotion film clips",
      "authors": [
        "C Gabert-Quillen",
        "E Bartolini",
        "B Abravanel",
        "C Sanislow"
      ],
      "year": "2014",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "59",
      "title": "Baseline brain activity fluctuations predict somatosensory perception in humans",
      "authors": [
        "M Boly",
        "E Balteau",
        "C Schnakers",
        "C Degueldre",
        "G Moonen",
        "A Luxen",
        "C Phillips",
        "P Peigneux",
        "P Maquet",
        "S Laureys"
      ],
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "60",
      "title": "Hierarchical-variational mode decomposition for baseline correction in electroencephalogram signals",
      "authors": [
        "S Fathima",
        "M Ahmed"
      ],
      "year": "2023",
      "venue": "IEEE Open Journal of Instrumentation and Measurement"
    },
    {
      "citation_id": "61",
      "title": "Spatial-temporal feature fusion neural network for eeg-based emotion recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "62",
      "title": "Eeg-based emotion recognition: Review of commercial eeg devices and machine learning techniques",
      "authors": [
        "D Dadebayev",
        "W Goh",
        "E Tan"
      ],
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "63",
      "title": "2024 mental health outlook: Growing demand for therapy among gen z & millennials",
      "authors": [
        "T Psychology"
      ],
      "year": "2023",
      "venue": "2024 mental health outlook: Growing demand for therapy among gen z & millennials"
    },
    {
      "citation_id": "64",
      "title": "Public attitudes towards psychiatry and psychiatric treatment at the beginning of the 21st century: a systematic review and meta-analysis of population surveys",
      "authors": [
        "M Angermeyer",
        "S Van Der Auwera",
        "M Carta",
        "G Schomerus"
      ],
      "year": "2017",
      "venue": "World Psychiatry"
    },
    {
      "citation_id": "65",
      "title": "Therapeutic relationships in psychiatry: The basis of therapy or therapy in itself?",
      "authors": [
        "S Priebe",
        "R Mccabe"
      ],
      "year": "2008",
      "venue": "International Review of Psychiatry"
    },
    {
      "citation_id": "66",
      "title": "Aspects of communication in alzheimer's disease: clinical features and treatment options",
      "authors": [
        "M Woodward"
      ],
      "year": "2013",
      "venue": "International Psychogeriatrics"
    },
    {
      "citation_id": "67",
      "title": "Treating addiction with tunes: A systematic review of music therapy for the treatment of patients with addictions",
      "authors": [
        "K Mays",
        "D Clark",
        "A Gordon"
      ],
      "year": "2008",
      "venue": "Substance Abuse"
    },
    {
      "citation_id": "68",
      "title": "Materialism and the evolution of consciousness.,\" in Psychology and consumer culture: The struggle for a good life in a materialistic world",
      "authors": [
        "M Csikszentmihalyi"
      ],
      "year": "2004",
      "venue": "Materialism and the evolution of consciousness.,\" in Psychology and consumer culture: The struggle for a good life in a materialistic world"
    },
    {
      "citation_id": "69",
      "title": "Individual music therapy with persons with frontotemporal dementia",
      "authors": [
        "H Ridder",
        "D Aldridge"
      ],
      "year": "2005",
      "venue": "Nordic Journal of Music Therapy"
    },
    {
      "citation_id": "70",
      "title": "Describing the Subtle Factors that Influence Moments of Interactive Responses During Music Therapy Sessions for People with Late-stage Alzheimer's Disease and Other Related Major Neurocognitive Disorders: A Multiple Case Study",
      "authors": [
        "C Barwick"
      ],
      "year": "2014",
      "venue": "Describing the Subtle Factors that Influence Moments of Interactive Responses During Music Therapy Sessions for People with Late-stage Alzheimer's Disease and Other Related Major Neurocognitive Disorders: A Multiple Case Study"
    },
    {
      "citation_id": "71",
      "title": "Cortical generators of normal electroencephalographic waveforms: A literature review",
      "authors": [
        "P Ojha",
        "N Pandey",
        "S Singh"
      ],
      "venue": "Indian Journal of Clinical Anatomy and Physiology"
    }
  ]
}