{
  "paper_id": "2211.14609v1",
  "title": "Beamers: Brain-Engaged, Active Music-Based Emotion Regulation System A Preprint",
  "published": "2022-11-26T16:37:13Z",
  "authors": [
    "Jiyang Li",
    "Wei Wang",
    "Kratika Bhagtani",
    "Yincheng Jin",
    "Zhanpeng Jin"
  ],
  "keywords": [
    "Human emotion",
    "emotion regulation",
    "music recommendation",
    "emotion instability"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the increasing demands of emotion comprehension and regulation in our daily life, a customized music-based emotion regulation system is introduced by employing current EEG information and song features, which predicts users' emotion variation in the valence-arousal model before recommending music. The work shows that: (1) a novel music-based emotion regulation system with a commercial EEG device is designed without employing deterministic emotion recognition models for daily usage; (2) the system considers users' variant emotions towards the same song, and by which calculate user's emotion instability and it is in accordance with Big Five Personality Test; (3) the system supports different emotion regulation styles with users' designation of desired emotion variation, and achieves an accuracy of over 0.85 with 2-seconds EEG data; (4) people feel easier to report their emotion variation comparing with absolute emotional states, and would accept a more delicate music recommendation system for emotion regulation according to the questionnaire.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Given dramatic changes in lifestyles in modern society, millions of people nowadays are affected by anxiety, depression, exhaustion, and other emotional problems. The demand for proper emotional care is accumulating and accelerating at an increasingly rapid rate. Music, as being extensively studied in literature  Koelsch [2014] ,  Taruffi et al. [2017] ,  Siddharth et al. [2019] ,  Ehrlich et al. [2019] , is proven to be one of the most effective and accessible manners to evoke emotions and influence moods  Goldstein [1980] . And it has been seen and reported that people are spending more time listening to music over the past decades with the development of portable music players and streaming platforms like YouTube, Spotify, and iTunes AudienceNet  [2018] , Nielsen  Music [2017] . Inevitably, music shapes our life in various Figure  1 : Workflow comparison of existing systems based on the emotion recognition model (left) and our proposed system (right). The upper framework in the block is the training process, and the lower framework is the testing process. MAs is the acronym for Music Alternatives. ways, including helping people better focus their attention, releasing more dopamine for a better mood, exercising with more energy, and boosting our creativity.\n\nPeople's sensation towards a music piece is subtle and might be distinct every time he/she listens to it. Biometric signals are collected along with people's self-reporting to understand human emotional reactions towards music. With the development of less obtrusive, non-invasive EEG devices, EEG becomes the dominant modality for studying brain activities, including emotion recognition in human-computer interactions (HCI) studies  Mihajlović et al. [2014] . A large number of studies have explored the music-induced neural correlates of perception and cognition, and provided theoretical supports for the application of music-based emotion regulation. People are looking forward to the day when wearable EEG devices could bridge the gap between the theoretical studies and the applications that can benefit users in their daily life. However, more accessible, easy-to-use, and interactive approaches for effective emotion regulation and mental health are still largely under-explored.\n\nThe introduction of EEG-based emotion recognition models makes it possible that computers can be empathetic. Existing studies are primarily based on emotion recognition models, as shown on the left of Figure  1 . Some of them employ definitive approaches without users' self-reporting to provide user-determined feedback  Ali et al. [2016] ,  Adamos et al. [2016] ,  Ramirez et al. [2018a] ,  Sourina et al. [2012] , Moore  [2013] . However, the emotion recognition algorithms vary widely and will reach different interpretations for similar data  Reed et al. [2017] ,  Yoon et al. [2017] . Users would over-accept and defer to the feedback, even if it contradicts with users' own interpretations  Hollis et al. [2018] . Another drawback is the limited music choices. To work with the emotion recognition model, the system needs to label the music pieces based on users' evoked emotions. With the limited emotional states, a large music library would make their selection strategy ineffective and inefficient. And it's unfeasible to ask users to listen to a large number of songs to acquire labels. Some research asked users or professional music therapists to select music before the experiment. Other studies labeled music by the emotion expressed in music (\"music emotion\")  Jun et al. [2010] ,  Lu and Tseng [2009] ,  Juslin and Laukka [2004] , and then recommended a specific song by matching its emotion to the detected user's emotional state. Such a strategy assumes that the music-listening preferences and styles of an individual are always consistent with his/her emotional state at the moment. However, the music-listening styles of users are so varying  Ferwerda and Schedl [2014] , and their emotional reactions change over time and under different situations  Thoma et al. [2012] . And more importantly, emotions that a listener experiences are distinct from the judgments of what emotion is expressed in music  Lewis et al. [2010] , which was proved by the low match rate in our study.\n\nMost existing research uses discrete emotion models with states like 'happy', 'angry', and 'sad', or continuous models like Russell's circumplex model  Russell [1980]  for emotion evaluation. One problem is that systems relying on the emotion recognition models assume that the stimulus would elicit emotions effectively Alarcao and Fonseca  [2017] . Thus when the emotion is not successfully elicited, self-assessment would be tough, and the performance of such systems would be questionable. A more essential problem is that the detected emotional states only act as a reference and don't link the music and users in a relation. Under such circumstances, users can regulate emotions themselves by searching a playlist with a certain 'mood' on streaming platforms and switching songs referring to their emotional states. Given all the limitations and shortcomings above, it is argued that the conventional emotion recognition models are insufficient and unreliable for the design of emotion regulation applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Valence",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Arousal",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Astonished",
      "text": "Figure 2: Russell's circumplex model of affect Russell  [1980] . Locations of affects on the model correspond to their valence and arousal values. The y-axis represents arousal (intensity) and the x-axis represents valence.\n\nTo design an individual-oriented, truly personalized music-based emotion regulation application, we need to ask and address two questions: (1) Will the user like the chosen/recommended music? (2) Will this music emotionally affect the user in a way he/she wants? Although some existing music recommendation approaches have partially addressed these two questions by collecting information about users' profiles and preferences  Hu and Ogihara [2011] ,  Schedl et al. [2018] , there is still one essential question to be considered in terms of the complex and diverse variations of the user's emotion over time: What is the user's current emotional state and what will his/her emotional state be after listening to this song? With the advances of ubiquitous HCI technologies, it is necessary and also becomes possible to address these questions for understanding and handling human's ever-changing emotions.\n\nTo this aim, in this study, we propose an emotion regulation system that predicts the emotion variation (i.e., the emotion change after listening to a song compared with before) before any recommendation. In this system, the song and the user's current emotional state are two factors that decide how his/her emotion is going to be changed after listening to this song. Therefore, two independent variables: the music and the current EEG information, define and indicate the consequential emotion variation, as shown on the right of Figure  1 . Concatenations of each song selected in music alternatives (MAs) and the current EEG information are fed into the prediction model sequentially, then a music piece is chosen from MAs based on their predicted emotion variations.\n\nUnlike the definitive emotional state, emotion variation describes how much the emotion is changed. That means, it will be determined based on an individual's current emotional state, and will be affected or induced by emotional stimuli (listening to a song in our case). It requires less effort for users to report even when the emotion sustains, which is \"neutral\" or a close-to-zero value. And the model is able to deal with the situation that \"I'm feeling better but still sad,\" which is a positive variation but not a \"joyful\" state after listening to a song. Discrete emotion models with the classes like \"happy\", \"angry\", and \"sad\" are not suitable for the assessment of emotional variations. Thus we apply Russell's circumplex model with valence and arousal coordinates (Figure  2 ) for users' self-assessment, which is called valence-arousal (v/a) scores in this study. Yet, the challenge is that, continuous values of emotion variation are not realistic or feasible for models to learn; instead, discrete emotion classes are learned to represent people's typical feelings  Lin et al. [2010] ,  Baumgartner et al. [2006] . Thus, for the proposed emotion variation prediction model, we seek to simplify users' continuous v/a scores into four classes as the four quadrants in the valence-arousal model by positive/negative values, by which we predict the direction of emotion variations instead of the absolute variation amplitude. The training process concatenates EEG features with song features to train the binary valence and arousal models offline. In the testing process, a user needs to first designate his/her desired emotion variation, and their current EEG information will be collected. Meanwhile, music alternatives (MAs) are coarsely selected to narrow down songs for the prediction model, and are combined with EEG information to predict the direction of the user's emotional variation on the fly. Finally, a song from MAs will be chosen if the predicted class of v/a scores matches the user's designated v/a score.\n\nIn this paper, we aim to break the limitations of existing music-based emotion regulation systems and make the following contributions: (1) our approach bridges the gap between the theoretical studies and the practical and usable interactive systems for daily usage by proposing a dynamic emotion variation model, instead of using the conventional definitive emotion recognition methods; (2) based on the qualitative prediction of emotion variations, our system is able to recommend proper songs, being optimized towards both users' listening preferences and their desired change of emotion; and (3) the user's different emotion variations resulting from one song can be distinguished by the system, by which we evaluate the user's emotion instability and imply that it could be utilized as an indicator for mental health-related applications. Lastly, a simple questionnaire was developed and conducted to evaluate the user acceptance and usability of the proposed system.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Models",
      "text": "In affective science studies, the terms of affect, emotion, and mood are precisely differentiated. Affect is not directed at a specific event and usually lasts for a short time. Emotion is intense and directed, which is stimulated by some events or objects and also lasts for a short time. Mood lasts longer and is less intense  Liu [2017] . Unlike affective science studies, our study leaves aside the debate of whether emotions should be represented as a point in a dimensional valence-arousal space, as well as the distinction between affect, emotion, and mood.\n\nResearchers introduced various emotion models to conceptualize human emotion, among which discrete and dimensional models are employed mostly  Eerola and Vuoskoski [2013] . Discrete models, relating to the theory of basic emotions  Ekman [1992] , have been widely adopted because their concepts are easily explained and discriminated. However, studies, even including all variants of basic emotions, can hardly cover the whole range of human emotions. And some of the categories can be musically inappropriate when evaluating the emotion evoked by music or emotion expressed by music  Balkwill and Thompson [1999] . Dimensional models are represented by continuous coordinates, the most famous of which is the circumplex model Russell  [1980]  represented by valence and arousal. The circumplex model has received significant support in studies of emotions, cross-culture comparison, and psychometric studies  Posner et al. [2005] . Affects represented by four combinations of these two bipolar coordinates can be induced by musical pieces  Rickard [2004] ,  Vieillard et al. [2008] , and show unique neural patterns  Altenmüller et al. [2002] . Because of a more inclusive representation of emotions, the dimensional model has been extensively used in the Affective Computing (AC) research community",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Music Emotion",
      "text": "The inherent emotional state of an acoustic event (\"music emotion\" van der Schyff and Schiavio  [2017] ) is conveyed by a composer while writing it or a musician while performing it  Weninger et al. [2013] . And it can be classified by low-level descriptors (LLDs), such as zero-crossing rate, Mel-frequency cepstral coefficients (MFCC), spectral roll-off, and so on. Extensive prior studies have been performed and dedicated to the development of automatic Music Information Retrieval (MIR) systems. Functions for calculating acoustic features were packed in toolboxes such as the MIR toolbox  Lartillot and Toiviainen [2007] , openSMILE  Eyben et al. [2013] , and librosa  McFee et al. [2015] .\n\nDatabases with music emotion have been developed for MIR competitions, such as the annual music information retrieval evaluation exchange (MIREX) that started in 2007  Downie et al. [2008] . However, they introduced five discrete emotions in the task instead of assigning a continuous affect model. For a better understanding of the music effects psychologically, we referred to the 1000 songs database  Soleymani et al. [2013] , where the annotations of all music emotions were done by crowdsourcing on the Amazon Mechanical Turk, based on Russell's valence-arousal model. Low-quality and unqualified annotators have been removed to guarantee the quality of all annotations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "Recognizing people's extensively varying induced emotions is demanding and challenging. A variety of physical and physiological information have been employed to estimate human emotions in the past two decades, such as facial expressions  Black and Yacoob [1995] ,  Essa and Pentland [1997] , heartbeats  Valenza et al. [2013] , speech  Dellaert et al. [1996] ,  Nwe et al. [2001] , body gestures  Shibata and Kijima [2012] , and EEG  Takahashi et al. [2004] ,  Lin et al. [2010 ], AlZoubi et al. [2009] . EEG is a non-invasive electrophysiological technique that uses small electrodes placed on the scalp to record brainwave patterns. With the advances in Brain-Computer Interface (BCI) technologies, EEG signals can be collected from wearable devices with one to hundreds of channels  Gui et al. [2019] . For example, Emotiv EPOC+ neuro-headset with 14 channels was proven to be user-friendly and effective for mood induction studies Rodriguez  Ortega et al. [2013] .\n\nNowadays, it has been well studied and recommended that, as a common practice, the power spectra of the EEG can be divided into five bands -delta (δ: 1-3 Hz), theta (θ: 4-7 Hz), alpha (α: 8-13 Hz), beta (β: 14-30 Hz), and gamma (γ: 31-50 Hz)  Mantini et al. [2007] . The band-related brain activities have been explored to represent various emotions  Dasdemir et al. [2017] , including the asymmetry at the anterior areas of the brain Schmidt and Trainor  [2001] .\n\nSelf-reporting is still the most traditional and common way to obtain the subject's emotional states; but it has been long questioned because of the subjectivity and instability of each individual viewpoint. Self-Assessment Manikin (SAM) Bradley and Lang  [1994]  was highlighted to assist the self-assessment of emotions in some projects  Nie et al. [2011] ,  Daly et al. [2015] ,  Khosrowabadi et al. [2010] . However, it is still challenging for most subjects to accurately assess and label their definitive emotional states. Instead, emotion variations -how does an event or stimulus change the emotion from the last state, either positively or negatively -is easier for subjects to evaluate and describe, which hence are selected as the benchmark in our study.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion-Oriented Music Therapy And Recommendation",
      "text": "Emotion-oriented music therapy helps people achieve a \"delighted\" emotional state (discrete model) or get a higher valence (continuous model). Ramirez et al. introduced a musical neuro-feedback approach based on Schmidt's study to treat depression in elderly people  Ramirez et al. [2015] , aiming to increase their arousal and valence based on the v/a model. The same approach was also applied to palliative care of cancer patients  Ramirez et al. [2018b] . Both studies indicated that music could indeed help patients modulate their emotions to positive arousal and positive valence and thus improve their quality of life. Earlier, Sourina et al. proposed an adaptive music therapy algorithm based on subject-dependent EEG-based emotion recognition, and detected the satisfactoriness of subjects towards the therapy music in real-time  Sourina et al. [2012] . They designed the recommender for six discrete emotions in Russell's v/a model, and participants can assign those emotions to the system for getting the corresponding music pieces, similar to the workflow shown on the left side of Figure  1 . One potential limitation of all those methods is that they all interpreted the user's emotions through a predefined algorithm (e.g., the difference of the alpha power between the left and right scalp to represent the subject's valence level) rather than personalized self-assessment. Another problem is that they acquired feedback from the users after they listened to the music. The predefined algorithm itself was calculated by the collected EEG thus the result could only be an indicator, rather than the benchmark. Instead, self-assessment with the participant's volitional involvement is the most reliable benchmark.\n\nMany other studies have been carried out to recognize music-induced emotions by physiological signals like electrocardiogram (ECG), heart rate, blood pressure, and respiration rate  Lin et al. [2010] ,  Agrafioti et al. [2011] . However, none of the prior studies has included people's current emotional state regarding the decision to make music recommendations. People are constantly changing over time, and the interpretation of musical emotion varies among different listeners  Soleymani et al. [2013] . Even though people's previous data was collected, music similarity and emotion analysis was conducted  Zhu et al. [2006] , or the dynamic model of users' preference was built  Rafailidis and Nanopoulos [2015] , the event of listening to music is never truly connected with people's emotional state in their equations. We believe that a system should be delivered by the functions of both user and music to benefit users' emotions in an interactive process.\n\n3 Data Collection",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Self-Assessment",
      "text": "Self-assessment/reporting approaches are the most common way to evaluate emotions and serve as the ground truth.\n\nOur application demands users to assess their emotion variations induced by a song on Russell's circumplex model, which is marked as the evaluated valence/arousal (v/a) scores as shown in Figure  1 . Participants were instructed to move two sliding bars ranging from -5 to 5 continuously with scores shown on the bottom after introducing the meaning and difference of \"valence\" and \"arousal.\" The vertical sliding bar is for the arousal score, and the horizontal sliding bar is for the valence score. The window of sliding bars showed up right after the subject listened to each song, and the window also showed up at the beginning of the testing experiment for subjects to designate their desired emotion variations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Music Database",
      "text": "Music pieces were selected from the \"Emotion in Music Database (1000 songs)\"  Soleymani et al. [2013] , which contains 744 songs (excluding redundant pieces). Songs were from eight genres in general: Blues, Electronic, Rock, Classical, Folk, Jazz, Country, and Pop, and were cut into 45-seconds excerpts with a 44,100 Hz sampling rate. The emotion expressed in each song was evaluated on Russell's circumplex model by a minimum of 10 workers found and selected through Amazon Mechanical Turk (MTurk). They both annotated static and continuous emotions expressed in songs. However, the excerpt provided to the user is considered as a variable overall in this paper -we only predict users' emotional variation after listening to each excerpt, thus we only employ those static annotations as their v/a annotations.\n\nTo reduce the experiment load and verify the feasibility of the proposed system at the preliminary stage, we provided the first 25 seconds of the 45-seconds excerpts to human subjects, which was long enough to induce listeners' emotions  Juslin and Sloboda [2001] . Values of v/a annotations of each music excerpt range from one to nine. We re-scaled them to positive and negative values by minus five, which resulted in four classes/quadrants for coarsely selecting MAs in testing experiments. It should be mentioned that, the v/a annotations here represent the emotion expressed in music, which is decided by the composer and entertainer. In contrast, the v/a scores represent the emotion variations evaluated by users after they listen to a music piece.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Participants & Procedure",
      "text": "A total of five participants (4 males; aged 22 to 31 years) were recruited to participate in the experiment, and there is no clinical (including psychological and mental) problem reported by those participants. Participants were labeled as s01 to s05, respectively, and the model was built for each participant individually. The training and testing processes were different: the former one was to collect data and build the prediction model offline, and the testing process was an online process that needs to collect and analyze the data in real-time to recommend music. To testify the robustness of the system towards user's varying emotional states, we split the experiment into 13 days: six days of training experiments, two days of testing experiments, and the other five days of the real-life scenario evaluations that were modified from testing experiments. In addition, 54 people, including the five participants, were invited to participate in an online questionnaire study. They were asked to respond to 10 questions according to their own experiences independently and anonymously. The responses were used to investigate the public's demand and acceptance of emotion regulation with an intelligent music recommendation system and wearable EEG device. All of the experiments were approved by the Internal Review Board (IRB) at [University is hidden for double-blinded review].\n\nParticipants were asked to understand the arousal and valence emotions respectively, with the help of the discrete emotions labeled in Russell's model as shown in Figure  2  before the experiment. And there is no restriction regarding their normal music listening experiences or the emotional state before an experiment. As a factor of music effects, music volume was adjusted by participants to their habitual level before the experiments.\n\nTo build the prediction model offline, songs in the training process were chosen randomly from each quadrant based on their v/a annotations. A trial of a training experiment had three sequential parts: EEG data collection (20s), music playing (25s), and users' self-assessment (15s). EEG data collected and a music piece played in each trial formed an observation in training data, and users' self-assessment serves as the ground truth. Considering the comfortable wearing time of the Emotiv EPOC+ device is about half an hour, we designed twelve trials (each trial lasts for one minute) for each experiment, in which twelve songs were from four quadrants evenly, and two experiments for each day. Plus five-minute EEG device setup and three-minutes rest between two experiments, in total 32 minutes were required for one subject each day. EEG data were collected at the Resting State with Open Eyes (REO), and the next trial started right after the self-assessment. It is possible that no song in MAs matches the participant's desired emotion variation. If so, the system will go back to step (2) and repeat the following steps until a new song matches. The maximum iteration was set to five to avoid the system delay, and a song from the fifth iteration will be randomly selected if still no song matched. In the case when several songs matched, the system randomly selected one. Thus, the time of each trial in testing experiments was unfixed. And the proper time length of EEG collection between playing music pieces will be decided by referring to the classification accuracy of training experiments with different time windows of EEG data.\n\nTesting experiments were modified to accommodate the requirements of a more usable system in the real-life usage scenario. In this scenario, users only designated the desired emotion variation once, and EEG data were collected once at the beginning of the system usage to reduce users' active interaction efforts. And the \"current\" information from users was used to select a list of songs instead of one song. We assumed that the desired emotion variation would sustain, and one-time EEG data would be representative of users' current state within one experiment. To verify this hypothesis, we calculated and compared the variance of EEG features extracted from the training sessions within one experiment (intra) and two experiments (inter). The inter-test was combined with the tail half and head half trials of two continuous experiments to guarantee the same number of trials as the intra-test. We drew the mean-variance of intra-and inter-experiments for each participant, as shown in Figure  4 . It is shown that, on average, for five participants, 89.5% of 40 EEG features in intra-experiments have lower mean variance than the ones in inter-experiments. We thus assume that, with reasonable variance, EEG information is representative of users' \"current states\" and one-time EEG collection would work properly within twelve pieces of music (one experiment) for real-life scenarios. Tempogram represents the local auto-correlation of the onset strength envelope, and thus holds an important position for its rhythm information. In the case of roll-off frequency, different percentages refer to the roll percent of the energy of the spectrum in that particular frame. We chose four different values of the roll-off percentage to approximate the maximum and minimum frequencies in different ranges. The music feature matrix was normalized before further processing. And the feature selection and reduction were processed through the following steps: (1) removing constant columns which have 100% same values;\n\n(2) removing quasi-constant features with threshold 90%;\n\n(3) removing columns with the Pearson correlation greater than 0.92. The number of extracted features is 1,665, and the number of selected features after the above steps is reduced to 174.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Eeg Feature Extraction",
      "text": "Emotiv EPOC+ provided 14 channels of EEG data (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, and AF4) with 128 Hz sampling rate. Raw EEG data were first high-pass filtered by 8 Hz and then low-pass filtered by 49 Hz through EEGLAB Delorme and  Makeig [2004] , as alpha (8-13 Hz), beta (14-30 Hz), and gamma (31-50 Hz) power bands are proven to be relevant with emotion  Kim et al. [2013] . Noises of eye blinks (around 3 Hz) were rejected by the high-pass filter, and high-frequency noises like eye movements, muscle activity, and environmental noise were rejected by the low-pass filter. The artifact was rejected by implementing \"Reject continuous data\" on EEGLAB, which used the spectrum thresholding algorithm to detect artifacts. The artifact channels were detected by implementing automatic channel rejection, which used the joint probability of the recorded electrode with \"kurtosis\" and returned the measured value for each channel, and we rejected channels with measure values higher than 20.\n\nBefore feature extraction, we defined three time windows in order to determine the proper time length for EEG data collection in online testing experiments. To accommodate the lowest frequency of interest, the half cycle should fit comfortably within the window Budzynski et al.  [2009] . Normally 0.5 Hz is taken as the low end, which mandates a time window of at least one second. And we double the low end to 2 seconds as some data with artifacts would be rejected by pre-processing. The other two window lengths are 5 and 10 seconds, respectively.\n\nFeatures were extracted based on three window lengths of EEG data separately. Fast Fourier Transform (FFT) was employed with 3, 2, and 1 point moving-average corresponding to the time windows of {10, 5, 2}. Then we re-grouped the frequencies into four bands: alpha (8-12 Hz), beta-1 (12-16 Hz), beta-2 (16-32 Hz), and gamma (32-48 Hz) bands.\n\nThe first set of features are power differences of each band between the right and left channels: AF4/AF3, F8/F7, F4/F3, and FC6/FC5, because differences between the right and the left frontal lobe were proved to be related with valence in many research Henriques and  Davidson [1991] ,  Davidson [1992] , Allen et al.  [2004] . The second set of features is the band power of electrodes placed on the temporal lobe: T7, T8, and the neighboring sensors: P7, P8, for the reason that the temporal lobe was directly related to the region where the sound is processed  Gloor and Guberman [1997] . The last set of features is the mean and standard deviation (std) of the power of all channels. Thus the total number of EEG features is 40 (10 × 4), and they were normalized feature-wise before being combined with music features.\n\nSequential Backward Selector (SBS) was utilized to decrease the dimension of music features before being concatenated with EEG features in order to balance the amounts of music and EEG features. SBS is a greedy search algorithm removing the feature one by one to maximize the performance until reaching the number of features that were designated. Three time windows {10, 5, 2} of EEG features were concatenated with selected features of the music piece in the same trial separately. Features for the arousal model and valence model were selected by SBS independently from concatenated features for each participant.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Classification Approach",
      "text": "Before testing, training data was used to build the classification model and decide the optimal time length for EEG data collection. As we discussed, predicting continuous emotion variation values is impractical and infeasible. For example, [-0.1, -0.1] and [0.1, 0.1] are two v/a scores, they are close in a regression model but distinct in terms of the direction of emotion variation. The former one would mean that \"the music makes me kind of blue\" (3rd class), and the latter one means that \"the music makes me slightly delighted\" (1st class). Besides, neither the value of the designated v/a score nor that of the evaluated v/a score could the participant be firmly certain about. Instead, participants are more confident about the direction or range of valence and arousal changes.\n\nWe chose the binary classification for valence and arousal models because the boundary of emotions on the continuous model is undefined. The problem appears for two close v/a scores when another hard boundary is defined other than xand y-axes. For example, if a line separated two relatively far emotions \"Aroused\" and \"Happy\" in the first quadrant, then it would also distinguish two similar emotions near the line into two different classes. Support Vector Machine (SVM) was employed due to its great generalization capability, computational efficiency, and proven performance for EEG-based emotion classification  Bazgir et al. [2018] . The formation of the arousal model is shown in Equation  1 :\n\nwhere y a represents the binary class of arousal, and C a is the parameter vector of the arousal model. Selected feature vectors of EEG and music for the arousal model are represented as E a and M a and they are concatenated together to learn the emotion variations. The valence model is expressed with the same equation. To manage the unbalanced observations collected for training, the regularization parameter was adjusted by the ratio of their binary labels. It is worth pointing out that, since the data collection of the testing process was separated from the training process, all of the observations collected in training experiments were only used to train the model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotional Instability",
      "text": "In addition to predicting the direction of valence and arousal changes, a secondary study of this research is related to participants' emotional instability. There were 25% -35% of songs that reappeared two to five times for each participant in different training experiments. And their varying feelings towards these songs were employed to quantify users' emotional instability. People listen to different songs at different times in real life, thus we set a range for the repeated songs and the repeating times when randomly selecting songs in training experiments. And we assume the same number of repeated songs and repeating times for all participants will mislead us with highly biased results and can't draw a general conclusion of whether the emotion instability scores are indicative in future usage.\n\nTo quantitatively represent and assess the emotional instability, the number of transitions of arousal/valence scores was counted. The arousal or valence scores of a song have two states, i = 0 (negative) or i = 1 (positive). We propose that the frequency of transitions of 0 to 1 and 1 to 0 towards the same song reveals the user's emotional instability. For example, the arousal score vectors [0, 0, 0, 1, 1] and [0, 1, 0, 1, 0] of a song have the same probability of 0 and 1, while the first score vector tends to result from a change in the subject's taste after listening to this song for three times, which is less related to the emotional instability compared with the second subject. We define the frequency of transitions as t-score and it is calculated following the Equation  2 :\n\nwhere the score vector s has the binary pattern, thus the summation of the absolute difference of two neighboring elements is the number of transitions. N is the number of times the song with id = m is listened to by a subject. M is the total number of repeated songs. By implementing the equation, the results of the two vectors mentioned before are 0.25 and 1.0, respectively, which is consistent with our assumption that the latter subject shows a higher level of emotional instability than the former subject.\n\nTo verify the proposed emotional instability function from a psychological perspective, we refer to the Big Five Personality Test Goldberg  [1992]  based on participants' self-reporting and psychological test on Open-Source Psychometrics Project OSP. Big Five Personality Test is based on the five-factor model of personality  McCrae and Costa Jr [1999] , including openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism. Among all five factors, neuroticism is referred to in our study because it describes emotional stability. Participants were required to finish the Big Five Personality Test within ten minutes and their respective scores of neuroticism were collected. The score is defined in a way as what percent of other people who have ever taken this test and performed worse than you; that is, a higher score indicates a more stable emotional state. We compared the t-score and the neuroticism score among all participants to verify their correlation level and thus prove that our study can be used as an indicator of an individual's emotional instability. To correspond to the instability scales calculated by Equation  2 , the neuroticism score is rewritten as (1 -score/100).",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Evaluations",
      "text": "Training accuracy of the valence and arousal models are shown by cross-validation separately. To validate the interactive performance of the system between the user and recommended music more directly, testing accuracy is decided by both valence and arousal models: the match rate between the designated v/a score and the evaluated v/a score. In addition, to explore the feasibility of using fewer EEG electrodes, we selected electrodes on the temporal lobe region: T7 and T8, and evaluated the training accuracy solely based on these two sensors. Lastly, we evaluated the performance of the real-life scenario, which was carried out two months after the original testing experiments for five continuous days to meet participants' schedules. The same five participants were recruited and their models were trained by all the data collected before. Meanwhile, participants listened to new & old songs (which were listened to in the training and testing experiments or not) from the same database. The accuracy was evaluated using the match rate as the testing experiments, and the emotional instability score was updated with new data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Data Collection & Feature Selection",
      "text": "Trials in training and testing experiments with rejected channels are excluded, and the remaining number of trials are shown in Table  2  as (#Train_song and #Test_song). Each trial represents an observation that contains the collected EEG data and the song presented to participants. The first three rows of Table  2  are the statistics from training experiments.\n\n#Uniq_song shows the number of songs without any repetition. Match_rate refers to the percentage of music pieces that have the same v/a annotations (the emotion expressed in the song) with users' evaluated v/a scores (the emotion evoked by the song). The mean match rate of five subjects was 0.4938 ± 0.0504, which was close to 0.5, we arguably conclude that recommending music based on only music v/a annotations for user's emotion variation resembles a random occurrence. To show the match rate visually, we plotted the training data of s01 in Figure  6 , in which each data point represents a song that is located by its v/a annotations, and the marker represents the class of evaluated v/a scores. It shows that the participant's emotional states could be varied to all four classes by songs from the same quadrant, and the song with overlapped markers means that it can vary the participant's emotions oppositely in different experiments.\n\nThe last three rows of Table  2  are the statistics from testing experiments. #New_song is the number of songs that have never been listened to in the training experiment (new to the prediction model). The match rate calculated in testing experiments is used to evaluate the system performance and called (#Test_accu to distinguish it from the match rate of training experiments.\n\nThe number of EEG features and music features are 40 and 178 after feature extraction. Fifty music features were eventually selected by SBS based on v/a annotations before being concatenated with EEG features. Considering the limited number of observations we obtained, the final number of features was set to 25 by implementing SBS on the concatenated features for both valence and arousal models. The remaining EEG and music features for the valence and arousal models of each participant are different. The remaining number of EEG features in arousal model is  [6, 8, 10, 13, 7] , and in valence model is  [8, 10, 11, 5, 14]  for participants from s01 to s05. And the first 25 most significant features would change if we re-train the model with new data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Classification Model",
      "text": "The arousal model was trained by {10, 5, 2} seconds of EEG data separately to select the proper time length for testing experiments. The result of the 7-fold cross-validation for the arousal model is shown in Figure  5 . The F-value and P-value for the classification accuracy of three different lengths are 0.321 and 0.726, which cannot reject the null hypothesis that the distributions under these three conditions are normal, thus there is no time length better or worse in   this experiment. Even though the longer, the more EEG information is contained, the window lengths of 5s and 10s don't outperform 2s for representing users' current emotional state. Thus, we chose the shortest time length for testing experiments. The same conclusion was obtained for the valence model.\n\nBesides, we calculated the classification accuracy of training experiments with only two channels (T7 and T8) and compared it with 14 channels to verify the possibility of using only a small number of electrodes. In addition to the 8 EEG features from T7 and T8 channels, we selected the 11 most significant features of music by SBS and concatenated them with EEG features. The results are shown in Table  3 . With a slight decrease in classification accuracy in both valence and arousal models, the system retains a reasonable level of accuracy with only 2 EEG channels, even though they contain 7 times less information than the original 14 channels.\n\nTesting accuracy in the last row of Table  2  was calculated as the match rate of the designated v/a score and the evaluated v/a score. One of five songs in MAs was selected and presented to the participant. Users' EEG changes every moment, thus songs that were filtered out from MAs were unable to be known whether it was matched or not at that moment. Therefore, the false negative rate and the true negative rate of our system could never be calculated and they both are insignificant to the system function. The true and false positive rates correspond to the Test_accu and (1 -T est_accu).  The accuracy of recommending new songs is significant for the Test_accu because it's the new observation for the prediction model. And there were (#T est_song -#N ew_song) old songs that were chosen for each participant in testing experiments, among which some songs had various influences in the training process but matched the user's designation in the testing process. The match rate of old songs is 100% for participants except for one incorrect prediction for s02, which was listened to by s02 three times and resulted in two different classes of the evaluated v/a scores. It turned out that the system might recommend incorrect old songs with the user's changing tastes and open EEG information. However, it could be solved by updating the prediction model with new data.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Emotion Instability",
      "text": "The t-score calculated by Equation  2 for the arousal and valence models are shown as t_Arousal and t_Valence in Table  4 . And the collected neuroticism scores from the Big Five Personality Test are shown in Big-5. The correlation coefficient between t_Arousal and Big-5 is 0.808, and between t_Valence and Big-5 is -0.473. We also found that the participant who had the highest t_Arousal received the highest Big-5 score, and the participant with the lowest t_Arousal received the lowest Big-5 score. Besides, participants s01 and s03 had the closest t_Arousal and received the same Big-5 score. The p-value between t_Arousal and Big-5 is 0.073, which is bigger than 0.05 but statistically not random. However, the participants' pool is too small and the songs chosen in experiments are limited, from which we cannot make a general conclusion about the relation between t_Valence and Big-5 score. Here we arguably hypothesize that the t-score can be a referable indicator in emotion-based applications.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results Of Real-Life Scenarios",
      "text": "The data collected in training and testing experiments were used to train the emotion variation prediction model of the first day in real-life scenarios. And the model was then updated each day with new data collected, as well as the emotional instability values. There were two experiments for each day; at the beginning of each experiment, participants designated their desired emotion variation direction and stayed still for EEG collection. A list of 10 to 13 songs was selected by steps (2) to (5) of testing experiments iteratively and played. The participants evaluated their v/a scores after listening to each song.\n\nResults were shown in Figure  7  for every participant. Day 0 showed the results of testing experiments. The dark cyan line is the testing accuracy of the day. Emotional instability (t-score) was re-calculated with new data and plotted by the yellow line. The proportion of new songs influences the system performance and t-scores besides participants' ever-changing EEG data, thus we also calculated and plotted the ratio of new songs of that day with blue bars for system performance analysis.\n\nThere are several drops of testing accuracy in Figure  7 , including the mismatch of new songs and old songs. When the drop in accuracy was accompanied by the increase in emotional instability (e.g., day 3 of s02 and s03; day 2 of s05), it demonstrated that the participant gave different v/a scores to the old song compared with the last time when he listened to it. However, the emotional instability value was mostly averaged by consistent scores of the repeated songs given by participants. The mean t-score of five days for each participant is [0.206 ± 0.0009, 0.118 ± 0.0002, 0.168 ± 0.0002, 0.241 ± 0.0017, 0.106 ± 0.0002]. All participants have lower instability values as we expected except for a subtle increase for s05, who has the lowest emotional instability among them. The small variance of the emotional instability value demonstrates that the songs chosen for participants were not biased, otherwise, it will change dramatically over time. The mean match rate of five days for each participant is [0.825 ± 0.0054, 0.922 ± 0.0046, 0.794 ± 0.0015, 0.973 ± 0.0013, 0.862 ± 0.0035]. The accuracy is challenged by open EEG data, multi choices of user's designation, and new songs, but remains above 0.8 and guarantees a satisfactory emotion regulation experience with four choices of emotion variations.\n\nDifferent from testing experiments where participants were suggested to designate different classes of emotion variation to cover all four classes, they are free to designate their desired emotion variation direction in real-life scenarios and they all designated the first and fourth classes (positive valance changes). It is not surprising that people barely want to change their valence negatively, especially in the experimental environment. But we speculated that people would choose to decrease the valence when they feel hilarious but need to be more \"calm\" and \"serious\". And we didn't build this system by assuming people's specific styles of changing their emotions. Meanwhile, the prediction model is challenged by the unbalanced data collected from real-life scenarios. Even though the regularization parameter is updated with the ratio of binary labels, the prediction accuracy would be degraded with more unbalanced observations. The results from five continuous days of real-life scenarios are insufficient to reveal the long-term performance with more unbalanced data for the missing participants' negative designation of valence. The strategy that makes up for insufficient coverage of all emotional classes and doesn't require users to listen to what they don't designate should be discussed in further studies.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Summary Of Questionnaire",
      "text": "We created a questionnaire on Google Forms with ten questions about users' experience of using music to regulate their emotions. 54 subjects responded, including the five participants in our experiments, in which 38.8% were females, 79.6% of subjects aged from 20 to 29 years, and 18.1% aged above 30 years. For the basic question of how much they like music, from 5 (very much) to 1 (not at all), 50.9% like it very much, 22.6% like it (score 4) and 22.6% chose 3 (moderate). Among all subjects, 86.5% have experienced different feelings towards the same song (without considering familiarity). The question about how well they think the recommendations of a music app could match their tastes & moods was answered from 5 (very well) to 1 (not at all), 46.7% subjects gave it a 3 and 17.8% were unsatisfied (score 2 & 1). When asking subjects how annoyed they are when listening to a music piece that they don't like but are recommended by music apps, 86.7% of subjects were annoyed to a varying extent, and the remaining subjects didn't feel annoyed at all. The question about how much they want to try a music recommendation system that can help them regulate their emotional states was answered from 5 (very much) to 1 (not at all). 60.3% of subjects chose 4 and 5, and 13.2% of subjects chose 2 and 1. For the extent of acceptance of two additional EEG sensors bonded with earphones, subjects answered from 5 (very much) to 1 (not at all), 60.4% of subjects chose 4 and 5, and 16.9% chose 1 and 2.\n\nMost people enjoy music and their feelings are diverse. Thus some of them might feel annoyed when the platform recommends songs based on others' feedback and their old feelings. Our system would particularly benefit users who are willing to regulate emotions through music but don't trust streaming platforms or try to connect with their neural-centric activities. It is not surprising that some people with stable emotions like s05 barely have different feelings towards the same song, and they even don't demand emotion regulation. However, besides emotion regulation, the system is open to music and thus can recommend new songs based on users' EEG data. And most people involved in our questionnaire don't refuse to put on a two-electrode EEG device if it could help them with their emotional problems effectively.\n\n6 Discussions of Limitations and Future Work",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Music Dynamics And Influences",
      "text": "The results of the real-life scenario evaluations revealed the feasibility of recommending a list of songs based on a one-time EEG collection at the beginning. However, the number of songs could be further explored since our choice is based on the number of trials in a training experiment. As people's EEG is dynamically changed by factors like the music they are listening to and the situation they are involved in, the collected EEG information can hardly be representative of more songs. Thus collecting EEG data before selecting each song would be more conservative, which emphasizes the advantage of our system that it is flexible towards people's ever-changing emotional states. Moreover, songs listened to by users in their daily lives are much longer than excerpts we used in experiments. And sometimes, users may enjoy the beginning of the music but start to dislike the remaining part, and vice versa, which diminishes the representation of the averaged features and system performance. In the future, the features that represent the high-level differences between chapters within a song could contribute to a fine-grained solution. Still, users' dynamic self-assessments for a song are necessary, which means a lot more workload for users.\n\nAnother concern is related to the impacts of music entrainment, which reflects on users' brainwaves and interferes with their emotional states unconsciously. The impacts of music would last after a song has finished and influence the EEG data collected between two songs. We considered the aftermath of music as trivial in our study, thus we didn't refer to its impacts on this system. But what is reflected on users' brainwaves, and how long will the impacts last are still open questions for studying emotions evoked by music.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Eeg Interference And Interruptions",
      "text": "Music features are extracted offline and from a stable source. However, EEG data is collected online and would be easily disturbed and thus mislead the system. The strategy should be considered to make sure that the EEG data collected online is reliable and could be rejected if it is defective. For now, we only employ pre-processing technologies to reject data and channels that are affected by artifacts. With more collected EEG data, it's possible that users' personal EEG profiles could be developed as a reference to evaluate the quality and validity of newly collected data.\n\nBesides, the computational efficiency of a real-time system influences users' experience for possible interruption. And the interruption could be salient and unacceptable when users are focusing on music for emotion regulation. The running time of the music selection in testing experiments is 0.763 seconds (on a 3.4 GHz Intel Core i5 processor) besides the time for EEG collection, in which the EEG processing time by EEGLAB accounts for approximately 80%. Decreasing the EEG collection time would significantly relieve the computational workload. Since the slowest component we employed is 8 Hz, a sufficient time for EEG data collection is allowed to be 0.25 seconds theoretically, which should be testified in future work. Another point is that, the acceptance of interruption would be increased when users acknowledge that the system is acquiring EEG signals during that time.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Emotion Models",
      "text": "There are several clear benefits that users evaluate their emotion variations instead of definitive emotional states, including that it relates the event and users' EEG in one equation, makes it easier for users to report after listening to a song and supports multiple choices of emotion regulation styles. However, since we didn't employ the emotion recognition model, the EEG information didn't reflect any emotional state. Thus we won't return the feedback regarding the user's emotional state based on the collected EEG signals. But we propose that, with sophisticated music information retrieval technologies and users' data involved, the interaction between users and music will provide significant information for neurophysiological studies. Furthermore, four classes of emotion variation choices would be insufficient for users' demands of different emotional change levels. For example, users may want their valence increased substantially but arousal increased slightly. Therefore, emotion variation levels on top of the four basic classes can be further explored for users' personalized, specific emotional demands. Lastly, users' comprehension of their emotions and music tastes may gradually change over time. With more data being collected, the system should take less account of old data and give more weight to the latest data. An online, adaptive preference learning method shall be further developed, instead of the traditional, static classifier.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "System Generalization",
      "text": "Lastly, the system is highly personalized for participants. However, the experiments for model training before using the online system are time-consuming: more than one hundred trials for each participant in our study. Therefore, the generalization method for new users is strongly demanded in future work. One possible solution is to use advanced classification models like pre-training or fine-tuning. They are built for trained tasks, from which the new customized model can benefit. Another possible solution is to categorize new users into groups by presenting discrepant songs. For users who have similar music tastes or emotion regulation styles, discrepant songs can be used to distinguish their general styles, and then the model can be shared and updated among users in the same category. Furthermore, the system is open to all music pieces, thus it can serve as a second-step music recommender to a user's personal playlist or the playlist recommended by streaming platforms. For example, the playlist could be the one that a user selects in iTunes by mood (like 'Sleep'). Then the system will choose the music pieces from the library for the user based on his/her EEG information by setting the designated emotion variation to negative arousal and positive valence, and filtering out the songs that would lead the emotion to higher arousal and cause sleeplessness.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "The interaction between users and music paves the way to comprehending the diverse and ever-changing emotions of people and assists them to regulate emotions in daily life. None of the existing music-based emotion regulation studies has pointed out the limitation of traditional emotion recognition models and come up with the idea that predicts emotion variations, which relate to the song and users' current emotional state. Our system bridges the gap between theoretical studies and real-life applications, and presents the system performance under all four choices of users' desired emotion variations. The robustness of the system is tested with new songs on different days spanning over a period of two months. From users' perspective, the system doesn't return deterministic feedback but follow their wills and emotional states to present stimuli carefully, which assists users to adjust their emotion and leaves space for them to comprehend the emotional changes. We believe that users can get benefits from such decent and user-oriented interaction, which really considers their variant emotions during the process.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Workﬂow comparison of existing systems based on the emotion recognition model (left) and our proposed",
      "page": 2
    },
    {
      "caption": "Figure 1: Some of them",
      "page": 2
    },
    {
      "caption": "Figure 2: Russell’s circumplex model of affect Russell [1980]. Locations of affects on the model correspond to their",
      "page": 3
    },
    {
      "caption": "Figure 1: Concatenations of each song selected in music",
      "page": 3
    },
    {
      "caption": "Figure 2: ) for users’ self-assessment, which",
      "page": 3
    },
    {
      "caption": "Figure 3: The ﬂow diagram of the training and testing processes in the proposed system. The gray arrow shows the",
      "page": 5
    },
    {
      "caption": "Figure 1: One potential limitation of all those methods is that they all interpreted",
      "page": 5
    },
    {
      "caption": "Figure 1: Participants were instructed to move",
      "page": 6
    },
    {
      "caption": "Figure 2: before the experiment. And there is no restriction regarding",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of mean-variances of EEG features between intra-experiments and inter-experiments for ﬁve",
      "page": 7
    },
    {
      "caption": "Figure 3: Testing was a real-time process and there were 22 trials for each testing experiment. Each",
      "page": 7
    },
    {
      "caption": "Figure 1: (1) the participant designated the desired emotion variation",
      "page": 7
    },
    {
      "caption": "Figure 4: It is shown that, on average, for ﬁve participants,",
      "page": 7
    },
    {
      "caption": "Figure 6: , in which each data",
      "page": 10
    },
    {
      "caption": "Figure 5: The F-value and",
      "page": 10
    },
    {
      "caption": "Figure 5: The validation accuracy of arousal model with three time-windows evaluated by 7-fold cross-validation.",
      "page": 11
    },
    {
      "caption": "Figure 6: Music excerpts listened by s01 in training experiments located on Russell’s valence-arousal model by their",
      "page": 12
    },
    {
      "caption": "Figure 7: Testing accuracy (dark cyan line), proportion of new song (blue bar), and emotion instability (yellow line) of",
      "page": 13
    },
    {
      "caption": "Figure 7: for every participant. Day 0 showed the results of testing experiments. The dark cyan",
      "page": 13
    },
    {
      "caption": "Figure 7: , including the mismatch of new songs and old songs. When",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence\nSVM\nModel": "Decision",
          "Arousal\nSVM\nModel": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "1st\n2nd\n3rd\n4th"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Brain correlates of music-evoked emotions",
      "authors": [
        "Stefan Koelsch"
      ],
      "year": "2014",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Effects of sad and happy music on mind-wandering and the default mode network",
      "authors": [
        "Liila Taruffi",
        "Corinna Pehrs",
        "Stavros Skouras",
        "Stefan Koelsch"
      ],
      "year": "2017",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "3",
      "title": "Impact of affective multimedia content on the electroencephalogram and facial expressions",
      "authors": [
        "Siddharth Siddharth",
        "Tzyy-Ping Jung",
        "Terrence Sejnowski"
      ],
      "year": "2019",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "4",
      "title": "A closed-loop, music-based brain-computer interface for emotion mediation",
      "authors": [
        "Stefan Ehrlich",
        "Kat Agres",
        "Cuntai Guan",
        "Gordon Cheng"
      ],
      "year": "2019",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0213516"
    },
    {
      "citation_id": "5",
      "title": "AudienceNet. Audio monitor u.s.: The overall music landscape",
      "authors": [
        "Avram Goldstein"
      ],
      "year": "1980",
      "venue": "Physiological Psychology"
    },
    {
      "citation_id": "6",
      "title": "U.s. music 360: 2017 report highlights",
      "authors": [
        "Nielsen Music"
      ],
      "year": "2017",
      "venue": "U.s. music 360: 2017 report highlights"
    },
    {
      "citation_id": "7",
      "title": "Wearable, wireless eeg solutions in daily life applications: what are we missing?",
      "authors": [
        "Vojkan Mihajlović",
        "Bernard Grundlehner"
      ],
      "year": "2014",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition approach for e-healthcare applications",
      "authors": [
        "Mouhannad Ali",
        "Ahmad Haj Mosa",
        "Fadi Al Machot",
        "Kyandoghere Kyamakya"
      ],
      "year": "2016",
      "venue": "2016 eighth international conference on ubiquitous and future networks (ICUFN)"
    },
    {
      "citation_id": "9",
      "title": "Towards the bio-personalization of music recommendation systems: A single-sensor EEG biomarker of subjective music preference",
      "authors": [
        "Dimitrios Adamos",
        "Stavros Dimitriadis",
        "Nikolaos Laskaris"
      ],
      "year": "2016",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "10",
      "title": "EEG-based analysis of the emotional effect of music therapy on palliative care cancer patients",
      "authors": [
        "Rafael Ramirez",
        "Josep Planas",
        "Nuria Escude",
        "Jordi Mercade",
        "Cristina Farriols"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "Real-time eeg-based emotion recognition for music therapy",
      "authors": [
        "Olga Sourina",
        "Yisi Liu",
        "Minh Nguyen"
      ],
      "year": "2012",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "12",
      "title": "A systematic review on the neural effects of music on emotion regulation: implications for music therapy practice",
      "authors": [
        "Kimberly Sena"
      ],
      "year": "2013",
      "venue": "Journal of music therapy"
    },
    {
      "citation_id": "13",
      "title": "Learning lexico-functional patterns for first-person affect",
      "authors": [
        "Lena Reed",
        "Jiaqi Wu",
        "Shereen Oraby",
        "Pranav Anand",
        "Marilyn Walker"
      ],
      "year": "2017",
      "venue": "Learning lexico-functional patterns for first-person affect",
      "arxiv": "arXiv:1708.09789"
    },
    {
      "citation_id": "14",
      "title": "Comparison of different algorithms for sentiment analysis: Psychological stress notes",
      "authors": [
        "Sunmoo Yoon",
        "Faith Parsons",
        "Kevin Sundquist",
        "Jacob Julian",
        "Joseph Schwartz",
        "Matthew Burg",
        "Karina Davidson",
        "Keith Diaz"
      ],
      "year": "2017",
      "venue": "Studies in health technology and informatics"
    },
    {
      "citation_id": "15",
      "title": "On being told how we feel: How algorithmic sensor feedback influences emotion perception",
      "authors": [
        "Victoria Hollis",
        "Alon Pekurovsky",
        "Eunika Wu",
        "Steve Whittaker"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "16",
      "title": "Music retrieval and recommendation scheme based on varying mood sequences",
      "authors": [
        "Sanghoon Jun",
        "Seungmin Rho",
        "Eenjun Hwang"
      ],
      "year": "2010",
      "venue": "International Journal on Semantic Web and Information Systems (IJSWIS)"
    },
    {
      "citation_id": "17",
      "title": "A novel method for personalized music recommendation",
      "authors": [
        "Cheng-Che Lu",
        "Vincent Tseng"
      ],
      "year": "2009",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "18",
      "title": "Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening",
      "authors": [
        "Patrik Juslin",
        "Petri Laukka"
      ],
      "year": "2004",
      "venue": "Journal of New Music Research",
      "doi": "10.1080/0929821042000317813"
    },
    {
      "citation_id": "19",
      "title": "Enhancing music recommender systems with personality information and emotional states: A proposal",
      "authors": [
        "Bruce Ferwerda",
        "Markus Schedl"
      ],
      "year": "2014",
      "venue": "Umap workshops"
    },
    {
      "citation_id": "20",
      "title": "Emotion regulation through listening to music in everyday situations",
      "authors": [
        "Stefan Myriam V Thoma",
        "Changiz Ryf",
        "Ulrike Mohiyeddini",
        "Urs Ehlert",
        "Nater"
      ],
      "year": "2012",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "21",
      "title": "Handbook of Emotions",
      "authors": [
        "Michael Lewis",
        "Jeannette Haviland-Jones",
        "Lisa Barrett"
      ],
      "year": "2010",
      "venue": "Handbook of Emotions"
    },
    {
      "citation_id": "22",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "23",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "M Soraia",
        "Manuel Alarcao",
        "Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Nextone player: A music recommendation system based on user bahvior",
      "authors": [
        "Yajie Hu",
        "Mitsunori Ogihara"
      ],
      "year": "2011",
      "venue": "Proceedings of 12th International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "25",
      "title": "Current challenges and visions in music recommender systems research",
      "authors": [
        "Markus Schedl",
        "Hamed Zamani",
        "Ching-Wei Chen",
        "Yashar Deldjoo",
        "Mehdi Elahi"
      ],
      "year": "2018",
      "venue": "International Journal of Multimedia Information Retrieval"
    },
    {
      "citation_id": "26",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Yuan-Pin Lin",
        "Chi-Hong Wang",
        "Tzyy-Ping Jung",
        "Tien-Lin Wu",
        "Shyh-Kang Jeng",
        "Jyh-Horng Jeng-Ren Duann",
        "Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "27",
      "title": "From emotion perception to emotion experience: Emotions evoked by pictures and classical music",
      "authors": [
        "Thomas Baumgartner",
        "Michaela Esslen",
        "Lutz Jäncke"
      ],
      "year": "2006",
      "venue": "International journal of psychophysiology"
    },
    {
      "citation_id": "28",
      "title": "Many facets of sentiment analysis",
      "authors": [
        "Bing Liu"
      ],
      "year": "2017",
      "venue": "A practical guide to sentiment analysis"
    },
    {
      "citation_id": "29",
      "title": "A review of music and emotion studies: Approaches, emotion models, and stimuli. Music Perception",
      "authors": [
        "Tuomas Eerola",
        "Jonna Vuoskoski"
      ],
      "year": "2013",
      "venue": "An Interdisciplinary Journal"
    },
    {
      "citation_id": "30",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "31",
      "title": "A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues",
      "authors": [
        "Laura-Lee Balkwill",
        "William Thompson"
      ],
      "year": "1999",
      "venue": "Music perception: an interdisciplinary journal"
    },
    {
      "citation_id": "32",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "33",
      "title": "Intense emotional responses to music: a test of the physiological arousal hypothesis",
      "authors": [
        "Nikki S Rickard"
      ],
      "year": "2004",
      "venue": "Psychology of music"
    },
    {
      "citation_id": "34",
      "title": "Happy, sad, scary and peaceful musical excerpts for research on emotions",
      "authors": [
        "Sandrine Vieillard",
        "Isabelle Peretz",
        "Nathalie Gosselin",
        "Stéphanie Khalfa",
        "Lise Gagnon",
        "Bernard Bouchard"
      ],
      "year": "2008",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "35",
      "title": "Hits to the left, flops to the right: different emotions during listening to music are reflected in cortical lateralisation patterns",
      "authors": [
        "Eckart Altenmüller",
        "Kristian Schürmann",
        "Vanessa Lim",
        "Dietrich Parlitz"
      ],
      "year": "2002",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "36",
      "title": "The future of musical emotions",
      "authors": [
        "Dylan Van Der Schyff",
        "Andrea Schiavio"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2017.00988"
    },
    {
      "citation_id": "37",
      "title": "On the acoustics of emotion in audio: what speech, music, and sound have in common",
      "authors": [
        "Felix Weninger",
        "Florian Eyben",
        "W Björn",
        "Marcello Schuller",
        "Klaus Mortillaro",
        "Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "38",
      "title": "A matlab toolbox for musical feature extraction from audio",
      "authors": [
        "Olivier Lartillot",
        "Petri Toiviainen"
      ],
      "year": "2007",
      "venue": "Proceedings of International Conference on Digital Audio Effects"
    },
    {
      "citation_id": "39",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "Florian Eyben",
        "Felix Weninger",
        "Florian Gross",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of 21st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "41",
      "title": "The 2007 mirex audio mood classification task: Lessons learned",
      "authors": [
        "Cyril Xhjs Downie",
        "Laurier",
        "Ehmann"
      ],
      "year": "2008",
      "venue": "Proceedings of 9th International Conference on Music Information Retrieval"
    },
    {
      "citation_id": "42",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "Mohammad Soleymani",
        "Micheal Caro",
        "Erik Schmidt",
        "Cheng-Ya Sha",
        "Yi-Hsuan Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of 2nd ACM International Workshop on Crowdsourcing for Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion",
      "authors": [
        "J Michael",
        "Yaser Black",
        "Yacoob"
      ],
      "year": "1995",
      "venue": "Proceedings of IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Coding, analysis, interpretation, and recognition of facial expressions",
      "authors": [
        "Irfan Essa",
        "Alex Paul"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "45",
      "title": "A nonlinear heartbeat dynamics model approach for personalized emotion recognition",
      "authors": [
        "Gaetano Valenza",
        "Luca Citi",
        "Antonio Lanata",
        "Enzo Pasquale Scilingo",
        "Riccardo Barbieri"
      ],
      "year": "2013",
      "venue": "Proceedings of 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "46",
      "title": "Recognizing emotion in speech",
      "authors": [
        "Frank Dellaert",
        "Thomas Polzin",
        "Alex Waibel"
      ],
      "year": "1996",
      "venue": "Proceeding of Fourth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "47",
      "title": "Speech based emotion classification",
      "authors": [
        "Tin Lay Nwe",
        "Say Wei",
        "Liyanage C De Silva"
      ],
      "year": "2001",
      "venue": "Proceedings of IEEE Region 10 International Conference on Electrical and Electronic Technology"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition modeling of sitting postures by using pressure sensors and accelerometers",
      "authors": [
        "Tatsuya Shibata",
        "Yohei Kijima"
      ],
      "year": "2012",
      "venue": "Proceedings of the 21st International Conference on Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Remarks on emotion recognition from bio-potential signals",
      "authors": [
        "Kazuhiko Takahashi"
      ],
      "year": "2004",
      "venue": "Proceedings of 2nd International conference on Autonomous Robots and Agents"
    },
    {
      "citation_id": "50",
      "title": "Classification of EEG for affect recognition: an adaptive approach",
      "authors": [
        "Omar Alzoubi",
        "Rafael Calvo",
        "Ronald Stevens"
      ],
      "year": "2009",
      "venue": "Australasian Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "A survey on brain biometrics. ACM Computing Surveys",
      "authors": [
        "Qiong Gui",
        "Maria Ruiz-Blondet",
        "Sarah Laszlo",
        "Zhanpeng Jin"
      ],
      "year": "2019",
      "venue": "A survey on brain biometrics. ACM Computing Surveys"
    },
    {
      "citation_id": "52",
      "title": "Validation of a low-cost eeg device for mood induction studies",
      "authors": [
        "Alejandro Ortega",
        "Beatriz Solaz",
        "Alcañiz Raya",
        "Mariano Luis"
      ],
      "year": "2013",
      "venue": "Annual Review of Cybertherapy and Telemedicine"
    },
    {
      "citation_id": "53",
      "title": "Electrophysiological signatures of resting state networks in the human brain",
      "authors": [
        "Dante Mantini",
        "Cosimo Mauro G Perrucci",
        "Gian Del Gratta",
        "Maurizio Romani",
        "Corbetta"
      ],
      "year": "2007",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "54",
      "title": "Analysis of functional brain connections for positive-negative emotions using phase locking value",
      "authors": [
        "Yasar Dasdemir",
        "Esen Yildirim",
        "Serdar Yildirim"
      ],
      "year": "2017",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "55",
      "title": "Frontal brain electrical activity (EEG) distinguishes valence and intensity of musical emotions",
      "authors": [
        "A Louis",
        "Laurel Schmidt",
        "Trainor"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "56",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "57",
      "title": "Eeg-based emotion recognition during watching movies",
      "authors": [
        "Dan Nie",
        "Xiao-Wei Wang",
        "Li-Chen Shi",
        "Bao-Liang Lu"
      ],
      "year": "2011",
      "venue": "Proceedings of 5th International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "58",
      "title": "Identifying music-induced emotions from eeg for use in brain-computer music interfacing",
      "authors": [
        "Ian Daly",
        "Asad Malik",
        "James Weaver",
        "Faustina Hwang",
        "J Slawmoir",
        "Duncan Nasuto",
        "Alexis Williams",
        "Eduardo Kirke",
        "Miranda"
      ],
      "year": "2015",
      "venue": "Proceedings of 2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "59",
      "title": "Eeg-based emotion recognition using self-organizing map for boundary detection",
      "year": "2010",
      "venue": "Proceedings of 20th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Musical neurofeedback for treating depression in elderly people",
      "authors": [
        "Rafael Ramirez",
        "Manel Palencia-Lefler",
        "Sergio Giraldo",
        "Zacharias Vamvakousis"
      ],
      "year": "2015",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "61",
      "title": "Eeg-based analysis of the emotional effect of music therapy on palliative care cancer patients",
      "authors": [
        "Rafael Ramirez",
        "Josep Planas",
        "Nuria Escude",
        "Jordi Mercade",
        "Cristina Farriols"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "62",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "Foteini Agrafioti",
        "Dimitris Hatzinakos",
        "Adam Anderson"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "An integrated music recommendation system",
      "authors": [
        "Xuan Zhu",
        "Yuan-Yuan",
        "Hyoung-Gook Shi",
        "Ki-Wan Kim",
        "Eom"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "64",
      "title": "Modeling users preference dynamics and side information in recommender systems",
      "authors": [
        "Dimitrios Rafailidis",
        "Alexandros Nanopoulos"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "65",
      "title": "Eeglab: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis",
      "authors": [
        "N Patrik",
        "John Juslin",
        "Sloboda"
      ],
      "year": "2001",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "66",
      "title": "A review on the computational methods for emotional state estimation from the human eeg",
      "authors": [
        "Min-Ki Kim",
        "Miyoung Kim",
        "Eunmi Oh",
        "Sung-Phil Kim"
      ],
      "year": "2013",
      "venue": "Computational and mathematical methods in medicine"
    },
    {
      "citation_id": "67",
      "title": "Introduction to Quantitative EEG and Neurofeedback: Advanced Theory and Applications",
      "authors": [
        "Helen Thomas H Budzynski",
        "James Kogan Budzynski",
        "Andrew Evans",
        "Abarbanel"
      ],
      "year": "2009",
      "venue": "Introduction to Quantitative EEG and Neurofeedback: Advanced Theory and Applications"
    },
    {
      "citation_id": "68",
      "title": "Left frontal hypoactivation in depression",
      "authors": [
        "B Jeffrey",
        "Richard Henriques",
        "Davidson"
      ],
      "year": "1991",
      "venue": "Journal of abnormal psychology"
    },
    {
      "citation_id": "69",
      "title": "Emotion and affective style: Hemispheric substrates",
      "authors": [
        "Davidson Richard"
      ],
      "year": "1992",
      "venue": "Emotion and affective style: Hemispheric substrates"
    },
    {
      "citation_id": "70",
      "title": "Issues and assumptions on the road from raw signals to metrics of frontal eeg asymmetry in emotion",
      "authors": [
        "James John Jb Allen",
        "Maria Coan",
        "Nazarian"
      ],
      "year": "2004",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "71",
      "title": "The temporal lobe & limbic system",
      "authors": [
        "Pierre Gloor",
        "Alan Guberman"
      ],
      "year": "1997",
      "venue": "Canadian Medical Association. Journal"
    },
    {
      "citation_id": "72",
      "title": "Emotion recognition with machine learning using eeg signals",
      "authors": [
        "Omid Bazgir",
        "Zeynab Mohammadi",
        "Seyed Amir",
        "Hassan Habibi"
      ],
      "year": "2018",
      "venue": "2018 25th National and 3rd International Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "73",
      "title": "The development of markers for the Big-Five factor structure",
      "authors": [
        "R Lewis",
        "Goldberg"
      ],
      "year": "1992",
      "venue": "Psychological Assessment"
    },
    {
      "citation_id": "74",
      "title": "Open-source psychometrics project",
      "year": "2020",
      "venue": "Open-source psychometrics project"
    },
    {
      "citation_id": "75",
      "title": "A five-factor theory of personality",
      "authors": [
        "Robert R Mccrae",
        "T Paul",
        "Costa"
      ],
      "year": "1999",
      "venue": "Handbook of Personality: Theory and Research"
    }
  ]
}