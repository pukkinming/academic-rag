{
  "paper_id": "2509.00025v1",
  "title": "Deepemonet: Building Machine Learning Models For Automatic Emotion Recognition In Human Speeches",
  "published": "2025-08-20T08:34:28Z",
  "authors": [
    "Tai Vu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has been a challenging problem in spoken language processing research, because it is unclear how human emotions are connected to various components of sounds such as pitch, loudness, and energy. This paper aims to tackle this problem using machine learning. Particularly, we built several machine learning models using SVMs, LTSMs, and CNNs to classify emotions in human speeches. In addition, by leveraging transfer learning and data augmentation, we efficiently trained our models to attain decent performances on a relatively small dataset. Our best model was a ResNet34 network, which achieved an accuracy of 66.7% and an F1 score of 0.631.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent decades, the advent of machine learning technologies has accelerated research in spoken language processing. In particular, the applications of neural network architectures like CNNs  (LeCun et al., 1995) , LSTMs  (Hochreiter and Schmidhuber, 1997) , and Transformers  (Vaswani et al., 2017)  have led to major advancements and desirable outcomes in automatic speech recognition and speech synthesis programs.\n\nOne interesting area of spoken language research is speech emotion recognition (SER). This problem involves classifying emotions like \"happiness\" or \"anger\" based on audio clips of human speeches. This is a highly important task, because enabling computers to understand human emotions can help facilitate communication between humans and machines. However, while there has been significant research in building AI-powered emotion detection systems, closing the gap between AI performance and human performance still proves to be challenging, due to the ambiguity and complexity of human emotions.\n\nTherefore, in this paper, we developed several machine learning models that utilized SVMs, CNNs, and LTSMs for automated emotion classification in human speeches. We also implemented transfer learning and data augmentation techniques during the training process, which allowed our models to achieve good performances with little training data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Over the last few years, there have been an increasing number of studies on speech emotion recognition (SER). For instance,  Schuller et al. (2003)  leveraged a Hidden Markov Model (HMM) to extract features from speech signals and used them to detect emotions. More recent research has utilized Mel Frequency Cepstral coefficients (MFCCs), which has proven very useful in automatic speech recognition. Specifically,  Demircan and Kahramanli (2018)  extracted MFCCs from the EMO-DB dataset, and then combined them with fuzzy C-means clustering and k-nearest neighbors (kNN) for emotion prediction.\n\nMeanwhile, together with recent breakthroughs in deep learning, many studies have focused on leveraging the power of neural networks for emotion classification. Particularly,  Lim et al. (2016)  applied CNN and LSTM network layers on top of short-time Fourier transform representations of the EMO-DB raw audio data. This approach demonstrated great improvements in predictive accuracy over traditional classification methods. However, most of those deep learning based systems required a large amount of training data in order to achieve high performances. Our project was different, because we trained our machine learning models on a relatively small database. We will demonstrate that incorporating data augmentation and transfer learning can effectively enable our systems to over-come the lack of data, address overfitting issues, and attain decent performances.\n\nIn addition, a number of studies have focused on building multimodal systems that harness additional information from videos or texts to improve speech emotion classification. For example,  Kim et al. (2013)  combined hand-crafted speech features such as pitch, energy, and mel-frequency filter banks (MFBs) with facial landmark features from videos. On the other hand,  Tzirakis et al. (2017)  leveraged 1D convolutional layers to encode features from speeches, while using ResNet50 to extract visual information from video frames. The combined features were passed through an LSTM module to perform final prediction. While this multimodal approach led to some improvements in accuracy levels, it is crucial to note that visual and textual information is not always available. Therefore, building audio-only emotion detection systems is highly important for use cases where we only have audio data. This insight motivated us to develop and train our machine learning models to output correct emotion labels solely based on input audio clips without using any visual or textual data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Models",
      "text": "Our machine learning system included an encoder, which was followed by a classifier. The encoder received an audio clip and then produced a vector representation of the input data. Subsequently, this encoding was fed into the classifier, which outputted an emotion label.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model 1: Mfcc And Svm",
      "text": "As a starting point, we implemented the feature extractor using the Mel Frequency Cepstral Coefficients (MFCC). Afterwards, we took the averages of these MFCC input features across the time dimension and then used them to train a Support Vector Machines (SVM) model  (Boser et al., 1992)  to classify different emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model 2: Log Mel Spectrograms And Lstm",
      "text": "Our second model encoded each data point by computing a mel-scaled spectrogram and then converting it to log space. We built an LSTM neural network as our classifier. This network contained 2 bidirectional LSTM layers, followed by a dropout layer, a linear layer, and a softmax layer. In this model, we also extracted log-scaled mel spectrograms for the input speech data. Since these features were similar to 2D image arrays (shown in Figure  1 ), we then fed them into a CNN classifier in order to obtain emotion labels. Previously, we intended to put raw waveforms directly through the CNN model. However, during our experiments, we found out that training the CNN on log-scaled mel spectrograms was easier and more stable.\n\nWe chose ResNet34  (He et al., 2016)  as our CNN architecture. Additionally, we experimented with two different approaches: training a ResNet34 network from scratch and using transfer learning to finetune a ResNet34 model that was pretrained on the ImageNet database  (Russakovsky et al., 2015) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Augmentation",
      "text": "As we developed and trained our models on a small speech dataset, data augmentation would be helpful in generating more training data and dealing with overfitting problems.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Image-Based Data Augmentation",
      "text": "In particular, since our CNN models were trained on image-like 2D arrays of log-scaled mel spectrograms, we applied several data augmentation methods on these input data, which include rotating by a small degree, zooming in, and changing brightness. Although such image-based augmentation techniques were more common in computer vision tasks and were not directly applied to audio data, we will demonstrate in Section 5.4 that these techniques indeed helped prevent overfitting and improve model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Progressive Resizing",
      "text": "Another augmentation method that we used was progressive resizing  (Colangelo et al., 2021) . Specifically, we first trained the CNN models on smaller versions of the log-scaled mel spectrogram arrays (128 × 128), and then finetuned the networks on arrays of larger sizes (256 × 256). This approach not only augmented the training data, but also allowed the models to train much faster.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mixup",
      "text": "In addition, we harnessed Mixup, a data augmentation technique that generated convex combinations of pairs of training examples and their labels  (Zhang et al., 2018) . Particularly, for two randomly sampled data points (x i , y i ) and (x j , y j ), this method constructed a new example of the form:\n\nHere, x i , x j are input vectors, y i , y j are one-hot label encodings, and λ ∈ [0, 1]. In this way, Mixup acted as a regularizer that encouraged the linear behaviors of the models, reduced their variance, and enhanced their generalization powers.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation",
      "text": "I implemented the code for this project in Python using PyTorch  (Paszke et al., 2019) , FastAI  (Howard and Gugger, 2020) ,  Scikit-learn (Pedregosa et al., 2011 ), Librosa (McFee et al., 2015) . All the code can be found here.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data",
      "text": "In this project, we used the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) database  (Livingstone and Russo, 2018)  and the Surrey Audio-Visual Expressed Emotion (SAVEE) database  (Jackson and Haq, 2014) . We combined them into a single dataset for training and testing our models.\n\nRAVDESS is an English language database that contains 1440 utterances. This dataset was made by 24 actors (12 female and 12 male), who said two sentences \"Kids are talking by the door\" and \"Dogs are sitting by the door\" with various emotions. Meanwhile, the SAVEE database consists of 480 audio clips created by 4 male actors, and each of them recorded 15 sentences. There are 8 different emotion classes, including neutral, calm, happy, sad, angry, fearful, disgust, and surprised. The duration of each utterance ranges from 3 to 5 seconds. The total duration of audio recordings is roughly 2 hours. In addition, we can see in Figure  2  that most of the emotional classes are relatively well balanced. The neutral and calm labels contain slightly fewer audio clips than the other 6 classes.\n\nWe split the dataset into 90% for training, 5% for validation, and 5% for testing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Details",
      "text": "For the SVM model, we produced 20 MFCCs for each input audio clip. We chose an RBF kernel for the SVM algorithm.\n\nFor the LSTM and CNN models, we generated 128 mel bands when converting input speeches to mel spectrograms. We trained the LSTM model and the vanilla CNN model (with no pretraining) for 200 epochs. Meanwhile, for the ResNet34 model that was pretrained on ImageNet, we finetuned its weights for 30 epochs. We used a batch size of 64 and a learning rate of 0.001, with a decay rate of 0.9. We trained the above neural networks using the Cross Entropy loss and the Adam optimization algorithm (Kingma and Ba, 2014).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Methods",
      "text": "Since this project tackled a classification problem, we used classification accuracy scores and F1 scores for evaluating model performance. the input MFCC features, but still learned to predict emotions with more than 50% accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "As Shown In",
      "text": "After that, the LSTM model performed slightly better than the SVM algorithm, with a higher accuracy of 52.8% and a comparable F1 score of 0.497. When investigating its training process, we can see that the performance was still quite low because the LSTM network was overfitting to the training data. In particular, the model learned to decrease training losses to a small value (around 0.5), but the validation losses were still high (around 2.9).\n\nA similar pattern occurred for the vanilla CNN model (with no pretraining), as it only produced 45.8% accuracy. In this case, another issue is that because the training set was too small, the Resnet34 network was not able to learn good representations of the speech contents, so it could not generalize well to unseen data.\n\nIn fact, when we finetuned the ResNet34 model with pretrained weights from ImageNet, the performance went up significantly (57.3% in accuracy and 0.528 in F1 score). Therefore, we can see that the neural network learned useful feature representations of the speech data after being pretrained on a large database like ImageNet. When it was finetuned on our small dataset, the model was able to transfer its prior knowledge about images to reading and extracting information from image-like log-scaled mel spectrogram arrays. The finetuning process then helped the model to adapt to the domain of our dataset even better, which enhanced its performance.\n\nFinally, the ResNet34 model with both transfer learning and data augmentation achieved the best performance, with an accuracy of 66.7% and an F1 score of 0.631. This illustrates the effectiveness of data augmentation techniques in boosting our model performance. Indeed, as we can see in the upper plot of Figure  3 , the ResNet34 network without data augmentation was still overfitting, with low training loss values and high validation loss values. This means that the gap between the train- ing losses and the validation losses was still very large. However, this problem was alleviated with the support of data augmentation, as shown in the lower plot of Figure  3 . Both the training losses and the validation losses decreased gradually, and the gap between them was significantly narrowed.\n\nMeanwhile, because the accuracy of our final model was less than 70%, there is still a lot of room for improvement. One of the main challenges faced by our models was that RAVDESS and SAVEE were two simulated datasets, which consisted of several actors repeating the same sentences with various emotions. Hence, the speech contents in these datasets were not diverse enough for our machine learning programs to learn proper representations of input audio data and detect correlations between human speeches and emotions. In addition, we can observe in Figure  4  that the  ResNet34 model performed well on certain positive classes like surprised, happy, and calm, while produced lower accuracy on some other negative classes like disgust and angry. Furthermore, there was some confusion between certain pairs of emotion labels, such as neutral and calm. This issue is understandable, because the audio clips from these two classes in our dataset often sound similar. Two examples from those two classes are shown in Figure  5 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "Overall, in this study, we developed a number of machine learning models, including SVMs, LSTMs, and CNNs, for inferring emotions from human speeches. Our models were trained and evaluated on small dataset created from the RAVDESS and SAVEE databases. Our best model was a ResNet34 neural network, which achieved an accuracy of 66.7% and an F1 score of 0.631. This is a promising result, given the small size of our training set. With more training data, the model will definitely be able to learn better and recognize emotion classes with higher accuracy levels. In addition, we demonstrated the benefits of transfer learning and data augmentation in boosting model performance. Particularly, transfer learning allowed the model to overcome the lack of audio data and learn good feature representations of speech contents, while data augmentation helped create more training examples, prevent overfitting issues, and enhance the robustness and generalization of the model.\n\nThe next step would be performing more hyperparameter tuning in order to improve our current models. Additionally, we are interested in experimenting with a combination of CNN or LSTM layers for better performances. Furthermore, given the great advantages of data augmentation, we want to implement several audio-based data augmentation techniques such as pitch shift, change in loudness, change in speed, and SpecAugment  (Park et al., 2019) , as they might be able to further reduce overfitting and generalization errors in our training pipeline. Finally, because transfer learning is also beneficial, we would like to finetune some pretrained speech models such as wav2vec  (Schneider et al., 2019)  and SpeechBERT  (Chuang et al., 2019) , and see how they perform in the speech emotion recognition task.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Log mel spectrogram features of an example.",
      "page": 2
    },
    {
      "caption": "Figure 1: ), we then fed them into a CNN classifier",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of emotion labels in the dataset.",
      "page": 3
    },
    {
      "caption": "Figure 2: that most of the emotional classes are relatively",
      "page": 3
    },
    {
      "caption": "Figure 3: , the ResNet34 network with-",
      "page": 4
    },
    {
      "caption": "Figure 3: Training and validation losses across 30",
      "page": 4
    },
    {
      "caption": "Figure 3: Both the training losses and",
      "page": 4
    },
    {
      "caption": "Figure 4: Confusion matrix for the best CNN model.",
      "page": 5
    },
    {
      "caption": "Figure 5: The waveforms of a neutral utterance (upper)",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , the SVM algorithm produced",
      "page": 3
    },
    {
      "caption": "Table 1: Performance of different models on the validation set.",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A training algorithm for optimal margin classifiers",
      "authors": [
        "Bernhard Boser",
        "Isabelle Guyon",
        "Vladimir Vapnik"
      ],
      "year": "1992",
      "venue": "Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory"
    },
    {
      "citation_id": "2",
      "title": "Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering",
      "authors": [
        "Yung-Sung Chuang",
        "Chi-Liang Liu",
        "Hung-Yi Lee",
        "Lin-Shan Lee"
      ],
      "year": "2019",
      "venue": "Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering",
      "arxiv": "arXiv:1910.11559"
    },
    {
      "citation_id": "3",
      "title": "Progressive training of convolutional neural networks for acoustic events classification",
      "authors": [
        "Federico Colangelo",
        "Federica Battisti",
        "Alessandro Neri"
      ],
      "year": "2021",
      "venue": "2020 28th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "4",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "Semiye Demircan",
        "Humar Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "5",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "7",
      "title": "Fastai: A layered api for deep learning",
      "authors": [
        "Jeremy Howard",
        "Sylvain Gugger"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "8",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "9",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Yelin Kim",
        "Honglak Lee",
        "Emily Provost"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "10",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "11",
      "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "1995",
      "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "13",
      "title": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "doi": "10.5281/zenodo.1188976"
    },
    {
      "citation_id": "14",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "15",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "doi": "10.21437/interspeech.2019-2680"
    },
    {
      "citation_id": "16",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library",
      "arxiv": "arXiv:1912.01703"
    },
    {
      "citation_id": "17",
      "title": "Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python",
      "authors": [
        "Fabian Pedregosa",
        "Gaël Varoquaux",
        "Alexandre Gramfort",
        "Vincent Michel",
        "Bertrand Thirion",
        "Olivier Grisel",
        "Mathieu Blondel",
        "Peter Prettenhofer",
        "Ron Weiss"
      ],
      "venue": "Journal of machine Learning research"
    },
    {
      "citation_id": "18",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "19",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "20",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "23",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond empirical risk minimization"
    }
  ]
}