{
  "paper_id": "2103.11436v1",
  "title": "Responsible Ai: Gender Bias Assessment In Emotion Recognition",
  "published": "2021-03-21T17:00:21Z",
  "authors": [
    "Artem Domnich",
    "Gholamreza Anbarjafari"
  ],
  "keywords": [
    "Responsible AI",
    "gender bias",
    "emotion recognition",
    "human-AI interaction",
    "deep neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Rapid development of artificial intelligence (AI) systems amplify many concerns in society. These AI algorithms inherit different biases from humans due to mysterious operational flow and because of that it is becoming adverse in usage. As a result, researchers have started to address the issue by investigating deeper in the direction towards Responsible and Explainable AI. Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction. Evolution of facial expression recognition from the feature based methods to deep learning drastically improve quality of such algorithms. This research work aims to study a gender bias in deep learning methods for facial expression recognition by investigating six distinct neural networks, training them, and further analysed on the presence of bias, according to the three definition of fairness. The main outcomes show which models are gender biased, which are not and how gender of subject affects its emotion recognition. More biased neural networks show bigger accuracy gap in emotion recognition between male and female test sets. Furthermore, this trend keeps for true positive and false positive rates. In addition, due to the nature of the research, we can observe which types of emotions are better classified for men and which for women. Since the topic of biases in facial expression recognition is not well studied, a spectrum of continuation of this research is truly extensive, and may comprise detail analysis of state-of-the-art methods, as well as targeting other biases.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "R ISING awareness regarding Artificial Intelligence (AI)   solutions brings up questions in social sectors, such as influence on job market, compliance with laws and ethics  [1, 2, 3, 4, 5] . Human Computer Interaction (HCI) reaches unbelievable level of entanglement between computers and mankind. Although AI is only a set of mathematical approaches (a lot of them were discovered 50 years ago, like perceptron  [6] , backpropagation  [7]  or neural network  [8] ), with rapidly grow of computational power, these methods have started to be adopted in various scientific and applied applications and thus AI has become flagships of science fleet.\n\nSolutions, development directions, daily affairs and sometimes even human fates became completely depended on sophisticated AI algorithms which are acting as black boxes. Unfair credit scoring  [9] , car crash and even death caused by autopilot system  [10, 11] , racist profilisation  [12]  and unexpected behaviour of AI defence robots  [13]  are nowadays reality. However, while scientists together with industries develop and integrate this kind of solutions into the real world application, customers (and other researchers) start to examine outcomes, questioning and dispute machine's will  [14, 15, 16] .\n\nSince social concerns have been growing, more frequently collocation such as Responsible AI (RAI)  [17]  or Explainable AI (XAI)  [18]  appears on the horizon of the research world. These concerns had reached governmental levels, reinforced by law and decrees, they became a serious issues, which scientific society and companies started to tackle. As could be derived from definitions, RAI framework comprises steps and actions for utilization of AI in responsible way. Meanwhile, XAI is about how to justify, explain and prove the background, behaviour and the decisions of algorithms.\n\nFrom these perspectives, all methods, which directly utilize personal data, came under close supervision of public. Nowadays, face detection and face recognition technologies have become ubiquitous and considered as something casual and simple. However, there are plenty other applications where the main input is a human face, although objectives might extremely vary. Digging deep in the jungle of AI technologies, this research work investigates on facial expression recognition (FER) in the center of RAI in order to facilitate the future research studies in this field.\n\nOne of the pillars of RAI is fairness. In simple words, AI algorithms should not be biased towards one or another group of people. Moreover, this convention could be rephrased as awareness of presence of bias by mentioning it, rather than hiding it. Since supervised machine learning algorithms are completely rely on training data, they often unintentionally inherit latent biases. The goal of this research is to investigate presence or absence of gender bias in deep learning approaches for FER. Usually, to claim whether a model is biased or not, researchers have to perform the same experiment on distinct test groups, which are sampled by target feature, which in this case is gender: male or female.\n\nTo conduct a research, several type of deep learning algorithms were chosen. These are neural networks either mixed type (convolutional recurrent neural network) or 3D convolutional neural networks (3D-CNN). Furthermore, training of particular neural networks were performed in two different ways: from the scratch and with pre-trained weights. The main source of data was a SASE-FE database, as described in 3.1, which is provided by the iCV lab at University of Tartu.\n\nThe contribution of this work is on fairness assessment in the context of RAI by reporting capabilities of different deep learning models for FER in terms of gender bias. In this paper, it has been denoted several definitions of fairness and according to them found which architectures are more gender biased and which are less. It also has been discovered, which emotions are more exaggerated and better recognized depends on gender.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Now we are reviewing the general information regarding responsible usage of AI, understanding facial expressions, and overview of deep learning method for FER. Furthermore, this section describes definition of biases, principles of fairness and has overview of related study about latter in machine learning.\n\nSpreading of AI technologies over various areas of mankind presence is allowing to achieve incredible performance and fascinating results. Computer vision, which takes honorable place in AI adopted fields, became ubiquitous used by different industries. Nowadays, computer vision applications became pivoting technologies in autonomous driving systems  [19, 20] , video surveillance  [21] , virtual and augmented reality  [22, 23] , robotics  [24, 25, 26]  and others  [27, 28, 29, 30, 31, 32] . This increases the immediate need of investigation on ethical and moral rules in order to assure that such assistive systems will not interfere with human rights.\n\nHuman emotion is an extremely wide field of research. During last decades, people were trying to justify, classify and investigate emotions  [33, 34, 35, 36, 37] . Emotion recognition, as a sub-domain of research, tries to find an answer on how to classify emotion which is displaying by human being. Overall, we, as humans, rely on different modalities, which are taken into the account each time when we try to recognize an emotion. Usually, the indicators are facial expression, articulation, gesticulation, body pose and context of the situation. Worth noticing, that emotion recognition is deeply entangled with psychology  [38] .\n\nHowever, still one of the easiest way to express and to understand an emotion is a facial expression. Even though, the emotion in facial expression (that is being displayed by a person) with high certainty can be distinguished even without context and additional information. Hence, over the time, FER became an independent task in computer vision domain, usually interpreted as emotion recognition. As a result, this is a task on the edge of psychology and computer vision, where from the first perspective scientists try to determine an emotion based only on face expression and from the second one, to study algorithms to classify them. There are various domains in which application of FER have been utilized  [39, 40, 41] .\n\nUsually emotions are described through Action Units (AU)  [42]  or determined as a point in Valence-Arousal (V-A) space  [43] . The AU methodology does not determine emotion explicitly, but provides a set of state for each different parts of the face, such as \"lips apart\", \"outer brow raiser\" and so on. Scientists can denote a displayed emotion as a group of AUs. Opposite from AUs, V-A space represents human emotion from psychological perspective, i.e. group them on imaginary Cartesian space in such way that the distance between similar emotions is lower than the opposite ones.\n\nRegardless of notation, scientists determine six basic human emotions: anger, disgust, fear, happiness, sadness and surprise  [44] . More complex, compound emotions, are usually contains two basic emotion and arbitrary number of additional ones  [45, 30] .\n\nData for FER usually presented as still images, sequence of images or videos on which person is displaying emotion. Usually, depends on purpose of data, there are artificial (laboratory)  [46]  and real-world (wild)  [47, 48]  type of data. The first type of data is usually collected in laboratories under special circumstances with particular requirements. The latter one is often collected from the internet, gathering images with an open access and label them.\n\nBefore discovering that deep machine learning algorithms has huge efficiency in image domain, approaches for FER typically were organized as a three stage pipeline: image preprocessing, feature extraction and classification. Feature extraction is a core of pipeline and these approaches were mainly distinguishable by method of feature extraction.\n\nThe most famous and common methods are Gabor Feature Extraction and local binary pattern (LBP)  [49, 50] . The first method uses a set of various Gabor filters  [51] , named Gabor Filter Bank, to extract particular features from image. This method is robust against scale, rotation and luminous intensity.  [52, 53, 54]  are good examples of usage of Gabor filters for encoding facial expressions. The second approach is based on the histograms of binary maps, which are local representations of relation between target pixel and its neighbours: if the center pixel of the map is greater than a neighbour, a corresponding map value equals to 0, otherwise -1. This simple yet effective approach has found extension in several works  [55, 56, 57] , consequently improving a technique.\n\nIn terms of classifiers, the choices are well known and proven machine learning algorithms. Essential methods such as k-Nearest Neighbours (kNN)  [58] , Naive Bayes  [59] , Space Representation-based Classifier (SRC)  [60]  and Support Vector Machine (SVM)  [61]  where used intensively.\n\nIn spite of the tremendous works that has been done in development of algorithms for FER, after resounding success of AlexNet  [62]  in 2012, it was only matter of time, when deep learning approaches would take a main stage in FER. In 2015,  [57]  and  [63]  presented convolutional neural networks (CNN) for FER, trained on LBP and still images correspondingly. Although feature-absence method showed much worse results, ability of deep learning algorithms to extract features independently had been already well known, therefore, research is this direction was continued. The variety of works in the following years have shown extreme performance gain, with respect to to the classical approaches  [64, 65, 66, 67] .\n\nMost recent CNN approaches have gone far beyond and involve sophisticated ways to tackle a very specific problems which rise only in FER domain. For example, Pyramid With Super Resolution  [68]  shows extreme performance of FERPlus dataset  [69] . Facial Motion Prior Networks  [70]  take lead position in AffectNet  [71]  benchmark. One of the fastest network is MicroExpNet  [72] , which also shows incredible performance on Oulu-CASIA dataset  [73] .\n\nSuccess in still image domain naturally was desired to be transferred onto domain of sequence of images or videos. The two main ways how to address sequential data with deep learning are recurrent based methods and 3D convolution based methods  [74] . The target for both is to utilize temporal relation between frames to catch a displayed emotion over time. However, most of the recurrent neural networks are using CNN as a feature extractor, meanwhile 3D CNNs are trained from the scratch and usually do not utilize any additional networks.\n\nThere are many different ways of how union of recurrent neural network (RNN) and CNN feature extractor have been utilized to achieve necessary goals. In  [75]  using bidirectional Long-Short Term Memory (LSTM) and CNN, a compound model has been trained for AUs recognition. As a competitors in EmotiW2016  [76]  challenge, authors of  [77]  used both CNN-RNN and 3D CNN models to extract necessary features to train a model. This kind of approach requires a lot of computational power, yet not showing stateof-the-art performance, therefore was not widely used in future.  [78]  is another good example of usage a combination of CNN and LSTM. CNN is trained using frames of different intensity expression-states, meanwhile LSTM utilizes features which are extracted with aforementioned CNN to learn temporal representations.\n\nIn  [79]  authors designed a novel architecture which comprised 3D CNN and Nested LSTM. In detail, each output feature map of 3D convolution layers is passed to the Multi-dimensional Spatial Pyramid Pooling (extension of Spatial Pyramid Pooling  [80] ), forming a feature vector of fixed length. These vectors are served as an input to the first so called T-LSTM, modeling temporal relations. Next, hidden states of T-LSTM are going to C-LSTM, learning dependencies between convolutions. As a results, according to the experiments, this method had outperformed state-ofthe-arts methods of that time.\n\nIn the area of 3DCNNs, the types of proposed methods were not that diverse, due to the extremely huge consumption of computational capacity. In 2014,  [81]  authors proposed a simple architecture, showing that this method outperform existed classic approached and heavily depend on the size of the network. Three years later,  [82]  presented a method, which comprises 3D convolutional neural network together with facial landmarks. Authors enhanced Inception-ResNet architecture  [83]  within 3D convolution and decreased model size in favor of compactness. As a results, this model beats all that time state-of-the-art methods. Recent years, this kind of model has found appliance in the micro expression recognition  [84, 85, 86] .\n\nAlthough deep learning has opened new horizons for many fields, thoughtless usage of this technology may lead to undesirable consequence. Recent years many problems have arose due to implementation of AI algorithm in fields with critical importance. In 2019, high-level expert group on AI released Ethics Guidelines For Trustworthy AI  [87] . This document defines what trustworthy AI is and determine a framework which has to help companies adjust their AI technologies. According to the framework, three main components of trustworthy AI are lawful, robust and ethical.\n\nNowadays, a lot of companies invest in research and utilize these principles. For example, Microsoft has own Microsoft AI Principles[88] which are correlated with aforementioned guidelines. Corporation tries to bring responsible AI through the Office of Responsible AI (ORA) and the AI, Ethics and Effects in Engineering and Research (Aether) Committee.\n\nAccording to the PwC 2019 AI predictions  [89] , only 3% of companies do not have plans to address problems of RAI. This is a solid proof that rising concerns regarding AI are pushing industries to make AI more explainable and responsible. However, following 2020 AI predictions[90], only one third of surveyed companies address problems of AI Ethics.\n\nAccording to Ethics guidelines for trustworthy AI  [87] , the one of the Ethical Principles in the Context of AI system is the principle of fairness. Fairness is a one out of four main principles of AI Ethics, along with respect for human autonomy, prevention of harm and explicability. However, this term is not unequivocally determined, opening a lot of doors to enter a discussion  [91] . Bias in machine learning could be represented from the very different perspectives: biased sampling, inappropriate feature selection, inference of \"black box\" models, biased labeling and others. As has been mentioned before, AI industry is currently tackling risks of AI, in particular, making efforts to find and mitigate different kind of biases in machine learning.\n\nDespite broad discussion, wide range of options and different definitions, in scope of this paper, fairness is treated in three different ways:\n\n1) Equalized odds. Predictor Ŷ is fair, if for protected attribute A and prediction Y , Ŷ and A are independent conditional on Y  [92] . In math formula\n\n2) Equal Opportunity  [92]  defines this type of fairness for binary predictor as follow:\n\n3) Demographic Parity. Predictor Ŷ is fair, if for protected attribute A and prediction Y the likelihood of prediction is the same  [93] . In math formula\n\nDirect discrimination  [94]  assumes biased results which are depended on particular attributes. Hence, usage of these attributes are considered as protected and defended by law  [95, 96, 97] .\n\nExplainable and Non-explainable discrimination are two sides of one coin. Explainable discrimination is legal in situation, when particular biases could be explained and justified. Meanwhile, non-explainable discrimination is illegal, and being considered such, when discrimination toward particular groups could not be explained. However, special techniques are existed to identify and even mitigate illegal discrimination  [98] .\n\nSince deep learning does not have clear features as input and decision are not explainable, often, due to the nature of data, different biases are arisen. There are more than 20 different biases  [94] . For example, Representation Bias  [99]  touches most of well known datasets (ImageNet  [100] , OpenImages  [101] ), since the population of data is biased towards one or another demographic group  [102] . Another common example is Population Bias, which \"arises when statistics, demographics, representatives, and user characteristics are different in the user population represented in the dataset or platform from the original target population\"  [103] . In other words, Representation bias is hidden inside the way of gathering data from the population, meanwhile Population Bias might be stashed in the population itself.\n\nLast couple of years, scientists around the world are trying to find out whether publicly available AI systems are biased toward protected attributes such as age, gender and race. And indeed, many of them are. Usually, AI is represented as trained neural networks, therefore, mimicking human behavior and being trained on datasets gathered by humans, these networks overtake one of the most prevailing bias -race bias. However, racist behaviour by default could not be expressed by AI, since it does not have own mind. Hence, usually race bias is inherited implicitly from the data, on which neural network has been trained. An excellent examples are researches about a race bias in face recognition  [104, 105, 106] .\n\nAlthough there are many existed biases, in this paper, we focus on a gender bias in FER. Unfortunately, only few studies have been conducted on examining different biases in FER, particularly gender bias. In  [107]  authors target underrepresented class, showing discrimination in AI for FER solutions. Race bias has been found in Microsoft's Face and Face++ API's by  [108] . Several studies have proven  [109, 110]  a correlation between gender and smile on the face (which implicitly mean positive emotion) in CelebA dataset  [111] .  [112] , one of the most recent complete studies of bias and fairness in FER targets three different biases: age, race and gender. Authors of this research perform experiments on aforementioned CelebA  [111]  database and RAF-DB  [113] . However, although authors divide test data into entirely male and female groups, they do not provide conclusions regarding per class differences in accuracy. Instead of that, they show that model is biased towards female group, gaining overall 3% less accuracy score. Another outcome from the study is that according to the Equal opportunity metric (Formula 2), gender bias is the smallest, comparatively to the age and race biases.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "This section contains detailed description of data, data preprocessing and deep learning models that were used in experiments. Data preprocessing comprises face extraction, decreasing sample size and data augmentation. There are five different architectures from two domains would be listed in total.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Description",
      "text": "The database from University of Tartu has been taken as a target database for research. It contains video on which people are displaying emotions. Despite other databases for FER, frame resolution is quite high: 1280 x 960. There are 50 persons and 12 videos recorded per each, therefore in total 600 videos. Each video has high frame rate, which is equal to 100 frame per second (FPS). As was mentioned before, there are six types of emotions: happiness, surprise, sadness, disgust, anger and contempt. Originally, this database have a bit different purpose, therefore, it has additional featurevideos are divided into the genuine and fake expressions. In other words, from 12 videos, on 6 a subject is expressing true emotion, on other 6 -fake.\n\nDisplaying of arbitrary emotion is always started from neutral emotion, following a command and expression which was asked. Unfortunately, there are no timestamp labels for the beginning and end of emotion, what is a considerable issue and approach to solving it will be explained further. Some samples from the dataset are shown on Figure  1 .   [114] . DLIB  [115]  implementation has been used for this task. However, it does not work well for particular dataset. Because of rich representation of skin color in dataset, the given face extractor could not extract faces for all subjects. More precisely, a race bias has been faced, since even with histogram equalization, it was not possible to extract faces for people with dark skin color. Hence, it has been decided to use another, more advance, approach -Multitask Cascade Convolutional Networks (MTCNN)  [116] . Using this model, four types of dataset were generated, each of them has different margin (0,  25, 65, 40) . The size of the cropped frame is 256 × 256.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Decreasing Size",
      "text": "Since database presented as video with high FPS rate, it was chosen to decrease number of frames to fit GPU capacity on High Performance Cluster (HPC). To decrease number of frames without loss of the information, K-Means  [117, 118]  algorithm was used. Simple yet effective, it allows to determine K most distinguishable frames for each video. For the test purposes, it was generated K = 10, 20, 50. For K = 10 the entire video was processed. For K = 20, first 100 frames (1 sec) are excluded. For K = 50, 20 frames from the first half of the video and 30 from the second.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Augmentation",
      "text": "The number of videos in database is relatively small. Since deep learning approaches operates better on large scale of data, data augmentation has been applied during training.\n\nIn total, there are three augmentation techniques applied:\n\n1) Horizontal flip 2) Random rotation 3) Brightness augmentation Each augmentation is applied independently within p = 0.5 probability on each sample. Figure  4  shows example of augmented train data. 3.3.1.4 Pooling layer: Pooling layer is somewhat similar to the convolution layer, however, does not contain any weights, instead just applies specific operation to the perception map. As a results, Max Pooling Layer and Average Pooling Layer are named after the function they apply. In general, pooling layer produces a single value by applying a function (maximum or average) to the perception field, sliding over input tensor dimensions. Detail information and exact implementation can be found in  [119]  3.3.1.5 Long-short term memory (LSTM): Introduced in 90s by  [121] , after huge success in 2013  [122] , LSTM layer became a core module for any sequence related deep learning architecture. In this paper, LSTM layer is used together with CNN feature extractors, since latter are operated only on single images, meanwhile the target input is video. LSTM itself has sophisticated structure, with 4 internal vectors named input gate(i), output gate(o), forget gate(f ) and cell (C). Along with these vectors, on each processing step LSTM contains hidden state (h) and cell input activation ( C) vectors. Visual explanation of how LSTM module operates is shown on Figure  5 , where t is time step, U and W are parameters matrix, which have to be learn. Implementation details could be found in  [119] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vgg-Lstm",
      "text": "One of the most famous model for face recognition which has been widely used and has become a background for further research is VGG16 architecture  [124] , named after Visual Geometry Group, who has conducted research. In this work, VGG16 model serve as a feature extractor which preceded LSTM block. Together with a fully connected layer at the end, they create a model for classification sequential data. The backbone model consists of series of blocks, which are two or three convlutional layers with ReLU and the MaxPooling layer on the end. The model ends with fully connected layer. Exactly this layer serves as feature vector, which further goes to LSTM module, next down to the fully connected layer. The entire structure is on Figure  6 . As were mentioned before, VGG architecture is wide used, therefore there are many good pretrained weights available for the research. It has been chosen to use pretrained weights from ImageNet dataset  [100]  and VGGFace dataset  [124] . To simplify further presentation of results, VGG16 architecture with pretrained ImageNet weights is denoted as VGG16, meanwhile VGG16 architecture with pretrained VGGFace weights is denoted as VGGFACE.\n\nFig.  6 : Architecture of VGGLSTM. Blue: VGG16 backbone with pretrained weights. Red: layers trained from the scratch.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Resnet50-Lstm",
      "text": "Another great example of a model, which is a result of consecutive research in deep learning is ResNet50  [125] . Following the same principle in construction, the model is presented as a CNN feature extractor. ResNet50 backbone begins with a sequence of convolutional layer, batch normalization, ReLU and MaxPooling. Next, four layers with 3, 4, 6 and 3 Bottleneck blocks respectively. Model ends with AvgPooling layer to reduce output feature map size. At this point there are no more backbone layers, and data stream goes down to LSTM module, following by fully connected layer. The entire architecture is visualised on Firuge 7. Since the model is too deep by itself, ImageNet pretrained weights are used. In this paper, when we are referring to ResNet50, meaning entire architecture, which consists of a backbone and LSTM module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Senet-Lstm",
      "text": "In  [126]  authors discovered, that integrating mechanism for learning relationship between channels can considerably improve results. The goal of such block is to explicitly train a network the cross-correlation between spatial channels. The main advantages of this approach are lightweight, simplicity to implement and application to all common layers. Basically, for any given transformation F tr : X -→ U , first, squeeze the feature map into the descriptor vector and then multiply it to the feature block channel wise. See Figure  8 . Fig.  8 : Squeeze-and-Excitation block  [126]  In general, architecture of SENet-LSTM consists of two parts: a backbone and LSTM module. The backbone is SENet model, which copies the architecture of ResNet50 (blue blocks on Figure  7 ), however, each bottleneck block enhanced with SE feature. In other words, Bottleneck layer plays role of function F tr , according to the Figure  8 . The LSTM module is a one layer LSTM layer withing one fully connected follow up layer. SENet backbone has 8096 output features. LSTM block has 256 hidden neurons. Since the number of training data is quite small, it has been chosen to use pretrained weights for a backbone. Ready to use weights, trained of VGGFace2 dataset  [127]  for SENet was taken from  [128] . For purpose of convenience, further this architecture is referred as SENetLSTM.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3D-Cnn",
      "text": "Among others approaches which utilize 3D CNN  [129, 130]  for FER, in  [131]  authors propose an architecture within optimized hyperparameters for CK+ and OULU-CASIA dataset. The network takes input 10 consecutive RGB frames 112 × 112 resolution frames. However, in purposes of this research work, to be consistent with other network, the input resolution has been changed to 224 × 224, and the sequence length varies. Hence, the model architecture has been preserved up to fully connected layers, since their size is directly depended on the input. The final model is presented on Figure  9 . This model does not have any pretrained weights and trained from the scratch. In the further sections this model is referred as 3DCNN.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Resnet3D",
      "text": "In 2018, a group of researchers proposed a new way to treat 3D data such as videos, presenting another two convolutional blocks: mixed convolutions and \"(2+1)D\" convolutions  [132] . Utilizing these block, authors constructed ResNet3D, an analogue of already famous ResNet2D. Defining the clone of shorted version of ResNet -ResNet18 -ResNet3D has got three different implementations: with 3D convolutions, with mixed convolutions and with \"2+1\"D convolutions. Since authors claim that the best performance has been demonstrated by \"2+1\"D convolutions, in this paper exactly this type of model is used. The idea behind \"2+1\"D convolutions is to think about 3D convolutions as 2D convolutions in spatial space, which are followed by 1D convolutions in the temporal space. The benefits from these approach are following: first, due to the additional ReLU after 2D convolutions, this block has twice more number of nonlinearities, increasing capacity of the model, second, authors claim that during the training, a tensor wise error rate is smaller, compared to the 3D convolution counterparts. The overall architecture of ResNet3D is similar to ResNet50, but with different blocks. The final model structure is presented on Figure  10 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "The following section has an overview of all details regarding experiments. Hence, below will be described data division and hyperparameters. In addition, this section contains description of necessary metrics, based on which conclusion would be derived.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Input Size",
      "text": "The raw instance of database is a video file. Due to high resolution of videos and limited computational capacity, as was mentioned in 3.2.2, each video has been squeezed for particular number of frames. However, the observations are that: within K = 20 models generalize data the best. Using K = 10, there were not any adequate results and with K = 50 there were not observed any increase of in performance. Moreover, due to the limited computational resources, bigger length of the input leads to decreasing of batch size, consequently increasing training time or even not allowing to fit a model. The input resolution is\n\nfor all models, except ResNet3D. Due to the enormous number of weights, available computational capacity is not enough to fit the model. Hence, the input resolution for ResNet3D is  (112 × 112) . Putting all together, the input sample for ResNet3D is  (10, 3, 112, 112)  and for all other models is (10, 3, 224, 224).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Division",
      "text": "SASE-FE dataset contains 18 female and 32 male subjects. The instances for the test group have been selected randomly and once. Overall we had selected 5 males and 5 females. Hence, all videos corresponded to these persons are not visible anyhow during the training. Since each person has 12 videos, therefore test set contains 120 videos, 20 videos per emotion. Often, during training process a model tends to underfit or overfit. To track these kinds of behavior, making training process more efficient and to pick the best model, it has been decided to organize a validation test with fixed size as 15% of train data. A validation set is generated randomly for each train run.\n\nTo sum up, each training process contains train and validation sets with 32 and 8 subjects correspondingly. Multiplying by 12, there are 384 train videos and 96 validation videos. Test set with total 120 videos is not involved during training process. The desired metrics which are related to the goals of this paper are calculated exactly from inference on the test set.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Training Details",
      "text": "All training experiments were performed on University of Tartu HPC. The target GPU is NVIDIA Tesla-V100 with 32 GB of VRAM. All code is written in Python with usage of machine learning python package -PyTorch. The hyperparameter search during the entire research was consisted of two phases for each model. First phase of hyperparameter search was performed to find out the optimal model, hence target hyperparameters were such things as number of fully connected layers, number of LSTM layers, number of nodes in these layers. After finding an optimal structure, in the second phase the optimal set of training hyperparameters for each model have been discovered. In the scope of experiments, such variables have been varied: learning rate, batch size and weight decay. The final hyperparameters are presented in Table  1   In order to perform gender bias analysis, test set has been divided into two subsets: pure male set and pure female set, such that each has 5 subjects. In this scenario, pure means to include subjects of only one gender. Hence, from here and further, there are three sets for testing, named as follows:\n\nTest set, Male set and Female set. Moreover, it was decided to look how performance and metrics differ depends on training data. For this purpose, all train data also was divided into pure male train set and pure female train set. Therefore, to follow results easier, models which are trained on entire train set are called Regular, trained on pure male set and pure female set are called Male and Female respectively. Training of all models have been performed using Stochastic Gradient Decent (SGD) optimization. To sum up, it has been trained 18 different neural networks, 6 different models by 3 different train sets.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Metrics",
      "text": "According to the previously mentioned definitions of fairness, the main metrics, based on which any conclusion could be made are accuracy, true positive rate (TPR) and false positive rate (FPR). Although these metrics are trivial, formal definitions are listed below:\n\nT P R = T P T P + F N (7)\n\nwhere T P -true positive, F P -false positive, T N -true negative, F N -false negative, ACC -accuracy, T P R and F P R -true positive and false positive rate respectively. The first definition of fairness (Equation  1 ) implies the equality of TPR and FPR simultaneously. The second definition (Equation  2 ), extending to the multi classification case, requires equal TPR for the groups with different unprotected attribute value. And the third, and the last, definition of fairness (Equation  3 ) means the equal accuracy for separated groups. In other words, the difference in these metrics shows how fair model is, having linear dependency -the higher discrepancy, the more biased model is.\n\nAlthough target database comprises six different emotions, usually it is extremely difficult to distinguish between contempt, disgust and anger. In order to relax constraints and make classification a bit easier, without losing general idea, it has been decided to fuse these three emotions into a single emotion and name it \"Upset\".",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "This section consists of 2 parts. First part contains confusion matrices and shallow analysis of each. Second part is served for more detailed and precise analysis from the perspective of fairness, emotions and test sets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Confusion Matrices",
      "text": "Regular models have shown decent performance on Test set. The major part of emotions are classified correctly, however, Upset and Sad emotions are frequently misclassified, which is an expected outcome, since they are close in V-A space. Confusion matrices are shown on Figure  11 . We can observe almost the same performance comparatively to the Regular models, however, the misclasification rate of Sad emotion as Upset is much higher. This implies that male training data includes more Sad samples which are visually much closer to Upset emotion. Confusion matrices are on Figure  13 .\n\nRegular models have shown almost excellent recognition of Happy emotion for Female set. TPR for Surprise in not high, however, we observe less misclassification between Upset and Sad. Confusion matrices are on Figure  14 .\n\nQuite unexpected results for Female models on Female set. Only TPR for Happy emotion is high enough to consider as acceptable. Recognition of Surprise is very low, which means that female training data has weak samples for Surprise. Confusion matrices are on Figure  15 .\n\nAs has been stated before, Female training data has weak Surprise samples, meanwhile Male models have shown average performance, hence, even with different gender domain, male samples of Surprise are much stronger. Confusion matrices are on Figure  16 .\n\nComparatively to other test sets, inference of Regular   Analysing aggregated results (Figure  21 ), several conclusions have been derived. For the Test and Male set, Regular models show the best accuracy for classification Surprise, while on the Female set, the best accuracy is for Happy emotion. Fused Upset emotion has the lowest recognition rate for all three sets. Worth to mention, inference of Female set has much higher variance rather than on Male set. However, average accuracy for Female set is higher.\n\nFrom the perspective of emotions (Figure  26 ), results as follows: classification of Surprise is better for males, Upset and Sad are expressed better by females and Happy is almost identical recognized for both genders. According to aggregated results (Figure  23 ), variances for Male and Female set are relatively equal, therefore recognition for both gender is considered as robust. The overall accuracy is better for Male set, which is unexpected results, since models are trained exclusively on the female data.\n\nIn the per emotion competition (Figure  27 ), classification of Happy and Surprise emotions are almost identical for both genders, meanwhile Upset and Sad are better recognized on female subjects.\n\nOpposite to previously mentioned type of models, Male models are trained on male data. SENetLSTM architecture is shown as the most fair architecture, according to the EQOP, EQOD and DP. For the most biased model, ResNet50 is considered of being so with respect to EQOP and EQOD, while ResNet3D is a choice according to DP (Figure  24 ). Noticeable, the according to DP, ResNet50 and VGGFACE share second the most biased place in ranking. Therefore, results for Male models are considered to be consistent. For different test sets, the best and the worst accurate classified emotion are the same. Upset emotion has the lowest accuracy among all test sets and Happy -the highest one.\n\nAggregate results (Figure  25 ) show that inference on Fe-  male set has high variance, being unstable, while inference on Male set has small variance, and therefore, more robust. Without a surprise here, average accuracy on Male set is higher rather than on Female set. In the per emotion accuracy (Figure  28 ), inference on Male set leads in recognition of Surprise and Happy emotions. Sad recognition rate is almost the same for both sets. Accuracy for Upset emotion is much higher for Female set.",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This paper provided a comprehensive overview on the face emotion recognition biases in the context of reliable AI. Taking the concrete dataset, SASE-FE, two different groups of methods for face emotion recognition have been analyzed on gender bias. Each group consists of three different neural network architectures, where some of them have been ready available and some have been manually implemented before the analysis. The test sets have been organized in three different ways: entire test data, only male data and only female data. The train sets have been organized in the similar way. All architectures have been trained, resulting into 18 different models, 6 architectures per each train set.\n\nSince model bias can be explained through fairness, there have been given three different definition of fairness, according to which proper analysis have been conducted. As a results, it has been found which architectures are most biased with respect to the definitions of fairness, and which ones are more likely to be fair. In addition, it has been discovered, which kinds of emotions are easier to recognize for men and women. In addition, using three distinct train sets the relationship between training data and inference has been analyzed.\n\nThe topic of gender bias is relatively young and not addressed properly. Therefore, the amount of existed probable directions to research is immense. Extending this research work, for sure, the next goal has to be to expand research on other databases, which are widely used in FER. Also, the models which have been utilized in this paper, are not competitors to the state-of-the-art solutions. Hence, another direction of future work is to implement these solutions and analyze whether they comprise gender bias. Far-reaching extensions include other aspects of RAI and XAI. For example study of other biases (race, age, culture) or working on explanation, understanding and discovering knowledge limits. Altogether, these researches will create a background to more standardized regulation and law creation in the field of AI. As a result, integration of AI in society will be reliable and safe. After all, modern AI still encompasses a decent amount of unknown and hazard, therefore future us have to be ready.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Fig. 1: SASE-FE database examples",
      "page": 4
    },
    {
      "caption": "Figure 2: Examples of cropped images",
      "page": 5
    },
    {
      "caption": "Figure 3: Example of emotion display through sequence of",
      "page": 5
    },
    {
      "caption": "Figure 4: shows example of",
      "page": 5
    },
    {
      "caption": "Figure 4: Examples of augmented data",
      "page": 5
    },
    {
      "caption": "Figure 5: , where t is",
      "page": 6
    },
    {
      "caption": "Figure 5: LSTM module [123]",
      "page": 6
    },
    {
      "caption": "Figure 6: As were mentioned before, VGG architecture is wide used,",
      "page": 6
    },
    {
      "caption": "Figure 6: Architecture of VGGLSTM. Blue: VGG16 backbone",
      "page": 6
    },
    {
      "caption": "Figure 7: Architecture of ResNet50-LSTM. Blue: ResNet50",
      "page": 6
    },
    {
      "caption": "Figure 8: Fig. 8: Squeeze-and-Excitation block [126]",
      "page": 6
    },
    {
      "caption": "Figure 7: ), however, each bottleneck block",
      "page": 7
    },
    {
      "caption": "Figure 9: This model does not have any",
      "page": 7
    },
    {
      "caption": "Figure 9: Architecture of 3D-CNN model",
      "page": 7
    },
    {
      "caption": "Figure 10: Fig. 10: Structure of ResNet3D-LSTM model",
      "page": 7
    },
    {
      "caption": "Figure 11: Confusion matrix per method for Regular models",
      "page": 9
    },
    {
      "caption": "Figure 12: Confusion matrix per method for Female models on",
      "page": 9
    },
    {
      "caption": "Figure 12: We can observe almost the same performance compara-",
      "page": 9
    },
    {
      "caption": "Figure 13: Regular models have shown almost excellent recognition",
      "page": 9
    },
    {
      "caption": "Figure 14: Quite unexpected results for Female models on Female",
      "page": 9
    },
    {
      "caption": "Figure 15: As has been stated before, Female training data has weak",
      "page": 9
    },
    {
      "caption": "Figure 16: Comparatively to other test sets, inference of Regular",
      "page": 9
    },
    {
      "caption": "Figure 13: Confusion matrix per method for Male models on",
      "page": 9
    },
    {
      "caption": "Figure 14: Confusion matrix per method for Regular models",
      "page": 9
    },
    {
      "caption": "Figure 15: Confusion matrix per method for Female models on",
      "page": 9
    },
    {
      "caption": "Figure 17: Female models on the Male set have probably the worst",
      "page": 9
    },
    {
      "caption": "Figure 16: Confusion matrix per method for Male models on",
      "page": 10
    },
    {
      "caption": "Figure 17: Confusion matrix per method for Regular models",
      "page": 10
    },
    {
      "caption": "Figure 18: Fig. 18: Confusion matrix per method for Female models on",
      "page": 10
    },
    {
      "caption": "Figure 19: Fig. 19: Confusion matrix per method for Male models on",
      "page": 10
    },
    {
      "caption": "Figure 20: (a, b)), the least biased is ResNet3D,",
      "page": 10
    },
    {
      "caption": "Figure 20: (c)) consider the most fair model VGG16, when ResNet3D",
      "page": 10
    },
    {
      "caption": "Figure 20: Metrics for models, which have been trained on",
      "page": 10
    },
    {
      "caption": "Figure 21: ), several conclu-",
      "page": 10
    },
    {
      "caption": "Figure 26: ), results as",
      "page": 10
    },
    {
      "caption": "Figure 21: Aggregated accuracy for models, trained on the",
      "page": 11
    },
    {
      "caption": "Figure 22: (a,b)), and VGG16 according to the DP",
      "page": 11
    },
    {
      "caption": "Figure 22: (c)). For each test set, classiﬁcation accuracy is",
      "page": 11
    },
    {
      "caption": "Figure 22: Metrics for models, which have been trained on only",
      "page": 11
    },
    {
      "caption": "Figure 23: ), variances",
      "page": 11
    },
    {
      "caption": "Figure 27: ), classiﬁcation",
      "page": 11
    },
    {
      "caption": "Figure 24: ). Noticeable,",
      "page": 11
    },
    {
      "caption": "Figure 25: ) show that inference on Fe-",
      "page": 11
    },
    {
      "caption": "Figure 23: Aggregated accuracy for models, trained on only",
      "page": 11
    },
    {
      "caption": "Figure 24: Metrics for models, which have been trained on only",
      "page": 11
    },
    {
      "caption": "Figure 25: Aggregated accuracy for models, trained on only",
      "page": 11
    },
    {
      "caption": "Figure 28: ), inference on",
      "page": 11
    },
    {
      "caption": "Figure 26: Per model architecture, accuracy comparison be-",
      "page": 18
    },
    {
      "caption": "Figure 27: Per model architecture, accuracy comparison be-",
      "page": 18
    },
    {
      "caption": "Figure 28: Per model architecture, accuracy comparison be-",
      "page": 19
    },
    {
      "caption": "Figure 29: Per model architecture, Test set accuracy comparison",
      "page": 19
    },
    {
      "caption": "Figure 30: Per model architecture, Female set accuracy compar-",
      "page": 19
    },
    {
      "caption": "Figure 31: Per model architecture, Male set accuracy compari-",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "3DCNN\nResNet3D\nResNet50\nSENetLSTM\nVGG16\nVGGFACE",
          "Learning rate": "5e-3\n5e-3\n5e-3\n1e-2\n1e-2\n5e-3",
          "Batch size": "12\n12\n12\n12\n12\n12",
          "Weight decay": "1e-6\n5e-3\n1e-3\n5e-4\n5e-3\n8e-4"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.8250\n0.8625\n0.8375\n0.9000\n0.9125\n0.9000",
          "Upset": "0.7375\n0.7125\n0.8000\n0.8375\n0.8500\n0.8500",
          "Sad": "0.8125\n0.8000\n0.8625\n0.8500\n0.8875\n0.9000",
          "Happy": "0.8500\n0.8250\n0.8500\n0.9125\n0.9250\n0.8750"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.45\n0.55\n0.55\n0.70\n0.75\n0.70",
          "Upset": "0.95\n0.70\n0.85\n0.95\n0.90\n0.80",
          "Sad": "0.30\n0.50\n0.45\n0.55\n0.60\n0.70",
          "Happy": "0.75\n0.65\n0.85\n0.80\n0.90\n0.85"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.05\n0.033\n0.066\n0.033\n0.033\n0.033",
          "Upset": "0.33\n0.28\n0.21\n0.1\n0.16\n0.13",
          "Sad": "0.016\n0.1\n0.0\n0.05\n0.016\n0.033",
          "Happy": "0.12\n0.12\n0.15\n0.05\n0.066\n0.12"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.7875\n0.8250\n0.7875\n0.8500\n0.8250\n0.8125",
          "Upset": "0.6375\n0.6250\n0.7125\n0.7250\n0.6750\n0.7250",
          "Sad": "0.7250\n0.7625\n0.7625\n0.8250\n0.7500\n0.7750",
          "Happy": "0.8250\n0.8125\n0.7625\n0.8250\n0.7750\n0.8625"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.35\n0.40\n0.25\n0.45\n0.35\n0.45",
          "Upset": "0.75\n0.75\n0.35\n0.95\n0.70\n0.80",
          "Sad": "0.30\n0.15\n0.65\n0.45\n0.10\n0.40",
          "Happy": "0.55\n0.75\n0.80\n0.60\n0.90\n0.70"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.066\n0.033\n0.033\n0.016\n0.016\n0.066",
          "Upset": "0.4\n0.41\n0.16\n0.35\n0.33\n0.3",
          "Sad": "0.13\n0.033\n0.2\n0.05\n0.033\n0.1",
          "Happy": "0.08\n0.16\n0.25\n0.1\n0.26\n0.08"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.8125\n0.7125\n0.7375\n0.8500\n0.8625\n0.8375",
          "Upset": "0.7125\n0.6500\n0.7625\n0.7750\n0.7750\n0.7875",
          "Sad": "0.8000\n0.7750\n0.7750\n0.7875\n0.7750\n0.8375",
          "Happy": "0.7750\n0.8125\n0.8750\n0.8625\n0.8875\n0.8375"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.60\n0.55\n0.65\n0.60\n0.65\n0.55",
          "Upset": "0.80\n0.50\n0.60\n0.70\n0.90\n0.65",
          "Sad": "0.20\n0.10\n0.20\n0.45\n0.10\n0.55",
          "Happy": "0.60\n0.75\n0.85\n0.80\n0.95\n0.85"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.1\n0.0\n0.0\n0.033\n0.0\n0.033",
          "Upset": "0.533333\n0.33\n0.1\n0.13\n0.33\n0.1",
          "Sad": "0.0\n0.0\n0.16\n0.1\n0.1\n0.2",
          "Happy": "0.033\n0.26\n0.3\n0.2\n0.23\n0.16"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.12\n0.23\n0.23\n0.066\n0.066\n0.066",
          "Upset": "0.316667\n0.3\n0.183\n0.2\n0.26\n0.16",
          "Sad": "0.0\n0.0\n0.033\n0.1\n0.0\n0.066",
          "Happy": "0.16\n0.16\n0.12\n0.12\n0.13\n0.16"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.850\n0.650\n0.675\n0.850\n0.825\n0.800",
          "Upset": "0.750\n0.750\n0.825\n0.775\n0.725\n0.825",
          "Sad": "0.800\n0.775\n0.750\n0.750\n0.775\n0.900",
          "Happy": "0.750\n0.725\n0.850\n0.875\n0.825\n0.775"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.875\n0.825\n0.825\n0.900\n0.875\n0.875",
          "Upset": "0.825\n0.750\n0.825\n0.875\n0.825\n0.850",
          "Sad": "0.850\n0.800\n0.900\n0.825\n0.850\n0.925",
          "Happy": "0.850\n0.825\n0.850\n0.950\n0.900\n0.950"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.033\n0.26\n0.3\n0.033\n0.066\n0.066",
          "Upset": "0.26\n0.16\n0.16\n0.13\n0.26\n0.033",
          "Sad": "0.0\n0.0\n0.0\n0.16\n0.0\n0.066",
          "Happy": "0.26\n0.3\n0.13\n0.16\n0.23\n0.3"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.775\n0.875\n0.800\n0.900\n0.925\n0.850",
          "Upset": "0.675\n0.650\n0.725\n0.725\n0.825\n0.750",
          "Sad": "0.775\n0.825\n0.800\n0.800\n0.875\n0.800",
          "Happy": "0.875\n0.800\n0.875\n0.875\n0.975\n0.850"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.033\n0.033\n0.033\n0.0\n0.066\n0.1",
          "Upset": "0.23\n0.16\n0.16\n0.13\n0.13\n0.0",
          "Sad": "0.0\n0.16\n0.033\n0.1\n0.066\n0.1",
          "Happy": "0.13\n0.16\n0.16\n0.066\n0.1\n0.066"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.775\n0.800\n0.775\n0.800\n0.800\n0.800",
          "Upset": "0.600\n0.675\n0.800\n0.825\n0.675\n0.800",
          "Sad": "0.800\n0.825\n0.800\n0.850\n0.725\n0.775",
          "Happy": "0.825\n0.800\n0.775\n0.825\n0.800\n0.875"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.066\n0.066\n0.16\n0.066\n0.033\n0.066",
          "Upset": "0.43\n0.4\n0.26\n0.26\n0.2\n0.26",
          "Sad": "0.033\n0.0\n0.0\n0.1\n0.033\n0.066",
          "Happy": "0.066\n0.1\n0.1\n0.033\n0.0\n0.1"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.800\n0.850\n0.750\n0.875\n0.850\n0.825",
          "Upset": "0.725\n0.575\n0.675\n0.575\n0.700\n0.625",
          "Sad": "0.700\n0.750\n0.775\n0.800\n0.750\n0.750",
          "Happy": "0.825\n0.775\n0.800\n0.800\n0.800\n0.850"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.3\n0.6\n0.4\n0.6\n0.5\n0.6",
          "Upset": "0.7\n0.8\n0.4\n1.0\n0.8\n1.0",
          "Sad": "0.4\n0.0\n0.6\n0.2\n0.0\n0.1",
          "Happy": "0.7\n0.5\n0.6\n0.3\n0.9\n0.4"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.033\n0.066\n0.13\n0.033\n0.033\n0.1",
          "Upset": "0.26\n0.5\n0.23\n0.56\n0.33\n0.5",
          "Sad": "0.2\n0.0\n0.16\n0.0\n0.0\n0.033",
          "Happy": "0.13\n0.13\n0.13\n0.033\n0.23\n0.0"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.800\n0.850\n0.900\n0.850\n0.900\n0.875",
          "Upset": "0.675\n0.575\n0.775\n0.725\n0.775\n0.725",
          "Sad": "0.800\n0.750\n0.775\n0.750\n0.775\n0.800",
          "Happy": "0.775\n0.875\n0.900\n0.875\n0.900\n0.850"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.7\n0.7\n0.7\n0.7\n0.8\n0.7",
          "Upset": "0.8\n0.6\n0.7\n0.7\n0.9\n0.8",
          "Sad": "0.2\n0.1\n0.4\n0.4\n0.1\n0.3",
          "Happy": "0.4\n0.7\n0.9\n0.6\n0.9\n0.7"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FPR": "3dcnn\nresnet3d\nresnet50\nsenetlstm\nvgg16\nvggface",
          "Surprised": "0.16\n0.1\n0.033\n0.1\n0.066\n0.066",
          "Upset": "0.36\n0.43\n0.2\n0.26\n0.26\n0.3",
          "Sad": "0.0\n0.033\n0.1\n0.13\n0.0\n0.033",
          "Happy": "0.1\n0.066\n0.1\n0.033\n0.1\n0.1"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "AI, Robotics, and the Future of Jobs",
      "authors": [
        "Aaron Smith",
        "Janna Anderson"
      ],
      "year": "2014",
      "venue": "Pew Research Center"
    },
    {
      "citation_id": "2",
      "title": "Artificial intelligence: Who is responsible for the diagnosis?",
      "authors": [
        "Emanuele Neri"
      ],
      "year": "2020",
      "venue": "Artificial intelligence: Who is responsible for the diagnosis?"
    },
    {
      "citation_id": "3",
      "title": "The social dilemma of autonomous vehicles",
      "year": "2016",
      "venue": "Science"
    },
    {
      "citation_id": "4",
      "title": "AI can be sexist and racist-it's time to make it fair",
      "authors": [
        "James Zou",
        "Londa Schiebinger"
      ],
      "year": "2018",
      "venue": "AI can be sexist and racist-it's time to make it fair"
    },
    {
      "citation_id": "5",
      "title": "Wozniak Warn About Artificial Intelligence's Trigger Finger",
      "authors": [
        "Eric Mack",
        "Hawking",
        "Musk"
      ],
      "venue": "Wozniak Warn About Artificial Intelligence's Trigger Finger"
    },
    {
      "citation_id": "6",
      "title": "The perceptron: a probabilistic model for information storage and organization in the brain",
      "authors": [
        "Frank Rosenblatt"
      ],
      "year": "1958",
      "venue": "Psychological review"
    },
    {
      "citation_id": "7",
      "title": "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors",
      "authors": [
        "Seppo Linnainmaa"
      ],
      "year": "1970",
      "venue": "Master's Thesis (in Finnish)"
    },
    {
      "citation_id": "8",
      "title": "Predicting the secondary structure of globular proteins using neural network models",
      "authors": [
        "Ning Qian",
        "Terrence Sejnowski"
      ],
      "year": "1988",
      "venue": "Journal of molecular biology"
    },
    {
      "citation_id": "9",
      "title": "Finnish Credit Score Ruling raises Questions about Discrimination and how to avoid it",
      "venue": "Finnish Credit Score Ruling raises Questions about Discrimination and how to avoid it"
    },
    {
      "citation_id": "10",
      "title": "Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam",
      "authors": [
        "Daisuke Wakabayashi"
      ],
      "venue": "Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam"
    },
    {
      "citation_id": "11",
      "title": "Tesla Autopilot System Found Probably at Fault in 2018 Crash",
      "authors": [
        "Niraj Chokshi"
      ],
      "venue": "Tesla Autopilot System Found Probably at Fault in 2018 Crash"
    },
    {
      "citation_id": "12",
      "title": "Facebook Enabled Advertisers to Reach 'Jew Haters",
      "authors": [
        "Julia Angwin"
      ],
      "venue": "Facebook Enabled Advertisers to Reach 'Jew Haters"
    },
    {
      "citation_id": "13",
      "title": "Crime-fighting robot hits, rolls over child at Silicon Valley mall",
      "authors": [
        "Veronica Rocha"
      ],
      "venue": "Crime-fighting robot hits, rolls over child at Silicon Valley mall"
    },
    {
      "citation_id": "14",
      "title": "Artificial intelligence and legal liability",
      "authors": [
        "John Kingston"
      ],
      "year": "2018",
      "venue": "Artificial intelligence and legal liability",
      "arxiv": "arXiv:1802.07782"
    },
    {
      "citation_id": "15",
      "title": "Online dispute resolution: an artificial intelligence perspective",
      "authors": [
        "Davide Carneiro"
      ],
      "year": "2014",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "16",
      "title": "Recent Studies Examine Lunit AI in Breast Cancer Detection",
      "authors": [
        "Keri Stephens"
      ],
      "year": "2020",
      "venue": "AXIS Imaging News"
    },
    {
      "citation_id": "17",
      "title": "Responsible AI becomes critical in 2021",
      "authors": [
        "Appen Wilson"
      ],
      "venue": "Responsible AI becomes critical in 2021"
    },
    {
      "citation_id": "18",
      "title": "Business Intelligence Trend",
      "authors": [
        "Tableau"
      ],
      "year": "2019",
      "venue": "Business Intelligence Trend"
    },
    {
      "citation_id": "19",
      "title": "End to end learning for self-driving cars",
      "authors": [
        "Mariusz Bojarski"
      ],
      "year": "2016",
      "venue": "End to end learning for self-driving cars",
      "arxiv": "arXiv:1604.07316"
    },
    {
      "citation_id": "20",
      "title": "Human activity recognition-based path planning for autonomous vehicles",
      "authors": [
        "Martin Tammvee",
        "Gholamreza Anbarjafari"
      ],
      "year": "2020",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "21",
      "title": "Deep convolutional framework for abnormal behavior detection in a smart surveillance system",
      "authors": [
        "Kwang-Eun Ko",
        "Kwee-Bo Sim"
      ],
      "year": "2018",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Pose estimation for augmented reality: a hands-on survey",
      "authors": [
        "Eric Marchand",
        "Hideaki Uchiyama",
        "Fabien Spindler"
      ],
      "year": "2015",
      "venue": "IEEE transactions on visualization and computer graphics"
    },
    {
      "citation_id": "23",
      "title": "Stress Reduction Using Bilateral Stimulation in Virtual Reality",
      "authors": [
        "Dorota Kami Ńska"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Object recognition using local invariant features for robotic applications: A survey",
      "authors": [
        "Patricio Loncomilla",
        "Javier Ruiz-Del-Solar",
        "Luz Martinez"
      ],
      "year": "2016",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Real-time ensemble based face recognition system for NAO humanoids using local binary pattern",
      "authors": [
        "Anastasia Bolotnikova",
        "Hasan Demirel",
        "Gholamreza Anbarjafari"
      ],
      "year": "2017",
      "venue": "Analog Integrated Circuits and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "A circuit-breaker usecase operated by a humanoid in aircraft manufacturing",
      "authors": [
        "Anastasia Bolotnikova"
      ],
      "year": "2017",
      "venue": "13th IEEE Conference on Automation Science and Engineering (CASE)"
    },
    {
      "citation_id": "27",
      "title": "CNN-based segmentation of medical imaging data",
      "authors": [
        "Baris Kayalibay",
        "Grady Jensen",
        "Patrick Van Der Smagt"
      ],
      "year": "2017",
      "venue": "CNN-based segmentation of medical imaging data",
      "arxiv": "arXiv:1701.03056"
    },
    {
      "citation_id": "28",
      "title": "Multimodal classification of remote sensing images: A review and future directions",
      "authors": [
        "Luis G Ómez-Chova"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "29",
      "title": "Deep learning in agriculture: A survey",
      "authors": [
        "Andreas Kamilaris",
        "Francesc X Prenafeta-Bold Ú"
      ],
      "year": "2018",
      "venue": "Computers and electronics in agriculture"
    },
    {
      "citation_id": "30",
      "title": "Dominant and complementary emotion recognition from still images of faces",
      "authors": [
        "Jianzhu Guo"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "Ensemble approach for detection of depression using EEG features",
      "authors": [
        "Egils Avots"
      ],
      "year": "2021",
      "venue": "Ensemble approach for detection of depression using EEG features",
      "arxiv": "arXiv:2103.08467"
    },
    {
      "citation_id": "32",
      "title": "Spatio-Temporal Based Table Tennis Stroke Type Assessment",
      "authors": [
        "Kadir Aktas"
      ],
      "year": "2021",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "33",
      "title": "Social functionality of human emotion",
      "authors": [
        "M Paula",
        "Markus Niedenthal",
        "Brauer"
      ],
      "year": "2012",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "34",
      "title": "Neuroanatomical correlates of externally and internally generated human emotion",
      "authors": [
        "Eric M Reiman"
      ],
      "year": "1997",
      "venue": "American Journal of Psychiatry"
    },
    {
      "citation_id": "35",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "36",
      "title": "Machine Learning for Face, Emotion, and Pain Recognition",
      "authors": [
        "Gholamreza Anbarjafari"
      ],
      "year": "2018",
      "venue": "SPIE"
    },
    {
      "citation_id": "37",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "J Raymond",
        "Dolan"
      ],
      "year": "2002",
      "venue": "science"
    },
    {
      "citation_id": "38",
      "title": "Psychology of emotion. Harcourt Brace Jovanovich",
      "authors": [
        "G John",
        "Elaine Carlson",
        "Hatfield"
      ],
      "year": "1992",
      "venue": "Psychology of emotion. Harcourt Brace Jovanovich"
    },
    {
      "citation_id": "39",
      "title": "Deeply learning deformable facial action parts model for dynamic expression analysis",
      "authors": [
        "Mengyi Liu"
      ],
      "year": "2014",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "40",
      "title": "Augmented reality-based self-facial modeling to promote the emotional expression and social skills of adolescents with autism spectrum disorders",
      "authors": [
        "Chien-Hsu Chen",
        "I-Jui Lee",
        "Ling-Yi Lin"
      ],
      "year": "2015",
      "venue": "Research in developmental disabilities"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition and its applications",
      "authors": [
        "Agata Kołakowska"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3"
    },
    {
      "citation_id": "42",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y-I Tian",
        "Takeo Kanade",
        "Jeffrey Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "43",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "44",
      "title": "Basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "45",
      "title": "Dominant and complementary multi-emotional facial expression recognition using c-support vector classification",
      "authors": [
        "Christer Loob"
      ],
      "year": "2017",
      "venue": "12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "46",
      "title": "Multimodal database of emotional speech, video and gestures",
      "authors": [
        "Tomasz Sapi Ński"
      ],
      "year": "2018",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
      "authors": [
        "B Gary",
        "Huang"
      ],
      "year": "2008",
      "venue": "Workshop on faces in'Real-Life'Images: detection, alignment, and recognition"
    },
    {
      "citation_id": "48",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw",
      "authors": [
        "Abhinav Dhall"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "49",
      "title": "Face recognition with local binary patterns",
      "authors": [
        "Abdenour Timo Ahonen",
        "Matti Hadid",
        "Pietikäinen"
      ],
      "year": "2004",
      "venue": "Face recognition with local binary patterns"
    },
    {
      "citation_id": "50",
      "title": "Facial expression recognition based on local binary patterns",
      "authors": [
        "Xiaoyi Feng",
        "Matti Pietikäinen",
        "Abdenour Hadid"
      ],
      "year": "2007",
      "venue": "Pattern Recognition and Image Analysis"
    },
    {
      "citation_id": "51",
      "title": "Gabor filter-based edge detection",
      "authors": [
        "Rajiv Mehrotra",
        "Kameswara Rao Namuduri",
        "Nagarajan Ranganathan"
      ],
      "year": "1992",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "52",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "Michael Lyons"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "53",
      "title": "Facial expression recognition based on Gabor wavelets and sparse representation",
      "authors": [
        "Shiqing Zhang",
        "Lemin Li",
        "Zhijin Zhao"
      ],
      "year": "2012",
      "venue": "2012 IEEE 11th International Conference on Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Facial expression recognition using Gabor-mean-DWT feature extraction technique",
      "authors": [
        "Govardhan Mattela",
        "K Sandeep",
        "Gupta"
      ],
      "year": "2018",
      "venue": "2018 5th International Conference on Signal Processing and Integrated Networks (SPIN)"
    },
    {
      "citation_id": "55",
      "title": "Robust facial expression recognition based on local directional pattern",
      "authors": [
        "Taskeed Jabid",
        "Md Hasanul Kabir",
        "Oksam Chae"
      ],
      "year": "2010",
      "venue": "ETRI journal"
    },
    {
      "citation_id": "56",
      "title": "Facial expression recognition based on local phase quantization and sparse representation",
      "authors": [
        "Zhen Wang",
        "Zilu Ying"
      ],
      "year": "2012",
      "venue": "2012 8th International Conference on Natural Computation"
    },
    {
      "citation_id": "57",
      "title": "Facial expression recognition based on improved local binary pattern and class-regularized locality preserving projection",
      "authors": [
        "Wei-Lun Chao",
        "Jian-Jiun Ding",
        "Jun-Zuo Liu"
      ],
      "year": "2015",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Classification of facial expressions using k-nearest neighbor classifier",
      "authors": [
        "Abu Sayeed",
        "Md Sohail",
        "Prabir Bhattacharya"
      ],
      "year": "2007",
      "venue": "International Conference on Computer Vision/Computer Graphics Collaboration Techniques and Applications"
    },
    {
      "citation_id": "59",
      "title": "Hierarchical Bayesian theme models for multipose facial expression recognition",
      "authors": [
        "Qirong Mao"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "60",
      "title": "A new method for facial expression recognition based on sparse representation plus LBP",
      "authors": [
        "Ming-Wei Huang",
        "Zhe-Wei Wang",
        "Zi-Lu Ying"
      ],
      "year": "2010",
      "venue": "2010 3rd International Congress on Image and Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Facial expression recognition using a combination of multiple facial features and support vector machine",
      "authors": [
        "Hung-Hsu Tsai",
        "Yi-Cheng Chang"
      ],
      "year": "2018",
      "venue": "Soft Computing"
    },
    {
      "citation_id": "62",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "63",
      "title": "Dexpression: Deep convolutional neural network for expression recognition",
      "authors": [
        "Peter Burkert"
      ],
      "year": "2015",
      "venue": "Dexpression: Deep convolutional neural network for expression recognition",
      "arxiv": "arXiv:1509.05371"
    },
    {
      "citation_id": "64",
      "title": "Peak-piloted deep network for facial expression recognition",
      "authors": [
        "Xiangyun Zhao"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "65",
      "title": "Convolution neural network for automatic facial expression recognition",
      "authors": [
        "Xiaoguang Chen"
      ],
      "year": "2017",
      "venue": "2017 International conference on applied system innovation (ICASI)"
    },
    {
      "citation_id": "66",
      "title": "Facial expression recognition using weighted mixture deep neural network based on double-channel facial images",
      "authors": [
        "Biao Yang"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "67",
      "title": "Facial expression recognition via deep learning",
      "authors": [
        "Abir Fathallah",
        "Lotfi Abdi",
        "Ali Douik"
      ],
      "year": "2017",
      "venue": "IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)"
    },
    {
      "citation_id": "68",
      "title": "Pyramid with Super Resolution for In-the-Wild Facial Expression Recognition",
      "authors": [
        "Thanh-Hung Vo"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "69",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "70",
      "title": "Facial motion prior networks for facial expression recognition",
      "authors": [
        "Yuedong Chen"
      ],
      "year": "2019",
      "venue": "IEEE Visual Communications and Image Processing"
    },
    {
      "citation_id": "71",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "72",
      "title": "MicroExp-Net: An Extremely Small and Fast Model For Expression Recognition From Face Images",
      "authors": [
        "Ilke Cugu",
        "Eren Sener",
        "Emre Akbas"
      ],
      "year": "2019",
      "venue": "Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA)"
    },
    {
      "citation_id": "73",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "Guoying Zhao"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "74",
      "title": "Facial expression recognition: A survey",
      "authors": [
        "Yunxin Huang"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "75",
      "title": "Deep learning the dynamic appearance and shape of facial action units",
      "authors": [
        "Shashank Jaiswal",
        "Michel Valstar"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "76",
      "title": "Emotiw 2016: Video and grouplevel emotion recognition challenges",
      "authors": [
        "Abhinav Dhall"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "77",
      "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
      "authors": [
        "Yin Fan"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "78",
      "title": "Multi-objective based spatiotemporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "Dae Hoe"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Spatio-temporal convolutional features with nested LSTM for facial expression recognition",
      "authors": [
        "Zhenbo Yu"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "80",
      "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2015",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "81",
      "title": "Facial expression recognition using 3d convolutional neural network",
      "authors": [
        "Young-Hyen Byeon",
        "Keun-Chang Kwak"
      ],
      "year": "2014",
      "venue": "International journal of advanced computer science and applications"
    },
    {
      "citation_id": "82",
      "title": "Facial expression recognition using enhanced deep 3D convolutional neural networks",
      "authors": [
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "83",
      "title": "Inception-v4, inceptionresnet and the impact of residual connections on learning",
      "authors": [
        "Christian Szegedy"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "84",
      "title": "Combining 3D convolutional neural networks with transfer learning by supervised pre-training for facial micro-expression recognition",
      "authors": [
        "Ruicong Zhi"
      ],
      "year": "2019",
      "venue": "IEICE Transactions on Information and Systems"
    },
    {
      "citation_id": "85",
      "title": "Micro-expression recognition based on 3D flow convolutional neural network",
      "authors": [
        "Jing Li"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "86",
      "title": "TSNN: Three-Stream Combining 2D and 3D Convolutional Neural Network for Micro-Expression Recognition",
      "authors": [
        "Chao Wu",
        "Fan Guo"
      ],
      "year": "2021",
      "venue": "IEEJ Transactions on Electrical and Electronic Engineering"
    },
    {
      "citation_id": "87",
      "title": "Ethics guidelines for trustworthy AI",
      "authors": [
        "Expert High-Level",
        "A Group On"
      ],
      "venue": "Ethics guidelines for trustworthy AI"
    },
    {
      "citation_id": "88",
      "title": "AI Predictions",
      "authors": [
        "Pwc"
      ],
      "year": "2019",
      "venue": "AI Predictions"
    },
    {
      "citation_id": "89",
      "title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness",
      "authors": [
        "Ani Nripsuta",
        "Saxena"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "90",
      "title": "Equality of opportunity in supervised learning",
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nathan Srebro"
      ],
      "year": "2016",
      "venue": "Equality of opportunity in supervised learning",
      "arxiv": "arXiv:1610.02413"
    },
    {
      "citation_id": "91",
      "title": "Counterfactual fairness",
      "authors": [
        "J Matt",
        "Kusner"
      ],
      "year": "2017",
      "venue": "Counterfactual fairness",
      "arxiv": "arXiv:1703.06856"
    },
    {
      "citation_id": "92",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "Ninareh Mehrabi"
      ],
      "year": "2019",
      "venue": "A survey on bias and fairness in machine learning",
      "arxiv": "arXiv:1908.09635"
    },
    {
      "citation_id": "93",
      "title": "Fairness under unawareness: Assessing disparity when protected class is unobserved",
      "authors": [
        "Jiahao Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference on fairness, accountability, and transparency"
    },
    {
      "citation_id": "94",
      "title": "Responsible AI-two frameworks for ethical design practice",
      "authors": [
        "Dorian Peters"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Technology and Society"
    },
    {
      "citation_id": "95",
      "title": "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",
      "authors": [
        "Lu Cheng",
        "Kush Varshney",
        "Huan Liu"
      ],
      "year": "2021",
      "venue": "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",
      "arxiv": "arXiv:2101.02032"
    },
    {
      "citation_id": "96",
      "title": "Explainable and non-explainable discrimination in classification",
      "authors": [
        "Faisal Kamiran",
        "Indr Ė Žliobait Ė"
      ],
      "year": "2013",
      "venue": "Discrimination and Privacy in the Information Society"
    },
    {
      "citation_id": "97",
      "title": "A framework for understanding unintended consequences of machine learning",
      "authors": [
        "Harini Suresh",
        "John Guttag"
      ],
      "year": "2019",
      "venue": "A framework for understanding unintended consequences of machine learning",
      "arxiv": "arXiv:1901.10002"
    },
    {
      "citation_id": "98",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "99",
      "title": "Openimages: A public dataset for large-scale multi-label and multi-class image classification",
      "authors": [
        "Ivan Krasin"
      ],
      "year": "2017",
      "venue": "Dataset available from"
    },
    {
      "citation_id": "100",
      "title": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world",
      "authors": [
        "Shreya Shankar"
      ],
      "year": "2017",
      "venue": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world",
      "arxiv": "arXiv:1711.08536"
    },
    {
      "citation_id": "101",
      "title": "Social data: Biases, methodological pitfalls, and ethical boundaries",
      "authors": [
        "Alexandra Olteanu"
      ],
      "year": "2019",
      "venue": "Frontiers in Big Data"
    },
    {
      "citation_id": "102",
      "title": "Face recognition performance: Role of demographic information",
      "authors": [
        "Brendan F Klare"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "103",
      "title": "Face verification subject to varying (age, ethnicity, and gender) demographics using deep learning",
      "authors": [
        "Hachim El",
        "Harry Wechsler"
      ],
      "year": "2016",
      "venue": "Journal of Biometrics and Biostatistics"
    },
    {
      "citation_id": "104",
      "title": "Demographic effects in facial recognition and their dependence on image acquisition: An evaluation of eleven commercial systems",
      "authors": [
        "Cynthia Cook"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "105",
      "title": "Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems",
      "authors": [
        "Ayanna Howard",
        "Cha Zhang",
        "Eric Horvitz"
      ],
      "year": "2017",
      "venue": "IEEE Workshop on Advanced Robotics and its Social Impacts"
    },
    {
      "citation_id": "106",
      "title": "Racial influence on automated perceptions of emotions",
      "authors": [
        "Lauren Rhue"
      ],
      "year": "2018",
      "venue": "Racial influence on automated perceptions of emotions"
    },
    {
      "citation_id": "107",
      "title": "Detecting bias with generative counterfactual face attribute augmentation",
      "authors": [
        "Emily Denton"
      ],
      "year": "2019",
      "venue": "Detecting bias with generative counterfactual face attribute augmentation",
      "arxiv": "arXiv:1906.06439"
    },
    {
      "citation_id": "108",
      "title": "Towards fairness in visual recognition: Effective strategies for bias mitigation",
      "authors": [
        "Zeyu Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "109",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Ziwei Liu"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "110",
      "title": "Investigating bias and fairness in facial expression recognition",
      "authors": [
        "Tian Xu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "111",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "112",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "Navneet Dalal",
        "Bill Triggs"
      ],
      "year": "2005",
      "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR'05)"
    },
    {
      "citation_id": "113",
      "title": "dlib C++ library",
      "venue": "dlib C++ library"
    },
    {
      "citation_id": "114",
      "title": "Joint face detection and facial expression recognition with MTCNN",
      "authors": [
        "Jia Xiang",
        "Gengming Zhu"
      ],
      "year": "2017",
      "venue": "2017 4th International Conference on Information Science and Control Engineering (ICISCE)"
    },
    {
      "citation_id": "115",
      "title": "Least squares quantization in PCM",
      "authors": [
        "Stuart Lloyd"
      ],
      "year": "1982",
      "venue": "IEEE transactions on information theory"
    },
    {
      "citation_id": "116",
      "title": "Some methods for classification and analysis of multivariate observations",
      "authors": [
        "James Macqueen"
      ],
      "year": "1967",
      "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability"
    },
    {
      "citation_id": "117",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "118",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "119",
      "title": "Speech recognition with deep recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Abdel-Rahman Mohamed",
        "Geoffrey Hinton"
      ],
      "year": "2013",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "120",
      "title": "Designing neural network based decoders for surface codes",
      "authors": [
        "Savvas Varsamopoulos",
        "Koen Bertels",
        "Carmen Almudever"
      ],
      "year": "2018",
      "venue": "Designing neural network based decoders for surface codes",
      "arxiv": "arXiv:1811.12456"
    },
    {
      "citation_id": "121",
      "title": "Deep face recognition",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Deep face recognition"
    },
    {
      "citation_id": "122",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "123",
      "title": "Squeeze-andexcitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE con-ference on computer vision and pattern recognition"
    },
    {
      "citation_id": "124",
      "title": "13th IEEE international conference on automatic face & gesture recognition (FG 2018)",
      "authors": [
        "Qiong Cao"
      ],
      "year": "2018",
      "venue": "13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "125",
      "title": "VGGFace2-pytorch",
      "venue": "VGGFace2-pytorch"
    },
    {
      "citation_id": "126",
      "title": "Learning deep facial expression features from image and optical flow sequences using 3D CNN",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Jian Zhang"
      ],
      "year": "2018",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "127",
      "title": "Spontaneous facial micro-expression recognition using 3D spatiotemporal convolutional neural networks",
      "authors": [
        "Sai Prasanna",
        "Teja Reddy"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "128",
      "title": "3D-CNN for Facial Emotion Recognition in Videos",
      "authors": [
        "Jad Haddad",
        "Olivier Lézoray",
        "Philippe Hamel"
      ],
      "year": "2020",
      "venue": "International Symposium on Visual Computing"
    },
    {
      "citation_id": "129",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "Du Tran"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    }
  ]
}