{
  "paper_id": "2509.18550v1",
  "title": "Hadasmilenet: Hadamard Fusion Of Handcrafted And Deep-Learning Features For Enhancing Facial Emotion Recognition Of Genuine Smiles",
  "published": "2025-09-23T02:20:43Z",
  "authors": [
    "Mohammad Junayed Hasan",
    "Nabeel Mohammed",
    "Shafin Rahman",
    "Philipp Koehn"
  ],
  "keywords": [
    "deep learning",
    "facial emotion recognition",
    "feature fusion",
    "multimedia data mining",
    "pattern recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The distinction between genuine and posed emotions represents a fundamental pattern recognition challenge with significant implications for data mining applications in social sciences, healthcare, and human-computer interaction. While recent multi-task learning frameworks have shown promise in combining deep learning architectures with handcrafted D-Marker features for smile facial emotion recognition, these approaches exhibit computational inefficiencies due to auxiliary task supervision and complex loss balancing requirements. This paper introduces HadaSmileNet, a novel feature fusion framework that directly integrates transformer-based representations with physiologically-grounded D-Markers through parameterfree multiplicative interactions. Through systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard multiplicative fusion achieves optimal performance by enabling direct feature interactions while maintaining computational efficiency. The proposed approach establishes new state-of-the-art results for deep learning methods across four benchmark datasets: UvA-NEMO (88.7%, +0.8%), MMI (99.7%), SPOS (98.5%, +0.7%), and BBC (100%, +5.0%). Comprehensive computational analysis reveals 26% parameter reduction and simplified training compared to multi-task alternatives, while feature visualization demonstrates enhanced discriminative power through direct domain knowledge integration. The framework's efficiency and effectiveness make it particularly suitable for practical deployment in multimedia data mining applications that require realtime affective computing capabilities.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions, as a universal form of non-verbal communication, play an essential role in social interactions, emotional well-being, and human-computer interfaces  [1] ,  [2] . Within the spectrum of facial expressions, accurately distinguishing between genuine (Duchenne) and posed (non-Duchenne) smiles is a critical challenge, as it involves subtle, often imperceptible differences in muscle activations  [3] ,  [4] . The capability of reliably identifying genuine smiles has significant implications for multimedia data mining applications across diverse domains including human-computer interaction, social robotics, psychological research, healthcare diagnostics, and marketing strategies  [5] -  [9] , underscoring its wide- Our approach surpasses all end-to-end models across four benchmark datasets: UvA NEMO, MMI, SPOS, and BBC.\n\nranging importance and driving a growing body of research in pattern recognition and related fields. Figure  1  illustrates the comparative best performance of different existing methods, and places our proposed method in the literature. Early approaches for genuine smile recognition have often focused on hand-engineered, manually annotated features  [10] -  [19] , most notably, the Duchenne Marker (D-Marker) features of the Facial Action Coding System (FACS) that highlight subtle facial muscular movements commonly associated with spontaneous or genuine emotions  [20] ,  [21] . Although effective, these hand-crafted methods are labor intensive, brittle to variations in data (e.g., illumination, pose), and often require significant domain expertise to ensure relevance and quality. With the emergence of deep learning, researchers have turned to architectures such as convolutional neural networks (CNNs), recurrent neural networks (including LSTMs), and more recently vision transformers (ViTs)  [22] -  [27] , thus achieving automatic feature extraction and surpassing traditional methods. Although these end-to-end learning strategies reduce manual efforts, they often disregard the rich, domain-relevant cues embedded in D-Markers, limiting their interpretability and potentially leaving valuable discriminative information untapped. To handle this limitation, a multitask learning (MTL) framework, DeepMarkerNet  [28] , was recently proposed that formulates smile recognition as a multitask problem, training one head to predict D-Markers and another to classify smiles, then discarding the former at test time. Although this strategy injects physiological knowledge, it introduces an additional classification head, inflates parameter count, and necessitates careful loss-weight tuning, factors that complicate training and reproducibility. The coupling of D-Marker prediction and smile classification may not fully exploit the complementary nature of these signals, as MTL frameworks often treat D-Marker prediction as a secondary supervisory cue rather than as a core part of the feature space. Furthermore, the indirect usage of D-Markers can hinder the model's capacity to learn robust joint representations, while inference-stage complexity remains similar to other deep learning models, offering limited additional interpretability or adaptability despite the added complexity in training.\n\nIn this paper, we propose HadaSmileNet-a novel framework that directly fuses D-Marker features with deep transformer-based representations for genuine smile recognition. Unlike MTL approaches that treat D-Markers as a learning signal to be predicted, we incorporate them at the feature level, allowing our model to leverage these domainspecific descriptors more explicitly. To achieve this, we systematically evaluate a comprehensive set of 15 simple and advanced feature fusion mechanisms, including various attention-based, bilinear, and multiplicative strategies. Among these, the Hadamard multiplicative fusion  [29]  emerges as the most effective, offering a straightforward yet powerful means of blending domain knowledge with the richer, datadriven features learned by modern transformer architectures. By directly combining handcrafted and learned representations, our approach mitigates the training complexities, suboptimal weighting strategies, and under-realized relationships that characterize MTL frameworks. Moreover, similar to the MTL framework  [28] , we use feature fusion in the training phase only, and leave them out during inference, ensuring fair comparison and simplicity. We evaluate the framework on all available smile databases, and achieve state-of-the-art performance on all of them while reducing computational overhead, making it well-suited for practical multimedia data mining applications.\n\nThe key contributions of this study are as follows:\n\n• We introduce the first parameter-free feature fusion framework for genuine smile recognition that directly integrates physiologically-grounded D-Markers with transformer representations, eliminating the computational overhead and training complexity of multi-task learning approaches.\n\n• Through systematic evaluation of 15 fusion strategies, we establish Hadamard multiplicative fusion as the optimal mechanism for domain knowledge integration, achieving state-of-the-art performance across four benchmark datasets with 26% parameter reduction compared to existing methods. • Comprehensive computational analysis demonstrates significant efficiency gains in training time, inference speed, and memory requirements, making the approach suitable for practical deployment in multimedia data mining applications requiring real-time processing capabilities.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "Let {X i , y i } N i=1 denote a collection of N video samples, where each sample X i = {x 1 , x 2 , . . . , x Ti } is a sequence of T i facial frames. The label y i ∈ {0, 1} indicates whether the i-th video contains a posed (y i = 0) or a genuine (y i = 1) smile. Our primary objective is to learn a classification function f : X i → y i that accurately discriminates between these two classes.\n\nIn addition to the raw video frames, we assume the availability of a handcrafted feature vector Z i ∈ R M for each video i. This Z i encodes D-Marker features-highly discriminative handcrafted descriptors known to capture subtle facial muscle activations associated with genuine smiles. While previous approaches have either relied on these D-Markers as direct inputs or as auxiliary supervisory signals in a multi-task setting, our goal is to integrate these features more directly at the representation level.\n\nFormally, let F be a transformer-based feature extractor that processes the raw frames X i to produce a learned visual representation H i ∈ R D :\n\nWe define a fusion operator ⊗ : R D × R M → R Q that combines the learned representation H i and the handcrafted D-Marker features Z i , producing a fused feature vector F i ∈ R Q :\n\nOur classifier C : R Q → [0, 1] then predicts the probability of the smile being genuine:\n\nThe fusion operator ⊗ can be implemented using various strategies, such as concatenation, attention-based weighting, bilinear pooling, or element-wise multiplicative integration. In this work, we systematically explore multiple such fusion techniques and identify those that most effectively combine the complementary information from H i and Z i .\n\nTraining the model involves optimizing the parameters of F, ⊗, and C to minimize a suitable loss function L, which in our case is the binary cross-entropy:\n\nBy jointly learning F , ⊗, and C, the model can exploit both learned representations from a powerful transformer backbone and domain-specific D-Marker cues. This direct fusion paradigm eschews the complexities and constraints of multi-task frameworks, paving the way for more interpretable, efficient, and effective genuine smile classification models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Handcrafted Feature Extraction",
      "text": "The handcrafted D-Marker features are derived following the physiologically-grounded approach established by Dibeklioglu et al.  [17] , whose empirical validation demonstrated the discriminative power of these features for genuine smile recognition. The extraction process involves three key stages: facial landmark detection and selection, geometric preprocessing, and temporal D-Marker computation across three anatomically relevant facial regions. Landmark detection and selection. Raw video samples X i are processed through the off-the-shelf AttentionMesh model  [30] , an automated facial mesh prediction system developed by Google MediaPipe. This model was pre-trained on approximately 30,000 manually annotated facial images spanning diverse ethnicities and demographic groups, ensuring robust landmark detection across varied population groups. No additional fine-tuning is performed on AttentionMesh in our pipeline; only automatic inference is utilized to extract 478 3D facial landmark points through real-time tracking.\n\nFollowing the protocol established by Dibeklioglu et al.  [17] , we select a subset of M = 11 key landmarks that correspond to facial action units AU6 (cheek raiser) and AU12 (lip corner puller), which are fundamental components of the Duchenne smile according to the Facial Action Coding System. The selection is algorithmic and based on anatomical relevance, requiring no manual annotation at inference time. These landmarks specifically target the contraction and movement of facial muscles orbicularis oculi and zygomaticus major, as well as subtle cheek and lip-corner motions critical for distinguishing genuine from posed smiles. Table  I  provides the mapping of the selected landmarks to their corresponding indices in the 478-point AttentionMesh topology.\n\nTo ensure consistent geometric interpretation, the selected landmarks of each frame are aligned to a reference coordinate system. Let p (x) j ∈ R 3 denote the 3D coordinates of the jth selected landmark in frame x. We compute a plane using eye and nose reference points, and derive a normal vector to this plane. Using this normal, we estimate and remove head rotations (roll, yaw, pitch), followed by scale and translation\n\nAmplitude Ratio\n\nMean Speed\n\nMean Acceleration\n\nTotal Number of Features: 25\n\nadjustments. This normalization ensures that extracted features are robust to head pose variations and camera viewpoints. D-Marker computation. Once the landmarks are normalized, we derive three key dynamic measurements that collectively capture the structure of a smile over time:\n\n1) Lip Dynamics (D lip ): Measures the relative distance and angular changes of lip corners from a stable reference, reflecting mouth opening and lip pulling (Eq. 5). 2) Eye Aperture (D eye ): Quantifies eyelid opening or closing, capturing the hallmark crinkling around the eyes associated with genuine smiles (Eq. 6). 3) Cheek Elevation (D cheek ): Tracks the vertical displacement of cheek regions, which lift prominently during genuine smiles (Eq. 7).\n\n,\n\nwhere, p x i represents the landmark at index i in frame x, γ() represents the Euclidean distance, and Γ(p i , p j ) represents the relative vertical location, which equals -1 if p j is located vertically below p i on the face, and 1 otherwise.\n\nFor each frame x, these metrics D lip (x), D eye (x), and D cheek (x) are computed relative to baseline configurations observed in the initial frames. Differences in these values over time encode the temporal evolution of the smile. Temporal Feature Aggregation. The three key phases of the smile, such as longest increasing segment (onset), stable apex interval, and longest decreasing segment (offset), are identified from the D-Marker metrics using the approach proposed in  [31] . Each phase is again divided into increasing and decreasing segments for detailed analyses. From these segments, we derive a comprehensive set of temporal descriptors: duration-related measures, amplitude magnitudes, velocity and acceleration cues, and various ratios that capture the pattern of smile in the videos. As shown in Table  II , a total of 25 features are calculated for each of the three phases, giving 75 features for each of the three facial regions (eyes, lips and cheeks). By concatenating all these features we form a final kdimensional D-Marker feature vector,\n\nwhere k = 225 in our implementation.\n\nThe empirical validation of these D-Marker features was demonstrated by Dibeklioglu et al.  [17] , establishing their discriminative power for genuine smile recognition. Our ablation analysis in Table VI demonstrates the individual contributions of Duration, Position, and Motion feature categories, confirming that each feature group captures essential aspects of genuine smile dynamics and contributes meaningfully to classification performance. This hand-crafted D-Marker representation encodes the geometric and temporal patterns of genuine smiles in a structured, low-dimensional feature space, offering complementary information to data-driven embeddings for effective feature-level integration in our proposed framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Hadasmilenet Architecture",
      "text": "The architecture diagram of the proposed method is illustrated in Figure  2 . The architecture mainly has two parts: the D-Marker extraction using manual handcrafted approaches, as detailed in the previous section, and the automatic deeplearning part. Automatic Feature Extraction. To obtain robust automatic representations of facial dynamics, we adopt the current stateof-the-art transformer-based architecture, MeshSmileNet  [24] . This architecture serves as our feature extractor F that processes temporal sequences of 3D facial landmarks to produce discriminative representations.\n\nGiven the normalized facial landmarks from Section II-B, we construct input sequences X i = {x 1 , x 2 , . . . , x Ti } where each frame x t ∈ R 478×3 contains the 3D coordinates of all 478 facial landmarks detected by AttentionMesh. To handle variable-length sequences, we apply temporal normalization by either truncating longer sequences or zero-padding shorter ones to a fixed length T = 64 frames.\n\nThe MeshSmileNet architecture consists of three primary components that collectively learn spatiotemporal representa-tions:\n\n(1) Relativity Network: This component analyzes the spatial geometric relationships among facial landmarks within each frame. It employs CurveNet blocks  [32]  to aggregate landmarks into meaningful curves based on their geometric proximity and functional relationships. For each frame x t , the relativity network produces spatial embeddings s t ∈ R ds that capture the geometric configuration of facial features:\n\nwhere d s = 128 is the spatial embedding dimension.\n\n(2) Trajectory Network: This component tracks the temporal evolution of landmark movements across the sequence. It utilizes a multi-head self-attention mechanism to model long-range dependencies and temporal dynamics. The trajectory network processes the sequence of spatial embeddings {s 1 , s 2 , . . . , s T } to produce temporal features:\n\nwhere S i = [s 1 ; s 2 ; . . . ; s T ] ∈ R T ×ds is the concatenated spatial embeddings, and T i ∈ R T ×ds captures temporal dependencies.\n\n(3) Feature Aggregation: The final video-level representation H i is obtained by applying global average pooling over the temporal dimension, followed by a linear projection:\n\nwhere W h ∈ R D×ds and b h ∈ R D are learned parameters, and D = 256 is the dimension of the final learned representation.\n\nFeature Fusion Mechanism. The core innovation of HadaS-mileNet lies in its direct fusion of learned transformer features\n\nWe systematically evaluate 15 different fusion strategies, ranging from simple concatenation to complex attention-based mechanisms. Through comprehensive empirical analysis, we identify Hadamard multiplicative fusion as the optimal strategy. To enable element-wise multiplication, we first align the dimensionalities of both feature vectors through learned linear transformations:\n\nwhere\n\nThe Hadamard fusion operation is then defined as:\n\nwhere ⊙ denotes element-wise multiplication. This fusion strategy enables direct multiplicative interactions between corresponding dimensions of the learned and handcrafted features, allowing the model to amplify or suppress feature components\n\nwhere\n\nis the sigmoid activation function, and ŷi ∈ [0, 1] represents the predicted probability of the smile being genuine. The simplified classification architecture, compared to the multi-head design in multi-task learning approaches, reduces computational overhead while maintaining discriminative power. The layer normalization ensures stable gradients and consistent feature magnitudes, while the single linear layer provides sufficient capacity for the binary classification task given the rich, pre-processed fusion features. Training Procedure. Unlike multi-task learning approaches that require careful balancing of multiple loss terms, our",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Setup",
      "text": "Datasets. We evaluate our proposed method independently on four widely-used benchmark datasets: (i) The UvA-NEMO smile database  [17]  is the most comprehensive dataset for smile authenticity analysis, comprising 1240 high-definition videos (597 genuine, 643 posed) recorded at 1920×1080 resolution and 50 FPS. The dataset features 400 subjects (185 female, 215 male) across a broad age spectrum (8-76 years), with controlled illumination conditions enabling focus on expression dynamics. (ii) The BBC dataset presents unique challenges through its collection of celebrity interviews, containing 20 smile videos (10 genuine, 10 posed) recorded in realworld conditions. The lower resolution (314×286) and varying illumination make it particularly suitable for evaluating robustness. (iii) The MMI facial expression dataset contributes 187 smile videos (138 genuine at 640×480/29 FPS, 49 posed at 720×576/25 FPS), offering diversity in recording conditions. (iv) The SPOS dataset  [15]  includes grayscale sequences captured at 640×480 resolution and 25 FPS. We utilize its gray-scale smile sequences, which comprise both genuine and posed expressions under controlled settings. These datasets present varying challenges through their different recording conditions, resolutions, and subject demographics, enabling comprehensive evaluation of our method's generalization capabilities. Notably, while UvA-NEMO offers ideal conditions for analyzing subtle expression differences, the BBC dataset tests robustness to real-world variations, and MMI and SPOS provide additional validation across different video qualities and frame rates. The dataset details are summarized in Table  III . Some data samples randomly selected from the UvA-NEMO database can be visualized in Figure  3 . Evaluation Protocol. We adopt rigorous cross-validation protocols for comprehensive evaluation across datasets. For UvA-NEMO, we employ 10-fold cross-validation following established protocols  [17] . The BBC, MMI, and SPOS datasets are evaluated using 10-fold, 9-fold, and 7-fold cross-validation respectively, ensuring fair comparison with previous works  [16] . To ensure robust performance estimation, we conduct ten independent runs for each dataset, with each subset serving as test data exactly once. Special care is taken to maintain subject independence across train-test splits, preventing potential data leakage. Performance is measured using classification accuracy averaged across all folds. Implementation Details 1  . Our framework is implemented in PyTorch and trained on an NVIDIA Tesla T4 GPU on the Amazon AWS EC2 server. The preprocessing pipeline extracts 478 3D facial landmarks using Attention Mesh, which   [13]  77.3 81.0 73.0 75.0 Dibeklioglu  '10 [14]  71.1 74.0 68.0 85.0 Pfister  '11 [15]  73.1 81.0 67.5 70.0 Wu'14  [16]  91.4 86.1 79.5 90.0 Dibeklioglu'15  [17]  89.8 88.1 77.5 90.0 Wu'17  [18]  93.9 92.2 81.2 90.0 Mandal'17  [19]  80.4 ---Mandal'16  [22]  78.1 ---RealSmileNet'20  [23]  82.1 92.0 86.2 90.0 PSTNet'22  [25]  72.9 94.3 87.1 95.0 P4Transformer'21  [26]  74.9 91.3 82.9 85.0 Vanilla ViT'20  [27]  78.4 99.0 93.5 95.0 MeshSmileNet'22  [24]  85.0 99.0 94.4 95.0 DeepMarkerNet'24  [28]  87.9 The spatial-temporal transformer employs 6 blocks for spatial attention and 3 blocks for temporal modeling, each with 4 attention heads. We utilize binary cross-entropy loss for smile classification, with all network weights initialized using He initialization. Layer normalization and dropout (p=0.1) are applied throughout the network to prevent overfitting.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Comparison With State-Of-The-Art",
      "text": "Table  IV  presents the empirical results achieved for our method versus all existing methods in the literature. From the table, one may observe the following: (i) We achieve state-ofthe-art performance on three out of four datasets, with statistically significant gains (p-value < 0.001) on SPOS (98.5%) and BBC (100%), surpassing both traditional feature-based approaches and recent deep learning methods. (ii) While Wu et al.  [18]  maintains the lead on UvA-NEMO (93.9% vs. our 88.7%), their method relies on manual landmark initialization and extensive preprocessing, whereas our approach is fully automatic and end-to-end trainable. (iii) Notably, we improve upon DeepMarkerNet  [28] , the previous state-of-the-art that also utilizes D-Marker information but through multi-task learning rather than direct feature fusion. Our method shows consistent gains across all datasets (UvA-NEMO: +0.8%, MMI: +0.7%, SPOS: +1.3%, BBC: +5.0%), demonstrating the superiority of explicit feature interaction over auxiliary task supervision. The effectiveness of our Hadamard fusion strategy is further visually demonstrated through the t-SNE visualizations in Fig-  ure 4 . The baseline approach without D-Marker features shows significant overlap between genuine and posed smiles. While DeepMarkerNet's multi-task learning approach improves feature separability, decision boundaries remain overlapping. In contrast, our Hadamard fusion method achieves clear class separation, demonstrating that direct multiplicative interactions create more discriminative representations compared to indirect supervision through auxiliary tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Ablation Studies",
      "text": "Ablation on fusion techniques. We systematically evaluate 15 feature fusion strategies to validate our choice of Hadamard fusion. Table V presents classification accuracy across datasets. The results demonstrate several critical insights: (i) Hadamard multiplicative fusion achieves optimal performance across all datasets, effectively preserving spatial correspondence between transformer and D-Marker features while enabling element-wise feature interactions. (ii) Despite their theoretical sophistication, attention-based mechanisms exhibit suboptimal performance, indicating that the complementary nature of transformer and D-Marker features is better captured through direct multiplicative interactions rather than learned attention weights. (iii) The integration of Hadamard multiplication consistently enhances other fusion methods, with notable improvements observed in FiLM and Bilinear Pooling variants, suggesting its fundamental value in multimodal feature integration. (iv) Simple concatenation achieves competitive performance, particularly on MMI (99.7%), demonstrating that allowing networks to learn feature interactions through subsequent layers can be effective when sufficient training data is available. Ablation on D-Marker feature components. We also evaluate the contribution of duration, position, and motion features extracted from facial dynamics across three regions. Table VI shows that duration features provide the most discriminative information for genuine smile recognition. The removal of duration features causes substantial performance degradation (UvA-NEMO: -5.5%), confirming that temporal dynamics of smile evolution constitute the strongest authenticity cues. Motion and position features contribute complementarily, with motion features demonstrating higher importance on challenging datasets. This hierarchical importance validates the physiological foundation of D-Markers, where timing patterns distinguish genuine emotional expressions from deliberate facial movements. The consistent degradation pattern indicates that all three feature categories capture essential aspects of genuine smile dynamics. Ablation on facial regions. We investigate the contribution of D-Marker features extracted from lips, eyes, and cheeks through systematic exclusion experiments.   The fusion mechanism automatically learns optimal feature combinations rather than relying on manually tuned loss weights, while t-SNE visualizations (Figure  4 ) demonstrate",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "This paper presents HadaSmileNet, a computationally efficient feature fusion framework that directly integrates transformer-based representations with physiologicallygrounded D-Markers for genuine smile recognition. By employing parameter-free Hadamard multiplicative fusion, the approach eliminates the training complexities associated with multi-task learning methods while achieving superior performance. Comprehensive evaluation across four benchmark datasets establishes new state-of-the-art results for deep learning approaches, with significant computational advantages including 26% parameter reduction and simplified training procedures. The systematic analysis of 15 fusion strategies provides valuable insights for the pattern recognition community, making the framework suitable for practical deployment in multimedia data mining applications requiring efficient affective computing abilities.\n\nFuture research directions could explore: (a) investigating alternative parameter-free fusion mechanisms for domain knowledge integration, (b) extending the framework to broader emotion recognition applications, (c) exploring cross-modal fusion with audio and contextual signals, and (d) developing end-to-end approaches that automatically learn physiologically-inspired features without manual extraction.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Evolution of smile facial emotion recognition methods:",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the comparative best performance of",
      "page": 1
    },
    {
      "caption": "Figure 2: The architecture mainly has two parts: the",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of our proposed framework for genuine smile recognition. The pipeline consists of two parallel streams: (1)",
      "page": 5
    },
    {
      "caption": "Figure 3: Data samples randomly drawn from the UvA-NEMO database showing neutral face (top), posed enjoyment smile",
      "page": 6
    },
    {
      "caption": "Figure 3: Evaluation Protocol. We adopt rigorous cross-validation pro-",
      "page": 6
    },
    {
      "caption": "Figure 4: t-SNE visualization of learned feature embeddings on the UvA-NEMO dataset. Left: baseline without D-Marker features",
      "page": 8
    },
    {
      "caption": "Figure 5: reveals significant advantages of the",
      "page": 9
    },
    {
      "caption": "Figure 4: ) demonstrate",
      "page": 9
    },
    {
      "caption": "Figure 5: Computational efficiency analysis showing reduced",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "99.0",
          "Column_9": "95.0",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "97.8\n95.0",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "98.5",
          "Column_20": "",
          "Column_21": ""
        },
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "93.9\n92.2\n90.0",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "94.4",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": ""
        },
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "85.0",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "87.9",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "88.7",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": ""
        },
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "81.2",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": ""
        },
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "",
          "Column_2": "T\nFea\n(2",
          "Column_3": "radit\nture\n004-",
          "Column_4": "iona\n-ba\n201",
          "Column_5": "l\nsed\n5)",
          "Column_6": "",
          "Column_7": "De\n(2",
          "Column_8": "ep L\nMet\n016-",
          "Column_9": "earn\nhods\n202",
          "Column_10": "ing\n2)",
          "Column_11": "",
          "Column_12": "M",
          "Column_13": "ulti\nLear\n(20",
          "Column_14": "-tas\nning\n24)",
          "Column_15": "k",
          "Column_16": "",
          "Column_17": "(",
          "Column_18": "Ou\nHada\nFus",
          "Column_19": "rs\nmar\nion)",
          "Column_20": "d",
          "Column_21": ""
        },
        {
          "UvA-NEMO MMI SPOS BBC\n99.7 99.7 100.0": "",
          "Column_2": "Ev\npro\nar\nim\nrec\ne\nte\nat\nten\n[\nf\nhl\ned\nh\nbr\nen\nan\ner",
          "Column_3": "olu\nac\nk d\np\nog\n1\nxis\nure\nf\n10]\neat\nigh\nw\neff\nitt\nre\nd\ns h",
          "Column_4": "tio\nh\nat\nort\nni\nill\ntin\n. E\noc\n–[\nure\nt\nith\nec\nle\nqu\nqu\nav",
          "Column_5": "n\nsu\nas\nan\ntio\nust\ng\nar\nus\n19\ns\nsu\ns\ntiv\nto\nire\nal\ne",
          "Column_6": "",
          "Column_7": "mi\nse\nUv\nnd\nnd\ns t\nho\npp\nn\nos\nhe\nfa\ntan\nhes\niat\nni\nW\ned",
          "Column_8": "le\ns\nA\ndr\nre\nhe\nds,\nro\nha\nt\nFa\ncia\neo\ne\nion\nfic\nith\nto",
          "Column_9": "fa\nall\nN\nivi\nlat\nc\na\nac\nnd\nno\nci\nl\nus\nha\ns\nant\nt\nar",
          "Column_10": "cia\ne\nEM\nng\ned\nom\nnd\nhes\n-e\ntab\nal\nm\no\nnd\nin\nd\nhe\nch",
          "Column_11": "",
          "Column_12": "oti\no-e\nM\nro\nds.\nati\nes\ng\nee\nhe\non\nlar\nnu\nfte\n(\nin\nerg\nur",
          "Column_13": "on\nnd\nM\nwi\nve\no\nen\nred\nD\nC\nm\nine\nd\ne.g\nex\nen\nes",
          "Column_14": "re\nI,\nng\nb\nur\nuin\n,\nu\nod\no\ne\nme\n.,\npe\nce\nsu",
          "Column_15": "co\nmo\nSP\nbo\nest\npro\ne\nma\nch\nin\nve\nm\nth\nill\nrti\no\nch",
          "Column_16": "",
          "Column_17": "tio\na\nan\nof\nrfo\ned\ne r\nlly\nste\nts\nns\nar\nnat\no\nee\nco",
          "Column_18": "n\ncr\nd\nres\nrm\nm\nec\na\nMa\nm\nco\n[2\ne l\nio\nen\np l\nnv",
          "Column_19": "me\noss\nB\nea\nan\net\nog\nnn\nrke\n(F\nm\n0],\nab\nn,\nsur\nea\nolu",
          "Column_20": "tho\nf\nBC\nrc\nce\nho\nnit\nota\nr\nA\nmo\n[\nor\npo\ne\nrni\ntio",
          "Column_21": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 88: 7%), their method relies on manual landmark initialization improvements observed in FiLM and Bilinear Pooling vari-",
      "data": [
        {
          "Mandal’16 [22]": "RealSmileNet’20 [23]",
          "78.1": "82.1",
          "-": "90.0"
        },
        {
          "Mandal’16 [22]": "PSTNet’22 [25]",
          "78.1": "72.9",
          "-": "95.0"
        },
        {
          "Mandal’16 [22]": "P4Transformer’21 [26]",
          "78.1": "74.9",
          "-": "85.0"
        },
        {
          "Mandal’16 [22]": "Vanilla ViT’20 [27]",
          "78.1": "78.4",
          "-": "95.0"
        },
        {
          "Mandal’16 [22]": "MeshSmileNet’22 [24]",
          "78.1": "85.0",
          "-": "95.0"
        },
        {
          "Mandal’16 [22]": "DeepMarkerNet’24 [28]",
          "78.1": "87.9",
          "-": "95.0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Gen": "Fake",
          "uine Sm": "Smile",
          "ile": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Intentionaldeception detection based on facial muscle movements in an interactive social context",
      "authors": [
        "Z Dong",
        "G Wang",
        "S Lu",
        "L Dai",
        "S Huang",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "2",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "P Barrett"
      ],
      "year": "2016",
      "venue": "The Expression of the Emotions in Man and Animals"
    },
    {
      "citation_id": "3",
      "title": "The duchenne smile: Emotional expression and brain physiology: Ii",
      "authors": [
        "P Ekman",
        "R Davidson",
        "W Friesen"
      ],
      "year": "1990",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "4",
      "title": "Mapping the emotional face. how individual face parts contribute to successful emotion recognition",
      "authors": [
        "M Wegrzyn",
        "M Vogt",
        "B Kireclioglu",
        "J Schneider",
        "J Kissler"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "5",
      "title": "Methods, databases and recent advancement of vision-based hand gesture recognition for hci systems: A review",
      "authors": [
        "D Sarma",
        "M Bhuyan"
      ],
      "year": "2021",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "6",
      "title": "On computational modeling of visual saliency: Examining what's right, and what's left",
      "authors": [
        "N Bruce",
        "C Wloka",
        "N Frosst",
        "S Rahman",
        "J Tsotsos"
      ],
      "year": "2015",
      "venue": "Vision Research"
    },
    {
      "citation_id": "7",
      "title": "Let the avatar brighten your smile: Effects of enhancing facial expressions in virtual environments",
      "authors": [
        "S Oh",
        "J Bailenson",
        "N Krämer",
        "B Li"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "8",
      "title": "Recognizing genuine from posed facial expressions: exploring the role of dynamic information and face familiarity",
      "authors": [
        "K Lander",
        "N Butcher"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "9",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "Dynamics of facial actions for assessing smile genuineness",
      "authors": [
        "M Kawulok",
        "J Nalepa",
        "J Kawulok",
        "B Smolka"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "11",
      "title": "A new descriptor for smile classification based on cascade classifier in unconstrained scenarios",
      "authors": [
        "O Hassen",
        "N Abu",
        "Z Abidin",
        "S Darwish"
      ],
      "year": "2021",
      "venue": "Symmetry"
    },
    {
      "citation_id": "12",
      "title": "Features selection for classification of smiles codes based on their function,\" in ISRITI",
      "authors": [
        "D Ratnawati",
        "S Anam"
      ],
      "year": "2019",
      "venue": "Features selection for classification of smiles codes based on their function,\" in ISRITI"
    },
    {
      "citation_id": "13",
      "title": "The timing of facial motion in posed and spontaneous smiles",
      "authors": [
        "J Cohn",
        "K Schmidt"
      ],
      "year": "2004",
      "venue": "International Journal of Wavelets, Multiresolution and Information Processing"
    },
    {
      "citation_id": "14",
      "title": "Eyes do not lie: Spontaneous versus posed smiles",
      "authors": [
        "H Dibeklioglu",
        "R Valenti",
        "A Salah",
        "T Gevers"
      ],
      "year": "2010",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Differentiating spontaneous from posed facial expressions within a generic facial expression recognition framework",
      "authors": [
        "T Pfister",
        "X Li",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "ICCV Workshops. IEEE"
    },
    {
      "citation_id": "16",
      "title": "Spontaneous versus posed smile recognition using discriminative local spatial-temporal descriptors",
      "authors": [
        "P Wu",
        "H Liu",
        "X Zhang"
      ],
      "year": "2014",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Recognition of genuine smiles",
      "authors": [
        "H Dibeklioglu",
        "A Salah",
        "T Gevers"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Spontaneous versus posed smile recognition via region-specific texture descriptor and geometric facial dynamics",
      "authors": [
        "P.-P Wu",
        "H Liu",
        "X.-W Zhang",
        "Y Gao"
      ],
      "year": "2017",
      "venue": "Frontiers of Information Technology & Electronic Engineering"
    },
    {
      "citation_id": "19",
      "title": "Spontaneous versus posed smiles-can we tell the difference",
      "authors": [
        "B Mandal",
        "N Ouarti"
      ],
      "year": "2016",
      "venue": "CVIP"
    },
    {
      "citation_id": "20",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "21",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "22",
      "title": "Distinguishing posed and spontaneous smiles by facial dynamics",
      "authors": [
        "B Mandal",
        "D Lee",
        "N Ouarti"
      ],
      "year": "2017",
      "venue": "ACCV"
    },
    {
      "citation_id": "23",
      "title": "Realsmilenet: A deep end-to-end network for spontaneous and posed smile recognition",
      "authors": [
        "Y Yang",
        "M Hossain",
        "T Gedeon",
        "S Rahman"
      ],
      "year": "2021",
      "venue": "Computer Vision -ACCV 2020"
    },
    {
      "citation_id": "24",
      "title": "Less is more: Facial landmarks can recognize a spontaneous smile",
      "authors": [
        "M Faroque",
        "Y Yang",
        "M Hossain",
        "S Naim",
        "N Mohammed",
        "S Rahman"
      ],
      "year": "2022",
      "venue": "Less is more: Facial landmarks can recognize a spontaneous smile",
      "arxiv": "arXiv:2210.04240"
    },
    {
      "citation_id": "25",
      "title": "Pstnet: Point spatio-temporal convolution on point cloud sequences",
      "authors": [
        "H Fan",
        "X Yu",
        "Y Ding",
        "Y Yang",
        "M Kankanhalli"
      ],
      "year": "2022",
      "venue": "Pstnet: Point spatio-temporal convolution on point cloud sequences",
      "arxiv": "arXiv:2205.13713"
    },
    {
      "citation_id": "26",
      "title": "Point 4d transformer networks for spatio-temporal modeling in point cloud videos",
      "authors": [
        "H Fan",
        "Y Yang",
        "M Kankanhalli"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "28",
      "title": "Deepmarkernet: Leveraging supervision from the duchenne marker for spontaneous smile recognition",
      "authors": [
        "M Hasan",
        "K Rafat",
        "F Rahman",
        "N Mohammed",
        "S Rahman"
      ],
      "year": "2024",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "29",
      "title": "Hadamard product in deep learning: Introduction, advances and challenges",
      "authors": [
        "G Chrysos",
        "Y Wu",
        "R Pascanu",
        "P Torr",
        "V Cevher"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Attention mesh: High-fidelity face mesh prediction in real-time",
      "authors": [
        "I Grishchenko",
        "A Ablavatski",
        "Y Kartynnik",
        "K Raveendran",
        "M Grundmann"
      ],
      "year": "2020",
      "venue": "Attention mesh: High-fidelity face mesh prediction in real-time",
      "arxiv": "arXiv:2006.10962"
    },
    {
      "citation_id": "31",
      "title": "Signal characteristics of spontaneous facial expressions: Automatic movement in solitary and social smiles",
      "authors": [
        "K Schmidt",
        "J Cohn",
        "Y Tian"
      ],
      "year": "2003",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "32",
      "title": "Walk in the cloud: Learning curves for point clouds shape analysis",
      "authors": [
        "T Xiang",
        "C Zhang",
        "Y Song",
        "J Yu",
        "W Cai"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    }
  ]
}