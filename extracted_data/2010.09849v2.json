{
  "paper_id": "2010.09849v2",
  "title": "Facial Emotion Recognition With Noisy Multi-Task Annotations",
  "published": "2020-10-19T20:39:37Z",
  "authors": [
    "Siwei Zhang",
    "Zhiwu Huang",
    "Danda Pani Paudel",
    "Luc Van Gool"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotions can be inferred from facial expressions. However, the annotations of facial expressions are often highly noisy in common emotion coding models, including categorical and dimensional ones. To reduce human labelling effort on multi-task labels, we introduce a new problem of facial emotion recognition with noisy multitask annotations. For this new problem, we suggest a formulation from the point of joint distribution match view, which aims at learning more reliable correlations among raw facial images and multi-task labels, resulting in the reduction of noise influence. In our formulation, we exploit a new method to enable the emotion prediction and the joint distribution learning in a unified adversarial learning game. Evaluation throughout extensive experiments studies the real setups of the suggested new problem, as well as the clear superiority of the proposed method over the state-of-the-art competing methods on either the synthetic noisy labeled CIFAR-10 or practical noisy multitask labeled RAF and AffectNet. The code is available at https://github.com/sanweiliti/noisyFER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As a window to the mind, face expresses various human emotions and intents in everyday life. This leads to a common assumption that a person's emotional state can be readily inferred from his or her facial movements. To automatically recognize facial expressions of emotions, large amounts of datasets (e.g.,  [31, 53, 63, 6, 11, 37, 28, 24, 25] ) and machine learning methods (e.g.,  [59, 46, 38, 3, 30, 21, 65, 58, 29] ) have been suggested. However, how people communicate and understand basic categorical emotions (i.e., happiness, sadness, anger, disgust, surprise and fear) vary substantially across cultures, situations, and even across people within the same situation. In addition, it is often challenging for people to distinguish several facial Figure  1 . Illustration of (a) biased annotations on categorical emotions  [37] , (b) the association between categorical and dimensional emotions.\n\nemotion pairs such as, anger vs. disgust, and surprise vs. fear. Fig.  1(a)  shows some examples where two experts perceive emotions differently. Moreover, the challenge of correctly annotating emotions increases dramatically when people are asked to annotate dimensional emotions, i.e., valence and arousal values, which are typically defined within a continuous range of  [-1, 1] . Therefore, biased annotations of facial expressions are inevitable and ubiquitous. On the other hand, as illustrated in Fig.  1  (b), the categorical and dimensional labels have close correlation, despite of them being targeted for different tasks, i.e., emotion recognition vs. affect prediction.\n\nIn this paper, we explore a new problem of facial emotion recognition from noisy multi-task labels. To reduce human efforts for labelling, we suggest to make use of cheap annotations, allowing noisy labels of the kind typically obtained from the web collection or using non-expert annotators. In fact, such labels for emotion already do exist in the form of different coding models across various datasets  [11, 37, 28] . Among many, two most commonly used facial emotion coding models are categorical and dimensional. Therefore, we suggest to learn emotion recognition from multi-task labels of these two kinds. Our general observation indicates that the association between image data and the available emotion labels, although noisy, are often correct when compared to the incorrect ones, for both coding models considered. Unfortunately though, directly learning from such labels results in undesired outcome during inference. In-spired by this observations, we advice to benefit from multiple labels per image hereby posing a problem of learning from related multi-task labels of different encoding models. Our proposed formulation addresses this problem from the perspective of joint distribution match, which aims at exploiting the correlations among data and multi-task labels to reduce the impact of the labels' noise.\n\nWhile the formulation facilitates us to better treat the new problem, it is challenging to achieve a unified model to play the trade-off between the basic emotion prediction and the constraint on joint distribution match. Furthermore, it is known that exact modeling of probability density function is computationally intractable  [45]  for all but the most trivial cases. Additionally for our case, due to the high heterogeneity between data and multi-task labels, it is nontrivial to learn their joint distribution. To cope with the second issue, we instead reduce the data distribution modeling to generative modeling of distributions, using generative adversarial networks  [15] , which has proved the empirical strength as generative models of arbitrary data distributions. For the third issue, we follow the idea of canonical correlation analysis  [51]  to learn the correlation (or joint distribution) of the heterogeneous data using multi-stream projections, with which a common space is pursued reducing the heterogeneity. The exploration of joint distribution learning enables a good collaboration between the emotion prediction and joint distribution learning tasks in an unified adversarial learning game.\n\nIn summary, this paper offers several contributions to address the practical case of facial emotion recognition. Major three of them are listed below:\n\n• We suggest a new problem of facial emotion recognition with noisy multi-task labels, which targets for readily available cheap multi-task annotations.\n\n• To address the proposed problem, we propose a generalized formulation with explicit joint and marginal distribution match among data and the heterogeneous multi-task labels.\n\n• In our formulation, we introduce a new adversarial learning model to optimize the training of emotion prediction with the joint and marginal distribution based constraint, which is shown to be suitable for the newly proposed problem.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Learning from noisy labels. A number of approaches have been proposed for learning noisy labels. There are several different lines to address this issue. The first line aims to parameterize the noise by a transition matrix indicating the probability to mislabel one class into another class  [48, 14, 50, 39] .  [39]  performs a forward correction and a backward correction to the noisy labels given a transition matrix.  [48, 14]  add a linear layer on top of the network to learn the transition probability jointly with the prediction network. The second line learns the label distribution  [13, 49, 56] . For example,  [13]  introduces deep label distribution learning by assuming a predefined distribution for the true labels and minimizes the Kullback-Leiber divergence.  [49, 56]  optimize both the network parameters and the label predictions as the label distributions to correct noise. Some works explore this topic with two simultaneous networks  [20, 33, 16, 57, 32, 27]  updated iteratively.\n\nThere are other methods such as sample weighting strategy to measure the confidence level of each sample  [20, 61] , or robust loss functions  [55, 40, 62]  from the loss level. Learning with multiple noisy labels has also been explored by estimating ground truth with crowdsourcing  [66]  or soft labels  [18] . For emotion recognition with inconsistent labels,  [58]  extents the single noise learning formulation by incorporating a separate transition matrix for each annotator.  [50]  proposes to add a regularization term to minimize the trace of the transition matrix for the convergence to the true label.\n\nTo summarize, previous works generally tackle the noise label issue by transition matrix, label distribution or other classification-specific methods, therefore are incapable of dealing with multi-task involving both discrete and continuous labels, while ignoring the joint distribution between the image and labels which potentially contains richer information to benefit learning. Generative modeling for joint distribution learning. Following generative adversarial network  [15] , there are emerging a few works that explore joint distribution learning in unsupervised inference tasks or conditional generation  [10, 7, 19, 8, 42, 4, 12] . For instance,  [10, 7, 2]  approximate the posterior distribution by matching two joint distributions from an inference network and a generation network in an unsupervised or semi-supervised manner. With a slight difference,  [4]  distinguishes three joint distributions including the image-label pair from data, while  [12]  incorporates two discriminators to distinguish among encoderdecoder distributions and fake-true distributions. More recently,  [42]  focuses on image synthesis from both marginal distributions and conditional distributions.  [19]  deals with conditional image synthesis by matching joint distribution. While these works have made some success on joint distribution learning, they either deal with representation learning or emphasize on image synthesis, which can hardly be applied to our noisy label learning problem without special treatment to the noisy labels. Furthermore, they generally lack abilities to learn the correlations among heterogeneous labels of different tasks. Multi-task learning for facial emotion recognition. Some works utilize auxiliary tasks to facilitate facial emotion recognition  [5, 64, 34, 41, 43, 35, 25, 54, 17] .  [54]  explores a multi-task learning model for expression recognition and action unit detection by automatically learning the weights for either task.  [17]  predicts emotions with facial landmark detection as a side task. Recently a large-scale human emotion dataset Aff-Wild2  [25]  is proposed with annotations of basic expression classes, valence-arousal values and action unit labels, and multi-task models are developed to facilitate learning for each individual task. However, these works do not treat the labelling noise problem, while noise extensively exists in common facial emotion recognition labels.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Problem Formulation",
      "text": "Noisy-labeled facial emotion recognition aims at training a robust model on facial images merely with noisy labels. Let D be the underlying truth distribution generating (X, Y ) ∈ χ × ν pairs from which n iid samples (X 1 , Y 1 ), . . . , (X n , Y n ) are drawn. After annotating these samples with a certain bias, we obtain corrupted samples (X 1 , Ỹ1 ), . . . , (X n , Ỹn ), and let the distribution of (X, Ỹ ) be D ρ . The biased annotation model P ( Ỹ |Y ) is unknown to the learner. Instances are denoted by x ∈ χ ⊆ R d , clean and noisy labels are denoted by y, ỹ respectively.\n\nA traditional way is to model the noise directly with the noisy label distribution definition p(ỹ = j|y = i) = t j,i . The probability that an input x is labeled as j in the noisy data can be computed using t j,i :\n\nwhere c is the total class number. In this way, we can modify a classification model using a probability matrix T = (t j,i ) that modifies its prediction to match the label distribution of the noisy data. Let θ denote parameters of the prediction model, and p(y|x, θ) be the prediction probability of true labels by the classification model. Then the prediction of the final model is given by\n\nHowever, modeling label noise by the transition probability matrix T has several disadvantages: firstly, the transition matrix lacks constraints for the convergence to the true T  [50] , which can lead optimization to a wrong direction, secondly, the assumption that annotators mislabel one specific class into another certain class in a constant probability may not always hold. Lastly, the probability matrix is a specific case to model discrete labels and fails in the regression task, while continuous noisy labels also exist in many tasks such as affect prediction. Hence, the relationship between the images and the corresponding labels should be considered for a more generalized model which is applicable in different tasks, as well as the multi-task setting.\n\nTo address the drawback of the traditional conditional probability modeling, we suggest to remove the transition matrix T and add a constraint on the joint distribution of (X, Y ). As noisy labels are generally outliers of the true distribution, our goal is to infer the true joint distribution to reduce impacts of outliers. Distribution-to-distribution supervision as a regularizor makes learning more robust to noise than one-to-one supervision, therefore the key idea is to match two joint distributions. If the aim is achieved, then we are ensured that all marginals as well as all conditional distributions are aligned to some extent. Here, we consider the following two joint probability distributions over two pairs of data and labels:\n\nwhere (x, y 0 , y 1 ) is a triplet of the input facial image, its predicted non-emotion latent variable, and emotion label, (ỹ 0 , ỹ1 , x) is a triplet of the random non-emotion latent variable, the noisy emotion label and the facial image inferred from them, and ϑ is the synthesis mapping from (ỹ 0 , ỹ1 ) to x. To achieve a reliable constraint, a natural way is to optimize Kullback-Leibler or Jensen-Shannon divergence for the alignment of the two joint distributions.\n\nIn the multi-task setting for facial emotion understanding, labels of different tasks such as categorical emotion labels (discrete emotion classes), and dimensional emotion labels (continuous valence-arousal values) convey complementary information, which can be utilized for better facial emotion recognition. Still, all types of labels are noisy and this joint distribution learning framework can be easily extended to the multi-task setting. Assume each sample X n is labeled by T types of noisy labels Y 1 n , . . . , Y T n , and the goal is to learn the joint distribution of the samples and all corresponding labels (X, Y 1 , . . . , Y T ), then it is desired to optimize the alignment between the following two joint distributions: p(x, y 0 , y 1 , . . . , y T |θ) = p(x)p(y 0 , y 1 , . . . , y T |x, θ), q(x, ỹ0 , ỹ1 , . . . , ỹT |ϑ) = q(ỹ 0 , ỹ1 , . . . , ỹT )q(x|ỹ 0 , ỹ1 , . . . , ỹT , ϑ),\n\nwhere y 0 , ỹ0 are non-emotion latent variables.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "As it is computationally intractable to model the explicit probability density function of data distributions of realworld data  [45] , it is generally infeasible to match the two joint distributions with the exact modeling. To overcome this issue, we resort to the generative adversarial modeling methodology  [15] , which models a distribution with a generator and approximates the model distribution to the true distribution with a discriminator.\n\nTo model the two distributions in Eqn. 4, we exploit two components for the generator: One is an encoder G Y that Figure  2 . Illustration of (a) the overall architecture, where a pair of symmetric encoder GY and decoder GX aims for simultaneous inference and image generation, with samples from both GY and GX fed into D to align the distributions, (b) the discriminator architecture, which is trained to learn the joint score Sjoint, as well as the marginal scores Sx, S y i . x indicates the real input image, y 0 , ŷ1 , . . . , ŷT correspond to the predicted non-emotion and multi-task emotion labels by the encoder GY , ỹ0 is a Gaussian noise, ỹ1 , . . . , ỹT are the noisy labels of image x, and x corresponds to the generated image by the decoder GX . learns the function θ to infer the clean labels from the input images, while the other is a decoder G X that learns the function ϑ to produce the facial images with the corresponding expression from the noisy labels. The architecture is illustrated in Fig.  2 (a) . As G X is an image generator that maps from the label space back to the image space, an additional vector y 0 is incorporated into our learning scheme to model other attributes of the image except for the input labels. The encoder G Y predicts the clean labels ŷ1 , . . . , ŷT along with the latent noise y 0 . In the meantime, the decoder G X takes a Gaussian noise ỹ0 , to generate image x conditioned on the noisy labels ỹ1 , . . . , ỹT :\n\nIn order to match the joint distributions captured by the encoder and decoder, an adversarial game is played between the generator and the discriminator. In particular, the discriminator (Fig.  2 (b) ) is designed to match the joint distribution of the group of the facial image, the noise vector, and the multi-task labels from G Y and G X . For the joint distribution alignment, a natural way is to feed a pair of the group (X, Y 0 , . . . , Y T ), sampled from the encoder p(x, y 0 , ŷ1 , . . . , ŷT ) and the decoder q(x, ỹ0 , ỹ1 , . . . , ỹT ) respectively, into the discriminator network for the adversarial training. Such distribution-to-distribution learning avoids overfitting to specific samples, leading to more robust inference compared to one-to-one supervision which is easily corrupted by the noisy labels.\n\nHowever, the data within each group is highly heterogeneous and thus the direct concatenation on them could hurt the distribution learning. To reduce the heterogeneity among the data and multi-task labels, we suggest to exploit multiple network streams to seek their common space where their feature embeddings are less heterogeneous and hence their correlation can be better learned. As shown in Fig.  2  (b), the output embeddings of all the network streams are finally fed into a common network where their joint distributions are learned by the joint score S joint . Besides the advantage to leverage noisy labels of each task, the joint distribution learning also takes advantage of the implicit relationships among related tasks by S joint to facilitate learning for each task. The full objective function is given by\n\nwhere y = (y 0 , ŷ1 , . . . , ŷT ), ỹ = (ỹ 0 , ỹ1 , . . . , ỹT ), and the integral forms of the two regularizers\n\nThe proposed generator and discriminator enable us to optimize the emotion prediction based loss and the distribution match based constraint within a unified framework. On one hand, the encoder G X is trained to predict the clean labels. On the other hand, the discriminator D learns to align the distributions by distinguishing them, while the encoder G Y and decoder G X are trained jointly to fool the discriminator in an adversarial game. According to such scheme, we exploit a min-max objective function for our final solution:\n\nwhere y = (y 0 , ŷ1 , . . . , ŷT ), ỹ = (ỹ 0 , ỹ1 , . . . , ỹT ), λ plays a trade-off between the multi-task based prediction loss f (G Y (x), ỹ) and the joint distribution learning constraint with the two component functions g(D(x, G Y (x))), h(D(G X (ỹ), ỹ)) and ĥ(D(G X (ỹ), ỹ)), which correspond to adversarial losses  [9] . We adopt the hinge loss for the adversarial loss that is commonly used by exiting works like  [36, 60] . For the multi-task learning, f (G Y (x), ỹ) can be realized by applying a regular cross entropy loss to the emotion recognition task, with a similarity loss for the continuous labels, which can be replaced by any targetspecific loss, such as the Concordance Correlation Coefficient (CCC) loss  [44, 52]   sample Gaussian noise ỹ0 1 , . . . , ỹ0 m from N (0, 1)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4:",
      "text": "ỹj ← (ỹ 0 j , ỹ1 j , . . . , ỹT j ) for all j\n\n))] 7: end for to be more efficient than the common L2 loss  [37, 24, 23] . Formally, the corresponding functions f , g, h, ĥ applied for the minimization on the generator are given by f ((y d , y c ), (ỹ d , ỹc )) = L CE (y d , ỹd ) + γL sim (y c , ỹc )\n\nwhere y d , ỹd denote the discrete labels, y c , ỹc indicate the continuous labels, and z represents the discriminator's output. L CE and L sim denote the cross entropy loss and the similarity loss for discrete and continuous labels respectively. γ is the trade-off between L CE and L sim .\n\nAlthough aligning the joint distribution by S joint implicitly matches the marginal distribution, the noise and true label distributions can be very different among noisy heterogeneous labels. In this case, explicit marginal distribution alignment is beneficial. The alignment between the synthetic image distribution and the real image distribution can guide the decoder to generate more realistic images, meanwhile enforcing the distribution of decoder's predicted labels ŷ1 , . . . , ŷT to match the distribution of the noisy labels ỹ1 , . . . , ỹT . Accordingly, each individual network stream within D is expected to learn the corresponding marginal distribution by the marginal scores {S x , S y 0 , S y 1 , ..., S y T } as shown in Fig.  2 (b ). Since we consider the facial emotion recognition as the target task, we use affect prediction as the auxiliary task to benefit the target task from both the imageto-label relationship and task-to-task relationship. The algorithm of the proposed method is illustrated in Alg. 1.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Evaluation",
      "text": "We evaluate the proposed model in two scenarios: (1) a synthetic noisy labeled dataset (CIFAR-10  [26] ) for image classification; (2) two practical facial emotion datasets (RAF  [28]  and AffectNet  [37] ) for facial emotion recognition. For a more real-world setup, we do not use clean validation labels for model selection, and thus the finally converged trained model of each comparing method is used directly for evaluation in all experiments. Please refer to supplementary material for more implementation and architecture details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation On Synthetic Noisy Labeled Dataset",
      "text": "Experiment setup. Following  [58] , the CIFAR-10 dataset  [26]  for image classification is selected to build the synthetic noisy labeled dataset, as a simulation case to study model behavior with multiple increasing noise. CIFAR-10 includes 60,000 images of size 32x32 in 10 different categories, among which 50,000 are used for training and 10,000 for the test. Images in CIFAR-10 are labeled only for image classes, from which we generate three different sets, in order to simulate our multi-task scenario. Our simulation creates three training sets with different noisy labels, by randomly flipping 20%, 30%, and 40% (each for one set) of the corresponding clean labels. We do not introduce any noise in the test set. The modified noisy labels are uniformly selected across classes. Although this setup is not ideal in the sense of multi-task labels, the proposed model is still applicable, where three inconsistent noisy labels are treated as ỹ1 , ỹ2 , and ỹ3 , respectively. λ is set as 0.8 in the experiment. Baselines. As the encoder G Y is a VGG-backboned network, we compare the proposed model with the following baselines: VGGNet  [47]  trained on clean labels; VG-GNet trained on the majority vote of the three noisy labels; VGGNet trained with all noisy labels; auxiliary image regularizer model (AIR)  [1] , symmetric cross entropy loss method (SCE)  [55] ; Co-teaching method  [16] ; and LT-Net  [58] . Among the competing methods, LTNet is proposed to deal with inconsistent labels, while other methods mainly tackle the single noisy label issue. We adapted the remaining methods to multiple noisy labels setting, by combining losses of all label sets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis.",
      "text": "The results are summarized in Tab. 1. Note that the results of AIR and LTNet are directly from  [58] , because AIR's result cannot be reproduced by ourselves and LTNet has not released its official code. 76.37 SCE  [55]  86.34 Co-teaching  [16]  84.21 LTNet  [58]  87.23 Proposed 87.90 As can be observed, noisy labels severely hurt the learning performance, when used without further treatment. The VGGNet model trained with majority voting label performs poorly because the majority voting only decreases noise ratio, but cannot model the label distribution. The VGGNet model trained with multiple noisy labels also tends to overfit on the noisy training set by one-to-one supervision, leading to a degraded performance in the clean test set. AIR is not trained end-to-end and therefore difficult to optimize. SCE and Co-teaching deal with single noise label issue, from the perspective of robust loss function or complementary networks, hence they lack generalization abilities to the multilabel or multi-task setting. LTNet, which is specifically designed for inconsistent label setting, performs comparable to the VGGNet model trained with clean labels. However, it is not applicable for continuous labels as it models noise by the transition matrix. In comparison, the proposed model trained with multiple noisy labels achieves the best result among compared methods, and is comparable to the model trained using clean labels. Note that our method is not restricted by the number or task types or the noisy labels.\n\nThe test accuracy curves of the baselines and the proposed model over training steps are visualized in Fig.  3 . Note that the baseline models trained with noisy labels first rise to a peak accuracy quickly and decrease later. In contrast, the test accuracy of the proposed model continues increase over the training steps. Furthermore, we observed that the one-to-one cross entropy loss of the proposed model does not converge to zero, instead settles at a relatively high value. However, the cross entropy loss of VGGNet baseline models converge to almost zero, which indicates overfitting. This observation demonstrates that the joint distribution learning proposed in this paper can avoid the negative influence of the noisy labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation On Facial Emotion Dataset",
      "text": "The distribution of the synthetic noise in CIFAR-10 dataset is straightforward and only includes one task, while labelling noise can have various patterns in practical emotion datasets due to many reasons (different annotators, labelling protocols, challenges in distinguishing certain expressions, etc.). To study the effectiveness of the proposed model in the emotion recognition task with noisy multi-task labels, we evaluate the model in real-world settings. Experiment setup. A cross-dataset evaluation protocol is proposed by LTNet  [58]  to train the model on the combination of AffectNet  [37] , RAF  [28]  training set (and some unlabeled facial images), and test on several emotion recognition dataset test sets including AffectNet and RAF. However, such setting is not suitable to our new problem: this training setting assumes that the labelling of AffectNet and RAF are biased and inaccurate, while reporting test accuracy on both AffectNet validation set and RAF test set to demonstrate the superiority of the proposed framework assumes that the same labelling on these two datasets can result in clean labels for test, which is contradictory with the training. Therefore, for the suggested new problem, we propose a more appropriate evaluation procedure on RAF and AffectNet. We use machine labeling by pretrained models, which inherently introduces noise due to domain gap and human-machine disagreements. Baselines. As existing methods for noisy label learning either are not applicable, or cannot be easily adapted to the multi-task setting or continuous labels, we choose to train two of state-of-the-art methods for single discrete noisy label learning, i.e., SCE  [55] , Co-teaching  [16] , only with discrete expression labels. For comparison, we also train the VGGNet model and a degraded version of our model in the single-task setup only with discrete expression labels or continuous valence-arousal labels. To evaluate the proposed multi-task model's full effectiveness, we train it with both expression and valence-arousal labels. With no existing work applicable for multi-task noisy label learning with both discrete and continuous labels, we train another VG-GNet model with multi-task labels with loss combination of both tasks for comparison.\n\nThe RAF dataset consists of 15,339 real-world facial with the original valence-arousal labels of AffectNet, and the trained model is evaluated on the AffectNet validation set. We denote the two experiment scenarios by RAF-base and AffectNet-base respectively. For valence-arousal prediction, we only report results in AffectNet-base case since no human valence-arousal labels are available on RAF.\n\nResults and analysis. The experiment results on facial emotion datasets are illustrated in Tab. 2. For the discrete expression prediction, in the single-task RAF-base case, SCE and the proposed model achieve similar performance, all higher than VGGNet, and Co-teaching achieves the best performance. Co-teaching trains two networks simultaneously, with each one selecting small-loss instances (more likely to have correct labels) and teaching its peer network. However, such instances are less likely to occur with in-creasing noise intensity. Therefore, Co-teaching is a strong baseline for the single-task RAF-base case, where the noise level is relatively low. For the AffectNet-base case where the model is exposed to higher noise, SCE and Co-teaching face difficulties for optimization with more complicated noise distributions, and perform even worse than VGGNet. Moreover, it is non-trivial to adapt these two baselines to multi-task. In contrast, the proposed model takes full advantage from multi-task noisy labels and achieves the best performance. For the continuous valence-arousal prediction, we utilize the commonly used Concordance Correlation Coefficient (CCC) and Mean Square Error (MSE) as the evaluation metrics. We can observe that the proposed model trained in the multi-task setting significantly improves prediction performance compared with the VGGNet baseline method and the single-task models.\n\nFor the case of clean labels, it is natural to combine loss functions of each task in multi-task learning  [25] . Nevertheless such simple loss combination is not necessarily beneficial in the presence of noisy labels for each task, or even harmful, which could be observed in Tab. 2. However, the proposed multi-task model demonstrates superior performance compared with multi-task VGGNet and single-task models. Our joint distribution learning scheme not only leverages noise for each task individually, but also learns a robust correlation between different tasks as well as the image, to take advantage of labels of both tasks and utilize richer information for emotion recognition without being corrupted by the noise in labels of each task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "Here we conduct the ablation study to investigate the contributions of marginal scores {S x , S y 0 , S y 1 , ...} and joint scores S joint in D, the decoder G X and extra labels of the proposed model. The ablation study is implemented on the following four cases: (1) single-label model on the synthetic noisy CIFAR-10 dataset with 20% noise in the",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conditional Image Synthesis",
      "text": "Our proposed joint distribution learning framework learns inference and conditional image synthesis simultaneously. Since the distribution learning only serves as a regularizar in our algorithm, the generated images are not necessarily of the highest quality, but should carry explicit semantic meaning. Fig.  4  presents generated samples on CIFAR-10 (Fig.  4  (a)), RAF (Fig.  4 (b) ) and AffectNet (Fig.  4 (c) ). The decoder G X can generate correct images given the conditional label, which demonstrates that the two joint distributions could be aligned with the adversarial training.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper introduces an interesting problem of facial emotion recognition with noisy multi-task annotations, which has a high potential to reduce human labelling efforts for multi-task learning. To better treat the problem, we introduce a new formulation from the view of joint distribution match. Following the suggested formulation, we exploit a new adversarial learning method to jointly optimize the emotion prediction and the joint distribution learning. Finally we study the setup of synthetic noisy labeled dataset and practical noisy multi-task datasets, and experiments demonstrate the clear advantage of the proposed method for the new problem. While we can roughly setup the trade-off between the emotion prediction loss and the joint distribution match based constraint, automatically adapting the balance would be interesting to study in our future work.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of (a) biased annotations on categorical emo-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) shows some examples where two experts",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of (a) the overall architecture, where a pair",
      "page": 4
    },
    {
      "caption": "Figure 2: (a). As GX is an image generator that",
      "page": 4
    },
    {
      "caption": "Figure 2: (b), the output embeddings of all the network streams",
      "page": 4
    },
    {
      "caption": "Figure 2: (b). Since we consider the facial emotion",
      "page": 5
    },
    {
      "caption": "Figure 3: Test accuracy vs. training steps on CIFAR-10 synthetic",
      "page": 6
    },
    {
      "caption": "Figure 3: Note that the baseline models trained with noisy labels ﬁrst",
      "page": 6
    },
    {
      "caption": "Figure 4: Samples synthesized by the decoder GX conditioned on the input label: (a) samples from CIFAR-10, and each column condi-",
      "page": 8
    },
    {
      "caption": "Figure 2: is necessary to encode",
      "page": 8
    },
    {
      "caption": "Figure 4: presents generated samples on CIFAR-",
      "page": 8
    },
    {
      "caption": "Figure 4: (a)), RAF (Fig. 4 (b)) and AffectNet (Fig. 4 (c)).",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Test accuracy on CIFAR-10 synthetic dataset.",
      "page": 6
    },
    {
      "caption": "Table 2: Evaluation results on facial emotion datasets. Single-task refers to models trained only with categorical expression labels or",
      "page": 7
    },
    {
      "caption": "Table 3: Ablation study for different components of the proposed model: w/o joint, w/o marginal and w/o GX respectively remove Sjoint,",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Auxiliary image regularization for deep cnns with noisy labels",
      "authors": [
        "Samaneh Azadi",
        "Jiashi Feng",
        "Stefanie Jegelka",
        "Trevor Darrell"
      ],
      "year": "2015",
      "venue": "Auxiliary image regularization for deep cnns with noisy labels",
      "arxiv": "arXiv:1511.07069"
    },
    {
      "citation_id": "2",
      "title": "Large scale gan training for high fidelity natural image synthesis",
      "authors": [
        "Andrew Brock",
        "Jeff Donahue",
        "Karen Simonyan"
      ],
      "year": "2018",
      "venue": "Large scale gan training for high fidelity natural image synthesis",
      "arxiv": "arXiv:1809.11096"
    },
    {
      "citation_id": "3",
      "title": "Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing",
      "authors": [
        "Xi Chen",
        "Qihang Lin",
        "Dengyong Zhou"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "Triple generative adversarial nets",
      "authors": [
        "Taufik Li Chongxuan",
        "Jun Xu",
        "Bo Zhu",
        "Zhang"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Multi-task learning of facial landmarks and expression",
      "authors": [
        "Terrance Devries",
        "Kumar Biswaranjan",
        "Graham Taylor"
      ],
      "year": "2014",
      "venue": "2014 Canadian Conference on Computer and Robot Vision"
    },
    {
      "citation_id": "6",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
    },
    {
      "citation_id": "7",
      "title": "Adversarial feature learning",
      "authors": [
        "Jeff Donahue",
        "Philipp Krähenbühl",
        "Trevor Darrell"
      ],
      "year": "2016",
      "venue": "Adversarial feature learning",
      "arxiv": "arXiv:1605.09782"
    },
    {
      "citation_id": "8",
      "title": "Large scale adversarial representation learning",
      "authors": [
        "Jeff Donahue",
        "Karen Simonyan"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Towards a deeper understanding of adversarial losses",
      "authors": [
        "Wen Hao",
        "Yi-Hsuan Dong",
        "Yang"
      ],
      "year": "2019",
      "venue": "Towards a deeper understanding of adversarial losses",
      "arxiv": "arXiv:1901.08753"
    },
    {
      "citation_id": "10",
      "title": "Adversarially learned inference",
      "authors": [
        "Ishmael Vincent Dumoulin",
        "Ben Belghazi",
        "Olivier Poole",
        "Alex Mastropietro",
        "Martin Lamb",
        "Aaron Arjovsky",
        "Courville"
      ],
      "year": "2016",
      "venue": "Adversarially learned inference",
      "arxiv": "arXiv:1606.00704"
    },
    {
      "citation_id": "11",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Triangle generative adversarial networks",
      "authors": [
        "Zhe Gan",
        "Liqun Chen",
        "Weiyao Wang",
        "Yuchen Pu",
        "Yizhe Zhang",
        "Hao Liu",
        "Chunyuan Li",
        "Lawrence Carin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Deep label distribution learning with label ambiguity",
      "authors": [
        "Bin-Bin Gao",
        "Chao Xing",
        "Chen-Wei Xie",
        "Jianxin Wu",
        "Xin Geng"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "14",
      "title": "Training deep neural-networks using a noise adaptation layer",
      "authors": [
        "Jacob Goldberger",
        "Ehud Ben-Reuven"
      ],
      "year": "2016",
      "venue": "Training deep neural-networks using a noise adaptation layer"
    },
    {
      "citation_id": "15",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Coteaching: Robust training of deep neural networks with extremely noisy labels",
      "authors": [
        "Bo Han",
        "Quanming Yao",
        "Xingrui Yu",
        "Gang Niu",
        "Miao Xu",
        "Weihua Hu",
        "Ivor Tsang",
        "Masashi Sugiyama"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Deep multi-task learning to recognise subtle facial expressions of mental states",
      "authors": [
        "Guosheng Hu",
        "Li Liu",
        "Yang Yuan",
        "Zehao Yu",
        "Yang Hua",
        "Zhihong Zhang",
        "Fumin Shen",
        "Ling Shao",
        "Timothy Hospedales",
        "Neil Robertson"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "18",
      "title": "Learning to recognize human activities using soft labels",
      "authors": [
        "Ninghang Hu",
        "Gwenn Englebienne",
        "Zhongyu Lou",
        "Ben Kröse"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "Bidirectional conditional generative adversarial networks",
      "authors": [
        "Ayush Jaiswal",
        "Wael Abdalmageed",
        "Yue Wu",
        "Premkumar Natarajan"
      ],
      "year": "2018",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels",
      "authors": [
        "Lu Jiang",
        "Zhengyuan Zhou",
        "Thomas Leung",
        "Li-Jia Li",
        "Li Fei-Fei"
      ],
      "year": "2017",
      "venue": "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels",
      "arxiv": "arXiv:1712.05055"
    },
    {
      "citation_id": "21",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "Heechul Jung",
        "Sihaeng Lee",
        "Junho Yim",
        "Sunjeong Park",
        "Junmo Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "22",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "23",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "25",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "26",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "27",
      "title": "Learning to learn from noisy labeled data",
      "authors": [
        "Junnan Li",
        "Yongkang Wong",
        "Qi Zhao",
        "Mohan Kankanhalli"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "30",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "Ping Liu",
        "Shizhong Han",
        "Zibo Meng",
        "Yan Tong"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognitionworkshops"
    },
    {
      "citation_id": "32",
      "title": "Dimensionality-driven learning with noisy labels",
      "authors": [
        "Xingjun Ma",
        "Yisen Wang",
        "Michael Houle",
        "Shuo Zhou",
        "Sarah Erfani",
        "Shu-Tao Xia",
        "Sudanthi Wijewickrema",
        "James Bailey"
      ],
      "year": "2018",
      "venue": "Dimensionality-driven learning with noisy labels",
      "arxiv": "arXiv:1806.02612"
    },
    {
      "citation_id": "33",
      "title": "Decoupling\" when to update\" from\" how to update",
      "authors": [
        "Eran Malach",
        "Shai Shalev-Shwartz"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Identity-aware convolutional neural network for facial expression recognition",
      "authors": [
        "Zibo Meng",
        "Ping Liu",
        "Jie Cai",
        "Shizhong Han",
        "Yan Tong"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "35",
      "title": "Dynamic multitask learning for face recognition with facial expression",
      "authors": [
        "Zuheng Ming",
        "Junshi Xia",
        "Muhammad Muzzamil Luqman",
        "Jean-Christophe Burie",
        "Kaixing Zhao"
      ],
      "year": "2019",
      "venue": "Dynamic multitask learning for face recognition with facial expression",
      "arxiv": "arXiv:1911.03281"
    },
    {
      "citation_id": "36",
      "title": "Spectral normalization for generative adversarial networks",
      "authors": [
        "Takeru Miyato",
        "Toshiki Kataoka",
        "Masanori Koyama",
        "Yuichi Yoshida"
      ],
      "year": "2018",
      "venue": "Spectral normalization for generative adversarial networks",
      "arxiv": "arXiv:1802.05957"
    },
    {
      "citation_id": "37",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Facial expression recognition",
      "authors": [
        "Maja Pantic"
      ],
      "year": "2009",
      "venue": "Encyclopedia of biometrics"
    },
    {
      "citation_id": "39",
      "title": "Making deep neural networks robust to label noise: A loss correction approach",
      "authors": [
        "Giorgio Patrini",
        "Alessandro Rozza",
        "Aditya Menon",
        "Richard Nock",
        "Lizhen Qu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Regularizing neural networks by penalizing confident output distributions",
      "authors": [
        "Gabriel Pereyra",
        "George Tucker",
        "Jan Chorowski",
        "Łukasz Kaiser",
        "Geoffrey Hinton"
      ],
      "year": "2017",
      "venue": "Regularizing neural networks by penalizing confident output distributions",
      "arxiv": "arXiv:1701.06548"
    },
    {
      "citation_id": "41",
      "title": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "authors": [
        "Gerard Pons",
        "David Masip"
      ],
      "year": "2018",
      "venue": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "arxiv": "arXiv:1802.06664"
    },
    {
      "citation_id": "42",
      "title": "Jointgan: Multi-domain joint distribution learning with generative adversarial nets",
      "authors": [
        "Yunchen Pu",
        "Shuyang Dai",
        "Zhe Gan",
        "Weiyao Wang",
        "Guoyin Wang",
        "Yizhe Zhang",
        "Ricardo Henao",
        "Lawrence Carin"
      ],
      "year": "2018",
      "venue": "Jointgan: Multi-domain joint distribution learning with generative adversarial nets",
      "arxiv": "arXiv:1806.02978"
    },
    {
      "citation_id": "43",
      "title": "An all-in-one convolutional neural network for face analysis",
      "authors": [
        "Rajeev Ranjan",
        "Swami Sankaranarayanan",
        "Carlos Castillo",
        "Rama Chellappa"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "44",
      "title": "Avec 2015: The 5th international audio/visual emotion challenge and workshop",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    },
    {
      "citation_id": "45",
      "title": "Deep boltzmann machines",
      "authors": [
        "Ruslan Salakhutdinov",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "Artificial intelligence and statistics"
    },
    {
      "citation_id": "46",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "47",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "48",
      "title": "Training convolutional networks with noisy labels",
      "authors": [
        "Sainbayar Sukhbaatar",
        "Joan Bruna",
        "Manohar Paluri",
        "Lubomir Bourdev",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Training convolutional networks with noisy labels",
      "arxiv": "arXiv:1406.2080"
    },
    {
      "citation_id": "49",
      "title": "Joint optimization framework for learning with noisy labels",
      "authors": [
        "Daiki Tanaka",
        "Daiki Ikami",
        "Toshihiko Yamasaki",
        "Kiyoharu Aizawa"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Learning from noisy labels by regularized estimation of annotator confusion",
      "authors": [
        "Ryutaro Tanno",
        "Ardavan Saeedi",
        "Swami Sankaranarayanan",
        "Nathan Daniel C Alexander",
        "Silberman"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Canonical correlation analysis. Encyclopedia of statistics in behavioral science",
      "authors": [
        "Bruce Thompson"
      ],
      "year": "2005",
      "venue": "Canonical correlation analysis. Encyclopedia of statistics in behavioral science"
    },
    {
      "citation_id": "52",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "53",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "Michel Valstar",
        "Maja Pantic"
      ],
      "year": "2010",
      "venue": "Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "54",
      "title": "Multi-task learning of emotion recognition and facial action unit detection with adaptively weights sharing network",
      "authors": [
        "Chu Wang",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "55",
      "title": "Symmetric cross entropy for robust learning with noisy labels",
      "authors": [
        "Yisen Wang",
        "Xingjun Ma",
        "Zaiyi Chen",
        "Yuan Luo",
        "Jinfeng Yi",
        "James Bailey"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Probabilistic end-to-end noise correction for learning with noisy labels",
      "authors": [
        "Kun Yi",
        "Jianxin Wu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "How does disagreement help generalization against label corruption",
      "authors": [
        "Xingrui Yu",
        "Bo Han",
        "Jiangchao Yao",
        "Gang Niu",
        "Ivor Tsang",
        "Masashi Sugiyama"
      ],
      "year": "2019",
      "venue": "How does disagreement help generalization against label corruption",
      "arxiv": "arXiv:1901.04215"
    },
    {
      "citation_id": "58",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "59",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "60",
      "title": "Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks",
      "authors": [
        "Han Zhang",
        "Ian Goodfellow"
      ],
      "year": "2018",
      "venue": "Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks",
      "arxiv": "arXiv:1805.08318"
    },
    {
      "citation_id": "61",
      "title": "Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition",
      "authors": [
        "Weihe Zhang",
        "Yali Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Zhilu Zhang",
        "Mert Sabuncu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "63",
      "title": "Facial expression recognition from nearinfrared videos",
      "authors": [
        "Guoying Zhao",
        "Xiaohua Huang",
        "Matti Taini",
        "Stan Li",
        "Matti Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "64",
      "title": "Deep multi-task learning for facial expression recognition and synthesis based on selective feature sharing",
      "authors": [
        "Rui Zhao",
        "Tianshan Liu",
        "Jun Xiao",
        "Kin-Man Daniel Pk Lun",
        "Lam"
      ],
      "year": "2020",
      "venue": "Deep multi-task learning for facial expression recognition and synthesis based on selective feature sharing",
      "arxiv": "arXiv:2007.04514"
    },
    {
      "citation_id": "65",
      "title": "Peakpiloted deep network for facial expression recognition",
      "authors": [
        "Xiangyun Zhao",
        "Xiaodan Liang",
        "Luoqi Liu",
        "Teng Li",
        "Yugang Han",
        "Nuno Vasconcelos",
        "Shuicheng Yan"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "66",
      "title": "Truth inference in crowdsourcing: Is the problem solved?",
      "authors": [
        "Yudian Zheng",
        "Guoliang Li",
        "Yuanbing Li",
        "Caihua Shan",
        "Reynold Cheng"
      ],
      "year": "2017",
      "venue": "Proceedings of the VLDB Endowment"
    }
  ]
}